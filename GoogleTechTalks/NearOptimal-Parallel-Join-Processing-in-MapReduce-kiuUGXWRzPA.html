<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Near-Optimal Parallel Join Processing in MapReduce | Coder Coacher - Coaching Coders</title><meta content="Near-Optimal Parallel Join Processing in MapReduce - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Near-Optimal Parallel Join Processing in MapReduce</b></h2><h5 class="post__date">2011-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kiuUGXWRzPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to welcome me wreck to Google
me wreck is a database faculty member at
Northeastern University where his
research focuses on analysis of
large-scale distributed data sets
specifically focusing on scientific data
prior to that he was at Cornell where we
collaborated on some few papers on data
stream processing but today he's here to
talk about his upcoming a sigmoid paper
where he talks about optimal joint
processing in MapReduce automatic all
right thank you having done so it's
great to be here and yeah so this work
actually is collaborative work with some
of the students at Northeastern so most
of the work I'm going to talk about
today's with upper of John and due to
other students you and buff are working
on other aspects of the system that I'm
going to briefly present in the
beginning today and a lot of the
problems are motivated by collaborations
with scientists and in particular with
the Cornell Lab of Ornithology was
collecting a lot of citizen science data
and I was trying to make sense out of
the data they have to find out our
species in danger are they affected by
environmental change and so on but also
all these techniques are applicable to
other domains so just to give you a
brief introduction so at Google you're
dealing with big data already anyway but
you know out in in academia most of the
data I have available is data sets from
scientists and it can be things like
ranging from petabytes what the
physicists are producing with a large
hadron collider to things like the data
that the lab of ornithology has
collected which is basically in the
millions tens of millions of data points
with a few hundred variables and that a
problem is not so much about the size of
the data but when you try to analyze the
data there's this combinatorial
explosion of possible ways how you can
look at the data and slice and dice the
space and try and find patterns in that
space so scientists have become really
good on the left side of the equation
when it comes to collecting data
producing data from simulations
gathering the data making them available
in data warehouses and similar
constructs but the problem is what you
do with all the data and to be
successful
today's scientists has to be both an
expert in the domain as well as in data
mining machine learning statistics and
parallel processing to actually scale up
the analysis for these large data sets
and what we're trying to do is we try to
help them in a way that we want to
essentially build a search engine for
them and instead of doing web search for
documents they can now put in pattern
preferences what they want to find in
the data in a fairly simple high-level
language and we try to then find the
best matches either in the raw data or
in the model that was trained on the
data so here's a quick overview of the
entire system so some of these
components already exist others we are
still working on so on the left side you
have basically the user front end which
is a deal in the end a user-friendly
input language instead of the key word
curry you cannot specify what type of
pattern you would be interested in then
the next part is essentially think of it
like a query plan in the database but
instead of the typical SQL operators we
need other operators that help you now
let's see analyze models extract
summaries from morals and then we want
to find interesting patterns in these
summaries and I'll give you an example
for that in a second now to make
everything run in the parallel
environment we need something like an
optimizer that is now taking such a
query plan and then executing it in a
parallel environment like a MapReduce
cluster or let's say in a cloud
environment and finally the result that
the scientists will see after this query
is executed will be some rank list of
patterns it's a little difficult to see
there but think of them basically in
this case they're one-dimensional model
summaries ranked by how well these
summaries are matching the preference
that the user has specified in the
beginning so for this talk let me look
at one concrete example of a pattern
that we found and that is actually very
important for a scientist and I call
that a function joint pattern so think
of the models that the scientists are
training essentially as a thousand
dimensional function surface so if you
have a thousand variables you have this
high dimensional function that you can
learn using some standard machine
learning techniques like decision trees
SVM so others and
now in this big high dimensional
function surface you want to find a
pattern that tells you something
interesting about a bird so on the left
side the left plot is for a region that
is bc r14 think of it essentially like
is region that is the size of a state in
the United States bc of 14 is in the
northeast so think of it like the state
of Massachusetts or New York and for
that region we now extracted from the
model a pattern that shows how high is
the probability of seeing a bird in this
case the common redpoll in 1994-95 96
and so on and the two things that are
interesting about this summary one is
this biannual trend that you see this
bird much more in in this case odd years
than in even years and the second one is
when you compared to another bird in the
same region the purple finch it has also
this biennial trend but instead of
having a high probability in odd years
it now has a low probability in adios so
these things are interesting for the
scientists because now when they find
something like this automatically they
can now come up with a hypothesis that
helps them maybe something about birds
competing for the same habitat and if
you know at the same time have also data
about plans in the area we might see
that there's certain plans that have a
biannual fruit cycle that might now
attract one of the birds and then scare
away the other bird potentially because
they're competing for the same habitat
so this is an example in general think
of it this way there might be millions
or billions of these summaries and I'll
be looking four pairs of summaries that
have such an interesting property like
one being the opposite of the other and
at the same time showing some kind of
regular regularity to them by the way
feel free to ask questions anytime
during the talk
so this is a concrete example for this
function joint pattern abstractly what
are we looking for we have to input
datasets SNT this could be anything
these copy these summaries or copy let's
say log entries from let's say Twitter
and Facebook and now let's say we want
to find all the pair's SI TJ that have a
certain property that satisfy some
predicate and a predicate could be
something like a similarity threshold
between the two summaries or could be
something that two posts on Twitter and
Facebook should come from the same
location or have been submitted by the
same person so these are examples for
the join and there are many possible
predicates we could come up with and we
want to actually try to support pretty
much all possible predicates that you
can think of to get these kind of sets
of pairs SI TJ from two given data sets
SNT and our goal is to minimize the job
completion time when running such a
joint computation on a cluster or cloud
so job completion time includes
everything from starting the job until
we have to finally resolve back in the
distributed file system so why are we
using MapReduce for that but actually
our results are generalizing to other
shared nothing architectures but the
good thing about my producers it's a
very popular paradigm there's an open
source platform out there we don't have
to re-implement everything from scratch
so the main question is it's this
platform or this paradigm MapReduce
actually supporting join computation or
not so traditionally MapReduce has been
seen as we have one input data set you
do some heavy weight processing on it
now we have to combine two data sets can
we do that actually efficiently or not
in MapReduce so here's a straightforward
way how we could implement what is
called an accurate join and databases on
a MapReduce environment so know if
that's easy to see but so on the left
side sees my mom's actually visible then
on the left side we have DFS notes from
the distributed file system that are
holding now these different chunks of
the data sets SNT and
now the John we want to compute as an
equality join where the tuple from
essent from t have the same value of
some attribute a so think of a like the
name of a person we want to find now
let's say different Twitter tweets that
are from the same person in two
different data sets SNT so we don't know
where these different tuples with value
equal to one or sitting they might be on
any kind of distributed file system know
there now we have a group of mappers
that are doing the map processing and
then a group of producers that are
combining results in order to set up a
joint computation where we want to find
out all the s and T pairs that have the
same a value we can do the following for
map we simply take an input tuple and as
the key we are using now to join
attribute value so for all the vet for
all the two boats that have a value one
we use key one for all the ones that
have a value to we use key 2 and so on
and it doesn't matter which mapper does
that mapping in the end we know that
everybody has the same key value will
end up on the same reducer in the same
invocation of a reduced function and so
that reduce function now gets all the s
and T two boats that have a value 1 and
can now do the corresponding combination
to create all the st pairs that are
matching on value i equal to 1 so that's
a very straightforward way of
implementing an acura join in a
MapReduce environment the problem is
actually three disadvantages in doing it
this way the first one is dat askew well
no the first one is actually the degree
of parallelism is limited by how many
different a values we have so if a has a
very small domain while we have only as
many keys at most as we have different a
values so we can't use more reduce
function in vocations than we have a
values that's one problem the other one
is if you have data skew let's say
attribute value I equal 1 occurs in
ninety percent of all the tuples we have
in our system then the reducer who is
responsible for producing the job for a
equal one gets that big chunk of work
and everybody else gets only tiny chunks
of work
and there's no way to rebalance it in a
standard MapReduce environment and the
third one it's probably the worst how do
we generalize that to other joint types
so instead of equality let's say we're
looking for inequality or in the example
that I showed for these two different
summaries it's a non-trivial predicate
where we're looking now for matches
based on how this similar similar to
function summaries are so we can't use
the function summary as a key in this
case so we have to come up with a very
different way of mapping and reducing to
get a joint computation for those types
of other joints so we started from
scratch basically rethinking how can we
do joins any type of join in MapReduce
or in any kind of shared nothing
environment and so I call this thing a
cost model but Cosmo in this case is
probably too much all right all i have
here is basically a qualitative analysis
of how do the costs in the system get
affected and based on that cost model i
will then show you how we can optimally
distribute the work in a MapReduce
environment so what does this cost model
saying if I look at a MapReduce job in
the beginning the face where I take the
data it comes from the distributed file
system goes to the mappers that part is
very easy to distribute all these map
jobs are independent you can use very
small chunks very big chunks you can
easily balance the load where the
difference between different MapReduce
implementations of a joint starts is
with a map function that is producing
the output of the mappers which is then
fed into the reducers which are then
producing the actual joint output so
when I look at an individual reducer
machine in the system the time it takes
for this machine to complete its job
depends on the input that it gets which
is the output from the map tasks plus
the time it takes to do the actual joint
uncommitted plus the time it takes to
write the output back to the distributed
file system and the only assumption i'm
making is that the time that is input
dependent which are the times mostly in
the left side is monotonic in a way that
the more input I get the more work I
have the more time it takes me to do all
these input related tasks similarly for
the output related jobs the more output
I have to produce as a reducer the
longer it takes me to do that work and
that's really true in general for most
of these joints sometimes there's some
dependency in the data distribution but
typically this monotonicity holds and
based on that you can now think about
how can we minimize the latency from the
beginning of the MapReduce job until the
end by doing the following if we look at
each of these individual reducers if you
have a job that is input size dominated
where basically the input is a lot
bigger than the output so that the cost
of processing input related work is
actually bigger than it takes for let's
say writing the output producing all the
output tuples then what I want to do is
I want to minimize the biggest input
chunk that any machine in the system
gets similarly if I have an output
dominated join meaning the output of the
joint is a lot bigger than the input I
want to make sure that none of the
reducers gets an overly large output
chunk to take to write back to the file
system for instance so what I want to do
is I want to minimize the max producer
input or max reducer output which is the
max over all the input chunks or a max
overall the output chunks that is
assigned to reduces in the system I make
another small assumption that all these
machines are similarly capable which is
usually the case if you have a cluster
you have very similar machines or if you
buy let's see virtual machines on Amazon
you have very similarly spect virtual
machines there so now the problem turns
from minimizing latency into minimizing
max reducer input or max reduce output
or a combination of the two so you can
think of it as balancing the input or
the output load or both in some way
together
so how do we find out how we can best
balance the load for a joint and a good
way to think about a joint is when you
look at this matrix damn so let's say we
have two data sets s and T as before and
maybe if I use the mouse here so in this
matrix on the columns we have the
different tuples from data set T on the
rows we have the different tuples from
data set s and now let's say we compute
a neck rejoin where s dot a equals T dot
a and I only put the a values here now
so what does this matrix show every
entry in that matrix tells me for a
combination of some SI with some TJ if
these two triples join or not so you can
see the five joints with a five because
they have the same value the seven here
does not join with a five from T but the
seven from s joins with all the servants
from T so you can basically mark all
these cells in the matrix and any join
the joint is defined as a some subset of
the cross product or Cartesian product
the matrix is basically the Cartesian
product and by marking a subset we can
now basically represent any join result
in that matrix so there are some other
examples for inequality join and for
some what is called a band join or
epsilon join so whatever you want your
favor join you can represent it with
that matrix now what do we do with this
matrix when we try to assign jobs in our
distributed system what we want to do is
we want to assign the jobs in such a way
that every reducer is covering some of
these cells of this matrix and that
means that reduces producing the
corresponding output tuples for the join
so we have to make sure that every
shaded cell is covered by some reducer
to produce the corresponding upper two
ball and we also in this case we want to
avoid having duplicates in the output so
we want to make sure that no regions
that reduce as a covering overlapping so
we don't want the same shaded cell to be
covered by two reducer regions so this
turns into a partitioning or covering
problem where we basically want to cover
this matrix or all the shaded cells in
the matrix
in particular in a way that every shaded
cell is covered and not as covered twice
so here three different ways for the
acrid join that I showed before how we
can do that covering the one on the left
is now the covering using the standard
algorithm that I showed you before where
we use the joint attribute value a as
our key so not surprisingly for a value
five we use key 547 we use key seven and
so on so these regions are basically now
determined by the values of attribute a
and as you can see it is region seven
here how many input tuples doesn't need
well this we have to send these 2's two
bolts that have value seven and we have
to send the 3t tuples that have value
seven all to the same reducer job that
is responsible for key seven so that
reducer gets at least five input two
bolts and it produces six output tuples
so no matter how we distribute a work
over let's say three reducing machines
at least one of the machines get it five
inputs and produces at least six outputs
so if i look at the max reducer input
and max reducer output the values are at
least five and six and it turns out
there's no better way to do that so one
machine has at least five input tables
and produce at least six operable here's
another extreme version of the same idea
now instead of covering it using the
actual joint values as the key you can
actually just perform a random
assignment basically every shaded cell
is randomly assigned to one of the
reducers and now you might ask you how's
that possible but let's just assume it
was possible to find exactly these cells
assign them randomly what would that do
well in this case it turns out let's say
for instance this s tuple with values
seven here it has to be sent to all
three reducers so now you have to
duplicate a tuple three times once with
key one with key to and with key three
it gets sent to all the three reducing
machines and if you do the math and the
end it turns out that the max reducer
input is X
Eliade and the max reducer output is for
now for this example so by doing that
replication on the input side we were
able to now reduce the worst-case chunk
of output that any machine in the system
received at the cost of doing more
replication on the input side so now we
increase the cost on the input side the
third example there is now for the same
join again but this is not the best of
both worlds so we found three regions to
cover all the shaded cells and now if
you get the corresponding s tuple with
values seven let's say there is second
tubule here it would be duplicated twice
once for reducer one once for reducer 2
by creating a copy of the trouble with
key 1 and ketu so compared to this one
the input duplication is not as high but
by using these regions in a clever way
we actually also at the same time still
broke up the big task so now turns out
that the max reducer input is five and
the max reducer output is 4 for that
example so this is the optimal solution
in this case because we were able to get
the smallest possible max with user
input and output at the same time so
that's kind of the gist of the idea we
want to basically find region
partitioning like that where essentially
we have the lowest possible input worst
case input or worst case output or both
at the same time so let me present an
algorithm that can actually achieve that
assuming we know a partitioning already
so let's say somebody magically gives us
a region covering like one of the three
that I showed you in this case let's
just take a covering that is covering
the entire matrix so this would be to
compute a cross product so the MapReduce
algorithm for doing for now computing
that in parallel is actually very simple
the mappers are doing the following for
every incoming tuple we check if it's an
S or a tea tuple if it's an estoppel we
now pick a random row in that in that
mapping matrix here and let's say if you
pick this row here we generate
a mapper output with key one and another
duplicate of the triple whiskey tool so
we're looking busily what regions are
intersecting the row and that tells us
what kind of output the map job will
produce them similarly for a teachable
we pick a random column we find out
which of the regions are intersecting
with a column and that tells us what
output the job is producing for that
incoming t-tubule so that's a very
simple algorithm it just picks random
rows and columns for incoming tuples it
can be done easily in parallel on these
different mappers independently so what
does the reducer doing well the reducer
basically each machine is responsible
for one of the region's so if we have
three reducers one is responsible for
key 114214 three and all we do is we
collect basically all the tuples that
are now getting the same region key and
what a reducer is now doing it
partitions is set into the tuples that
come from s and those that come from T
and then it locally runs your favorite
join algorithm to find out who's
actually combining with whom in this
group so for cross product it would
simply combine everybody with everybody
for other joint predicates it would
evaluate a predicate and then find out
whose matching and who's not so here's
just a brief overview why does it work
actually so let's look at the tuple the
same example where we head to triple s
equal to s terday equal to 5 in the
first row and T equal T dot equal to 5
in the first column now if that s tuple
comes in we might pick randomly instead
of the first row we might pick the third
row then we send it to reduce as one and
two for the T tuple with value 5 we
might randomly pick the last column and
we send it to reduce as two and three
and as you can see the two actually
intersecting exactly one field so the
corresponding output to triple where s
dot a equals T dot a for value 5 is
produced now by reducer two and we can
go through the entire join and it turns
out it produces exactly the correct
result
so the algorithm is provably correct but
now the question is for instance why do
we randomize right why do we not just
kind of try and assign the tuples to the
correct rows and columns Riley they're
actually several reasons for that one is
to actually know in which row and s
tuple fits we have to actually find in a
pre-processing step what are the
corresponding row numbers for each of
the S tuples and what are the column
numbers for each of the T tuples so
there would be some extra work to do
that pre-processing and that's actually
not really necessary the second one is
by randomizing rows and columns even if
you have no statistics about the data it
might actually happen that in the matrix
all the shaded cells on the upper left
corner or somewhere in some small part
of the space while doing at
randomization except for some very
special corner cases we're actually able
even without any knowledge about the
data we can now distribute the shaded
cells essentially evenly over their
matrix so the reducers automatically get
a fair share of the output even if we
don't know which tuples combine in which
of them don't so that's another big
advantage and in terms of input size the
randomization is not a problem because
we're dealing with it's a tens of
thousands or millions of tuples per
machine so you can use chain of bounds
to basically proved that the probability
of getting more than let's say ten
percent of your fair share is pretty
much zero so randomization doesn't hurt
in terms of creating let's say uneven
input load and other side benefit is if
we don't really have to map each tuple
to explicitly one row or column if you
can pick a random row and column we
don't have to use the matrix of the size
s by T we can use a much smaller matrix
and we can just pick our random rows and
columns from that and it makes the
algorithm also cheaper so we don't have
to use a gigantic matrix to do our look
up stair
so so far so good if we have a mapping
we have an algorithm to deal with it and
implement it in MapReduce but how do we
find that partitioning that I showed you
how do i know which region goes for
reducer one for user 2 and so on and if
I have a join how do I know what sells I
have to cover up maybe it's not always a
good idea to cover the entire matrix
that might be waste of resources in many
cases if you have only tiny join result
so these are two problems that I'm going
to address now in the next part of the
talk so let's start with how do we find
actually the best possible partitioning
of our entire matrix so let's start with
a cross product where we have to cover
the entire matrix the basic idea is it's
fairly easy to show that you want to use
square shaped regions as much as
possible think of it this way if I have
to produce a certain number of output
cells how many input tuples do I have to
send to the corresponding reducer to
produce that many output cells well to
produce let's say 36 output sales I can
send six by six which is 12 inputs or
can send 36 x 1 which is 37 inputs to
produce 36 outputs so to minimize the
input work that I give to a reducer for
a certain output area that it covers I
want to use the square-shaped sell their
a square shape region so now let's look
at the entire matrix so we want to cover
the entire matrix that has size s by
size t cells so in order to minimize the
worst case work that anybody gets for
the output related costs we want to give
everybody the same area chunk from that
big matrix so we have our reducers every
reducer should be assigned to s times T
by our output cells so that's a lower
bound you can't have less than that as
the worst-case chunk of output work
using the dilemma that i just showed
before we can now easily prove that that
implies a lower bound on the input costs
as well because to produce s times T by
our output cells you need at least two
times square root s times T by our input
to apples that you need to send to that
reducing machine so we have a lower
bound for both the input and the output
maximal the max reducer input and Max
reducer output so we can't do better
than that no matter how we distribute to
work for cross product so can we match
those lower bounds well if we're lucky
and actually the matrix is shaped in a
way that we can use our optimal square
shaped regions of size s times T by our
and described of that in both side
lengths and we can cover the matrix
exactly with those optimal squares then
we can actually match the lower bound
for both input and output size right so
that's the easy case now what if our
area our matrix is really long and thin
right so s is a lot smaller than T or
the other way round well in that case
the idealistic region might be too big
to actually fit perfectly into that
matrix well what can we do even though
we cannot match lower bound the lower
bound is basically impossible to achieve
in this case and it's easy to show that
the optimal region is a long rectangle
that is as high as the smaller of the
two datasets so that means essentially
if you think of it as a reduced job the
reducers are getting every reducer gets
all the s tuples so it's essentially
like sending the entire data set to all
the reducers and sending chunks of tea
to the reducers to produce the output
and it turns out you can even do that
without the reducers you could run that
in a mapper only job as well but that's
as a side note only but this is also
easy to find the optimal solution here
so the hard case is when you have
something that is
kind of a little thicker but you don't
really perfectly fit these optimal
squares into it so let's say we have
nine reducing machines the green area is
our matrix we are fitting six of the
reducer areas in perfectly but then
there are three left over areas that
don't really fit well so if you just put
these square regions there we don't
cover the entire matrix so in that case
what do we do all right how can we
distribute the work how do we want to
partition that matrix in a good way so
in terms of deriving some proof on what
is the worst case it's actually easy to
do the following let's say we just
inflate all these little squares and
make them a little bigger I'm just
making so much bigger that we just cover
that green area so that's on the left
side there so we started out with these
six squares that didn't cover the green
area completely we inflate them by
certain factors so that they just cover
everything so now every output cell is
covered we're using only six of the
machines which is okay but now what is
the amount of input and output work that
each machine gets in the system so now
the worst case is actually this one on
the right side where we have a green
area that's just big enough so that we
only fit one square and there's enough
left over so that we cannot fit another
square to the writer on the bottom but
there's always leftover green area and
then the worst cases that we have to
double the size of the original square
in both its width and its height so it
actually turns out that this is the
worst case it can't be worse than that
so in the worst case our region is twice
as big in length and height than the
optimal region so now if we go back to
the lower bound the perfect square was
the one that matches the lower bound by
using basically squares that are twice
as high in twice as wide we get it in
the worst case four times the area that
means the max reducer output is at most
four times the lower bound similarly for
the input the boxes are at most twice as
high in twice as wide as the optimal one
so we get at most twice
the lower bound for the input size so
this algorithm guarantees that we're
within a factor of two or four of the
optimal possible input or output
respectively and actually in practice
it's not that bad because if you run
your job on let's say 100 machines if 9
x 9 already fit you don't have to double
the squares you just multiply them by a
factor of let's say 1.1 and you cover
the entire green area so in practice if
you work with many reducers the bount is
actually much closer to something like
1.1 1.2 instead of two and four so we
can be guaranteed optimal near optimal
within a very small factor for all these
types of problems where we have
essentially a computer cross product so
for cross product we have essentially I
would say an optimal algorithm it's very
hard to improve on that now what about
other joint types so in this case I
might not have to cover the entire
matrix so all these optimality results
only hold if I have to cover the entire
matrix if I only have a tiny join result
I might get away with much smaller
regions actually so there's actually a
negative or positive from our point of
view result which is essentially any
type of joint where the output cost is
dominating you can't improve on this
result because what happens is because
we randomized all these roles and column
assignments when we look at our reducers
each of them almost always gets a
perfect fair share of the output work so
if you have a job that's upper dominated
did algorithm even though it covers the
entire matrix is still the best you can
do it only becomes not a good choice if
the input side of the of the problem is
the high cost so if the input is much
bigger than the output what our
algorithm is doing is duplicating the
input even more to get this nice
distribution on the output side so now
it adds to the problem on the input side
so we only have to worry about improving
over this algorithm four joints that
have a bigger input and output
intuitively so how can we now improve
over this one bucket theta algorithm
well we can do that by essentially
identifying empty regions in the matrix
and avoiding to cover them so we only
want to focus on the non empty cells
that are there so actually instead of
going through that let me just show you
a picture to illustrate the idea so
let's say how do we even know what cells
we have to cover in practice right if I
know exactly which sells I have to cover
I pretty much know the join result
already so here's an example of what we
instead can do in practice so let's say
this is our matrix and the John result
are these black cells here on the
diagonal so this could be like a typical
akwid join result we don't know that
because then we would already know
otherwise you would already know the
John result what we could know is let's
say we bucket eyes the space and we
create one dimensional histograms on
entiendes on s and all we know is what
are the boundaries of these histogram
buckets and how many tuples fall into
each bucket so if we have a neck we join
and let's say we know that the t values
in this bucket are between 0 and 10 and
the s values in this bucket are between
11 and 20 if you want to have a neck
rejoin 0 to 10 can never be equal to
anything in between 11 and 20 so we can
eliminate some of these entire buckets
based on knowing what is the the
property of all the troubles that fall
into each of these buckets so in this
case instead of getting the exact join
result instead of knowing exactly what
these cells are we can fairly easily in
a cheap way yet like a course
approximation of that that tells us
which of these regions we can completely
ignore and which of them we have to
cover using the reducers and obviously
the more fine-grained a histogram the
more cost you spend on the production of
that histogram to actually find out how
many tuples are in each of these buckets
very bucket boundaries on the other hand
you might save in terms of computation
time because you're eliminating more
of the white regions that are not part
of the John result yeah is it straight
forward to do this estimation for
arbitrary joy other than usual yeah so
they so the question was can we do that
also easily for any other joins instead
of only ekri joins fight and that's a
very good question actually if you have
a user-defined join predicate it's very
difficult actually it's actually
impossible because how can I prove that
a certain bucket does not contain any
join result but essentially I have to be
able to show that none of the tuples
from S or T that falls into that bucket
can join with the corresponding tuple
from the other side ride bike rejoins
it's easy for inequality joints it's
easy for band joins where you have s and
T within some Epsilon it's easy to do as
well if I already do something like this
join that ahead to find similar or
opposing patterns it's alright it's a
lot harder and in some cases it's
impossible you might actually run a Sat
solver or something to prove that region
might not have any result tuple but also
depending on how you order the SNT
tuples you might scatter the output
results in different ways so depending
on the matrix that you're working with
it might not be even easy to find a
large empty region at all so that's a
challenge so the algorithms that we have
here they're basically working well for
a cure joins inequality joints and other
joints with simple predicates if you
want to move on to more complicated ones
we have to do more extra work to design
a specific algorithm that takes
advantage of those right so the so the
question if I rephrase it is basically
what if i don't have just SNT but maybe
a big furry plan with multiple operators
and SNT is only known very late in the
processing stages right so that's an
issue also in practice so right now
we're focusing on one joint at a time
now if you give me let's say five joints
that have to be all executed one thing
you can do is you can look at the
five-time a well 5 plus 1 dimensional
space and it gets a lot harder to prove
any kind of optimality for that and it's
something we started working on now if
we have now multiple operator
how can we estimate in that case what is
the right like what is the best matrix
how do we actually even sort these
tuples to get basically the smallest
possible non-empty region and n matrix
so as I said right now these algorithms
are working fine for two-way joins where
basically we're having simple enough
join predicates to be able to identify
empty regions and how do we identify now
the best possible cover for the non
empty region so the basic idea of the
algorithm is as follows essentially
after identifying in these histogram
buckets which of the buckets contain
possible join output values you know
trying to cover them in a greedy way by
centrally trying to find chunks of rows
and cover them with as few regions as
possible so instead of going through
this discussion let me just show you an
example that illustrates the idea let's
say our joint matrix looks like one of
these long things here on the left side
and we start out initially only trying
to cover the first row so let's say we
have now a limit that we don't want to
use more than three input values /
reducer so now the first cell here to
cover it we need to input values right
to send this s tuple and this T tuple to
a reduce that it is responsible for that
region so we can still actually have one
more input table so we start growing the
region to try and cover more of these
black sails day so in the second step we
groped by one then we groped by one more
and then we reach the end of the matrix
so now the region is completed for that
first attempt of covering only one block
of a block of only one row and now how
many cells did we cover we covered one
black cell and we needed one region to
cover it so the average number of cells
covered by a region is one now let's try
to cover the first two rows together so
we have a long region like this covering
the first two black cells now we're
using up already all three inputs
we need to as inputs and 1 T to cover
these two cells so the region is closed
and we start a new region to the right
of it we can now expand it and now we
reach the end of the matrix again so now
we found that we can cover three of the
black cells with two regions so we have
basically an average of 1.5 cells per
region so we really take that over the
first one because we cover more cells
per region and we can keep on our doing
that with a third row or we just stop
and we just fix the two regions like on
the right side and then we start the
same algorithm on the third row and so
on and there might be other ways to do
is to do the covering in a better way
but when we looked at things like set
cover most of the algorithms actually
didn't really fit well the optimization
goal that we have order guarantees we're
not really relevant for this problem but
so there might be some other algorithms
out there that we might be able to
employ because the problem is very
similar to set cover or two
multi-dimensional scheduling problems
but we didn't find a like a good way to
map these algorithms in this case anyway
even with a simple heuristic which is
now finding a cover of all the black
cells we can now as we run the heuristic
we know at some point either we cover
all the black cells are we running out
of regions so we're running out of
regions we have to try to cover this
with bigger regions so we essentially
perform a binary search to find the
smallest possible regions that we can
use to know cover all the black cells in
the area so that we're not using more
than our region's for rsd number
produces that we have available we have
a similar algorithm instead of on the
input side we can also look perform a
binary search on the area that we cover
but that one are just going to skip over
one other side note the algorithm that
we're using here it creates always our
region's for our reducers so every
reducer gets exactly one region it's
responsible for but sometimes let's say
if you have a memory limit and you in
your regions are exceeding that memory
limit you have too many input tables to
fit in memory you can also run the same
algorithm now creating smaller regions
and just more of them so that each
region has
fewer input tools and all of them fit in
memory to do in memory only processing
on the reducers so that's very easy to
do so let's look at a few numbers so we
rent some experiments and here's some
results that are kind of representative
for what we found we have a cluster of
10 machines each of them are quad core
standard cpus with eight megabyte cash
eight gigabyte RAM to 250 gig hard disks
and we're running Hadoop 20.2 and the
system is configured in a way that we
have one head node using up an entire
CPU and all the other nine machines are
worker nodes so we have a total of
thirty six cores that can perform work
in that system and data is stored on all
the 10 machines we have so everything is
very standard configuration we have
essentially in this case I present the
results for three different data sets
one is a big net real data set from
scientific applications is essentially
some cloud measurements from ships and
land stations and their 382 million
records in the data set with 28
attributes latitude longitude time some
cloud cover measures that this stations
did so the data set is not very big but
when you run a joint on a small cluster
like this then that's already big
workload action on then we have to 5%
random samples from the data set that we
will be using for output dominated joins
and then we have photocall sin alpha
data sets these are pairs of data sets
where one is uniform and the other one
is zip distributed to examine how the
sku of the second data set effects our
joint computation so here's the first
result this is now using this synthetic
data sets one is uniform the other one
has skew of different alpha and you can
see basically in the different roles
this queue of the second data set is
increasing from 0 to 1 so these are more
more skewed data sets and the output
size is almost always the same all these
joints are output dominated we're
computing now a neck we join that has a
bigger output and input so now we're
looking at how do these
different John algorithms distribute the
output work over the different machines
and as we can see on the right side we
have two standard algorithm the standard
algorithm is the one that uses the joint
attribute value as the key and that's
the one that I mentioned is skew is
affected by dat askew and as you can see
oops be reporting here what is the ratio
between the biggest chunk of work that
any of the machines in the system
receives for the output compared to the
optimal amount of work that it should
have received and you can see for the
uniform they're both datasets a uniform
that's no problem both get almost
perfect a perfect amount of output work
but as we increase this skew at some
point the reducer that has the biggest
chunk of work does five times as much
work as it should have done if
everything would have been perfectly
distributed and not surprisingly the
runtime the latency from starting the
job until finishing the job increases
accordingly similarly for our system for
our algorithm for one bucket theta it
doesn't matter how big the skewers we
always almost always achieve very close
to perfect balancing of the output
because of that randomization that we do
and the runtime not surprisingly is
almost always around the same the
fluctuations are basically just whatever
the fluctuations are in a distributed
systems so that shows that our
simplified model also captured the
essential cost trade-offs and so by
balancing the output work we actually
also got at the same time the same level
of latency no matter what rescue of the
data is
right so so the question is are we
performing more work even though we have
less latency maybe the total work is
higher if I'd and that's actually it
might be the case so in many like if you
have an algorithm it doesn't duplicate
anything that algorithm if you sum up
all the work that's performed probably
has a lot less work to do let's say if
you have 36 reducers we might have six
by six partitioning of the matrix so
every input tuple might be duplicated
six times so we add up all the work
that's performed it might be a lot
higher but in this case since it's an
upper dominated problem even if you do
six times more work on the input side
since we perfectly balanced the output
and we don't duplicate on the output
side the total work wouldn't even be
that much more than for the simple
standard algorithm yes so we measured
also the so we looked at image the
utilization and we also measured what is
the total duplication of records in the
system and yet it work is more but it's
not much more because the output work is
much higher than the input related work
right so even though the CPUs are busier
to do the work that's not the dominating
cost in our case right I don't have the
numbers right now but yeah but so that's
actually good point in the end we need
to know where is the actual bottleneck
right so if you have really slow CPUs
right and even though the output may be
bigger than the input if the CPUs are
basically slowing down everything and
the work of the CPU is more determined
by the input than the output then we
actually have an input size don't mean
an input dominated joint problem so we
need still some cost model to determine
is it input or output dominated and it's
not only the size of the input or output
that matters but also where's the
bottleneck in the end in the system
right but for most practical purposes
the size of the input versus output is a
good measure of which cost is dominating
in the end here's another example this
is now a joint that we run on the real
data set so we perform a self join
between the data set of all these cloud
cover measurements so these 382 million
measurements and we use it twice to join
it with itself and that means we have a
total of seven
164 million input records and the output
is a little smaller that's a fairly
selective joint condition that is not
finding stations that are measuring on
the same date within the same longitude
but at some latitudes that are near each
other so for that joint and we have 390
million output records so this is an
input size dominated join so for this
one we want to actually find out Kemp
can eliminating empty regions make the
algorithm more efficient compared to
covering the entire matrix so the graph
and the upper left shows the
distribution of the load of the machine
that got the biggest input chunk
compared to the optimal chunk size that
is not like their work that it should
have received for the input so as we can
see the one bucket theta algorithm the
simple algorithm that is basically
covering everything has a perfect share
of input that every machine receives
depending on the histogram granularity
so to the right we see more and more
fine-grained histograms that we're using
initially if we have very coarse
histograms we get some slightly higher
imbalance on the input side because it's
harder to find exactly how many input
tables every reducer will receive but as
the histograms we've become more
fine-grained we have a better balance as
well now if we look at a runtime of the
join the pictures actually very
different because it's an input size
dominated join even for the one bucket
algorithm is very good at distributing
the input load evenly since it does such
a high replication of the input tuples
an input is not a dominating cost the
total runtime the latency from starting
the job until ending it is much higher
than for this m bucket algorithm the one
that uses this heuristic to only cover
the non empty cells so those ones are
much better even if they have a slightly
higher imbalance and as we can see the
more fine-grain two histograms do better
the performance of this MapReduce
implementation because we cover we need
to cover a fewer and fewer of these
cells on the diagonal so here definitely
for the input willett input dominated
joins it's much better if you are able
to eliminate big empty regions on the
matrix you want to do that and use an
algorithm that can do that so here's
another join this is known up
dominated join it's again looking at the
latitudes but now instead of also
joining on date and longitude it only
looks at the latitude being within
epsilon of each other from different
measurements and now we have to use
actually the smaller 5% samples of the
cloud data because for the big data set
the result would be so big that this
whole thing would be running basically a
couple of days probably so we have a
total of ten million input records and
22 billion output records so the output
is clear dominating the input size so
now again we compare now the version
that is optimizing file output of this M
bucket algorithm using histograms
against our simple one bucket algorithm
and as we can see again the simple one
bucket algorithm has an almost perfect
distribution of the output load as
expected because of the randomization
the m paka dogum now has some trouble
because it has to essentially estimate
inside these big black boxes where are
the output tuple is actually located if
it generates multiple regions inside
such a big black bucket so if you have a
very coarse histogram it's easy to get
it wrong and so we have a much bigger
imbalance for the algorithm that uses
only very few histogram buckets let me
see if I get a mouse pointer back so and
here actually the imbalance is directly
corresponding to the runtime of this job
because you know we have an opportunity
to join since you perfectly distribute
the output and we don't duplicate any
output tuples the one bucket algorithm
is pretty much the winner and the m
bucket algorithm can only catch up once
it users really fine grained histograms
so it needs a very accurate knowledge
about where the actual output tuples are
before it can actually find a good
partitioning good region covering footed
matrix so actually the number 5950 one
that's exactly the number of distinct
latitude values in the data set so we
essentially have a bucket / output value
so it's not really a histogram right is
actually exact domain knowledge for that
attribute value
there's our more detailed numbers in the
interest of time I'm going to skip over
them if you have questions about knowing
how much it costs to compute the
histogram versus running the actual join
I can later go back to that slide so let
me quickly summarize what I talked about
today so we're working on a tool that is
essentially a search engine to find
interesting patterns in scientific data
and we've done some work already in the
past so some work actually bizarre not
when he was a student at Cornell
developed some techniques to generate
summaries for models efficiently and now
we have operators like joints and others
that are now combining these summaries
to find interesting patterns in the data
and we have a very simple algorithm
called one bucket theta that is provably
near optimal for any join that is
outputs dominated and for input costs
dominated joints we have a bunch of
algorithms for simpler joint predicates
that are possibly doing better now
looking at a model that we have we used
it for MapReduce but actually the model
is very general and we could also use it
for instance for cases where we have a
mapper only implementation of a joint to
determine which data set should be
replicated on all the mappers or should
only be half of the data set we sent to
some of the mappers so we can apply the
model also to map only implementations
we can also imply apply to any other
kind of essentially distributed shared
nothing environment where we have a
number of working machines and we have
to decide which inputs are sent to which
output to which of these working
machines so as the next step we are
looking into how can we generalize our
ideas to multi-way joints or in general
more complicated career plans and also
to look into how can an optimizer now
choose between different possibilities
to choose either the one bucket seat
algorithm or an M bucket algorithm what
kind of histogram granularity should it
use to basically minimize latency in a
environment where we have to take into
account costs like let's say CPU versus
network versus i/o costs okay so that
concludes a presentation for today so if
you have any questions I'll be around
more minutes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>