<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Efficient and Flexible Information Retrieval Using a... | Coder Coacher - Coaching Coders</title><meta content="Efficient and Flexible Information Retrieval Using a... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Efficient and Flexible Information Retrieval Using a...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gQVbpj1_uc8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody my name is Sandra
Haven
I'm going to present Monet dbx hundred
there is also include some demo and talk
and it joint work with march in
zhukovsky who was here recently as an
intern are in the fries and peter bones
okay so let me give an outline of the
talk i will start with a short
introduction on what we're going to do
what is I are on top of a relational
database system then I'll explain the
basic architecture of mana dbx hundred
which is our experimental database
kernel that we develop and mainly the
two key characteristics which are
vectorized in cash processing and
storage engine this will mainly be about
compression and I will end with
experiments on the terabyte track on top
of 600 database colonel so at CWI we
conduct a lot of research into hardware
conscious database architecture so how
we can optimize stage database
architecture to utilize modern hardware
correctly and yeah this is with the goal
to process query intensive workloads
over large data sets efficiently and one
of the areas where this is important
this information retrieval a large-scale
information retrieval and that's what
this talk will focus on so let's again
for a repeat the highlights of mine a
dbx hundred the first one is a
vectorized query engine this is to
maximize the CPI cpu utilization and
also optimized for the CPU cache I will
quickly explain what how this works in a
couple of minutes and after this I will
explain the transparent lightweight data
compression that is built into x-hundred
to maximize I opened with and to
increase buffer manager capacity so what
are we going to do we're going to do a
keyword search on top of a relational
database system well what does it
involve given that you have an inverted
file with the database relational schema
is defined on top so with a TD table
with Dermody doc ID and score columns
we're first starting with to arrange
selects to select those parts of the
index that corresponds to our two
keywords are two search terms then this
returns ducati score columns two columns
which we merge join on doc ID to get
boolean end of the search terms then we
perform a projection to add the scores
per document at the scores for each term
and we perform a top end to select the
top and ranked documents as the most
relevant so it's not really hard but the
hard part is in how to do this fast as
fast as a customer our engine which is
generally hand coded and all the
inverted list processing is just code in
C C++ so it's often very fast and we
want to approach this using generic
relational technology well one of the
key ingredients to this is to vectorize
the database engine so our Minetti bx
hundred colonel employs a standard
relational engine based on the volcano
iterator pipeline you see an example of
a query plan on the left in the figure
so to arrange scans you so antiquarian
virtue and project but the main
difference is that the parents goal next
calls on the child to retrieve tuples
through the pipeline but traditional
database systems return a single tuple
at a time and we return a vector of
tuples or a collection of column vectors
so a column vector is just a simple
array with let's say thousand values
from a single column and a collection of
these to be able to forward multiple
columns and advantage of this is that we
significantly amortize function cool
overheads so we have a function call now
at granularities of these vectors so per
thousand tuples and furthermore
operators are now implemented as loops
over arrays and this gives a lot of
opportunities for compiler and CPU based
optimization so cpus can exploit data
parallelism eskandar compiler and
furthermore the vectors are still small
enough to fit the CPU cache so while
this plan is executed as the vectors
flow through the operator pipeline they
stay within the CPU cache so let's demo
I hope the impact of exercise let's
check it's all right open here so first
we're going to run a query with the
vector size 1024 please take a moment to
read the right side we have the director
by track collection 25 million text
documents almost half a terabyte and you
see the size of the indexes in
compressed and uncompressed format get
back to compression later we're running
on a mid end server with 10 days great
and yes single three gigahertz see on
end four gigs of RAM so first we run it
on top over with the vector such a 1024
it executes in 323 milliseconds you see
but this was still on disk resident data
to make the comparison against vector
size one fair we're going to execute it
again and in memory it only takes 40
milliseconds and please note in the
right you see the profile operator tree
you can see that on a per tuple basis we
only need a couple of CPU cycles now
let's change the vector size to one to
simulate a traditional database engine
we see that execution time goes up
considerably and explanations on the
right we were first in order of 10
cycles per tuple and it's now six to
eight hundred cycles per tuple for the
most important operators in our plan but
you cannot do it too much if you
increase the factor sights too high it
will be counter effective as you can see
here we see this curve we see the
execution time of a query as a function
of the defector size at factor size one
it's slow it goes down down down but
after a while it starts going up again
this is because if the factor size is
too big results will not fits the cash
anymore as they propagate between the
relational operators so we have to
materialize intermediate results in main
memory and this starts to become an
overhead and we can see that there is an
optimum and that's what we try to
achieve enix hundred and it's close to a
hand coded c program but the problem
that occurs is that when a dbx hundred
becomes fast it's 10 200 times faster
often than most relational query
processing engines and therefore it
becomes extremely i/o hungry and we try
to tackle this using data compression
and the goal of this is to both speed up
io even on the first rate system when to
also catch more data in memory and yeah
our algorithms need to be lightweight to
minimize overheads we don't want if we
store cached data in memory compressed
data memory we don't want it to be
slower than or much slower than if we
would store uncompressed data and also
yeah to be able to speed it up on a fast
rate system you need very high
efficiency so this is our goal speed up
the i/o bandwidth that's available by
the compression ratio of the data
however let's look at an example we have
a 300 x per second raid compression
ratio for this would already require a
decompression bandwidth of 1.2 gigabytes
per second uncompressed data generation
and that assumes hundred percent cpu
utilization but we also need to cpu for
the query processing so at forty percent
for cpu for decompression we already
need three gigabytes per second
decompression bandwidth and clearly
generic compression algorithms won't
work for this scenario as you can see in
this example decompression speeds even
the lsat 0 which is considered very fast
doesn't achieve the speeds we want
so we're going to design our algorithms
first for speed that means that they're
very simple we exploit knowledge that we
have in a database that data has a
specific layout we can exploit that data
types they come from a certain domain
the data values and we can exploit these
things and we're going to implement
algorithms in such a way that they
minimize CPU and main memory of
overheads these are key ingredients to
achieve this this we are going to
compress relations on a per coulomb
basis and we decompress smooth at small
granularity directly into the CPU cache
and feed it into the operator pipeline
and we're going to use very lightweight
you view efficient algorithms let's
start with the first so we have a
problem of values of a fix of a certain
type and these are split into fixed
sized blocks and we are going to
classify all the values into codes or
exceptions codes get compressed
exceptions are stored in uncompressed
form and so are they occupy their
original size and clearly the
compression ratio of this scheme depends
on the number percentage of exceptions
present so what does the disk block
layout look like we first have a forward
growing densely packed a section of code
words these are arbitrary bit widths
code words for fixed be and we have a
backwards growing exceptional list where
we store the uncompressed the
uncompressed values which we could not
compressed non compressible and what are
the three algorithms they are p4p for
Delta and be dicked and I'll introduce
them now they are very simple before
encodes values as a b-bit offset offset
from a certain base value so values
within the given range get encoded and
values outside this range become
exceptions p for delta is similar to p4
but then on the deltas of subsequent
values in a column so we just subtract
the subsequent values in this column and
perform p4 on them and this is very
useful for inverted files because the
inverted files often bliss often sorted
on doc ID and we can compress the gaps
efficiently using this scheme and
there's a third one the dictionary
algorithm where the code words represent
integers into a small dictionary which
is which is stored in the header of a
block so then we'll come to a rum CPU
cache compression here you see two ways
to integrate compression into a database
engine on the left you see the naive way
where you first read a compressed block
from disk then you feed it into the CPU
to decompress it uncompressed data is
written back to main memory into the
buffer manager and the buffer management
then feeds it into the operator pipeline
the disadvantage of this is that we
across the rum CPU boundary three times
as you can see and this pressure is
unnecessary in our case and it's a
unnecessary memory traffic and on the
right you see the alternative where we
read a compressed page from disk and
then immediately on demand as soon as
the operator pipeline requested request
a factor we decompress it directly at
the small granularity into the cache
there it gets fed into the operator
pipeline and after that it can be thrown
away we don't need to materialize the
uncompressed result then the third is to
use lightweight CPU efficient algorithms
we need a quick recap on superscalar
CPUs you see an illustration of how CPU
works on the bottom where we see two
parallel CPU pipelines where each
pipeline has three stages this is just a
simplistic figure in reality we have
three or four pipelines with 10 to 30
stages and the challenge is to keep
these pipelines filled all the stages
need to execute the instruction and all
the pipelines in parallel as well but
there are some dangers to this and these
are branches in program logic and
dependencies between data items so
values that still need to be computed to
be able to execute the next instruction
this can limit the throughput in your
CPU so we avoid branches and data
dependencies yeah that gives gets to the
design guidelines for decompression code
we implement the decompression as loops
over the code words we decompress these
and we want to avoid if-then-else in
them and also the data dependencies and
I'll show you soon how naive way to mark
the positions of the exceptions within
the code section is to use a specific
reserved symbol for them and be the this
doesn't work because it requires an
if-then-else in the decoding loop so
first we show a code example we unpack
from the code words which is in we get
inputs that we unpack them into code
which blows up the be a bit densely
packed code words so we can access them
directly in memory and then we iterate
over these unpacked code words and check
whether it's an exception value
exception marker or not and yet
according to whether it is we decode the
value or just write the raw value from
the exception section at the end of the
block and yeah as I told you if the
nails this prevail presents control
hazards and this limits CPU throughput
and furthermore it has a risk of branch
mispredictions which I will show you now
so where we've conducted a micro
benchmark which de compra measures
decompression bandwidth as a function of
the exception rate the percentage of
exceptions present in the data and we
measure the IPC the instructions per
cycle and the branch misprediction rate
of the if-then-else in the body of the
loop using Hardware counters here you
can see the bandwidth
the x-axis you see the percent of
exceptions present the friction of
exceptions present in the data and
y-axis the decompression bandwidth and
you see a very interesting shape the
higher the exception rate becomes then
the decompression been twit goes up
again so we don't see a linear decline
in the decompression bandwidth as we
would expect due to the linear decline
in the data size and this is caused by
the if then else in the body because at
fifty percent exception rate it's
impossible to predict as shown in this
figure we see the thick line is the
branch misprediction rate and the thin
line is the instructions per cycle so
this yeah the branch in the body in the
loop body clearly degrades IPC and
therefore throughput what's the
alternative the alternative is to patch
the scheme instead of using a marker to
have stored exception positions to mark
them we maintain a patch list through
the code word section as illustrated
here and during decoding we first decode
just from the code words without
anything fancy we don't do anything else
nothing and after decoding this we patch
up the we walk the linked list and patch
up the exception positions with the
correct values how does this look well
first we have again the unpacking which
I skip now the blown up and then we have
a decor loop which just decodes
everything from the code section
regardless whether it's an exception or
not and then in second loop patches up
the exceptions and the second loop
should be taken considerably less often
in general than the first loop because
yeah we want to keep the exception rate
actually as low as possible for
compression to be effective so what does
the bend would look like now we see that
compared to the naive approach I showed
earlier we have a much more linear
decline we don't have this weird curved
shape and we also achieve higher speeds
even while we do more work
and this is how we would expect and like
it's to look like and please note that
we get pretty high the decoding speed in
order almost 2 gigabyte per second this
is 32-bit integers compressed into 8-bit
code words and now the IPC and branch
miss rate again the branch miss rate is
the thick line we see that we hardly
have any in the patching scheme and
therefore the IPC we made the IPC curves
remain quite flat which are the thin
lines and as I said by doing more work
we added the second loop we actually
obtained a significant speed up and let
me show the effect of compression now in
the demo we're going to use it second
query to avoid main memory caching and
first without compression we get an
execution time of 384 this is IO based
it's not a memory yet and then using
compression we speed it up to 220 which
is quite a speed-up given that we have a
300 350 megabyte per second rate and
then to show that the overhead is very
small we're going to execute it in
memory again we see 46 without
compression and with compression it's 53
so yeah the over that we have an
overhead over seven milliseconds which
is still considerable and in right in
the profiling graph you can again see in
the column scans that we really only
need a few cycles to decompress a tuple
so we used our X hundred system to
participate in the track terabyte track
as I already mentioned it's almost half
a terabyte we constructed a inverted
file index on this and which we compress
from turn using P for mp4 Delta from 28
29 gigabytes and yeah we performed
100,000 keyword searches as was required
by the task and one optimization and an
optimization we did in the executional
logic is yeah well you saw the plans
they contain the merge join but this
actually can conduct boolean end of
together keywords and sometimes this is
too restrictive so we first execute the
plan with a merge-join boolean end and
if this does not return enough results
to satisfy our query the top n then we
execute the second stage where we use a
merge outer join instead of the merge
join and this will result in a
disjunctive search so boolean or of the
keywords but the disadvantage of the
merge outer join is that it blows up the
result significantly in therefore it's
slower so as long as we can do with the
merge show and we do that furthermore we
distributed our execution / aids cluster
nodes or a desktop machines dual-core
machines just petitioning the document
collection in 8 partitions using a
simple round-robin partitioning scheme
and centralized broker to coordinates
the query execution which is quite
simple to do and illustrate here there's
an incoming query to the broker the
broker broadcast its to all the nodes
the notes execute the query against
their own inverted index return their
local top ends and the broker merge is
it into a global to Penn and returns the
results we ran the terabyte track on the
following hardware platforms or actually
on more than these but these are not
showing the results i will present
and please take a while to read the
result the story the machines and the
results of these were as follows
compared to the other systems in the
terabyte track we see that throughput
wise we do reasonably compared to the
custom I our engines but it has to be
mentioned that all the other systems
they've proven their index actually and
we process the full inverted file here
if we would prune as well we could kill
become considerably faster but as you
can also see in the precision at 20 of
precision of the top 20 results you can
see that this pruning very often also
results in a loss of precision so that
concludes my talk and demo I hope I have
shown that a large scale information
retrieval on the DBMS is feasible and
that the key ingredients to make this
feasible are compression and cpu
efficiency in the form of vectorized
execution and yeah I believe we believe
that the database a relational database
environment is a provides a flexible
environment for an ir researcher to play
with models and therefore could speed up
I our research that's it thank you any
questions so this is a publicly recorded
talk so keep that in mind if asking
questions so any questions
his next one materialized as a product
the question is whether x-100 will be
materialized as a product or open source
the answer is yes we're definitely going
for that in which form we don't know yet
yeah open source would be fine with us
but if one day there would be an
opportunity to go commercial that might
also be we didn't decide on this yet but
yeah until now it's just a research
system where you only with a few people
and yeah we'll see where it where it
will end but we'll for sure we will
continue with it okay any more questions
all right thank you very much coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>