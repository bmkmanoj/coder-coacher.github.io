<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2013: Empirical Investigation of Software Product Line Quality | Coder Coacher - Coaching Coders</title><meta content="GTAC 2013: Empirical Investigation of Software Product Line Quality - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2013: Empirical Investigation of Software Product Line Quality</b></h2><h5 class="post__date">2013-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fiG-SdNcjTE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back hopefully you are all
rested and ready we are in the final
sprint we have three more talks here the
first talk that's up is going to be done
by Katerina go Shiva pupster anova and
she is from West Virginia University and
she will be speaking to us about
empirical investigation of software
product line quality and you know the
the best question I guess this time or
maybe even the first question at the end
of this talk is going to earn baby John
Oh up here and some people have told me
their kids have named this Andy and lots
of other very cute little names so with
that Katrina thank you first of all I
want to say that it's a pleasure to be
here I've been joining hold the talks
yesterday and today I am an associate
professor of computer science at West
Virginia University and I've been doing
research related to software Quality
Assurance testing software reliability
for over 10 years now the this specific
talk is on empirical investigation of a
software product line quality and I
would like to acknowledge the
contributors Tom Devine is my graduate
student at WVU
Robyn Lutz is a faculty and her PhD
student at Iowa State University and
generally from my labs this work is
funded by the National Science
Foundation before I go into details
about what software product line is and
a couple of other things I felt that
seems appropriate to give a little bit
more motivation for the talk that I'm
giving today so basically what this is
about is trying to find the patterns in
the data that can help getting better
idea of how the how good the quality of
our product is how good predictions we
can make for the future and how we can
benefit to make both software testing
more effective and efficient as well as
the post release quality of the product
being better so it's about data
there is so much data out there from
which we can learn different patterns to
have these benefits from the specific
data that we have been looking in this
and some other work is a defect or
software back and change repositories
that every buddy that develops software
keeps then into the software code
repositories version control system that
also keep logs of all changes that has
been made into code metrics and many
others so the deal is what are the
metrics that are representative to show
us what are the most fault prone parts
or change prone parts of our system and
how we can use that to make the testing
more efficient as well as geared towards
the most full prone part of the system
as well as the hand quality of our
products to be better now the specific
topic of this talk and of our project is
on software product lines so basically a
software product line is a family of
products that tries in a more systematic
way to use commonalities that are shared
among the products and have a
well-defined set of variabilities a
cruise control is a good example to
illustrate a software product line you
have basic common functionality of
cruise control software no matter what
vehicle it you are using it on but then
you have specifics for different
vehicles that are very abilities I would
think Android is also another good
example of a product line in which you
have a core functionality of a mobile
operating system but then you have for
all these different hardware platforms
and many other configuration related
issues that you can define as
variabilities so the whole goal of this
more systematically use is to mitigate
the production cost and improve the
quality and this slide here tries in a
graphical way to represent what we are
looking at every software nowadays have
goes through multiple releases actually
we have been hearing this a couple of
days even more than
well defined releases every six weeks of
or every year it's in a way a continuous
evaluation through different builds in
deployment now software product lines
has this other line of evaluation
through multiple products where you
introduce a new product in your software
product line and not all products exist
from the beginning some can come later
there is going to be a next hardware
phone by samsung for example and you
would need to have an Android that is
going to run on that phone so what is
the basic motivation is to try to see
does the systematic reviews in software
product clients prove provides a
measurable benefits is it really what we
expect to see there are studies that
exist that show the benefit of reuse you
know other context but not in a software
product line context and that's what
we're doing is basically our project is
an evidence based research so we try to
learn from data to do assessment and
predictions and since evidence base you
have to deal with case studies one of
the case studies is the medium-sized
industrial product line called poly flow
and another is a large evolving
open-source product line eclipse I don't
think that the Eclipse needs any
introduction later on we will see wow it
fits in the concept of software product
lines now the main research questions
actually I have more research questions
along the way he is first does the
social product line development really
benefits the quality in the way it's
expected and the second is do the
structural reuse and high degree of
commonality or shared code allows us to
make better more accurate prediction for
a future false based on the previously
experienced false change matrix and
source code matrix and I will get into
more details about the matrix later on
now just a little introduction about the
case study one this is stuff that we
have presented that
last year I CST software testing
verification and validation conference
in Montreal poly flow is the product
line of a software testing tools which
is developed by Avaya corporation Jenny
Lee is our collaborator that was kind to
provide us the repository of the source
code as well as the change and back
tracking system data these we looked at
for products that together consists of
42 components and half around 65,000
lines of code so it's not really very
big how it is a product line this is
actually two for testing software that
they are developing in-house but it's
supposed to work for different operating
systems for different languages so you
will have parser for Java but different
parts are for C++ the rest commonalities
and variabilities and in this specific
case study we only looked into the
pre-release faults my fault I mean a bug
that you have in the system it's either
in your code or in your data or any
condition then when three Gers leads to
a failure and in this case the post
release information wasn't available
this is still in a stage of development
the next slide here gives you a little
bit of an idea of these four products
you see the components there is quite a
lot of shared code there in the lines of
code per product and you also see the
timelines of the development of the
products like product one and to start
developing in parallel and then product
tree kicked after product one was
developed and product 5 after some more
time so it's that another dimension of
evolution of introducing new products
that share commonalities with the
existing ones but also have very
abilities that are code that is specific
to them now the first thing that we
actually learned from this study was in
a way not very surprising to us maybe
it's not for most of you
based on the whole ideas and books that
exist on software product lines it was
in a way saying you have a well-defined
set of commonalities and each product
has their own very abilities it appeared
that it's much more complex picture than
that and this Venn diagram is probably
the best way to represent it so in the
sense the the circles are the four
products P 1 P 2 P 3 3 4 and in the
center you see the products that are
shared among call then in the adjacent
areas we see number of components that
are shared between three products and
then we see some that are shared between
two products which we call low reuse
variation components and others that are
used only in one so it's pretty much
more much more complex than all and
variability so there is different levels
of reuse and some are highly reuse which
means used in all or some but not all
and others are just single used another
huh these are the metrics for this
product line the metrics are sometimes
in a way predetermined but what has been
kept for that specific product so what
we are seeing here is we collected two
types of metrics code metrics and change
metrics I would want to make a statement
here that a lot of related work earlier
on trying to predict the fault prone
parts of the software was based only on
collecting code metrics because they are
easy to collect you just run a tool and
you get the metrics well change metrics
are somewhat harder because you have to
go through the history and see what has
been changed and how has been changed so
we see some change metrics here on the
right side such as Cochin which is
number of lines edit and numbers of
lines deleted file churn is the when
files new files are introduced or files
are deleted as well as improvements and
new features they actually kept track of
good
of what change request was done for
improvement what was for a new feature
and what change requests was for fixing
faults and again this is only for
pre-release folds that you box that you
find when you test your software now
each of these results have two parts one
is the assessment knowing what's your
current state and the second is the
prediction can we predict what's going
to happen in the future so for the
assessment part our first question was
is the number of pre-release fold
correlated with any of the gut a guitar
metrics the deal here is I would like to
make the comment of the tally mate
yesterday in his keynote if you cannot
measure you cannot control it if you do
automated testing there's so many
different logs that the system's you
keep in your systems and after all you
have your version control so you can
extract what has been changed and when
you have your code and so on and so
forth but really what's the good
prediction and what's correlated with
how voltron your software is now why
this is important historically people
were trying to come up with
recommendations to run a to write a good
code that will say don't write functions
that are more than 50 lines of code and
100 cyclomatic complexity because they
tend to be more fault prone but in the
community of modeling things it's just
garbage in garbage out if your metrics
are not good your predictions may not be
good either so if we look through more
metrics we actually can see here is
there any way I can see the slides here
on this monitor some more okay I will
try it some are too far for me to read
specific actually yeah I can use that so
the most important part of this slide is
the first line where we see how these
different metrics are correlated with
the number of pre-release faults with
the box that were detected while testing
this software and the very general
conclusion is that the
in the RET pre-release fold that you
find when you test the software are
highly correlated with change matrix
more more highly correlated with change
matrix than with source code matrix so
we see that for example those components
they have higher new features metric had
more faults the correlation is 0.76 so
that's very intuitive right the more new
things you introduce more faults you
introduce likely you see the code churn
is also very high how much the code has
been changed in the lines deleted and
lines are extracted in the lines of code
and cyclomatic complexity are really
much lower correlation now why this is
important for a long time there was this
really very intuitive belief if your
code is more complex and the
psychrometric complexities number of
independent part to your path to your
code it's likely to be more fault prone
or more buggy
there is a positive correlation but it's
much lower than with the change matrix
and in a way this actually confirms in
the product line context some of the
results that were in the related work
that started extracting other metrics
than only code metrics for the sake of
assessment and prediction of fold
proneness the second question I am sure
you have experienced it is just proving
what we all tend to know that's a small
set of components contained majority of
pre-release faults and as you can see on
this graph on x axis of the graph we
have the percentage of the components
from most fold prone to least fault
prone and you see that in 20% of the
components we see 80% of the faults that
you find so it's a very skewed
distribution many things are we're fine
but you have some 20% that are really
very fault prone and that's some sort of
heavy tail distribution and this is what
actually motivates the whole work if we
can predict what these are I had
time then we will be much more efficient
into allocating our resources when it
comes to those that graph when you want
to run all these tests you you just
start exponentially requesting
exponential growth in your resources now
the next question is that's all these
things fault pronouns and change
pronouns vary with the level of reuse
remember there was another Jose
anecdotal believe that the commonalities
are not going to change much if you
design your system as a product line
because that's well divine design core
functionality that's going to stay
stable now there is a lot of bars on
this graph but if you look at the
rightmost side you see the code churn
but 10,000 lines of code the darkest one
for the commonalities highly used loader
use and single reuse and you see
actually that the normalized code
journal how much things change appears
to be highest for the commonalities than
for anything else basically new features
are added and also commonalities keep
adapting to the rest of the system and
to the introduction of the new product
now if you look at the middle graph and
look at the fold per kilo lines of code
which is the fold density or how many
bucks you see per thousand lines of code
we see it the commonalities part that's
the middle line that it's actually
fairly low so even though they keep
changing their fault density remains to
be fairly stable which in a way
indicates that there is a benefit from
having this reuse in place I will go to
this slide much faster there is some
numbers here that try to show do we
benefit from product lines and we see
actually for the later introduced
products like P 3 and P 4 a lot of
faults that are previously fixed are in
shared parts so we actually
do benefit from reuse and only see very
few Falls that are in the newly
developed parts of the system now the
production parts of this study is I
would say very small because it's much
smaller study and we only have couple of
new components to look at but it was a
good motivation for what's following and
basically we see when we learn our
models using clear simple linear
regression on the existing products p1
and p3 and try to predict the number of
faults that are going to be there in p3
and p4 we were successful the one
component of p3 didn't have any faults
and the other one has only one so this
is too small of a sample to really show
that this is a good prediction and I'm
going to discuss more on the next case
study which is a much larger one so this
is on Eclipse you know when you work on
in academia you don't own the data of
software development and open source is
almost like a promised land and eclipses
oper source and you can get the source
and the bug tracking system and
everything available so the what follows
here is actually what we did as a team
hand and this is a paper that we
submitted to journal and it's currently
under review so very some basic facts
about Eclipse it's a very large product
line its product line because it has
parts that work for different different
different members the ones that we
looked at our classic c c c++ java and
java ee diff these four products have
together 125,000 files and 20 millions
lines of code when we looked at the
evolution through 7 releases to see what
are the trends and unlike the previous
study our focus here was actually on
post release faults which is what the
users found after the product products
were released
this table here gives you an idea of the
time line so in the first couple of
releases there was only one product
classic in Europa which is three point
three Eclipse starts looking like a
product line remember that graph that I
show you in the beginning so we now have
classic C C++ Java and Java EE that all
keep evolving through releases and we
here looked specifically into the rear
yearly beak release of of Eclipse now
I'm trying to point out things that are
consistent here we also saw that beak
picture of different degrees of reuse so
there is some core of functionality
shared by all but there is also like
sixteen files that are shared between
here these are packages classic Java and
Java EE or maybe 75 that are used only
in Java EE so this just gives you the
picture of the evolution true releases
Europa Ganymede Galileo and Helios for
this for product metrics that we
collected here this is not an easy task
I must say that the students put so much
effort and so much scripting into it you
have the bug tracking system but you
have to do your own work to link that to
the changes that has been made to the
code from this in this case CVS
repository to get your change metrics we
collected change metrics six months
prior to release code metrics on the day
of the release and we predict post
release faults six months after release
all these bugs that were find based by
fine by the customers so here are the
metrics it's much more matrix here
because we have more things available on
the left side you will see the code
metrics such as lines of code number of
statements percentage of branch
statements method calls and so on on the
right side you see the change metrics
such as number of revisions that has
been made to each this is on a package
level
during that has been made bugfixes
number of times a given package has been
changed pre-release for fixing backs
alters number of people that were
participating into revisions refactoring
bugs fixing culture which is number of
lines of code edit numbers of lines of
code deleted of course I don't have time
to go through all methods but Judy's age
how old it is is it introduced now or
it's something that exists for 50 weeks
it's age in weeks new versus all so the
whole point here is to go again through
the assessment and the prediction part
and there is probably too many slides to
cover them all in detail especially
because I went with little bit longer
introduction but I will try to give you
the main ideas here because eclipses
have us through releases here we are
trying to see does the quality improve
says the product line matures from one
release to another the graph on the left
side shows the box plots for the bug
post release box that are find by
customers for each of the releases and
you see that the median decreases as
well as the variability decreases so
there is a indication of improved
quality although there is new code edit
all the time especially in Europa there
is a huge amount of new code added
because there were three new products
introduced the graph on the right side
shows you that trend which we actually
statistically tested by kruskal-wallis
test and then the post hoc test that
shows that there is a statistical
significant difference of the decreasing
trends of the fall density so there is
new code but as it keeps evolving and
maturing the quality improves because
the customers see less Falls per
thousand lines of code remember that
graph with where the folds are it's
actually shown to be true here this
shows four releases from 2.0 to Helios
that from
66 to 93% of post-release faults were
only again in 20% of the packages with
an average of 81 so it's only 20% of
your packages they have 80% of all your
post release box another thing that we
explored here is does the product line
benefits from the reuse really for three
of the releases I'm not showing the code
churn graphs because there's not enough
time to go through all details but you
see here although again for different
levels of reuse as in the previous study
one is used in only one product two is
used in two products and four is used in
all four you actually see although
through releases there is a lot of new
code edit the post release fault density
is very stable and very low so basically
anything that was pre-existing tend to
have stable fall density now for the
newly developed packages that are not
existing in the previous release the
situation is really not that clear you
see some bars are much larger than
others so it's a lot of variability some
like in the last graph used in only one
component in Helios we have 18 different
packages and only two faults but on the
other side those that were reused
between Java and Java II had a lot of
faults a general idea here seems to be
that things that are shared between two
products tend to be more fault prone
that those that are used only in one
maybe because of adjustments needed but
this is not large enough sample here -
there's not too many new packages to
really draw sound conclusions and by
that matter this is a characteristic of
a long tail heavy tail distribution you
have many good things and then once or
two that are very really very bad same
like with insurance claims you have many
small insurance claims but one or two
that are really
very bad and what we care about here is
to be able to predict those that are
really bad because that's how we can
efficiently put our resources and more
importantly for this study what we are
trying to do as I said from the matrix
we are trying to use pre-release data
actually we are building a model on the
previous release which is a machine
learning model we specifically used
generalized linear regression models to
predict number of post reliefs faults on
the next release from the pre-release
data so it's pretty much before you
release your software can you tell what
are going to be the most fold from parts
of it and if you can then how you do the
testing more efficiently to prevent that
we build 19 different models for each
product that we had through releases and
then we use to predict for faults on all
releases there now this is an
interesting graph that gives you an idea
of what we are doing here it's called
albergue diagram on the x-axis you see
you see the percentage of packages in
which the faults were find ordered by
the most fault prone to Lee's fault
prone so the ones that the left are the
ones where most of the box post release
box are the full line is the actual box
so and the dotted line are the
predictions that we are making based on
our models trained on the previous
release so we we build the model on 2.0
try to predict post release box from
pre-release data for 2.1 and the closer
these two lines are the better our
predictions are the vertical line of 20%
is actually that magical weighed eighty
twenty percent because it appears that
20% of your code has 80 percent of your
box then the question is can we predict
the numbers of folds and packages that
are going to appear into 20 percent of
the most fault prone packages that is a
that can be anywhere in this graph but
20% is the one we chose based on the
data of 8020 rule this here now shows
the predictions so I will probably need
a little bit time to explain what this
is about so this is so-called heat map
in which you see the darker cells are
the better results and then on this
graph at the bottom you see the releases
where we make the predictions two points
1 3 0 Europa and so on on the left side
C C++ classic Java and Java EE are the
products on which we make predictions
and on the top you see the products on
which we build the models now for the
time being we don't look into any sort
of patterns what we look at is how good
the predictions are no matter on what
they are trained and where we do the
predictions so it appeared that these
models were able to predict from 76 % to
97% of where the faults are in those 20%
of most false packages which is fairly
good results there are two exceptions
and here I would like to make a note
that we wanted to make this as practical
as possible in the sense that we didn't
take any outliers out basically because
the outliers that have most of the box
are actually the most interesting that
you want to find our predictions we have
been more accurate but not really very
realistic so the light areas are where
the predictions weren't good and we went
to look at why that was the case in the
raw data on the C C++ anything that we
trained on C C++ appeared not to make a
good predictions because there was one
very high faulty package in the previous
release that as an outlier make the
generalized regression models not to
work very well the other light area is
on the rightmost side of prediction made
on
that is only because of one package that
is shared between Java Java which was
again an outlet which having a large
number of faults and the models were
predicting the number fairly accurately
but warrant placing this among the
twenty percent of topmost ranked
packages so they were missing this
package and that's how the predictions
weren't so good now here I will
generalize little bit the question and
say can we benefit from the information
about additional products when we make
predictions and as you can see on this
graph if if there is no benefit of the
additional information for predicting
your fault prone packages then what's on
the diagonal is supposed to be the best
value and that's almost never the case
if we want to look at some sort of
patterns here it appears that whenever
we try to make predictions on a smaller
products such as C C++ and classic which
mainly consists of common components
then those predictions are better now
with respect to where you build and
train your models if you build the
models on a larger product such as Java
and Java ie then the predictions were
also better I will just quickly talk
about something that's really very
interesting we did feature selection to
find out what are the most better
predictive the best measures that are
having good predictive power of where
your box are we use the stepwise
regression for feature selection machine
learning method and if you see for our
models anywhere between one horse up to
sixteen features were selected out of
112 features basically this says that
only very few features matter and have
enough information to predict where
what's going to be fault prone this
histogram here shows the frequency of
this
features that has been selected by
different models and look at the
features at the top that are used in
more highest number of models they are
all change metrics total box in maximum
total bug fixes and maximum bug fixes
total otters and then total code churn
total revisions out of the first 15
metrics only for our code metrics so
this simply says if you want to predict
what's most fault prone part you better
use change metrics than simple code
metrics code complexity didn't appear to
be very good at all head it was chosen
only by one model so if we want to we
also looked at the correlation between
each metric with our response variable
which is post release box but if you
want to tell this whole machine learning
part as a simple story it really sounds
very intuitive the most but the better
the best predictors of where what's most
fault prone in number of post release
box are packages that has a lot of bug
fixes pre-release that has been handled
by many authors that has a lot of cold
churn
edit and deleted lines of code as well
as many revisions of the static code
metrics only for such as the maximum
statements and level 1 and 4 and maximum
method call statements were among the
first 15 lines of code complexity don't
appear to be a good predictors of where
your bugs are going to be and lessons
learned very fast so I can allow some
time for questions these are very high
level lessons learned that are
consistent across both studies there is
a wide spectrum of reuse levels
commonalities but things that are used
in some but not all as well as
variability is used in only
one product then we found that both
pre-release and post release faults have
very skewed distributions which means
very a large number of good modules and
components but very few 20% that have
most of your box 80 percent less than 3
although pre-existing all packages
including those shared among all
products continually changed they retain
to low fall densities which shows the
benefit of reuse both pre-release impose
release Falls are more highly correlated
with change metrics than with static
code metrics so if you want predictions
those are the metrics that you should
look at predictions of both pre released
and poetry's fault can be done
accurately from pre-release data this is
the most important question with can i
from the data that I collect during
testing in-house testing find out how
good my product is going to be and
predict where the problems are after the
release and the predictions benefited
from information on additional products
remember training on a larger product
gives better results and revisiting
those two big questions of the
systematic reviews in a software product
line context with respect to the quality
products benefited from faults that were
fixed in the used components and
packages and with respect to the fault
prone as prediction predictions also
benefited from the additional product
line information so if you have data you
better use it your predictions are going
to be better I hope I was able to
convene the main idea of you do
automated testing but you also can
collect a lot of metrics and do
automated predictions of where the
problems are now or gonna be after you
release the product so I would say
that's one sentence that summarizes the
general idea of my talk and I will be
glad to answer any questions
that was a very fascinating talk Thank
You Katerina I learned very two very
important lessons number one Venn
diagrams are beautiful and garbage in
garbage out so thank you thank you
actually at all seriousness one of the
ideas I've been exploring for a while is
this whole idea of the behavior around
the code is more important than the code
itself in terms of predicting failures
and you have scientifically nailed it so
congratulations that is that is quite
awesome so thank you so with that I
think we're gonna go to questions I
think you were up first so please this
way who you are where you're from and
you're certain packages I used more than
others like Java is used more than C C++
I hope did you take that at account no
no but it's a very good point in that
sense that the information about usage
of the data is not available when you
deal with an open source and I don't
know whether it's very hard to get the
information so so that is possible but
pay attention that we don't deal with
failures here of how often things fail
we rather deal with faults which means
underlying reasons for a failure now you
are right in the sense that if you use
something more often it's more likely to
hit the place where the bug is but
that's one of the metrics that's almost
impossible to collect especially if you
are a third party and for many products
would you know what the usage of chrome
is across the board so no it will be
good to know but it's high a metric very
hard to get that's a great question and
since you work at Amazon please put this
on your desk so the question over here
please space for your case study 1 did
you include any of the defects found in
developer unit testing before it
actually went into
traditional test for the case study one
yes actually that case study because we
had access and our co-author was one of
the developers dr. Jenny Lee from Malaya
we actually even had bugs that were in
the early design stage there but we only
kept through and that's explained in the
paper in details we only kept
cold-related box because that's what we
try to predict here from the code and
yes some were unit testing some were
system testing for Eclipse we only have
bugs that are into bug tracking system
which for the pre-release which unlikely
is that has unit tests thank you okay
sure next question please
hi I'm Dylan salisbury from google um
this is a really exciting topic and
brought to mind the time at a previous
company when i remember real clearly a
program that had like the bad module
that was the source of all the bugs and
i had a lot of the characteristics that
you described and you suggested that you
for another product may be able to use
us to decide what when to focus your
testing on before release what I
remember from my experience is that this
module was very untestable you know
because of the things you talked about
it had poor interfaces
it was very core to the you know was
there's no way to write small tests for
it because it had been you know baked
throughout the product and the way it
was finally improved was that a single
engineer went in and refactored it the
whole thing to make it testable so I
just want to suggest if maybe a takeaway
that something that we kind of
intuitively feel but it was might
provide evidence for is that even before
release it may be worth targeting one of
these modules to be not just do more
testing it but even refactoring or
rewriting to make to allow more testing
this is a really very good point because
this methods that I discussed in
addition to being good predictors of
where your faulty most fault prone parts
of the system are that gives you other
ideas as well one of the static code
metrics was a number of method calls
that was highly correlated with the post
release box so if you see a model that
has plenty of those then it's a model
may need more refactoring another thing
is to look at things they have high
culture and why is the reason you know
the serious keeps a very good track of
what has been committed and when so why
some change promise can be and we have
done some work which we there's no time
to present here is another topic why
thinks them to change it's also
important because we see that the change
also introduces a false some of the
point that I'm trying to make what
you're saying is right some metrics on
your own can give you an idea of how
your code is then I go back to what Ali
said if you cannot measure it you cannot
control it and when you guys automate
everything then keep as much data as you
can that then can be digested with some
sort of machine learning and other
methods to really find out more about
your code and how to improve it yeah
thank you for your question we have time
for one more and it's about a thirty
second question plus the answer and I
think you were up here first so please
say nothing from Google did you find any
correlation between contributors
individual contributors and and faults
you want to put the blame blame for the
faults to people we only looked at the
metric that was extracting the number of
authors that has been evolving to
refactoring and all kinds of things I
know of some other work by companies
that do own the data and own information
of communication between developers or
testers that try to figure out other
metrics such as how often people tend to
communicate about specific buck or
module or component how that reflect and
certainly if you own the data you can
attribute things to people and if that's
what the goal is if you want to post
somebody on that blackboard yeah
attribution is definitely perilous I
actually did that one time at another
company and I was a very popular guy but
not the kind of popular you want so so
that thank you got rid of those</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>