<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Web Testing at Google: Infrastructure for the World's Most Demanding Web Developers | Coder Coacher - Coaching Coders</title><meta content="Web Testing at Google: Infrastructure for the World's Most Demanding Web Developers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Web Testing at Google: Infrastructure for the World's Most Demanding Web Developers</b></h2><h5 class="post__date">2011-01-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M47-FGvVJx0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I would like to welcome our first
presenter it's Ted Mao Ted is a software
engineer at Google and he's been working
on various infrastructure level code
that we developed at Google and some
cool stuff that he cannot share with you
so he says but I'm sure if you take him
to the side and ask him some probing
questions you'll get some more data so
let's welcome Ted to India and Hyderabad
to the top Thank You Jay my name is Ted
mal I'm here visiting from Mountain View
California it's a little bit jet-lagged
but doing all right today I'm going to
be talking about web testing at Google
and particular some the infrastructure
we've built to support web testing so
let me start with a definition what is
web testing we use the term web testing
within Google but it's not something not
a term that you find widely used in the
outside world we define a web test to be
any test and requires like a real blood
browser to run the test you know not a
fake where was like HTML unit or
something like that a real web browser
and the type of test we've run using
that we considered web tests include
stuff like unit tests like closure test
or j/s unit or you know those kind of
tests and also browser automation style
tests or end-to-end tests that use tools
like selenium webdriver puppet
those are things and these tests
typically you know tests not things like
web application functionality stop you
know when you click on a button in your
web application does it respond the way
you expect it to they also are used to
test layout and rendering within web
applications you know so you know does
the application will look the way you
expect it to in every browser and
finally these web tests are sometimes
used to test performance let's say you
go to a series of 10 different web pages
in a row or you two--you a series of
actions within your web application has
the poor performance regressed since the
last time you ran the test let me just
also choose myself a little more and the
team that I'm working with so we have a
dedicated web testing infrastructure
team at Google and
we work on all sorts of testing
infrastructure of specifically for web
testing so this includes you know things
like the test ap is when you write your
test in JavaScript or Java or Python
using a penis like webdriver selenium
closure GS unit etcetera
we hope the lows api's we also work on
the browser and for automation tools
like selenium and webdriver as well as
the test winners and all the lower-level
infrastructure that integrates with our
Google infrastructure to run these tests
at scale the focus of this particular
talk will be the bottom three layers
that is the the runners work ok I'll
just keep talking to the mic instead of
looking at my slides actually I have a
thing of a screen up there I can look at
as well the focus of this talk will be
the bottom three layers that is the test
runners the browser providers and how we
set up the test environment actually
just to spin on that there's gonna be
another talk tomorrow afternoon by Simon
Stewart which will be talking about what
about the test API in browser automation
and inbound our they'll also get talked
by Nathan York that will focus on more
some of the more general infrastructure
we have for spewing builds at scale test
at scale and so on so what are some that
these are not my slides out for reload
ok maybe I think these are out-of-date
ok yeah that's ok they're going to be
out of order but this will work so let
me talk a little about the history of
how we've been running the writing web
test at Google so in the beginning we've
but then it you know sort of the most
obvious way if you're a developer no
matter how big your team is you can run
these web press on your local machine
you run a test is access to whatever
browsers you have installed on your
local machine for Google employees
that's typically a Linux workstation
that has Firefox and Chrome installed on
them and that's fine you know but there
are problems with this approach the
browsers that you have are limited
though to those you can access on your
local machine if you're lucky enough to
have you know up more than one machine
then you can
you have access to a larger range of
browsers and also the pro ilysm is okay
it's that better as we're talking to the
mic instead of looking at people your
parallelism is also limited by the fact
that you're only running on one machine
there are browsers like you know pretty
much all the inner explore browsers and
Safari that keep global states so if you
try to run more than one test at a time
on these browsers they're gonna stomp
all over each other so you're pretty
much limited to running one test at a
time in serial there are some browsers
like Firefox and Chrome that support the
notion of profiles what that basically
means is that they try to keep different
profiles they can run mobile profiles in
parallel and Firefox Firefox and Chrome
will try to keep these parties profiles
in depend on each other they don't
always succeed so sometimes even then
you're gonna have tests that stomp on
each other for example if you have two
profiles that download content to the
same place on your filesystem then you
know they're they're going to see each
other's content and finally you know you
have a problem where if you're just
running testing a local machine then
they're not necessarily going to be
reproducible on another developers or a
station you know this is because of sub
problems like you know that the weather
they're not the test passes are not
depends not only on the test itself in
the version and operating system the
browsers are running on but might depend
on such things as the minor version are
you running you know firefox 3.6 or if
36.1 you know 1 2 etc they also depend
on configuration of your browser which
might be different from the browser's
others are people are using for example
you know what security level D every
browser set to how how are you setting
your cookies are you saving all cookies
by default you know are you clearing all
cookies after each session that sort of
thing but at least you know if when
you're running a test on your local
machine it's pretty easy to tell what's
going on you can see the browser window
as your test is running and you know you
can plis ticking breakpoints into the
job
script or the test itself and you can
use debugging tools like Firebug or the
chrome inspector to really get an idea
into this the nitty-gritty details of
what the browser state is so how do we
try to solve these problems like many of
you we we try to decouple the test from
the browsers that the test is access to
so the first approach we each attempt
tried was to build a service but we
called we're calling it here the farm
which is backed by a bunch of machines
each one that supports a certain set of
configurations of browsers for example
there might be a set of machines that
support ie8 another set of machines that
support Firefox 4 and so on so the test
in this case the test is still running
on your local machine but when it wants
a browser rather than trying to see
what's available on your local machine
it makes a call to what we're calling
here the farm front end and says please
give me a handle - let's say I eh and
Firefox 4 and then it uses tools like
selenium RC or remote web driver to
control remotely control those web
browsers you know this has some
advantages you know you're no longer
limited to the browser is available on
your local machine it also helps you by
freeing up their local machine resources
so that all your local machine has to do
is run the test itself rather than a
bunch of browsers it also helps a little
bit with determinism it makes your tests
more deterministic because everyone is
drawing from the same pool browsers if
you do this right all those browsers are
configured in the same way it still has
problems ok and I'll get another mic ok
ok
it still has problems for example you
know as I said before each there are a
bunch of statistical machines that each
one's configured for different set of
browsers there's no good way to
dynamically load balance between the
browser configurations
now you need to have machines with it
for every browser configuration if
someone wants you know most of the test
market will be running hopefully with
the newest browsers but then you're
going to have you're going to have tests
that will want to test let's say Google
web search against firefox 1.5 just
because you know we want Google web
search to work with every browser and
there is some minuscule set of users
that are actually still using firefox
1.5 or IE 4 or netscape 3 or you know
whatever the problem is that we have
it's really hard to predict ahead of
time how many of each browser
configuration you get to keep on hand we
also solve the problem that their test
is still running on your local machine
and it's going to be a bottleneck
actually for these large integration
style tests where you might have
multiple servers running databases etc
okay another problem is that even though
all the you know if you have driven
developers running tests they're sharing
the same set of browsers these browsers
accumulate state over over time we've
written we wrote a bunch of cleanup
scripts that we attempt to run after
each brow after each test run to say you
know clean up the cache clean up the
history you know set all the
configuration of the browser to be what
we expect it to be but it's not doesn't
quite work that well primarily because
we can't you know ask whether or not two
machines have equal state and actually
if we did it would would it fail because
they obviously would not have equal
stage it's really hard to configure to
compare the state of two browsers and
finally we have a property of a we've
made the problem debug ability worse
because these browsers are no longer
running on your local machine they're
now running in a cloud somewhere else
and you can't really it's harder to
debug them you know we met to open a VNC
session or something like that to the
browser but it's just not as convenient
as being able to have the browser on
your local machine and um you know we we
tried to fix this for a while just by
you know collecting data to to figure
out what set of browsers we you know how
many
each browser to make available on this
farm we also try to improve the
robustness of this farm by increasing
the sophistication of our cleanup and
monitoring strips you know after all we
would try to clean up she new cleaning
up the browsers to get for a long for a
while and then if you know it failed for
some reason you know maybe we put the
browser in an inconsistent state because
we're trying to clean up the history
while it was reading the history
something like that then we could just
restart the Machine and that worked
better but it still wasn't it wasn't
quite good you know the web tests were
still flaky this is what it looked like
after adding all those monitoring
processes you know like the previous
diagram the left side is the same but
what's changed is the right side in the
cloud we've added more you know I was a
little monitoring box in each browser to
show that you know we're adding
complexity more stay in there more
monitoring you know so like I said the
same promise will apply even after
adding these these improvements for
example the test is still running
bottled in the bottleneck because
running on your local machine and these
chest wants a clean browser environment
doesn't really know whether it's getting
one and sometimes it's not you know
previous tests may have affected the
browser in such a way that subsequent
tests will get affected and fail for
some reason that's not reproducible so
we had try another approach and it
worked a little bit better so to solve
the the problem of the test itself being
a bottleneck we introduced this concept
called test starting and I've got to
gloss over this a little bit because we
don't have that much time but
essentially the way it works is that if
you have a test suite with let's say 100
tests and you shard your your test suite
five ways okay then what you actually
have is five different machines running
one how many how many different ways I
say so if you start up five ways you
have five different machines running one
fifth of the test each okay and then if
you add on to the other dimension of
browsers let's say you want to run em
browsers so each of those five shards
would then be running so rather than
having five stars you would actually
have
five times M shards and each one those
charges to be running with one browser
me that wasn't a very good example so
you know in the slides as you know he
started end ways on M browsers you get n
times M chars total okay if we have time
in near the end I'll go over this some
more but essentially it's a way to
increase parallelism by sharding by
moving not just the browser's through
cloud but also that the test as well
okay and doing this actually opened up
some more options for us you know if we
started well each test wants its own
clean browser right we want each test
will want a browser that has no state
created by test before it so why don't
we have each test go ahead and create
its own browser you know this might be
done by you know start starting with a
VM image from a save state this might be
done by starting what we call a hermetic
binary which you know pretty much a
statically linked binary in a well-known
stage but anyway you know give each
shard its own browser and this actually
worked really well for us so then what
we have after doing this is that the
test Runner the test is the plumber'
running on your local machine but it
still has to kick off all the tests so
what it does is says okay I want to run
these tests
I want to Charlotte anyways and run it
on each one each test on M browsers we
push all the input we push the test as
well of all this dependencies into the
cloud run the test and that's how it
worked
so how did this help if this really
helped us scale because even the houses
are its own browser the fact that we can
paralyze these test runs made them run a
lot faster for example rather than
having one test your local machine run
let's say a hundred tests in serial now
we have let's say if we start at twenty
ways we have twenty different machines
each running five tests and that
actually made think the wall type a lot
faster he also feed up
local machine resources so that rather
than you know usually that you could do
other work while you're waiting for your
test to run or say you know some
minesweeper or something like that and
finally in really improve the
determinism because every start is
guaranteed to get a completely clean
browser untouched by anyone else now if
you know you your test started creating
state and effective tests after them in
the same shard but at least someone
else's test won't affect your tat yours
um however this you know we still have
the problem of debug ability once you
not only are removing the browser's to
the cloud now we're also moving the test
to the cloud and therefore all the
outputs in the test you know log outputs
any display etc are no longer on your
local machine well we we solved the the
bugging problem in two different ways
one was we realized that there are two
different ways people actually typically
run their tests one is through of a
batch mode where they try to run all
their tests as fast as possible and see
which ones break okay
then you you know that in that mode they
don't really care to see display of the
test but they do want the output of the
test after it's done if it breaks the
second mode that people run in which is
that if they're developing a new test or
they want to debug the test I just broke
they just want to run one test okay in
that case we don't need all bility we
don't need to run tests in the cloud as
long as we the test in your local
machine still has the ability to create
a browser in the same way as the test
running in the cloud then it should be
deterministic and it reproducible in the
same way for the other problem of you
know what happens to all these outputs
when it has to run in the cloud we built
some instructor around that to basically
aggregate outputs this is an example I
guess it's hard to see here but what's
going on is that each row is one shard
of one test and the ones that failed are
in red one saira green had passed and
the long list on the bottom
of the this screenshot shows that you
know the last test of sweet was actually
started see how much 15 15 ways and even
though each jar took between one in two
minutes you know so for a total of about
17 minutes for the entire test suite the
wall time was actually two and a half
minutes which is basically the max of
all the walls of all the wall times of
the shards and I'm not showing it in
this slide but essentially you can you
can drill down to a particular shard to
look at the test run on that shard the
log output from that shard no and so on
so that's our current system now we have
a test runner that you know usually
running on your local machine we also
have you know continuous integration and
so on so the tests aren't necessarily I
mean the tests aren't initially kicked
off by our local machine the tests
themselves are running the cloud but in
the in the sharded system and the
outputs all aggregated the end of the
test run so that you can view it in a
nice you know web UI so you know how
well does this actually work today so
what we accomplished is you know we've
been slaving webdriver J's unit closure
puppet GS has Shiro tests and more
actually but those are the ones that
people know about because are the open
source frameworks that we write tests
with we run a little bit over a hundred
over 1 million tests per day and we have
over 1,000 can support over 1,000
concurrent router sessions so what this
means that you know if you have a
hundred tests that you want to run in
parallel and you want each one to run in
in ten browsers you know we can actually
run all those tests at the same time we
also have you know continuous
integration so that when you check in a
change that affects when these tests all
the tests that are affected rmally get
kicked off we've done a pretty good job
of creating a deterministic test
environment and one of the side effects
of having a deterministic test
environment and testing the cloud is
that you can actually do
pretty efficient test caching if you in
another user happen to run the same stat
tests you know with against the same
browser our infrastructure will actually
only run those ones you'll run your
tests if someone else tries to run the
exact same test against the same
browsers it'll just say you know those
paths were succeeded we know that
because the previous person ran those
same tests and you know they passed or
succeeded
okay and finally you know we've been
pretty good job of out getting all of
our log output and other test output to
a central system so you know we still
have some challenges we saw some future
plans actually this web testing
infrastructure team that I'm part of has
only been active for about a year now
part of that we kind of built
infrastructure in small silos you know
each product had their built their own
infrastructure and although that worked
it wasn't very it wasn't a good use of
our time what we want to now is we want
to do scale you know of course more that
means more browsers right now we support
most of the windows browsers and Linux
browsers and some Mac browsers we want
to add on you know more we want to add
on mobile browsers
Android iPhone Blackberry we want to you
know add on new versions every browser
this quarter or probably adding Firefox
for IE 9 in and so on and we want to do
scale or capacity as well probably you
know we're on track for about 2x
capacity every quarter for about next at
least for the next quarter or two and of
course we want these has to run faster
right now we have a problem where you
know doing stuff like starting a virtual
machine for every test yard it's kind of
expensive you know it's if you start
from a that would call a VM snapshot you
can take maybe three to five seconds to
startup the VM and then if you want to
run a particular version of Chrome we
have to upload that version of chrome
into the VM and so on so all that test
setup time can take 20 or 30 seconds if
your tests are like big selenium tests
that take a minute or two it's probably
not a problem if your tests or unit
tests that run very quickly no we
really don't want to have that kind of
overhead and that's our point I want to
build on is that we really need to
improve the conceptual model that our
users have of our infrastructure you
know in addition building this
infrastructure at Google who you know
are really brilliant they don't always
know what's going on so they'll
attribute blame for a test failure or
two all sorts of strange things we
really need to help them understand you
know where the failures and the tests
are happening are they happening in
their tests are they happening in the
infrastructure if so in the structure
which part of infrastructure and what
they should do about it you know
developers or users also and they can
get confused just as other users do we
also really want to measure everything
we do so you know what does this mean we
want to know how many of each type of
tap running how long an average test
takes who's running these tests all that
sort of stuff
because if we don't measure it we can't
quantifiably improve it I think that's
about all I want to say today we have
two minutes any questions is there a mic
yeah I see a thing lures over there too
I'm not sure if Bangalore has a mic you
if anyone wants to ask the question you
can just raise your hand and I think the
mic will come to you
there you go oh right this is you know
my question is given that you have a
very complex infrastructure
what's the reliability of that
infrastructure for running these test
cases like 99 point something are much
lower it depends on how you're measuring
the reliability of course but it's such
we want the reliability to be such that
what in the when the users test fails
they don't blame the infrastructure
first if the pressure failing is kind of
a and we want to be an edge case where
you know you know first that users will
look at their tests first and see
whether it fails and only then might
look into the interest rate the only
reason why we can actually build such
stable infrastructure is because we
depend on a lot of other Google
infrastructure Google's production
infrastructure that we you know we
already used to serve all of our hosts
all of our web applications the same
structure is the one we use to compile
all of these tests that we run compile
or Rob all of our binaries and so on so
you know we haven't when the reasons we
have an open source is this kind of
structure or or exposed it at all is
because we don't think it'd be very
useful to the outside world it's very
heavily integrated into Google's
internal infrastructure or our
production services
hi this is nursing Natalia how do you do
we do a stress testing if the number of
users is more than 10,000 with different
browsers and different environments
settings and with different operating
systems so how do you do the stress
testing I'm sorry actually I did I
didn't understand that is there again
with different set of different
operating system and different machine
and environments how do how do you do
the stress testing the browser how you
do stress testing yeah so what are you
stress testing the web the application
yeah Google um um okay search engine
okay so what we found is that using web
browser to do stress testing actually
isn't that useful because web browser
the real web browsers are useful for is
testing functional interactions between
the browser and the application unless
the browser has a funny bug in which you
know it puts more stress on the
application that we should expect it
should be sufficient to just do a stress
test where you make you know HTTP calls
to the web application as if it were as
if it were a web browser because we
don't really care about the actual
browsers actual functionality right
we're just trying to stress the
application itself and that case it
shouldn't matter what web browser
browser you're using
hi this is Ruth Raj here I have a
question about are you storing the
results of the test execution in some
sort of database and doing some analysis
on what or what were the major reasons
because of his tests were failing and
then trying to see if that test failures
can be reduced like in my case it's like
I see in that environmental failures it
tends to be more as compared to the
application issues or or or other issues
yeah yeah so as you saw on the slides we
are we are storing all of the test
results and we do a lot of analysis on
the original tax for example we can
predict you know what the flakiness of a
particular test is compared to other
similar tests we can do based on the
stack traces of you know if there's no
exceptions or test failures you know
where are these failures happening are
they happening in the infrastructure are
they happening in the test itself you
know after a major release do they get
more common do they get less common and
so on we're still building allow a lot
of structure for the UI is for users
this data we're collecting to expose
this data and make it useful but we you
know we have all the data stored to be
able to do that I don't know where the
mic was hi this is bharatiya from
National Instruments I want to know one
thing about profiling do you do any code
code coverage profiling when you do
these tests or if you do what are the
ways you incur these tests and the code
so alright I say Coker which I mean
javis I mean like application JavaScript
code coverage you were talking about you
know there's there's different code
right there is the test code itself the
the code that's you know doing it making
actions in the bra that's telling
browser what to do and making assertions
about the state of the code under test
and so on so this is that kind of code
coverage and you know if you're running
if you wrote like a java web driver slam
test you'd use the typical Java tools
for doing those technical coverage if
you're talking about the JavaScript code
running in the browser
itself we also have the bill to do that
in the way it's usually done is by
instrumenting the code that's running
inside the browser and making it part of
the test results that we then you know
export to the output aggregator with
everything else usually what kind of
code coverage you do MN is it functional
coverage or on any decision coverage
which is a high priority coverage you do
that's not my area of expertise actually
I build influential allows people to do
code coverage and I don't really know
what they do with it thank you okay a
question how do you distribute yep how
do you distribute the load across the
process and do you ensure that each
browser gets like hundred percent
coverage of the application and
alternatively we could not have done by
just replicating the browser on the
flaut like we could have created
multiple instances of the same browsers
and loaded it would that not be a
simpler rather than creating a shader so
what is the advantage of this over that
that is I so what was the first question
again the first question was that do you
insure hundred percent coverage of the
for each browser or it it does not
matter to you and so we say 100 percent
coverage do you mean that we support we
support every web browser No
oh I see we're saying okay even
functionality is Penn State I use a new
browser they see now we don't really so
the application teams themselves or as
possible choosing which web browsers
they want to test against we merely
provide the infrastructure that says if
you want to test against a certain
browser you can now we have teams that
build internal applications and they
know that all they really info test
against is Firefox and Chrome because
that's what we use internally there are
there are excellent really pleasing
product teams that will say well we only
support the latest you know some of the
latest browsers and those are
we're going to test against and there is
another teams like Google web search
which says we wanted to we want to it
Haskins every possible browser we can
oops that's not my that's not my laptop
can't log in there so finished up there
I know there's a second question for all
the inside so</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>