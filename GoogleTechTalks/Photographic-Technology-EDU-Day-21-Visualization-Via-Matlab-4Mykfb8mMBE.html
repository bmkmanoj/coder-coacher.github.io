<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Photographic Technology EDU Day 21: Visualization Via Matlab | Coder Coacher - Coaching Coders</title><meta content="Photographic Technology EDU Day 21: Visualization Via Matlab - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Photographic Technology EDU Day 21: Visualization Via Matlab</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4Mykfb8mMBE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming there's a lecture 21
out of a series of 12 something like
that we'll have to revise that
announcement yeah dyslexia so today I've
got three topics that involve sort of
matlab programming they're kind of
leftover additions to things we covered
before so we'll we'll get into some
stuff on icc color profiles people want
to know more about the internals of
profiles and how color management works
inside we'll take a look at ray tracing
in a in a particular simple example
that's relevant to some of the digital
cameras we have around here and and a an
example of some diffraction calculations
that are also relevant to some of the
things we do so let's start with
profiles this is this will be at least
half the lecture we've we heard from
both Steve Johnson and you wish Stan
Muller about color management and
various contexts and cameras and
printing and so on and so since it's a
technology course people want to know
what's inside these things and how they
work so if you pull down the the spec
which is it's a freely available
standard unlike most standards that you
have to pay money for this one was
jointly developed between the
international color consortium and some
other standards organizations and
they're not the business of charging
money for their standards so you can
actually get this one and read it you
can read every version of it since about
nineteen ninety four and it's a it's a
simple file format it's got a header
that's got a bunch of miscellaneous junk
in it and then it's got a sort of 11
level of simple self-describing tag
structure so there's a table of tag
signatures that tell you where to get
the tag data so that's that's nice that
makes it easy to parse the file and once
you've once you've pulled out a block of
bytes called sig for the signature and
data for the data you can write a simple
switch casement in matlab go through
there and do something different for
each different signature type so this is
just a piece of the code i'm sure
here where we simply convert the data to
character strings and print it out if
it's something that looks sort of like a
character string and then we have a
bunch of more specialized routines for
parsing the data block if it's something
interesting that we want to look at so I
had to kind of go through the specs and
see how its structured and write little
bits of code so you can you can whip out
tables and XYZ data and strings and
stuff this way so I have this little
routine called read icc profile and if
you give it the more audience members
good if you give it the srgb profile
which is a standard profile for the srgb
color space as provided by
hewlett-packard it'll it'll run through
there and it'll print out the header
first thing you notice in the header
well there's a bunch of junk in there
hold on i underlined a little piece here
MNT are that's uh that's a that's a 4
byte block of numbers that says this is
a monitor type profile and they pick
their numbers to be printable as ASCII
as well so you can read it and then it's
got other other blocks of numbers in
there and i lost my cursor again hold on
put this back in Aero mode keeps wanting
to disappear on me RGB here means it's
it's a profile for an RGB color space
that is the input data the device data
is in RGB space the XYZ means that the
profile connection space is XYZ and I'll
show you what that means in a bit this
is a little code that means it's an icc
profile this is the primary device
manufacturer and this is i forget but if
you if you go down through these tags
there's so there's all kinds of stuff in
there like the copyright tag it turns
out there's a there's a list of required
tags in a profile and copyright is one
of them which is kind of annoying
because it means you can never give
somebody a picture with an embedded
profile without giving them a copy of
some copyrighted piece of material so
why they required profiles to be
copyrighted as sort of beyond me but but
that's in there so anytime you transfer
photograph you're potentially doing a
copyright violation than that sweet and
you can't leave it out you could put
something in there that's not a
copyright notice of course if you create
your own color space in Adobe Photoshop
which is easy enough to do you can
create your own custom profile for your
own RGB color space and it'll have
Adobe's copyright in it there's a
description tag here and what you see in
a lot of these tags is what looks like
the same text printed out twice and it's
really because the structure of that tag
type allows you to have both an ASCII
string and a Unicode string in there and
the Unicode prints out a lot like a ski
when you ignore the nulls in there so if
you go on through that profile you some
of the tags you pick up in there there's
a there's a measurement tag that says
that the stuff you want to measure
according to this profile will be
illuminated with a d65 illuminant it's
got some codes in there that specify the
measurement geometry and stuff like that
but since it's really a monitor profile
it's not obvious what that stuff is good
for there's a white point tag that
specifies the white point of that have
the srgb color space in terms of XYZ
coordinates so you can pick up those
numbers and divide them by the sum of X
plus y plus Z to get the little X little
Y chromaticity coordinates and throw
them up on a plot and that's what I've
done here and it's kind of hard to read
where it says white point here because
there's a there's a black point that and
an illuminant that plotted into the same
location so the text just stomped over
each other but again this is just an
illustration how you can use trivial
code to see what's inside of a profile
it's got also the XYZ coordinates of the
three different primary colors so the
red primary here maps to this XY
coordinate and so on same with green and
blue so you can see what the color
triangle looks like and I've plotted on
here the locus in this space of the pure
spectral colors which is easy to get
from a pull down the XYZ standard
observer curves from somewhere and just
just plot it
there's other tags in there like the
device manufacturer tag and device model
tag which don't really apply very well
to a standard RGB color space designed
for monitors so they they stuck into
those tags the the International
Electrotechnical consortium or whatever
it's called the standards body that
defined this profile they put their info
in there in case you want to look them
up there's a viewing description tag
that again it references some viewing
conditions that you can look up and so
you know it says when you're looking at
your srgb monitor it tells you how
bright the room lights should be and
stuff like that in order to see the
right colors and the other thing that's
in there besides the XYZ descriptions of
the chromaticities are the the curves
that define the nonlinear encoding of
primary intensity into numbers in the
file and actually it it it goes the
other way so this is an expansive curve
which means it's it's indexed here by
essentially the the numbers in the file
this they always use ranges from 0 to 1
but then they map them into the min and
max numbers in the file so this really
corresponds to 0 to 255 and then the on
the ordinate is the intensity in which
you're supposed to reproduce that
primary on your monitor this is not
exactly a power-law curve we've
discussed this srgb funny curve before
if it is exactly a power-law curve then
the the TRC curve that's a tone
reproduction curve can be specified as a
curve having a single point and if
there's a single point there then it
interprets that as a gamma number
instead of as a curve so there's all
these kind of options where the the data
format has one simple format but the
interpretation of that format has has
you know a few things going on in this
case it's just is it one point or is it
more than one point so the srgb space
has the same same curve for all three
colors and these other numbers that I've
printed out here toast slope pop slope
and top gamma are things that I
calculate
from this curve I just looked at the the
slope of the curve at the two ends and
characterized it in such a way that you
could say approximately what gamma is in
there so in general if you run this
profile reader over a profile that has
some such curve in there that's not srgb
it'll it'll give you an estimate of what
the corresponding gamma is for that
color space what's in the header of the
profiles all kinds of junk in this this
is where things get a little bit nasty
and I didn't I didn't actually write
code to parse all this because I don't I
don't need all these random bits of
information and you have to go study the
spec to do it and wasn't worth the
trouble but but it's good to have an
idea what's in there so for example I
showed you that little string em NTR
that's the profile device class that
means it's a monitor class profile now
you may wonder why a srgb profile that's
used in typically embedded into the
images you get out of all digital
cameras is a monitored class profile as
opposed to a camera class profile first
of all there is no camera class defined
there's no such animal but more
importantly I think this this gets at a
that Steve Johnson brought up on which I
had a slight disagreement with him where
he he said that the the specification of
srgb as the color space and the images
coming out of a camera was essentially
meaningless and fraudulent and I
disagree that's it has a very clear
meaning it says that if you display this
image on an srgb monitor you'll get the
the colors that the camera manufacturer
intended you to get that's what it means
and that's all it means and it's not
fraudulent it just says you know if you
display this on an srgb monitor you'll
see the colors the same way we intended
you to see them it doesn't tell you
anything about how that camera
interprets the colors in the scene what
it used as the scene white point or what
it used as it's rendering strategy it
just doesn't tell you any of that stuff
it only tells you view this image on an
srgb monitor that's really all it's for
the notion of a profile connection space
I showed you that the profile connection
space for this srgb profile was the XYZ
space there's there's only two options
available here in the in the color
management profiles which are RGB space
or lab space and there's a essentially
one-to-one relationship between those
except a black works mini 21 so those it
doesn't really matter which one you use
except it affects perhaps the structure
of your of your mapping tables in the
case or you use table based mapping
there's a thing in there called
rendering intent that's interesting
there's a when you'd when you do pro
when you do conversions between
different color spaces you can convert
with different rendering intents and
they have by default the one that you're
supposed to normally use is called
perceptual rendering intent which means
keep keep the colors looking right
whatever that means and it's not all
that well defined although in more
recent versions of the standard they've
tried to define it better and there's
things in there like the the the XY the
XYZ coordinate of the aluminum or white
point of the profile connection space
and you can you can put the coordinates
of d50 color in there as the white point
of your profile connection space and in
fact you have to put those numbers in
there and if you put any other numbers
in there it's not a valid profile so
there's a there's a place you could
specify a different white point but it's
defined that no other white point is
allowed so its expansion room so here's
another kind of profile this is a
scanner profile this one I found laying
around on the web somewhere called a eps
n1 po4 die cm so that's a that's an
epson scanner profile and if you run it
through the same program prints out the
header it's it's starts off with kc ms
that's the kodak color management system
which was one of the one of the earlier
color management systems that was out
there and working this is copyright 1992
294 so you can see this one is 15 years
old
and if you if you go through it the the
code that I have it has this big switch
statement that hasn't otherwise in there
and it says you know if you didn't
recognize what the tag signature was
printed out say it's unknown so it
prints out a bunch of these tags whose
signature starts with a K and I thought
ah that must be a Kodak signatures night
I read the spec and found this line that
says private data tags allow CMM
developers had proprietary value to
their profiles and as you can imagine
that creates all kinds of problems
because if you've created some kind of
proprietary value for your profile and
the only compatibility hack is that
other people reading that profile ignore
tags that they don't recognize then
you're going to lose some stuff and it's
not clear what all is in there there's a
bunch of strings it's pretty clear from
this one that the the scanner for this
profile has been calibrated for
measuring ekta color plus I think that's
a color print paper so when you when you
put color prints on a scanner you want
to reproduce the color of that print it
helps to know what the media is that
you're putting on there if it's a
certain color print paper with a certain
set of colorants in it then the scanner
will respond to that at a certain way
and this is this is particularly
important for scanners because they they
generally do not attempt to be color
metric devices they're really
densitometric devices that is they they
have either three sets of filters or
three illuminance like color leds or
something that are designed to measure
the densities of the cyan magenta and
yellow dyes in the print that you're
scanning those can be tuned up to work
well for photos or for printed ink type
pages you know optimized for one of the
other make it work pretty well for both
you might need slightly different
profiles for the things if you put some
real color objects on there like you put
your face down on a scanner and the
colorants in your skin are made out of
oxygenated hemoglobin instead of magenta
ink you're going to get different kind
of responses and the the colors are not
going to be right but that's not what
those
scanners are designed for and it's not
what they're profiled for but it kind of
illustrates one of the issues and why
dance laughing he's thinking I wouldn't
put my face on that scanner you don't
know what other body parts have been on
there yeah so with a camera you have
kind of the same issue although they try
to design cameras to be more like color
metric devices so they can shoot all
kinds of different materials and get the
relative colors about right so you try
to get the observer metamerism of the
color to be as low as you can whereas
with a scanner that's usually not an
issue because you're usually scanning
stuff where you have a pretty good idea
what the colorants are and you could
make a profile for different kinds of
colorants if you want to so what's in
this scanner profile is a lookup table
thing yeah Martin yeah so the question
is what does a scanner profile really
mean and that's that's what I'm kind of
getting into here so what's in this
profile next is this thing called a
lookup table and if you read the the
section in the spec about that you find
this tag called a a to be 0 tag you say
well what's that and there's there's
three different when you when you find
the tag signature and you go get the tag
data you get this block of data you
still don't know what type tag it is you
have to actually read the tag and read
more of the spec to figure out how you
tell what it is you got so there's like
three different types here there's this
eight type of let 16 type and a let a to
be type and and what they're in there
for is to map the colors of the device
to the profile connection space so the
device in this case is a scanner and it
measures these three color densities of
some sort and reports them in some way
maybe it has already done its own best
estimate of a conversion to an RGB
colour space so that if you display it
on your monitor it will look about right
but you want to what you want your
profile to do then is to take that point
in 3-space that the scanner thinks of as
representing in RGB color and you want
to know really more exactly what color
that is and the way the color profile
does that is it maps that point in
3-space to a point in a known
standardized space namely the profile
connection space so this is a color
transformation from device to pcs and
then let's see so what's a pcs so the
profile is I just had tells you how to
get colors to or from between a pcs and
a device in one direction or the other
the pcs is usually a space that's
represented using 16-bit fixed point
fractions linear CIE colorimetry XYZ
space with a d50 white point or
optionally it can be sorry that should
say d65 white point which is what most
monitors use so that that brings up a
problem right there that the monitor
colors don't use the same white point as
the profile connection space but that's
a separate issue the pcs can also be a
lab space that is l star a star B star
which is a transformation of XYZ that
transformation itself depends on what
white point you use so once you've
specified the white point that becomes
one to one but whether different but
with a non-linearity in it and with
because of that non-linearity you can
actually get a getaway in most cases
with a precision instead of 16-bit
precision and one of the reasons they
gave this kind of flexibility instead of
standardized standardizing on a
particular profile connection space they
gave this kind of flexibility in order
to allow the tables to be smaller so you
could use a data instead of 16-bit data
that that was important back in the
early 90s when you were embedding
profiles and images you didn't want you
know if you could if you could cut the
amount of data in half on your tables
you wanted to do that nowadays memory
and disk being about a thousand times
cheaper that does
matter as much but there's all these
options in there that have to be dealt
with so the this profile connection
space has a lot of details hanging on it
too like the measurement geometry if the
the whole color management system is
centered around viewing prints under a
d50 actual illuminant and you want
things on your monitor it'll look like
they're going to print and you want
things from your scanner to take prints
like that and reproduce the right number
so that if you print them again you get
the same colors back so those prints
have to be measured and they get
measured with a certain color light and
a certain lighting geometry and that's
all specified as part of the profiled
connection space so I mentioned a couple
of classes of profiles there's the we
looked at the the scanner class in the
monitor class there's also a printer
class there's a device link a color
space conversion and an abstract and a
named color profile most of which you
don't need to be too concerned about but
again there's no camera class of profile
cameras when they do get profiled they
use the monitor I mean excuse me the
scanner type profile ones people often
refer to as an input device profile but
as originally conceived as a scanner
profile and it's really doesn't really
do great service for cameras and the
trouble with cameras is at the whole the
whole response of the camera depends too
much on what the what the lighting is
and you really need sort of a custom
profile for every lighting situation and
it may also depend on a lot of other
things going on in the camera like some
subtle sensor nonlinearities your sensor
may behave slightly differently for
short exposures than for long exposures
and stuff like that so generally instead
of characterizing a camera with a
profile the manufacturer characterizes
it in much more detail than you could
even get in a profile over a broad range
of conditions and they put all that
calibration information into the camera
and they use it to convert their images
into something like this srgb space or
something that
represents their best job of mapping it
into sort of known known color space
with some preferred rendering of the
scene based on whatever the
manufacturers preferences if you use a
raw data type capture that same
transformation is represented in the
manufacturer's code for for later
converting raw data into some kind of an
image people people continue to want to
put profiles on that but the profile is
kind of a it's kind of a blunt tool
compared to what's already done so I
don't understand the attraction for
wanting to profile cameras so among the
other things in there and the data
header there's a bunch of signatures for
different color spaces so right up in
the header you can choose X Y Z or lav
space as your profile connection space
for the profile and you can also choose
the color spaces that this profile
applies to so the both the scanner and
the srgb space we looked at were RGB
device spaces one of them was XYZ
profile connection space and the that
was the srgb and then the scanner used
the l a b profile connection space so
let's go back and look what was in that
scanner thing yeah
yeah when you change your camera or your
rock converter from sunny to flash yeah
on the camera or the conversion you can
think of that as changing a profile and
but the calibration data for those
situations is generally not represented
as a profile but it's usually done in a
more proprietary way because there's
more stuff you can calibrate about the
camera and the sensor than just the
illuminant one one place you can look
for how that's done is in the the open
specification for adobes digital
negative format dng well will be here in
a lecture on that actually in two weeks
I think on raw files so we'll hear more
about that but they they basically
represent in the DNG file they represent
the cameras profile for two different
illuminance and then they give you the
option to interpolate and extrapolate
between those so you can you still have
the flexibility of changing the
illuminant using a profile for sunlight
and a profile for tungsten light as sort
of a reference point you can pick any
anything in between or even beyond that
range so if you just pull this data
block out of this scanner profile this
eight to be blocked you want to know
this this is a thing that was described
as mapping device colors to profile
connection space colors you can just
plot it and see what's there you can
learn a lot by you know just making
plots of things and see what's there so
here we interpret it as 16-bit shorts
make a plot and you can see there's
obviously three what looked like look-up
tables here if you look closely you can
see three look-up tables there and you
get this big mess in the middle that's
got some nearly periodic structure in it
so you say okay you some idea what's
going on in there some kind of tables
you can go look at the specs some more
and figure out what's what it turns out
what we're looking at there is this
thing there's all these different things
that can be in there we're looking at
this thing that looks like a a clot B
which means there's a set of input
nonlinear tables a there's this
multi-dimensional color look
table and then there's a set of output
curves be and there's other things that
could be in there and say you have to
read about how to parse it all out and
so on and when you pull it out you can
plot those things separately so what the
scanner profile is doing is it's taking
the numbers out of the scanner and it's
running them through this compressive
non-linearity it's sort of like a gamma
curve so you put a lot of gain near
black and less gain their bright colors
I don't know if that's because the
numbers out of that scanner we're not
already gamma compressed or maybe they
were already gamma compressed but we
want to compress them more so it's more
like a cube root compression like you'd
use in a lab space anyway then they get
mapped through this multi doing this is
a three dimensional let there's there's
three tables here each one is indexed by
the three colors from the three input
channels it's there are 16 points or 15
steps spread across the range so you
interpolate within those so you take the
3d color you compress each component
through this you can look up and
interpolate through this and then you
take what comes out of that you run it
through this set of curves here and that
gets you to the profile connection space
which in this case is lav space and you
can see these curves there's a identity
curve here for the first dimension and
that would be the L and then they're
these steeper curves for the a and the B
so they for whatever reason they didn't
put that extra a and B gain into their
multi-dimensional lut they put it into
this table that follows it and there's
because of all the power and flexibility
in the ways you can factor this problem
into curves and tables and so on there's
a lot of room for people who create
files create profiles to do a good job
or a bad job or make them easier harder
to deal with and I'm not showing you the
3d structure of this thing here but
that's the next thing you could do is
you could kind of plot these things and
see what they look like yeah Jim
the space or is it a higher order to
avoid banding effect is the
interpolation linear or higher order I
don't actually know I think it's linear
but I'd have to go back and read the
spec and verify that with with 16 steps
and reasonable nonlinearities going into
it you probably won't see any Mach bound
effects from doing linear interpolation
there but this the kind of thing that
again Steve was complaining about that
profiles take this kind of a lookup
approach that he thought was kind of
course and it can be kind of course
especially at the bottom end where you
approach black and you need the most
fine curvy adjustments and so on the
table can be a kind of a blunt
instrument there oh yeah do straight
lines remain straight well obviously
they're going to have little kinks when
you cross the boundaries and yeah yeah I
think they're minor enough that you
probably won't see them people are
generally reasonably happy with the way
the internals of color management works
I think I mean I haven't seen anyone
point out that this is a problem or a
limitation I think it's if it is a
problem or a limitation you solve it by
putting it better non-linearity before
you go into the table so there's a whole
bunch of optional things you can do an
optional extras in the profile in this
table illustrates one space of those so
you've got all these different profile
classes on the left the the one they're
calling input profile class is what I
called the scanner display is what I
called monitor and output is what I
called printer because four letters
codes look like scanner monitor and
printer to me this eight to be zero tag
is I is generally required or its you
either use the matrix and tone
reproduction curve approach
in the center column here or you use
this nonlinear look up approach and if
you use the nonlinear look up approach
the requirement is to use the perceptual
rendering intent that's what 8 to be
zero means and then eight to be one
means a color a metric rendering intent
and 8b to means a saturation rendering
intent and those are all just sort of
tweaks on how you want to map your
colors and that's again a an area where
you get into a lot of complexity and
interpretation both user confusion about
what those are good for as well as color
management implementer confusion about
exactly what strategy you should take
each of these optional situations the
spec at least in the later revs does
have more information on order of
preference for these things if multiple
things are present and so on you can't
have in the input profile these things
on the right the B to a type tags map
the other direction from the profile
connection space to the device space and
those would obviously be most useful on
on the output side but you can have them
on the input profile and I think part of
the reason there is to make it easier to
go both directions because if you've got
an image if you've got an image in say
srgb space and you want to convert it
back to an image in the scanner space
you can do a profile conversion you have
to be able to convert from a an image
with an output type profile to an image
with a monitor type profile which means
all these tables need to be able to be
run backwards so if you've already got
the table designed in there to run to
run it backwards that's a lot easier
than if you have to try to run look-up
tables backwards which may actually not
even be a one to one thing anyway so it
gets complicated that just to illustrate
that but what's what's actually in the
profiles is usually not so much of this
stuff and it's pretty easy to look at
see what's there so that's all I'm going
to say on color management I want to get
into two more short topics shorter this
is an example just a image I pulled off
the web to illustrate the notion of
optical spot diagrams with ray tracing
rom told us all about ray tracing in the
in the stigmatic condition that is where
you where you know that all the rays
from a given point on an object converge
to a given point in the image there's a
lot of simplifying tricks you can use to
to map out how that optical system
behaves and get its magnification focal
lengths and all that but there's another
whole class of stuff people do with ray
tracing which is much more numerical and
that's to work out the behavior of
systems that are not perfectly stigmatic
and see what their aberrations are so
this is just a plot where the different
the different colors scattered around
here represent different wavelengths it
looks to me like from the I don't even
know what I found here but it looks to
me like from the numbers that this is a
in infrared system because they all look
like long wavelengths eve of that are
there in some funny units and you can
see that in the center things tend to be
kind of compact and circular and as you
get out to the corners of the field you
get these funny shapes called coma
because they're shaped like a comet and
the different colors don't all converge
in the same place and so on there's all
kinds of info on here this is plotted
out by a program I'm not sure which
program it is it might be Z max or one
of those commercial programs it says
reference chief ray that means that the
center of each of these boxes centered
on the chief ray the chief ray is the
ray that that would still come through
your lens if you stop the aperture down
to a pinhole so it's the it's the ray
that comes through the center of the
entrance or exit pupil and so that's
that's just an idea of what you could do
with ray tracing but if you want to do
your own ray tracing you can you can
work simple problems much more easily so
for example here's a problem that comes
up a lot in digital cameras in in film
cameras there's usually nothing between
the lens and the film and the the lens
manufacturers go to a lot of trouble
with these programs to make lenses that
converge all their raised very nearly
perfectly onto the film and make very
nice little spots so if you take one of
these very well corrected lenses and you
put it in a digital camera where between
the sensor and the lens you've got a
sheet of glass like the cover slip of
the CCD imager or another sheet of glass
with a filter on it to reject infrared
light or something like that or both of
these pieces of glass
set up to one to two millimeters or
something like that you can ask the
question what is this flat piece of
glass in there due to your image and for
a long time people kind of ignored that
didn't didn't see it as a problem but
but it can be a problem so you can just
trace some rays numerically you just you
know throw a bunch of rays as if they
were going to perfectly converge at a
focus and then and then see what happens
when you refract them through a flat
sheet of glass yeah Lance
oh yeah yeah yeah the you mentioned the
wrong name there but yeah the giggle
giggle pixel project guy Graham
something else right mentioned that they
were able to use a plano optic in the
middle of their optical system somewhere
to correct an aberration that was
otherwise very hard to get rid of to to
allow them to simultaneously correct
chromatic and other aberrations so yeah
the point is the same there that the an
optical element even if it's perfectly
flat can have interesting effects on the
point spread so let's just trace some
rays and see what happens so if we if we
start with these these magenta and cyan
raised over here that are all converging
toward this point in the focal plane say
we assume these Ray's come from
somewhere far left there where there's a
perfectly corrected optical system big
exit pupil and they're all going to
converge on a point and that's the focal
plane now we put a piece of glass
starting right here so this this ray
that comes in as cyan gets refracted to
this green ray and comes over here we
just it's trivial piece of code to at x
equals zero find the angle and compute
the refracted angle and draw a line from
there and we just draw that line make
this picture that's all it is so it's a
completely trivial piece of code I'll
show you the code and what happens is
that the because of the refraction the
rays converge further back so the first
order thing is that the focal point
moves back so that the if it would have
taken about a millimeter in air it takes
about a millimeter and a half and
glassware that millimeter and a half
that 1.5 is the refractive index
we could have put another glass surface
here and recomputed the refracted angle
as it comes out of the glass and see see
where it goes I didn't do that for this
simple case I'm just looking at the
convergence of rays as if it's within a
block of glass and if you put if you put
another if you put a sheet of glass in
there where these things converge
essentially what happens is they go in
at some angle they jog over they come
back out at that same angle again and
then it doesn't really matter if you put
that glass further left or further right
doesn't matter where you put it it's a
given thickness of glass will have a
given effect if you zoom in on where
those refracted rays are supposed to
converge the rays that are very close to
the axis converge here the pair axial
focus that that's the point that has
just moved back by a factor of
refractive index from where the other
focus was but a lot of those rays missed
that point those green rays are actually
making a smallest bundle back here
somewhere so the focus moves further
back for rays from the outer zones of
the lens and it's closer in four raised
from the inner zones of the lens and if
you get off off axis that's what these
other colors are we go back as the
caption on this image says we've got two
different chief right angles and what
this is supposed to really be simulating
is two different positions in the focal
plane the chief ray comes from the same
place in both cases and hits the focal
plane in different different places but
I've remapped the problem to where the
Rays are going to the same place so they
were all converging here so these red
Ray's correspond to to the case where I
take a 35 millimeter sized sensor and I
go 18 millimeters off-center which is
the the middle of the short edge of the
24 by 36 millimeter imaging area and the
angle is computed based on an assumed
distance to the exit pupil of the lens
that is where the where the pinhole
would be if I stop the lens all the way
down will give me that chief ray angle
so it's it's 80 millimeters away and
it's 18 millimeters off center and so
that's arctangent of 18 over 80 is the
angle that corresponds to the middle of
that bundle of red rays so when you when
you plot those raised there they're
really following exactly the same laws
the same code they come in they they're
aimed they're all initially aimed at the
same point they have some angle of
incidence they get an angle of
refraction so there's a there's a red
ray and a green ray go of both going
through the same place here but the red
rays continue to go further off angle
and as they get further and further off
that point where they cross the axis
gets further and further back due to the
aberrations of that flat plate and the
smallest spot you can get is way back
here so this and there's another spot
here which is pretty compact it's got a
lot of rays but it has this long tail so
you sort of anywhere between here and
here you get this kind of a compact spot
by different measures but it has this
comet look this long tail coming off of
it this is labeled in millimeters so you
can read right away here the the thing
has defocused itself due to this glass
plate not just by the half millimeter to
account for the glass but by another up
to 80 microns depending on how you focus
it at the outside of the field so if the
center of the field is focused here and
the outside of the fields is over here
somewhere that corresponds to a
curvature of field the spherical
aberration corresponds to the fact that
different zones of the lens focus in
different places so there's a region in
here that's kind of variably sharp and
fuzzy and so on so you can see all these
effects pretty clearly with this
completely trivial piece of code this is
all the code right here top half is just
sort of setting stuff up and then
there's this this little loop here and
all the work is done in this line that
says that the refracted angle is the arc
sine of the sine of the angle divided by
the index and and you just plot it so
the point is that you can make these you
know arbitrarily complex
hundred-thousand-dollar ray tracing
packages that you can pay for and do big
problems on or you can do this you know
one off quick hack thing to make a plot
and see see where you can get your
results by making measurements off a
plot and that that whole range of
options is useful I think to be able to
to deal with so here's a here's a case
this figure is actually from the I think
this one was from the 1911 Britannica so
it's in the public domain it's one that
I found and stuck in the Wikipedia so
it's in there too this illustrates what
happens in the kind of higher
dimensional situation that these these
lenses are really three dimensional and
I I just was enough if you had just to
cut through the middle where you're
looking at these rays converging at a
point here and I showed that you know
some other image height how it converged
differently and I had the plate in here
and so on but there's this other
dimension in and out of the board here
and things in that dimension they can
converge at different distances so you
get a stigmatism so if you really want
to work these effects in three
dimensions the codes going to be a
little more complicated instead of just
having X's and Y's and angles you can
have to have three space representations
of your array positions and Direction
vectors and surface normals and so on
and of course all you guys that have
done graphic ray tracing and so on that
be a piece of cake but if you want to do
it for optics probably better off just
buying the tool and doing it right yeah
why
hmm
yeah so the question is about how with
digital there's maybe new opportunities
to use unusual optical elements like
fiber optic bundles and so on or fiber
you talking about bundles or some other
ways of using fibers
right right things with non planar focal
planes and then you use an optical
faceplate to bring that down to a flat
sensor yeah so there are a lot of a lot
of technologies that use fiber optic
bundles as face plates to collimate
light and move it from place to place
and you can you can make bundles into
tapers so you can make a take a big a
big image plane down to a small image
plane and things like that there's all
kinds of cool things you can do they
tend all be relatively expensive because
making those fiber bundles reasonably
precise is quite an art have a friend
over at collimated holes incorporated
that does that kind of work for for
business and they make all kinds of
really wonderful stuff ok so that's
enough on geometric optics so the final
short topic is on simulating diffraction
now again you can you can use the
theoretical approaches with lots of
integrals and get a lot of insight into
the behavior that way but you can also
use numerical approaches to get some
some quick insights and sometimes some
of the problems you solve this way can
be easier or have the easier solutions
so if you just if you just some a bunch
of spherical radiating waves from some
set of source points that could be in a
in an aperture or something like that
and you sum them in all the locations
you care about as complex waves and then
take the absolute value you'll get the
light intensities and those places you
care about don't have to be in the focal
plane which I'll illustrate a second
here so for example you can do the
pattern of diffraction from a single
slit here I made a I made a slit that's
four wavelengths wide and I put some
number of sources in here and then for
each point out here I add up the waves
from each of those sources which is just
it's got a phase delay and it's got some
amplitude that falls off with distance
and you just add them up and then you
take say the real part and you can plot
it here or you could take the absolute
value if you want to just see the
intensity this one's this one shows the
phase by taking the real part so you can
see how there's a phase reversal at
these nulls and things like that
so this is not an imaging system because
it's not coming to a focus but it
illustrates a few things when I first
did this I and i got these ripples
across the slit I thought oh that's
that's messed up need more points so I
threw more points in there no matter how
many points I threw in there it had this
nice stable ripple pattern i finally
realized that's probably the actual
diffraction pattern of that slit and
that's one that you don't normally read
about because they they usually talk
about the far-field diffraction where
you have this this sinc function pattern
of distance out here if you take the
magnitude instead of showing the phase
of that thing it looks like this and if
this if this is correct and I have no
reason to think it's not then that means
that behind a slit you actually have
sort of a concentration of light in the
middle before it starts to spread out
and you have this kind of a interference
pattern of the the waves across the
edges of that for wavelength wide slit
and okay that's a that's a view of
diffraction I hadn't seen before and you
know as far as I know it's probably
correct because it comes from simple
numerical approaches but I haven't done
is to model what's going on on the other
side here and this this could be
complicated by the fact that I just sort
of assumed there's plane waves that stop
at this boundary and in fact they'd have
to either reflect or be absorbed or
something and so I make it more
complicated to actually try to
incorporate the physics of what's going
on that's not in the slip so let's
consider a lens so like we did in the
ray tracing if we assume a perfect lens
that is all the all the Rays are coming
to a point in the wave or diffraction
space that means all of the sources are
phased such that when you get to that
point they're all in the same phase and
they'll all add up constructively so you
can you can take an array of points
across a circular aperture and put them
all in the right phase so that they'll
have the same phase at some point or to
simplify that you can actually take a
spherical cap and distribute everything
in the same phase across that spherical
cap because those points are all equally
distant from the center of that sphere
so your focal point will have every
in the in the right phase and I did that
just because the code was easier and
then when you just add up these waves
complex waves and then take the take the
magnitude you can see what the spot
looks like and there it is so the focal
plane is here where it's most
concentrated and this is the focal plane
running vertically here and you can see
that nulls the nulls are spaced at them
they kind of look like wavelengths here
but these are not wavelengths these are
the the nulls of the airy disk and the
focal plane that are actually spaced at
n times the wavelength where n is the
f-number but you can and and so that's
pretty classical to cut it that way and
see what that looks like it's an airy
disk in the focal plane but what's what
I was doing this for us to see what
happens when you d focus it and the
thing that I never knew before is it if
UD focus it enough you get to a point
where you get a black spot in the center
so your brighten it up some your optical
spot if UD focus it that much will turn
into a donut with a dark hole in the
middle surrounded by a bunch of bright
stuff and it's it's generally kind of
complicated to analyze this defocusing
of a diffraction-limited spot there's a
there's a classic paper on it that I
have on the spie reprint book on MTF
analysis from the 50s and a couple of
papers on it where they they plot it and
analyze it and so on it's kind of
complicated but I want to see what it
looked like so again trivial piece of
code lets you do these kind of
explorations and wave optics ray optics
and so on and you may learn some things
you didn't know before like the the you
know the spot size it's not like you
just took the diffraction limited spot
and convolve with a disc because which
is cavalli what a disc is what you
normally think of happening when you d
focus and it's the it's what would
happen if everything was correct from
the model of ray optics that a point
turns into a disc and so any image when
it's D focused you just convolve it with
a disc but because of the the coherent
nature of light and the wave
faqs the effect of defocusing
diffraction-limited spot is quite a bit
different from taking the diffraction
them in its spot and convolving it with
a disc so this lets you have a good look
at what that is and the depth of field
on this thing you might if you write it
in terms of how much the MTF is reduced
or something is probably from about here
to about here so if that's between the
first nulls here is two point four four
times the f-number times the wavelength
then maybe it's it's like you know five
or ten times that much in the depth of
field so it's only a dozen wavelengths
or so that are in focus here for this
relatively low f number system that I've
that I've simulated but you can do that
for any f number and the whole thing
just kind of stretches out for for
higher f numbers yeah
yeah good good Bucky yeah there however
they said
the the mirror lens has gets its donut
from from the geometric ray optics
because the in the mirror lens you've
got this primary mirror and the center
of it is blocked by a secondary mirror
and or a hole in the main mirror so when
you d focus you can think of that as
being an image of the main mirror which
is a donut now if you combine that with
the with a diffraction-limited wave
aspect you could you could do that
simulation here just by blanking out the
sources in the center of that circle
that I simulated simulate that you know
coherent addition of waves from a donut
of sources you can see what the actual
boki is that takes into account both the
geometric and wave effects of that of
that mirror lens would be an interesting
problem yeah of course sort of like a to
slit diffraction maybe it is sort of
because you got these two regions that
didn't interfere with each other so it
probably get some extra big fringing
from that yeah okay any other questions
oh thanks for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>