<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Incremental Bayesian Networks for Natural Language Parsing | Coder Coacher - Coaching Coders</title><meta content="Incremental Bayesian Networks for Natural Language Parsing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Incremental Bayesian Networks for Natural Language Parsing</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ai1QZGuIC6A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I'm going to talk about a
graphical model of developed for doing
natural language parsing but I'd want to
give you a quick list of what I'm not
talking about I've also worked on spoken
language understanding have a project
currently for dialogue systems also
learning dialogue management policies
combining supervised and reinforcement
learning a bit undated to find kernels
earlier work on text mining so before I
talk presents the the talk i went to
give you a kind of historical
perspective on my work on using hidden
variables in parsing i did some earlier
work on neural network parsing looking
at the what kind of neural network
architecture you needed to capture the
generalizations appropriate for parsing
that didn't actually use hidden
variables but when i applied those ideas
to a learning-based architecture here
did use hidden variables and that's that
that in this later work is still the
only really successful application of
neural networks to natural language
parsing okay if you use laterz I applied
it to this benchmark Wall Street Journal
penn treebank dataset and got stated to
the art results with the generative
model then I did a discriminative
okay so um right so then I applied a
discriminative training method and got
this is one of the still one of the best
parsing results on the benchmark task
done some work on data to find kernels
derive from this model but that's mostly
the work of Yvonne cheat off so I won't
talk about it here and most recently the
work be present focusing on here which
is formalizing what the neural network
is doing in terms of a latent variable
model a graphical model with latent
variables so the talk i'll be giving is
actually starting from here moving
backwards but see that i've been working
on this stuff for a long time okay so
i'll start with a brief introduction
then talk about the graphical model how
applied it to parsing and particularly
how to approximate inference in the
model results and then some some further
discussion of previous models
so as anybody who's worked on
statistical models knows one of the most
important features is choosing the set
of features you're going to condition
your probabilities on usually that's
done by hand but we would like to be
able to learn that to induce those
features as part of learning and
specifically we want to avoid the need
to have data that's been annotated with
those features traditional you need the
data annotated with those features so
that you can just apply supervised
learning but if we use latent variables
then we can induce those features
automatically and we don't need the
annotated data and graphical models are
a principled way to define a probability
model with latent variables so in the
last couple of years there's been a lot
of interest in latent variable for
parsing because of these fact all these
models use they start with a pcfg or
something like that with a set of atomic
categories for the labels and then went
to expand those categories to more
detailed set of atomic categories and
they until very recently they've they've
actually had a lot of a lot of
difficulty getting very good results and
I think that that's basically because
it's very hard to learn the latent
variable annotations it's a difficult
search problem to learn those
annotations so they have some split
merge techniques and hierarchical dishes
processes that help them do that search
and that's why I think they've succeeded
we take an alternative approach where
instead of having atomic categories we
have vectors of latent features and I
think that the the reason that's so
successful is because it gives us a
richer similarity space in the latent
variable annotations
you know a given annotation has a lot of
similar annotations just by flipping
some bits okay right okay so by
searching in a in a feature space
instead of searching in an atomic
category space the learning becomes
easier so it's my intuition as to why
this is is working so well okay but as
soon as we have vectors of features as
as our latent variables and inference
becomes intractable so we need to start
looking at at approximation so I'm
proposing a graphical model which is
specifically designed to make the
approximation efficient and I'll discuss
two approximations one's a feed-forward
approximately they're both mean-field
approximations one is a first is
equivalent to the neural network I
mentioned before and the second one is a
more complicated mean field
approximation so in order to argue that
this graphical model is a good abstract
model that we should be working with I
bait first thing you need to show is
that when you have a better
approximation you end up with a better
parser so that's what we'll look at in
the in the evaluation section at the end
and also just that we get end up with
good absolute numbers so it's worth
worth continuing with okay so now I'll
introduce the different pieces
that go up to defiant go into defining
the graphical model and then the
specific architecture so it's a Bayesian
network which means it's a directed
graphical model the nodes are variables
the links to find conditional
probability distributions as if probably
most of you know the probability
distribution for given variable is a
parameterize function of the input
variables values and variables can be
divided up into visible variables the
ones where we we know they're not their
value and hidden variables the ones
where we don't don't know their value
and we want to marginalize out those
hidden variables okay so sigmoid belief
networks are kind of Bayesian network
where the activation functions the
probability distributions are defined in
terms of a sigmoid function or in
general a log linear model so a could be
normalized exponential and as I said
we're going to use vectors of binary
latent variables for these these hidden
variables here so the third component is
dynamic bayesian networks which is a way
to deal with unbounded ly long sequences
because we're dealing with parses
sentences can be arbitrarily long
parcels can be arbitrarily large we need
that
yeah yeah so that the so this would be
the probability just these are actually
normalized Exponential's but these are
are just binary random variables and so
the probability that the value of that
variable is one is a function of the
parent variables which are these here
and it's just a weighted sum their
weights on these I just so weighted sum
of the the input variables and then this
is a sigmoid function and that gives you
a probability just between probability
of being one which tells you the
probability of being zero
okay so um dynamic bayesian networks for
dealing with unbounded sequences tipping
the the typical example of a dynamic
bayesian network is a hidden Markov
model where the you have the Markov
assumption every state is dependent only
on its previous state or some bounded
window previous states and okay but for
our application that's not appropriate
because we're modeling derivation
sequences so every tree for any tree we
can define a derivation for that tree
and then we modeled those sequence of
derivation steps so it's the or the
actions that a parser does in
constructing the tree these sequences
are not Markovian a particularly neural
network work has tried to pretend that
it is Markovian and it just doesn't work
there the dependencies in the sequences
you there are dependencies you need in
the derivation sequence that can be
reaching arbitrarily far back in the
sequence and that's because the locality
the appropriate locality is in the
output parse tree if you want to know
the probability of say a label of the
tree it generally is sufficient to just
look at the local context in the tree
but that doesn't map to local context in
this in the derivation sequence so we
want to have some way to define our
graphical model so that those links
between variables represent locality in
the tree and not locality in the
sequence so the method we use is the
incremental specification of model
structure so the idea is you're
proceeding through the derivation and at
this point we wanted to say estimate the
probability of this variable here these
are all visible variables we define this
set of
pendants the statistical dependencies as
a function of these visible variables so
it gives us a kind of switching model we
switch this structure of the model here
depending on what the visible variables
here are and that switching is done the
definition of these statistical
dependencies is done based on structural
locality in the partial structure that's
been built up as defined by this set of
decisions that you've already made right
for constraining the parse tree not
local in the sequence such writers award
correct that's right by having the
switching so that say this link can go
arbitrarily far back to previous states
and as we'll see as things are in the
sequence if I use the right the the
states are in the sequence of actions
that the parser does the derivation and
it could be that say you you you started
out building a known phrase and then you
went off and built a relative clause and
it took you a long time and when you
come back to say find the predictive
verb of the sentence you need to then be
linking to a variable associated with
the head noun not with what's at the end
of the relative clause so you need that
local relation so it's very local
relationship in the tree but it can be
arbitrarily far in the sequel derivation
sequence because you've got that
relative clause in between so
that so you need to have some way to
define these these long links and
they're defined by looking at that tree
that has structured the tree that you've
built with the sequence of actions
that's all your doesn't to sign
decisions and deciding how to how to use
this framework apply it to parsing so
yeah that's where the domain knowledge
comes in left to the modeler right right
so I don't assume a top-down derivation
or anything like that that's all left up
to the modeler okay so this is what we
end up with in a dynamic sigmoid belief
net work with incrementally specified
model structure and this is the the
detail we have these vectors that are
representing the state of the parse we
actually decompose our decisions into a
sequence of decisions and we have
dependence on previous states but also
we allow dependence on on previous
visible variables previous decisions via
some arbitrary function extracts
features from those decisions and those
are explicitly input yeah
well there there's no bounded window you
can there's no bounded window that you
can use it can be arbitrarily large okay
so and also just making bigger and
bigger windows only works if you have
more and more data and and then you know
it you're you you're not imposing any
linguistically appropriate bias if you
just use bigger and bigger windows it
could be any kind of string but you know
that it's a parse and so you want to
encode that structural information in
your statistical model because otherwise
you're learning problems just hopeless
a bounded number are the tray arbitrary
but a bounded number need you need to
have a fixed set because you need two
for each relation there's a set of
weights and you need to learn those set
of weights you need a finite number of
parameters okay so this is the first
slide about why did we choose of all the
different ways of defining graphical
model why did we choose this one m and
it's so that we can do the approximation
efficiently in particular so that we we
can have lots of possible model
structures without ever having to sum
over all the possible model structures
we don't need to marginalize over the
model structures and that's for three
reasons one because of the incremental
specification model structure it's only
the future decisions that you don't know
the model structure for yet and all the
unknown edges in this model structure
are directed from the past or current
structure to the future and there are no
visible variables in the future you
don't know anything about the future
because it's a generative model so given
those three things we can derive that
those edges can have no effect on the
probability that we want to compute
which is the probability of the next
decision so all that future model
structure is just irrelevant we can
ignore it we don't even have to think of
what it looks like and therefore we
don't have to marginalize over that all
possible model star structures okay so
now i'll show how this can be applied to
to parsing and then once we see that
we'll talk about approximations okay so
as I mentioned it's a history based
model where Mont were modeling trees as
isomorphic dare
servations and then we're parameterizing
that as a probability of the next
decision conditioned on the history of
previous decisions just the chain rule
we also allow non-atomic decision seek
for our for our decisions it could be a
sequence or hierarchy of decisions so
what we want in the end is the sub
decisions conditioned on the history of
previous decisions and previous sub
decisions so we do that by we estimate
that probability by building the
structure for the history plus the
current decision we only need this much
of the structure as I said because the
future structure is irrelevant and then
we do inference over over this model
okay so one of the crucial features of
this model as I've mentioned of these
vectors of features that are
representing a compressed compressed
representation of this unbounded history
here you can also think of it as a label
of the state of the parser but if we
think of it as a compressed
representation of the history it's used
for computing the output but it's also
used say this one is then passed to a
future compressed representation so you
can have a feature passing situation
where you learn something relevant here
and that feature then gets passed to
here or maybe passed here to here so you
can have feature passing all through the
the network through the sequence of
latent variables completely induced
during the course of learning and if you
design it right you can end up with a
model that has no hard independence
assumptions which is unique in in
parsing work on the other hand it does
have soft independence assumptions it
has soft biases because the longer the
chain
of states it needs the information needs
to pass through the more likely it's
it's not going to be learned so there's
a strong locality bias and so when
you're designing this what these links
are going to be what you're doing is
designing that locality bias that's
going to bias your learning to learning
certain correlations and and ignoring
others but it's a soft bias depending on
locality and as I said locality in the
structure okay so um uh as was mentioned
to to build one of these models we need
to define a derivation order so we use
predictive lr derivation order which is
the kind of left left corner derivation
and that's because of efficiency of the
parsing and the statistical dependencies
again are designed by hand and those are
also taken from my earlier neural
network work rather than going through
those i'll just give an example we start
out with a generic root node and predict
the first word project the next project
and NP and at each step we're building a
new bit of the partial structure by
depending on the decisions we chose here
we're assigning the next state to one of
the nodes we assign it to the node
that's on top of the parsers stack at
that given time and then we derive the
set of links to previous states based on
structural locality between the the note
on the top of the stack and other nodes
in the partial structure that's been
built up so far and then we want to do
inference over over this graph
yeah well I can show it
you need to do search it makes the
decoding problem more complicated
because you have to basically guess what
the best derivation is going to be keep
a lot of them in parallel and it's only
when it comes to actually predicting
that future word that you discover that
some of those things were really bad and
some of them were good so that's that's
true of all generative models I'll talk
about that at the end so here's another
example that might look a bit more
familiar to people familiar with hmm
it's a dependency parsing where we're
just trying to recover these these head
dependent relationships we don't have
internal structure and so again we're
assigning our states to the to the node
that's on top of the stack at that
particular time there's also a node
that's at the head of the queue and the
derivation orders defined in terms of
previous work and dependency parsing and
then we defined the set of statistical
dependencies based on the structure
that's been built so far and at each
point were computing a new state
defining the set of dependencies based
on the structure and then predicting the
the next action of the dependency
parsing
okay so that's the inference so the
inference problem we need to do is is
that we need to compute the probability
of one of these things given all these
visible variables and that's a very
complicated inference problem so we need
to do approximation it's completely
intractable to do exactly its sampling
methods if you try to do them exactly
also would be too too complex we're
looking at approximate sampling methods
which probably are also a good way to go
but so far we've looked at mean field
methods so mean field methods it's a
variational method so we have an
approximate model that we define and
mean field it's a fully factorized
approximate model which we choose
because it can be easily computed we can
do inference in that model tracked ibly
this model has a set of free parameters
namely the means of the the means of the
variables in queue and we want to set
those means to minimize the coblich lie
blur distance between the approximate
distribution and the true distribution
okay so that's the mean field approach
that's also too complicated because in
general every time you see a new visible
variable you need to update the means
for the entire structure which is can be
expensive so the first approximation we
look at is the most efficient possible
one namely every time for every variable
you only compute its mean once the first
time you see it the first time you need
that variable to compute a probability
you estimate its mean and then you you
never update it again you continue to
see more more evidence about the pars
but you assume that it's irrelevant for
that that mean this gives us a strictly
feed-forward computation which is very
fast and it turns out equivalent to a
neural network and specifically because
of the way of design the graphical model
it turns out to be equivalent to the
simple synchrony networks that I
developed my earlier work on neural
network parsing so that's the sense in
which we can think of the neural network
as an approximation to a graphical model
with latent variables so each one of
these variables has a mean associate the
queue distribution is just exactly like
this only without any links or it's
fully factorized and each one of these
has a mu Amin and so you're trying to
adjust all these these well we're trying
to adjust these means so that they
maximize or minimize the difference
between the resulting distribution here
and what we would get if you had the
true full model yeah these are 0 1
discrete random variables yeah yes it's
the expected value of that 0 1 value and
that that mean is the the activation
value of the neural network it turns out
to be equivalent to the activation value
of the neural network
right so I was back here okay so that's
the the feed-forward approximation the
problem with that approximation is that
it's it doesn't do any backward
reasoning or bottom-up reasoning when we
when we then observe the when we then
observe the decision the next decision
in the parse we should go back and
update the means based on that
observation and we don't do that so we
this is mostly the work of avanti tough
but we developed a method that does
update these these means but only for
the current step we don't go back all
the way through the parts we only update
the current set of states when we see a
new piece of information about the
derivation for this state and that
involves a numerical method that we run
into convergence to minimize the screw
black lai lai blurred divergence okay so
the first question is are these good
approximations so we did a simulation
with just a sequence labeling problem so
it is a Markovian problem and we did
that because then we could do a gibbs
sampler to give us an upper bound on how
good an approximation we could possibly
expect we use unigram as a lower bound
and the neural network has a sixty-three
percent error reduction over the you
know gram the incremental mean field has
a seventy-three percent and the
incremental mean field has a
twenty-seven percent improvement over
the neural network and error reduction
so incremental mean field is
better approximation than then the
neural network but also the neural
network is also a pretty good
approximation this is just artificially
generated problems so we we took a
Markovian version of this model we
randomly generated the parameters of the
the ISBN it's part of speech tagging
it's just to seek sequence labeling you
either forget and we had small number of
labels and define a model generators
bunch of sequences then we verify that
that these two had a sufficiently
different values so that we could get a
meaningful result took those I think we
had ten experiments and Randy's on it on
them so it's completely artificial data
so we know what the abstract model is
supposed to be doing and regenerate the
data no I think it's a bigram model on
the vectors of tagged features yeah but
with vectors of features it's a hidden
work yeah so this is a much simpler
model than than the parsing models but
we did that because then we could run
the gibbs sampler and get an upper bound
okay so incremental mean field is a
better approximation neural network that
they're both good approximations okay so
how does the neural network end up being
at all reasonable as an approximation so
this is the second aspect of the model
that that makes it designed to be
approximate and that's these links here
if in a hidden Markov model you wouldn't
have these links all the information
about these variables would be
pass through backward reasoning to these
variables and forward reason to these
variables by adding these links we're
providing a shortcut so with with
forward reasoning we can compute an
approximation to this backward chain of
backward forward reasoning so that's why
the neural network manages to do a
reasonable job even with just forward
reasoning so when training the parser we
train the approximate model both these
approximations to find perfectly good
probability models so we just train the
approximate model we don't try to find
an abstract model directly and that
training just involves gradient descent
so it's back propagation for the neural
network and for the mean field we need
to assume convergence of the numerical
method and then inverse inverse some
solve some equations okay so the last
bit I need to tell you about is the
decoding as I mentioned before we chose
the derivation order so that it we can
parse efficiently and specifically it
means that we can do beam search with a
very small beam so after every word
prediction which is where you get new
information about the sentence prune
down to only a hundred twenty or forty
still does very well possible
derivations and then there's also
branching factor limit just to keep the
really bad cases from blowing up but it
doesn't have much of an impact so we're
doing a beam search through the space of
possible parses okay so evaluation as I
said we will have two questions one does
is it worth trying to come up with a
better approximation will it give us a
better parser and
two are we getting good results in
absolute terms so we use a standard
benchmark data set but we're only
looking at at words sentences at most 15
long several people have done this
before we do that basically because the
mean-field method is slow to train and
we didn't have time to run a sufficient
number of experiments we're running this
on the the mean field on the incremental
mean field on the full data set now but
I don't have those results yet so the
results on on that data set are that the
incremental mean field does do
significantly better than the neural
network almost a full percentage point
which is a lot for this for this task in
these levels of performance and
therefore we conclude that is beans are
a good abstract model there there's
something worth trying trying to
approximate so these are the results
from previous papers on the exact same
data set so training and testing on 15
centons most 15 words the IMF is is not
statistically distinguishable from the
best results on this data set
okay so that answers our second question
the intern in absolute terms we are
getting good results so again it's worth
pursuing this this approach okay so but
that's the 15 words data set so I wanted
to look at I wanted to show you the
results on the standard complete data
set these are the results of generative
models on this on this data set where
we're very close to the best results
with our generative model even though
that we have a fairly small vocabulary I
think I can get better results because
this is done with an external tagger if
I incorporate the tiger into the
probability model I get a better results
on the validation set but I haven't run
that I haven't written it up so I
haven't run it on the test set yet and I
think it'll probably be around the same
if not better than the best result there
in terms of latent variable models that
have been run on this test set we get
the best result this is not the the
generative model I've been talking about
that's a little bit worse than the
Berkeley people but I'll this is the
same model that's been trained based on
the discriminative criterion I'll talk
about in a minute so that's that's the
best latent variable models these so
this is a basically the best results
from all the different groups that have
run experiments on the benchmark data
set again were one of the best see that
there's this whole 89 290 group where
we're on either end of that and then a
couple little better than 90 but what
one of the best and the
is a much bigger and much slower model
they're both much much slower in terms
of parsing speed okay another set of
results this is from this year's Connell
shared task on dependency parsing we
just sat down designed a model
implemented it ran it on the the data
they gave us for the task and we end up
with the third best result this is the
average result across all 23 systems
that were submitted and very close to
the best result for the task and all
there were only two better models and
those are both multi version systems
there were combinations of multiple
models which you can always get
improvement by combining multiple models
so we were the best single model system
so in that sense we have the best
dependency parts single model dependency
part so this is a graph of of parsing
times for the dependency parser to get
the accuracy we're running way out here
somewhere to get the absolute best
accuracy but you see it really flattens
off so with just a small decrease in
performance you can get a much faster
parser just by searching with a smaller
beamwidth so we're getting 60 words a
second for one percent decrease from the
maximum and so that's that's as far as
I'm aware much faster than other
dependency parsers this is for for this
sample of languages
well this difference up here it's not
there no search problems left because
you're really flattened out well
otherwise increasing the beam would
improve the performance and it doesn't I
you can cruise the beam as long as you
want and it just doesn't make any
difference so at least they're not
they're not random search problems maybe
some systematic search problem but I
really doubt that okay so I mentioned
those discriminative ly trained version
of the model so i wanted to talk about
that well i guess i've had some
questions during the talk so can i can
write go over my 45 minutes so instead
of optimizing this this generative joint
probability we normalize out the the we
normalize it so that we get a
conditional probability but this
involves and then we try to maximize
that objective function problem is that
then we have to sum over all possible
parses a testing time we actually use it
this model as a rear anchor over the
generative model those are the results
I've showed before and that's because of
the way we train it instead of summing
over all possible parses we sum over the
most probable parses as determined by
the generative model but it's not just a
simple reranking model if you just sum
over the the candidate parses and
normalize like that it doesn't learn
very well at all instead I have a method
where the prune derivations their
probabilities get approximated so you
can think so the set of possible
derivations forms
tree and you have some subset of that
tree in your candidate set and there are
places where the possible derivations
have been pruned off by the search and
if you approximate the so the
conditional probability ends up you
you're looking at the forward
probability and comparing it to the
backward the sum of the backward
probabilities if you approximate the
backward probabilities for the pruned
searches prune derivations in such a way
is to minimize the difference between
the discriminative and generative
objective functions then you then you
get a the result that I showed before we
get this very good result so it's a way
of doing discriminative training Yeah
Yeah right I didn't know if I'd put it
that way well I can tell you that for
the data to find kernels we did a rear
anchor version of the data fide kernel
and trained a thing we did a voted
perceptron and we got almost as big an
improvement but that that's a large
margin a method which generally produces
better results than just a conditionally
trained method the conditional training
what it does is basically in in the
generative model you have to learn all
these word predictions and you're trying
to actually learn what is the next word
going to be and that's a hard thing to
learn with the discriminative ly trained
model all
trying to learn is given my history if
this is the next word is that evidence
that i'm wrong or is it evidence that
i'm right and that's a much easier thing
to learn so you're basically just
learning to check whether your
derivation is consistent with the word
and that's where you get the advantage
here so it's a simpler problem okay very
briefly because i know people here
interested in speed and because of the
question about the search of the
generative model i defined a different
probability model that includes the
conditioning on the look-ahead string so
it the probabilities that you're using
to search with are actually using that
look ahead string and therefore the
pruning is is a conditioned on what's
got what you're going to see in the
future problem is it's very difficult to
condition on an unstructured string even
though I'm using a neural network run
backward of the string so there isn't
actually a window effect but there's a
kind of soft window effect anyways the
absolute performance is is here is is
lower than the generative model because
it's very hard to condition on that
future string but as we see we can get
away with a much smaller beam this is I
think 23 being beam width of only two
three four five so very small beams that
you can then search with so the natural
thing to try is what if we prune with
the discrim re-rank with a generative
model and then we get this nice curve
here that smoothes out between the two
curves so we can get good performance
even if in this region where we're we're
we're running fast so these absolute
numbers aren't appropriate I ran these
experiments a long time ago but that's
the kind of curve you get if you do
pruning the search with a look ahead
model and
you you actually choose the best parse
with more accurate generative model okay
so conclusion as I said with espn's are
good abstract model they provide a
powerful method of feature induction we
can approximate I spins in a tractable
and effective way even when we use a
very fast approximation it's still very
effective for parsing and not
surprisingly discriminative methods give
us an improvement over the generative
messages so that justifies looking at
more accurate or more efficient
approximations to isbns using this as a
framework for for formalizing how to
improve these models specifically I'm
interested in looking at regularization
we should be able to have a regularizer
such that when we learn we're learning a
model that can be effectively
approximated extra accurately
approximated and it turns out that
standard weight decay Gaussian
normalization on the neural network does
minimize a bound on the approximation
error but we can come up with with
better bounds so it's a very interesting
way to justify regularization and then
of course there are other things we
could apply this architecture to not
just parsing polymer lows look working
on semantic parsing I'm collaborating
with her on that we could also do
synchronous parsing for smt or and if
anybody's got a problem I'd be
interested to hear about it thank you
sigmoid vs what ah it's it's just what
you get if you have a log linear model
and everybody uses log-linear mods I
don't really have a better answer than
that it's smooth it's easily direct
differentiable yeah so the the parameter
set is for the model for the slides that
had well maybe it's
ok so the this has the individual links
and each one of these links has a weight
and that's that's the set of parameters
and each one of these has a bias yeah ok
so we have for the best results the
vocabulary size is about 4000 tag word
pairs and anything else is treated as
tag unknown word there in penn treebank
I think 36 non-terminal labels you need
to predict and yeah that's it's just a
structure the labels in the words so
that's what goes the these sequence of
decisions down here are things like okay
first we decide to project a new node
and then we decide to choose a label of
that node so that lets the sequence of
sub decisions
well they mean field is itself
incremental so we don't actually update
the means for the more distant things we
only update the means for the current
state so I guess I guess you might just
yeah I guess you would you would because
because you have a more accurate
estimate of those means it not only
gives you more accurate estimates now it
gives you more accurate estimates in the
future the future states that then
condition on this state are getting more
information out of those those hidden
units so I would expect it to be better
at passing features I ran long we we had
some results some particularly for the
dependency parsing that our method
compared to other methods are actually
very good at at a relatively good at
just outputting the long-distance
dependencies in the dependency structure
and I think that's largely because of
this ability to pass features yeah so
the questions one if you look at your
knowledge that you have a sequence of
predictions that we like you will find
each of those as how many such events
and in your training and then how many
parameters do the events and training
data the pen and the training set is on
just under a million words I think and
number of events per word is it gets
under two so maybe around two events
pervert
yeah events in the training data and the
number of parameters I have 80 hidden
units so there's a T squared of these
parameters small number structural
decisions there are huge number
parameters in the word prediction
because we have 4,000 possible tagged
word pairs you can predict and that x 80
so there's a huge number of parameters
there but they own that they only get
well there they're either not used very
often for the numerator they're not used
very often for the denominator they do
get used because you need to normalize
over the set of words or set of words
very given tag so they're they're not so
crucial there's a huge number of them
but they're not so crucial well it's not
quite so simple because every pram
except for the word predictions of
virtually every parameter sees virtually
every event and its regular eyes so it
just stays near zero if if you don't if
you don't have enough data it just stays
near zero or goes to zero and my second
question was more general I'm sure
got somewhere else you see parsing as a
problem in itself or you would like to
use this to so long um well I think
parsing now in NLP has become a
benchmark task force structure
prediction problems for in machine
learning applications to structure
prediction problems people aren't so
much just trying to get a better car sir
if if I was doing that I would do
something like char neck and Johnson
have huge number features to a Mac sent
you know SVM's etcetera throw everything
you can think i have 20 different models
etc then you'd get a better parser you'd
have the best parts in the world nobody
really cares you want to run an
experiment where you have a new you know
you have a method you you want to see if
it works you do a controlled experiment
so that's really what people are using
parsing for now but that's just kind of
from the people interest in structure
prediction problems otherwise i think
you know it parsing doesn't have a great
history in improving people's
applications but i think the time will
come when it when it will we're kind of
running out of what we can think of for
Engram models in my opinion and the fact
is language has these very clear
structural statistical dependencies and
if you can somehow exploit that
information in a robust statistical
system that's tailored to your
particular application and I think we
will get quite big improvements in the
not-too-distant future so so you're
interested that's what I've done in the
past that's what I've done in the past
but I would be interested in the more
application side there's a lot of
interesting problems and how to
integrate tasks and I mean it's not
enough to just come up with a parse and
pass that one parse to to the task
that's certainly not enough all right
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>