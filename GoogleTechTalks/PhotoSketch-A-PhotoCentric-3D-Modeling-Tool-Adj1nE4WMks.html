<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoSketch: A Photo-Centric 3D Modeling Tool | Coder Coacher - Coaching Coders</title><meta content="PhotoSketch: A Photo-Centric 3D Modeling Tool - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoSketch: A Photo-Centric 3D Modeling Tool</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Adj1nE4WMks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is George Wahlberg and this is
joint work with see
zouk I at brainstorm technology which is
a New York City startup dedicated to the
development of 3d photography software
okay so to give you an overview of what
we're doing imagine that we have a scene
over here as shown in the upper left and
we take various photographs around that
building and we apply it through this
black box called flow to sketch to come
up with a photo texture 3d model that is
lightweight suitable for web-based
applications so our inputs here are
photographs and also the user sketching
upon the photographs and in essentially
something similar to Google Sketchup but
augmenting google sketchup complementing
it with the ability to have multi view
geometry from an array of photographs
now the input doesn't just have to be
the photographs and the sketching upon
the photographs but it could be the
photographs and another 3d model so our
input could be lets say a Sketchup model
is shown on the on the left right here
okay so that's the Cooper Union Building
in New York and these are photographs
taken around that building so the
problem here is how do you automatically
map these photographs to the 3d model
and the result we would get is a photo
textured google sketchup model and
notice that the the model of course is
lying here on google earth but you could
also see all the positions and
orientations of the cameras that took
those photographs and they're overlaid
on Google Earth as well so we know where
we were when we took those pictures not
by any GPS information anything like
that just deriving it directly from the
photographs okay so that's what this
talk will be about how do we do this
here's the vanilla version of that
Google Sketchup 3d model of Cooper Union
and this is in lower Manhattan so what
we did is we went around the building we
took photographs and the frustum shown
here represent the positions and
orientations of the cameras that took
those photographs again they were
derived directly from the photographs
and the camera was not pre-calibrated
okay just go around take arbitrary
pictures around the building we can
figure out where they were with respect
to that build
and then just to compare the vanilla
version of the Sketchup model on the
left with the photo textured here now I
only showed the texture applied from one
of these photographs just to make it
clear the difference between the regular
model and the photo textured model so
the purpose of this talk is to explain
how to go about walking walk around the
scene take pictures and automatically
take those photographs and texture the
3d model okay so some background this is
all related to our work in 3d
photography software which represents
the capture of 3d geometry of existing
large-scale structures and so we're
trying to have photo textured and
thereby photo realistic 3d models of
large-scale urban scenes this image that
you see on the Left represents a
photograph of one of the landmark
buildings on the city college campus
it's a neo-gothic building in Upper
Manhattan and on the right you see a 3d
model derived from that building this
particular model was was generated using
laser range scans all automatically
stitched together now we don't want to
just have that 3d model display directly
we want to be able to photo texture it
so we went around the building and we
collected photographs and the center of
those white balls they represent the
positions of the cameras they are
connected to the little green balls
which represents the optical axis of
each camera so we automatically recover
the position and orientation of each of
the images that was used to capture the
the scene of this 3d model and then we
projected those images onto the 3d model
so the applications of this work is
photo textured models for google earth
or Microsoft Virtual Earth also 3d auto
navigation systems realistic sets for
movies and video games and
pre-visualization which means the direct
you may want to see what the camera
shots would be what what happens when
you apply certain camera path to a scene
so you have to populate the scene with
with lightweight 3d models of course
their applications in urban and military
planning
virtual tourism and modeling theatres
and stadiums so that you could see what
your vantage point would look like from
any particular seat in the theater so if
you could model it then you would be
able to do that now traditionally
there's been two ways of acquiring the
3d models by active sensing or by
passive sensing and by active sensing we
mean that we get laser range scanners to
actually sweep a laser beam across the
scene and we measure the depth by
recording the time of flight of the
reflected beam so that would be the time
of flight version of active sensing
there's also the phase based version
whereby we measure the difference in
phase the time of flight method is best
suited for long-range applications and
outdoor applications as well as indoor
whereas the phase based result is good
for short shorter measurements and only
indoor scenes because they are affected
by solar noise so you can't use them for
outdoor scenes now a more recent
development in this arena has been in
flashlights are and there you send out a
single pulse and get back an entire
frame at once at video rates basically
the frame is generally smaller it's
perhaps 128 x 128 or even as much as VGA
resolution if you're using z chem but
again I I should point out that that's
only good for indoor use and for shorter
ranges perhaps 10 to 20 meters obviously
for buildings you need larger range you
need outdoor usage so we have to go with
time-of-flight laser range scanning that
sweeps across the scene but in this talk
I want to devote the talk to the second
the second mode of acquisition which is
passive sensing thereby we just acquire
photographs and we acquire many
photographs so that we can apply
multi-view stereo so that that will
allow us to derive depth measurements
via triangulation and so that's what
this talk will be devoted to how to use
the passive sensing mode the acquisition
of photographs to photo texture the 3d
models just as a point of comparison
there's a big difference between passive
and active sensing in terms of cost wait
speed resolution and accuracy you know
the cost of an active sensing devices is
a thousand times greater than passive
sensing the weight is significantly
greater the speed is much lower
it's much slower and the resolution of a
frame of active is about 1 million 3d
points and vs. several megapixels for
photographs but of course the accuracy
is generally higher when you use our
active sensing the accuracy is higher
for active sensor now in either case we
derive heavyweight models which means we
have to store a lot of information you
know a lot of points in the point cloud
for instance in the previous example
with the hunter building there were 14
million 3d points and the file size was
several hundred megabytes you know if
you really want high accuracy and keep
all the information about the 3d
building that's generally useful for
civil engineering applications but for
these web-based applications like Google
Earth or virtual earth you generally
want much lighter weight so you know
this is not generally an approach you
want to have to have heavyweight models
so we have to go with lightweight models
here's an example of a lightweight model
the same building and it's on par with
thee with the resolution of the other
models that already available on Google
Earth so this requires less than a
megabyte and I didn't show it photo
textured here but because I just want to
show the model itself these are the
other models that are already available
but the talk will follow this particular
overview there are basically two
workflows that I'll talk about today and
in both cases the inputs to both
workflows is what's shown here at the
top under multi new geometry I should
point out that every box that show that
is shown great out is in either an input
or an output and every white box is a
module through which the input goes to
create the output so at the very top we
see that we start with multiple photos
either from a pre-calibrated camera or a
totally uncalibrated camera in either
case we recover the camera pose which is
the position and orientation of the
camera if the input was pre-calibrated
we only recover the extrinsic camera
parents but not the intrinsic ones like
the focal length from the central point
because technically that's known if it's
pre-calibrated if the photographs are
uncalibrated then we have to
cover the extrinsic and the intrinsic
and by intrinsic we refer to the focal
length if you have the choice of course
you would like to pre calibrate your
camera which involves just taking a few
pictures of a test pattern like a
checkerboard image so that we can
recover the radio lens distortion and
focal length information and central
position information so if you have that
option if you have that chance just take
a few pictures of it pre-calibrated you
know of a test pattern so that you can
calibrate the camera if not we can
recover the focal length anyway but we
can't recover the radial lens distortion
in either case you come down here and
the input photos as well as this camera
parameter information comes down to the
photo sketch module whereby you actually
trace upon the photographs and so the
photograph serve as a drawing stencil
and you can lay down footprint and
extrusion operations directly on these
many photographs to produce a
photorealistic lightweight 3d model we
have a second branch of this workflow
which says what happens if I don't want
to sketch on these photographs because I
already have a 3d model available
someone already made let's say Sketchup
model of the scene so I can use these
same photographs to automatically text
map onto them and that case the input is
not just the photos and the camera
parameters but also 3d model as derived
from Maya you know 3d studio max or
Sketchup or whatever else is your
favorite to a modeling tool okay so
let's talk a bit more about multi-view
geometry because that plays a central
role in this so the first step is to
automatically extract salient features
from the images and we use a shift
algorithm which is scale invariant
feature transform it's based on the work
of david lowe and matthew brown and
after we recover these feature points
then we establish correspondence between
them automatically from one image to the
next and we can begin to initialize the
camera pose and structure from just the
first two images alone and then we can
extend that and keep tracking these
features from one image to the next to
recover all the remaining camera pose
and structure
and we use bundle adjustment as a key
component to solve for this and
redistribute the global error so here's
an example of features as you can see on
the right image you see that the little
plus signs there those represent the
positions of features that were
recovered using sift and these are
actually two different images of the
base of the Empire State Building and
these lines here represent the
connection between feature found in one
image and its corresponding feature in
the second image so you can see that
there's a clear path that you know that
that is displayed here that is match
from one image to the next so the
general approach that's used here is we
start out with frame 0 K represents the
intrinsic parameters namely the focal
length other information about central
position and initially we assume that
that's our frame of reference so there
is no rotation no translation so we use
an identity matrix rotation 0 for
translation and for each feature points
found in this image we find the
corresponding feature point in the
second image by establishing you know
which vector I should say that every
feature point that's found is associated
with a vector so in order to find the
best match in the second image we find a
vector in the second image which is
closest to the first vector that we're
trying to match with so we just do a
search on all the vectors of the second
image and ask which one is closest to
the current vector for this particular
feature in this image so we can
establish a match based on the proximity
of these two vectors and that allows us
to connect let's say this point to that
point that's that's a match and this
point to that point etc and for these
then we can go ahead and recover the
unknown rotation translation of the
second camera visa via the first camera
by doing a QR decomposition on the
essential matrix so you know initially
we have a fundamental matrix with
fundamental matrix we can get an
essential matrix do decomposition on the
essential matrix to recover the unknown
rotation and translation that best
matches this camera with the reference
frame of this camera
okay and now that we have that that
means that we can triangulate and you
know establish points out there in 3d
and this is what allows us to get the
point cloud the sparse point clouds that
we recover from an array of photographs
and and their basic basically two sides
of the same coin you know on the one
hand we are trying to recover these
unknown rotation and translation
parameters for each camera because
that's often what we're interested in
the pose but in doing so we do need to
come up with these 3d points because
they are both the flip side of the same
coin so once we have these 3d points
let's say this one which is
correspondence with this one corresponds
to this 3d point well imagine that this
point is then matched up to that point
we know that this point should also be
correspond to the same 3d point so that
means that for all the remaining images
we can compute the unknown rotation
translations using a six-point ransack
the reason it's six point is because
there's actually 12 parameters that are
unknown right it's a three by three
matrix which is nine plus three more for
translation which is 12 and and we're
doing this matching on 2d points so we
need six 2d points to solve for those 12
unknown parameters and we can thereby
recover the unknown rotations and
translations for all the remaining
cameras and so that's the gist of the
multi-vue geometry that allows us to
recover those camera poses okay so what
we are doing is using the global bundle
adjustment to distribute the error of
the camera poses and structure and we're
minimizing this this route projection
error ok so capital P is the projection
matrix that projects 3d point to the 2d
image Capital m is the 3d point and we
are saying that you know for all the 3d
points that get projected back to the 2d
image we try to minimize the distance D
it's a Euclidean distance measure
between the 2d feature point and the
projected 3d point back on that image
and so we minimize this reprojection
error and you know basically this is
what we end up with in the end you know
after all these photographs are taken
and each photograph that we took of this
building is
shown on this frustum and to what we
recovered in all this are these white
dots which represents the camera
position and the red green and blue axis
here represents the orientation the
frame of reference of that of those
cameras so the orientation of those
cameras as well so notice that we not
only get back all these white dots and
the orientation information but also a
sparse 3d point cloud and remember that
every point in that sparse 3d point
cloud represents a feature from that 2d
image so if we found the thousand
features in the 2d image would be a
thousand points in the point cloud that
represented wide base line this example
shows it on video which is small base
line because you know the distance from
one frame to the next of a video is very
small so initially the starting point
here is okay I find sift features in
there and then track them across time on
this video okay so yeah outliers right
so the question is what do you do about
outliers features whose correspondences
are incorrect that is the purpose of
ransack so ransack well I'm not going to
go back to that slide but we use a
six-point ransack which means you know
even when there are outliers there we
will find the in lies we will focus will
focus on that because it's many
iterations in the in the ransack so as
long as the as long as you're not
dominated by ever you will find the
right results applying a ransack
algorithm so in this case a video you
could see the individual frames of the
individual camera positions and
orientations that were recovered as well
as the sparse point cloud through that
video sequence here I want to show
another example of wide base line input
images so here is like a little
synthetic example of a building and
three different snapshots of it okay so
from those different snapshots of it we
recover the camera positions of that
building as well as the sparse point
cloud of it
okay so that represents the the results
of multi new geometry the ability to
recapture the camera positions and
sparse point cloud from an array of
photographs that don't have to be
calibrated in other words a camera that
took those photographs don't have to be
calibrated so the question is well if we
have these photographs and let's say we
had a 3d model of that scene and that's
what's shown here on the left this is a
Sketchup model of the Cooper Union
Building this by the way is probably one
of the better Sketchup models on 3d
warehouse of buildings in Manhattan so
you know you have many famous buildings
in Manhattan but ironically that one is
probably the best one because it was put
together by john backus who was not only
an alumni from Cooper Union but also
product manager of Sketchup so so he you
know he put a lot of effort into that
building so we walked around that
building and we took a lot of
photographs of it and we recover the
camera positions and orientations and
automatically project that onto the
Sketchup model to produce that result
there so again this multi DG ama tree
plays a central role and what it really
is doing is automating the camera
calibration task which traditionally has
been very tedious and and error-prone
and in fact even on photo match which is
you know recent addition to the sketchup
tool the user has to manually you know
with handles pick out the vanishing
lines okay that's something that the
user should not have to do right now
they have to do it because the
assumption is that the input is only one
image but why assume that the input is
only one image when you could just as
easily take many images and when you
take many images you can automatically
recover this information and besides you
want many images because you generally
want to a photo texture all around the
building not just one side of it so this
is currently not something that's well
built into Sketchup that could that
could be better done if this technology
is there to complement what sketchup is
currently doing okay so so I want to
talk about this branch of the workflow
whereby the top branch here is the multi
new geometry we said before and this one
here is the automatic texture mapping so
this is a high-level overview
that automatic texture mapping in fact
it's very similar to a slide I showed
last year when I spoke about multi-view
geometry for texture mapping and in that
slide the bottom row here represented
many range scans that will automatically
stitch together to form a 3d model so
that's easily replaced by the 3d model
itself if you have it in me a Sketchup
model so at the very top you see the
collection of 2d images and at the very
bottom you see the other source of input
which is a 3d model now it's it's
trivial to go from a 3d model to 3d
edges to do edge extraction on that
model and then furthermore we can take
this these 3d edges and these 2d images
and have them go through a 2d image to
3d model registration so some of that
work is best described in a 2005 cvpr
paper by professors thomas and ling
young liu and lincoln is now here at
working at Google and basically what
that does is says that from all these
images there are sub set of images from
which you can extract 2d lines and match
them to the 3d lines here so that you
can recover the camera positions
directly from that in other words not
from all the images alone but from a
combination of the images and the 3d
lines okay then on the other hand you
could also say I will take all these
images and produce a sparse point cloud
plus dense pose information and then we
could bring these two together so they
form as you know you basically align
with a sparse point cloud here with the
dense information here and when you
bring them together directly into the
same reference frame well you don't have
the right scale or anything like that
but they are unregistered but then you
can go through a 3d model to 3d
structure from motion registration so
that this now conforms to the same scale
and position and orientation as this the
purpose of this step is so that we can
bring in all of these dense camera poses
into play here see right now we are
limited by whatever features we can find
from 2d and matching them to 3d and
that's generally is a small subset and
it may not represent all the views that
you want you may not be able to leverage
the fact that you had all these images
but you can only use a subset of them so
what we're doing is using that
information together with this
information to bring them together so
that you can truly leverage all the
photographs available to you and you can
project them all onto your 3d model so
this serves in a sense like a proxy in
which you could in which basically
allows you to hook in and bring in this
dense information which is what you want
into this frame of reference and you
could project yes now we're not trying
to refine the model what we're trying to
do is is bringing all these cam imposes
into the same reference frame as a 3d
model we can't we have no way of
correcting the 3d we're not trying to
correct the 3d model
that's true it's true that there are
interesting opportunities with
displacement mapping there but we could
talk more about that offline okay so
what so what you have here is the
ability then to bring in all those
photographs into the same reference
frame as your 3d model and then project
them okay so the first step is to do 3d
line extraction from the model and then
cluster the 3d lines to find the 3d
major directions and then we do face
extraction and extracting rectangular
parallelepiped an important element of
this work is this is this right here the
clustering to find a 3d major directions
which right now is being done manually
you know when you look at the photo
match featuring in Sketchup I mean you
know someone's going in there and
establishing the major directions by it
by hand okay and picking out those
vanishing lines so all we're saying is
that you know if you just simply allow
for more images to be used here you can
do this automatically and this tends to
be a very tedious and error-prone
process if you have to do it manually
likewise you can extract the 2d lines
from the images and extract the
vanishing points directly so that you
can recover the rotations between the
cameras and the 3d scene so again the
point here the take-home messages use
many images multivu geometry to recover
the camera pose and orientation
information and thereby we can take
those images and project them onto the
3d model and you know here are examples
of that from different viewpoints okay
now that brings us to another well the
second half of the talk i should say
whereby let's assume that we don't have
a 3d model available let's say you have
to make it yourself so we can call this
part the semi automatic approach whereas
the previous part was automatic this is
semi-automatic the reason is
semi-automatic is
camera calibration remains automatic but
the act of laying down the footprints
and doing extrusion operations a las
ketchup is the manual part so between
the automatic camera calibration and the
manual sketching you have this semi
automatic approach so the problem here
is to provide users with a tool to
create lightweight photo textured 3d
models and what we're creating a tool
that basically merges the simplicity of
Google Sketchup with the automatic
camera calibration of Microsoft
Photosynth now I only bring that up not
because Microsoft invented automatic
camera calibration of course they didn't
but it you know if Google and Microsoft
were to collaborate on any project and
this is perhaps one of the outcomes you
know and that's why we call it photo
sketch because photosynth you know the
automatic camera calibration well you're
using photographs to get that and the
sketch part is Sketchup so photo sketch
is really the merging of two different
capabilities the automatic camera
calibration on the one hand which you do
not find in Sketchup with the simplicity
of the tool that is sketchup and bring
the two together okay so photo sketch is
a photo base 3d modeling tool and its
features include the automatic
calibration to recover camera pose from
photographs the easy to use sketching
interface intuitive push-pull extrusion
operations a rich set of tapering
operations for instance we could taper
to a point align arbitrary curves that's
not currently done in Sketchup it's a
little bit clergy to get that same
results like the taper to a point
something as simple as that but you know
we have that and we use the photograph
photographs to guide the process they're
like drawing stencils and we also use
dynamic texture mapping to give a visual
cue if you've done the extrusion
operations and tapering operations
properly okay so that represents this
branch of the workflow ok so the
technologies is for them here basically
multi-view geometry sketching a
footprint push-pull interface dynamic
texture mapping and one important
element that we recover from multi-view
geometry is they is the ground plan
which is necessary upon which to lay the
footprint that is why in
you know with photo/matt you have to
specify the the vanishing lines manually
so that you can know what that ground
plane is but we could do it
automatically from the multiple
photographs so here's a little demo of
that here we go ahead and we say let's
import several photographs so in this
case we pick out five photographs that
we want to bring in okay these this is
from a synthetic case we basically show
but I'll explain why it's synthetic in a
moment we basically show you know the
five different views there the reason I
want to show this instead of cases
because it makes it really obvious when
you make a mistake in the extrusion
operation with these textures okay so
camera calibration is done which means
that the features from one image of the
next are found you can go in there by
hand if you want and delete outliers
that that were not deleted already or
you can insert new in liars if you have
to but generally that's not necessary so
we're just viewing this that you know
that things are done correctly so you
see that we recover the camera positions
and the sparse point cloud of that model
directly okay so those are the counter
positions of the of the five different
images and the point cloud so now we go
in there and we lay down the the
footprint so we click and we get a
magnifier window so we could clearly see
where we're clicking and because we will
recover the ground plane you could see
that moving the mouse now is moving you
knows creating a rectangle properly on
the ground plane and then we do the
extrusion operation and there's dynamic
texture mapping going on so that you
could see visually whether you've gone
too far in lifting you know in doing
this extrusion so the images actually
help guide this process not only do the
images help guide it but also the point
cloud itself because when you lay down
the footprint you may take a top view of
the point cloud to see are the clusters
of dots around the edge of the building
are they properly aligned with the
footprint so so here from the top then
what we could do is we can insert a new
line and that would represent the ridge
of the roof that we then have to pull up
so we could taper to a line and this
snaps to the centre of the edge there so
we don't
to be precise we could snap near the
center and then we could pull up what
you see it on the other view as well if
you pull up too high you can see that
the wrong texture is applied there so
the act of projecting images on to this
while you're doing this extrusion helps
guide the extrusion operation to visual
cue
okay so so that represents like how
simple it is to take the images
automatically recover the ground plane
and then and then just start drawing you
know footprints and extrusion operations
and the benefits of dynamic texture
mapping as a cue just to go in more
detail about the sketching of the
footprint again you click on the corner
move it to the other corner click again
you see a magnified window move the
mouse in the other direction and it's
automatically aligned to the ground
plane of course from that view from the
upper left you would hard to tell but
from the upper right view you could see
where you should end and even from here
the top of you on the bottom right you
could see that the point clouds do you
know serve as a visual cue where you
should be limiting the extent of the
ground of the footprint on the ground
plane and then there's the push-pull
interface so after the footprint is laid
out you pull it up and you know then you
stop them in and then you place another
line at the top so you select that face
you click on one edge and the other edge
and you lay down a line which you can
then pull up to create a roof
and at that point then you could see
that you have the recovered information
about where you were standing when you
took the pictures alongside the 3d model
and here's a simple way of adding a
chimney to that so what the first thing
you have to do is is turn off some of
the faces so that when you poke when you
draw a circle and you pull it up it's
being pulled up with respect to the
ground plane and not the side of the
roof which would be making the chimney
come out at an angle so that's a feature
I don't I don't think it's there in
Sketchup you know to turn off facets so
that you could begin to draw it from
somewhere else and and pull up
perpendicular to another plane okay so
when you turn off the texture it's clear
that you could do that
okay so we can apply this also to
airborne imagery so we have let's see a
video taken from a helicopter flying
over Manhattan and this represents three
different views from that video so we
can recover the position and orientation
of the camera that was on the helicopter
and this represents Madison Square
Garden and penn plaza area in manhattan
so you get the you know the sparse point
cloud or the buildings there you get the
camera positions and then it's a simple
matter to lay down you know the circle
that represents Madison Square Garden
and the other boxes that represent penn
plaza and do extrusion operations and
then the the point clouds themselves
help to just refine how far up you go
when you do that extrusion because you
could use the point cloud and the
photographs from different angles to
serve as a guide for how far you should
go when you do those operations so
here's another example of a Street View
imagery so here are some buildings and
you know you could see that it's a
lightweight because you could see how
few polygons are to model this but you
know this took just a matter of a few
minutes to draw you know the footprint
extrude and do that in some of the
buildings here in the corner so you know
this represents lightweight photo
textured models that are suitable for
google earth so as a point of comparison
um you know there are disadvantages of
existing photogrammetry solutions and
the only reason I mentioned
photogrammetry here is because it does
share the same goal of deriving
measurements from photographs but
photogrammetry is has traditionally been
manual I mean if you look at the work of
you know in real vis photo modeler or
image modeler from eos you know these
are very manual workflows so the idea
here is to automate that especially by
having a camera calibration done through
the use of multi-view geometry so the
traditional approaches are very intently
labor-intensive and it's difficult to
scale up for for many images and also
modeling is done by inserting 3d
primitives generally the rec
which is rather cumbersome and it's it's
it's just a difficult process there are
some other techniques that are based on
silhouettes but they are limited to
small objects on a turntable so that
obviously wouldn't apply for the large
buildings that we are after here so the
solution we have here is its photo
sketch which has fully automatic camera
calibration modeling is based on the
virtues of Sketchup which involves 2d
sketches and a set of extrusion
operations and taper operations which
are guided by the photographs so I think
that this represents the status of where
we are now okay the top part here the
multi be geometry is completed the photo
sketch part we have to extend the UI so
we're more comparable to sketchup and
this part here the automatic texture
mapping is well suited for a plugin for
other programs like maya or 3d studio
max or sketchup itself so the conclusion
here is that we've demonstrated an
automated way to photo text your
existing 3d models we've developed a
semi-automatic tool to create
lightweight photo textured 3d models
from scratch and we've leveraged the
virtues of basically two different
approaches namely the google sketchup
for building models largely by hand by
sketching and the automatic automated
camera calibration that you find in in
microsoft Photosynth and so when you
bring these two elements together you
have the virtues of photo sketch so that
that concludes my talk I'd be happy to
entertain questions at this point
yes so you demonstrated how to match
aerial images to airborne models and
what happens if they are not the right
size save one mile your building is too
big or too small something like that how
does the match work then yeah that's a
good question let me repeat the question
what happens if your 3d model is just
plain wrong you know in other words
we're taking photographs that are actual
and trying to project them onto 3d
models which you hope are right but if
people make mistakes making those 3d
models how are these features going to
line up and that represents a key
problem and we have a solution which I
didn't describe here which sometimes it
which involves a bit of warping
basically you need to warp the
photographs to best match the model the
alternative would have been better to
basically refine the model to best match
the photographs but we can't go in and
take a ready-made Sketchup model and try
to you know reverse-engineer it saying
oh you should have added this or change
that so it's easiest to go ahead and
just take your photograph and warp it
the best match for your model and that's
the tool I haven't spoken about that
that still work in progress yeah I had
like a related question about you seem
to be matching 2d features on the photos
with 3d features extracted from the 3d
model but even if the 3d model is
accurate like it might not have like
very many edge features to match against
like your matching features in texture
with features in geometry and you might
not have enough edge features and like a
quartz 3d model that you might create
like is there like how many does it does
this approach require like a fairly
fairly accurate 3d model and even in
terms of geometric features well like I
said if you don't have an accurate 3d
model then you have to go and spend more
effort with warping the image
which is a simple matter of saying okay
here are the lines that are extracted
from the 2d images and here are the
edges that I extract from your model
even if your model is wrong poor but I
can establish line correspondences
between these lines and those lines
right between the two damage in the 3d
model and and then you could just do a
warp on on the images to best match
those models so you're establishing
correspondence by hand in essence went
just to compensate for the fact that you
have an inadequate 3d model well I guess
I'm just considering a case where it is
an accurate geometric model but it's a
very simple model like let's say simple
building rectangular building block
right and it's fairly accurate it's just
that you have very few edge features in
geometry but guard of line features in
texture now you will have you will have
it line features in the 3d geometry if
we are making a Sketchup model yeah but
a few not too many it is like if you
have you don't need to make you don't
need too many because you're basically
taking this image and applying it as a
you know face onto the facade of a
building so you know you just need the
edges you know you don't need all the
edges in between along the facade if
your model is too simplistic then it's
just a simple case of taking the image
in like four corner to four corner
mapping you know it's you don't need
that many correspondences of course it's
best when you do have them but if you
don't then it becomes then this resorts
to how simple your model isn't and and
in essence you will have perhaps some
texture from the image that's mapping
onto a flat wall even though there
should have been more detail there that
you didn't incorporate oh thank you yes
can you elaborate a little bit on the on
the stage where you actually have a 3d
model and you are matching the you're
trying to texture map it do you use
something else then so you alluded to a
way how to basically map and texture map
rectangular regions which are
corresponds to facades and that I can
imagine that you can reliably match
those and and texture map but what if
you have some very small extrusion and
intrusion on the 3d model which are very
hard to make the correspondences with
the images and do you use other texture
like primitives than other than
rectangles because you just a little bit
to the stage of detecting lines which
are orthogonal and instantiating some
rectangular regions so are these all the
primitives you use well all that work is
basically towards the problem of finding
the camera poses that you can then
project onto the 3d model so once you
find where the cameras are it's it's a
projection that's going on on to your
existing 3d model now if if this process
is not done right then you might be
projecting you know the wrong part of
the image onto the 3d model I'm more
actually trying to understand the part
of what is the model your texture
mapping so you can have a triangulated
mesh which is texture mapped but if you
already have a 3d model which is fairly
simple then what are the individual
primitives of that 3d model which are
being texture mapped right what we're
doing is we're segmenting the 3d model
into planes and then the 3d features are
the edges of these planes or at least
where these planes intersect okay so yes
you may have some triangulated mesh but
we are segmenting this by pipe so the
models which you assume is sort of set
of planes which are somehow aligned with
the rectilinear
world right in that system right oh um
in photo sketch couldn't you make the
interaction have kind of a snapping
behavior if you've got a point cloud
with a dense that appoints on the wall
of a building then you could help the
user by having a line or a plane kind of
snap to that set of points right I agree
with you that that's a very important
element in and that's where I get back
to the point of extending the UI the
snapping interaction is one of those
elements so yes you're absolutely right
and then you know if you're pulling up
the the peak of the roof you could have
it snap at the point where right now
we're just using visual cues to assist
but you're right that the snapping
should happen also you know by having
these the sparse point cloud pull these
faces into place and the tapering lines
is willing to place so that snapping
behavior as part of the extension of the
UI that we're working on so couldn't you
in the automatic texture mapping version
of this couldn't you also find places
where there are errors in the
registration and somewhat like as Lance
was suggesting perform edits to the 3d
model to you know like displacements
well well not necessarily implement it
as displacement mapping you could just
alter the polygonum model but right well
we haven't we haven't done work on
altering the polygonal model because we
weren't sure who created that model like
if it's Sketchup then you know it's a
different series of instructions it
sounds like a script that says you know
pull this up pulled that out we you know
so we weren't going to edit that
sequence of instructions even though
that would be the most compact way of
editing that model if we could so then
we would have to go and just deal with
it like generic you know triangulated
mesh model and you're right we would
have to it would be best to edit the
model rather than warp
images right now we just warped images
because that's the more the simplest
thing to do right now but that's not the
optimal solution the optimal solution is
to edit the 3d model and that's a
somewhat trickier problem but it should
be added there yes now is created with I
think there were five by photographs yes
I'm just wondering so we start with a
lot of photos and then you create the
model and then recover the where you
took the photos so presumably can also
project back and render new photos so if
you increase the process with a stable
after one move or you can have some sort
of iterative improvements going on let
me let me understand I it so you want to
create not new images views is that what
you're saying um from where you um found
right yeah right okay that means that
yes if you have the photo textured model
then you could move your synthetic
camera to you could you could go to any
other position and project from the
photo text remodel on to new positions
we haven't done any analysis on that
problem you know but we could we could
just as well render it from any new
viewpoint now but it would be yeah it
would be no problem it would be pretty
direct to do that no so I'm just
wondering you do that it's like um egg
become chicken becomes egg again and
then become chicken again no egg again
and then and then you iterate the
process so it's a stabilized everyone
move or it would it be some sort of
convergence we have we haven't looked at
we haven't done that you know in
analysis to see but that would be
worthwhile you're just saying what
happens if you go back and use different
set of views as in putting then if you
go back to the original vantage points
do you get back what you started out
with but we haven't done that that would
be worthwhile to try okay thanks ru
merging actually the texture for
multiple images for one phase or are you
just picking one and you hope that there
is no occlusion going on on a
pixel-by-pixel basis we are not merging
multiple pixels because then we would
have more blurring so we just find the
best candidate to choose the best
candidate photograph to use to paint
that one particular pixel based on on
viewpoint based on angle in other words
the closest image may not be the best
image to use because it might be at a
grazing angle to that particular point
so we use the one that's coming out to
perpendicular from the plane and we use
one
pixel from the photograph as opposed to
multiple so we don't have blurring
issues forum is aching we're mosaicing
from multiple in fact there are
sometimes problems and that's something
I didn't address here image editing is a
big issue what happens if your textures
have like people walking by in some
images not in others or as we had some
times you know buses were in the way on
some images and then they were not in
others so there is need in the extension
of the UI to also do data redaction data
redaction or or I would call it
diminished reality as opposed to
augmented reality where basically you're
trying to take out unwanted artifacts
from the image because something that's
secluded from this vantage point may not
be included from another one so we need
to do more image editing work to come up
with cleaner textures that don't have
these occlusion artifacts I think it's
worth mentioning that in kinoma paul de
Becque adopted a different resolution
which was in rendering the model to
choose the texture map that's closest to
the view angle of the observer so that
actually handles the occlusions you'll
see the columns from one side in one
view and from the other side of another
view given enough textures a good point
however features to find is it just by
sharp contrast in colors or some other
way well the 2d features are defined by
sifts which is scale and varying feature
transform and you know it's basically
features that stand out across a wide
range of scales and they're rotationally
invariant as well you know in the past
before see if there were other methods
like Harris corner detectors that were
used which you know looked for sharp
corners but sift works well and it gives
us a large vector that we can use to do
a good match with features found in the
subsequent images to each feature is
represented by a vector and then it's
just a matter of finding
corresponding vectors that have minimal
distance error and our features of like
a specific pixel number of pixels or are
they like sizable we're all ages single
points now well there because its scale
invariant I mean we have to look at if
you know the features across many scales
so features are represented by the fact
that they are persistent across many
scale so that encompasses the fact that
there's a certain neighborhood upon
which this feature really stands out so
I would refer you to the paper by david
lowe and matthew brown i think it's a
2004 paper that talks about the virtues
of sift yeah okay thanks so you
mentioned that you choose the best pixel
for each phase so do you do any blending
of the texture so if I have half of the
face they can for coming from a
photograph taken on a sunny day and the
other half on a cloudy day then you're
going to have some contrast there okay
that's a good point about the you know
when are these photographs taken we
generally want to take them all within a
span of a minute or two so that they all
have the same lighting condition
otherwise it's a really big problem
having to deal with photographs that are
taken under diverse lighting and in fact
that was one of the motivations early on
when I showed you that the neo-gothic
building and I showed you the photograph
so we're taking around it you might ask
why did we even have to take photographs
of that if there's a built-in camera in
the laser range scanner but because it
took us 15 minutes per scan and we had
many scans it took us the large part of
a day just to collect that 3d model and
obviously the lighting would change
drastically so we had to walk around
that building you know within like a
minute or two so that the lighting stays
constant and that's an important
consideration so if you were to use
photographs of photographs from an
external source they would have to kind
of be taken at the same time so would
you would assume that they would have to
be taken at the same time or you'd have
to have some very sophisticated way of
dealing with lighting changes too yeah
actually if you if you use in light
laser scanner it's not sense it it's not
that sensitive to the to lighting
condition I think but if not use the
images then probably if you if your
browser is based on a white base line
probably orgasm radiometric programs
here asked so I just curious well what
kind of accuracy could could could this
this matter get and so yeah and how is
comparable to like a result from laser
scanners okay first comment about your
point about laser scanners not being
sensitive to to light to sunlight that's
true for time-of-flight but not for
phase base that's why phase based
scanners are always used indoors and as
far as accuracy goes it's clear that
that active range scanning is higher
accuracy than what you would get from
photographs but our motivation for doing
this is that the average user does not
have a laser scanner at home you know
that cost over a hundred thousand
dollars so everyone has cameras and the
question is how do you know how do you
get consumers to just walk around
structures taking pictures and create
their own models and you know that was
the whole thrust of I guess why google
purchased at last offer you know which
is sketchup because they wanted to
populate Google Earth maps with these
lightweight models that are built using
Sketchup and we want to complement that
further by using photographs to help
this photo texturing of these models so
it's not not because these are any more
accurate than the lasers surely they're
not but they are lightweight and you
don't need that level of accuracy for
these web-based applications another
question about the cloudy day and
saturday so we'll see detect the the
same features from the two photographs
one take another sunny day and the other
on a cloudy day yeah sift will detect
the features some of the features will
be outliers because they may be due to
you know the shadows or you know these
things that change but you know we are
using a ransack algorithm to help find
the best correspondences even in the
presence of these outliers so as long as
these outliers don't dominate your your
scene then ransack will find the right
result
after enough iterations oh thanks so so
the ransacked you you just pique the six
best points or something no no we pick
six here we try to find six here and we
keep it's kind of like a boot for sit
sit sit er ative finding six and six and
you know it's actually the 25th
anniversary of the introduction of brand
sex it was nineteen eighty two or maybe
1981 and there was a venue at CVP our
last year which talked about the
advances of ransack but I would direct
you to how ransack works it's you know
those papers on how it works but it's
basically picking out six you know it's
random sampling and consensus so you
randomly sample six points here six
points a try to do the best match and
and with enough iterations the the
outliers will be thrown out you know
from the consideration to find the
unknown rotations and translations thank
you very much right right the other
question is so what's the ratio between
good and bad features because that
affects how many times you have to have
the iterations you have to have in the
ransacked method i believe it was like
one quarter or so you know it's a or
thirty percent that were outliers but
still you know ransack finds the right
six points as you could see the result i
no further questions I'd like to thank
George for speaking today and invite
anyone to speak to any further</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>