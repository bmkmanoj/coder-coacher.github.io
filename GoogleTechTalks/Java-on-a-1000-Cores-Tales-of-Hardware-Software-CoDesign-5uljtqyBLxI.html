<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java on a 1000 Cores - Tales of Hardware / Software CoDesign | Coder Coacher - Coaching Coders</title><meta content="Java on a 1000 Cores - Tales of Hardware / Software CoDesign - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java on a 1000 Cores - Tales of Hardware / Software CoDesign</b></h2><h5 class="post__date">2009-09-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5uljtqyBLxI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's my pleasure to welcome today to
Google dr. cliff click he's the chief
JVM architected Azul systems he's one of
the lead arc and he was the lead
architect on hotspot server compiler
back in the days when he used to work at
sun and he's here today to talk to us
about azula's experience with lots and
lots of cores and hardware software code
design so here he is Thanks hi sojung
Pytel from slides you're getting
recycled slides from eek ook eek oop
keynote but I think they're very
interesting it's a very interesting talk
here so so a quick look at what Azul is
and what we do and sort of what we've
done we design our own chips they're
actually fab by Taiwan silicon
manufacturing company we build our own
systems out of those chips we target the
systems for running business Java we
have a very large core count on 54 cores
per die we can make up to 16 of the dye
cache coherent so the big box has 864
cores in it we have a very weak memory
model but it meets the Java spec when we
put in the right set of memory fences
the memory speed is flat it's uniform it
is not Numa which usually expected of a
hike or count machine and that's because
business Java is very irregular
computation so there's a no sane layout
you can do we can get some your memory
you know the local memory close to you
or pick some chunk of memory you know is
going to be local and instead we just
gave up on that and said ok it's all
medium speed there's no fast no slow we
do have a lot of bandwidth we hit the
we're still on the top 20 list for super
computer bandwidth couple years later
the CPUs themselves have a fairly modest
per CPU cache is the button aggregate
there's a lot of cash on the die 16 ki
cash and D cash each x 54 cores and then
every set of nine cores share a 2 mega
l2 or six those on the dice those 12
megs of l2 it's an aggregate there's a
lot of cash from the dye CP let's go
there goes and I went too fast too far
let me back up here yeah ok so um
i I didn't yeah I'm trying to her what
button I pressed that made it go there
fine ok so the core is our classic 3
address risks they're very simple they
clock slower than an x86 I'm a part of
that's just because there's a lot less
area consumed 4 cores no replication
parts everything has to go through a
very simple pipe we can sustain a couple
of cash missing ops on each core and
each l2 can sustain a number of pre
fetches so an aggregate across the
entire box you can have well over 2,000
outstanding memory references in flight
at once and that's not hard to achieve
on the box and that's pretty common and
that will not cause anyone to slow down
we have bandwidth to spare to do that
we've hardware transactional memory
support a bunch of special ops to make
java's life better including reading
right berries for GC a bunch of the
things that are also to nickel and dime
change make Java run faster and we're
really we're targeting thread level
parallelism in a managed runtime kind of
system that's sort of the goal target
there so when we look back in time when
his goal was started the job of placing
COBOL a lot of that came out of sort of
y2k driving people realized at the time
of the y2k you know non disaster that
they had to replace all the cobalt code
and their COBOL programmers were
literally dying of old age so these big
businesses decided to flip to Java I
don't know why and but they did and
suddenly there was millions and millions
of lines of Java code being written on
an annual basis and a lot of it had
thread level parallelism built into
application servers beans you know
weblogic websphere jboss those kind of
things so it's all task level
transaction level parallelism thread
pools work lists that kind of stuff and
you know in conjunction with that people
are hitting the power wall you can't add
more power you can't clock faster
because that consumes more power more
power in your typical melts so clock
frequencies are topping out people
predict in there going to more
transistors there's no point adding more
cash you can't clock faster so you're
going to get more cores and we
fast-forward 2009 in the fact clock
rates of stall the gamers get three and
a half gigahertz the cheaper guys get
two and a half gigahertz but everyone's
got four cores that's sort of a lower
bound now
can get us so obvious synergy between
where the motor back then and what was
obviously coming to run all these
transactions on separate course so you
want to have a multi-threaded multi-core
machine right so that was what we were
thinking when we started designing the
box but the only had to look at it and
say well really who buys custom hardware
most I like a really really good reason
to buy custom hardware for the obvious
x86 solution yea merrily you know 5x
price performance kind of gains not
really good enough to make people jump
ship there so you had to do something
very special so what else can we do
besides more cores well big business
apps were already pushing memory limits
of what you could squeeze into a box so
that was the whole 32-bit versus 64-bit
heap so we said okay we're just going to
jump straight to 64-bit heaps not bother
with 32 bits why we're gonna have lots
of thread support lots of thread count
support GC what can we do in GC well the
notion of using read berries for GC he's
been around for like over 20 years you
look like the old symbolic skies they
did this kind of game long long long ago
and so we said okay we can do this in
Hardware it's a tiny bit of extra
hardware we also asked you can we have
the extra tag button hardware to have
pointers and hard worked as a tagged
thing and the hardware guys jumped up
down right away and said no no no no no
must have commodity d Rams must be 64
bit d Rams cannot have a fifth 65th bit
there okay fine transactional memory is
a hot topic hardware support for
transactional memories of hot topics all
over the academic literature is going to
save us from our complex locking schemes
we all know that complex locking schemes
are a big problem we don't know how they
will work no one understands where they
missed the atomic so they throw
synchronized keywords everywhere and
effort to save themselves no one wants
to actually rewrite all their
applications using some sort of new
language construct until there's like a
knob it's obviously going to work and
there's an obvious momentum of people in
the community producing both the
programs and the implementations that
hasn't happened this is still an open
research problem how to implement the
sky so instead we're using hardware
transactional memory to support la
collision done an accommodation of
hardware and software so Java la
collision that's what we're aiming for
that hardware a couple other things we
can do we know that walking is going to
be an issue but most locks are not
contended as
in 9 million percent case so you need an
atomic operation of cos to go take a
walk in the first place but usually it's
never contended so we wanted uncontained
calves to be fast so we arranged our
Hardware memory models such that an
uncontained cask and hid in your l1
cache and it runs at an l1 cache speed
so Kaz can hit in cash so one clock
pipeline same thing for a memory fence
if you don't have any outstanding memory
references that aren't going to be
resolvable and there's no contention for
memory you can do a memory fence a
proper java memory model ordering in a
clock pipeline right turns out that Kaz
as implemented by every other cpu on the
planet includes a memory fence and that
memory fence is not the correct spot for
how hot spot does lock so hot spot emits
a second fence on top of the cast as
part of the standard cost of taking a
java lock it's kind of a painful thing
so we have a no offense cast we turned
out we found a bunch of other
interesting use cases for this atomic
updated performance counters bunch of
lock free agha rhythms which don't need
ordering so we use this no offense Cavs
all over the place and it makes us sort
of run faster than this highly contended
highly concurrent code we do have fences
for four so when we need it there's not
otherwise much ordering it's a much
weaker memory model than say x86 Azure
sparks TSO we we allow all kinds of
reordering set these other memory models
don't allow and we rely on fences to get
the answer correct for which the jet run
running java code obviously does the
right thing and so you get the job or
memory mom okay so we've had hardware
transactional memory support in from day
one I must a little bit talk about what
goes on with this there are extra
opcodes the speculate commit and abort
opcodes and there's extra tag bits in
your l1 cache and there's nothing at all
in your l2 about how the HTM works the
hardware guys are very clear on that you
cannot they the l2 is what int'l calls
to LLC the last level cache is the point
of coherency between the hundreds and
hundreds of CPUs and system that is
already one of the most complicated
parts of the system there's no freaking
way you're going to go in there and add
some new weird-ass thing that no one
knows what's going to help or not so
nothing on the l2 on hardware
transactional memory it's all done in
the l1 level
so when you start an in a transaction
whether it succeeds or fails or whatever
it does to l2 is utterly unaware that
you're doing this all right okay so you
have extra time bit scenario one you
turn on the speculate mode with the
speculate opcode every time you don't
read or write it tessa sets a little bit
and the l1 cache saying this line has
been read speculatively or written
spectively and then if you ever lose a
line when these tag lines out of your l1
for whatever reason you abort and if you
abort you'll lose them all we just take
all the spec lines and throw them out of
the cash so all those rights you
accumulated in your you know l1 cache
they all get thrown out and then you go
to a software recovery mechanism which
essentially decides to retry the
transaction or switch to heavyweight
lock or do something whatever do
whatever it's going to do to recover the
situation since you're doing java level
walking it's entirely plausible to do
software recovery where you just retry
the lock a few times and see if you
can't get the transaction to float
through we routinely see transactions on
the order of thousands of instructions
long and it turns out that this is not
helpful in practice I have another talk
on why HTM isn't going to save the world
but the short answer is there's no dusty
deck speed up from la collision we alid
dozens of locks in these big concurrent
programs but never the ones that allow
actually more threads to run
concurrently right and usually because
they have a true data dependency and the
true data dependency prevents all kinds
of transactional memory hardware
software whatever going to do you have a
true data pensee and usually that true
data dependency is due to something
stupid like a performance counter that's
no one's looking at which you add one to
a shared counter it's a true data
dependency and you're stuck you just
can't make that transact so someone has
to hack the code as soon as someone's
willing to hack the code by the way then
all bets are off why hack your code make
it transactional memory friendly when
you could hack it to be fine grain
locking friendly absolutely yeah already
done sin high scale lib and it turns out
that in practice when we get to these
high cpu counts big box systems what
prevents scaling is DC it's not locking
people right they have GC issues that
that they
consume more memory you have a large
enough people that they have GC pause
times that prevent them from trying to
make a bigger system because the pauses
are killing them as it is so instead
they cluster smaller JVMs right oh yeah
contrast this to Sun Niagara Rock
product we do not abort for function
calls or TLB misses or nested locks or a
thousand of the things that rock does we
have a much stronger implementation than
sun's rock processor much weaker than
most academic paper presentations that
never see the light of day but it
doesn't matter it's not going to save
the world okay what other things are
going on here is this time the think
back to 2000 2002 we're designing a
system okay the multi-core obvious risk
is you're going to run out of bandwidth
because each one of these CPUs is going
to those own you know working set of
memory it's going to touch its own set
of cache misses it's got these
relatively small caches going to need to
go to memory a lot you're gonna run out
of bandwidth right you can have fifty
four cores you can have 54 as much
bandwidth requirement well not quite but
something like that so it turns out that
we have a bunch of things we can do we
did what we call a seals the instruction
the just-in-time zeroing in your l1
cache its support for streaming a memory
allocation turns out that if you look at
how standard garbage collectors do
allocation it's this very efficient bump
pointer style allocation but that also
means that you're constantly moving to
new addresses that have never been seen
by this cpu since the last full GC cycle
so long ago those addresses all left or
cash so it's a constant stream of cache
misses you promptly zero that memory by
writing it entirely and then you fill it
with good data and use it for a while
and then most of it dies young in your
cash because the generational assumption
works and most objects die young okay
fine but what it means is you read a
bunch of stuff from memory in order to
touch this new cache line that you
promptly 0 so you didn't actually need
to do the read you don't want that data
you're going to fill the zeros you just
want coherence so we have this just in
time zeroing which is zeros on line in
the cash without actually reading memory
lowers your read bandwidth turns out
that that in practice very measurable
very real thirty percent reduction in
bandwidth that's a big ticket pile of
bandwidth reduction um
we have stack allocation support which
is essentially escape detection as
opposed to there's a lot of literature
about escape analysis this is much more
effective when you do escape detection
you can optimistically assume objects
have stacked lifetimes and you're almost
always right and when you're right the
you don't have to read the data but you
don't have to write it either it stays
in your cash and your recycle the same
memory in your cash and it cuts both to
read and the right traffic out so that's
another nice win on bandwidth we have
this very loose memory model which lets
us scale to big core counsel as things
go out of order it's a one way to you
still have the bandwidth issue but you
can get more time to cover the bandwidth
more Hardware opportunities to reorder
things to cover the bandwidth so an
escape analysis you do a conservative
analysis on the lifetime of an object
and if you can prove it has this nice
stack lifetime then there's an obvious
allocation technique where you allocated
on your stock when your function exits
it's deallocated so it's free to
allocate free to deallocate right great
um but to do so you have to prove that
it doesn't escape your analysis order
and in small programs small too modest
that kind of analysis technique turns
out to work out fairly well there's a
bunch of interesting benchmarks we get
this giant gain you don't go your cash
you quit doing GC cycles I mean quit
leading your cash quit doing TC cycles
everything runs hot in your cash that's
great for the bigger programs the
analysis gets ever more conservative
because the program just gets too big to
do the whole analysis and turns out that
you know IBM did it on these nice big
app stores the one where the people pay
real money for real hardware those
programs get so conservative so quickly
that they get nothing out of escape
analysis adjusts they can't prove
anything remains live for a short
duration escape detection on the other
hand says let's make this thing like as
if it's going to have stack lifetime but
every time you do a store which might
escape a pointer out of your survey test
to see if it's really escaping out of
the stack lifetime so it's totally
optimistic it's totally fix it up after
the fact if something goes wrong there's
no pre analysis and turns out in
practice for these big app servers we
can get between sixty seventy percent of
all objects stack allocated whereas the
good
analyses are like well under twenty
thirty percent they're like down in the
noise kind of range so yes it's a big
difference um yeah okay so yeah well I
have a nice big talk on how that is and
why it works and why it's useful but too
much in this talk already okay so other
things we play here supports for lots of
cash mrs. I mention that before similar
to for the Niagara model instead of
having one uber fast core you have lots
of month of course each independently
you're spewing out cash misses and then
all those cash accesses are mrs. are all
coming back in parallel and so you get a
big bandwidth to memory effectively out
of it but you have to do it by having
all these different threads that are all
running in their own little direction
right so the goal is through poet it's
not single threaded performance we
really built a throughput machine we
have lots of memory controllers there's
four per chip we stripe the memory
accesses across all the chips so you
don't hot spot any individual memory
controller so if you know successively
accessing addresses in memory in actual
hardware implementation those will go to
all the different memory controllers in
a hashed random fashion and then and
then come back to you so the one chip
will light up all your memory
controllers and and so all the chips
internal take I am their their turn
lighting up all the memory control so
really spreads the effort of going to
memory out so that avoids hotspots and
lets you use all the bandwidth you have
and you know one of the observations
here is that there is no saying memory
layout of local and remote of i'm going
to use all this chunk of memory and none
of that chunk so move this chunk close
to me you can't do that business logic
java there's no saying memory layout
since there's no sane layout if it's
just random then you know 15 16 of you
memories on the other memory controls on
the other chip rights remote so then if
you speed up the local case you sped up
the 116th case and like what's the point
of that so at that point we gave it up
and she said even local memories just
loop back off the chip come back on same
cost as go and remote but there's no
gain either no gain no loss it's the
same cost it turns out there are other
uses where you want caches they work
great for your stacks for your young new
gen objects a couple other cases where
you want to cache and for everything
else we just do bandwidth at it
alright what else can we do um short
cache lines you avoid false sharing some
of the bandwidth saber we have these
nice short cache lines also allows more
concurrency because it avoids some false
sharing issues support for faster
virtual calls we went ahead and put
metadata in the pointer so we have a
64-bit pointer we threw it extra few
bits like no everyone else on the planet
influences six the only like the lower
40 42 addressing bits that's already
couple terabytes of memory the upper 20
bits that you see they have all zeros or
all ones we said well we'll put some
data into those bits right the hardware
will strip them out when it goes out to
the address buses but we got data in the
pointers and the metadata pointers we
got some in for the garbage collection
I'll talk about a minute but we can also
use it to speed up virtual calls because
those have to do a predicted subclass
test and we can put the class of an
object in the header and do the test
straight on the object pointer itself
not off the contents of the pointer
right bunch of little stuff on
cooperative self suspension is actually
kind of a big ticket item we're going to
save point thousands of runnable threats
not just thousands of threads where most
of blocked and I owe thousands of
running threads have to come to save
point allow GC so we have some
cooperative self suspension mechanism
well we can halt thousands of running
threads in milliseconds and get them to
all agree that we're all stopped or
start our conversely all started again
okay so I had a lot of fun conversations
through the years with the guys the
harbor guys on how to do all this kind
of stuff right and so routinely I had
you know especially when I started here
I had games where I'd play like gosh i
really wish i had an instruction that it
acts whatever X was and I go to the
hardware guys I can't give me
instruction to do this he said sure I
give you one about three clocks and here
are 3 1 clock instructions that do X and
now all the harbor guys says you know
and now tell me why I really has to be
faster than three cocks is it so fast is
it so common is it so whatever you need
to be three clocks and I'd have to come
back and say well you know no I don't
really need a fast from three clocks it
comes around every few thousand
instructions so one two or three no big
deal so out of that Ken's this really
clean architecture I didn't get what I
wanted a lot of the time but I got a
simple chip which is easy to use and in
exchange I got comments back from them
like we can do is really cool things in
hardware like directly
by codes you know the old hello MIPS was
it MIPS guys who did jython know Giselle
one of these Pico Java thingies and I
was like oh my god don't do that it's
been tried before it's disastrous we're
gonna jet give me a nice jit target nice
three address risk I'll make you pretty
code for it and it'll run really fast
don't try and screw around with
executing bike hose directly because
after I get you won't care for it and in
any case you can't handle all the
bytecode so you'll have to bail out the
software all over anyhow so we can put
in fancy branch target buffer to speed
up virtual calls and again I was like
you know but look at the instruction
traces the number of times I do a jump
register is like next to none because we
had these nice things called inline
caches so we don't do direct register
jumps instead we do inline caches almost
everywhere and all I want a standard
branch prediction hardware don't get me
anything fancy basic stuffs more
important to get right I'd rather have a
five percent bump and clock rate over
any one of these other things those kind
of fun games going back and forth really
the core design philosophy boiled down
to what can we do easier and hardware
that and software and vice versa right
so in hardware it's easy to detect
interestingly bad situations such as
cache lines getting kicked out for your
hardware transactional memory you lost
line native Akash that's really hard to
do in software people who write software
transactional memory systems doing
roughly the same kind of thing see a 10x
slow down to do it all on software it's
a tough thing to do in software hardware
can do it for free they have to do it
anyhow that's part of the cache
coherency protocol so it was just like a
freebie from them about cache line 0 we
don't have that ours does not order with
the standard memory fences because if
you do so then every time you enter
licks a walk with an allocation then
your pipeline of incoming zeroing get
stalled for the the lock and so in fact
you don't get the game you're looking
for you get instead of instead of being
the bandwidth covering issue you're
looking for which really is you stall a
CPU so you don't actually make progress
as fast as you were before your
bandwidth goes down because you're not
making progress so we have this funny
thing where here's a right to memory
it's going to 0 some lines in your cash
and that's
outside your coherency domain and you
have to do something special about it in
the heart software's all these games
around using cache line 0 in sort of a
speculative way knowing that he's going
to get some zeroes in but if he has to
abort and retry a harder transactional
memory attempt for something the zeroing
and memory is going to be different GC
barriers for read and write barriers
that's a big ticket item people who do
academic papers on read barriers
typically reports slowdowns in between
the ten to twenty percent range and for
us then a GC barriers essentially free
now it's not quite free but it's very
close to free that by the way I'll talk
more about when i get to GC that about
that Ribery allows us to change GC
algorithms to one which scale so
insanely high levels and works really
really well stack lifetime another you
know fancy right Barry detection thing
the predicted virtual call for doing
your type sub checking on objects on
virtual calls all done we do detection
in the hardware and in the software you
do all the complex fix-up logic so
there's no register roll backs on
hardware transactional memory for
instance some of the other most of the
other academic papers expect you to have
out of order engine and your hardware
which means you have all this register
renaming gear so you can do specular to
register updates and then blow them back
and reroll them and retry them in your
hardware we don't have that fancy
hardware but we don't need it either so
we don't have register roll back on our
hardware memory failure it's all done
entirely in software um relocating
objects for GC the software in line cash
means we don't need a branch target
buffer or getting so we don't need the
fancy piece of hardware to do direct by
code execution and so on and so on and
so on so complicated stuff you doing
software simple stuff it's usually
detection ends up going on in hardware
okay other games no way customers buys
funny new hardware from unknown company
and a funny or less they have to support
as well so we have a plug and play
thingy it goes in your data center you
install our JDK and you point your Java
program at the new JDK and Boomer worms
and literally it's ten minutes from
installed to sort of a max score jbb on
any oiled machine in your entire data
center that was a big ticket item for us
in terms of getting in the door
are at a lot of very conservative bank
like companies right we can speed up
older son HP gear just Institute install
jdk on your old son box it was running a
spark thingy you liked how that was
stable and working just too slow poof
suddenly its thousand times faster also
you get to avoid all the user-visible OS
business that leads to you know DLL hell
or patch hell right there's no device
drivers there's no legacy crud there's
no swap we don't we swap on the box it's
entirely stateless so big Colonel walk
issues if we picked up linux our
scheduler was definitely use a custom
schedule we got hundreds of cpus
thousands runnable most schedules on the
planet have never even remotely come
close to dealing with that size scale of
thing so lots of games you play there to
make that run well okay a bunch of other
stuff in the OS hard performance
guarantees so you can guarantee
individual processor process ease have a
required amount of either memory or CPU
or things like that so they get a
guaranteed lower bound level performance
but the box is big in terms of number of
jvms on it so they you knows a lot of
sharing going on but you can sort your
processes by a priority and guarantee
that the important ones get what they
need when they need it but if they're
not using it then all the other jobs is
sort of best effort can pick up the
extra memory make their GC more
efficient or actually CPUs to run faster
or whatever bunch of the stuff the GC
requires both fast he'll be mappings and
relapse and shoot down bulk fast virtual
to physical mappings these are not
available on any other OS on the planet
so we had to a bunch of work there um we
do have virtual memory support because
you know pm's crash so you don't want
one crash to bring your whole box down
we had people ask at all time you rider
you need teal bees if you got you know
strong type safety answer as well you
got bugs okay so other robustness think
si si Senor caches got chip kill a lot
of error reporting stuff OS cantiga
Deacon figured dead CPUs dead caches
dead dead memory chips and stuff like
that so we have our own OS why do we do
our own CPU
interesting question well um we can't
find a multicore 64bit cpu with you know
where the designs for sale actually
couldn't find one at all what had all
the features we wanted you have to have
ECC on your l1 turns out you want parody
at least something on your register file
because you got so many of them you got
32 registers that are architects to be
visible 64 actual Hardware registers per
core you have fifty four cores you're
talking thousands of registers you get a
single bit error you're going to be you
know you want to find out about it you
want to you want to fix it or do
something so we had parody on the
register file we need metadata stripping
in the load/store barriers we need to
redesign the load/store barriers to deal
with Hardware load store unit to deal
with hardware transactional memory to
deal with you see in the l1 cache to
deal with the weak memory model required
for scaling and by that time you're now
redesigning about half were CPU anyhow
so we could have picked up you know NEX
86 implementation you know and chip
layout and then gone after the l1 the
load/store unit l1 cache Union on the
x86 but that have been just this
disaster for you know how much the CPU
have to redesign and other the issues
alike you know you get a nice high
quality port of GCC and the toolchain
all that but that's only like a nice to
have we totally can do our own port to
any target hotspots entirely portable so
we ported GCC to our own chip we ported
hotspot to our own ship and that was
didn't take all that long to go pour it
so we have our own CPU her own OS own
interconnect all that stuff because we
couldn't get it elsewhere ok what else
can we do with all these cords well
anything we do on another thread is
essentially free because we got more
cpus and you know what to do it right so
we have big compiler thread pools for
instance it furiously in the background
when your big application starts it's
common to get 30 40 50 compiler threads
compiling get megabytes a second of high
quality chittod code with profiling
pouring into your code cache and after a
couple minutes it's all done and your
whole program has been compiled for you
on the fly while you're watching it
brick come up right obvious background
GC we do this giant GC thing we have
like a sort of orders of magnitude
better GC and anyone else in the planet
and we do this by having all those GC
threads do their work in the background
on other
Rad's on other l2's because a standard
an operation for GCS go touch every
element of the heap and this has some
really bad locality you get you scatter
gun all over the heap if you're running
a GC on the scene multi-core chip same
shared cache then the GC thread thrashes
the cash that's shared and the mutaters
the actual java threads doing work all
slow down because they're effectively
their lowest level cache is being
trashed by the GC thread so we run them
all on a different l2 cluster there's no
speed race for GC they just have to get
done before the next GC cycles done so
they have lots of time so the fact
they're essentially running without an
l2 cache is all fine they got an l2
cache it just happens to miss every time
you go to it and you can do prefetching
UTC that's a easy it's it's actually
really pain in the neck but it's doable
the other fun games you know background
profiling pre zeroing pages CPUs hot
spin on Io so it's you know it's clock
cycle times between when data arrives in
memory and when you start processing and
doing stuff with it it's very very quick
there okay so having come up with a
basic design we have to go build it oh I
guess go design it something like that
actually make ideas turn into a netlist
turn to a tape and a TSMC right higher
hardware team there's a top bust in
progress lots of good engineers on the
street we got a lot of good guys hire an
OS team hire IBM team we start porting
GCC and hotspot to the new chip we write
a simulator at the same time eventually
into booting the OS on the simulator and
it's actually fast enough to run hot
spot on a fast x86 at about 20 megahertz
of the Vega chip ops so that's fast
enough to run big interesting
applications entirely under simulation
lots of cool tools rebuild data rates
detector found lots of real data races
both an hotspot and in the application
itself get much a Kastner straight cash
layout visualization tools trace
generators all kinds of fun stuff I
learned interesting lessons here your
simulators have to be run on a true
multi CPU machine or you will not get
realistic interleaving that will happen
in real life so when we went from a
single CPU x86 to a dual cpu x86
huge count of new data races appeared
all under stimulation obviously that had
never appeared before even though the
simulator was simulating a multi CPU
machine and that was just you just had
to have a real multi CPU machine in your
simulator or you weren't going to
simulate any interesting Gator races
right first chip they go 124 course
grouped in three clusters of eight
sharing a one may L to each core has 16
ki and D cash four ways so she ative
extra tag bits for the hardware
transactional memory is short cache
lines the l2 is also for way people who
can do math can suddenly realize that
I've got eight CPUs each for way on the
D cash and four way on the I cash
sharing 1 l 2 that's only for way so
we've heavily oversubscribed the ways on
the l2 and we're obviously at risk for
false sharing where two or more you know
four cpus all agree to use the same line
in l2 and the fifth CPU wants use the
same line boots out the first one and
then the eight each all boot each other
out and you get constant mrs. because
everyone's false sharing on the l1 and
l2 because of the small associativity so
it's a big risk for that but we were at
the limits of die size and yield that we
could get away with here this was the
biggest chip TSMC had ever done at that
time did a lot of profiling i'll look at
traces there to come up with these
numbers but they were obviously pretty
much a crapshoot at that time we did
have the support for the full 16 chip
interconnect and the l2 miss is roughly
the same cost as going to another l two
are going to memory so it's you know in
your l1 senior l2 it's not in your l2
sort of the cost hierarchy for memory
accesses 384 cpus we built it and it
worked it's pretty pretty amazing okay
cpu is this easy jit target that i asked
for classic in order through address
risk 64-bit cpus one hit on a Miss cash
store buffer it's not really store it's
a store lateral a store buffer it's only
one entries is very simple CPUs get our
masking of metadata we have a full FPU
per Coral it's a very simple one there's
no separate floating point registers
no flags no mode bits no weird stuff you
get on CPUs have been around for 20
years background spill fill register
stock to help with fast high speed
calling you up and down boo she stacked
all traces special ops 2 a bunch of
simple stuff make it easier array math
checks virtual call things the read
barrier is a very high frequency
operation so it's crucial to be fast the
right barrier replaces a very bulky
thing it's hard to do all the energy
drops fast that you need to do the stack
barrier although if you're just didn't
card marks in the right barrier it
doesn't save you so much saves you some
though we get our first silicon from
TSMC about two thousand four right it's
not quite dead and it's dead Jim not
quite ok so the l2's are mostly dead a
few l2's can run because they got the
ECC and they're in the sleet correcting
errors and also we have this one way
because the limp home mode like your
transmission let you guys know this but
if your transmission dies it has a limp
home mode where you can stuck in first
gear and the car will be drivable
although the tranny will have exploded
internally like I said all it's the
older drivers are not in your heads yeah
I remember this so these are
manufacturing errors of various kinds
that we don't know what's going on yet
so at the time we get the chip back it's
just totally obliged with a guy at TSMC
in Taiwan he gets on a plane with a box
and he shows up in our office 24 hours
later and we you know score the chips
break him open them and put them under a
you know put a probes on my way we go
right and and obviously the first thing
happens is they're dead okay what's
going on okay so um but we have this one
way mode two of the four ways and your
one Miguel to you can turn off three of
them that are so dead that they're just
killing you and the one way lives with
massive ECC errors but it's living so
you got a one way to 56 scale to write
you get register rights from the even
numbered registers are bleeding into the
odd numbered registers periodically
that's kind of really ugly things every
time you do add our X plus ry and de
register R Z if Z is even then sometimes
the bits that you wrote in RZ showed up
in our Z plus one so it's kind of
so so we get to a subset of the even
subset of registers right although we
have other issues like the decoder the
instruction decoder treats branch offset
bits as red shirts and parallel decodes
the instruction type as a branch but
also is decoding the register bits out
of the instruction so if you're
branching to an even or an odd offset
you get an even or odd register decode
and that cycles the address port for
that registered piece of retro file and
again you get these updates to the wrong
bits you have to branch to even
addresses now that's not exactly even
it's like the fourth bit over whether it
was some random bit that you had to
always have zero on a branch we still
get a few good tips out of all this mess
we have two overvoltage them so so that
they'll so the registers will behave
well enough you can run at all and so
the chips have electron migration issues
that the actual bits of silicon or
migrating in the matrix and going the
other side and so the chips are
literally cooking to death at about a
month so it's like guys have these chip
with all this gear on at this giant fan
right on the chip and you know toweling
away on the fan and you know and it's up
and it's working against doing stuff and
it's running at one-third you know
one-third the clock frequency and five
times the voltage and it's dying but
it's cooking a little bit right so we
make progress we get the OS guys to
actually get to where they can boot on
this sick thing they actually get a
great chance to test all their error
correction codes all their d configure
bad cpu codes all this stuff they got
that all working out in the first few
months but we go back to TSMC and say
you know what happened here it turns out
that actually what happened was a bunch
of the silicon IP we'd purchased from
other vendors from the DSP market mostly
had spected internally at their site to
run at the frequencies we were looking
at but that was about four three one two
or three times or frequencies they'd
ever actually shipped to customer at so
in theory was supposed to be good the
frequencies were looking at in practice
had never tried it and in practice
course it didn't work so we paid for
this to work at you know x frequency and
it wasn't working at that frequency so
they had to come back to us and work
with them and they kind of figure where
they'd screwed up and we had a metal
masks bed so he took the existing wafers
of which three quarters we had stopped
before the last metal layers go on and
have the
add a new metal layer so you had a bunch
of new extra wires turns out in all
these cool tricks to harbor guys do they
throw a lot of extra transistors on the
chip that no one has it used for and
then if you do a metal mask spin you get
to pick up new transistors that you
didn't ever use before have a need for
so you could just route around you sail
instead of running this wire from here
to there I'm going to run it over here
and do this some of these transistors
that I just left lying around for this
purpose and then drag it back over here
a maths how they correct all these
issues so they do this metal mask spin
correct a bunch of these issues and it
turns up the second silicon is actually
pretty darn functional there was a minor
security crew fix I think only us would
have ever noticed it but we spun it for
that anyhow and other than that you know
bagel one was alive two weeks from
getting the silicon to getting the OS to
boot a day later get hello world out of
C program four days later gadaa badesh
version works right we enter you know
year under stimulation and the simulator
hours really paid off here they still
took a some a year its system robust not
just the metal spin thing so a lot of
true data rates bugs that just never
happened before because no one had ever
seen his system is out of order and
concurrent before we have way more cpus
and anyone expects we got way more out
of order allow than anyone's seen before
all kinds of issues with just true bugs
a half be found there a lot of
performance warts i mentioned earlier
about having a four-way el to being over
committed well it was we've got a lot of
conflicts heavy l2 miss rate really bad
a lot of TLB miss rates turns out that
we can fix nearly all the l2 miss rate
issues by doing random offsets on your
stacks because all the stacks were
neatly aligned on one meg boundaries and
they all would charge into about the
same depth in their call tree and
scribble up and down furiously at about
the same depth in the call tree all one
mega offsets with the land politely in
the same way and the l2 cache and so all
eight stacks would be fighting for four
ways and l2 causing in this Misha's with
little random offsets and poof with palm
solved we did a little random offset the
code cache little page color in LS and a
lot of these issues went away but they
certainly started out there turns out it
is okay to have a four way l to share
840 al once the Miss rate is entirely
tolerable but not sort of out of the
door is take some work get there
bunch of other issues with the
virtualization layer I mentioned we were
plug and play well we virtualized the
JVM to do that that wasn't virtual
enough it was showing through we had to
fix a bunch of warts there I Lee had a
ball performance fixes thought the years
to make a life run faster and faster ok
so what worked out of all of us and what
didn't um well the chip worked after the
second metal spin right plenty of
bandwidth go around even more CPU cycles
unbelievable number of CPU cycles
certainly compared to like an x86 we
have way more MIPS per chip MIPS per
what that an x86 like 5x more it's
unreal more we eventually got our
predicted cache miss rate so it doesn't
happen right away there are a lot of
funny places where we got weird miss
rates due to that L to being low such
activity before we finally beat it out
you know figured all the issues and
solve them cpus remain slower than we'd
ever hoped for and that's just sort of
the limit of an in order low frequency
CPU and not having an uber huge design
team to go do the the you know fabulous
job you can do on Hardware there um
there are a lot of hardware features
that were not turned on for a long time
because we had stability issues in
software we had to get you know get this
off to a robust and reliable you can't
stall too especially to a bank until
things are pretty darn rock-solid I'm so
we have our hands full with basic code
gen issues and data races and stuff like
that the hardware guys and their own set
of issues for about a year we're weird
low frequency d round bugs a lot of more
mass by ECC where the hardware had a
real bug but the ECC was content
continuously correcting it I mean they
eventually find out and fixed all these
issues but it did take a while the
motherboard had to go through quite a
few iterations to get a lot of the
timing issues out with the dm's we can't
get power supplies as reliable as
claimed that was kind of surprising I
thought it'd probably be a done deal
well I you know 2500 watt power supply
you go doing these big power supply
vendor guys the Jitsu whoever we have
bunch people you know sell such things
and go through a specking process with
us and bid and ship us models and then
they would just die in the lab straight
up here's this new 20 under 15 zoopla
2500 watt load on it funkin died the
heck
yeah what are you doing so that went
round and round while before we got that
squared away I'm a bunch of scheduling
problems into OS it's hard to schedule
800 CPUs it's just not a it's not a
plug-and-play solution kind of thing a
bunch of i/o stack issues having to do
the virtualization of the iOS tax so
it's not just virtualization sufficient
virtualization the i/o stack that's hard
read barriers arm every time we get a
new integration from sign there are new
unburied loads cuz son doesn't Udrih
barriers and their implementation so we
have to go find them all and fix them
and of course not having a read barrier
cause some very low frequency hard to
debug GC issue that takes forever to
track down it turns out that our GC
certainly the first cut of it was a
single generation only would you a full
GC cycle every time a de GC cycle now a
GC cycle at that time would cost you a
100 millisecond pause no matter how big
you keep was for hundreds of gigabytes
of heap but we do it every five seconds
so if you let son's code if you take a
full GC cycle and they do some stuff
during a full GC cycle they don't do it
any other time and they would normally
in the course of a business day to a
fold you see cycle about once a day
we're doing them on one about 135
seconds so we found lots of bugs in the
full GC cycle behavior that were related
to the fact that we only happened on a
full GC cycle because we're doing so
much more often so these days were
generation on the GC is more efficient
but you still get a GC cycle every five
seconds of some kind okay a lot of
internal engineering debates raging on
over should we work on stability should
we turn on hardware transactional memory
you should return on stack allocation
escape detection thing how about bigger
thread pools for faster startup time get
your jiading code jaded faster tiered
compilation generational GC read the
list goes on turns out that hardware
transactional memory support and the
stack allocation the escape detection
bits both lose for a while just out of
engineering man-hours they're hard
problems to get right takes a while to
diagnose and fix it we had a lot of
other issues were debating like
engineers helping with sale calls
because the customers have code that is
not correct but it's happening to work
okay on their x86 so they die once a day
in production
Azul box and dies every 10 minutes and
so now you can't get a sales through
because you know your application can't
run 10 minutes or falls over right get a
lot of true data races in their own
buggy code that they weren't aware of or
they knew they were there this couldn't
find him the HTM itself the performance
of it was buggy for quite a while mostly
caught and essentially live lock where
he would attempt to transact through a
lock and that would fail for whatever
reason and so you would try again you
make no forward progress endlessly
failing it's essentially live lock fail
fail fail fail eventually you track
those down and and and flip over to the
OS lock but you have to after you
tracked it no trap fail out to the OS
lock and go ahead and proceed through
the lock and make progress periodically
you have to go back and retry the HTM
because there's a lot of startup
conditions for which the HTM is just
going to fail and there's no point to
doing it chilla system so that hits
steady-state then you start trying the
hardware transactional memory again and
if it works you can sort of bump the
steady-state up as you go and get higher
and higher throughput issues out right
at this point it's been on turned on by
default been shipping for three years
now it rarely helps customers for like I
said it's mostly note talk it never
hurts occasionally you get a big wins
you get a 2x speed up or something but
the usual case is there's nothing we
will always transact a couple dozen
locks it's never enough locks that they
can actually get more throughput out of
the box and it's because of other issues
that they can't get through put out
either either though you can't transact
the locks that you need to because of
true data dependencies or else it's not
locking as the scaling limit it's the
network's not fast enough the DB is not
fast or whatever it is right stack
allocation has more issues the good case
is really good big busy app service can
have huge percentage of objects stack
allocated but the bad cases are really
bad and we just ran out of time to go
get the bad cases all squared away and
our standard GC is really good these
days are standard GC is we'll do 3 and
we have customers are 300 gigabytes
heaps in production three to 400 500
gigabyte keeps with 35 gigabytes a
second allocation rates with max pause
times in the low tens like 10 to 15
milliseconds so that's your day in day
out running continuously
so this is several orders of magnitude
better than the best you can get on x86
these days so we don't have any drive to
fix the bad cases so stack allocation
just lost ok here it is GC works really
well now first time a new customer it's
really common to do this you install the
gear you take their current command line
launch strip out all their old GC augs
double the default each size they were
running with and run again and they
never have a GC problem ever again now
gc's probably in doubles probably too
much you die the back you trade off CPUs
for GC at some point down the road you
solve all the other problems before you
get into a sales job and let the
customers do the dial back and figure
out the right trade-off between CPU and
memory there it's it's a combination
there's all these great tuning flags
that don't make any sense on our GC at
all and in fact we have no tuning flags
at all on the GC if it doesn't perform
well that's a bug for us we go fix it on
whatever your app is whatever shape GC
cycle thing it's doing so yeah there's
some combination they're doubling the
default heap size it removes a lot of
corner case issues people wedge
themselves into that they've pushed
themselves up to the limit then they did
all these things try to preserve heap
space that are kind of ugly internally
because they can't get a bigger heap
because the GC pauses kill them and so
as soon as you like uncork the GC limit
like a lot of things cookies like the
caches are all too small because they
can't have a big cash because they're
keeps too big right so she's okay double
the heap size what mediately they grow
other caches well okay now everything's
the cash works well so they've made a
bunch of progress there right so there's
a bunch of stuff going on there so I'm
have this really cool internal profiling
tool has a horrible name customers who
first saw it like totally demanded as I
go I gotta have this thing we go just
now it's okay fine get to you and then
that's now it's a major selling point um
we get to see the guts of a running JVM
of every running JVM so I could always
add on profiling tool you attached it
with a browser on a vm that's been
running for a month in production and it
will tell you everything you can
ever hoped and want to know about a vm
you know right now as it's running so
like it's always on you get thread
stocks you get GC cycle speeds and feeds
you can surf the heap you can look at
all your hot locks and all the callback
stack traces you get you know call or
call a sage profiling data always
available always on just plug it and
look at it and figure what's happening
so that's always deals where you know
that the vm has been up in production
for a week and then some load spiked hit
and throughput on the vm drops down down
the toilet what the heck happened I
can't repro this ever in QA because I
can't ever get a week run followed by
the right set of magic conditions they
could fail so but now i can i mean i
don't know i don't care i pull this
thing out oh what's going on okay now i
can see um chips OS they're solid now we
have over a year up times and some
systems the standard engineering box
that i do daily work on at azul now goes
down when we have power failures and
that's it last time it went down hard
was google chopped are our chilled water
line to our datacenter i guess it went
down huh wash your chips yes okay oh
here's our TPM okay i do it slide on
this ya number to feature behind the DC
and the stable performance under load
thing right live peek into any JVM with
any web browser always on overhead
always kind of cool stuff I i yes yes
like you get a snapshot and you get
refreshed in the browser you get the new
snapshot of a new stack and it's run on
four how many seconds and then here's
the new stack oh um well no you can get
them as fast as your browser can refresh
right but of course you're going to get
no no no you couldn't keep up obviously
no no every time you ask it's a pull
model you ask you could new stack right
it's what the stack is right now okay
well we also have all like network stuff
in there dimension that's like net stats
on in the vm we know all your sockets
that are open how much i owe is going
through them and what's doing where it
going that kind of stuff
okay rolling along yeah big ax two came
out a couple years ago 48 CPUs perk or
higher clock rate faster memory bus some
tweaks to the hardware the read barrier
had to be changed to support
generational GCE we drop some less use
instructions that were not binary
compatible from generation to generation
we also change like the encoding layout
for convenience for the the guys doing
decoding so that was an instruction
change but the binary bits were
different for that piece but we actually
dropped some instructions and added some
others 2008 we're now having Vega 354
cores actually we had nine cores per
cluster and Vega to but we had reserved
them for yield issues so of the nine on
a core we would look and if there was
one dead we would whack it out with an
ion beam you know mask out game and then
you have eight and then you get a good
ship or all 48 CPUs are available right
and by Vega 3 the yield is so high that
we just said we'll just keep a 9 is 9
it's bigger cash higher clock rate
generates from GC better support for
profiling better HTM now we're working
on fourth generation which I can't talk
much about let's see what else is in
there okay so muscles learn some fun
stuff here right if you own the whole
stack hardware and up you can do
progress even with something really
awful hardware bugs right we worked
around some ridiculous things in
software um some of them we never
bothered to fix in hardware we just
fixed in software and we're done some
really hard Harbor palms like wedding on
a bandwidth is a hard hardware problem
we fixed well we improve hugely by using
coz to cut our bandwidth by a third that
was a big ticket item the hardware guys
like you know we can give you x
bandwidth and all our predictions say
that's going to be marginal we could cut
a third off and suddenly you're in the
sweet spot right GC I'll claim it saw
well at least I saw a certain domain of
the small to very large but not the
microscopic not hard real-time although
it's hitting closer um or at least we
can handle something that's quite a bit
larger than anything I've ever seen
anyone else handled by you know by two
orders of magnitude we can do la
collision with some very simple hardware
transactional memory and it
and really help it's very sad we might
be able to get some help doing like in
calves and algorithms for people who
like Doug Lee who going to write
concurrent algorithms doing very
specific things where an in cast is
going to let them do some fancy trick
they can't do any other way but it's not
going to be like it shows up in your
everyday programming work you can't just
call out to an HTM and have it do
something useful for you wha okay huge
counts of simple course has turned out
to be useful in production that was kind
of surprising I wasn't quite sure if
that was true or not it no question we'd
live around faster cores but we can get
away with simple cores and loads of them
and do jobs that you can't ever do on an
x86 and that is probably it yeah okay
guess I'll take take questions okay okay
so a lot of the features question is
something like a lot of the features
here resolve revolve around GC and
wouldn't be feasible for Intel to do
something along those lines and the
answer is that we have talked a lot with
Intel and it's like moving the Titanic
you know it doesn't matter how hard you
push that just like a millimeters at a
time um I don't know what it would take
to get them to move it's like for Intel
not to do a read barrier it's like an
obvious crying shame and a waste of like
huge market potential for them so
suddenly own the whole Java server
market right for son not to have done it
should have been criminal offense on
their their their stockholders should
have sent their you know exact staff to
jail for being like you know after we
did it and said oh look at what we can
do that they didn't Amelie announced
that we're doing it on our next tip was
just like criminal
yeah well and it's um they haven't okay
yes so the answer is totally in toca do
this and totally we've attempted and I
can't say more what where that's at
other than say I'm frustrated as Jeremy
points out it's not just Java that would
get helped here everyone doing a managed
runtime with garbage question we could
helped here seems like an obvious win
from all parties no no from our
customers point of view okay i mentioned
binary compatibility to be sipped GCC
dances we do not ship GCC we are not
binary compatible our binary
compatibility layer if you will is java
bytecodes and we're compatible at that
sense but not any other way yeah yeah i
think those straight the fact that we've
gotten rid of the binary compatibility
Bugaboo that bites Intel by switching to
java bytecodes as the binary
compatibility layer yeah in the read
where how do we negate the cost of traps
okay so so the sort of the issue is we
do and we don't and it doesn't matter
altogether at the same time the we have
in the hardware support support for fast
user mode traps so when I say fast user
mode trap when to read barrier traps
essentially you get a function call to a
fixed address takes about four clocks to
get things set up and you're in the
function call and a return takes you
back to where you were and you re
execute what you return to the following
instruction you have to dork the return
pc if you want to re execute the
instruction that faulted which would be
the read very on top that fault okay
fine so that's cheap in the sense that
it's actually there's no colonel hopping
we the Colonel's not involved here at
all then the algorithm the way the
algorithm works when you update the
faulting value which is a value in a
register you also update the value that
fell faulted in memory so the next time
you load it you won't trap so there's a
sort of a self-healing property here as
you as a mutator works through here
working set he cleans out the the
pointers that are in front of him that
are in his way as he goes at a cost of
you know 4050 clock cycles per fail so
at agc face flip boundary when you flip
the magic bit on what's correcting what
false um the mutaters all throw up a few
hundred traps before they get there
working set and the GC threads all
furiously charged ahead of the mutaters
over what they hope is the mutaters
working set flipping that's ahead of the
mutaters and in practice after a few
hundred traps they didn't they quit
trapping you know they have a very steep
tail off and then they get occasional
traps way out there but you know mostly
they quit rapping so the cost of a phase
slip the trap storm that happens on a
face flip is so small that we have
trouble measuring it with good hardware
performance counter implementations so
it's got to be there we know the exact
count of traps it's too few to get
matter yeah how do we okay with all the
courses in threads how do we mitigate
the stop of the cost of stopping all of
them okay so the first thing is is that
um GC cycle pauses come every few
minutes and then we do cooperative self
preemption as a means to stopping assume
so most of the GC pauses most the GC
cycle facelet things you would think
about we do with a ragged barrier in
that case each individual core passes
some line in the sand he calls his own
stack which is hot in his cash he does
some GC thing he announces to everyone
else he's done it and carries on when
everyone announces that they've all done
it you're done with that phase no one
CPU has ever been halted except to do a
stack crawl and verify that his stack
was you know pretty looking however we
wanted it done there are a couple times
a minute we appoints have to stop
everybody and so there you can't start
the GC issue until the last guy checks
in right so the longest pause is the
first guy quits going to the last guy
checks in your GC thing and yell at them
all go and a cooperative self preemption
what we do is we first announced
all threads that we want them to come to
a safe point as soon as they can and on
the flip side threads that are knowing
that they're going to go do some sort of
blocking operation including getting
preempted out of the process all come to
save point first so the only guys who
are running other guys are actually on
the cpu is running everyone else is at a
safe point so you cut the thousands down
to hundreds is however many cpg are
actually running then the hundreds all
first attempt to cooperatively a safe
point while the GC guys are trying
individually to shoot down the non
cooperative guys so they have a week all
that you know Night of the Living Dead
kind of race where each guy that checks
in now goes it helps the next guy check
in kind of thing so as a log treaty
effect and they all pull to a halt in
practice it's usually a handful of
milliseconds to drag everyone to a halt
when you have hundreds of running
threats if you have tims of running
threads it's usually microseconds they
get them all to a halt it's at the
hardware level the the chips have a have
a bit procore that says that when you
pass an instruction that contains of the
appropriate flavor you will halt the old
trap and steadfast user mode trap
instead of executing like a couple
branch instructions so backwards
branches do it places you put safe
points in the test for this is done by
running a single instruction ah so you
go to make it happen you go to the OS
and you say please tell all the CPUs to
say point themselves and the OS
replicates a message throughout all the
cores saying light up all the bits and
all these cores and then the course will
start checking in so that's all done in
microseconds you get into the OS and
have them light up all the chips and
then that all the threads start lighting
coming back in at you so there's some
hardware support there
ya know the GC was so good that no one
that we never got around to doing the
the the hardware the stack the stack
escape analysis stack allocation support
as far as I know and then the question
says anyone else done that as far as I
know no one has gone ahead and attempted
to do escape detection and assault pure
software level just to see how it would
work although I've proposed it to a
number of people I haven't seen any
literature suggesting that's been
attempted it seems obvious to me that it
should work well you can do that kind of
escape we did a pretty aggressive one in
hardware because we could you could do a
simpler one that wouldn't be quite as
pretty in software and a handful of
instructions and that would probably be
good enough to get the fifty percent
mark which would be a big win for a lot
of these big heaps I don't know what's
the Java programmer supposed to do throw
up his hands so what's the cost model so
question is whether the cost model for
volatile read verses a volatile right so
you know to be honest you have to code
to the x86 platform and what's the cost
model with volatile read verses a
volatile right on the next 86 and
everyone else is live with whatever you
do okay so under the x86 a volatile read
i believe is almost the same as a cinch
as a plain old read and a volatile right
has an extra fence the extra fence is a
cache miss which on x86 is like a
thousand clocks worth of issues that
didn't happen or something Horrible's
300 clocks times for white issue or
something ridiculous thousand opposite
didn't happen so you should always
favored volatile reads just period and
you know it be fair that's that's our
problem now having said that for us no
NGC all GC issues that don't work or
yeah our bugs Yeah right for for us
reads or writes that don't contend with
other threads are one clock either way
so if you do volatile reads or volatile
rights there's a clock for the read of
the right there's a clock more for the
fence the fence is just one clock if
you're not and actually got contention
going on if you've got contingent going
on you probably don't have an algorithm
that scales to hundreds of CPUs unless
you're you know you've been on one of
our boxes so it doesn't matter you're
not going to scale were duly anyhow and
that's just the case that's a way co-ceo
Doug's happy when he goes to 64 but then
I beat on em and see no no do this and
it goes to hundreds and I have
algorithms that do go to 100 s go to the
you know for a while 864 so it can be
done it's just it's not it's not a
straight for operation so you know if
Doug's not going to do it I'm not
expecting your average guy to do it so
II favor reads
yeah so how much trouble do we have so
the usual story the way it kind of goes
is the first time you get into a
customer site it's always an oh my god
saved my bacon kind of thing and they
already have their best people on it and
they're stuck so you have really sharp
guys and you show them what's going on
explain how it works you show them our
TPM stuff and they get it and then they
go after it and they you know our TPM
gives the tool even missing for years
and years and they tear after their own
code and lo and behold it happens they
get to performance out sometimes there's
a couple rounds of a sort of lecture by
me or by somebody else doing how to
write scalable code why this is that or
whatever but not much and then the smart
guys go to town and they make it work
after it's been in the customers hands
for a while that knowledge spreads out
to more and more people and then they
get apps that they try testing on your
gear and when it doesn't work they go to
talk to their in-house expert and they
figure out what's going on so we've
gradually seen an uplifting level of
education in the market as a whole of
why it's what scalable and what's not
and what's a bottleneck and what's not
why and how it all works so things are
improving slowly Google is pushing at it
as well in your own way you have
different you have distributed memory
but you have the scalable issue and so
I'm sure you have to educate every new
programmer comes in the door but then
some of them will walk out and talk to
other people and and so there's some
level education going there yes sir I
thought you raised your hand to eat like
this is that okay um we've gone back and
forth on this mostly we don't want to
support ah we don't hand out the love of
support the we don't have enough
engineers to handle the level support
into the universe's want we don't handle
the toolchain and then have to help the
university guys figure out what they
screwed up so to do it we have to really
hand them a box and say if you run Java
will honor the contract on maintenance
and guarantees and works and doing
anything else what it is just your
problem and and that's been a harder you
know part of job after having said that
we do have a number of people who are
putting out for government grants for
boxes and we do do academic licenses
where you log in remotely on a zoo box
which Doug does but sort of like near
chevette skies Mercer Lee's grad
students log in pretty routinely and run
big parallel weird you know algorithms
but they run it in java bytecodes yeah
oh it's windows so we SAT a hard marhaus
register spill Phil yeah it's variable
sized windows done in the hardware yeah
it's two stacks yeah register stack on a
regular or sort of see execution stack
it's just to make function calls faster
and because it's variable size from have
a spark issue of a recursion blowing
your stack out forcing whatever I
believe in the Vega three ended up at 64
which given the spill fill rate was
plenty good enough like can you look at
the instruction traces so I look at the
hardware level traces where the cycles
go it's much less than 1% going the
register spill Phil engine like I have
no desire to speak to that guy up right
it's not that's not where my time goes
yeah and Vega three it's two instead of
one turns out that that's fine and
that's because you don't stall for
rights to the lines that you're
immediately allocating because you've
preserved Emmanuelle one and they've
arrived just in time and so they're all
hot in your Owen so all your new
allocation objects you don't do the
zeroing rights in the first place so
it's just less rights and you also don't
have to wait for them to convince didn't
need a big store buffer to handle the
cache miss so on an x86 you want a big
store buffer because you're going to get
a cache miss he predicts some of the
time and brings that line in are they
asked to read make a coherent is the
bandwidth costs but it's not always so
he wants to be able to eat an entire
cache lines worth of rights and probably
actually several cache lines with the
rights because that's how far you going
to get behind if he gets behind there
right I know spark has a giant write
buffer and they did it because they're
running java code for which what they
would what would happen to them would be
people you know to bunch of rights and
rode out to zero out new objects and
there weren't in their cash and so
suddenly they had to eat those rights or
stall so they put a giant write buffer
in
yeah so question is you know what's the
collector style is it a copying
collector or what there are phase flips
it's there's a ve 2005 paper that
describes the algorithm in a lot more
detail yeah pause less GC paper so it's
a it's a copying collector we copy every
cycle we copy the whole heap every cycle
and compact wastage in what way way
extra wett extra CPUs we always got
extra CPUs apparently the GC guys never
seemed to mind or care or anything
there's no issue there one of the games
that goes on is that we don't have to
run the heap so close to the out of
memory state that other collectors end
up getting because of the pause issue so
we typically have a sort of a generous
slop extra on the heap and then that
eases a lot of GC pressures but when you
run the Box down to tighter and tighter
memory limits GC cycles faster and
faster it's burn CPU but it will
comfortably run at you know ten twenty
percent over live heap you'll just be
burning a lot of CPU to do it it's not
necessarily where you want to set
yourself</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>