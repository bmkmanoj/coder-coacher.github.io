<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Motion Estimation from Image and Inertial Measurements | Coder Coacher - Coaching Coders</title><meta content="Motion Estimation from Image and Inertial Measurements - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Motion Estimation from Image and Inertial Measurements</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ylcHi9-4quM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the motion for images and the inertial
measurement dan and I went to Cal and
when let's go together in carnegie
mellon in 1998 every good friends since
we were drinking together and taking the
same classes and then is graduating last
year from CMU and what's working on
Honeywell for a year now he's here for a
candidate for the research scientist
thank you I don't usually talk into the
microphone so if you catch me lowering
it or not repeating a question not just
let me know okay good to go ok so the
title of my talk is a motion from image
and inertial measurements mi can you
hear me ok a little louder um so let me
start off right away saying these
materials around the web the slides my
thesis and things like that are up there
ok so it's pretty well known that for an
image sequence we can recover the six
degree freedom camera motion without a
knowledge of the camera surroundings or
a priori knowledge and without an
external reference like GPS this is a
good example so this is a sequence taken
from a turntable by Andrew Fitzgibbon
and what you're seeing across the top
are six frames from this maybe 50 frame
sequence and on the bottom you're seeing
the results of that so each of those
little frames you see in the circle on
the bottom are the six degree freedom or
three dimensional rotation and
three-dimensional translation of the
camera at the time of each image and
then in the middle you see that from the
camera positions he's reconstructed a
little what we call a sparse model of
the dinosaur okay so there are a lot of
applications for this type of thing so
modeling from video is one this is an
example from an old collaborator of ours
ambuj cheetah across the top you see
three frames from a short sequence of
the Grand Canyon and what he's done is
he's used the factorization method which
I'll talk about a little bit to get the
position of the camera at each time then
once you know the position of the camera
at each time you can do like a multi
baseline stereopsis to figure out kind
of the dense structure of the scene that
you saw and that's what you see on the
left and then on the right he's just
taken that structure and texture mapped
it
Oh some reverb or something okay
Oh
I'll speak right into it okay okay so
another possible application that we're
interested in is micro air vehicles so
these things are really coming on the
scene now they're little things that can
fly say in cities or in other restricted
areas for surveillance a good example as
aeroVironment's Black Widow it's a
little 6-inch aircraft that can fly for
about an hour on its battery it carries
a little camera that's just a few grams
so if you could estimate the motion of
this thing using the camera you wouldn't
say have to carry a GPS receiver or you
could go under the canopy in the forest
or something where GPS wasn't available
came over navigations another good one
you know there's no GPS or other
external positioning on Mars so for
example the Hyperion rover the CMU belt
is a kind of prototype intended to test
Mars rover navigation so we put our
camera on Hyperion and used to have the
camera to try to estimate at sixth off
motion and I'll talk a little bit more
about that a more terrestrial
application for ground vehicles is David
mr. system and I believe that's his
equipment on a perceptor program vehicle
from CMU okay search and rescue robots
is another good example these things
okay so this guy's called Rex he's being
developed by several universities
jointly and he's a very simple kind of
cockroach like design he's got six
actuators or legs that kind of move
independently and without a lot of
intelligence but that lets him get over
rough terrain in particular the thinking
that if they could send him into rubble
at an accident scene maybe he could find
survivors but of course there's no GPS
in rubble so if you could estimate its
motion from the images that the thing
that the cockroach sees that would be
helpful for locating the survivors
position and then another thing just one
last example nASA has been working on
this thing they call NASA Ames has been
working on this thing they call the
personal satellite assistant it's a
little kind of disembodied head that
flies around the station using fans and
they don't want to put up fiducials or
other external references on the space
station because
space is constricted but it's got four
stereo cameras for stereo pairs on it so
if you could estimate the motion of this
thing in sixth off three dimensional
rotation translation using the cameras
you'd be in good shape all right so for
all these problems there are a few
things in common we want six degree
freedom motion meaning the 3d rotation
translation we don't have to instrument
the environment and we don't have to
require GPS or some more archaic
absolute positioning system in addition
for things like math or or robots
looking for survivors we want small
light and cheap sensors and disposable
sensors and really the thrust of this
talk is that we want to do this over the
long term and by long term I really mean
two things one is the obvious thing that
the absolute time or distance is long
the other thing though in the context of
vision is that only small fraction of
the scene is visible at any one time and
i'll show you some examples of that all
right so given all these requirements
cameras are promising sensors and a lot
of algorithms for estimating the motion
of the camera already exist but at the
same time you know where the kind of
easily run or consumer type systems for
us to being the motion of all of these
applications and for doing things that
people want to do like automatically
modeling rooms buildings or cities from
a handheld camera okay so my answer to
that is that motion suffers from some
long-standing difficulties and really
this work what it does is it says we've
been trying to do this for so long how
can we step back and try to objectively
examine the reasons why we we still
can't do this after all these years and
so the four thrusts here are I'm trying
to add cheap what I call epsilon and
Herschel sensors to the problem which
are kind of the least or the cheapest
least accurate inertial sensors that we
could add to kind of regularize the
problem robust image feature tracking
and I'll talk a little bit about that
recognizing previously mapped locations
that we can limit drift over the long
term when we revisit a site and then
sliding omnidirectional images I have
some slides at the end I won't talk
about this and less people are really
interested but I can point you to the
website also all right so this is my
outline I'm going to start by giving you
a little bit of a refresher and
technical introduction to motion from
images then I'll build on that with my
algorithms for motion from image and
inertial measurements we'll talk a
little bit about robust image feature
tracking this idea of using
reacquisition or recognizing a
previously map scene to do long term
motion estimation and then I'll just
conclude very briefly all right so when
you're trying to estimate the motion of
a camera or a vehicle to which the
cameras attached images a kind of two
step process is common so the first is
sparks sparks future tracking and the
second step then is estimation for
suppose feature tracking the inputs are
the raw images not in the outputs are
the projections are the tracking data of
sparse points in that sequence okay so
this is that kind of a classic example
it's the hotel sequence taken by I
believe Carla tomasi at CMU it's about
20 the subsequence I've taken is about
20 images we've automatically extracted
about a hundred points that are in well
textured parts of the image we've
tracked them using Lucas kannadi through
that Tony image sequence okay now
they're kind of I think of two paradigms
for doing this type of image feature
tracking the first is template matching
which is a kind of Venerable method and
basically you've got some point in the
first image you want nor that point is
in the second image so you just extract
a little say 11 by 11 or 13 by 13 mask
of intensities around the first point
then you do maybe a raster search in the
second image to find out what location
maximizes the correlation between that
mask and the mask around the kind of
candidate candidate point in the second
image ok now Lucas kannada also at this
point of pretty old method is just a way
to do that a little more efficiently so
basically it says we've got some error
that's similar to the correlation error
that sum of squared distances and
instead of doing a raw raster search
we'll just do a non-linear minimization
to minimize that error and i'll talk a
little bit more about that and then
there's the second paradigm or at least
in my kind of mental hierarchy which is
independent
direction matching some set of
extracting features in one image and
then trying to find them anywhere they
might be in the second image we'll just
extract salient points in both images
and then try some combinatorial
optimization to match those two sets of
points a lot of people these days for
efficiency are using Harris features
there's they're kind of similar to the
features I showed you in the hotel
example but they're there efficiently
extractable and then something that's a
little less efficient but more accurate
as scaling bearing feature transform key
points by David Lowe and I've used this
in some of the work i'm going to show
you here ok so the second step in that
motion for me which is paradigm is the
estimation part so the input of course
is the output from the tracking phase
which are these projections are tracking
data and the outputs are the six degree
freedom camera position at the time of
each image and the three-dimensional
position of each track point ok so if
you if you do bundle adjustment which
I'll talk about in a little bit for that
hotel sequence this is what you get what
you're seeing on the left or i should
say what you're seeing all together is a
kind of a top-down view of the
reconstructed 3d points on the left and
the six degree-of-freedom camera
positions on the right viewed from above
so for example this area here is the
front of the hotel and this is one of
the roof lines in the other part there
is the other roof line the center of the
axes over there is the position of the
cameras center or say lens position at
the time of each image and the motion
that bundle adjustment recovered there
is at least roughly the correct motion
alright so the methods were doing the
second step the estimation step are a
lot of them are pretty well-known bundle
adjustments kind of the go-to method
when you have the time and computational
resources and I'll talk about that in a
little bit of detail basically you're
just using nonlinear minimization to
find camera positions and point
positions that are you know consistent
with the projection data that you
extracted and a key part of a bundle
adjustment is that's a batch method
meaning that you assume that you have
all the data at once so if you have a
hundred image sequence you're going to
assume that someone handed you all 100
images before you start cullman
filtering at least in this context is
similar to bundle adjustment
a way to minimize the bundle adjustment
error iteratively meaning that someone
hands you tend to see images to
initialize you do that with bundle
adjustment and then on images 11 12 13
14 you produce a intermediate result for
that image from the new data okay the
variable state dimension filter is
becoming more common now it's a
combination of these two ideas instead
of just finding a camera position for
the current image you'll have a sliding
window so if you initialize with 10
images you'll do bundle adjustment on
images on through 10 then when you get
image 11 instead of just finding the
position for image 11 you'll have a
sliding window so you'll refine your
positions for 2 through 10 and you'll
find a position for 11 and then there
are some methods that I think of as ad
hoc up some people would argue not but
they're what I call the two and three
frame methods so instead of trying to
minimize any error just say apply bundle
adjustment or epipolar geometry
estimation to two or three frames at a
time and then just concatenate those
coordinate systems that you found so I
say that it's ad hoc in the sense that
you know you can't point to some image
in the air that you're minimizing with
respect to my ok can you hear me in the
back all right so let me talk a little
bit about bundle adjustment because some
of my work on image and inertial
measurements is built on top of it so
from the tracking data again this is the
hotel sequence what we have our kind of
a 2d matrix and measurements there are
the positions of each point J that we
track in each of the images I and
suppose that from some magic source we
also have estimates of the unknowns we
want to estimate so for example the
camera rotation and translation at the
time of each image and the
three-dimensional position of each point
that we tracked given those estimates we
can compute the reproductions so that's
just taking the estimate of the
three-dimensional point and converting
it into the camera coordinate system at
the time of the current image using the
estimate of the rotation and the
estimate of the translation and then
once you have the estimate of the point
the camera coordinate system you can
just compute a reprojection of that
three-dimensional camera coordinate
system point into the current image and
now this projection operator I've
written it pi typical
this would be perspective projection for
conventional camera but some of our
works been on Omni directional cameras
which combined a conventional camera
with a mirror that expands the cameras
field of view so in that case this would
be something more complex but still
tractable okay so once you've done that
you have the observations which are in
red and you have these reproductions
which are in blue and basically what you
want to do is find estimates of the
camera positions and point positions
that minimize this error or some error
like it ok so what bundle adjustment
minimizes is the the square error
between the reproductions you know pie
etc and the observation xij with respect
to everything we're trying to recover
those camera positions and 3d point
positions if you do that for the hotel
you get this result that I showed you
and the problem is that you know even
for bundle adjustment which is this
batch kind of heavy hammer method the
estimation step can be very sensitive to
incorrect or insufficient feature
tracking camera modeling and calibration
errors so for example if you don't know
the cameras focal length properly if
it's got some poorly modeled radial
distortion it can be very sensitive to
outliers rush holds and sequences with
degenerate camera motions are still
problematic for this method okay in
addition an iterative batch method like
bundle adjustment can still have poor
convergence or fail to converge if
observations are missing which is a case
we're really interested in you know for
example if at any one time in any one
image we're only seeing ten percent of
the senior 1% of the scene and we don't
have a good initial estimate of what the
the full motion was that's a problem or
if the initial estimates pork okay now
recursive methods like the I EKF or the
recursive I'm sorry the Kalman filter or
the VSD f also suffer from some problems
so for example they tend to have prior
assumptions on the motion so you may
have some prior expectation of how fast
the cameras moving or how fast it's
accelerating and in addition you know if
these recursive methods basically what
you're doing is you're doing a
minimization over the current frame or
over the current window frames with
prior that results from you know all the
all the data that went before and you're
incorporating all the data that went
before into some distribution on the
camera positions and point positions and
for instance in the Kalman filter and vs
DF that distributions modeled as a
Gaussian which is grossly wrong in a lot
of cases can I think of the resulting
errors in two ways at least for long
sequences so there are gross local
errors if you have gross miss tracking
and you don't catch it you know your
camera motion may have been something
came like this and you estimate it to be
you know from here to Santa Monica
that's a gross error or us to mate a
point that you saw in front of the
camera to be somewhere behind the camera
that's a gross error in for a long
sequence it kind of shuts you down right
because you're doing basically odometry
and once you get a girl's hair and your
dama tree you're done for the day okay
and then the other thing is long term
drift so even if you work hard and you
do robust estimation to eliminate all
the cross local errors like I said
you're doing odometry so you're probably
going to accumulate some long-term drift
okay so this is just a nice example of
some of these difficulties this is an
arm in one of the robot labs at CMU and
we've attached a camera to the end of
the arm and we put it through this
motion if you would look at this motion
from above the robots workspace you'd
see kind of that it traces out a
cloverleaf pattern in x and y and then
it kind of alternates up and down in z
okay so this is the sequence that the
camera sees its 151 images I've tracked
23 points on it by picking out some
points by hands running Lucas kannadi on
it and then manually correcting the
results of that tracking and if you then
put that tracking data into Lucas
kanadia with a reasonable estimate you
get this kind of horrendous estimate
that's the the crooked black line and
what you're seeing here is the XY view
of the robots workspace so you see this
cloverleaf pattern that's basically the
correct motion which is that smooth dash
dotted line the seven boxes there are
the control points for the motion so we
told the robot to go through those seven
points and basically execute a circular
arc between them and then like I said
that that black erratic line is the
quote optimal estimate from from images
only which is the bundle adjustment
estimate and what's happening there is
that because there are so few points
because you see so few the points in
every image it's just really sensitive
to the small one and two pixel errors in
their tracking data and it's sensitive
to the small errors in the camera
calibration of meaning of the focal
length radial distortion and things like
that okay so that's just a brief kind of
crash course in motion estimation from
images and the kind of lingering
problems with it one way that we've
tried to address those problems is to
incorporate like I said that the epsilon
or at least possible or at least least
useful inertial measurements and do it
and I'll talk a little bit about that
okay so it's possible to just take
inertial sensors like a gyro and
accelerometer and integrate those over
time so if you integrate gyros which
give you angular velocities over time
you'll get rotations and if you
integrate accelerometers which give you
acceleration over time the integrated
twice you'll get positions so if you do
that and if your model the inertial
sensors is good enough and if the
inertial sensors the sensors themselves
are good enough you can do quite well so
I've been told you know I've never
actually seen it that submarines carry
inertial sensors that are in bold and
they're about the size of a side-by-side
refrigerator and using those inertial
sensors and 50 years of experience in
inertial sensor modeling
they're able to go from one side of the
planet to the other underwater without
coming up for GPS correction okay but in
a lot of our applications like the Mavs
you know it obviously are six inch of
vehicle isn't going to carry a sensor
the size of a side-by-side refrigerator
instead what we have are maybe sensors
that are about an inch wide so this
thing on the left is our rate gyro there
are experiments we've used three of
these they're single access gyros the
thing on the right is a cheap
accelerometer intended for airbag
deployment applications and cars and
that's also about an inch cubed that's a
three axis accelerometer and if you just
take these and try to integrate their
results once for the gyro twice for the
accelerometer you'll run into a lot of
problems one reason is noise it's just
they're noisy sensors and they're cheap
and every reading you get has some
independent Gaussian noise that will
integrate over time and then they're on
modeling nonlinearities so for example
you know you would typically assume that
the angular rates reported by a gyro are
some linear function of the voltage that
it that it all puts about which isn't
really true and then kind of an even
harder problem is that at least with the
accelerometer its output depends on all
sorts of things like its orientation
relative to gravity you know the actual
acceleration are trying to uncover the
slowly changing bias in the
accelerometer and so on and unless you
have some independent way of of
estimating all these unknowns you can't
even really start a fortunately image
Nerissa measurements measurements are
highly complimentary with inertial
measurements we can decrease the
sensitivity that I talked about in the
image only estimates too bad tracking or
bad calibration you can establish two of
the rotation angles without drift
because the output of the accelerometer
does depend on gravity and you can
establish the global scale so I've been
really talked about this but if you just
move one camera through the environment
you can recover the three-dimensional
structure the environment and the motion
of the camera but you can only do so up
to some global scale and the analogy
that I like to use is two eyes looking
at a tree all right so if you if you
know that your two eyes are horizontally
displaced and you see two images of a
tree
you can figure out kind of what the
shape of that tree was but unless you
know the distance between the eyes you
don't know if your eyes were six inches
apart you're looking at a model of a
tree or your eyes were three meters
apart you're looking at an actual tree
so on the other hand going on with this
complimentary discussion with the image
measurements we hope to reduce the drift
and integrating these inertial
measurements so for example if we happen
to see the same thing for quite a long
time or if we come back to a thing that
we've seen before that will work and
also gives us a way to bootstrap this
process of estimating all the unknowns
that the accelerometer measurements
depend on okay so my work on image and
inertial measurements has developed
batch and recursive algorithms for doing
this and I'll give you a little feel for
both of those first let me talk about my
simple model of the Excel of the
inertial sensors so the accelerometers
they're relatively simple we'll assume
that the angular velocity that we see is
the actual angular velocity modified by
some slowly changing bias and some kind
of / measurement independent Gaussian
noise the accelerometer measurements are
hairier like I said they're the actual
acceleration we want modified by gravity
rotated into the sensor coordinate
system and then like the gyro modified
by a slowly changing bias in an
independent Gaussian noise okay so with
that model of the sensors in hand we can
try to do a minimization analogous to
bundle adjustment and this is the error
term that I'm looking at it's a it's in
it's an error that combines image and
inertial error terms and the image term
is just a bundle adjustment error that
we saw before the inertial error term is
a little more complicated in its three
terms rotation velocity and translation
and as an example of how this error term
works let me show you the translation
term and I'll start with one of the one
of the terms in that some okay and this
is what that looks like so we're
thinking about the time between two
image readings so I minus 1 and tau I
and we've got some estimate that were
iteratively improving of the camera
position at time I so or the camera
translation in this case
time I so what we do is we take all the
inertial measurements between those two
times and we integrate them forward from
that first estimate of the translation
until the second time and then what we
do is we're also we also have some
estimate of the translation at that
second time that we're iteratively
improving so the air we want to minimize
is the difference between our estimate
at that time and the result of
integrating from the previous time um in
our prototype system we we took images a
kind of the standard frame rate which is
about 30 frames per second and we took
inertial readings at the highest rate
that our little acquisition board would
give us which was about 200 Hertz so
between two images were typically
integrating over say six or seven and
herschel readings okay so that was one
one term of the translation error and if
you look at the whole sound basically
what you get is a chain of these errors
and you just minimize try to minimize
all those errors jointly and if we
didn't have any say image measurements
to kind of get this process under
control you know obviously the solution
that would drive all those errors 20
would just be the solution that
integrates the inertial measurements
okay so that was the translation error
the rotation air and the last year are
similar but they're simpler because they
don't depend on as many unknowns now
that translation error is the one that
depends on the most unknowns it depends
on the two times which we assume that
will know all the inertial measurements
between those times which again will
assume we know the camera rotation
translation at the time of the previous
image which are unknowns that we also
estimate a raw bundle adjustment and
then the gravity direction the gyro
accelerometer biases and then something
we don't estimate in a bundle adjustment
which are the linear velocities of the
camera at which we need to integrate the
accelerometer and if we do that for that
robot arm example that I showed you we
get this which is the same chart that I
showed you before there I describe this
cloverleaf estimate is kind of the
correct estimate and in fact that's
actually the the estimate from the
search image and inertial minimization
or bundle adjustment
so now we you know we're interested in
doing this on live on robots so we also
built a recursive common filter or
iterated extended Kalman filter to do
this and I'll just briefly talk about it
so the dash dotted line here again is
the same batch or bundle adjustment that
I showed you before and then that solid
line is for this example the best common
filter result that I could achieve by
kind of tweaking the parameters of my
filter okay but I did encounter a lot of
problems of the iaaf for this
application these assumptions that I
mentioned at the beginning of the talk
the iaaf does require that you have some
estimate of how fast you expect the
camera to be moving or accelerating and
if that if that expectation is wrong
then you're kind of done for the day it
doesn't model the relative error between
adjacent camera positions which you kind
of want to know because we're using
relative camera positions to triangulate
the position of points and the reason
for that is that it only has one camera
position in the current state so you
know kind of based on these statements I
think a nice approach for the future
might be to take that batch algorithm
and convert it into a sliding batch
algorithm using the VSD f and that would
fix up probably both of these problems
and one example of kind of failed I EKF
result is shown on the right this figure
on the left is the one that I showed you
it's the it's the best I EKF result that
I could generate from this data and the
one on the right is is generated that
that horrific solid line there is just
generated by picking the the propagation
variances or expectation of the cameras
changing accelerations incorrectly okay
so just to recap the work on image and
inertial measurements our sensors are
images gyro and accelerometer we did a
batch algorithm which uses all the
measurements it wants an ax recursive
algorithm which is more efficient and my
thesis actually includes a very large
suite of experiments trying to evaluate
these methods and trying to salvage some
basic facts about motion from image and
inertial measurements so for example you
know given just image gyro and
accelerometer can we reliably estimate
the gravity direction
and can we reliably estimate the scale
and for instance one thing I found is
that even with Mike this may be on the
next slide no okay one thing I found is
that even with my trip cheap and
Herschel sensors you know my $200
accelerometer we could estimate the
global scale to about 5% all right so
image and inertial measurements is good
but one thing I did find was that adding
these epsilon inertial sensors didn't
really improve the system's robustness
to miss tracking so even my arm is
tracking even though we had inertial
sensors could still produce a pretty bad
motion estimate so i revisited the
future tracking problem and the other
issue is that even though inertial
sensors do help the robustness of the
situation of course the vast majority of
sequences that you're just handed or
that you may find on the web you know
don't have inertial measurements okay so
we tried to look at better tracking for
specifically 46 degree freedom camera
motion estimation and the result
remaining results in the talk will be
image only so kind of put up a berlin
wall in your mind between the inertial
stuff that went before and the following
okay so Lucas kannadi which I talked a
little bit about before has kind of long
been the go-to feature tracker for these
types of applications for example this
is one of the baseline methods
implemented in Intel's open computer
vision library and the idea Chester view
is that it minimizes an error like
correlation using general minimization
so it's using a second-order Gaussian
minimization and it evaluates the
matching area only a few locations so
raw correlation matching if you have a
640 by 480 image would be looking at 640
x 480 locations typically Lucas kanadia
if you have some reasonable initial
estimate of where the point is in the
second image may only need five or six
evaluations and one thing that certain
charm of Lucas kanadia is that it can
find your positions to sub pixel
resolution and what I call the folk
accuracy of Lucas Kannada or the the
number that people kind of site without
any evidence is about one tenth of a
pixel
okay so like I said look as kanadia is
this nonlinear minimization applied to
basically correlation tracking but just
that isn't enough to generate a tracker
right so you need to add in some other
glue to make that work you need to know
which features you're going to track you
need to identify miss tracked features
or features to become occluded because
of three dimensional structure or
features that leave the image and you
need some way to handle large motions so
you know Lucas canal use a nonlinear
minimization which means that you need
some initial estimate of where that
thing is if the image motions 100 pixels
well it's not good enough to just say
start where you are in the previous
image you need some initial estimate of
the hundred pixel motion and the
heuristics on the right are the ones
that are usually used with Lucas cannot
a basically the tracker that i'm going
to show you analyzes lucas kannada it
says all three of those heuristics are
wrong on certain important scenarios so
we're going to try to build a track or
specifically for sixth off motion that
fixes up those problems okay so this is
the same sequence that I showed you
before like I said this was a bunch of
points picked out by hand track of Lucas
kannadi and then there was a pretty
excruciating manual correction to get
the correct position of those 23 points
over 150 frames and the reason is that
if you look at the sequence there's a
lot of poor texture when there's some
texture on the ceiling tiles that you're
seeing there's also a lot of one
dimensional texture that isn't friendly
to Lucas cannot it in the bottom right
hand corner you see a the great on a
light there's a lot of repetitive
texture so you know every XY position on
that great looks the same to Lucas
cannot it and then if you remember the
sequence it kind of rotates pretty
wildly so it goes through about 270
degrees in 150 frames and that kind of
rotational motion is not very friendly
to the model that Lucas kannada uses
okay so smalls I call the new tracker
smalls its targeted specifically at six
Dolph estimation meaning that it
exploits the rigid seen assumption you
know you're moving your camera through
rubble or what the military people call
the urban canyon you know something that
that's basically a non
it eliminates those three heuristics
that I described that you normally use
the phookas kannada and then it uses
like i mentioned the very beginning of
the talk it uses this scale invariant
feature transform the David Lough
developed okay so it's four steps and
I'll try to be brief here the first step
is estimating the epipolar geometry of
every pair of images so will you sift to
establish matches between two images
tentative matches will estimate using
just a to frame bundle adjustment the
relative motion meaning rotation
translation between those two images and
then we'll get the epipolar geometry
relating those two images and what I
mean by that I'll kind of say quickly
here so these are the sift matches
between two images taken from Hyperion
in the Atacama Desert and Chile and what
you're seeing on the top and bottom are
the two images with the line showing the
correspondence is that sift found
between those two images and these two
images are pretty highly textured and
the motions not extreme so out of those
I think 100 matches one hundred percent
of them were correct which is which is
good but actually not a typical of sift
and then the set people or geometry that
I mentioned basically you've got some
points in the first image you want to
know where they were in the in the right
hand image and you know the geometry or
the motion between those two camera
times so what that tells you is for
every point in the left hand image just
back project that ray from 2d into 3d
and since you don't know how far away
the thing is a priori that gives you a
ray out into the scene but you know how
the camera move so you can reproject
that Ray into the second image and
that's what these lines are for these
five points and so you know where the
point is somewhere on that line but you
don't know where on that line it is
because you don't know the depth okay so
the second step then is to track along
that people are lines to determine the
depth and you want some initial estimate
so i use the position of the nearest
sift translation is the nearest estimate
and then just do a kind of basically
correlation search along that line and
then refine that position that discreet
position with Lucas
cannot a which will give you the say
one-tenth pixel accuracy okay the third
step is pruned away features so I prune
exclusively based on the geometric
consistency the features between those
two times so it's pretty straightforward
ok and then extract new features so
typically what trackers do is they try
to extract new features and texture
barriers areas on the assumption that
those are the features are going to be
able to track now what we found when we
track we constrain the tracking just to
the epipolar lines is that we can track
almost anything you know even a very
weakly textured point or say a point
that may be on the line perpendicular to
the epipolar line we can really track
without too much difficulty so we get
rid of the heuristic that demands
texture we replace it one that says give
us as much coverage as you can in the
image and the rationale there is that
ship for motion or six degree freedom
counter positions generally easier if
you have a wide field of view camera so
if you're letting your features off pump
up in one part of the image you're kind
of throwing away say 50 degrees of your
field of view what you don't want to do
so try to get coverage everywhere in the
image to say get the full 70 degree
field of view and field of view of your
camera ok so that was I know that was
very brief but here's an example of what
it does and this is a sequence that we
took by putting a camera on the back of
the Hyperion Rover when we were in Chile
and the camera in this case kind of
points 45 degrees towards the horizon on
the back of the rover and what this does
is it's intended to give us a view the
horizon so that we can see receding
points for a long time but also see some
points that are that are moving quickly
off to the left so that we can get some
good parallax and this is the result of
the smallest tracker on the sequence now
I know it's kind of fast one here I sick
when you're looking at tracking data and
image sequences it's kind of blur your
eyes a little bit and actually your your
visual system is pretty good at seeing
you know it's a little bit jarred by
inconsistent tracking and in this case
the tracking is almost a hundred percent
accurate
and you know almost to the point that
the points look like they're part of a
scene this all the stuff that you're
seeing on the border this stuff is a
like a haze filter that we put on the
camera it restricts the cameras field of
view a little bit the thing you're
seeing at the very top is Hyperion solar
panel and then this thing at the bottom
that kind of comes and goes is the
shadow Hyperion shadow
so you know the idea smalls is that
we're going to use it as organ is its
output as the input to our sixth off
motion estimation and now these are the
results of that motion estimation shown
from above so the graph on the left is
the ground truth odometry from Hyperion
and one reason we're doing sixth off
motion estimations that odometry does
drift over time but over the scale of
this example and given Hyperion's kind
of magically go to dama tree and we
found that actually that that estimate
on the left is almost exactly the
correct motion now the estimate on the
right is the estimate that we get by
using the smalls output fed into a
common filter and it's it doesn't have
any gross local errors but it does drift
over time and I'll come back to that
okay so just to recap smalls is just a
revisitation of this long time tracking
problem it exploits the rigid seen
assumption meaning that we intended
specifically for sixth off motion and it
eliminates the heuristics that kind of
limit Lucas conatus application to a lot
of real sequences it allows hands-free
tracking for real sequences so for
example at honeywell if we're writing a
proposal or something someone can just
hand me a sequence from honeywell's you
know new vehicle the lighthouse just
generates the right tracking without any
tweaking or any manual correction from
me but it can still be defeated by
texture listeria 'he's or repetitive
textures like this ceiling great and
i'll show you some examples of that okay
so let me talk briefly about this
problem of long-term motion estimation
which is really what all this builds up
to you know the image and inertial
integration oh and the smallest feature
tracker are kind of designed to remove
these gross local errors once you've
eliminated gross local errors how do you
move on and how do you really try to
estimate your motion over some
indefinite time with some accuracy okay
I think this is clear if you don't see
the same thing in the images all the
time you're kind of reduced to a dhama
tree because you're not estimating the
camera's position with respect to one
set of points this is the example again
from the Attic ama it's a great
temple for instance if you look at the
very top and the blue there you'll see
we're on two of the loops the rover came
back to exactly the same position and
when we look in the image sequence we
can see that in fact the images that
those two times are almost exactly the
same image so it seems that that's
exactly correct the estimate by piping
smalls into the common filter is the
thing on the right and for instance
you'll see there at that position they
don't actually come together there's the
like a two or three meter air there okay
so to limit drift we need to do two
things we have images so we need to look
in the images to see when we come back
to a place where we've been before and
we need to exploit that in the second
step which is the estimation step I
built a kind of proof of concept system
two tries to demonstrate these ideas now
I know this is a lot of text but it's
actually just a few things the system
maintains a state that's a set of image
indices say maybe an image image indices
1 through 100 but generally not at some
subset of all the images we've seen so
far it maintains a small stracker state
for the 2d tracking data for just those
images so if the image is in the state
our image is 1 through 50 and then
images 75-300 you've got smalls tracker
output for those 75 images then there's
a variable state dimension filter for
the images in there and again you know
if you've got say 1 through 50 and 75
through 100 you've got 6th off camera
position estimates and three-dimensional
positions of the points you tracked for
75 images and then the last thing are
just the sift key points for the most
recent image which we're going to
extract and try to use to see if we've
come back to a place where we were
before okay so it's just a really brief
example suppose we've been through the
first nine images in the sequence images
0 through eight at the end of that you
know our state will contain nine images
0 through eight and in the this doesn't
have to generally be true but in the
proof-of-concept system that i built i
divided the states or the state sequence
into roll back images and non roll back
images and the rollback images are
basically just the states that you know
as we go on through time we're going to
consider
as possible revisitation points so for
example if we're up to image a and for
some reason we suspect that we're back
close to the position where we were at
the time of a Mitch 3 then we'll reload
the state for image 3 and we'll try the
smallest tracking and the vsd F
estimation into image 8 okay and then if
we decided for some reason that the
estimate that was produced by those five
images 0 through 3 and 8 was better than
the estimate we got by integrating all
the way forward from 0 to 0 to 8 maybe
will prune off that relatively poor
estimate from nine images and we'll just
take the 5 image estimate okay and then
you can continue to do this you know if
you continue to take every third image
as a rollback image then you build a
kind of tree over time that shows kind
of the best way to estimate our current
position using all the data we've seen
so far and by kind of opportunistically
rolling back okay and then the question
is when to roll back so the process that
I used which is efficient but well not
too efficient it could use some more you
know it's a proof of concept is a look
at the the estimated covariance we have
for the current position and look back
at our rollback states and kind of
immediately throw away those rollback
states where the camera covariance is
larger you know rd is to is to minimize
the error in the camera position so we
think the air is larger in some previous
rollback state it's probably pointless
to roll back to that higher state then
we do sift matching between the current
image and all the surviving rollback
states if the number of sift matches is
higher than some threshold i think it
was 300 in the example i'm going to show
you than we then the things where the
number of sift matches is higher than
the threshold survive we then extend
from the candidate state being the
possible rollback states into the
current image and then once we've done
that we reexamine the covariances again
so for instance if we've tracked from
three to eight we examine the covariance
the results from that roll back against
whatever our previous estimate was
and I should say you know we take you
know for if we considered 0 3 and six as
rollbacks you know we roll back to what
image produces the smallest covariance
okay so I've got a nice example of this
what I did was I came into the CMU field
robotic Center late one night at like
2am when nobody was around it was after
a poker game but I was sober and I took
940 the number will come up it's 945
images I think and these are some of the
images from that sequence so the first
one is kind of looking at one of the pc
workstations second one is a the door to
a bridge between two buildings here's
the Hyperion work area and then this
thing on the right is a like an overhead
door in the high bay and then this is
the floor plan of the high bay that I
worked in and what this what the
sequence does is it does a what I call
the first forward pass it's the first
213 images we start there on the right
by the computer workstation we back up
and we move forward to that door we back
up again and we go a little further in
the high bay and we we kind of move up
to the Hyperion work area we back up
once more and we go out to the overhead
door and that's the first 213 images and
then in the subsequent 160 images or so
we do what I call the first backward
pass so we back up basically the whole
way and then there are four more passes
two more forward passes and two more
back or passes that are similar to these
four totally 945 images the question is
you know it's a relatively long motion
we see a small bit of the scene at any
one time you know can we really
accurately estimate our motion over that
time now before I show you the video one
caveat is that like I said generally
this idea rolling back and produce a
kind of general tree of states in this
case for clarity and because my
implementations are relatively slow
prototype implementation what i did was
i looked at the sequence and i said well
we map the whole place in the first 213
images so i'm not going to let it roll
back
213 images but in the subsequent nine
hundred and some images I'm going to let
it roll back only two states within the
first 213 and I've kind of schematically
showed it there as every third image
being a rollback state but in fact I
used every tenth image for this example
okay now the movie shows you the smalls
tracking output on the bottom for the
sequence and then on the top you'll see
its estimate of the sixth off camera
motion and the three-dimensional point
positions viewed from above so on the
far right you'll see when it starts out
during the first 217 images it Maps the
whole place with the camera positions
and their covariances in red and then
it's estimated three dimensional point
positions in yellow you know again
viewed from above now during the the
other five passes the the three roll
back passes in the the two forward
passes what you'll see there is the same
global map that it constructed during
the first 217 in white and blue and then
just the current state in red and yellow
so for example here what's happening is
we're up to image 301 but it decided it
could roll back to 110 so we have as a
state that's the first hundred and ten
images plus image 301 and the red and
the yellow are the camera positions and
the point positions that that we can
derive from those okay let me show you
that
okay so it starts off like I said by
this workstation it goes to this door
going to the other tunnel and yours
again you're seeing the 2d smalls output
on the bottom and a top-down view of the
three-dimensional position on the top
there are the three dimensional
estimates it's getting close to the end
of the first pass now it gets out to the
high bay door and then it starts this
first backward pass we're kind of
retraces it steps if you watch the top
occasionally it will decide it's close
to some previous image it rolls back and
it you know produces an estimate with a
better camera error than what I had ok
so that was the end of the first
backward pass we're on the second
forward pass and you know this process
kind of continues it error always kind
of accumulates there's a little
different on every pass and if it thinks
it can opportunistically reduce that
error by rolling back it does it now
like I said a little early on the talk
I'm smalls actually can be still
defeated by a really bland area or a
really repetitive texture in the images
so for example coming up I'm you'll see
that it will hit a kind of grid wall
right here there's a big area where it
can't really track anything accurately
so at that point it's kind of defeated
and it throws up its hands and it says I
have no idea where I am until it hits an
image that it can roll back to the
overhead door is another one of those
areas and some of the images basically
all it's seeing is the overhead door
which is almost purely a one-dimensional
texture okay and that's the last image
944 you know we've come back to the
workstation you know and because you
know can roll back to image 0 or image
10 it pretty much knows exactly where it
is even though it's 940 images later
okay so let me just conclude you know
this entire system is it was a lot of
chunks actually when you think about the
image based estimation the image and
inertial estimation the smalls tracker
the rolling back it's a pretty large
code base and it's all done in C++ by
hand so none of its really optimized you
know for a real system like a map or
something like that you'd want to run
this in real time so you one have an
optimized system like I mentioned
earlier in the talk the invention
inertial online algorithm which is a
common filter probably should be
replaced with a V SDF for sliding bundle
adjustment type method that fixes its
problems with smalls you know we have
the inertial data one way to specialize
smalls that really be helpful would be
to incorporate the gyro data now the
inter accelerometer data is harder to
incorporate because it you need to
estimate more things but a gyro is
actually pretty simple instrument to
incorporate so with a little bit of pain
we could do then get better smalls
output and then with this long-term
system I've done a lot of heuristic
things for the rolling back process that
should be fixed for scalability okay and
that's the end I just want to say thanks
and then i'll give you the URL again for
these online materials and i'm happy to
take any questions
ok
yeah the question is in bundle
adjustment higher represent the
rotations and I've represented the
rotations using Euler angles Euler
angles which are kind of the minimal
three representation for 3d rotation
they do have some technical problems for
instance some rotations are represented
by an entire space of equivalent Euler
angles which is an ideal for those types
of applications we probably want to
switch to coach Arians or something like
that all right well no
and camera
large lots of places
that's yeah well we so the question was
is it is it correct that by adding an ER
shal data into the estimation that we
didn't really improve the robustness to
stay large motions or losing our
position actually that's almost it's
almost exactly right really the problem
was on a on a finer level now is that
even it sometimes we if we miss track
between individual frames just move on
frame to the next that's a killer for
bundle adjustment or a common filter
image only method and we found that
adding an ER shal data didn't really
give us robustness to even that smaller
problem and and certainly with our cheap
sensors the issue you address which is
if we lose our place over a longer span
of images you know or we're kidnapped
where we don't know what position we
might be at at all certainly with our
cheap inertial sensors we can integrate
over such a long interval to fix that
problem so that was it honey well we've
been talking a little bit about you know
honeywell's their world experts on
inertial navigation and so we're talking
a bit more about trying to use some more
real hardcore inertial sensors to fix
that
with my hospital of the time
ok
yeah so the question is roughly if you
have an erratic motion that type of
quick short term motion seems to be the
type of motion that inertial sensors
would actually be very good at and
that's true the only advantage we lose
in that case is that you know we're not
we're not exploiting the images in that
case to try to minimize the inertial
motion we do a split the the joint
estimation problem using image and
inertial measurements into basically two
separate almost two separate estimates
that are joined by an inertial
integration which is fine but we can I'm
happy to talk more about it so I haven't
the question is have I experimented with
subspace tracking and I haven't done
that for 3d no I haven't done anything
like that
okay well thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>