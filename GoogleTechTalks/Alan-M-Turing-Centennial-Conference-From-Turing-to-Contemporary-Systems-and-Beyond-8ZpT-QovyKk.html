<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Alan M. Turing Centennial Conference: From Turing to Contemporary Systems and Beyond | Coder Coacher - Coaching Coders</title><meta content="Alan M. Turing Centennial Conference: From Turing to Contemporary Systems and Beyond - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Alan M. Turing Centennial Conference: From Turing to Contemporary Systems and Beyond</b></h2><h5 class="post__date">2012-08-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8ZpT-QovyKk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so in this presentation I want to start
by looking back at touring and i'll make
that somewhat brief but i want to put my
spin on what he did and perhaps what he
didn't do as it relates to modern
computer science and then i'd like to
pick up on that and try to look at what
we are doing today perhaps beyond and
extending what he thought about beyond
what he did in to some degree trying to
to fulfill some of his goals and
artificial intelligence and kind of look
at the future from that perspective so
that is my goal here so my references
for this beyond my normal thinking as an
executive and research at Google are
these I assume these materials will be
on the web just so you'll know there is
an incredibly complete volume on all of
touring's material with that long title
the essential touring dot dot dot
there's an enormous amount of material
on that on on the breaking of enigma on
his theoretical work his later work and
morphology and biology etc there's a
very new book that just came out that I
was able to get just at the last minute
that's come out from the british
computer society that's a discussion of
all the machines that were built in the
early days very much from the focus of
the united kingdom but it's also an
interesting one one thing that i urge
you all to take a look at is the touring
archive which is online and there there
are photographic representations of many
of his papers so that you can see his
papers with his own mark up his own
handwriting on them and that's fun
google was very pleased to have
contributed money to purchase these at
the at Bletchley Park so I'm not sure if
these were because we've contributed
money that they were available or we
have additional ones but Bletchley Park
now has a large set of those papers and
of course Wikipedia is always great
those are my references so we know about
the date this was meaningful to me I was
able to put it in perspective my father
was born in nineteen twelve i was born
in 1954 so it's sort of an interesting
coincidence for me
there were many dimensions of this of
this man as we certainly heard about as
a logician and a mathematician um he was
a crypt analyst he was an engineer and
he's clearly a pioneer in artificial
intelligence and then on a different
dimension he was a hero having one and
been awarded the order of the british
empire as we heard earlier so a man of
many dimensions the paper which to me
sets the standard for the maximal number
of least publi these publishable units
combined into one document is on
computable numbers which i read and i
urge all of you even if you take in the
course to find it on the web and leaf
through it even if you don't read all of
the notation it really is the you know a
very significant portion of the first
course and computability that you take
it's not everything we've gone beyond
that but it's a tremendous piece of work
and it certainly demonstrates to me that
you have we have to give immense credit
for touring for the foundations of
mathematical computer science there's
just no question about it however if you
look in the document while we might
think of it as mostly a mathematics
document I think there's the foundation
also of an engineering discipline within
it and I think that's a very important
part certainly the fact that he defined
the touring machine so precisely and so
elegantly as an engineering activity as
well as a mathematical activity and then
there's this quotation in the document
actually there's slightly different
notation that I had trouble doing in and
in the presentation program that I had
but the the basic idea is the paragraph
says which you can read yourself is it
describes in one paragraph his concept
of the universal machine which is what
we think of as the modern computer so
that's an extremely important
engineering contribution but also he
thinks about subroutines so there are
these notions of what he refers to as
subsidiary tables which are subroutines
I don't know if it was because he needed
something like that because he's a
mathematician and wanted economy of
notation or because he was an engineer
and needed economy
of storage or economy of representation
just because it was onerous to have to
do things many times and keep copying
materials but the whole notions of
method invocation or procedure call or
whatever is certainly based in the
notion of saving memory and economy and
engineering initially eventually
developing into more techniques for
abstraction and readable programs so we
get aspects of both him as a
mathematician and engineer in that in
the domain of crypt analyst he combines
many things so he has to combine
mathematical analysis he combines
engineering and he is involved as an
engineer designing these systems and
he's clearly involved in heuristics
there is no clear exact solution to
doing his code breaking he has to use
heuristics and they're very interesting
approaches you may hear more from Korea
on that up so I took this picture i
happen to have been in Paris I was at
the lays in the lead which is the Museum
of of war in Paris I just with my kids I
happen to walk by this enigma machine on
the left so I grabbed a picture of it
thanks to modern computing the one on
the right I took off the web that's the
machine that was used to break the codes
that were done on this machine as an
engineer further he did demonstrate
engineering capability with his proposed
electronic calculator specification for
the machine called ace ace EE and this
was done at the UK National Physical lab
he had seen von neumann's report on EDD
back when this was done so I think in
terms of you look at where things were
done first we can see that on the other
hand von ointment had seen his work on
universal machine so in the grand
tradition of standing on each other's
shoulders this is how the science are
progressed it progressed so interesting
the engineer here made the decision to
focus on economy of implementation not
simplification of programming so this
system was designed to minimize the
number of vacuum tubes forgiven
performance but it was exceedingly
difficult to program
and like some engineers some of them we
may know he kept there was a lot of
change that was going on as it was being
designed and as you know when you start
changing specifications very hard to get
anything finished this didn't finish all
that fast in fact I'm not certain that
the full machine was ever built a small
version of the machine was built in this
the so-called pilot ace but any case as
an engineer we are always faced faced
facing challenges of deciding what the
right engineering trade-offs are for any
given technology look at risk versus
sisk as a notion that happened in the
same 1980s as a major argument in the
development of systems risk to minimize
the number of transistors and minimize
cycle time at the expense of
programmability for sure he made the
same kinds of decisions and his pilot
ace was ultimately with 1800 to 14
hundred and fifty vacuums I got
different numbers are from different
points in the literature so I don't
actually know which it was I
interestingly enough it had subroutines
and had the best first class support for
subroutines something all of us in
modern computing depend upon and
floating-point was added to it because
they worked with the very good early
numerical analysts that we're looking at
how to deal with the fact that numbers
quickly outran the range of 32-bit
integers or 40 bit integers so it was
able to they were able to add floating
points what I gather that was the first
use of that so he did eventually focus
on programming a little bit particularly
with students he did write some sort of
a language for the specification of
mathematical formulas or with a student
maybe the student did more than he did
but he did a little bit more in that and
finally there's artificial intelligence
so this is um she's remarkably visible
let's hear it for projectors these days
I'm surprised it's so visible this is a
discussion near the end of a document
that he wrote on playing chess was
nineteen fifty three or so and he wrote
in connection with my question number
four above that is the ability of a
chess machine to profit from experience
which I would argue is learning right
that's a definition of learning we can
see that it would be quite possible to
program
the machine to try out variations in its
method of play and adopt the one giving
the most satisfactory results this could
certainly be described as learning
though it is not quite representative of
learning as we know it so there was the
issue here as he clearly understood you
could see bad moves and not do them
again and maybe see good moves and do
those again but he wanders a little bit
and you see this on the next piece of
Correspondence the degree to which one
can generalize so this is the most
simplistic form of learning if we just
learn a specific item can you generalize
well Christopher strake he wrote to him
in 1951 based on an earlier paper that i
don't have up here and engages in a long
discussion with touring i won't go
through all this about the ability to
generalize a little bit so you can see
very early in the 1950s well before
Samuels is playing checkers with a more
advanced form of machine learning I
would say there's the beginning of
machine learning although just the
beginning and certainly not like what we
practice today so he didn't so in my
opinion he certainly didn't address
large scale a machine learning like we
heard and discussed and periphery in the
previous presentations and will be
discussed later today and he certainly
didn't think about vast open systems
that we have so I'm you know I am amazed
by the number of hundreds of thousands
of programs that are available for
Android phones and iPods and iPads and
things like that I don't think that was
near his consciousness how we would
create a sort of democratically
accessible ecosystem of computing where
all of these programs would be able to
in a certain degree interoperate and
cooperate together without having been
defined at one point in time with a
single objective so I think that's been
a very big change in the field so as I
put all this together key contributor
and the mathematical nature engineering
nature a major contributor in the
empirical nature that is to say the
learning of the field this empiricism
that's now part of computer science just
a little bit and this large-scale open
systems realm not at all
all and then finally an AI motivation
huge right he really was a major
proponent for AI it was a very
significant contribution now just
looking at the change in the field I was
able to use a little bit of empiricism
in the construction of this chart this
is the google books and graham viewer
how many people are familiar with this
in the room okay about twenty percent so
what we do is we sample n-grams
collections of words from the google
books corpus approximately 20 million
books have been scanned I forgot the
number of billions of words and you can
choose different languages many
languages and and smoothing etc this
shows if we look at in yellow the
occurrence of computability which grows
over time and you can see it begins to
grow sort of in the 1960s when this work
becomes more prevalent in the world you
can see the growth of computing machines
as terminology term he would have used
begins in the 1940s and and then peaks
as the word is really replaced by
computers you can see software
engineering sort of the engineering side
of computer science really take off in
the 1970s right machines are available
you many computers or a huge number lots
of people are writing about it it's not
just for men and white and women in
white coats it's commonly used and then
finally you see the growth in this sort
of empirical nature of the field machine
learning only really begins to happen
and happens at the right time in this in
the early 1980s when people start
writing more about neural networks and
such things like that so it's not
surprising that that touring wasn't
really part of this of this realm and in
the in the 30s and 40s in empirical
computing so with that now the question
is where do we go based upon that so in
my opinion um we're in an amazing field
almost all of us in this room many of
the people perhaps that will be watching
this or computer scientists we know all
know how how amazing it is but it's
amazing for at least two reasons one is
the
core of our field is continually
changing so we have enormous
opportunities in a vastly change in core
of computer science and then as the
sphere keeps expanding as our field
keeps expanding it keeps bumping into
other fields and there's a hybridization
that goes on there's a natural
connection between our field and I think
every other field in the planet where
our field makes the other field better
and probably vice versa so it's a
fascinating part of it i'll discuss both
of these so as i look at this what's
changed a lot safe from my university
training in this field this is
considerably after touring but open
systems this was i remember when i was a
student in 1972 it was a very big deal
to build a programming language that
could run on multiple instruction sets
that was that that was sort of the state
of the art that was really good to be
able to support portability across
multiple computers with the same program
distributed systems in parallel
processing was very new at that time and
really distributed systems as a comer as
a software field came of age beginning
in the 1970s that led to cloud computing
as we scale things up by another safe
orders of magnitude or maybe more
information retrieval well it was
beginning at that time but it really
took off later so salt ins work at
Cornell and others was beginning but it
really took off machine learning took
off after that time as I've already
discussed I must say in a security and
privacy world most of us most of us did
not worry anywhere near as much about it
the reaction of most of us at the time
is yeah we know we should never type
anything into a computer that we don't
expect to see on the front page of the
New York Times but that was easy to do
and we also thought who would really
want to break into my email well if you
operate the Google Cloud a lot of people
presumably are interested in in breaking
into things there have been many new
foci for algorithms work certainly
relating to scale and parallelism user
interfaces have dramatically grown and
one can see that by the ratio
of the number of pages describing how to
use a program to the program we very
rarely read a manual on how to use a
program anymore because the interfaces
are so good it's a remarkable change in
you I a and user interface methodology
and human-computer interaction and
they're more things we could talk about
robotics and other things but those are
some of the major changes that I've seen
so far a lot of them driven by
prodigious pneus by just great scale and
everything we have to get used to the
words on the right hand side I envision
that in systems we must be thinking
about how to program cloud computing
systems with a billion CPUs we're not
that far away maybe you know with the
expected growth that still is coming in
the miniaturization of transistors say
verticality and transistors we should be
able to get two billion node systems
because of lots of processors on a core
within 10 or 15 years so that's a huge
management issue the network today is
capable of carrying 32 kilobytes per
second to every want to a billion people
on the planet a separate 32 kilobyte
stream to everyone on the planet that's
the kind of scale that when 32 kilobytes
is enough for relatively high frequency
radio so one grand challenge we have to
face is how do we program these clouds
of these many CPUs given security given
resource allocation optimization program
ability and availability we heard issues
of systems in one of the talks that we
just heard we know how hard it is to
build systems that are reliable and that
are available that are there when you
need them when they get to this scale so
there's going to be an enormous amount
of work in that space second one is that
if we fuse many of the AI Grand
Challenges of the world together and add
a few more we end up with a goal of
really saying can computers break down
all barriers to communication for people
based upon either finding information
that's been published or written before
or actually communicating amongst
ourselves so we think that will become
increasingly feasible it is to a degree
already but
can we make sure that no matter what the
device whether it's some specialized
healthcare device or a telephone no
matter what language that we speak
across all human languages no matter
what form in which we are presenting
information whether an image or text or
audio or video and across all corpora
that have been written and that exists
can we automatically transform
information across all of these domains
and it might seem like this is you know
completely out of the realm of
feasibility but it's actually not right
so for example we do voice to text all
the time that's voice recognition voice
search is extremely commonly used on
Android phones and now on Apple phones
where people speak into the phone and we
translate it into text the text is
searched against the text on the web and
results come back image to image is
commonly done find similar images is a
very common way of navigating among
images on google and our competitors so
we do all of these transformations to
some degree they will be done ever
better and it is a extremely interesting
journey to do as well as the best humans
and each of these things some of the
algorithms before I gave a somewhat more
technical version of this talk I asked
our team in New York to get me a list of
some of the algorithms work we're doing
where the algorithms focus on scale on
processing billions of entities or tens
of billions of entities at millions or
transactions per second and some of the
ones come up on the line that are
listing up here but one of them relates
to find similar images there's a
streetcar in San Francisco a cable car
how do you find across all of the images
that people have uploaded at Google who
knows how many there are the other cable
cars in that group with today's
technology given that we're really not
good at symantec labeling of everything
in the broadest sense so we can't just
like say here are the five obvious
semantic labels let's do a search across
those semantic labels although that's a
goal and I don't think we're that far
away from it in many dimensions now
turning back to this notion of
empiricism and learning and I think in
particular because touring was so
focused on artificial intelligence the
one thing we have really learned for
sure nai is that if we practice machine
learning at scale and information
retrieval at scale our systems can get
ever more intelligent the more they're
used I paused for a few seconds because
i think it's an amazing result that
that's proven to be true so how many
people have used spelling correction at
Google or perhaps at a competitive site
it's okay competition is good and how
many people know how it works so a small
group so they're really two approaches
right so one is we could hire linguists
and every language around the world and
we could be scouring the world for new
words that are entering the lexicon and
each language and then determining by
some heuristic what are likely
misspellings we know people interchange
keys we know people mistake certain
letters combinations etc and they double
consonants when they shouldn't be
doubling consonants and like so we could
go do that and we could probably employ
two people in every language all around
the world to go do that and we would be
reactive and a little bit late and
picking up on things the other approach
as those of you that raised your hands
knowing how we do it is we continually
have our systems monitor the log of all
search queries and in the search query
log there are clearly times when there
are sequences that are typed that have a
small edit distance between them they're
almost the same and they often have the
situation that the first sequence one is
not does not produce a search result
that people like and the second one may
not and the third one but the last one
in the set produces a search result that
people like and since that they're all
similar one can infer this
or at least say this is good evidence
that the fourth one is the correct
spelling of the first three and we use a
number of algorithms and doing this it's
actually very elegant the way it's
actually done but effectively we look at
these the we compare these sequences and
we learn spelling and we learn it very
quickly because people that don't get
the right result they figure it out
people figure out the right spelling
they look in the newspaper they try
something else eventually it works we
humans are good at eventually filling
out the rights finding the right
spelling so that's an example of a
system where the more it's used the
better it gets all right so now with
that in mind um we do the same thing
with speech recognition today so on some
phones today speech recognition improves
the more it's used because people
correct the errors more on that later as
well and how how speech recognition is
so integral e tied with with machine
learning you'll hear more from corona
who's speaking later in the day I think
on that subject so in general we have
this notion that we can create a system
that works if we get feedback from users
it will work better and that will
probably make it useful to even more
people and that virtuous circle can
continue for a very long time I call it
hybrid intelligence and I think it's a
little bit different than what people
had assumed in the earliest days of
machine learning and AI where we thought
maybe we'd have a few experts teach a
system now we can have the entire world
teacher system it's not just a few
cooperating experts and it's really a
fantastic result and not one that I had
anticipated when I was a student it was
completely distant now we have another
grand challenge that I think I know at
Google we're trying to solve and I think
the world would like to solve it it
really is needed if we're going to do a
really perfect job in dealing with the
transformation of information with high
accuracy and that's understanding
semantics of information so how do we
understand what we mean by a paragraph
how do we understand we mean by an
image or a video how do we summarize it
how do we gather that now my take on
this has been for now about 10 years
since actually this world wide web 2002
and Honolulu talk that I gave is that
all of the proponents of different
approaches to semantic understanding are
right that it isn't the Samantha sis
versus the sin tax experts versus folks
that use machine learning without
particular structure everybody is right
and that we're going to apply all of
these proaches in combination to reduce
in a sort of systematic way the errors
that are inherent in understanding and I
say reduce them because we'll never
eliminate them because things are
subject to multiple interpretations and
the best of times by the best of humans
so what I mean is that will apply one
algorithm to producing some degree of
understanding of something and that will
reduce that that'll be approximately
right they'll be lets say a five percent
error in that and then we'll apply
another algorithm to that result set and
that will further reduce the error and
so maybe the remaining error will then
be will have reduced the error so 99% of
that will be right etc and we'll keep
applying techniques until we get our
error rates down to what we feel is
possible so we'll do semantic annotation
many ways we will find of adding to
information what we mean by it so we'll
know that a is a pet will know that
Sonny is a type of weather and many
things of that form so these semantic
annotations will be added by authors
they will be added by people that are
contributing to open source projects or
professional raiders or they'll be
learned from systems and I don't know
how the fact it's going to be a mixture
of many different things as to how that
semantic annotation of information will
be done syntactic annotation will be
done that will probably be done mostly
by machine
since rather be boring to be done by
human but we can do it awfully good job
now with parsing so syntax I think does
play a role and I think Chomsky will be
born out that it's useful not that it's
the only thing and then machine learning
plays a role in doing these activities
and how it is we do annotations but also
I think machine learning will play the
role and how we complete combined all of
the features that we have about
information we're going to have a
plethora of features what is a dog there
are many things that a dog is which ones
are applicable to a situation that we
have and we will learn those and just as
I think we learn those if if we're a
veterinarian things we may know about
the poetry of dogs may not be relevant
if we're treating a dog but if we're
poets what we might know as a
veterinarian may not be relevant to the
poetry of dogs so a lot will be learned
by the situation just as I think we do
all the time so my overall concept here
and what I think is being proven to be
true is the combination of techniques
with independent types of errors will
result in rapidly more accurate
semantics or understanding perhaps the
best piece of evidence for this now I
think is the Watson program of that IBM
did that one in jeopardy saw reference
to it earlier today it was based upon a
system called you eema developed at IBM
many years ago starting developed in the
about ten years ago and they built on it
a huge number of different approaches
all of which are combined with machine
learning at the top to decide which of
the signals are most relevant in
jeopardy and we do many similar things
as you would guess at Google as well
what did I do with the clicker I have a
different failure mode than you do
Michael I lost it here we go all right
so i mentioned parsing we are doing
parsing at scale at google this is not
unrealistic at this point this is
something that is ongoing we're quite
good at it and we're going to get much
better we are in fact paying for more
training data for our machine learning
algorithms
we are developing techniques we have
developed techniques where we can run
this on large numbers of processors to
develop the actual systems that do the
parsing and we have to be able to do
things very fast we can now process
according to this slide and I believe
true a thousand seconds per second so we
have sentences per second we have to be
able to do get huge amounts of data
uploaded to Google all the time it's no
good to be able to theoretically say
let's parse stuff into practically you
know run aground when we can't do it so
lots of opportunity we use it in machine
translation so despite the fact Google
is known for doing machine translation
across about 3,600 language pairs as the
leading machine translation site now by
far and despite the fact we're known for
doing it with statistical machine
translation we use parsing as one of the
signals particularly with respect to
verb motion in many languages the verbs
are near the end of sentences and we
need to move verbs around to make things
sensible in Russian or in Japanese or
Korean German so parsing has been the
key to that it's a beginning it's an
application of this combination
hypothesis um we bought a company that
has a system called freebase you should
go play with it it's on the web if you
want I'm told the number of entities has
vastly increased since this 25 million
maybe 5 or 10 times more than that so
the entities are things like you know
people or authors or books or company or
location etc those are the types of
things and we're trying to draw the
connections across all of these things
and we're trying to do it with very high
accuracy and by the way not only do we
have to do it with high accuracy we have
to maintain it with high accuracy
because things change right people are
no longer in office and political
parties go up and down and things change
so this is quite quite a large activity
but you can imagine if you believe in
the combination hypothesis this is going
to be an important source of information
about a word that you might see in some
parse in my view for
sample if we look at voice recognition
and that system today tries to
transcribe pet log to PE TLO g the
semantic analysis might raise a question
and say don't you really mean pet dog
and the semantic system eva the the the
speech recognition system will look at
that and say gee it really sounded like
log i'm pretty sure it was log but i
have a very strong vote in favor of dog
from the semantic analysis how does that
get resolved in my opinion I think the
machine learning system gets experience
with what's correct based on user
acceptance and it probably determines
people never say pet log and that there
was some noise in the room when it was
being recognized and it pushes the
recognition in the way that we humans
would probably take it um we can even
begin to now do finally after many many
years of work enough enough natural
language processing to get very
significant information out of the
structure of a paragraph so this is work
maybe many of you have been involved in
it it's been a very large effort by the
community broadly to do natural language
processing but in this here you see that
Eric weisner will talk about his parents
his refers back to Eric weisner that's
important because his research is Eric
wise nurs research that's important to
know for many reasons you can just
imagine in a naive search engine that
would make it possible if someone types
eric wise nurs research to do an exact
match on the text if you had that but
there are many more things that we can
do today by pulling out entities by
relating pronouns to their antecedents
and more in all of this there's a lot
that's going to be done in in
visualization there's so much
information available that we need to be
able to visualize more I don't actually
know if this is set up to is this on the
web this machine
so you can tell this is live at least
right so it's translated that's good
cool so here we have an example of a
posting by this lady called Amanda
Blaine and you can see in real time how
that post is distributed through the
google plus social network and you can
see that it's posted to some people that
consume it and then show it on their
site or that repost it to others and you
can see how it happens over time we
could mouse over this and spend more
time of course the goal is to make this
available to the user community as well
so it can't be the case it shouldn't be
the case that the Google's or
Microsoft's or IBM's have this
technology internally if you believe in
the expanding sphere and the importance
of our field to every other we have to
make learning and data and scale
available to others so we've done this
you can look at the prediction API which
simply lets you upload some data that is
training data for the system to generate
machine learning systems from that
training data and then you can pass it
new data and see what's predicted it's
actually quite easy just as an example
to go build at least a rudimentary spam
filter based on this approach but it can
be used in many other ways also so this
will all get externalized as well as we
move from the core to the periphery of
the field so I'm going to go through
this quite quickly but I think you'll
see some ideas that are important so the
first is I think that this notion of
computer science with other fields CS
plus X for all X is extremely important
we've had well-known impacts in many
fields but there's much more to happen
in the digital humanities and Social
Sciences and other places where you
wouldn't think of computing being so
important I already mentioned the Ngram
viewer and I showed you its use in this
talk let me just give you two other ones
that are interesting so two guys at
Harvard worked with us on this what they
did was they showed that you could see
the rate at which irregular verbs
because
coming regularized in English that
should be a pleasant thing for everyone
for whom English is not a native
language that the rate at which these
verbs become regular is inversely
related to usage so if usage is very
high than they never become regular eyes
and if usage is low they become
regularize drathir apid ly burnt is a
unused form relatively speaking in the
language and it's becoming regularize
daz burned this one is very interesting
from a notion of analyzing history and
political science and when the United
States was formed Jefferson had a very
significant impact through the federal
through the early years of the United
States and arguing that the United
States is really a Federation of
separate states so the term in most
commonly used in in English is the
united states are such and such after
the civil war in the 1870s and 80s
there's a crossover point when the
united states becomes more monolithic
and it becomes the united states is so
if you haven't explored this you can
actually spend many productive hours
looking for hypotheses in the world i
did the touring one you know in 15
minutes then the afternoon on saturday
thinking this would be fun to show when
the rise of machine learning and
empiricism took place so you can do that
as well in healthcare we can do a lot
more in search we've started symptom
search at google recently so you can
just type your symptoms in and we can
then go back and see where have those
symptoms occurred and other people
search results and then tell you the
names of the diseases that they typed to
get those symptoms interesting kind of
approach if you do large-scale learning
you can possibly predict outbreaks of
the flu and other diseases as well by
correlating the patterns that people
search with the outbreaks of the disease
without any a priori knowledge that
people might be chewing fever or malaise
or whatever you we can just learn all of
that and it may very well be useful for
bringing drugs to the right place in the
Geo world's it's possible to go from
tape
rules of information with maybe geocodes
two extremely interesting views of the
world this is one that was a mash-up
that was done just after the japanese
earthquake and tsunami that shows that
they're an awful lot of nuclear power
plants in purple on the Ring of Fire and
the Pacific and in California and
they're not many other nuclear power
plants near earthquake zones this is
easily done this takes 10 minutes to
build or 15 minutes to build if you have
a tabular data with it this was a
shelters map and Tokyo similar way of
plotting information let me just show
you this one here quickly this is a map
of Mexico it's the most detailed forest
map created as of a couple years ago I
think it probably still is we produced
it from a vast number of computer images
taken by many satellites from Europe
from the United States all put together
on a large parallel cluster and you may
wonder why is it at just one picture
well it's very detailed and of course at
any given time these are tropical
rainforests are often covered with
clouds so it's an immensely complicated
image processing algorithm to do this
and our Earth engine project allows
scientists or will increasingly allow
scientists to put their computations
into the system operating on vast
amounts of data that we have to do to do
interesting work on climate on
Environment and other things which we
think will be a great contribution
here's an example of a problem but
brings up an opportunity just prior to
the tsunami that hit japan and
eventually to a degree hit hit the
Hawaiian Islands if you would type
tsunami Hawaii to Google you would have
gotten a bunch of results like the
Pacific Hawaii tsunami Museum and you
might have seen results like a tsunami
Hawaii or the International tsunami
innovation Information Center but you
would have not seen tsunami please get
the high ground which would have been
the right thing for google as a trusted
part of this world two halves have said
so I think on this one two hours five
hours after the Japanese earth
wake and two hours after before the
tsunami hit the Hawaiian Islands this is
what we showed I think we get an F on
this we have since then now put together
an alert pipeline we're getting alerts
in the United States so far so please
don't count on this in Israel at this
time we're getting alerts in the US will
make those alerts available to external
developers to consume we're going to put
them into our vast index and then we're
going to find ways of getting that
information to people when they need to
know it so for example there's work in
our Israel lab on one boxes that show up
where we can surface that information to
users the interesting thing we already
do is show it on maps so here's an
example as of a few weeks ago of I guess
winter storm warnings and hurricanes in
the United States that's good it's a
start but the really interesting
question is how do I give people an
alert when they need to know something
this came up when i joined and google
org as its senior technical leader I was
looking at amber alerts on the highways
in the United States for missing
children thinking what a silly place to
put them who's going to look up and be
driving by at the right time why was I
not hold on my cell phone when that was
the case but the issues of giving alerts
to people in real time is very
complicated there are in New York City
probably a 50 subway alerts a day that
go on I'm personally interested only in
maybe one or two of them if they
coincide with lines that I take while
I'm taking them that requires degrees of
personalization that are very
interesting so this whole issue of
privacy and personalization is an
extremely important one for us to get
our arms around as computer scientists
as Google we can do immense good we have
to do it in a way that societies like
from a privacy perspective which in my
case would be knowing where i am whether
i'm at work and what subway lines i
usually take some people might be
concerned about that i'm not but some
might more things we can do in elections
we're trying to promulgate information
about elections we're very interested as
a company and what can we do to
to improve governance and to have people
know more so that they can have better
governments we've seen the power of
social networking and the so-called Arab
Spring probably a good thing in the long
term clearly complicated what more is
going to happen as our field moves even
further afield into elections so last
slide is on education I'm going to echo
what Mikhail said earlier there are vast
opportunities in education Miguel talked
about how we can do the right kinds of
adaptive video that's one place um how
about the social networking so that we
can interact with students anywhere in
the world or faculty at any time and
have the immersive experience of a
dormitory but on the web no I don't know
that we can do as well as you know a
beer in someone else's dorm room at
eleven o'clock at night but we can do
better and we can make things available
for people and then finally I think also
we have the ability to use machine
learning to customize remediation and
testing for people and I think that will
be really interesting the thing that has
catalyzed this more than anything our
23,000 people finishing peter norvig and
sebastian runs course in artificial
intelligence how many of you finished
all the problem sets in it here anyone
how many people took part of it okay
whew not too many in this room but India
was the lead place by the way for this
so closing thoughts saw clearly touring
was we know he's legendary when you
review the papers and things he really
was we owe him a great service our core
has fantastic opportunities but so does
every other discipline and I think we
have to grow talent not only for our
core but so that we can seed all the
other disciplines that need to rely
increasingly on computer science and at
Google I can tell you we see no end to
the opportunity and we see no way around
our responsibility to help drive this
towards value for society thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>