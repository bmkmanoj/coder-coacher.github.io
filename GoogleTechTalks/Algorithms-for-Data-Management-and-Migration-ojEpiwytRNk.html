<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Algorithms for Data Management and Migration | Coder Coacher - Coaching Coders</title><meta content="Algorithms for Data Management and Migration - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Algorithms for Data Management and Migration</b></h2><h5 class="post__date">2008-01-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ojEpiwytRNk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I think it's fair to say I think I
haven't met Sameer it's very likely so
Sameer has I mean programs at Maryland
and he's only associate chair and I
think recently he just won distinguished
so today he'll be talking about
migration thanks son
so you can blame all her misfortunes in
life on my misguided advice anyway I'll
talk about some work that we've been
doing for almost 10 years now and in
fact several of the people who worked on
this problem with Mia right here on and
I started working on this problem while
she was an undergraduate in Maryland way
back in 1998 when I first got interested
in questions like this and that's the
algorithms of the data storage part and
then after she left just in another PhD
student of mine you are Kim continued to
work on this project and then Justin
finished his PhD and joined Google and
UI is now in Connecticut so and actually
at the end of the talk I'll give you
list is more like a survey talk and so
the end of the talk will give you a list
of all the papers and the various people
who worked with me along the way on it
okay so this is a theoreticians view of
storage system so if I if there are real
systems people here who disagree with me
I'd be happy to hear from them after
they talk about what constraints are not
modeled in our model or our way of
thinking about things
essentially the view was our original
application was if some company like
Netflix is going to have movies
available online which has just been
announced very recently but we were
thinking about this 10 years ago
and they want to place all the movies on
some collection of storage devices then
how should they handle high demand for
example you may need to replicate data
if some data objects are in very high
demand also for fault tolerance and so
on so there are sort of two issues one
is how do you lay the data out that's
the problem finding the data layout and
moreover as a demand for data items
changes over time then this layout may
need to be changed dynamically also our
assumption is that the data is not is
only being read by users it's not being
updated in rate change all the time so
it's like video some movies two-hour
long movies that have been shot and then
they are stored on your servers so at a
high level here's the view is the
collection of storage devices which can
store data and there's sort of two
important parameters that model these
storage devices one is simply the
storage capacity of the device how many
movies you can store in the device and
the second parameter is the load
capacity which is an upper bound as to
how many customers or clients can be
admitted into the system so for example
if a client wants to watch a movie and I
say ok you want to watch movie a for the
next two hours I'm guaranteeing them
some bandwidth and I'm going to
continuously stream movie a to this
client and they're going to add
something to the load of the system
there's some bandwidth constraint on
this device so not too many clients can
be admitted who can watch say movies a
and C from this device moreover you may
view these devices as logical disks what
I mean by that is that each disk here
could actually be a cluster of this and
you might stripe data across this
cluster of this okay so abstractly a and
C are two data objects that are stored
here but this logical disk might be a
set of disks on which data is striped
and so on so let's look at the first
simply defined problem where I have a
collection of movies that I want to
store on these disks and for every movie
or data object I have some expected
demand how many users might be
interested in this device okay so for
example there may be 20 class interested
in a to Class B one client in C and so
on and now how many copies should I
create
example of a and of B and of C and a B
and one of which this should I put them
on I don't want to put too many popular
objects on the same device right so
that's the first layout question that
we're going to address and the second
question is as this demand for the data
changes over time how should we quickly
reorganize our layout so here's a very
very simple example here we have two
storage devices each can only store two
objects so I can store a total of four
objects in the system and I have four
objects a B C and D each device has a
load capacity of hundred each so have
can admit up to 200 customers and let's
say we have 200 customers with different
amounts of demand right so for example
this is one potential layout I might
decide to put items a and B on the first
disk C and D can you see from here
actually and C and B on the second disk
and notice here that 180 clients have
had their demand satisfied they can be
admitted to the system there are still
20 clients for a because the demand for
a was 110 and 90 class were admitted
into the first disk so 20 clients for a
that are left out and the reason they're
left out is that this disk that has a
has no more load capacity for that right
the second device has load capacity but
it has no storage to store a so a
slightly better placement might be where
you toss out be completely move C to the
first device and simply replicate a now
you can see that in fact this solution
is slightly better I made two copies
away I handled all the demands for a
which was a 110 and I just dropped
complete be completely on the floor
those 10 customers don't get what they
want right so you might say how should I
choose a layout to maximize utilization
of the system right so that's
essentially the problem that we started
looking at so here we have n disks each
disk a storage K and load capacity L I'm
going to assume that the total number of
data objects that we have does not
exceed our total storage capacity which
is simply n times K every disk and store
K objects so right now actually assuming
that all these logical devices are
identical so I have n identical disks
some of the results that we have also
extend to the case when these disks are
not all identical they may have
different
and storage and load properties so the
technical result actually assumes that
as your disks are getting bigger they're
also getting faster so you might build
your storage system over time where you
might buy a bunch of this today buy a
bunch of this tomorrow the disks that
you buy in the future if there are
bigger and faster then in fact all the
results would extend not otherwise okay
so here's a simple situation write the
total demand is at most n L the total
number of data objects at most n K but
the problem as you saw in the previous
example was that because of an imbalance
in demand some of the demand cannot be
satisfied right so here I might for
example have two disks that can store
only one object each and I have two
objects a and B but the demand is very
skewed the demand for a is 150 the
demand for B is only 50 so what happens
that once you put a on the first disk
that that this is completely full it has
say 100 customers for a interested layer
and now you have 50 customers who want a
and 50 customers who want B and since
your disk and only store one data object
no matter what you choose a or B you're
going to leave 50 customers out in the
cold so our total 200 customers you
would only satisfy 150 customers right
so as you can see here I only satisfy 75
percent of the demand right okay so this
was just a discussion about the previous
example where 100 customers for a were
assigned to disc 1 and you can put
either a or b on the second disc it
doesn't really affect the quality of the
solution so this is the trivial case
when K is 1 and in fact a simple greedy
algorithm here actually gives you an
optimum solution but it turns out that
the problem becomes np-hard even the
moment you go to K equals 2 and it's in
fact np-hard for any fixed value of K
bigger than 1 okay so the main results
that we were able to show for the
following so there was an algorithm that
I'm going to describe to you called the
sliding window algorithm and this was
proposed by Jacque Nye and Tamir in
their original paper the running time of
the algorithm was somewhat high it was
the product of the number of lists and
the number of movies and we improved
they're essentially 2 to n log n so
notice here that n is the number of
discs and M is the number of movies but
the more interesting result I think that
we were able to show was a tight operon
lower bound on the number of requests
that can all be always served for any
instance at all so I'm not making any
assumptions on how the demand comes up
it could be any arbitrary demand we're
going to show that this algorithm is
going to always satisfy at least a
certain number of clients and the number
of class we're always going to satisfy
some function of K and this function of
K actually very rapidly approaches one
so as K of X becomes larger and larger
the number of movies that you can store
in a single disk this function is very
rapidly going to one and it doesn't
depend on the load capacity at all
we also show that this problem which was
np-hard also has a polynomial
approximation scheme but that algorithm
is not practical at all so it's like a
very theoretical result it's not clear
that that algorithm would ever be fast
enough to be deployed in practice but
the sliding-window algorithm is
extremely simple and so on and that our
focus was in trying to explain the good
behavior of the algorithm okay so here
is a description of the sliding-window
algorithm the goal of the algorithm is
to maintain actually I think one slide
disappeared in between there was a slide
that reference check my anti-mage paper
which maybe when they blanked out I
skipped it anyway that's fine or we
maintain a sorted list of movies in non
decreasing order of number of unserved
requests okay so you have a collection
of movies with some demand and I'm going
to sort them essentially in increasing
order by demand and then we go to decide
what set of items or movies to put on
every disc one at a time so I'm going to
consider this one at a time and we're
going to figure out which a set of items
to put on this disc I'm going to explain
this algorithm by an example very
shortly so this description doesn't make
sense it's fine okay the rule of the
thumb is the following I'm trying to
decide what to put on the disc I can put
it two K movies in this disc
our goal is to try to slide this window
so that I find the first consecutive set
of at most K movies with a sufficiently
high demand namely L and that would be a
good set of movies to put on these days
okay so let's quickly go to an example
see what's going on so here we have
three discs each disc can store two data
objects and it's load capacity is 16
and and here are our movies color-coded
okay so you have a bunch of movies and
the numbers tell you what is the demand
for each movie so there were six lines
interest in the first movie six class
interest in the second movie and so on
so let's started to figure out what to
put in the first disc so I look at the
first pair of movies the reason we're
looking at a pair of movies is because K
is two we look at the first pair of
movies and see what is the total demand
of this pair of movies it's it's twelve
right which is less than L so if I put
the pink movie in the yellow movie on
the first disc that's not a very good
choice because we wouldn't actually load
saturated discs so if we're going to
start sliding this window this window is
going to slide in the first place where
the window is going to stop is the first
place there where the sum of the demands
of the pair of items is at least as
large as well okay so six plus twelve is
actually 18 so what's going to happen is
then we're going to pack the first movie
with all six customers of that object
and terror of twelve customers of this
object and now what's going to happen it
is still two customers interested in
that light blue movie and they're going
to be reinserted back into the sorted
list so the new state of the list is
going to be the following so I insert a
piece of size two back in the sorted
list the two movies that got packed have
disappeared and now again for example if
used to restart the window for the
beginning it would slide all the way and
come here right our algorithm is a
little bit more careful and doesn't have
to restart the window right from the
beginning as Chuck nine Tamir described
it in their paper and so the second disc
would again pack these two movies and
again you would have two customers left
out for the dark blue movie and then you
would get this sorted list right and now
there is no consecutive set of K objects
which would give you a load saturated
disk so you will just take say the last
two pair of movies the ones in highest
demand and pack them on the thermos and
then these customers are just left out
okay so what happened here there were a
total of 48 customers it actually 44 of
them got assigned to this and four of
them got left out so we did quite well
the number of clients have we could
serve was over 90% okay any any
questions about the algorithm itself and
then I'll so my first reaction when I
saw this algorithm was this algorithm is
way too simple
maybe we
do better right so spent a long time
trying to improve an algorithm finally
came to the realization this is actually
a very very good algorithm and that's
what we've got to show next so there's
some notation that I need to talk about
to explain how we analyze this algorithm
the basic idea behind the analysis the
following are we're going to look at the
number of discs that have the property
that load saturated namely they're the
number of clients assigned to the disk
is exactly L the load capacity okay so
here there were two loads such a discs
and there's one base that is in fact not
load saturated because only the number
of clients assigned to it was strictly
less than L and that's this is called
storage saturated shear here NS is one
and we're also going to measure this
ratio C which is the percentage load on
this disk which is they were 12
customers assigned the total load
capacity was 16 so it's 0.75 okay so see
what is C so there there was only one
storage center disk we in general we
could have a collection of storage
Sagittarius which I've Hall demand less
than L so C is defined as a load ratio
of the disk with the lowest load among
that set of disks so there's a bunch of
them you just take minimum one so if L
100 for example and there were two
storage saturated disks with those 55
and 70 C would simply be 0.55 okay so
now what is Tammy you're in shock now
show about this this algorithm what this
showed was that essentially if you ran
the sliding-window algorithm
artificially assuming that every disk
has storage capacity K plus 1 then they
show that all the clients in fact get
packed this is sort of a consequence of
of their analysis of the algorithm so
they sort of assume that M wasn't as
large as n times K it was somewhat
smaller and with that assumption they
showed that all the clients got packed
okay now if you run the algorithm
assuming that every disk has storage
capacity a little bit more than k k plus
1 then if at all the clients get packed
but every disk is artificially holding
an extra object that it has no space to
store so if on every disk if you just
dropped the object with lowest demand
then you'll get a certain performance
which is 1 minus 1 over 1 plus K okay
now if K is 1 notice is only 50% so what
it's showing is that at least 50% of
demand can be satisfied and the function
that we derive is in fact not only
better but it's actually tight and I'll
explain what tight means in a few
minutes so coming back to the analysis
there's going to be let's see what what
time to here few minutes of analysis the
idea behind the analysis at the high
level is fairly straightforward so as I
mentioned earlier we had a collection of
this overload saturated with the actual
node was exactly L that in fact in that
example was - we had a bunch of storage
scientists with a load was less than L
in that example was only 1 and C was the
smallest fraction of a disks actually
right now we try to show that somehow
the algorithm did well so to analyze how
well the algorithm did they were some
number of requests had got dropped on
the floor that's the number of unserved
requests you in that example that
happened to be 4 and then there were
some number of requests that got served
s that happened to be 44 right but we
try to prove some properties about these
two numbers so these 2 lemmas tell you
as a function of these parameters that I
defined how many unserved requests are
there an upper bound on them and a lower
bound on the number of serve requests ok
now where did this this bound come from
let's actually look at lemma 2 because
then once to luxury trivial what is a
power of the number of serve requests
it's just by definition right I had I've
served a lot of requests if there were n
L load saturator disks they have several
clients each if I had n s storage has
rear discs each of them is server
atleast C times L demand so this is
completely trivial and the number of
unserved requests in the same way I can
simply bound as to what got left out so
there's nothing really happening here
the more interesting bound is actually
in lemma 1 in level 1 it's really a
product of two terms we have a
collection of load saturated collection
of storage saturated disks what we're
going to prove is that the number of
movies the number of movies that got
left out in the end that were not served
is at most n L so remember that in the
example there were two loads such a
release and exactly two movies got left
her that was not a coincidence it's it's
n L is an upper bound on the number of
moves that got left out moreover how
many clients for each of those movies
got left out that bound is something
like CL over K okay why is that if you
think about these discs each of these
discs is satisfying at least CL amount
of demand each of this this by
definition has exactly K movies on it so
the average demand for any movie here is
CL over K whatever we left out there
were less popular movies than what we
stored right that's the property about
the algorithm so so the second property
is actually more easy the first property
is not totally obviously - why there's a
relationship between the number of
movies left out and nl so that involves
a claim namely that the load saturated
discs also have exactly K movies on them
okay so remember here we had load such a
decent storage saturate this these of
them all had exactly came who is on them
now observation is that the load size
reduced also have exactly K movies right
now it's possible that they don't but in
that case in fact what we can prove is
that all of the requests were self there
was nothing left out at all so we in
fact it very very well we satisfy 100%
demand so if even one load center disk
has less than K movies on it then then
we have in fact satisfied all the demand
okay so that just takes a little bit of
thinking about the algorithm the
intuition is the following the intuition
is that I actually lied to you about the
algorithm a little bit I said that we
have these windows of size scale that
are sliding right the truth is that the
window starts out with size one and not
size K okay so we sort of grow the
window in the beginning and then we
begin to slide it when it hits size K
for example it could be that you just
look at the first movie and that has
popularity more than L right it's
possible it is very very high demand for
the first movie so you only store one
thing on the disk but only L of the
customers could be assigned to that disk
and you would break off a little piece
but in fact if it ever happens that a
set of K minus one or fewer movies has
demand at least L and that happens
because we were to write to the beanie
of the list and all the other sets that
we're going to look at off size K minus
one are also going to have demand at
least l when I break off one little
piece and put it back in the list that
one piece with the next K minus one
movies will again create a node
Sagittarius and you keep going and you
can show that all your endless or load
saturates so in fact all the demand got
satisfying okay so that takes a little
bit of thought so if that didn't make
sense that's perfectly fine so what's
happening is that in fact all our disks
are storing K movies right but you say
how is that possible we started out with
n times K movies all the disks have
stored K movies each and there are n
days so everything got stored no the
problem was that we were breaking off a
piece every time we had a load saturate
disk and every time he broke up a piece
we created this new object and the
number of new objects we created was at
most n M okay so those were the ones
that got left out so here's the formal
argument but anyway I already kind of
described that okay any questions so far
it is a good time to go
Demond right and so in fact a couple of
times when I gave this talk a few years
ago people said that's completely the
wrong thing to do why you ordering them
an increasing order demand why don't you
start your window at the right hand with
the popular movies the problem that
happens then is that when you start
sliding the window from the right you're
sort of packing all the very popular
movies initially but now towards the end
you're left with a whole bunch of very
unpopular objects and then you can only
group K of them at a time even though
they have very low load right what you I
didn't want to do in this case is to
pack the unpopular objects along with
the popular objects and that algorithm
would not do that if you start with a
very popular moves and this algorithm
does that very nicely if you had some
very unpopular objections are very
popular objects the sliding window will
slide and skip over all the unpopular
objects in the beginning until it first
finds say a collection of K minus one
unpopular objects and one popular object
and then makes that as the right choice
into the okay so now the rest is all
elementary calculus it turns out that
this function of K I didn't describe to
you is a bit strange that's the function
of K it's 1 minus 1 over 1 plus square
root K square
I have no intuition to tell you why we
get this function this is just this
analysis very plugged in based on these
bounds that we got and then we optimized
the function we were able to show that
that ratio is at least that bound right
so the very very stretched bound but
notice that when you plug in K equals 1
this bound is actually 3/4 surprise
surprise when K was one where I showed
you earlier there's an example where I
can only satisfy 3/4 of the demand no
matter what I do the optimum solution
can only satisfy 3/4 very much right so
so we've done quite well one case one
right if you let's say plug in K equals
4 then you're doing even better because
you get 1 plus 2 and that's 3 3 square
is 9 so what it says I've only dropped
1/9 of the demand in the flow right now
the interesting thing is that we can
show that this bound is tied so namely
there are input instances for which so
this is a more complex generalization of
the K equals 1 example but there are
input instances for which even the
optimum solution cannot do better than
this bound ok so so in those instances
in fact sliding-window has given you the
best possible
the problem is still np-hard there might
be other instances in which the optimum
solution is a little bit better but the
sliding window is unable to find it yeah
okay
so then this is sort of how the
algorithm does in practice so what we've
plotted here is actually wastage SK SK
increases so the topmost curve tells you
the bound that you get when you plug in
the consequence of the Shack nighta me
result that's the ways to do something
like 1 over 1 plus K so the waste is
very high when K is small and then
progressively that's also approaching 0
and the interesting thing is the
analysis I actually showed you it gives
a very trivial proof of their main
result which is a very complicated proof
by induction but if you use this style
of analysis the proof of their result
just pops out that if you started off
with the number of movies not being as
high as n times K but as high as n times
K minus n then everything gets back that
this comes out as a very simple level
this is the function that we got that
applauded the wastage 1 over 1 plus
square root K square and this function
starts over 25% wastage when K is 1 and
then very rapidly approaches 0 and this
is how the algorithm does in practice on
a lot of test example where the wastage
is in fact very very small so the next
question is as the demand is is changing
over time then how do we reorganize this
layout to find a new layout right
because I might think that movie is B
and C are very popular I make lots of
copies of them but then when I start
writing system I say well users want
something else so something's wrong with
that layout so we want to convert our
existing layout to a new layout so this
leads to the problem that we call the
data migration problem so you have some
current layout that you thought was very
good and you maybe ran the sliding
window algorithm came up with it and
then you run a sliding window I'll go to
a game with a new demand pattern and you
find a new data layer which looks like
that so our goal is to somehow now
convert this one layer to the other
layer okay so let me say a few words
about our assumptions here so assumption
is that these devices are sitting around
some network on which you can they can
communicate and so any device can
transfer an object to another device by
using this communication network all the
data objects have the same size so it
takes the same amount of time to
transfer a to me as a C to D and so on
and you can basically pick any matching
for example here and say do this set of
transfers right so this is immortal
refer to as the half-duplex model in the
literature where you know in a round you
perform some set of migrations in
parallel which are all happening at the
same time and then they nose around you
decide what you're gonna transfer the
next round so on and in fact our
algorithms also extend to the case where
this communication network doesn't have
enough bandwidth to support all possible
transfers if you had a thousand disks
you might say I might pick a matching of
size 500 and say this dude I said if I
wanted transfers but maybe the network
doesn't have enough bandwidth so you can
say you can pick any matching of size 50
and we can do those transfers and and
all the results basically extend to that
model as well where you bound put an
upper bounds to a number of transfers
that can happen in a single run and then
more recently we were able to extend
some of these results to the full duplex
model where every device can send and
receive an object at the same time okay
so that's the full duplex model it sends
and receives North necessary from the
same disk so you might be receiving
something from one disk and be sending
something to another disk in the same
round so that's the full duplex model
okay so this problem of of data
migration was looked at in a slightly
different setting by a group of
University of Washington
Jason Hart line was one of the authors
of paper and Anna Carlin and some of her
students and a group at HP Labs worked
on this Wilks I think is the name of the
person so the main problem that they
looked at was you're given a collection
of storage devices the model by vertices
in a graph and you're given some set of
transfers that you want to perform so
every transfer is I want to move object
a from this distance as a directed edge
and what they showed is that for example
if you want to minimize the number of
rounds it takes to transfer the disk
what you have to really do is to find an
edge coloring of this multi graph
because each color class simply
represents a matching right and those
set of transfers can be done internal
and and finding an edge covering with
the smallest number of colors
corresponds exactly to simply minimizing
the number of rounds the more complex
constraint of the dealt with is an issue
of space constraints where the main
problem is that if this disc maybe has
to send out three objects and has to
receive some objects if all the objects
it has to receive arrived before it
sends out any objects maybe it doesn't
have any space to store them so what
they showed was that you can still get
good approximation guarantees to this
np-hard problem as long as you make the
assumption that every disk has one unit
of extra storage capacity so I might
first receive an item I should have some
place to put it but before I receive any
other item I should at least send one
thing out okay
so you sort of trying to intuitively
alternate the transfers in and out okay
so this was working with a fixed
transfer graph right so now I work the
original layout may in fact have
multiple copies of some data items right
so you get into a slightly more complex
issue of how which transfer graph should
you choose because if there were three
copies of a and some disk wants a you
have a choice now right so in their
model they sort of fix where this object
is going to come from and the other
point I want to make is this issue of
cloning namely that once you make a copy
of an object by moving into some other
disk that this can now serve and help
you move the object to some other copy
right which is sort of not allowed in
their model really so here's sort of a
more concrete example so I have an
initial layout shown above and I want to
convert to this this new layout so here
one disk three wants item one this four
wants item two the two red items that it
doesn't have right so here's a
possibility you say well this one has
both one and two and maybe it should be
responsible for sending items one and
two to this three and four but that's
going to take you at least two rounds
because disk one can send something to
one person in one round only so it's
going to take a second round to transfer
the second item but what you could
alternate you do is that say I'm going
to use disc one to send item one but
notice that I also have a copy of item
two and a different disc and I can use
that so choosing a different transfer
graph might in fact be a better choice
the other issue like I mentioned earlier
was clothing if some objects become very
popular you may want to make lots of
copies in their model Yuri force to
create a transfer graph of this type and
this is going to take a very long time
to schedule so here you can just build a
tree and propagate the items very very
quickly okay so I know we have to clear
out of this room before noon so in the
remaining 20 minutes I'm going to try to
give you some explanation both about our
formulation of the problem and some
intuition about how this set of
algorithms work and I'm going to talk a
little bit about how these algorithms do
in practice so in our formulation we
don't really work with the transfer
graph at all we have some initial layout
and some target layout and so I'm going
to say for every item I there is some
set of source discs that contain item I
right so for example you look at item
one the second is the third disc in the
fourth disc all have item one so that's
the source set for item one discs two
three and four and then there's some
disc that needs that a one but doesn't
have it yet and that's this five it
needs I
I don't want so the target set for item
one is just a singleton object namely
this file so for every item I have a set
of sources and I've itemized a set of
target is the need item i but they don't
have it and we somehow we want to
propagate this these items very quickly
to minimize the tour the mac the
transfer time or minimize the number of
rounds yes so we sort of look at both
cases so one observation is that so the
first algorithm the one I'm going to
describe to you in fact does not do that
so the total number of transfers you do
is exactly optimum so I will only send
you an item if you wanted it and still
we can give a bound on the number of
rounds right and it turns out that if
you are allowed to use these
intermediate helper discs where I want
to send you an item but you are busy
right now I'll send the item somebody
else because in a future round I am busy
and then they will send it to you so
then you can improve the bounds on the
algorithms but then you the total number
of transfers done goes up
yes that's a very good question so in
the paper where we looked at the
experimental behavior of these problems
we actually defined a new problem called
the correspondence problem and which
exactly addresses that issue that a real
labeling of the disks might be a very
good thing to do I returned so that that
actually has a lot of but here right now
what I'm just Ram do you know but we did
look at that issue as well where they're
real labeling in us around okay other
questions so what this interesting joke
one time I don't I have no idea that's
true or not okay Nobel laureate used to
go around to places giving talks and his
chauffeur said to him one day I've heard
your talk so many times I think I can
give it now so Nobel already told him
you know next week I'm going to a place
I don't have a webpage they don't know
what I look like you can give the talk
so the chauffeur said fine I'll give the
talk for you so the chauffeur showed up
when you have the talk then somebody
asked a question it wasn't we didn't
know how to answer the question so he
said you know this question is so easy
that even my chauffeur sitting in the
audience can can answer the question so
I feel the same way like the chauffeur
if you have questions you know the two
experts right in the room just in and on
and so then point to them to answer
questions so they can step in and answer
questions anytime they want okay okay so
like I mentioned earlier the problem is
np-hard
and so here's sort of a summary of of
result so the first thing that we showed
was that there is a constant
approximation for the problem the way I
described with these arbitrary source
sets and then we also looked at some
special cases where these sets have some
nice properties because it turns out
that the this problem really with sets
sirt is a generalization of a lot of
questions that people have looked at
before namely you may have heard of the
broadcast all over the gossip problem
these are all generalizations of of
those gossiping in broadcast and
multicast problems right so more
recently we were able to improve this
nine point five five approximation to
something like six point five and then
the bound for the full duplex model
where this can send and receive an item
that's even better it's like a four
approximation so the nine point five
approximation is a little bit
complicated so what I'm going to do is
I'm going to just describe to you very
very simp
case you explained it is some intuition
about how to approach this problem so
we're going to assume in the simple case
that in fact every data item has exactly
one source I which is distinct but the
target discs can be completely arbitrary
so this is the special case so I have
arbitrary collection of target discs and
a special set of sources which have an
item that they want to propagate so the
sets are s i--'s are all disjoint and
namely Singleton's so this is sort of
like trying to do a whole collection of
multicast simultaneously right because
one multicast is s one wants to send
some stuff to t1 right but now I'm
trying to propagate different items and
do all these multi-class at the same
time so what happens in this algorithm
is fairly simple you might have some
target sets which are very very big Ti
so you need somebody to help you send
the information very quickly to them so
we're going to choose some helper cells
that are called representatives and
these helper sets are restricted to
being a subset of the target set because
helper set should only get some item if
they wanted it to begin with in the
first place and namely the helper set
should be disjoint from each other
because each one is involved in helping
some item and this should be sort of big
enough so the high-level algorithm is
very simple I'm going to send the data
from the source disks to these helper
sets and then these helper sets are
going to help me propagate the item to
the target set so here's sort of an
example of what the algorithm is going
to do so we're going to choose helper
sets carefully the size of each helper
said is something like the ratio of the
size the target set and B'Etor and what
is this parameter beta beta is simply
the maximum number of items our disk
needs
notice that media is a lower bound in
the optimum solution namely if there's
some disk for example say this disc you
can say how many target sets does it
belong to maybe it belongs to two or
three target sets if it belongs to three
target sets well it's going to take you
at least three rounds to send that set
of items to that disk so that is beta
it's a maximum number of items any disk
needs so we're going to choose our
helper sets of size roughly TI over beta
take the floor and so if some target set
had had size 6 like this one and B is 3
I have to help her
the ones marked in the green and these
helper sets as you observe they're all
disjoint so how did we find these helper
sets well that turns out to be simply a
network flow problem so you have to
build a certain flow Network where you
have a source node in a sink node a
vertex corresponding to every disk a
vertex corresponding to every data item
you put in a certain capacity you know
that data item can needs so many so many
the representatives at RI should be a
certain size and then we want to find a
max flow in this graph and notice that
because these are unit capacity is this
this can only belong to one of the sets
and the flow will tell you how to make
the right choices and essentially why
this parameter beta gets chosen is that
we can show that there is a fractional
flow here that in fact saturates all the
edges that go to the sink so if you just
find any integral flow that does that
because all our capacities are integral
then D that gives you the answer okay so
that the high level is simply Network
flow calculation to find the right
representative sets and then you're left
for this problem where you have source
sets but the problem is that the source
set itself could be a helper for some
other data object right so you get some
picture like this you want to somehow
move the data very quickly from the
source sets to these representative sets
and for that you're going to need to do
a little broad local broadcast like for
example item 4 gets sent from s 4 to all
of our 4 right but the number of rounds
it takes you is roughly log of the size
of the set to do that but all of this
has to happen in parallel for all the
different items and so it's not that
difficult to show that this scheduling
can be done in time roughly proportional
to log of the size of the largest or I
said ok and then after all the helper
sets get the item in the first phase now
they can help you propagate the copy of
the item right but what is the ratio of
the size of the helper set to the target
said well that's roughly by a factor of
beta by the way we chose it so what we
can show is now that you can build a
certain transfer graph in which the
degree of every node is bounded roughly
as a function of beta and then if you
take an edge coloring of this graph the
number of colors you need is
three proportional to degree that's
classical edge covering result and then
so the edge coloring will now give you
the schedule right so this sort of a
two-phase algorithm in a sense you first
send the data from the sources to the
representative sets and then from the
representative sets you migrate the data
to the target sets and and for the last
phase we use this transfer graph idea
and then that gives you an edge coloring
and so on so these are just the bounds
that you have used for edge coloring so
why is in the original theorem showed
that any simple graph with maximum
degree D can in fact be edge colored
with D plus 1 colors and this coloring
can be found in polynomial time and it's
in fact NP hard to tell whether or not
this D plus 1 can be reduced to D so so
this is almost in a sense the best you
can do
but the graphs that we deal with are
multi graphs actually and so the bound
that you need is something that depends
on the multiplicity of an edge so the in
the simple graph K is this u is this one
and so we have to bound both the
multiplicity and the dick maximum degree
so the analysis is now a fairly standard
analysis of approximation algorithms it
shows you and this bout is not not the
best possible so I showed you the
algorithm I showed you has a four
approximation that which somewhere can
be improved to three and if it some more
work can be improved 2.5 ok so what
about the general case when s and T is
an arbitrary the main problem here is
the following we assumed earlier that
the source sets had only size 1 right
you may have larger source sets for some
item right so I cannot assume anymore
the log of the size of the
representative set is a lower bound on
the optimal solution because even though
I want to propagate hundred copies if
only you had one source of the item that
I need log 100 rounds right we have had
50 sources I don't need log of 100
rounds so that's the basic problem that
you run into that all of those lower
bounds that we assumed earlier don't
apply so and then there's some other
issues that come up like for example if
all the data that needs to be quickly
replicated is all concentrated in a
small set of discs then again you sort
of have a problem because all these
source sets overlap significantly so
that's the reason why the algorithm gets
a little bit messy so so you need some
new network flow things to get some new
lower bound some number you have time to
talk about these in detail
but essentially this algorithm ends up
with this complicated 9.50 approximation
we also compared the algorithm to some
heuristics so the most interesting
heuristic that I want to mention is a
heuristic actually based on matchings so
let's move on a little bit to this
heuristic based on matching so here is
the heuristic we have these sets s int I
and so we can sort of build a graph
which puts edges between pairs of disks
where this disk has something useful to
sender that is right so if I'm in some
set s I and I have Adam I and somebody
else walks out of my you know this is an
edge which potentially does a useful
transfer so you build a graph of this
type with all these useful edges and
then this graph you go ahead and find a
maximum matching you do imagine doing
those transfers but now the structure of
the source has changes because some
disks have acquired a new item so you
updated idea and repeat this and again
in every round you find a matching and
then we looked at some places where this
heuristic may get stuck and then looked
at some ways to break ties that might
make it do better so so in general you
start off with a graph of this time with
s int I sets and then these are sort of
interesting transpose right I mean any
of these edges would help you transfer
some item one namely from s1 to t1 and
so you take the Union over all the s int
I says will transfer graph and now find
a maximum matching it is a set of useful
transfers that we can do in one round
after I do this transfer I simply update
the structure of the sets get a new
instance and do this and this heuristic
actually interestingly does this very
well so in the experimental setting they
were collection of roughly 60 disks the
initial and target layouts were created
by using sliding window algorithm and
then we use different methods for
modifying how the demand changed right
and then we want to compute a migration
schedule and then see how well it does
so here sort of roughly the behavior of
different algorithms and as you can see
the alga the heuristic based on on this
matching based thing even though it's
worst case behavior is is not very good
in practice it does really well and here
we are comparing the quality of the
schedule found as to how many rounds it
took compared to the best lower bound
that we could show
for the problem right because in
development of these approximation
algorithms we also come up with certain
lower bounding techniques along the way
right so I can take the best lower bound
that we can find on the optimum solution
and simply compare the performance the
heuristic to that best lower bound so
that was certainly one interesting
consequence of doing an approximation
algorithm because those four stands to
think about these lower bound in
technique but in general I have no idea
what optimum is right and and the
algorithm that I described you the 9.5
approximation a small variant of it is
shown here in white and so as you can
see it's reasonably competitive with the
matching algorithm except for a couple
of cases here otherwise it's not much
worse okay so of course the moral
stories that heuristics very often
outperform approximation algorithms on
specific situations of this type where
the useful data is concentrated on a
small set of disks what happens is that
then the matching based algorithms begin
to do really badly okay so so our
algorithms do really well in those cases
but these matching based algorithms as
the number of items is going up their
behavior is getting worse and worse so
but those are sort of instances where
this phenomenon happens that the useful
data you want to propagate is in a small
collection of this okay
so the first paper that Sarah's working
on this problem was published in soda mm
that's the algorithms for data layout
and it was joint work that Leanna gulab
chick currently at USC sanjeev khanna
japan romkey 3-mile are denver and
subsequently i worked with students in
ewaz Kashyap where we looked at the case
where all the movies don't have the same
size where we assume that all the data
movies are the same size and every disc
had this storage capacity of K so you
can say what happens when these movies
have different sizes so I still have a
storage capacity of K but not some
movies are to our movies some whose a
three our movies one are moving so on so
we were able to generalize the sliding
window algorithm to to work and prove
certain bounds but there are small gaps
there in a sense that the functions that
come out like for the unit size K is the
bound is completely tight right it's
matching up and lower bound so there
that there's a small gap between the
so this potential is some interesting
theorem that could be shown there which
is namely with no gap at all right but
we were also able to show that even this
problem if the number of different sizes
of the movies is not too big like I have
movies of five sizes or seven sizes then
in fact that problem also has a
polynomial time approximation scheme
then we looked at some special cases
this was a paperweight just in and you
are where we looked at this gossiping in
broadcasting problem and the problem
that I actually talked to you about was
this multi source multicast it was a
very special case where I have set of
sources and this multicast have to
happen at the same time okay and then
the main work on data migration appeared
in several papers so this paper actually
had very complicated 9.5 approximation
and in this paper with an undergraduate
student with Lana Shar course kaya who
has implemented some of the algorithms
we looked at the experimental behavior
of many of these heuristics including
the question you asked about what
happens about the correspondence problem
when I can relay about the disks okay
and then more recently with joint work
with as a Rakesh Maliki and a PhD
student of mine who's currently doing
her PhD we were able to improve some of
the bounds both for the half-duplex
model and then for the first time we
actually looked at the full duplex model
where every disc can be the source and
target in a round of some item and that
bound is now down to four
again the techniques are a little bit
more complicated now you can say well is
this really the right approach so the
answer is somewhat no unfortunately so
we can ask the following question so far
in our framework we actually assumed
that we started off with an initial
layout and as somehow we ran some
algorithm to compute the target layout
based on the demand of the data right
but you can say maybe I could choose a
different target layout right they could
be potentially several target layouts
that are all reasonably good and some of
them may have the property that you
could migrate to them quickly right so
this also is optimization on on space of
layout if we never really address that
so at least the experimental results
that there should seem extremely
promising in the sense that previously
when we were running our algorithms
let's
I changed the layout in the demand
pattern in some way and then rents ran
the sliding window value defined a new
layout it might be that it takes 50
rounds of migration to do the
transformation right but at the same
time you could show that there are other
layouts that you could get almost as
good behavior by just doing four rounds
of migration right so because we were
never really try to optimize the
workspace but we really don't have a
theorem that says if you give me Delta
rounds of migration then this is how
much I can get close to the best
behavior of sliding-window values
because a sliding window algorithm
completely ignores your existing layout
it just takes some demand pattern and
comes up with a layout right so they
could be some interesting work done here
where you take into account the current
layout so that you can migrate to that
layout with link okay so we're done in
time for a page says you are Kim
currently at Connecticut just in at
Google as a rock still doing her page at
Maryland says romka to Romelu who worked
and actually implementing the data
layout algorithm then were lianna have
been collaborating with for almost a
decade on Google and Sanjeev cannot Ben
who worked with us on the approximation
scheme
so one last paper basically what happens
is that there's sort of two aspects
actually the first aspect is the layout
namely deciding for each movie how many
copies to make and which is to put them
on the second aspect is when the
customers arrive which copy should I
assign them to so that assignment
problem the second phase is actually
just a network flow or am matching
computation in fact I didn't say this
but the sliding window algorithm without
even running the matching of flows
actually comes up with a assignment
automatically that is optimal for that
layout it wasn't obvious at all and it
takes a little bit of work to prove that
that in fact the sliding window
algorithm as it is deciding which movies
to put on which this is also the center
of assigning the clients the discs but
it actually tells you
the question once I have this layout
maybe I can use the network flow phase
to reassign clients to come up with a
better assignment but but it turns out
the sliding window layout and assignment
is optimal ok so you don't have to run
the floor matching but if you had an
arbitral layer that you gave me and then
I had a demand pattern then I would have
to run the flow computation so even
without changing the layout very often
by by redoing the assignment and moving
nothing you can just do very well so
even though demands changed slightly we
just compute the net work flow without
changing anything and magically all your
clients get transferred so the really
the real layout gets triggered only when
drastic changes happen when small
changes happen everything works very
nicely actually
you in the data migration yes the first
problem one and I thought about LP
rounding and for what I remembered they
were actually some pretty bad GAAP
examples there where the at least our
formulations had bad GAAP examples so so
there could be other ways of formatting
the problem that do well but think about
for the future so I took a lot of
pictures and this is the only one that
came out with very few people in it so
and there were thousands of people on
the beach at this time so it wasn't
actually trivial to get this shot
because everybody came there to watch
the sunset work on kind of the para
position where you might not do discrete
transfers but instead rate limit one of
the transfers to dribble out in the back
while you're using you know proportional
band width or several other saltiness
transfers so we haven't looked at that
so there is some work that one of my two
lobbyists actually did so you I had
worked in this problem many years ago
where she kind of looked at the issue of
if you fix the transfer graph and in
fact the transfers themselves could take
non-uniform times and you want to move
see a collection of files in a dis
storage system then you want to minimize
things like the sum of completion times
and things like that so in our model
we're simply focusing number of rounds
maybe a lot of the transfers happen
right at the end which may be less
desirable so she actually looked at the
problem where you want to minimize the
sum of the completion times either of
the transfers or of the Nords themselves
so a nodes completion time is when the
last transfer it is involving yes so she
has the people in
2003 if I remember correctly where she
looked at it and given approximation
algorithm but they all work with a fixed
transfer graph it's not in this general
model where you have source sets and
target sets and then that work was
subsequently improved by developing so
her algorithm was based on LP rounding
and then Rajiv Gandhi and Julian maestra
had a paper a couple of years back where
they actually gave a combinatorial
algorithm to give the same guarantees as
hers and in actually this surah which
just happened concluded yesterday my
student Julian presented a paper where
he showed how to improve that pound of
three a little bit more by using a
technique called adaptive local ratio
which is actually a bit more complex it
involves a solution to several linear
programs and so on yeah but but people
have looked at that question there's one
paper that looked at that question I
think David Jones and one of the authors
of paper I don't remember all the
authors but they sort of defined this
framework like where you're going to
minimize sum of completion times of
these nodes and edges but all with a
fixed transfer graph right so there has
been some work on that I don't know if
so much about the Oh our literature
though this is mostly in the computer
science literature Jesse's</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>