<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Big Machine Learning... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Big Machine Learning... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Big Machine Learning...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aq66c3geSis" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you've got plenty of components you've
got you know many principles and
techniques and algorithms and if you
don't understand them in might the
picture might look like this but if you
happen to be an expert then you can you
know build an engine throw that into a
car and you can drive such a complex
machine you send two pedals you can use
you know a break and a gas pedal and
you're moving a very complex machine so
the same way our engine has to move big
data and it has to move to really big
data and also not so big data so I'm
going to demo our web application but
before doing so I want to introduce four
important concepts that are used in our
application and you probably all know
these but I just want to make sure we're
on we're all on the same track and we
call a source you know the data that you
blood or stream to us in that data can
be saved saved and formatted to to get
useful analytics and that's what we call
data set and that can be turned into
something actionable and clickable and
that's what we call a model in you can
input us new data and we'll give you
immediate predictions so I'm going to
switch my screen now and I'll show you
the website there is as we're having
some connectivity issues i'm already
logged in because the dns is doing weird
things and here you can see the main
dashboard of BML and as you can see
there into so many we've got the these
these resources I've told you about the
source is the data said model
predictions in tasks well I haven't said
I haven't said anything about task but
it's it's a simple one in services I'm
not going to blow the file and Astarte
we've got some
sources lowering and datasets generated
and models because it takes time and I'm
going to show you some some of the
details and here you can see the sources
that has have been uploaded to the
website we can click on one of these for
example and we can get details of the of
the source and what we're doing here is
we're trying to do we do a best match
for parsing the source you gave us and
throw a table in which you can see the
attributes of the shores you pick you
pass to us in the date you have and this
is a preview actually you've got a
button here in which you can play with
some configuration options for
generating data set and out of these
data set out of the short story we've
got data set generated in it's this one
i think i was showing covered well it
doesn't really mind you probably all
know the adult csv file and if we look
at in the data set we generate
histograms and we give you some
statistical information like the minimum
the maximum the average and you can move
over the histograms and get information
in nice tool tips and this has been
generated with our parameter free option
we've got something we call the one
click data set you can generate one
source out of what you can generate the
data set out of a source with just one
single click in out of this data set the
G that you can see here we've generated
a model in if we look into the model
what connection is a little slow
hopefully works
in today to set well you've got with
with I'm going over that now so the data
set is mainly statistical information in
histograms for your for for the source
you you blow it to the website at all
csv is a famous csv file that gathers
information for census data of people
and tries to classify those those people
in into two groups the people that make
more than 50k a year and people that
make less than 50k a year so it's it's a
famous csv file and it's online you can
do can you can get it's like the hello
world of machine learning and this is
the model we've generated for for this
data set in you can you can look at it
and as you can see we're working on the
visualization of the models we put in a
lot of detail into these because we want
new people that not so experts in
machine learning to understand the
concept of model and here we are using
rangel Tilford layout algorithm that
makes it more static in its it's less
with in so it fits in the screen and if
you see the width of the edges between
the branches they're different and they
represent the proportion of training
data used in that path and we're using a
color is a smart color schema to color
the most common nodes in a way that
makes sense in we can we can go over
some of these paths i'll go over one of
these for example and in on the right
side because we don't want to the screen
to be messy we're we're explaining
what's going on in the path you selected
and we're telling you that of in this
case and this path the classification is
this one and you're going to be getting
a person that has an income of less than
50k and also we're doing right now
binary trees but in the future we will
do
friend different trees too and on the
top of you you get the breaks into and
I'm not going to be so in the
predictions forms and all that we're
working on this working on different
ways you can ask us for predictions and
what I'm going to be serving now is the
API we are working on an API here it's a
restful api and with formatting all our
data and Jason so what I'm going to be
doing is I'm going to create a source
what oh sure sorry up can you say it no
more sorry better okay so what I'm doing
here we I'm demonstrating the API and
for that we're using an IV CSV file also
a very known CSV file or data source in
the machine learning world and that has
created a source in my account so i
could go here in the sources and as you
can see Sir minutes ago sir right right
now I've loaded this file to the system
and we can now go in sorry about that
this is not a riser we can go now in
create a data set picking the resource
that was passed here if you go pick
these here and place it here this is
going to be will now generate the data
said well right now we're processing it
and generating assets as it is a small
file it takes very little time to to to
be done in here this we can look into it
and the histograms are built and I'm now
going to create a model for that so you
can see how they how it works and i'll
grab the data set here and paste it and
we're requesting our API now for
creating the model in given ops is
giving us an information and
interesting thing of the API is that we
are passing a lot of data and we're
building a very extended API that lets
you build a lot of things on top of us
and it will see it's it's already here
the model we can look at it and here is
the representation of of the 34 for that
model and it's cut its classifying types
species of flowers and also we could go
in create a prediction for that model we
just need to say what model we're using
sorry but that Miss mr. see we're going
to pass an empty query and there is it's
saying we'll get a nice virginica
flowers pieces so I'll continue with the
presentation now how we're doing this
I've shown you the website I've shown
you how we're doing the API and I want
to talk about some of the details behind
it our website is going to be on baml
com it's already there you can you can
go on the website and actually we're
we're now with now with not with did
with lunch today the the new landing
page and we're going to be a starting
our alpha testing side before the end of
the year and the the API is going to be
big imelda Rio of course we've built
this on top of the cloud wearing it now
using Amazon Web Services and as this is
a machine learning conference we it's
Barry would talk about the details
behind some of these histograms in we're
using a streaming approximate algorithm
streaming approximate histogram
algorithm ben-haim this let us build a
memory constraints summarization of our
of our data source in in in one single
pass and this implies scalability and we
can actually work on independent samples
of the of the data in and turn that into
a parallel leasable algorithm and of
course the numbers of the being the
number of beings uses for the histograms
determines the dilution asst of the
compression the more bins you use the
less loss you have in the compression in
there is a trade-off between CPU memory
and time consuming for doing that in our
standard histograms got information of
the objective fields in the within the
day within the dataset in that way when
we start computing our our history arms
we need to pass it the full data set
just once in the beginning also I graph
now that explains better and this
variation of course historians for
finding the approximate basis please
when grow in our our trees in the way it
works is the the master node
communicates to the workers the full
data set and and then the workers start
calculating the the histogram but first
in the step two they need to get the
carbon state of the tree in step 3 they
start calculating the the data sets the
histograms sorry and when when they're
done they communicated back to the to
the master node in step 4 and the master
node merges this this results in and
finds the bestest plead and grows the
next generation of the tree and after
that he communicates the new car in the
state of the three back to the worker
nodes and we start buy-in and the remedy
this really trees is we're using just
horn and where is torn because as you as
you probably know machine learning
algorithms are iterative in its being
talk right now that Hadoop MapReduce
strategies done really much the sum of
these algorithms and with the storm we
can we can lay out our an apology and
choose the degree of validation we have
in all stayed in the nodes when we're in
the process and if the data sets are
supple supple we can do early split
which means that the master node when
the master can request the partial
results in the water notes for
histograms and if those histograms are
similar we did in early split in that
speeds
think speed stops things a lot and our
testing so serves that in the early
stages of building the tree it works
really well and so you might be
wondering why trees and the reason
behind we're just increase is that we
find them approach approachable in their
easy to visualize so if we're trying to
make machine learning is for everyone
when it you know something our
representation people can understand the
presentation people can rock and we are
now working on the classification and
regression field we're not doing ranking
algorithms we're not doing time series
we're getting anything of that and as we
grow the tree breadth-first we have any
time algorithms that means that we can
grow the model while it's being computed
and that you can also request as
predictions even if the model is not
finished so building all this has a lot
of challenges of course and first one
might be that you know we have this
really net worth clients that are
working with us and we need to be
reliable but the problem most important
one is to make big things go fast and if
we come back to the previous example I
put about the cars and engines you know
fast cars are sport cars and they have
fast engines but in in sport car you
have a little drunk and you can barely
feed anything into it so what we need to
bill is something that has a fast engine
a certain sport card but that can move
big things as a track that means doing
the elephant run as fast as a horse nor
even faster then we want all of this to
be as easy as pulling a switch and how
do we measure easiness well you probably
have seen this before when you have you
know several remotes in your home and
they all look like you end up not
knowing which one is for what and you
know there are so many bottoms and it's
very complex and you look like these
like whoa what I'm going to do now and
well you end up doing things like they
is you simplify your remote you you can
even put notes from the paper and not
super simple you have a power
in a volume in channels simple but what
happens if something goes bro you've
covered all these bottoms that you know
they're there for a for a reason
but you don't have it you don't have
them anymore and we want to build
something easy but we want we want you
to have the poverty to the to do stuff
on it and that means that you know
following our driving example the
amateur driver in the professional
drivers both use our card but they don't
write the same way and here we're going
to have you know people new to them new
to the world of machine learning and
also professional experts on this field
and and we need we need them to have
power and we're trying to build an
interface that works more like a
sidekick that you know you can hide and
unhide advanced features depending on
what you're looking for in this way you
can tune up how you're building your
models in your data set and we don't
want our users to loot to stare at the
screen and now we know what's going on
what the data means what is this poor so
that's why we're using it for graphics
we're using a representation of the data
that people are familiar with and we
don't want people to have infinity steps
and with with this I mean those web
applications that have forums that you
know they are always telling you you're
about to finish and then they pop up
saying well you there's another step you
have to feel now we want things to be
simple and that what we created things
like one click model which means that
you can create a model out of a source
with one single click and that's amazing
we think that's amazing and of course
also you have to do an API in as I serve
as I've shown you we're building an API
in which you can rely and build stuff on
top of us in in in in that way you can
integrate with us create a new project
and and and and hopefully we'll have
happy customers so what's the theater of
BML well as I've said we're going to be
throwing the Alpha
we're going to be launching an alpha
version before the end of the year so if
you want to join us I recommend you go
into the website and leave us your email
and of course we're trying always to do
better but they better means even
working more on the interface trying to
make it more intuitive so people that
get lost and that's going to take us you
know it's going to take us some time in
policing details and we want to be
faster we will always look for being
faster and at the same time we want our
predictions to be more precise and we
don't want we don't want this project to
become a black box we want virgin in the
sense of the engine we want you to have
control of the parameters and control of
the tune-up of the engine and in the end
we are looking for building what we call
a marketplace in this would mean that
you could have your data sets and models
with us make them public if you want in
cert them with the world in and get a
part of the benefits we get of the use
of those data sets and models in well
this is it will of course we were hiring
so if you're interesting you can get in
cut in contact with us I brought some
t-shirts with me so I'm going to be
giving one teacher per question per
person till I run out of them okay so
this is it
what great because right now it takes a
lot of effort and a lot of knowledge to
do the same thing we thought our setup
products or applications and what people
in India in the industry world look for
are in the business world look for is to
do is to get predictions you know you
don't want the hassle of all these you
just want precise predictions in which
you can rely that's what does there's
our answer the question okay
well we're going to be handling these in
two different with two different systems
you can we're going to have URLs you can
pass to you can strain your data to us
and also you you're going to be able to
upload big files and for the streaming
part pretty much solves the big data
part like you could be also streaming
real time data with us
well I don't know the details but uh yes
we're going to be of course charging for
the use of the service we're still
discussing some of these of these points
but that will be charges for some things
and we've got our credit Simpson working
now but we need to we don't have fixed
prices yet what we aim to have any kind
of user which which is a hard task of
course I know but our typical user will
be I guess because it's a hard question
I some somebody interesting in
predicting you know that's very nerve or
example somebody in the marketing world
that needs to track information about
the data they have or somebody in the
technical department of a company you
know
no now we're being realistic with this
and we are doing very complex heuristics
for finding out you know how your data
is laid out and also if we don't happen
to to find a good match you've got the
option isn't there to tell us this is
why you're wrong and we will try to
improve from the user experience so
we'll try to also learn from the way
that we're failing at analyzing the data
well the sorcerer say I sewed it's not
perfect actually it's got some columns
that are empty so we we put a talk in
there that no value you know and you can
you can actually tell us what no value
means or what token to use in in if
there is a lot of data missing and it
doesn't make sense then we're going to
request you to manually operate the file
just extracted the three
no at the moment we're not looking into
into that into having like support
system for stretching data or that
one type of what sorry well uh we now
work with binary 3 so as I said but but
we will it's not here it's not released
as well but we are actually now working
with other trees for now we'll have
trees but I we probably have more things
in the puter sure yes we're playing that
in at the moment what you have to do is
actually reblog like for example if your
data change and you will have to reload
a whole the whole day to gain but we're
now working on that and we're playing
because we know you know many people
have requested that and have people that
we we talk with and tell them to tell us
to to get this into a con like you know
my change my data changes of course data
changes all the time
there are of course we've read
competitors and I guess in the startup
world they want that things that doesn't
have competitors is a fool and what I
guess you know the key here is the
execution of the idea and this is
something everyone says all the time but
we're trying to be a different product
we're not trying to be able to get the
competitor spreading and we some of
those competitors work for very specific
things they work for sample owner
analyzing texture or time series or but
we want to be more widest pick me and I
I didn't hear I didn't hear the last
part
well I guess this is my personal opinion
okay uh I'm not saying everyone is going
to understand this not everyone
understands everything in but we've sown
you know these two people that are not
in the field and they really understand
what we're doing and they are very
interested into into seeing more and
they they get interesting into the
machine learning world which looks like
now from the outside world looks very
scientific you know very very technical
and not very practical so people get
afraid of it well that I'm going to ask
you to forward if you have any question
to charge surcharge the person in charge
of the storm part I think there's
there's a talk today hear about you know
between the difference between Hadoop
and is torn for machine learning
algorithms which is going to go very
into detail but I've said what I know
basically you know this is not my field
so come on grab your t-shirt if you ask
a question
yeah come on the second</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>