<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>deBGR: An Efficient and Near Exact Representation of the Weighted deBruijn Graph | Coder Coacher - Coaching Coders</title><meta content="deBGR: An Efficient and Near Exact Representation of the Weighted deBruijn Graph - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>deBGR: An Efficient and Near Exact Representation of the Weighted deBruijn Graph</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/09RVX85Ljhw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today we have Prashant fambly here
with us too I'm listening I apologize I
was just calling for Sean who the
student at Stony Brook working with
Michael bender and Robert Johnson so Rob
bouncing still and he's worked on a lot
of like um sort of integration of like
system stuff and algorithmic stuff so
he's done like cryptographic file
systems he's done a bunch of stuff with
fixing errors and DNA so he's gonna talk
to us about today you know so we're
gonna you know data structures that we
can use for representing like K more
graphs and you know he's kind of gonna
explain more than I'm gonna be able
explain it but I think it's kinda
interesting stuff I especially like the
idea of you know what kind of algorithms
can you do in a compressor out I think
that's kind of an interesting question
so there's on take it away all right so
today I'm gonna tell you about debugger
so debugger is an efficient and near
exact representation of the weighted
agenda so in the talk I'm gonna tell you
at a high level that's what is it a
billion graph and how it is used in the
field of computational biology and some
other issues with those kind of graphs
and then we'll see how we can to like
represent these graphs that are very
very huge and in a very small amount of
space efficiently and directional spell
on them so let's start by talking about
sodium - so the bloom graphs are
ubiquitous in the field of computational
biology they were introduced in 2001 in
this field by and then since then they
have been used in many different
applications across the field the input
in computational biology is raw
sequencing data it comes in the form of
like strengths of databases and likes
lots of them and all this input data the
Rosses like sequencing data is first
converted into a different graph like
representation and then later on it is
used in many different applications in
the field like sequence search and
transcriptome SMD genome assembly and
also like tweets there any correction so
in short depend graph is the data
representation at the heart of a lot of
sequence analysis
so let's see what does the d-brane graph
looks like
so the brain graph in computational
biology is is subducted of the actual
d-brane graph in the math bearing there
is an edge which is a string of length K
and the edge or the the tail and string
it connects its to K minus 1 sub strings
in the graph for example in this case
the edge is a string of length 5 which
is over the alphabet 0 and 1 and it is
connecting its to K mention sub sub
strings so the 0 one's raised on the
left and 0 1 0 1 is on the right so
that's pretty much how Dipendra looks
like so all the nouns of K minus 1 yeah
so when you created the brain graph you
first decide how how long is going to be
and all the nodes of K minus 1 also I
want to make this clear that in this
talk we're going to use this like
definition here which is the length or
the edges are of length K and the nodes
are of length K minus 1 but they are
like different applications where they
use vice versa where they create all
these nodes that of length K and edges
are of length k k plus 1 and so on so
but they're gonna use the edge central
definition of the graph in this talk and
are you going to be having all 2 to the
K minus 1 possible nodes or is it going
to be a subset of that's a good question
so when you talk about the planar graph
in Mac then yes you have all the
exhaustive like all possible routes in
the graph but in the computational
biology these input like these edges are
coming from the input dataset so now
they are not exhaustive so they're all
the distinct edges that the present in
the input dataset on top of it so and
the edges are always in pairs with the
property that you can fix one to the
other
exactly so you can see that these nodes
like these nodes are of length K minus 1
that is 0 1 0 and 0 1 0 1 so they always
share like prefix of length K minus 1 so
between them and so on so that's the way
you actually create this graph or a pen
put in a set so
an example in in computation biology the
input data is present in the form of
leads and read is actually a string of B
basis over the DNA alphabet so in this
example you can see this is a read which
is at the any alphabet and if a straight
and from this all those reads we
actually extract gamers and we're a
Kimura is a substring of length K and
the extract came as using sliding window
so here we extract the chaos of length 5
so we first state in the first 5 bases
and then we do the like right shift by
one position and then we extract the
next k-mer and so on so we have the
input data is in the form form of leads
and then we extract the K list and then
we use these gamers to build a stapler
in graph and these came Camus acts as
edges in the different graph so to see a
complete example you can in this case
you can see there are two reads and we
extract Camus of length 5 from these
reads and so there are only three
distinct gamers in this input dataset
here and then we build this graph on top
of it so you can see for example in this
case this edge a T it connects nodes a
that is of length Force which is K minus
1
pretty prefix of this particular edge
and the other node on the right is again
K minus 1 sub substring of that fridge
and so on so that's a very simple the
green graph so potentially we could also
connect like the first know on the on
the left and some of the nodes from the
from the right yeah but we don't do this
because they are not I just sent in the
in the input so once you have extracted
all the killers then you do have
information about the the positions of
these schemas in the actual input
dataset so in the graph you can so
connect anything's possible so even if
the they could be loops there could be
nodes across different places in the
actual input data set they can be
connected in the graph because once you
have extracted
k-mers then you don't have this
information about where they're actually
present in the input data set so you
just take these cables like edges in the
graph and then you represent these
graphs which of these four possible so
the rule is
two loads they share K minus-1 like
strength and they will have an H in
between them that's pretty much rule
there but you only take the edges at
your present yes so I just comes from
from the input dataset of course yeah I
guess the question is so you had five
five murmurs here you define these edges
yeah I think you're saying that if there
were a string C a AC Indian but you
definitely put an edge from that left
node to the right bottom question is if
CA a AC was not in your input you still
put the edge or not know if the edge was
in then they won't be because okay
so here in this case since we are taking
the edges from the input then if there's
no agent they will then there's no
camera in the input then they won't be
an edge but if we take notes as an input
then you but here in this case because
it's essentially all the cameras are
shuffles around that's exactly what the
motivation of this if you graph this so
the problem here is that in biology you
have these long like genomes which are
like very long strings and for a human
it's the length of genome is like 4
billion bases but when an instrument
reads that it reads that gentleman in
facts and so what the issue is now you
have some multiple ribs and these reads
are parts of the genome so you have to
align them together to form the ACTU
genome back so basically in this example
you can see if we have some multiple
reads and you will this different graph
on you using those reads and now what we
do is that in a typical assembly process
in which you have to construct the whole
genome back from this input dataset so
what he knows that you traverse this
graph or like walk along the path in
this graph and what he do is that all
non-branching paths in the graph they
are also called as contacts and you all
put those and then they are used in
downtown stream applications to actually
clued them like together to build the
final genome in this talk I'm not gonna
go into the details of how they glue
these together because they use
different kind of algorithms and
like you just Excel to glue things
together to find the final genome but
the motivation is that you have input
dataset you represent it in the form of
a different graph and then you walk this
different graph to find all the
non-branching fats which are then used
to actually form the final genome and
these non-branching paths we do that
because these long behind branching
paths across fields they actually like
represent some part of the actual genome
so in this case you can see if they are
like the genome is there and all these
non matching parts like in these colors
like orange or like blue they represent
some part of the actual genome so we can
do them together to form the actual give
them back so the motivation is that
again so you have this graph you bakit
diversity and then you create the genome
back so this is only so one of the
applications in computational biology
there are so many different applications
they use this graph now so when you talk
about assembling assembling a genome
then only the green graph that has the
topological information works well
however there are many different
applications wherein you need so much
more information than just the topology
of the d-brane graph for example there's
a kind of application known as
transcriptome assembly where in the deal
with data set like RNA data set from
multiple like different species in a
single graph and the grass tend to be
more complicated in those cases and also
when the instrument suite those like
arrays from the actual organism don't
really with an equal coverage so they
read it for some parts very efficiently
and some parts not very efficiently so
they really need the abundance
information in the graph from that I
mean they just they they don't just want
the pure graph but they also want the
information about how many times or so
what is the abundance of each edge is
there in the input dataset and for that
kind of use cases we have operated
different graph so beta-tubulin graph is
similar to in topology to patty brain
graph however now with each edge you
have the abundance of that edge in the
underlying data set attached to it so in
the same example they have two reads
here
you can see the edge CA appears twice
across different needs so it's
abundances too and it is attached to it
in the graph so this is a typical way to
develop graph and the challenge here is
that now with the graph you have to also
store the beats or the witness of each
each edge in the graph and later in this
talk I'm going to tell you that even
though storing extra information is an
obligation but it's also an opportunity
at the same time I'm going to tell you
that how we exploit to our advantage
these available information in the form
of weights in the graph okay moving on
so we have talked talked talked about
like back the green graphs how they are
used in the field and whatever
beta-tubulin graphs and why they need it
now the big issue here is that the space
usage of video the green graph so the
brain graphs they store only Kaymer's
and so the memory usage of the
representation of a weighted Imperium
graph it scales linearly with the number
of unique gamers in tena set for example
to represent the brain graph of a human
genome that has few billion this
thinking is in the input dataset the
state-of-the-art system takes something
around hundred gigs of space but when
you go to Tran transcriptomic assembly
and they use the meta genomes and for
example if you take the meta-genome of
soil wherein you have millions of
different species in a single dataset
then the amount of space that the
state-of-the-art system takes to
represent the weighted different graph
is of the order of few terabytes so this
like researchers in the past they have
been using beefy server machines and
testers to actually do the analysis on
this used data sets and graphs that they
create all of these data sets so like to
represent these graphs
compact is big issue now that if you
don't want to go to me so many people
like different machines and if you want
to do the analysis on a single machine
then super one thing is that you want to
like represent these drafts as compactly
as you can so to this and in this talk
I'm going to tell you about compact
representation of the beta-tubulin graph
and a compact representation as I said
earlier it will enable to
transcriptomic assembly on machines with
very less amount of resources and it
would also enable you to assemble some
of the fundamentally large data sets
that were not possible to assemble on a
single machine before because the data
sets and the size of the representation
was very huge that's why you needed to
multiple some machines to store the data
and do some other kind of tricks but if
you can store them compactly then you
can store them on a single machine and
still do all the analysis now there you
go in the the second part of the talk
they will start to talk about how do we
actually represent a different graph for
a beta-tubulin graph and a start up will
first talk about the past work that how
they did some work to actually like
represents the green graph compactly and
we'll know about the tricks and then
we'll talk about like representing
equated in compactly so to start off by
representing a debryn graph so on your
right on your right hand side you have a
deplaned graph where you have some edges
and the nodes and the edges are coming
from the input data set and since the
beam graph has these edges we can't
represent the green graph as a success
as a society of all these edges and all
we need to do in the graph is that we
want to traverse the graph so we can do
that by storing only the set of edges
because if we want to find out what are
the named the neighboring edges of a of
a given edge are so what you do is that
you just shift the given edge by one
place and you append all the four
possibilities of the basis and then
quarry for all those possibilities and
if you find any of those edges are there
then you know that how to go forward in
the graph and you can find all those
paths so to represent a different graph
all you need to do is that only
represent all these edges in a set and
since the number of distinct edges or
the elements in the set are huge given
the data set or the input data set one
way to reduce the space of the
representation is to actually store this
set compactly
and when we destroy the said so
compactly is using an approximate set
representation and to do that we have
these approximate membership query data
structures and AM queue data structure
supports two kind of operations you can
insert an element and you can also query
for the existence of a given element in
the set representation MQ is an lossy
representation of a set as and it is
often very compact but the space earings
comes from the fact that it occasionally
returns a false first positives to you
queries from that I mean occasionally
when you query for an element X in the
set it may say yes that this element is
there even though that element is
actually not present in the set so
that's the false positive query or an
answer the most commonly known or
commonly used a mq data structure is a
bloom filter and I guess it's used
across different fields and computer
science and yeah so obviously spell at
all in 2012 first showed that how you
can actually represent at the green
graph using a bloom filter however since
bloom filter is an approximate
representation of a set the graph that
you will get when you use a bloom filter
to represent a set is not exact graph
and you will tend to see some errors and
all these errors are the topological
errors in the graph and these appear
because so they're in the graph you can
see the nodes and the edges in the green
are coming from the actual input date
data set but nodes and edges in the red
are introduced in the graph because of
the errors in the bloom cetera and these
appear because for example if we are at
a node say so I if you write an edge say
C CGA and you want to find out if there
is any neighboring edge on the right
hand side of this particular edge so
what you do is that you shift that edge
back one base on the left and you get
CGA you append four different bases to
it and you try to query fir for that
edge now and the educators maybe see G a
and G and if you qualify that edge even
though that edge
doesn't exist in the input dataset it is
present in the sent because now you're
representing the set as a bloom filter
so now you will find a false part in
your graph and some falsehoods in the
graph as well so you when you represent
Dupree in graph using a bloom filter and
you traverse that graph then you would
end up seeing some extra of false edges
in the graph one of the Rif like results
in the paper was that if the error rate
of the graph sorry if the error rate of
the bloom filter is less than some
percentage say one person and the
overall structure of the graph is still
intact and this comes from the fact that
if in your graph you have two components
that are far apart and then there's very
less very small possibility that two
distinct components or edges in the
graph have part in between them that is
a false path because to connect two
different k-mers with a path length of
say four the probability of that is very
small because it will be a
multiplicative ability so if the error
rate of the graph is in general one per
percent so they have a path and have
four false paths between two cameras or
two edges will be one over hundred to
the power 4 so which is already very
small so one of the result they showed
empirically is that they created like
they have been bunch of graphs by they
they had a structure of the graph and
they inserted all the elements of that
graph in zoom cetera and they try to
traverse the graph in the in the so
using the bloom filter and they ended up
with the same shape of that sugar so the
error rate was small so this overall
structure of the graph who doesn't
change much so ever to me in this the
connected components not change yeah so
like one of the results on the graph in
the paper was that they had a big graph
which was a big circle actually so the
structure was a big circle and now once
they inserted that graph in the bloom
filter and they tried to traverse it did
find some false paths but the false path
they were not very long like they died
off very soon of like two or three hops
so the overall structure of the graph
would still remain the same you would
see some tweaks here and there when you
do the path length traversal but
otherwise the overall structure would
still remain this
that was to buy one of the results and
most applicant even though this is an
approximate data structure most
applications in computational biology
they want an exact representation of the
graph for their analysis so it was a
starting work but it's not so getting
used in so many of the applications in
Kampai because they still need an exact
representation so building on top of
this work hello at all in 2016 they
showed that you can actually exploit
some inherent redundancy in the input
data set to actually like reduce the
error rate of the bloom filter without
increasing the space usage so what they
showed is that there's a very high
chance that every Kaymer
will have it's a decent game is also
there in the in the set so what you can
do is that if we query for a given k-mer
in the bloom filter and the bloom filter
says yes and if whoops adjacent cameras
are also they in the set then there's a
very high chance that that came in is
not a false positive answer however on
the other hand if we query for a k-mer
or an edge and the bloom filter says yes
and if none of his Edison cameras are
there in the set then there's a very
high chance that it is actually a false
Kaymer so in this example you can see so
before there was an edge a T in the in
the set because it was there in the
bloom filter even though it was not
there in the input dataset now if you
query for adjusting edges of that
particular edge so none of them out
there in the actual bloom filter so you
can say that with a very high confidence
that that edge is actually a false
camber and not a real camera so if we
just do some extra queries in you can
your data set you can feed out a lot of
false cameras from your bloom bloom
filter well why do you get rid of
hh-hey a lot yeah I'm going and coming
to that so that is because here you can
see this edge is GA GT right so you can
see if you look for a right neighbor of
that edge there is no right neighbour
like a GT or something like that so
there's no fight game edge or the
neighbor of that edge but there's a left
camera of that is there so what you can
do is that you can have different kind
of rules in your in your algorithm so
you can have a fake strict rule
to make sure that any game is actually
real like real gamer both of his Edison
cameras have to be there in the set so
in this case only one of its neighbors
is there so you can say it's false kima
however then you can so question me that
they could be a real gamer or an edge in
the data set that actually only has it's
only like right or a neighbor of it
because it could be the start of a read
so you don't have any of its left
neighbour so before they do is that in
that case they store some extra exact
information and that extra exact
information is how many times an edge
appears at the start or the end of the
read so if this appears at the start of
the read then even if it doesn't have a
left neighbour they're still okay and
some thing appears at the end of a read
even if it doesn't have a look like
right now it is still okay but for
everything else if something is there in
the data set it must have all of its
books both of its side and the left
neighbor in the graph and said so has to
be in not false positive came so in the
case your stories of the outside the
moon filter yes and they can do it when
they actually insert everything in the
data set so they make each and every
read they can store that information but
the number of reads is pretty tiny in
one wheel so I can give the exact number
so we have done our experiments with
bunch of data 6 and 2 a honest amount of
information you need to store this thing
like information about start and end of
reads is about 10 to like 20 percent of
the actual data set so it's very soon
like it's not very big but it's Google
quick and we're gonna use some some of
these tricks in act in hard work as well
moving on in 2013
such a key in Rick's and Celico they
showed how you can convert this
approximate representation into an exact
one but they did it only for a specific
use case and the use case that you can
only that reverse this graph and you
can't sue queries like randomly in the
graph and the inside the hiders once
you're conversing this graph and you
don't want to go on any false paths
so so all you need to do is that like
remember all those edges that are
directly connected to the actual graph
from that I mean in this graph if you
are trying to traverse this graph all
those green nodes and edges are the real
northern edges and if you want to not go
on any of the false paths all you need
to remember is - edges that it directly
connected to the actual graph and as
soon as you see those edges you can stop
your Tercel on that particular path just
by remembering a small subset of all the
false premise in your graph you can stop
going on any for any of the false paths
and you can say say say that when you
traverse this graph you will always have
all the exact answers and you will never
go on to false paths and they call these
false team as a critical false game is
because these gamers or edges are
directly connected to the actual graph
is that not clear what they're going
like a complete reverse of the graph in
the proximity of structure and they see
each time they traverse by keeping all
so what they do is that they use a
two-phase algorithm to do this so in the
first phase they go through all of you
all of the data and they insert
everything in the bloom filter and the
belt is bloom
okay now this bloom filter they're going
to use it to actually traverse the graph
so what you would end up doing is that
anything that is present in the data set
you will query for for all of its edges
and cameras on the right hand side
adjacent cameras on the left so the
number of queries you can do in this
graph to traverse it is very limited so
you if you know all the distinct
chambers that they named you later said
you can find all this all those queries
you might end up doing and if you have
all those queries you can do another
pass on your data set and find out all
critical cameras that are the critical
false cameras that are directly
connected to your actual graph and then
they can store that small amount of
information in RAM they don't need to
read their data back again so they just
do it to puss like algorithm on the gate
which is stored on disk and then they
store a small amount of information the
bloom filter and this small exact data
structure of
all schemas in diamonds and they
considered an exact representation as
far as the traversal is concerned okay
just end this for to restate what a
critical false positive this is the
first edge that went wrong yes the foot
was des that went wrong interest in sur
by traversing the approach via the bloom
filter version of the ground so they
know all possible things that might be
that first paw and then they're on the
second pass they're checking to see if
any those actually decision the original
take yes yeah the how are they given
that this whole thing isn't fitting in
memory like how are how are they doing
so they do it otherwise so that's good
once they build the bloom filter then
they find out all the possible like
where is the set of that and then they
do second pass in there on the data set
and they find if this camera is there in
this right or not and at the end of the
day when they have gone through all the
data again they would know that which of
the keys are there in this set but not
in the input which is a schema but that
one may also take some non-trivial
structures yeah yeah it's like temporary
they don't need to store it for so I'm
telling you about this work especially
because one thing to note is that the
kind of guarantees for exactness they
can give is only when you want to
traverse this graph and to do that you
must need one seed Kaymer from each
connected component in the graph because
otherwise you can't start your search or
like the traversal
however they do not have any guarantees
if you find them Vickery for a given
k-mer in the graph for example here you
could see that using this technique
technique of like redundancy you could
like remove this schemer as a false
Kaymer
because they know that none of its the
neighboring edges are there in the graph
so they can say that it's false camera
however in this case since that edge is
not directly connected to the graph they
don't have any guarantees if you query
for that particular edge if it is false
or not so even though this is exact this
is exact for every constraint case in
which you only can traverse the graph
and then only it is exact otherwise it
is not exact so they are not exact for
membership query kind of
station so this is an important point so
these are the three kind of past work
where people have used bloom filters to
actually like represent debris enough
compactly and did some electrics to
either make them exact but like we'd use
there the rate of the bloom filter we're
going to use some of these tricks and
take them to their logical conclusions
to actually represent a peter degraph
approximately and sorry so compactly and
exactly as well so now let's go and talk
about how do you represent operator
deployed graph so beta-tubulin graph is
like similar to the brain graph but now
you have abundance attached to each edge
in the graph so instead of a set now you
need multi set to represent the weight
of the blowing graph so in the multi set
I mean you have the edges and you have
the balance of each edge in that graph
attached to it so similarly or analogous
to like representing a set compactly and
then you need an AM queue data structure
so to read like represent medicine
compactly and approximately you can use
accounting failure guide a structure so
counting fate of data structure is a
generalization of an e mq data structure
it supports three operations it's a you
can insert an element you can also
delete an element and you can securely
for the count of a given element similar
like analogous to a name key data
structure accounting so filter is a
lossy representation of the multi set as
you had false positives in the in a name
created a structure you have over counts
in this kind of a data structure from
over counts I mean whenever you query
for the count of a given element at
times it might like result in an
overground
but it will never return in an
undercount so the errors are always
one-sided in this counting filter data
structure commonly known counting filter
data structure is accounting bloom
filter but they are not of like issues
with accounting tune builder to use for
a very skewed data set I am NOT going to
go deep explaining to those issues
but I just want to point out here is
that the state-of-the-art counting data
structure is accounting quotients a
filter is like our work it appeared
earlier this year in at excite mode
and so it is a state-of-the-art data
structure it is very so good first cute
kind of distributions and I'm saying
skewed because all of these like
real-life data sets like these graphs
and these multi sets or the graphs
they're all very skewed like few of the
cameras are going to appear like million
times and others will appear only once
or twice and so on
so there's very skewed distribution of
the abundance and so again earlier this
year we had another paper at its
bioinformatics they'll be sure that you
can actually use a counting filter data
structure to represent a weighted
ability brain graph approximately and
using a very small amount of space
however one thing to note here is that
now along with the topological errors as
you had already seen in a deployment
graph or the approximate representation
of a given graph now we have abundance
errors as well so a banasura appeal in
in your set then two elements or the
edges in your input data set that do
exist in the input data set they collide
with each other and they give a line
like they obtain the result in the
account of both edges for example the
actual page of yeah so the actual weight
here for this edge is two and this is is
also two but when you represent this
using a counting filter data structure
what happens is that they could to
collide with each other and they can
result in an account for both the edges
and so now we have two kinds of errors
awareness errors and the topological
errors when you have an approximate
representation operated upon graph and
now the challenge here is that we want
to get rid of all these errors in the
approximate representation of the beta
to grain graph so in this paper debugger
we have an algorithm that uses the
counts in the approximate representation
in an accounting shuffled data structure
it's relatively self correct all the
approximation errors it connects both
kind of errors abundance errors and the
topological errors in the graph and also
be give higher guarantees in terms of
the structure of the
because now we not only support
traversal queries but we also supports
membership queries so with a good
confidence or with high confidence we
can say that you can sukhani for any
cable in your data set or from the
universe and we can save this that
Cayman is actually there in the site or
not
and doing all this we difficult takes
only like a lien to kind of contain
person extra space compared to the
approximate represent excitation and has
no empirical errors in the
representation so now we're going to see
how we do that so we do this so in the
people that we used a simple
beta-tubulin graph invariant and the
invariance is says that the total
incoming abundance at any node is equal
to total are going abundance or that no
so in this graphical you can see the
bold blue line is the path of the feed
in the graph so they're two reads coming
in at node eh-eh-eh-eh and then two
leads going out so the total incoming
abundance is actually two because the
edge evidence of the edge coming in is 2
and the total are going up - is also 2
so you hire them up and that it should
hold at every node in the weight in the
red graph however there are a lot of
nodes in the graph where you need some
extra information to actually enforce
this invariant and those nodes are the
ones that appear at the start of the end
of reads so for example in this case you
can see the node again label - it's
coming in so this total incoming
abundance is 2 but there's only one tree
going out so saddam like outgoing
awareness is only one however if you
record that how many times that node
appears at the start at the end of any
of the feeds you can use that
information to actually impose this
invariant so this node appears at the
end of a read one so you can use that
information to actually the percent so
if you notice at the start with one
reading at the end of another so it's
one kind of one you can get away with
skipping that right because so yeah I
mean so the
of Norden actual graph there's no twitch
ever at the start there are no papers
which are there at the end and then as
you said that it could so connect like
two leads together and it conclude two
reads together and there could also be
cases in the actual graph where where an
edge leaves the node and also comes in
the node as well because it can have a
loop of itself like for example if a
node is there like a and then if there's
an another node if you just go to the
left if we append and B you will have a
again come back and so on so there are
lot of corner cases natural
implementation that we dealt with but on
high level like if you just or this
amount of extra information and how many
times given node appears at the start at
the end of read you can enforce the same
weight on all the nodes in your craft so
here is a representation of the
beta-tubulin graph in a system which is
debugger so the graph is there and we
have three data structures the one on
the far left is abundance data structure
and in which we actually store the
abundance of each and every edge in your
input data in your graph and these two
data structure on the right as I told
you before that these actually store the
information about how many times our
given node appears at the start to the
end of the reads
so these to the right to a data
structure exact data structure they are
not approximate and the one on the far
left is actually an approximate data
structure which is the company filter
data structure and the size of the data
structure on the right is again very
small because given the actual amount of
data set the number of reads in the data
set is very small and it accounts for
like 10 to 20 percent of the overall
data size so the one on the left is the
actual data structure which is very very
huge and the other two on the right is
mod leader so even though they are exact
now let's see how the invariant works
and how do we actually do that like the
error correction in the graph so as you
as I told before the one on the left is
an approximate plan should they could be
errors and in this graph they are too
kind
kinds of errors the one is an edge CCGT
a that edge doesn't appear
dataset but it appears in the in the
counting super immature because of the
false positives in the data structure
and so that's an edge it's a new edge
and it's topological error in the graph
and we have another error we're in the
paid or the abundance of forgiven edge
which to exist in the graph like appears
to win over count so we have an
abundance error as well so we have to
address and they in this graph and we'll
see now how do we fix all these errors
so the algorithm is very simple what we
do is that we go to every node and try
to enforce the d-brane graph so weighted
degree enough invariant and if the in
invading doesn't hold and if we can
change the single thing in the graph
that can fix the gradient then we do
that and we go ahead however they could
be cases in which they could be some
multiple ways to modify the graph in
which we can enforce the ingredient and
in that case what we do is that we go to
nearby nodes learn more information
about the graph and then we come back so
that they must be only one way to fix
the modify the graph to actually enforce
any will end on that note so let's try
to fix the errors in this graph so let's
first pick that node on the far left you
can see the invading doesn't hold here
because incoming abundance is 0 and the
minus is 1 so it doesn't hold so even if
we use extra information that is the
number of times it appears at the start
to the end of frites it's still 0 so we
know that there's only one way to fix
that fix the graph or modify the
graphical actually impose the invariant
and ways that the total outgoing
abundance should also be 0 and so if we
do that if you use the abundance of that
edge we can fix this we can enforce this
ingredient and now you know that the
peak of the abundance of that edge is
actually zero and not per and so you can
fix these like topological errors in the
graph now let's try try to enforce this
and we ain't on this node and you can
see the it doesn't hold because in
coming up - is - and the outgoing
abundance is 3/2 plus 1 the abundance of
like outgoing edges is 3 however now to
fix this we can
to modify the graph in two ways we can
either decrease the abundance of this
edge back run like that case the
Imperium will hold or we can also
decrease the or like decrement the
abundance of this edge by one and they
main would still hold so there are two
ways to modify the graph and we don't
know which way is the correct place and
you're not allowed to increase the one
on the Left or three because the
counting booth over over count yes so
you can you can only detect decrement is
always one-sided so you can always let
the crease there yeah so here we don't
know enough information how to modify
the graphs about what to do is that if
you go to neighboring edges and we learn
more information so if you go today like
this edge and try to enforce the
invariant and you can see the incoming
abundance is 0 but this node appears
twice at the start of reads and are
being AB minus s2 so we don't need to do
anything in the graph to actually
enforce in vain because anything else
holds here but we learned there
information that the weight or the minus
of the this edge is actually correct
because we didn't have to do anything
now we go to this node in frightened for
saying gradient and you can see here the
incoming of line assist one hour doing
abundance is zero but this node appears
buns and and of lead so again we don't
have to do anything that variant holes
here and so we get this information that
the fate of the neighboring edge is also
correct now we can go back to this node
and try to again mod so you only know
that's correct
after examining all us or possible
incoming and so if we go back now to
this node and try to modify the graph
again and now we know that there are
three edges involved in this equation
and two of them we know that German is
correct so there's only one way to fix
the graph and we do that and we can fix
basically all the errors in the graph by
using the simple invariant the algorithm
here is a standard super QL Gotham where
we bootstrap the got'em with a set of C
edges for which we know the abundance is
correct and then we try to expand this
set by going to each and every node and
trying to enforce invariant and if we
can fix the graph
a single way to do that and we move on
so we have a proof in the paper that
this algorithm is linear time algorithm
like it is n times log n over log 1 over
4 epsilon where epsilon is the error
rate of the underlying counting filter
data structure so the amount of
guarantees of the amount of space you
might end up doing is depends on the
error rate of data structure let's
quickly take a look at some of the
results so we did some experiments and
we took like four different data sets in
in computational biology and all of
these data sets had like a few billion
distinct gamers as input so all these K
demos will act as edges in the graph and
we try to like compare our system with
other approximate and exact weighted
d-brane graph representations so
approximate beta-tubulin graph
representation fiddle stood up the art
in that is squeaker which uses another
counting multiply the structure and the
squeaker exact is exactly presentation
of that of a vegeterian graph and it
uses a hash table to do that and you can
see in this graph on the x-axis we have
four different data sets and on the
y-axis we have the space usage in terms
of number of bits per cable needed by
the system or the representation so
since the bias is spaced lower is better
you can see for all the different David
intersect debugger being exact takes
almost half of the space compared to
state-of-the-art other exact
representation and compared to the
approximate when it takes some extra
space and that extra space is 18 to 20
percent for all different data sets
however when you talk about the accuracy
you can see that the errors in boots a
squeaker exact and debugger receiver
because they're both exact however the
amount the number of errors in squeeker
is very large it's of the order of
millions so even though debugging takes
only eighteen twenty-eight percent extra
space compared to squeaker but there are
no errors in debugger and we fix all
these errors using the simple error
correction algorithm in the graph so to
conclude one thing to note here is that
as I told you before in
that storing this extra information in
the graph in terms of debates or the
abundance is an obligation but it's also
an advantage because now we can use that
extra - information to give higher
guarantees on the graph structure like
structure and actually fix all those
errors in the graph using only the paid
information in the underlying data set
and the last point which could be more
interesting to us is that now they are
started like looking into other graphs
in different fields like like paragraph
in Amazon and some of the graphs as well
and we are trying to see actually
exploit some domain specific information
though though those graphs to actually
like represent them approximately and in
a very small amount of space and still
fix all those edges in the graph the
code for this lecture stem is available
open source at github you can go and
give it a shot thanks yeah so that
algorithm that was he said the run time
was s log n over log or Epsilon what's
the space usage of that like you mean
there I think space usage yeah okay so
it is pretty much I mean we don't need
any any like extra space during the
runtime so because we maintain this set
of C edges and you can you use that like
when you have this graph you already
have set off the actual like number of
edges right yeah so what we do is that
we traverse this graph go to each and
every node and we find we try to impose
this invariant on each and every node
and if the invariant that doesn't hold
we know that there's something wrong
nearby in this node so what we do is
that we have some coverage people are
bound in the paper actually so I didn't
notice the ingredient doesn't hold we go
to D hops from from that particular node
and insert all those edges in a separate
set and say that the - of any of these
edges can be wrong with high probability
and so we and
so when you tell us this whole graph the
number of nodes actually where'd the in
vain desin whole is a property of the
amount of error in your actual data
structure so that is a factor of your
epsilon here so let me ask another
couple wise what's n there anything the
number of distinct edges in your graph
okay so what's the output of this error
correction algorithm so the output is
that so here so you can see that in this
in this like representation you have the
abundance like data structure right and
this awareness data structure can only
result in an overcome so at the end we
store this small exact data structure
that only store the daily daily dollars
for any edge that has a wrong count in
the actual abundance data structure and
so what we do is that now I worry for
any particular edge in the day in the
representation you first find the
awareness in the actual data structure
and then find if that Cayman or the edge
is all super is present in the final
error correction data structure and if
there is some abundance in that then you
subtract that abundance from is given by
this data data structure so I guess what
I'm having trouble understanding is well
it seems like the output of this
algorithm is lie sounds like that's
maybe sort of like what squeeker is so
you have an edge here and if you have
this speaker exact
okay exact is exact hash table it's a
it's an exact hash too but like you can
use any hash table if you want and
that's exactly that that is quicker
exact but hash tables take a lot of
space now in develop what we have is so
we are this small
representations like the ones there that
stole the come for the number of time
and he loaded up using this time to the
end of G it's a small amount of
information but when you run this a
declaration algorithm on this
representation then you build outputs
another exactly destruction hash table
again but it does that is citizen edge
like a CP here and abundance is school
here but when you did this error
correction and if found out that actual
abundance is not good but it is three so
as you have to declare society Cuban
somewhere so it will happen key here
with the same edge and it will have one
here - one exact match so now when you
return any debate of anybody came on you
can eat here first and it will give you
four and the need you can afford acting
here and if it was here solutely try
again uh so why don't you just store
this entire thing is your compressed
format what's the usage of the somewhat
smaller format that's really difficult
to query there as well because it's
something based on like those things
aren't okay so destruction
this you need like during the
calibration but you don't need to take
your own bungee up on this table can you
take these tables actually so we just
need like this okay so the sizes here
recording or after this actually is here
because okay you can on the fly you
actually
nearby fix the errors there itself
because this elevation is very local you
don't have to know everything or the
graph you have to know and only about
your neighbors so you can do that thing
on the fly and do and with other
versions back and if you do that on the
fly the error like the time for that
that's when you do it for all the edges
but if you want to do it only for one
then this will be you carry that
actually
particularly okay it was very fast and
the most of the cases you don't have to
do any recollection because this thing
is very smart usually and for us
this thing was like point zero zero so
the number of this so question so I
understand if you're starting at a point
you can sort of walk and that's a way of
discovering errors but if you if you
have these disconnected components are
you just doing exhaustive search over
all four to the K k-mers to see if
they're in your like there's no way of
look just looking at your bloom filter
and figure out what are the false pause
is Right right so so you have to query
it just to see is this in there yeah
right and so are you doing is just some
exhaustive search over like how would
you how would you find yeah how would
you find CC GTA how do you know the
query for it how do you know yes so so
you might know some some node and you
can traverse I understand that how do
you discover edges that are in your
approximate data structure that
shouldn't be there how do you even know
to query for them
you're just creating fun are you looking
at all four to the fifth combinations in
this case and this message is gonna say
yes it is there I honestly haven't you
choose that came came here to query for
applications used is representation so
they could be many applications that use
this as a number line representation and
they want to do queries that's fine for
the under Phi one but you said you're
doing global error correction so if
you're doing global air attraction you
need to sort of found them all first
right yes okay no I mean what do you
handle that on global as you construct
this system you
available to you yeah and you can start
with a seed Kaymer
so you need to see seed came out here
right but you don't need exercise
because you already have the same nodes
because these are already exactly so we
use these functions to actually start
our search okay you started every seed
because because we have three structures
if we don't have these then you actually
one cable from each connected component
in your graph to start your like
whatever exploration of the ground and
then go in and this kind of a use case
is a different one because what people
do in many different application is that
they take all of this data they
represent it as a Topanga unveil a
degree in graph and what you want to do
is that they have super is in the form
of a field so what they have is they I
will give you a feed and I will want you
to tell me if this breed is there
anywhere in this whole graph and you
want to slice that path so you again
play I'll take that treat or chopped it
up into gamers in the new query for each
game that is line and quarry and that
case will take different applications
for this so summer like to talk and you
know I was wondering what kind of
invariance you had thought about as
interesting for you know like this graph
is a really great property that median
degree is equal to the out degree or
something right so you can you can
always kind of exploit that if I think
about like Twitter you know the in
followers are not always equal to be a
lot lower and you know like I'm
wondering you know if you could use like
distributional invariants like most
users aren't popular you know so if you
get a thousand of just coming into Brian
you know something's totally wrong but
Justin Bieber you know maybe it's okay
like you know on most users don't have
many edges stuff like that you know like
actually take advantage of the power law
distribution yes something like so even
in this world when we started off we
didn't really realize this country
invaded initially so what people do is
that we using some those kind of
techniques because when you have this
charge so like when you
so all of these like to each other
because they know and you want to find
out this is so and now you do need some
specific information other things going
to look like and the similar things for
like Amazon have sex when like if you
have all the products and just between
them if they were in a car together then
if we have a laptop and you have see a
screen guard for the laptop and the
number of times these to protract
products but in the contract
looks very high compared to only laptop
and visible backs and if so this is your
actual graph and you can have this false
edge in graphs because of the underlying
data structure and so if you find out
the date here and you find out the
weight here and you can see that all the
neighboring edges of this particular I
have a page which is very high but only
this edge has a weight which is like
different so if you calculate the
weights of all the neighboring edges and
you can get out there or something like
that and you think you can have any like
results about like you know if your edge
distribution looks like this then you
definitely find the bad ones like a like
a I'm gonna be I've been interested to
read that paper okay so so please write
it those wraps now so any other
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>