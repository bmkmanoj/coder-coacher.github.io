<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Inferring Road Maps from GPS Traces | Coder Coacher - Coaching Coders</title><meta content="Inferring Road Maps from GPS Traces - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Inferring Road Maps from GPS Traces</b></h2><h5 class="post__date">2013-03-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/byJzKjCta3c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you everybody for coming in
particular thank you to James forgot
Yanni our guest speaker he's a PhD
researcher at the University of Illinois
at Chicago his research interests
actually mirror the research and hobby
interests of a lot of us he's interested
in sensor data and transportation
networks dear to all of our hearts what
kind of sets James a party though is his
ability to speak and write about what he
does presentations went away and what's
more he impressed Brian at six facial
this year so much but Brian said we
simply had to have her for a talk so
what you say is the culmination of that
if anybody in any of the overflow rooms
has any problems let me know by a check
in the meantime though if you are not
presenting you commute and then we'll
unmute at the end for questions without
further ado I turn it over to James all
right great thanks Andrew okay can
everyone see that okay to start with yes
I mean I'm going to assume yes so thank
you Andrew of course for your kind
introduction and for all of your help
arranging this talk I very much
appreciate the opportunity to speak with
you all today so is Andrew said my name
is James be a Joni I'm currently a PhD
student at the University of Illinois at
Chicago and today I'm going to present
my work on the problem of inferring road
maps from GPS traces work which has been
co-authored by my advisor at UIC
professor Jacob Erickson okay so today
we're going to talk about making maps
and fortunately for us time to change it
a little bit in particular thanks to the
ubiquity of GPS sensors in people's cars
and in people's smartphones which are
also often in cars we now have an
unprecedented opportunity to collect
large sets of GPS traces like these
which we can feed into algorithms that
automatically generate or infirm apps
like this they represent all of the
roads that were travel now while this
may appear straightforward right feet in
some GPS data out pops a map we'll see
shortly that there are a variety of
problems that make this process a
challenge but before we get into how to
do this let's take a quick look at
why we might want to infer maps in the
first place because after all companies
like navteq seem to do a pretty good job
accurately surveying the road with these
SUVs everywhere they go but here's the
problem right here these SUVs don't go
everywhere which means there are parts
of the world where there aren't any road
maps and even where there are road maps
there quickly out of date because of new
road construction and road closures
which are sometimes far from temporary
and so we would argue that since of all
all of this GPS data is being collected
anyways we should turn around and put it
to good use creating maps that reflect
all of the newest construction and
closures but crucially in order to be
able to do that we need an algorithm
it's able to handle the inherent
problems that come with this type of
real-world data so let's see what our
options are now the current state of the
art map inference techniques can be
broken into three different categories
and are well characterized by these
canonical algorithms we have edel camp
and shuttles method based on k-means
clustering cowan crumbs method based on
trace merging and Davies at Alice method
based on kernel density estimation let's
take a look at how they work first up is
edel camp and shuttles method based on
k-means clustering this algorithm starts
out the cetera GPS traces you see
depicted here it then runs along the
length of each trace one at a time
dropping a series of clusters seeds at
fixed distance is d along the way now
it's important to note that the cluster
seed is only dropped when there's no
existing seed nearby within this fixed
distance that way we end with a sparse
representation of the underlying traces
based on the collection of seeds as you
can see here ok so now that we have our
K clusters their position and heading
are adjusted based on the location and
heading of nearby GPS traces and once
their locations are settled they're
linked with edges forming road segments
based on the pattern of raw traces
passing between them and it's these
segments that represent the map inferred
using this technique ok next up is kalin
crumbs method based on trace merging
this algorithm starts by first
pre-processing the rod GPS data pulling
together those traces that are nearby
and oriented in the same direction to
form tight bands as you see here the
method then iterates through each
recorded GPS trace and adds its edges to
the map as
long as no existing map edge is close by
in distance and heading now when we
encounter such an edge rather than add
an additional edge to the road map we
simply increment the existing edges wait
examples of which we can see here
illustrated by the three edges on the
bottom of the figure and the little plus
ones now once all the GPS traces have
been processed in this way edges would
wait below a certain threshold they're
considered spurious under pruned we can
see in this figure that edges were the
weight less than two are proved and are
marked with the red x's and it's the
edges that remain here that form the
road map that's going to be inferred
using this technique lastly we have
Davies at al's method based on kernel
density estimation this algorithm starts
by first splitting the region covered by
the traces into a fine grid and then it
counts the number of trace edges that
fall on each of these grid cells this
produces a two-dimensional histogram
we're here a darker color represents a
higher value two dimensional Gaussian
smoothing is then applied to this image
which produces an approximate density
estimate of the underlying trajectory
data the density SM is then passed
through a threshold function producing a
binary image of the underlying road
outlines and finally the center line
between the road outlined is extracted
using a method based on voronoi
tessellation and the resulting center
lines represent the inferred mat now the
standard technique for evaluating these
algorithms is purely qualitative and in
most cases this amounts to simply
eyeballing whether the inferred map
lines up well with google maps will give
that method to try and see what we can
learn about these algorithms this is the
raw data we're going to use for our
evaluation then it consists of 118 hours
of GPS trace data from our campus
shuttle fleet of 13 buses let's start
out by looking at the section of data
outlined here in red in the corner there
thanks to a lack of high-rise buildings
in this area we can see that there's
very little GPS noise now this
represents a fairly easy case for these
algorithms so let's just take a quick
look at how they do here we can see that
the k-means algorithm makes a big error
in the top right corner of this figure
where one spurious GPS trace leads it
astray but for the remainder of the
region it does a good job accurately
separating lanes of traffic and you know
they fit very closely with reality as
you can see from the picture try
merging also does an excellent job of
generating a map that fits closely with
reality and we can see here that because
of the proving step the error that was
present with the k-means algorithm is
now absent so that's good lastly we can
see that kernel density estimation or
KDE has all referred to it from here on
for brevity's sake also does a good job
of creating an accurate looking road map
however the observant among you might
notice that the eastward section of
Harrison Street outlined here is lost
now because we have to select one
threshold value for the entire map with
this technique those areas that are
traveled infrequently and have low
density are often inadvertently removed
but for the most part we can see these
an algorithms work reasonably well in
areas with low GPS noise but of course
not all GPS data is this well behaved as
we know let's take a look at a much more
difficult part of the map outlined here
in red now because of the abundance of
high-rise buildings in this area we have
a significant GPS noise in particular
there are a large number of traces going
straight through the building's examples
of which you can see everywhere in this
figure but especially here outlined in
red or I promise you there really aren't
any roads coming across these buildings
now this is what we're going to learn
something about the robustness of these
algorithms and we're going to find that
both the k-means and trace merging
algorithm have a hard time coping with
this hiya noise data in the k-means
algorithm for every trace there has to
be a road explaining it so we end up
with this because some of the drives
through buildings can be explained by
actual roads as you'd expect it just
creates new ones as you see here
similarly due to the high error in the
first step of the trace merging
algorithm where it performs that banding
step it's unable to create one coherent
band of traces because they're just
spread so far apart and this results in
new roads being drawn across buildings
as well KDE on the other hand produces a
highly accurate looking map again now
because this algorithm treats GPS traces
in aggregate it's able to create the
single road center line from a large
collection of diverse traces as you saw
however I have to point out the obvious
here right the roads it produces are
pretty jagged as you can see in this
figure and this is a result of the
central line extraction method that they
use also those of you who are paying
attention will again notice that the in
frequently traveled roads here and here
are missing from this map
and this is a key weakness of this
algorithm as it requires a big trade-off
between coverage and accuracy moreover
it has to be pointed out that KD isn't
able to separate roads into distinct
paths of travel nor is it able to
capture turn restrictions done all of
this begs the obvious question which one
of these methods is best I mean they all
seem to have good qualities in bed and
it's hard from these images to draw any
real obvious conclusions so to answer
this question we need a way to
objectively compare their performance
using numbers the method we propose for
this task evaluates the accuracy of an
inferred map with respect to a ground
truth map now while existing
quantitative schemes do exist in the
literature they largely concern
themselves with the spatial
correspondence or geometric similarity
between the inferred and ground truth
map and we argue that this is only half
the story and for an inferred map to be
considered an accurate representation of
reality it's also going to have to
correctly represent the interconnections
that exists between roads and accounting
for both of these characteristics is
important because what we want is a
navigable map that produces accurate
routes both in terms of distance and
directions so for this task we propose
the following method starting with the
ground truth map as you see in the
picture here we select a street location
at random and market with an X as you
can see in the figure from this point we
follow the road dropping virtual holes
as we're going to call them at fixed
intervals until a maximum radius from
the start location is reached and when
an intersection is encountered along
these pads we follow all connecting
roads lead away from the start location
we're then going to repeat this process
starting from the closest point on the
generated map were marked here again
with an X except here we're going to
drop what we called virtual marbles we
then overlay these holes and marbles and
intuitively if a marble ends close to a
whole it falls in and if it lands too
far away it just remains where it landed
now looking at how they fall is going to
allow us to measure geometric and
topological similarity specifically
holes that are filled correspond to
matched locations and are marked here in
green unmatch marbles correspond to
spurious parts of the generated map and
they're marked here and read with the
little plus signs lastly holes that
remain empty are going to correspond to
parts of the ground truth map that are
missing in our
third map and those are marked here in
red with the little minus signs when i'm
going to quantify the accuracy of the
generated map based on the proportion of
spurious marbles and the proportion of
missing locations to be specific we
compute the proportion of spurious
marbles as the number of spurious models
over the total number of marbles
produced and similarly we compute the
proportion of missing locations as a
number of empty holes over the total
number of holes produced to produce a
combined performance measure we're going
to use the F score and in our case we
define precision is 1 minus spurious
recall is 1 minus missing this is going
to allow us to write our F score like
this and now that we have a quantitative
measure of accuracy we can easily and
objectively compare the performance of
these algorithms this graph represents
the performance of all three algorithms
with F score on the y-axis a matching
threshold on the x-axis matching
threshold here refers to the allowed
distance between a hole in a marble the
fort will fall in and we can see that
overall the KDE algorithm performs best
followed by trace merging and inkay
means this is a result that largely
confirms our suspicions of course one
other performance aspect to consider is
how long these algorithms take to run
however here again KDE takes the top
spot coming in at 25 seconds followed by
k-means at 15 minutes and trace merging
it a whopping two and a half days so
does this mean our work here is done
right since katie is the most accurate
and fast is running algorithm do we just
go go go with it and move on
unfortunately there's a little more to
it than that so although the Katy
algorithm goes a long way towards
dealing with the problem of GPS noise it
falls flat on its face when it comes to
handling disparity in the number of
traces failing to infer these lightly
traveled roads as we saw previously so I
think we can probably all agree this
looks fairly bleak as all of these
algorithms are thwarted in one way or
another by these noisy disparate traces
so the question then is what do we do if
we want to infer maps from real-world
data where these problems of noise and
disparity are in abundance well we start
out by taking stock of what we've
learned so far first
the KD method looks promising it's
largely noise resistant it's able to
extract more or less accurate Road
center lines and it's very very fast
second despite these good qualities the
KB method isn't the Silver Bullet it
requires us to choose a single threshold
trading off coverage for accuracy and
it's only able to extract a single level
a single center line from bi-directional
roads lastly it's unable to capture term
restrictions which is a bad thing sir
there's immense value in trajectory data
as we saw with the k-means and trace
merging algorithms it can be used to
separate roads into distinct paths of
travel and to ensure the maps were
producing or topologically accurate and
so it's with these lessons in hand we
can take a step forward and we're going
to fuse together the best aspects of
these existing algorithms with some new
ideas of our own to produce a hybrid map
inference method more specifically we
propose the following approach we start
out by taking the raw GPS traces and use
them to produce a kernel density
estimate we then take this density
estimate and produce an initial estimate
of the underlying map now at this stage
the map we've produced this initial
estimate is lacking in any sort of
trajectory information so to add that
back in we're going to take our raw
traces and Matt match them on to our
initial map estimate then using the map
match data we're going to refine the
maps topology and then it's geometry to
produce a final clean map representation
okay so now you have an idea of how all
the pieces fit together let's take a
look at them one at a time first up is
density estimation this part of the
process is the same as we saw before
with the standard KDE algorithm we split
up the region into cells in this case 1
meter squares and count the number of
traces that fall on each cell this gives
us a two-dimensional histogram which we
then smooth with the two dimensional
Gaussian producing this density estimate
of the underlying trajectory data done
now with our density SM in hand we're
going to generate an initial estimate of
the underlying map to do this we first
pass the density estimate through a
threshold function producing a binary
image of the road outlines as we saw
before however next rather than using
voronoi tessellation we're going to use
a binary skeletonization' to find the
road center lines a method which erodes
the boundaries of the image until only
one pixel wide characteristic scale
remains shown here in red as you can see
this method avoids producing the jagged
looking back we saw previously with the
standard KD ele so that's nice but it
doesn't fix the more glaring problem as
you've likely noticed by now the choice
of threshold value has a huge effect on
the resulting map specifically choosing
a high threshold as we've done here
helps to discard noise from density
estimate but also results in
infrequently traveled roads being
removed and since we're trying to create
a map that represents all of the roads
that were driven this is clearly
unacceptable so Gail the only
alternative is to choose a lower
threshold value and it's going to allow
us to capture the extent of all the
roads that were travel okay so let's
apply a skeletonization' to this image
and we can be legally see this isn't
going to work either as the amount of
noise we've had to admit in order to
recover those roads as resulted in a map
that bears very little resemblance to
reality so clearly having to choose a
single threshold value isn't going to
cut it what we need is a way to
skeletonize our density estimate
directly that's going to preserve the
high density high accuracy regions but
also include the low density potentially
noisy regions to accomplish this goal we
propose a new grayscale skeletonization'
algorithm that operates on the density
estimate directly in the paper we
published on this technique we describe
its operation algorithmically but since
we have this lovely visual medium here
and it's they're going to just
illustrate how it works and refer you to
the paper for the gory details okay this
process is iterative with three major
steps in every iteration first we start
out by fresh holding our density
estimate the highest possible threshold
level so we just get this little piece
second we perform skeletonization' which
is going to produce a highly accurate
road center line as you see here and
third we're going to what we call walk
in the result now in the next iteration
we're going to threshold their density
estimate that the next lowest threshold
level and repeat these three steps but
we also want to carry over the result
from this level to the next so in order
to preserve this result at this stage we
walk in or mark the pixels in a way
that's going to prevent them from being
removed okay so let's keep going with
this illustration next we threshold
events the estimate the next lowest
threshold level care
over the center line pixels from the
previous iteration you can see them
there and read then we skeleton is
producing more highly accurate road
center lines including those from the
previous iteration and finally we lock
in the combined result okay let's keep
going next we go down one more threshold
level we skeletonized lock in the zone
then we go down one more threshold level
skeletonize unlock in the result so
hopefully now you get the idea
essentially we're going to work our way
down from the highest to the lowest
threshold one step at a time walking in
and carrying over the result from one
iteration to the next also important to
note here is that we're actually
remembering which threshold level these
center lines were added to our evolving
map with lower threshold levels being
represented by lighter shades of red in
this figure okay let's go through a few
more iterations to make sure this is all
crystal clear so down one more threshold
level skeletonized lock and result down
one level skeletonize and walk in the
result all the way until we reached the
lowest level we skeletonize lock in the
result producing our initial generated
map as you see here ok now this stage
our initial map is lacking any sort of
trajectory information as I mentioned
earlier so in order to add that in we're
going to take our raw GPS traces and
Matt match them on to our initial map
using a hidden Markov model based map
matcher however before we do that we
make one slight modification to the
standard hmm approach that is rather
than have a uniform transition
probabilities between roads as we find
in hiya Gary Jones V track paper we
first compute the mean threshold level
of each road and then set the transition
probabilities according to the relative
density of each outgoing road well this
is going to do is it's going to
encourage our map matcher to use more
popular roads when it's possible and
reduce the number of traces that are
going to end up getting matched to
spurious roads okay so now that we've
assigned every GPS point to a road in
our initial map we're going to use that
information now that it's attached to
refine its topology but before we do
that we want to make sure we're only
using traces that are fitting the road
segments they're assigned to reasonably
well so to determine their goodness of
fit we measure the Euclidean distance
between every point p on the trace and
its nearest
point on the road segment e to compute
its overall root mean squared deviation
now if the traces rmsd is less than some
maximum threshold call it rmsd max we
keep it otherwise we throw it away this
process is then repeated for every trace
in our data set resulting in the
collection of well fit traces for every
road segment in our initial map we're
then going to use this knowledge to
prune away spurious Road segments from
our initial map specifically we're going
to remove any road segment that has less
than two well matched traces now our
argument here is since all it takes is a
single noisy GPS trace to create a
spurious road we're going to require at
least one piece of additional
corroborating evidence to support
keeping it and so this is the result of
our pruning process and as you can see
it goes a long way towards refining the
mats apology but we can also see that it
leaves a few little artifacts behind in
particular we have some orphaned Road
segments as well as some small road
segment spurs that we'd like to get rid
of in order to have a clean and polished
Mac so to clean this up we once again
Matt match our full set of GPS traces to
this map and one more round of pruning
results in the well-groomed map you see
here ok but we're not done yet is
there's one one more small issue that
still needs our attention in terms of
topology specifically here where you can
see that we have two three way
intersections where you know based on
the diagram behind there should be just
one four-way intersection and now this
is an artifact of our initial density
estimate that we need to come back and
fix so in order to fix this problem what
we do is we find all pairs of adjacent
intersections that are within some
distance threshold and then consider its
collapsing them into one like that then
to decide whether the change that we've
made is valid and whether to keep it we
look at the collection of GPS traces
that were matched to all of these roads
segments previously and if this
refinement we've made doesn't reduce the
total number of well matched races
across all these segments meaning
essentially the traces to fit the new
topology just as well as they used to
it's made permanent otherwise we revert
the change and move on to the next
intersection pair ok so things are
looking pretty good but as you've seen
currently our map has a very simplistic
geometry in particular two-way median
separated roads are positioned on the
median
and complex intersections are
represented by simple four-way junctions
so in order to make our topologically
accurate road map more geometrically
accurate we use a method based on
k-means clustering specifically we start
out by placing a series of cluster seeds
it fixed distances along each Road
segments like this and for what it's
worth there are another set of seeds
behind me to account for both direction
to travel what you'll see in just a
second okay then what we do is we adjust
the seed locations based on the position
of the GPS traces that are assigned to
these segments and this is going to
allow us to pop out the individual lanes
and give us a reasonable approximation
of their position but we're not done yet
as you can see we still have a single
intersection point in the middle giving
us this odd-looking hourglass shape and
we'd like to fix that so to do that we
applied the same technique again only
this time we're going to fit and adjust
clusters for every pair of Road segments
the transition from one to the other so
for this road segment on the right-hand
side we're going to fit clusters for the
right and left-hand turns as well as for
the road straight ahead now keep in mind
that we're actually doing this for all
transitioning Road segments at the same
time I've only shown one example here
just to keep things simple okay so now
what we do as before as we adjust the
seed locations based on the position of
the traces that are assigned to these
transitions and this allows us to pop
out all of the appropriate turn lanes as
well as create geometrically actree
create a geometrically accurate four-way
intersection okay so there you have it
now you've seen the entirety of our
pipeline and how we manage to take noisy
dispar GPS traces and turn them into a
clean and refined road map so the
question of course is how good is our
map and how does it compare to those
from the current state of the art that
we looked at previously to answer those
questions I'll evaluate and compare our
map in two ways first quantitatively and
then qualitatively here we have the
overall performance of our algorithm
compared to the current state-of-the-art
methods using the metric I described
earlier with F score on the y-axis
matching threshold on the X we can see
that our method outperforms the existing
algorithms over the entire range of
threshold values or perhaps more
importantly for thresholds 15 meters and
above we've actually managed to have the
distance to perfection
so this is good clearly we're winning
but for all of our efforts why aren't we
doing even better well if we pull apart
our F score into its component pieces
with precision above and recall below we
can see that our precision is excellent
right almost a hundred percent in some
cases but our recall doesn't fare so
well hovering around seventy percent now
the reason for this comes back to our
topological refinement step where we
prune away roads with less than two
matches because there are several roads
in our data set that were traversed once
and we throw them away we end up being
punished here so we'll hang on to that
one for future work okay numbers are
great but let's see how they translate
into pictures for our qualitative
analysis I'll draw our inferred map on
top of a Google map as I did earlier
with the other algorithms and see how
well it corresponds here we can see the
final map produced using our algorithm
and you'll notice that we're able to
capture almost every road that was
traveled with only a few small segments
missing shown here in red furthermore
you can see that overall our roads are
largely unperturbed by GPS noise and in
our biggest problem area where
everything else fell over we're able to
do an excellent job so there you have it
where do we go from here well for one
finding a way to properly include roads
that are driven only once should allow
us to see a substantial performance
improvement with our hybrid algorithm
more broadly speaking however there are
three big problems of lurking out there
they were actively working towards
solutions for so since I've got a bit of
time I'll spend a few minutes now and
give you some details on exactly what
these lurching problems are first is the
problem of dealing with sparsely sampled
data now with our Chicago data set we
have the luxury of working with traces
that are sampled once per second where
every pushpin here represents a GPS app
now with data like this we get a very
good idea of where the vehicles traveled
and a fairly clean representation of
where the roads are however the reality
of using real-world crowd-sourced data
sets means this is likely much more
likely the exception than the rule and
that means we have to be prepared to
handle data sets that look more like
this where in this case samples are
recorded once every 16 or 61 seconds by
taxis in Shanghai as you can see from
this diagram all of a sudden that's not
at all obvious where the roads are and
perhaps even less obvious how we could
hope to infer
a map from data like this now in some of
our earlier work prior to the
development of our hybrid model we
experimented with two approaches to this
problem using the standard KDE algorithm
I'll give you a quick overview of that
work to illustrate the implications of
this problem and as you'll see there's
still a lot of work to be done in order
to produce a truly general solution to
this problem so our first idea was to
just naively apply the standard KDE
approach off the shelf and hope that
with enough traces we'd have enough
signal to outweigh all the noise we
experimented with this idea using six
and a half million of the sparse
location samples you just saw collected
over this region of shanghai by 2300
taxes this resulted in a set of raw gps
traces that looks like this now while
this may appear to be just a black
square it actually consists of half a
million sparsely sampled taxi trips
hints of which you can see in these
corners now as we've already seen a few
times today we then just split this
region into cells and count the number
of traces that fall on each giving us a
2d histogram which we then smooth to
produce this density estimate you see
here now you can probably see the
problem with this approach although were
able to find high density regions from
which will be able to extract parts of
the map we've also admitted an
incredible amount of noise shown here is
the gray background color amidst all of
our roads this means we have to use a
very high threshold to get above the
background noise essentially resulting
in this map with lots of missing roads
highlighted here in red so we needed a
different approach the standard KDE
method makes the assumption that
vehicles travel in a straight line
between successive GPS samples and well
that's somewhat reasonable when we're
dealing with high density data it tends
to break down with data of increasing
sparsity as a common example consider
these two sparsely sample points a and B
where the actual path travelled between
them is depicted by the dashed line now
if we follow the naive assumption of the
standard KDE approach and simply draw a
line between them we end up with an
incorrect trajectory that cuts
diagonally across the corner rather than
the actual trajectory followed by the
vehicle between these points and it's
exactly this phenomenon that led to the
raw GPS traces appearing as a black
square previously so as a means to avoid
the
problem we considered a very slight
modification to the KDE method that is
when we generate our histogram rather
than incrementing the grid cells were
the assumed trajectory falls like this
we incremented only those grid cells
where the actual location samples fall
like this this is going to help us to
keep out potentially noisy trajectory
data now of course there's no such thing
as a free lunch right this approach
actually makes the data sparsity issue
much worse as unless we have enough
samples to cover a road entirely we're
not going to be able to infer it later
but let's see what happens regardless
this is a result of drawing in just the
location samples from our raw traces and
we can see that because of the abundance
of data roads appear to be well covered
by these points okay so let's count up
the number of points in each cell pretty
histogram and blur to create this
density estimate and we can see that
this contains significantly
significantly less background noise in
our previous approach so when we extract
the center lines from this we end up
with a map that looks like that because
we don't have to dredge the roads from a
sea of noise we're able to use a lower
threshold here and extract more of the
actual roads that are present this means
when it's compared to the previous
method we're able to recover a
significantly more complete map Oh too
fast significantly more complete map a
difference which is clearly illustrated
by the missing roads drawn here again in
red now while this is an interesting
finding the approach is still rather
naive as I mentioned earlier if we were
dealing with a data set in which there
was less spatial overlap and there
weren't enough samples to entirely cover
or paint the roads we wouldn't be able
to infer anything at all and since we
certainly can't rely on having massive
amounts of overlapping location samples
as we did here in general there's
clearly more work to be done to produce
a truly general solution for arbitrary
data sets okay the next big problem
lurking out there is the problem of how
to infer complex road geometry while our
current data set is a great case study
right contains clean and noisy dense and
sparse traces its lack of geometric
diversity means you're only having to
infer planar right angle intersections
and one of the first things we're
realizing now is we start to use other
data sets is that we suddenly have to
account for a variety of complex road
trees with one of Chicago's more
challenging cases hiding in plain sight
right here outlined in red known as the
circle interchange there isn't a right a
single right angle planar intersection
to be found and if we simply follow our
hybrid approach taking our traces and
creating a density estimate then
applying grayscale skeletonization' we
end up with this which perhaps on the
surface of things doesn't actually look
all that bad but if we look closely we
can see that it creates tons of
intersections between that don't
actually exist and this means that our
map matching faith is going to end up
erroneously tying together segments from
completely separate roads as we attempt
to thread a curved paths through this
maze this is also going to make our
later refinement steps completely
incorrect as they attempt to refine
intersections and geometries which
should be interconnected in the first
place so how we go about handling these
complex geometries is a challenge that's
square in our sights and once we've come
up with a good solution for this we can
then look at one other big problem
lurking out there and that's how to deal
with the big data again although our
data set is great for what it is the
relatively small number of traces
involved and they're relatively small
spatial extent doesn't test the
scalability of these algorithms and if
you consider that it currently takes our
hybrid technique half an hour to process
just one hundred and eighteen thousand
samples over this relatively small area
given our more or less linear run time
this means we've got some work to do if
we want to scale up to hundreds of
millions or billions of samples over a
city or maybe statewide area however
with that in mind it is worth mentioning
that one of the big benefits of growing
up our location data into cells and
using these pixel-based representations
is that it does make many of our
processes easily parallelizable and
conceivably well-suited for perhaps GPU
or MapReduce implementations so it's
clear there's still a lot of work to be
done here and these are all directions
that we very much look forward to
exploring in the future and with that
I'd like to mention that all of the
source code and test data for these
albums are actually publicly available
on our website if you want to play with
them thank you so much that was a
fantastic talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>