<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jure LESKOVEC - Research Scientist - Dynamics of real-world networks | Coder Coacher - Coaching Coders</title><meta content="Jure LESKOVEC - Research Scientist - Dynamics of real-world networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jure LESKOVEC - Research Scientist - Dynamics of real-world networks</b></h2><h5 class="post__date">2008-05-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5AxNTJ3Yesk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">for a graduate student two best papers
award ktd 2005-2007 he's also won the
ACC AC MKV Cup in 2013 these are pretty
competitive events also top the Battle
of the sensor networks 2007 competition
I'm not sure what that one is
I'll pass it it's interested in granting
thanks a lot thanks for coming
so what I'll talk about today is
basically something that I call the
dynamics of networks basically two
things how the process is spread over
the networks and what can you do with
that and how do networks evolve and how
can you model that networks as graphs
okay good okay so if you think what do
we have today right we have like large
online systems that have different
traces of social human social activity
what I mean by that is for example we
have online communities like Facebook
and MySpace that have millions of users
with billions of actions every day
similarly for communication we have like
instant messaging that all over has like
a billion of users and people use this
all over the world to to community okay
we have news and social media which
again like things are blogging and
things like that where we have the
estimate is 250 million blogs worldwide
presidential candidates run blogs and so
on the other thing that's also coming up
is like online worlds like Second Life
World of Warcraft that that are like
this huge internal economies so GDP of
Second Life is seven hundred million
dollars whatever that means right but
basically what the point is that we have
these huge data sets or more large
online applications where we have
detailed traces of social human activity
and why is that important because we can
now connect human activity and computer
science or we are interested in these as
computer scientists because these are
computing applications and we have
humans so the influence sort of goes
between social sciences and in computer
science and this
as great opportunities for like impact
Baofeng like science and industry what
I'll talk about today are networks and
what I mean by that are basically graphs
that are that are given to us as data so
for example world wide web is naturally
a graph or a network where webpages are
nodes and we have hyperlinks as edges
between them internet autonomous systems
again we have nodes and edges social
networks are naturally graphs right
they're nodes are people and some kind
of end connections means some kind of
interaction we have communication
networks where an edge could mean that
people two pair of people exchange an
email or exchanged a message over I am
and so on citation networks like of
scientific papers patterns are again
networks that every node is now a paper
and there are references between them
there are also like biological networks
like protein interaction networks or
gene networks so this is the date the
type of data that I'll be looking at and
if we go and ask how are this how is how
does this data look at we know a lot
about their structure so for example we
know about the properties of their
structure so for example we know things
like scale-free we know clustering we
know how to navigate in them we know
that they have bipartite course
communities things like that right so
this is what we know about this the
properties or the structure of them and
then we have a bunch of models that
connect our empirical observations with
sort of theoretical instead intuitions
and just to give a few right you know we
have models like preferential attachment
copying model small world things like
that right what I what I will argue here
is that we know much less about the
processes and dynamics of this networks
right while we understand the structure
of networks aesthetic they're inherently
dynamic right and we know much less
about it so basically my talk what I
what I'll show you are two aspects of
dynamics of these networks as I said in
the beginning one will be about network
evolution where the question is how does
the network structure change and evolve
over time this is the first question as
the network grows right and the next
second question is about diffusion and
cascading behavior networks where where
we are basically asking how the rumors
and diseases spread over this
these networks okay and one common theme
in my research is that I want to look at
massive datasets for the reason that
once once you take enough data better
and start to image try to get this
emerging behavior type of things and a
few datasets that I worked on is the
largest one is the MSN Messenger network
where is which is basically the largest
social network ever analyzed it's like
all the messenger users that's 240
million two hundred fifty five billion
exchange messages a month and it's
terabytes of data and I think for like a
scale of things you are looking at
people at Google are working with four
point five terabytes is maybe not so
impressive but elsewhere it is then I'd
also show something about product
recommendations were basically what we
had is four million people with 60
million explicit product recommendations
that you can see whether that particular
recommendation resulted in a purchase or
not and how do these product
recommendations now spread over networks
and I'll show you also something under
some results on the blogosphere how does
how does influence in information spread
on the blogosphere so basically the plan
for the rest of the talk is the
following I want to show you two lines
of work one on Cascades and diffusion
the other one on network evolution and
then each part if each of these these
two lines that I would like to get
through will have two parts first will
be sort of empirical part where we take
some data analyze it see what kind of
patterns and properties the data has
answer answered the questions and then
we use these intuitions or these
measurements to design models better
algorithms so basically for the rest of
the talk I want to fill in the give
answers to these four questions or these
four boxes that I have here okay so
first I want to talk about diffusion and
Cascades and basically what will ask is
how do Cascades
that I'll define on the next slide look
like and then how can we then use this
to find influential nodes or how to how
do how do we quickly detect epidemics in
networks okay
so just intuitively watch what we should
think of when I talk about diffusion and
Cascades you should think of behaviors
that cascade from node to node like an
epidemic so you can think of like how
news opinions rumors tactics
propagate from person
person or you can think of word-of-mouth
viral like viral marketing when you get
the recommendation about the product you
go buy it you are all excited to tell
your friends and this way now the
recommendation spread over the network
diseases right are another common
example where like flu right right the
only way to spread through is to come to
come to someone in sort of covered up
cough at the top right and then they get
sick and so on right so but what happens
here is that we have these these
activations spread through the network
and they leave a trace and this stress
is called a cascade so what I mean by
that is if I have a network and let's
say the disease starts here and then
propagates right in this particular way
from neighbor to neighbor then then this
graph the Cascade induces I call a
cascade like a propagation graph right
and the question will be asking is how
do these things discuss guides look in
your life okay just what shapes do they
have yes about the meaning of the of the
arcs in the Cascade so when you have a
cross heart saying contagion contagion
that would you have the cross arc even
though the this destination is already
infected or not so if I if you are
representing a cascade for infection
uh-huh so would I would you have those
cross art so who do you have a tree it's
not necessarily a tree because let's say
the diesel so Nick what I want to say
next is like introduced to to real-world
domains where I was able to observe
these cascades going on and they are not
necessarily only trees right they will
be DAGs you will have no cycles but you
can have a case like this well you get
one recommendation if these are product
recommendations right if that's yourself
you get one product recommendation you
said you don't care you get the second
one you go by so I understand the
question is what is the meaning of an
arc in this case is it that there was a
contact or that some state change
happening in the destination er um there
was a contact yeah yeah that has to be
explicit contract okay okay so it's not
an induced sub graph on this particular
set of notes if there's a question
because for example here I don't have
this heart right okay okay so I have a
social-networking that over there
some processes propagating and some uses
some edges and he doesn't do it some
other edges okay so there are two as I
said - - like natural things where we
had data for this to happen first one is
product recommendations right the idea
here is that there is a person so the
domain is like a big online retailer
right the idea is there is a person
comes to their website buys a book at
the time when they are they are checking
out they get asked they want to
recommend this book to any of your
friends so if you say yes you type in
email addresses of these people right
these people get your recommendations if
they go decide to buy through through
this particular website they get 10%
discount and the person who made the
successful recommendation gets 10%
credit okay and now you can see how how
then this person again as they as they
bought they get a chance to further
recommend the particular product okay
and this way we see how reckon the
recommendations spread or cascade
through the network and the data we had
was from an anonymous large online
retailer there was data on four million
people 16 million recommendations and
half a million products right so DVDs
books musics and video videos
16 million recommendations 4 million
people ok that's the first dataset the
second dataset is is more piece like
public it's basically blogs right so the
idea is that there are bloggers that
post that make posts and link to other
posts on the blogosphere so if you go
crawl the blog's you can trace this
linking patterns and since everything is
time-stamped you can now say you can now
trace how the information propagated
right so the idea is if I have the world
of what 7 blocks here when there if
there's a post right that lets say that
then Slashdot decides to blog about this
post so they so first first this post
happened and then Slayers wrote another
post pointing to it and now you can see
how the information spreads right so the
idea would be saying that the
information started here and then
cascaded in this particular pattern
right and then somebody else writes a
post and again these propagates in a
different way ok and now since
everything is time-stamped we can we can
trace this propagation this linking
patterns
the blog that they're exactly so what
what I did is I only considered
hyperlinks so I don't use any textual
similarity but just if you know there
was a if somebody wrote a post yesterday
today somebody blogs about it by linking
to that then I say yeah this guy got
influenced by that piece of information
okay and this was the data set so here
mu is like top fifty thousand blogs over
one year did not so many posts and a few
million tens of millions of things um so
the first question we asked is how do
these cascades look like right there are
they like more like please that was us
before chains things like that and then
that's one way to look at them so what
I'm showing you here is information
cascades so basically from the blog
blogosphere domain I just interviews I
have Cascades ordered by frequencies all
right this would be the most frequent
cascade and they go this way and they go
much farther there right to the right so
these are blogosphere cascades and these
are viral marketing cascades right so
the semantics for the information
cascades is that there is a there is a
post and then somebody else where I'd
suppose in points points to eat okay
while for viral marketing cascades so
here in fluid influence goes top-down
right
this happened the the post happened
first and then somebody else cited it
for my real marketing the arrows point
the other way because this says a person
purchased I know something they made the
recommendations somebody else purchased
they made the recommendation and that
friends also purchased right and again
I'm just sorting them by the decreasing
frequency and there are a few
observations that you can make just from
looking at the shapes basically is that
viral marketing cascades tend to be more
social in a sense that there are there
are collision type things right where
people receive recommendations from
multiple people independently and you
also see this more Richard non-tree like
structures that on the blogosphere it's
it's it's are mostly like just three
structures so this would be if you go
look at this like the technology
propagates in this particular way where
for example slash says something and a
lot of geeks jump out link to it but
nothing else happens afterwards right
yeah the reason I want I was asking
semantics of length is in the in the way
described viral marketing so you require
that the nodes will purchase yeah but
you know very often the you I get a
recommendation say oh you recommend in a
so-and-so recommended the 210 one my
friend oh it's also recommended is so
I'm sure it's good and they will go and
buy it and I will be distracted or
something and I won't buy it
so it sales so there's sort of a
question of what the semantics of the
links and nodes semantic means that I
buy a product you get an email the email
says you bought this product and is
recommending it to you there is a link
in your email you go click on that link
and you go you buy okay okay and then
then the sort of the interpretation is
that I influenced you so here it's right
it's this guy sort of pushing influence
right here it's like pulling influence
in some sense right it's like the
initiative is on the guy below to create
a link so so it's naturally a sort of a
different way so the influence still the
way I plot it propagates top-down but in
one case you know I as a blogger so
meals here has to link to slashdot while
here I'm sort of on top and making
recommendations and pushing it down yeah
exactly
okay okay so this was the first thing
sort of something more empirical but now
I get I want to get a bit more technical
about how do you now detect disc escape
okay so here are what we should be
thinking about is basically a few
different applications where the setting
that I want to investigate next make
sense so for example on blogs what we
have our information epidemics right
there's a piece of news that propagates
through the network so the question you
can ask that is who are the most
influential
bloggers on the blogosphere for viral
marketing this the the question makes
sense in that you can ask is who are the
trendsetters who are the most
influential people right who creates the
biggest skates or for disease-spreading
you could ask where should I place my
monitoring stations to detect these
epidemics as quickly as possible okay so
these are all questions that we are able
to answer to to show you what I'm trying
to do the idea is the following if you
have a network and let's say that a
particular node gets gets infected so
let's think about a disease propagation
right and then these disease propagates
through the network right and there if
some other node would get infected the
disease would propagate in this
particular way right so the question is
where do I place which regular place a
sensor or a monitoring station so
wherever there is an epidemic spreading
it hits my node as quickly as possible
in the Rays ala an alarm okay that's
that's intuitively there are two parts
to this problem right first part is is
the cost right what is the cost of
monitoring the node so if I have a
network every node costs some some
number some amount of dollars okay and
then there is also a notion of reward
what's the objective function right and
for the rest of the talk I'll be working
with this objective function that says
let's minimize the number of affected
nodes so what I mean by that is if I
have a cascade right something that
propagates in this particular way and I
decide to monitor this green set of
these two nodes Hey right then the
reward I get is the number of people I
save from infection right so if this
would be a disease that propagates in
this particular way if I money if I put
monitoring stations here then I raise an
alarm as soon as one of these two nodes
gets hit and these are the people that I
prevent from being infected right and
the more people I save the higher my
reward
the better I am I can also work with
other objective functions that make
sense in other other them in other
domains so for example one one way would
be to say I want to minimize the time to
detection so that's a separate objective
function that would try to minimize the
time between when the disease what it
was introduced and the time when we
detect okay that's one or the next one
the other one that's again separate is
we want to maximize the number of
detected outbreaks where the idea would
be just I want to I
want to detect that there was a disease
outbreak but I don't care how many
people died how many people got infected
and so on
okay so these are three different
objective functions that we can work
with okay to introduce the problem here
here is what I want to do I'm given a
graph and I'm given a budget you can
think of budgets just in the number of
dollars I have and I have data on how
Cascades spread over time so I have
basically for any possible disease
outbreak I have it how it how it
propagates through the network and what
I want to do is I want to select a set
of loads maximizing this reward right
I'm saying
I want to select a subset of loads so
that my reward for detecting a
particular cause sum over the rewards of
detecting particular scale is maximized
and here I just have the probability but
you can forget about that right and of
course I have to obey that cost
constraint sorry probability of AK
escape I'm going I'm summing over all
cascade probability over all Cascades
probability of Cascades times the reward
for detecting that escape right and the
reward can be like 0 1 if I just want to
detect the reward can be how quickly I
detect the reward can be how many people
I save because I detected it quickly
okay so for I'm for any for I'm summing
over all Cascades
that's just probability of the Cascade
occurring and that's the reward that for
me by selecting a set of nodes a for
detecting that cascade ok yeah I'll show
an example I just want to know that
doing this exactly is np-hard ok and
what I want to show you now is basically
what is the problem structure that I
will exploit to get a algorithm with
approximation guarantee then I'll show
you the algorithm and then I'll show you
how to speed it up and if you know about
this KKT so compact Kleinberg entire -
paper from all three basically we
generalize that because we also consider
cost ok so here's what I what I'm
basically doing or what is the problem
structure that I'm exploiting so if you
consider a network and you consider that
you placed let's say two sensors or you
are monitoring two nodes and each of
these two nodes cover some area right
now you can ask what happens if I want
to add another node to the whole
right and if you say okay I consider
adding this monitoring this particular
note this is the area that is know this
by monitoring this note additional area
I will cover right and the idea is that
here monitoring this note helps helped a
lot now you can consider the same thing
but just you are now monitoring for
notes right and you can ask how much how
much benefit do I get now for placing
this for monitoring the same note right
and what you can see now that the
additional area we cover is smaller
okay so adding s here helps little right
so basically the idea is that again of
heading a note to a small set is larger
than the gain of heading a node to a
larger set right the additional area we
covered here was big the initial area we
covered here was small right and you can
eyes to formalize this by something
called sub modularity I'll define it on
the next slide this is also caused by
diminishing returns and you can think of
it some at some kind of combinatorial
analogue of concavity okay so here is
what I mean by sub modularity right I
say if I have some reward for a small on
a small set of nodes I consider adding a
particular load to it then the gain the
marginal gain I'm getting will be higher
than adding the same node to the larger
set right a is a subset of V okay and
that is a very natural example of this
that you can consider so consider that
we have some sets these circles and the
reward I'm optimizing is the size of the
union of these sets right and now I
basically the size of the covered area
so now I can say if I if I want to add
this particular set u to the whole thing
what's the benefit I'm getting how much
new area do I cover I cover this much
new area right so we are covering lots
of new area but now if you consider a
larger set of set a super set right and
consider adding the same the same set u
to the whole thing now the benefit is
smaller okay and basically this is a
natural example of a submodular function
right it's easy to see that this will
only get smaller as as the beat as
bigots logic and the other thing that we
know about sahaja traction is that we
can combine them okay so what we can
prove about our three objective
functions is that each one of them is a
modular
and for the one that I'm focusing on I
can quickly show you what that is - so
if I have a cascade and I I define this
R sub I of U of K is a set of notes that
I save by monitoring particular note so
the idea is if I monitor this this is
the set of nodes I save if I monitor
some other node let's that's the set of
nodes I save so what's my reward for for
detecting a particular cascade is
exactly the size of the union of this of
this set okay so that's natural it's a
modular just by the example I gave in
previous slide and what's my global
optimization problem is just the
combination of of this submodular
function so the whole thing is submerged
okay and I'll show you why why why this
is important yeah you become infected
and you die and you no longer can infect
anywhere else or you get infected you
can inoculate it then you may or may not
get infected again so or all those cases
gonna be haha so what that's that's a
good point what I'm considering right
now is you get so you you get infected
and sort of disease propagates so under
so what I'm us what I'm so assuming
right now is that you that you never get
well again in some sense or if you get
well and get infected again that's a
separate cascade right nobody you you
can never get that actually if they burn
themselves out very quickly taken they
just you know if I touch him and he dies
before you touch her then I can't spread
exactly most of these infections don't
really spread right on blogosphere most
of the you write a blog and a post and
nobody cares about it or I'll show a
water distribution network type of
example there also most of the
infections don't really spread right if
you have a reservoir here and the water
always flows that way so if you put a
poison in this at this particular
location it will spread very very very
little right if you put it here it will
go over the whole network so there's
lots of freedom in how you set up these
cascades to go right so to me this
cascade data
I assume it's given so I'm not assuming
any particular model of how this is
spread and whether you get you die or
not and you can you can create multiple
cascades and so on
okay so basically what we have is
something that we call self which is a
stands for a cost-effective lazy forward
selection algorithm which is basically
two independent runs of a modified
greedy algorithm so we generate two
solution sets so a prime just says I the
way I generate it I ignore cost and
greedily optimize the reward and then I
also generate a separate solution by
greedily optimizing this reward to cost
ratio right so the idea is to select
nodes that give me the most reward for
the cost they have okay and what you can
show is if you pick if you if you would
do any of these two separately that can
do arbitrarily bad but if you take the
best one of the two then you can prove
that effectively you are getting like a
factor three approximation okay so you
can basically run greedy model or a
smart greedy twice and you know you'll
do yield well okay that's that's the
bottom line
the other thing that you use the cost
I I mean like I look at such a problem
source inside okay randomize it right
you look at set cover you look at
feedback feedback or ik problems right
they are all share similarities and
randomized albums you just boost the
confidence by you know looking and I
have an answer so this is a factor three
right what we also have is a data
dependent bound that we calculate from
the data we are given when we do that we
actually we actually see that our
solution is like at point nine of
optimal okay and then for example what I
also did is I took the solution I get
from this just greedy thing right and
through simulated annealing for to eat
for a week my in solution improved for
like two percent so if that that that's
a data dependent answer I don't have a
randomized type of answer I think that's
okay there are still problems minutes
right I'm running greedy and I just want
to show you what greedy algorithm does
right so the idea is if that's my
network these are these are my rewards
the idea is I'd consider placing a sense
know placing the sensor to know they and
that's the reward I'm getting right and
I consider placing each each of the
sensors and I pick the best one
okay now this rewards that I calculated
in previous step getting getting valid
and now I'm asking which are not which
other nodes should I add to my current
solution to make which other node should
I add to my current solution again I
considered how much benefit do I get
from adding that node and I would again
pick the best one right and I continue
in a greedy in a greedy way so that's
what greedy algorithm does and it's slow
the reason it's slow because at every
point of that at every point I have to
go and consider all consider adding any
any one of all the other non non
monitoring nodes yet okay and basically
for the size of the problems we are
interested in this is too slow so now I
can show you how to how to speed it up
and the idea is to use sub modularity
and one thing that comes is that some
majority basic
basically guarantees that these
marginals rewards decrease the rewards
decrease with the solution size right so
the idea is that if I monitor no de
sorry let me start again so I'm
interested in note D okay and that's the
reward I'm getting from monitoring you
know the D now if I monitor if for some
reason I decide to monitor a and now I
ask how much benefit do I get from D now
the benefit only shrinks right and if I
would say ok now I also monitor a and B
together how much reward do I get now
from Edie placing the sensor to note D
that only shrinks right so the idea is
to use this mark to use marginal reward
from previous time step as a as an upper
bound on the current reward so to show
an animation
if I have my network and these are my
rewards I consider placing each one of
the sensors I pick the best one and
place it but what I do now is I I
reorder these by decreasing reward right
and now I will consider adding placing
this sensor first and so on right so
let's consider that I so this is what
this was the reward in the previous time
step now I consider adding sensor D to a
and that's the real reward I'm getting
so I put it down now I consider placing
B and let's say that this is the reward
I'm really getting from it and I know
now I can stop and the reason for that
is is because I know if I would consider
e this will only shrink right so because
there is this difference
I can I can place okay and basically
what is what is and boils down to is
that I don't really have to go and
consider all all the sensors but very
quickly I found the maximal one and I go
and place it ok and this saves me lots
of time just empirically I'll show next
slide so to show you to show you an
application how well our think is doing
I want to explain this picture first and
then and then and then I'll show you the
results so what I'm showing you here is
that every box is a blog every circle is
a post and hyperlinks now are like time
labeled
ordered hyperlinks right and if I follow
them I basically find Cascades right
this was what happens first then
somebody cited it then somebody else I
did it and somebody else I did it right
and now I can ask which blocks should I
read so I'm given a budget and the
question is which block should I read to
be most up-to-date so the idea would be
if I read this b1 then I detect two
cascades right I detect this one and
this one and I detect them very early
right so I'm the first to know on the
other hand you could say I will go read
this block that is that is large erect
it has what six six posts versus two
posts I detect all the stories right I
detect this one this one and this one by
did I detect them late so I have these
trade offs between the cost and and the
benefit I'm getting okay and just to
quickly show results is the question is
which blocks to read to be most
up-to-date I have the number of selected
the blocks I'm reading on the x-axis the
reward this is the fraction of
population I know the news before the
rest of the population knows about it
that's our solution these are solutions
that you could get if you would decide
to read blogs in some particular order
like by the number of feelings they are
getting the number of outings number of
posts if you decide to read random
blocks right and this basically tells
you is who are the most influential it
on the web to know before anyone else
knows about a particular story and to
show the scalability result that's the
number of selected blogs run time lower
is better exhaustive search experts
quickly greedy's goes up quadratically
and lazy evaluation trick I showed
basically goes up linearly so in this
particular case forget like 700 times
faster then we get by naive greedy
algorithm okay and another application
that that also works is the following
problem if you have a real water city
distribution network right so now these
are houses and these are physical pipes
between them you can consider saying
that you know if there was some
contamination happening at this house
this contamination spreads this
particular way the question is where do
you place a sensor so that whenever
there is a contamination you you raise
an alarm as quickly as possible and
that's something that these guys care
about okay
and that was you relying on a generative
model for how these cares
it's happen not so I my algorithm
assumes this data is given to me so for
example in this particular case we the
US Environmental Protection Agency
actually and civil engineers it's one of
ours no I have to see the Cascades I
have to observe them I crawl the data to
see the how information spreads right so
my algorithm assumes you give me these
data now you can use simulator to
generate these data you can go is is
it's the this one right so here right
right if you if I if I decide to to read
here then the the reward I'm getting
these three right because I detect
before this one this one and that one
okay so that's the reward function I was
considering in this case does this make
sense so I guess when you stated it in
words but my fear is that this would
lead you toward reading all the posts
about Britney Spears instead of the
posts about things that might interest
you because they have a larger number of
people in the population who are
interested in but doesn't mean that you
will be um exactly so it if there's a
generative model it's more or less
uniform then you might end up actually
you know if all topics are essentially
equal in it of interest to you then
that's a good point so right now I say
you know the the magnitude of the you
know if everyone talks about Britney
Spears then yes I want to know about
Britney Spears but what you can do is to
say I I care about you know Google
Cascades and you take whole Cascades in
dr. on on Google and you say now which
blog should I read to know about Google
right so you can throw away cascades
that you are not interested in and
optimize only over cascades or topics
that you are interested in so you can
still do that what the thing you said
right so for example one interesting
question would be I want to know about
the topics that happen on Slashdot
should I be reading Slashdot right is
there it's some other blog that talks
about the same things but tourists about
them sooner so that's another way how
you can ask so I think like what you are
asking is a sub problem of what I'm
sorry okay so I did this I show this and
basically the bottom line here is that's
how well we are doing this is the
fraction of population saved versus the
number of censorship
that's our solution these are like
heuristic solutions you can compare
against like placing in big junctions
placing randomly placing where there's
lots of population and so on and there
was this competition roberta mentioned
and yeah we did really well like better
than Sundra and all these other civil
engineers or people that actually work
on this for a number of years and
basically what we did here is we had a
simulator and we simulated for every
house every five minute of the day what
would happen if that you know five
minute interval there would be there
would be a Poisson added in that
particular house so we had like 3.5
million different epidemic scenarios
okay so I browsed through this now I
want to show this part and I I promise
to go quickly so what I want to ask now
is how to generate realistic synthetic
grafts so the idea is the problem is
give me generate a network that
synthetic but looks realistically so you
give me a real graph and ask me to
generate something synthetic so that now
if you compare these two things by let's
say calculating some statistics over
them on the statistics match like
statistics like the grid distribution
okay and why would you want to do that
there are many reasons like you'd want
to do simulations you'd want to like the
predictions you need new models for
hypothesis testing if you want to share
privacy sensitive graphs you could share
the synthetic copy and so on right and
there are two questions that you
naturally can ask is what network
properties what statistics do you care
about when you are doing this and even
if you determine what are these
statistics the next question is what is
a good model for graph generation how do
you fit that model okay and I'll show
you basically that here you don't really
have to commit on any set of statistics
but you can do this in a nice principled
way and these statistics come out on
their own so first what I want to show
you are is some empirical results how do
networks evolve and there will be two
questions I'll be asking the first
question will be very simple what's the
relation between the number of nodes and
the number of edges in the network as
the network grows over time right and
prior work make this assumption that the
average degree remains constant over
so what and that's not true and what you
can do is the following so you can
create plots like this where you say
every dot is a graph so this is a
internet over time so this is this is
like seven hundreds
autonomous system snapshots over two
year time period every point here is a
graph that had this particular number of
nodes in that particular number of edges
right and there is an opposite
observation to make is that this this is
on a line I'm plotting on log log scales
and this line has a non-trivial slope
and I'll explain what I mean by that and
also these are like physics citations
again this is the network of physics
citations in 2003 that's in sorry in
1993 this is in 2006 and this is and
every part every point now is these are
like year by year snapshots and what do
you see from this is that networks
become denser over time following this
densification power law equation which
basically says number of nodes at number
of edges at particular than T is
proportional to the number of nodes at
that time raised to some power alpha
right and this this this power so far
people assume this one in theory it can
be like between 1 and 2 right if it's 1
you have constant average degree if it's
- you're like building a clique right
everyone is connected to a constant
fraction of other nodes in the network
what turns out is that this this
exponent is non-trivial for internet
it's 1.2 so Internet is slowly
densifying citation networks are then
defined in a quicker way so what what
for example for citation networks this
means is that either people are writing
more survey papers over time or our
citation lists are getting longer over
time just using Emily using Emily Emily
or like least squares fitting or I would
I did this and I did Emily estimate
why is Dalia noise at night
I need you can say yeah I consider other
feats and this feedback time is it
should you consider other tricks I'll
take this shape aha you mean like n log
n type type things yeah I think I did
didn't so here I'm showing you two
examples I give it over like ten
different time evolving over long
periods of time networks
he had no here does so oh you mean the
kiddies like what almost three orders of
magnitude here here the range of
magnitude is smaller so this is like ten
years of data this is two years of data
every every snapshot taken every day so
here's like 700 data points here is I
think like 14 data points I have like
graphs where I had like 30 years worth
of data like US Patent citation networks
they go over 30 years and they go from
like order of hundreds to order of
millions I mean you can take your
favorite one and show that it's
different I mean I don't know what else
to answer here to distribution we have
some significant tests that this is
actually the power AHA sure I I had I
have resolved I have I have I have done
these things I mean I'm not reporting
them here but I took I took a bunch of
networks and we did this like simple
measurement and let's inter internet or
networking community today except this
densification that before they didn't
know about and I'm not a networks person
so they seem to agree with this for
autonomous systems Network and now they
are studying why this is happening and
which which internet providers which
sort of tier what level internet
providers are our cause are the cost for
densification
so I think that's another way of right
they agree with that and they are domain
experts and I'm sort of not a networking
person computer networks person okay I
guess the question is they might be
domain experts at this that this helical
statistics
because I mean testing fits for for
these types of distributions is actually
cooki oh of course of course of course
no that's that's that's the reason and
sure you can run kolmogorov-smirnov and
things like that and it won't give you
results as you expect them to be given
right I mean if you ever work with power
law and Capitol degree distributions
then you know that this desire this
these distributions of it
behave very differently than like
distribution with exponential tales
we're like talking about averages make
sense right these distributions have
like infinite means and like infinite
variances and things like that right so
I mean it's different and yeah if you
don't have two good tools to work with
them I mean that's another point of the
story okay another one I hope I hope I
don't know here here I'm not so the
diameter right how does the diameter of
the network grow over time what I mean
but by the diameter is like let's say
the longest shortest path between any
pair of nodes right and the idea is if
you have a small object a small graph
than your diameter is small if you take
a bigger graph your diameter would be
would be larger so that's what people
been assuming and or what models that
that people have been using so far were
given to us and this diameter would like
increase slowly right into this
logarithmically or like sub logarithmic
given the network size and if you look
at the data it's actually not like that
right so I'm using showing you the same
two dataset here's the size of the graph
versus the diameter this is I'm plotting
you the effective diameter which is like
the 90th percentile of the pattern
distribution okay this is the Internet
these are physical situations
okay I'm not making any claims on what
is the shape but I think we'll all agree
that the the diameter of the network is
decreasing right as the as the network
grows over time these are like from
order of hundreds to an order of a
million the diameter the diameter
shrinks okay
I haven't had a chance to try them so
all the graphs I had a hold off and I
was able to try this this was the case
there was only one and this was the very
early new Telegraph and the reason and
actually the reason for that was that
the cost of links the the cost of links
there gets sort of the overhead of
communication starts to hurt you so
you're shrinking your degree so that
graph was sparse II find in the end the
diameter was increasing
undirected so even though these graphs
are directed I considered this as being
undirected sorry because it tells how
many hops are between any two pair I
mean you can also consider director
diameter but citation graphs are
inherently directed right they there is
always you can always only cite into the
past because I don't think whoa it
wouldn't make any difference
oh but the question is how often do
people you know do you only cite five
years back and they if everyone is
citing just five years back right then
as the time goes the longer you go the
longer will be the diameter if you are
citing you know all like if you are
citing also very early papers then okay
okay so these are two empirical
observations and now the question is
what is a good model right and if you
try anything that people had so far this
doesn't they didn't really they don't
pretty well and and yeah that I want to
like it is to generate graphs
recursively why do I want to use this is
because recursion usually goes this
recursive self-similar fractal
structures usually go hand in hand with
these cute degree distributions so
that's that just the intuition and
here's the model right will generate
graphs recursively so assume that we are
given some graph in initial graph let's
call it initiator so this is the graph
that's the adjacency matrix right one
means that there is an N 0 means there
is no edge so what you can do now is you
can use stencil product over this
suggestion see matrices or Kronecker
product so what i'm doing here is i'm
taking this matrix putting it in every
cell cell of itself and multiplying with
a constant in front right so I'm getting
this copies of g1 here and I have this
block of zeros 3x3 right so this was 3x3
this is now nine by nine okay
and if I keep doing this I'm getting
this nice recursive adjacency matrices
okay and if this is the model that I
assume basically this this operation is
the
is canonical product of graphic jccc
matrices you can actually make a nice
story why the world could be like this
you can also make this model to be
probabilistic right instead of having
zeros and ones now every every cell has
a probability of an edge occurring so
you can do the same the same trick so if
you have probabilities here then you
will have probabilities there and this
will be this would be why why I want
probabilities it will be clear on the
next slide what is nice that if this is
the generative model you assume you can
prove that this will give you like
heavy-tailed degree distributions it
will densify it will have a shrinking
diameter that become that gets constant
if it will I expect there are properties
that you find in the real graphs okay so
so that's the model the question is
still now what I want to do is I want to
estimate this small so basically I want
to learn how this initiator is okay
given a real graph I want to learn this
parameter matrix okay so what I'll be
doing I'll be doing maximum likelihood
estimation so the idea is the following
if I have my little parameter matrix
let's go let's say it's 2 by 2 right I
will use Kronecker product to expand it
to big adjacency matrix where now every
cell here is a probability of single net
okay then you are given me a real graph
in what alt-right estimate is this
probability right I'll try to estimate
how likely is that this set of
parameters would give me this particular
real graph okay and if you look how you
could go about this you can see that the
complexity of this will be like n
factorial N squared so the n factorial
factorial term comes because you have to
like integrate or sum over all possible
different node labeling right the idea
is if I go and permute the rows and
columns of the adjacency matrix maybe if
I choose the right permutation this
particular matrix would look exactly
like that so I have to go and consider
all possible row and column permutations
okay so that's that's the N factorial
square term and you can get around it by
using metropolis sampling so basically
got four men factorial to constant time
there is a n squared term which
basically says even once you once you
settle on the on the node labeling you
still have to go and compare cell by
cell
you know I saw basically what we are
trying to estimate here is
given coins with these particular biases
I want to observe this particular
sequence of zeros and ones right and
this means that I have to go through
adjacency matrix element by element so I
get an N squared term so for the size of
the graphs I want to work with even N
squared is too big so I can't even
calculate this matrix because I ran out
of memory I ran out of time so you can
use some properties of Kronecker product
and some tailor approximations to get
this down to linear time so and once you
are able to estimate this this
probability you calculate the gradients
and you are basically doing stochastic
gradient descent over the parameter
space okay to find the most likely set
of parameters that gives rise to your
really Jason C matrix and we can do this
in linear time so just to show you
experiments on not to be graph right so
on a seventy six thousand nodes and half
a million edges the search of the space
of all permutations like ten to a
million I can fit this in two hours and
that's the parameter matrix I get like
four numbers now if you take the real
graph and the Kronecker graph and
compare the statistics this is the
degree distribution red is the real
green is what we get this is the
cumulative petland distribution in an
undirected version and that's the
distribution of the values of the first
eigenvector of the graph adjacency
matrix and what you can basically see is
that we capture all these statistics by
optimizing the likelihood of interest
and we can do this our fairly large
graphs right and these microscopic
properties seem to match okay so that's
basically the bottom line here and know
you can take you can take any end-by-end
right so I just chose so this is like a
model selection type of problem right
you know how you know will you fit a
linear versus quadratic or whatever
right so how strong model will you pick
and you can use like bi C or any any
criterion like that where you are like
trading off between your your model the
fit of the model and the end
strength of the model okay and actually
okay so I think I'm done so what I want
to briefly show you is so basic what I
show that these are these two these two
stories one on diffusion one on network
evolution I showed you like patterns and
observations desiccation and shrinking
diameters for networks and then
chronicle graphs and basically I think I
also try to convey that like big data
matters so now I now can I get another
four minutes it's 58 okay okay okay so
let me show you a few future directions
that I think are very cute so one one
thing is basically a planetary scale
look at the small world phenomenon right
so in 67 the Milgram conducted the small
world experiment right where the idea
was that people in Nebraska were given
some letters and they were supposed to
send them to some stockbrokers in Boston
and you can you are only allowed to
forward the letter to your to your to
someone you know to your friend right I
mean the question was how many steps
does this take right so the idea is if
this is the social network of United
States here's Nebraska here's Boston you
can only you would like to get a letter
from here to there but you're only
allowed to forward it to your neighbors
in the social network right and let's
say that this is how the letter travel
then there is another set another pair
of people that would send letter in this
particular way right and the question is
how many steps does this take on the
average and this is the graph from
original Milgram's paper and like 64 of
these letters came to the end so from
this he calculated the average and the
average was 6 so this is today known as
six degrees of separation okay you can
do the same on a bit larger Network
right so you can take the the messenger
people so everyone who uses messenger
you get you have like this 240 million
people and you can try to estimate the
same thing right and if you do that I'm
plotting the same thing I'm plotting the
distance the number of hops between any
pair of people I'm plotting the number
of pets here I'm plotting in logarithmic
scale because I have like four million
data four million times more data points
and what you can see from basically what
if you carry the average the average
turns out to be six point six right so
people are at six point six
hops away on the average and the next
the question that I think I would like
to address here next is basically how do
you learn in these short paths or how
should you search in networks right if
you know you want to get a message from
from some some place to somewhere else
and all that you are allowed to is
forward this message to your neighbors
how should you forward how should your
out you know how much does geography
versus your personal characteristics
versus the network structure matter in
doing this as as well as possible right
because here here I was doing a shortest
path type of computation while in the
real life right people only have local
information so how can you how can you
learn these things that's one the other
things that that I think are coming up
are basically diffusion on the
blogosphere right a typical picture
people have our bloggers have is the
following right there is a sort of an
obscure technological story that then
slowly propagates the mass media and the
questions that are interesting here is
basically how does this propagation
process happen how do people influence
in one another and what does it mean to
be important right what are the ranking
and influence measures for bloggers
what is also interesting here is how
could you model this right so far I was
I showed you how to do the optimization
how to select important people what you
would like to have is also a model of
you know saying if I put piece of
information here this is how it will
spread and you can obviously use that
for marketing right and actually what I
what I what I what what also happens is
that this depends heavily on the topic
so why the reason I have these two
things here is that this is like a
typical Slashdot cascade right
technological Cascades that says
something and then a lot of people cite
it but nothing else happens this is a
cascade from like michelle malkin right
when people talk politics it's like less
people get excited in some sense but
they have much wilder linking patterns
right they actually discuss or Park or
are you so this isn't the diffusion side
I think on the network side interesting
questions are basically very basic wire
the network is the way they are right
what are the good models for
understanding the network structure and
for example the work I showed you on
generating synthetic grafts
where base
the first were able to scale beyond 100
or 200 notes right to scale to hundreds
of thousands or millions of nodes the
other question I think people here will
be also very interested in that that I
just I mean I don't have more than this
basically what is the health of the
social network right what's what you
have if you are Facebook for example
what all you have is your social network
you know how healthy it is how can you
steer your social network not to fall
apart or not be taken over by brazilians
or whoever right right I mean if you
wanted that to happen then sure but and
once you know that you would also like
to say how do I better design network
services right how do I better design
the next social networking site okay so
basically that concludes my talk that's
the summary and thanks a lot for
listening questions
or get
oh the largest connected component is
like 99.9% it's like everyone's
connected and what I have actually is so
I took all the users the tech TV so I
would take one month of user activity
and two people are connected if they
exchange the message okay so it's not
buddy list but it's actually active
communication and if you take that and
see what how connected is yeah it's like
99.9 it's like I think the next largest
so we have 240 million here and then the
next largest component is like on order
of 100,000 you look at what the cockpit
that distribution the posterior
distribution is so I'm kind of curious
is that that estimate you got how stable
is how sensitivity yeah I'm working on
this right now actually so what I what I
what I was doing right I would like
start my gradient descent from a random
point and then see where do i converge
like what I found is that if I have a I
found that I can always recover the
parameters so if my graph is Kronecker
using generated some parameters I start
from a random point and see where I
converge
I always recover the parameters so
that's good that's like good good news
what I was also playing is with just
like is how much these parameters are
sensitive and they're it's sort of hard
to give an answer they are a bit
sensitive but not that much right it
won't happen that you know you change
one parameter for like point point one
or something and you shoot off it's like
things are quite smooth in some sense
but yeah III would love to chat more
about this
although school average
you mean for densification is thinking
diameters yeah the things there so we
would also take we would take this
affiliation graphs where it is sort of
bipartite networks of people two papers
or you can then multiply that with
itself to basically get people-to-people
networks the problem it's a bit
collaboration graphs is that you have
these big clicks especially like in
physics you have a paper that's like 20
30 50 by 50 people on that right so what
you're getting is like a clique of 50
people but in terms of this sort of
global statistics yeah I got the same
actually with citation networks there's
another another point is this problem of
missing past right that you know you
start collecting data at some point in
time and there are all these citations
you know to 1950s or you know if we
start collecting data in 1950 there are
all these citation to 1930s so we would
actually also consider this and see
whether it's the reason why we are
seeing the shrink in diameter is because
we are missing this past and actually we
do the experiments and we were able to
show that that's not true but for
citations that's for example the problem
right for the internet that that's not a
problem or for link I mean I also like a
tried linking things like that okay
thanks a lot</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>