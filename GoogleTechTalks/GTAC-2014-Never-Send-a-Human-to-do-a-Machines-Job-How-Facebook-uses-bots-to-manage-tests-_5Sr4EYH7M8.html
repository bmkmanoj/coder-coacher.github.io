<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Never Send a Human to do a Machine’s Job: How Facebook uses bots to manage tests | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Never Send a Human to do a Machine’s Job: How Facebook uses bots to manage tests - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Never Send a Human to do a Machine’s Job: How Facebook uses bots to manage tests</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_5Sr4EYH7M8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">So with that, our next speaker, Roy.
Roy is going to tell us how he is replacing
humans with robots for testing at Facebook.
&amp;gt;&amp;gt;Roy Williams: Yeah.
Cool.
Thank you.
So, yeah.
So the title of this talk is never send a
human to do a machine's job.
Basically inspired I was watching the Matrix
as I was thinking of titles for this talk,
but it's basically about how we use bots to
manage tests at Facebook.
Other candidates for the title were, &quot;Yo dawg,
how to use automation to manage automation,&quot;
but as you've heard, legal doesn't like names,
so, oh well.
So let's go ahead and get started.
We'll do a bit of introduction.
Talk about myself, talk about testing at Facebook.
We'll get into a bit of the history of testing
at Facebook, kind of how we got here.
And then we'll talk about the robots we have
and how we use them to manage tests and leave
some time for question and answer.
Sound good?
Cool.
So why talk at GTAC?
This is actually one of the questions I had
to answer when applying to speak at GTAC.
I thought it was interesting to share the
answer with you guys.
First of all, I think we've built a lot of
really good stuff to deal with flakiness and
a rapidly growing series of tests in the code
base and hopefully some of these ideas can
be useful to all of you in here.
The second reason why I wanted to talk if
GTAC, and I'll dig into this a little bit
more later, is I probably had this -- a version
of the conversation that oh, Facebook doesn't
have tests or Facebook doesn't have testers
or tests various different things.
Maybe a dozen times here and every time I
go and talk at a meetup or a conference like
this so I want to get a chance to talk about
how we are actually make this work.
How we are able to have high quality test,
have testing but not have a separate role
for testers.
So a little bit about myself.
The TLDR of this is that I really love developer
activity.
This is what I have built my career around
and this has kind of taken me bouncing between
SDET or SET and SDE at various points and
times.
So Microsoft I flipped.
Came to Google, became an SET again.
Went to Facebook, was an engineer, and now
I'm an engineering manager.
So I'm manager of the product stability team
at Facebook.
And you as may have heard, we don't have testers.
There's no SET or SDET or role like that at
Facebook.
We have a small team of people who are focused
on the tools that it takes to build automation,
and the tools that it takes to actually have
tests at Facebook scale.
So we don't actually write any tests.
We just more think about how can we possibly
run all these tests.
So, yes.
So Facebook doesn't have any testers.
We have very little QA.
There maybe is a few people that are responsible
for making sure you can log in before a release
goes out.
But we do have lots of tests.
We -- Developers own everything about their
code at Facebook.
So if you're a developer, you own writing
your code, getting your code reviewed, you
own pushing it out to production.
The release process could be a whole separate
talk.
If something goes wrong in production, you
get paged.
There's no dev ops.
There's no tester.
It's really, the responsibility is on you.
Developers care deeply about shipping quality
products.
So you're expected to have a test plan associated
with your diff.
This might involve actually having unit tests
or how you validate this manually but we do
have tests.
We do really care about tests.
Just we don't have a separate track in the
career path for that.
Cool.
So let's talk about the how and why of testing
at Facebook.
So I thought it was actually kind of funny
to have this slide in here given that the
keynote was move fast and don't break things.
To touch on this a little bit, we eventually
did replace &quot;break things&quot; with be bold.
So we broke break things, but it is one of
the mottoes at Facebook, move fast and break
things.
So kind of the world we're dealing with, as
we talked about, there's no separate developer
track careerwise.
And the code that we're shipping, we have
a few monolithic repositories.
I realize that's a bit of an oxymoron, having
multiple monolithic repositories.
But the idea is like in the Android code base,
there is not a messenger branch or something
like that.
Everyone's moving forward.
It's all developed at head.
There's no strongly version dependencies or
things like that.
The Web site ships twice a day.
So we ship everything once a week, and then
twice a day you can cherry pick things into
master.
On mobile, Android alpha ships every single
day.
So we did a blog post about a year back saying,
hey, if you want to be on master or Facebook,
it's going to be crashing bad, and there was
a surprisingly large number of people that
said, yeah, give me some of that.
We have a beta that ships multiple times a
week on Android, and then outside of that,
the Android and iOS apps ship every two weeks.
So the TLDR of this, that we don't really
have time for protracted testing process anywhere
here.
We don't have time for ZBB or stabilization
or anything like that that you might see in
more traditionally shipped software.
Let's talk about how we got here.
The genesis of testing at Facebook.
So around 2009, hop in our time machines,
we had hundreds of developers.
The complexity was starting to exceed the
point that any one developer could page in
all of the code into their brain.
And this is a very interesting point.
I have a yet to be proven theory that codebases
will only safely change at the velocity that
a developer can comprehend the implications
of every change that he's making.
And so once you start exceeding that, you
have two options.
One is to either move unsafely faster, or
is to slow down, or think about how to better
comprehend the implications of your change.
Developers were increasingly blocked by trunk
breaks.
So if you're trying to work on the home page
and news feed doesn't load because someone
broke news feed ten diffs before you, you're
going to have a bad time.
That's not great.
And too many bugs make it into production.
Engineers spend too much time firefighting
instead of working on new features.
So to make sure everyone is awake and knows
why we're here, if I presented these problems
to you, what would you come up with?
Anyone?
Don't be shy.
&amp;gt;&amp;gt;&amp;gt; Manage complexity.
Refactor.
&amp;gt;&amp;gt;Roy Williams: There's two words in the name
of the conference.
Automated testing maybe?
Yeah.
Automated testing.
So we had a few people spin up very kind of
skunk works, bottom-up, and focus on automated
testing at Facebook.
There was a few people working on just the
actual unit testing frameworks themselves
like PHP unit.
Some people were working on things like making
mocking out data easier.
Some people were working on things like making
the results pretty and easy to look at.
And this is great.
We have people chugging along, writing tests,
working on -- working on new features.
It's great.
We're getting to thousands of tests.
Awesome.
Still no testers.
Still just a few people working on this.
But there was a problem.
We had what we called the curse of success.
So we got to the point around mid 2011 where
we hit what we think of as the testing death
spiral.
So the red line is the number of failures
in trunk.
The blue line is the number of tests.
So we're continuing to grow the number of
tests.
And around mid 2011, we kind of hit a knee
or hit a hockey stick growth, but not in a
good way.
We hit it with test failures.
So what happens here is pretty interesting.
Imagine you're a developer, you're working
on a feature.
We have all these great automated tests.
There's thousands of them.
You put up your diff for review, and you get
a failure.
And it's totally unrelated to the change you
make.
Your first reaction is, man, this is great.
Like, I just had this automated testing framework
let me know about a failure I didn't think
to test.
I had no idea I was touching this code.
So you start looking into it, spend an hour
or two, you're digging into it.
You realize this test is actually flaky.
Maybe it's reaching out to iTune.com or something
else.
Anyway, that's kind of annoying.
And you push your diff with a test failure.
A week later, the same thing happens, you
debug and you start looking into it.
Still same thing.
Not your fault.
You're starting to get frustrated.
Third time it happens, you're just not going
to look at it.
Not only do you have people stop caring about
flaky tests because they feel like they're
just wasting their time investigating it.
You also start to get a big of broken windows
where people think there's 30 failing tests,
what's 33 failing tests?
No one really cares, obviously.
So this is bad.
So it's -- as with any death spirals and negative
feedback loop, people stopped caring, so more
things started breaking and we quickly started
losing the value of these tests.
If you remember one thing from this talk,
and in fact there are a lot of people talking
about this here -- flaky tests are poison.
You can have the best tests in the world,
the most sophisticated regression testing
that finds the craziest bugs no one would
think of, but if developers can't trust the
results and they can't trust the results 99%
of the time, it doesn't matter.
They're not going to look at it.
It doesn't matter if you really wish they
would look at it because 70% of the time it
is a real failure.
They're just not going to look at it.
And rightly so.
Because it's just slowing them down.
They're not able to work on what they want
to work on because they're spending all their
time dealing with flaky tests.
So I want to drive this home.
That's why I put it on its own slide.
And I was glad to hear other people talk about
this at the conference, too.
So in keeping with the theme of machines,
we had the rise of the machines.
So we started a project called Project Greenhouse.
First thing we wanted to do was do no harm.
We didn't want developers to have to spend
time dealing with tests that weren't actually
helping them.
Someone in a believe talk had mentioned that
flaky tests are worse than no test.
And I agree 100%.
People aren't going to listen to them when
they fail.
And all they're going to do is train people
to ignore the tests.
We need to rebuild trust.
We are really lost the developers' trust.
They had get getting red on their diffs all
the time.
So they really just stopped caring.
We wanted to keep the test in a good state
without human intervention.
One of the things that a lot of my peer managers
make fun of me for is I like to think about
human problems in terms of Big O. And if you
have any fixed number of humans responsible
for cleaning up all of the tests, it's going
to be awful.
I don't know -- does anyone here have a sheriff
rotation or someone who has to keep the build
green, anybody?
A few people.
Okay.
So I don't know if you've experienced this,
but at first, it starts off maybe there's,
like, four or five breakages a week.
It's not really that bad.
And then in a year, when you're sheriff, that's
all you do, all you do is deal with test breakages
or build breakages.
And it's awful.
You hate your life that week.
And so having machines do this is really important.
Machines are still getting faster.
Moore's Law is still in effect.
You're just getting more cores.
Humans, not so much.
So we really wanted to trade machine time
for human time.
So some of the goals we wanted to do were
to cull the bad tests, get rid of any of the
tests that weren't providing value.
We wanted to reduce the noise on everyone's
diffs.
And we wanted to be able to automatically
manage this.
So these are the kinds of features that we
wanted.
So you might have heard, like, some of the
things we've talked about before that people
can sometimes get worked up about is deleting
tests.
This is where we want to make sure that any
test we have is actually pulling its own weight
and has value.
Cool.
So this is the system we came up with.
And this is what we'll spend a bunch of time
talking about.
We started this -- this was sort of a manual
process at first.
Then we started putting bots in place as we
got more sure of the different things we were
doing.
So this is the state machine that effectively
goes through its life.
So first you have a brand-new test.
It's great, awesome.
You've written a new test.
But we don't really know anything about this
test yet, nor do we trust it.
So we actually do not report this on other
people's diffs until we can prove that it's
good.
Imagine that you get a test failure on your
diff, you look at the history of the test,
you see it was added 15 minutes ago.
You're likely not going to trust that test.
And we found that, too.
We found most often, if there was a failure
on a brand-new test, it wasn't the developer's
fault who had the failure.
It was usually the test author's fault.
It usually wasn't a very good test.
So -- see if we can do this -- a brand-new
test comes in.
It's an unknown quality.
And one of our robots, TestWarden, picks it
up and stress tests it.
So it will pick it up after some time, run
it a bunch of different times with itself,
on the same machine, different machines, interleaves,
trying to find flakiness.
Should it fail any one of those runs, it does
not get promoted to a good test.
It stays in the unknown test quality.
We file a task on the owner.
We say, hey, awesome you added a new test,
but it's not good enough to run on everyone's
diffs.
Please fix your test.
And then it just kind of stays in this state
here.
Oops.
There we go.
Just kind of stays in this state here.
So then we have a good test.
This is great.
We have a good test.
We use this for a few things.
So when we have a good test, it is running
on every diff, it is providing developers
feedback that they can break the tree.
The good tests are also responsible for holding
up something called stable.
So we have a branch that just lets developers
know, this is the last time that we had all
of the builds pass, all of the tests pass.
And so if something is failing, we will not
promote stable until that test is passing
again.
This can hold up the push.
Remember we talked about we have two pushes
a day.
If you mark a test as push blocking, nothing
will go out the door until that test is passing
or you've said that it's okay, we can push
without it.
So good tests are great.
We get a lot of value out of them.
But they don't always stay great.
So if a test starts failing, one of our other
bodies, failbot, comes in and picks it up
and moves it into the failing test state.
With the failing test, we will then hide it
from all of the rest of the developers.
We won't let you know that this test is failing,
because why bother?
It's already failing.
We likely know who broke it.
We don't really even get value out of spending
the CPU cycles running it.
We'll file a task on the owner, we'll say,
hey, this test is broken.
We'll see if we can figure out who broke it
with a bisect.
Should that fail, every test has an owner
and we'll assign it to the owner of the test,
say, hey, this test broke.
We don't know why.
It doesn't seem to blame to any given revision.
Can you please figure it out.
So we're in the failing test state.
This is no good.
A few things can happen here.
If -- So we have another run that is just
sitting there on a continuous basis, rerunning
these tests as diffs come in.
If something then starts -- so if something
starts immediately passing again, we will
move it back into the good test state.
And importantly, we will close out the task.
If you're looking at your task list at Facebook
and you have all of the test failure tasks,
you don't have to look at it and say, oh,
I've got 300 tasks about failures, but 250
of them are already fixed, so why am I going
to bother?
Technical, it's all about reducing noise and
making sure that if we have something on a
developer's plate, we're really sure this
is actually something they need to fix.
If we see that this was flaky, another bot
comes in called GreenWarden.
So if one of the tests seems to be passing
and failing sporadically, we'll expect that
some flakiness was introduced.
And then we'll move it back into the unknown
test quality.
We'll file that same task saying, hey, this
thing looks bad.
Why don't you please come in and fix it.
Yeah, so now we're back to the unknown test
quality.
Someone would have to fix the test.
We'll periodically run it.
We'll try to put it back in the good test
state.
If something keeps failing, failing for a
week or so, we move it into our disabled state.
And disabled test will then file a task saying,
hey, just go ahead and delete this code.
We're going to delete it anyway.
And you have five days to fix your test when
it's in the failing state before it becomes
disabled.
At a company like Facebook, a week is actually
a really long time.
Like, we've already pushed the site ten times.
So it's actually not that crazy to just go
ahead and disable a test altogether if no
one's paid attention for a week.
But we will actually keep running it.
We will run it just a little bit less often
in the disabled runs.
So this might just be a few times a day.
If something seems to start passing the disabled
runs, we'll then put it back in our initial
state and start the whole process over again.
So the great parts about this system are that
when you're -- anytime that we tell you something
has failed on a test, we actually are fairly
confident that it is actually your fault.
And we don't get this cruft built up over
time of tests that are failing or disabled
or things like that.
We just go ahead and automatically clean them
out.
Because, clearly -- because people don't care.
And that's okay.
If people aren't prioritizing getting these
tests fixed or if they're testing a feature
that's not important anymore, that's okay.
Cool.
So just digging a little bit more into each
of the tests.
TestWarden is responsible for stress testing
tests of unknown quality.
It is responsible for filing all of the tasks.
Failbot disables test if they've been failing
for a week, files tasks about failing tests,
hides failing tests from blocking other developers.
PassBot runs the test on a continuous basis,
runs failing tests on a continuous basis,
and closes tasks associated with failing tests.
GreenWarden finds the flaky tests -- and the
threshold here is around -- I think if it
fails two out of 100 times.
If it randomly fails two out of 100 times,
we will mark it as flaky, put it back into
the disabled state.
Disabled runs run periodically, we just check
to see if anything starts passing again.
We want to make sure if we've turned something
off if there was a task already on your plate
about something being bad, it really is bad.
Eventually, we just say go ahead and delete
the code.
You don't need it anymore.
Cool.
So something else that we do is, while we're
running each test, again, with the theme of
keeping -- it's a weird footnote.
I didn't mean to have that there -- of keeping
the signal high -- yeah, we even have bugs
in slides.
When we're running a test, we will run it
multiple times on the same machine for a variety
of reasons.
So we run a test initially, retry count number
of times to see if there's any flakiness.
If it passes any of those times, we'll call
it good.
We'll say, yes, the test passed, and we'll
keep going.
Should it fail all of those times, we run
it against the merge base on the same machine.
So you might have to recompile here.
A lot of our code is in PHP, so this isn't
a huge issue.
And we'll run that &quot;N&quot; number of times looking
for -- looking for a failure on the merge
base.
And if there's any flakiness in the merge
base, then we'll assume that, you know what,
this probably is just a flaky test.
It's probably not your fault.
And should that still pass every time against
the merge base, we run it again one more time
against the diff, just to be extra sure.
And this uses a lot of CPU.
This uses a lot of CPU, uses a lot of machines.
But we think we get a lot of value out of
it for a lot of reasons.
The first thing that it hides is refactoring
issues.
So if you've touched a ton of code just because
you're trying to change method names or something
like that, we're going to trigger all of the
tests that we think were affected by your
code.
And, really, most of them -- any of the failures
most likely are not your fault.
So really the only things that you care about
are the new failures produced by your change.
We also try to focus on having a single thesis
per diff.
So we want to make sure that when you -- when
you're making a change, there's really only
one thing associated with that change, so
adding a new feature and then fixing a bunch
of test failures isn't really something we
want to do.
So it helps us run that.
By rerunning on the same machine, we're able
to find either tests that are machine-dependent
or bad machines.
So an example of this is that we'll have some
tests that will fail on machines with low
memory or machines with a given CPU architecture,
like maybe it'll fail on Ivy Bridge but not
Haswell or something like that.
Then we also ferret all that stuff out.
And when we start aggregating the data, we
find interesting patterns about tests and
machines and how they intersect.
And we're also able to find bad machines.
So if a machine is always failing all of these
runs, then we know, hey, maybe this machine,
something's wrong with it, disk is bad, memory's
bad, something like that.
So, yeah, we get a lot of value out of that.
Cool.
So some of the other important parts about
managing tests at Facebook with the -- So
some of the other big benefits we get about
managing tests with bots, as opposed to people
-- and to go back here real quick.
So we effectively did all this stuff at first
with human beings going back and finding test
cases manually.
And it turns out that's really hard to scale.
So that's why we've been investing so much
in the bots.
One of the big kind of fun points here is
that developers aren't the jerks.
You're not a jerk toward your fellow developer
about disabling a test.
The machine is.
If you ping a developer and say, &quot;Hey, looks
like your test is failing, it's holding everyone
up, I'm going to disable it,&quot; usually, the
response is &quot;No, no, no.
Don't disable it.
I'm working on it.
I promise I'm going to fix it.
It's just going to be another five minutes.&quot;
Things rarely just take five minutes.
They usually take longer.
So it's not me disabling your test, it's TestWarden
disabling your test and who knows who controls
that thing.
[ Laughter ]
And so then you kind of get to shift the blame
from the -- it becomes a lot more absolute
and the bar becomes much more even, as opposed
to having to have a human in the loop who's
making an arbitrary judgment about what's
good and what's bad.
We talked about this a bit, but filing tasks
and sending emails on failure just isn't -- winds
up not being enough.
You really need something sitting there, watching
it, and having a human being sitting there
watching the test, seeing if it's going to
pass just isn't a great use of time.
Also, this is also about building trust.
Like, having people know that when there's
a task on them, it really is a failure and
not -- not just some random crud that's still
around in the codebase.
All tests need owners.
If we fail to find -- if we fail to blame
a revision, we need to know who we can file
a task on to fix this test or who is going
to care if we disable this test.
And because you have all these bots managing
this, this winds up being a big forcing function
and getting people to actually claim ownership
of the code.
Because we will disable a test.
We will turn it off.
If you have an email list that just is a black
hole, it will go to the black hole.
The black hole won't do anything about it,
and then things will get turned off.
So did this work?
So this is roughly the same graph.
This data is a little bit old.
So we were able to drive down the failures
on diffs dramatically.
We then started to see this bump here.
This data is a little bit old.
It is better now.
And this is where we started focusing on bots,
because when we just did things manually,
like we did before, things just started creeping
back in.
We started seeing more failures come in.
So, yeah, so we had a lot of success with
this.
Yeah.
So that went a lot faster than I thought it
would have.
Maybe I talked faster than I should have.
But, yeah, I wanted to open up to questions
to see how you guys manage tests.
How do you guys handle this?
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you, Roy.
That was a great talk.
So we have lots of questions.
&amp;gt;&amp;gt;&amp;gt; Yeah.
&amp;gt;&amp;gt;Sonal Shah: Let's start with live questions.
&amp;gt;&amp;gt;Roy Williams: Cool.
&amp;gt;&amp;gt;&amp;gt; Hey.
Thank you.
This was a good presentation.
&amp;gt;&amp;gt;Roy Williams: Thanks.
&amp;gt;&amp;gt;&amp;gt; You do some really cool stuff to find
flaky tests that give you false failures.
Have you thought about looking for tests that
give you false positives?
I mean, they're passing when they really shouldn't?
&amp;gt;&amp;gt;Roy Williams: Yeah.
No, that's a really good question.
It's -- So one of the things with flakiness
that we get a lot of concerns about is, are
we missing things -- are there problems that
actually are happening that would have been
caught by a flaky test that we had then turned
off?
In practice, we actually haven't really seen
this happen.
One of the things that I skipped over a little
bit is that we -- whenever there's a big site-wide
outage or something bad happens, we go through
an incident review.
We go through what happened, how we could
have prevented it from happening in the past.
And this was one of the big levers we had
to get testing in the first place.
But then when we started disabling tests,
there was a lot of worry, like, what are we
missing.
And we didn't really see any problems with
that.
On the test passing when they shouldn't side,
we haven't seen this happen too much.
Where we do see this happen is a lot of the
ways we manage releases we've talked about
publicly is a system called gatekeeper, where
we can turn features on, turn them off or
gate them for different sets of users.
And so, occasionally, we'll miss a gatekeeper
that's actually really critical.
So maybe we're testing login or something
like that, and then it fails if you're in
a given gatekeeper but not in this other gatekeeper.
That's -- We're actually working on that.
We had an intern help us out, like, doing
some more combinatorial testing with all of
these different settings that you can have.
So that's where we've seen that happen before.
&amp;gt;&amp;gt;&amp;gt; So for the critical items, it sounds like
you have a second layer, the gatekeeper layer.
&amp;gt;&amp;gt;Roy Williams: Yeah.
&amp;gt;&amp;gt;&amp;gt; For things that escape, you do the root
cause and you find out --
&amp;gt;&amp;gt;Roy Williams:.
[ Multiple people speaking simultaneously
]
Yeah.
Gatekeeper winds up usually being the only
source of problems like this that we've seen
recently where the test user might just have
a totally different code path than an actual
user, because all test users were blocked
from some gatekeeper.
Or maybe we have an employee only gatekeeper
where we're trying out a new feature.
We push out a new version to employees, then
get annoyed because they couldn't log in,
but all the rest of the users can because
we were using this experimental feature.
&amp;gt;&amp;gt;&amp;gt; Okay.
Thank you.
&amp;gt;&amp;gt;Roy Williams: Cool.
&amp;gt;&amp;gt;&amp;gt; So what's your current process for getting
from a failed test to an action item, a bug?
&amp;gt;&amp;gt;Roy Williams: Yeah.
&amp;gt;&amp;gt;&amp;gt; And who does this?
&amp;gt;&amp;gt;Roy Williams: So let's go back to our state
machine slide.
So when you have a failed test, there's a
few things that happen.
One, if it's -- if we can identify the diff,
we will immediately file the action on the
diff.
And if this broke, one of the things we're
not great at and we want to get better at
is if this broke 700 tests we might file 700
tasks to say this broke the world, and usually
it's just one root cause.
So you get a failing test.
We file the task on the owner, either the
owner of that task or the person who we think
broke the test.
And then it's up to them to figure out what
to do with it.
If they don't think this test is important
anymore, they're free -- like we talked about
before developers own all of this.
There's no, like, separate tester.
So they could make a judgment call they don't
care about it.
If it's a deprecated feature -- like say we
did the news feed redesign and there were
a bunch of test cases for the old news feed.
Those probably start immediately failing,
and then we probably care about them for a
little bit, but then after a while we wind
up deleting them.
So does that answer your question?
So we'll file the task on the developer.
Should they not fix it, it will go to the
disabled task and eventually we'll file the
task for deleting the code.
Cool.
&amp;gt;&amp;gt;Anthony Voellm: Thanks for the talk.
I'm going to give him a hard question.
I used to be Roy's mentor years ago.
So one of the challenges and one of the things
I'd love to see is using metrics as a way
of sort of gaining quality on the front end.
&amp;gt;&amp;gt;Roy Williams: Yeah.
&amp;gt;&amp;gt;Anthony Voellm: So have you thought about
how you might take developer apathy, which
is they don't care about the tests, all their
tests break, they've done nothing about them,
and it's this one particular developer and
you know that they should be doing something
but they're not.
Do you have ways of sort of gate them from
checking in anymore and sort of fix that problem?
[ Laughter ]
&amp;gt;&amp;gt;Roy Williams: So we don't.
And what -- we actually don't really have
anything that's blocking.
All the systems that we have here are strictly
optional, and we have fail-safes to get out
of everything.
So we really don't want to have some fixed
process where you say you can't do things.
Like we don't do things like readability reviews
or honors reviews, which is good and bad.
Like you can get two interns rubber stamping
each other's diffs.
What are you going to do?
But we think the opposite is worse -- like
we think it's worse to have people blocked
by process than to have people circumventing
process because we can fix that later; right?
We can find your bad diffs and back them out.
So on the actual committing code end that
would be very un-Facebooky and against our
culture to do something like that.
Where this does come in is in the release
process.
There's -- One of my peers is a guy named
Chucker who talked about how he's the only
person on the planet who has a thumbs down
button.
And so he can mark people as saying you've
asked for things in the release or you've
broken a release before, and this kind of
winds up being a decent trust metric for how
much I actually trust you.
And so if you break the release repeatedly,
you're likely not going to get your cherry
picks in, you're not going to get your fixes
in.
And so that's where a lot of that stuff comes
in.
But generally, yeah, we just depend on -- we
trust people.
We truss them to do the right thing.
For teams that are generally having problems,
like if you show up to incident review, you
know, more than N number of times and the
follow-up item has been you should really
write tests, people aren't terribly psyched
and then that gets taken care of other ways.
But, yeah, a lot of this stuff winds up after
code gets committed, not before they commit.
&amp;gt;&amp;gt;Alan Myrvold: So a great talk.
A question from the moderator.
How do you decide whether a failing test should
gate a release?
&amp;gt;&amp;gt;Roy Williams: Developers are responsible
for deciding whether or not they want to promote
their tests to being push blocking or not.
Yeah, so that's really the -- what -- So developers
have to decide whether or not they're push
blocking.
We do provide metrics so you can see the quality
of the tests, the flakiness of the test, how
many other machines does that test actually
wind up hitting.
And we use that to inform -- we, like, put
that day in front of the developer.
We let them know hey, this test is flaky.
It's been picked up by GreenWarden a few times.
Are you sure you want this to actually block
the release?
And then it's kind of peer pressure that takes
over; right?
Then it's up to -- if the release is being
held up by bad tests, other developers are
going to start getting annoyed and ask that
developer why they continue to have bad tests,
hold up the entire release.
&amp;gt;&amp;gt;&amp;gt; Great talk.
I noticed that the number of tests at Facebook
is increasing pretty quickly on those graphs,
and this must be expensive to find enough
horsepower to run those tests or all the tests
run really slowly.
So similar to the previous question about
catching false positives, how do you find
tests that don't provide any value and ones
you can weed out?
&amp;gt;&amp;gt;Roy Williams: Yeah, this is a really good
question.
This is something I'm actually thinking about
now.
There was a talk from somebody I don't see
here now, from Google about the core of the
problem is you're hiring more developers over
time, probably an exponentially growing rate,
and they're writing more tests.
So if you multiply those two together you
get something growing quadratically and this
is a problem.
This is something we need to think about.
We don't really have a good automated system
for handling this yet.
We went through, when we went through the
project green house phase and cleaned up a
lot of what we thought were low value tests
that never -- when they failed -- so the thing
we care about is when it fails, was it actually
a code problem or a test problem.
We don't have any good automated metrics for
this.
One of the metrics I heard thrown around externally
in the test community is tracking when a test
fails, what was the response of the developer.
Did they fix the test code or did they fix
the actual production code?
And if it's a test where the answer is always
to fix the test code, then likely it's not
actually having a value.
Likely, it's never catching a real issue.
It's just something people are limping along
and bringing with them for no apparent reason.
Yeah.
And we don't have things like that, but these
are the reasons why I want to engage with
the overall test community because they have
great ideas like that.
&amp;gt;&amp;gt;&amp;gt; Thanks.
&amp;gt;&amp;gt;&amp;gt; Hi.
Thanks for the great talk.
I have a question.
Since obviously avoiding (indiscernible) test
is really important.
This seems like a great way to do that, but
won't you also sort of miss flaky dev changes
that break things in a nondeterministic way
in production?
&amp;gt;&amp;gt;Roy Williams: So, yeah.
The flakiness isn't always the test fault;
right?
It could also be the underlying code.
It could be like the code that we're exercising
is UM'ing or the code we're exercising is
dependent on the CPU.
It's not always the tests fault.
Yeah, that's where we file the task with the
owner, and it has to be up to the owner to
decide what they care about.
And we don't really determine whether or not
the flakiness was because of the test or because
of the underlying code.
It's flakiness overall, and so we send it
back to the user -- or send it back to the
person who created the test.
And it's kind of hard to tease apart.
Like maybe you could look at the top frame
to see whether or not it was in the test code
or the actual code you're exercising, but
it's -- yeah, it's definitely a tricky problem.
&amp;gt;&amp;gt;&amp;gt; So most of the time people look at it
in time to kind of catch it when it's a nondeterministic
product failure as opposed to a test failure?
&amp;gt;&amp;gt;Roy Williams: Yeah.
If something is failing that will -- If something
is failing and they care to hold up the release,
it will hold up the release.
Yeah, most times, people take care of it.
Yeah, again, most times people want to do
the right thing so it's not a huge issue.
Cool.
&amp;gt;&amp;gt;Alan Myrvold: Another moderator question.
You guys can come up here, too.
I know some of you are in the room.
Are you really only talking about automated
UI tests like WebDriver?
And what do you do to reduce the number of
UI tests and still maintain your coverage?
&amp;gt;&amp;gt;Roy Williams: Yeah.
So UI tests are interesting in that they tend
to be the flakiness, so that's where this
stuff winds up being really useful, because
they're actually, like, hitting the end-to-end
system.
But we do have flaky unit tests, too.
We have flaky unit tests that will -- UM'ing
ones being one of the big ones.
We hit JVM crashes every now and again, funnily
enough.
So -- Yeah.
So UI tests is where this tends to be the
most valuable, or tends to catch the most
issues, but we also have unit tests as well.
And not just UI tests but integration tests
overall, like any test that's hitting the
end-to-end system.
Sorry; in terms of the other part of it was
driving the number of UI tests down?
I don't know if we're quite there yet.
We still encourage people to write unit tests.
We talk about the test pyramid, where you
want to have a good basic unit test -- I think
we talked about this in the beginning.
Good basic unit test, a few integration tests,
and very few end-to-end tests.
And it just winds up being like a developer
education thing.
I think a lot of times the reaction after
an incident is we should have had an end-to-end
test here which is really tempting to do and
then we just have to go back and think about
okay, where are the units we could have tested
here as well.
Cool.
Any other questions?
&amp;gt;&amp;gt;&amp;gt; I have a question regarding manual tests.
Do you believe that manual tests are needed?
&amp;gt;&amp;gt;Roy Williams: Man, that's a tough question.
So we do have some manual testers.
And, yeah, they're looking to catch really
-- like, can you log in, really bad things.
Personally, I think there's a lot of value
in not necessarily manual testers who exercise
the script because that's just like you didn't
get around to automating it or you need better
automation tools.
I'm a really big fan at manual testers who
are good at doing exploratory testing, are
good at figuring out where software can break.
So that's where I think there can be a lot
of value.
We haven't really invested in that too much,
but yeah.
So, yeah, I think there can be a lot of value.
It just has to be like the right thing.
&amp;gt;&amp;gt;&amp;gt; So, okay.
If you said yes, then my question would have
been then will that manual test follow the
same pattern as the automated tests or in
the?
&amp;gt;&amp;gt;Roy Williams: No, not -- So it's not in
-- it's not hooked up to our same number of
bots.
We still file tasks, we still have people
looking at flakiness.
Yeah, it's not really hooked up to the same
work flow.
&amp;gt;&amp;gt;&amp;gt; Okay.
Thank you.
&amp;gt;&amp;gt;&amp;gt; Hi.
Your product is highly visual.
How do you test for the experience part?
You open a page and it actually looks nice
and renders in a particular order and stuff
like that.
This is stuff I find hard to automate and
experience.
&amp;gt;&amp;gt;Roy Williams: So one of the things we talked
about at the Mobile at Scale conference is
on the iOS side, we rely heavily on snapshot
tests.
Or we use snapshot tests for those kinds of
things to make sure the layout is correct.
If you look at an app like paper or something
like that, it's really visual.
It's really -- it's not really a grid.
It has spring semantics and things like that
in it.
So it's not really a traditional application
in terms of a normal grid DOM layout.
And so there we on the iOS side we have snapshot
tests to catch that kind of stuff.
And we -- we have haven't really done much
in terms of snapshot testing on the end-to-end
side, and that just becomes we rely heavily
on dogfooding.
Like I -- you know, as soon as you join Facebook
you wind up using the release before it goes
out and so we catch a lot of issues that way.
&amp;gt;&amp;gt;&amp;gt; Yeah, because with snapshot what I find
is it's hard to catch like transition issues,
like when you go from one place to another,
it's --
&amp;gt;&amp;gt;Roy Williams: Yeah and for an app like paper
that's really important because it relies
a lot on pinching and zooming, or chat heads,
flinging them across the screen and making
sure that's smooth is tough.
Okay.
Cool.
If there aren't any more questions, we'll
hand it back to Sonal.
Cool, yeah.
&amp;gt;&amp;gt;&amp;gt; What do you do for internationalization?
I imagine most Facebook employees are good
at English but not too good at --
&amp;gt;&amp;gt;Roy Williams: So if you go to Facebook,
there's a really interesting application there.
It's called Translation Center or something
like that.
And so the cool thing there is if you are
in a -- one of the nonhead languages, you
can help translate Facebook.
So if you speak Afrikaans or a language like
that, you can look at the strings that we
have and help translate them into that language.
And that's how we scaled our translation to
-- I don't even know off the top of my head
how many languages it supports but I think
it's most of them.
Cool.
Sorry, one more question.
&amp;gt;&amp;gt;&amp;gt; Hey.
So how do you manage test users?
I guess -- test owners?
I guess the owner would be the one who wrote
the test always, but people tend to quit or
go on vacation a long time.
Is it always like that?
&amp;gt;&amp;gt;Roy Williams: The reason why I laugh is
recently we had a problem where a task got
assigned to Zuck because, you know, some various
people moved on and Zuck wound up being the
last reporting person in the company who was
in the hierarchy.
It was just kind of a funny incident.
So we try to have on-calls be responsible
for the test, not an actual individual.
And that's -- yeah.
So then the on-call usually is -- like people
who own the on-call are responsible for making
sure it's up-to-date.
And that usually winds up being okay.
People move things around, and if they get
a task that's not related to them, people
usually just bounce it to the correct person.
But that winds up being the big thing, just
have it actually be a list or a team responsible
for it, not an individual because, like you
said, individuals move, they change teams,
all sorts of fun stuff like that happens.
And if you don't do it, you'll wind up assigning
tasks to your CEO or someone else random because
it will track to see who left the company.
Cool.
&amp;gt;&amp;gt;&amp;gt; Thanks.
&amp;gt;&amp;gt;Roy Williams: One more question.
&amp;gt;&amp;gt;&amp;gt; Thanks for the talk.
So you had mentioned that you guys push out
updates to the clients twice a week.
So have you had pushback?
And the other thing is do you guys use like
the Google alpha beta channels for the rolling
out to the clients?
&amp;gt;&amp;gt;Roy Williams: Yeah, we use Google offline
beta just from the normal PlayStore.
It's been great.
It's really cool.
Yeah, so on Android, that's what we use.
We haven't really had too much pushback on
rolling every two weeks.
I mean, there's a lot of companies that are
at that same velocity.
So, yeah.
Like on the Android side of things, as long
as you're not changing permissions, it's usually
not a huge deal.
People don't notice.
And also Android has the incremental updates,
so that usually reduces the amount of bandwidth
that's required to get the update.
And then also that usually winds up being
the main concern, if you're using the live
bandwidth downloading.
We also shard the application to be as specific
to the device as we can, so like if you look
at the Play store, you'll get a different
APK like if you have an MDPI device versus
like an XXHTPI device.
Like there's no reason for you on a Samsung
Galaxy Neo to download super high resolution
images that you'll never be able to display.
Cool.
So I'll hand it back to Sonal.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you so much.
That was a really great talk.
Thank you for giving us a peek into Facebook's
testing world.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>