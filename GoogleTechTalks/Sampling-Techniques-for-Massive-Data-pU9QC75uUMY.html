<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sampling Techniques for Massive Data | Coder Coacher - Coaching Coders</title><meta content="Sampling Techniques for Massive Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sampling Techniques for Massive Data</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pU9QC75uUMY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello thank you for coming to the talk
it's my great great pleasure and I
really like to come to Google to give
this talk because a lot of my material
are I borrowed a good website some
content so and I have lots of questions
to ask so so now and the reason that
well here I'm sure everybody is well
very well motivated but I still like to
repeat some of the motivations like now
we call modern massive data age and this
people people like more data because
there's no data like more data and to
some people they go more extreme they
say well more dad has more important
than better algorithms so and that like
this is a what is a okay yeah so this is
the the paper from ACL and is for some
kind of machine learning tasks for
natural and natural language processing
they use up to a billion training
samples so this the order is millions so
use up to a billion training samples
they show that no matter what method
you're using for the same test sample
and the accuracy improves consistently
so this is the one with the motivation
that well let's use a mu data and as now
that we have more data we can fire lots
of people so just get more data so
that's a well but but today I'm talking
to talk about sampling so assume that
you have lots of data which is organized
by matrix suppose it's a matrix we in
reality may be hard to get into matrix
form but now let's assume it's in matrix
and both has n rows and P comments and
both N and D very large nothing the in
this kind of order so I'm going to talk
about like my goals are how to store the
data using very small space for example
users most sample and now the second
goal is to how to recover some summary
statistic like the original distances in
the data from the samples so these are
two goals are very they're correlated so
this step can be considered as an
encoding this stuff can be comes
decoding so the techniques I'm going to
present are called conditional random
sampling which is only going to be a lot
so a few million faces by the way so
conditional random sampling is only
going to be used for for sparse data and
another another technique which is
course stable random projections is very
popular technique and it's - in theory
so so just to give it the overview how
how the algorithm works there at least
give very very high-level overview so we
start with that we start with the sparse
data so now the first step is to do a
permutation with the columns so like for
example the first column becomes the
last kind of here after the permutation
so there are lots of efficient ways to
do it permutations so which basically
evolving one pass over the nonzero
entries of the data matrix so once it
was once you have done the permutation
there's a next step is well we don't
have to look at all the zeros so that so
the white Y so the white blocks are 0 so
we don't have to look at all the zeros
so let's only look at the non zeros but
I'll assume that we don't we can out for
to store all the non zeros so let's only
store a small fraction of the non zero
from the inverted index so this is a
basic step for conditional random
sampling it's incredibly simple and the
next algorithm is called stable random
projections which like we also start
with the matrix a but instead of instead
of doing the permutation of the colors
and we do a matrix multiplications with
the matrix R which is very skinny then
you get another skinny matrix B so if so
this is the whole idea of random
projections so the choice of what are
depends on what one norm you like to
work with so so suppose your goes to is
to only is to only use the l2 norm of
the data so you only need your sample
are from iid samples or normals
and the supposed to you goes to get the
l1 norm of the data so you sample from I
I do she and more generally if you want
the L alpha norm between zero and two
use some kind of stable distributions
and this works particularly well it
heavy-tailed data but it's not very
flexible because if you do the normal
random projection for l2 distances you
cannot use that way l1 you have to start
over so that's the disadvantage and it
does not take advantage of that as
positive the question is what is the D
so what I mean by distances in a that
distance a I mean the distance between
the rows next suppose every row is 1
data points it's a corner representative
a feature or dimensions so the distance
I mean the pairwise distance between the
rows thank you so sampling techniques
are going to be useful whenever your
goal is to provide approximate answers
very fast and user only versus more
memory space for example is going to be
useful for web search and databases and
data streams machine learning and is
parsing or processing as parsing on a
recovery and at the many many examples I
try to give some examples or the like
this is one of my favorite example so if
you input two words render and
projections in Google and well obviously
the web is so here you see the web is a
huge and nobody want the response on the
fly and it's in some sense it's critical
distort the data in the memory and I put
a question mark here because I'm
guessing there are two kind of sampling
algorithms here the first sampling
algorithm is for County the word
co-occurrences the second algorithm is
to remove duplicate documents and so I'm
just questioning so maybe people know
the exact answers I'm happy to know
because from outsiders that's so if you
trust me this to words means random and
projections so you see lots of papers at
least so lots of papers on this topic
but ignoring all of this you see the
first mine is the Google tells tells us
that amount of 10 billions of pages that
Google has a collective and they are
about 1 million pages contain words rent
and possessions very very quickly
impressive so so I'm guessing that this
number is from some kind of some kind
sampling because for example in 1990 in
2005 August that time when I was
Microsoft I was a very bored so I I did
some experiments my my experiments are
anak so if you type America so I get a
hits and I got by the way this was all
dated so I can hit that almost a fill
before four million then add another
words
China and then hit should be smaller as
we would expect then add another should
be smaller but now then at a Japan the
his actually getting larger and well
yeah we talked to some Google people
they they say well it's some kind of
sampling maybe maybe some parts maybe
break or oh I don't know the true reason
yeah maybe some different definitions of
what this numbers means so I'm very
interested to know what exactly what it
is so yeah so this is a sample algorithm
for magazines or sampling algorithm for
County work or crevices and so this is a
steal for the same same query words
random projections so they have 81
pagers if we click 281 pages people
don't use you to do that but if we do
see that now you know that you're not
good will only show you the unique pages
and also very quick so I I do not know
exactly how how it is done but usually
for removing duplicates and so usually
down by some kind of sampling technique
some kind of sketching techniques for
example this is a and reproduce very
well
work on duplicate remover the commune
was hashing so at that highs 1997 Andrew
what was working for Auto Vista maybe
still there and so yeah I'm talking
about out of Vista still there okay yeah
yeah I know but I was asking about out
of vista okay yeah people don't even
know so so at that time when I go out of
Vista has a thirty million pages and
injury and heating Hector Burnett almost
one third of them were nearly duplicates
and he developer of burdens and the
first step is to pass the webpage by
something coaching goes there's L
continuous worse so if L choose as three
so the sentence is nice they become a
set it's a it's a nice uh nest day the
reason is obvious why you wonder why one
continuous words the reason is because
the continuous was giving a lot more
power to discriminating the pages so now
once you once you have the
multiprocessor data now the similarity
between the two pages just become a
similarity between two sets and so the
user resemblance which is the
intersection the size of of the
intersection over the size of unions
like perfect reasonable measure of
similarities and instead of talking
about like boolean talking about
similarities it's equivalent you say to
talk about a vector similarity in high
dimensions because when the set when the
elements is in the set is there is 1
otherwise it is 0
and now the problem is that is on the
dimension is way too high because there
may be it's like a million single
English words and if you use elf laser
three shingles it becomes a tendril
eighteen dimensions and this is a dream
in high dimension and a very highly
highly sparse also so therefore next
thirty million pages so pass into enact
term document matrix or thirty million
in this dimensions so it's clearly
infeasible especially 1997
and the brother the developer meanwhile
hashing up wouldn't which we call
brother sketch and there's very
successful algorithm right away if you
search brother sketching Google you can
find our papers because we give the name
proto sketch and I hope they he doesn't
mind and so so this is a just like like
just just just to show that people has
done lots of work on removing duplicates
using sample algorithms I don't show you
exactly how this is done because many
people may be familiar with that and
well the text data the tests that are
usually organized between document
matrix and if we want to consider our
ABS absolute presence for simplicity
it's only a boolean matrix so the size
is very large maybe 10 billion documents
and it depends on how you define words
maybe maybe a million or maybe 10 to the
18th and obviously it's a highly sparse
and so therefore giving a vector like
matrix like this if you want to study
the similarities between the documents
you just take a dot product between the
Collins if you want similarity between
the words just take a dot product
between the between the rows and of
course we only need to store the non
zeros there's the Korver two index so
that the inverted index is nothing but
whether the locations of whether non
zeros are in the boolean case and that
the problem is still way too large for
computer memory for example google has
24 million pages in 1990 a and the
inverted index for the single words is
37 gigabytes and this the works probably
down weight before 1998 because that
paper was rejected by a by by CGI R or
something
Danika so accepted by cheaper W and
turns out to be very susceptible paper
so it's very stochastic in terms of
publications and now we have 10 billion
pages so it's a even only store the non
zeros may make hast lots of space and I
could skip this so
sampling and talking about sample need
to do County the world called cursors is
happening to help removing duplicates
and the sample can help reduce
logistical learning in high dimensional
data the problem where the high
dimensional data especially the data
size means too large is that you cannot
load the data into memory the problem
with that is what the training becomes
we're slow for example if you want to
fit mixtures of gaussians is very very
slow in high dimensions and also in a
lot of applications machinery in a
classification clustering multiple
dependent dimensional scanning usually
you only care about pairwise distances
but computing that a times a transpose
can be very very intensive and the
nearest neighbors just no tourists for
very computationally intensive and a lot
of kernels for support vector machine
are based on distances and which is a
considered one of the bottleneck in
support vector machines so if you do
sampling you can help all of these
procedures and well this is a one last
example I want to show it's kind of fun
too
it's not simply helping doing databases
for example data query optimizations so
suppose the goal is to join the four
giant side so it being a C and D and you
get a same answer no matter what order
you join but the customer very different
because especially when the
intermediates I our results is too large
so you have to write the intermediate
results to a disk and then load the disk
load the data back so it's a so the
order actually matters so the query
optimization they try to minimize the
intermediate results size of the
intermediate results I don't have data
base that hapa we have Google data so
this is not the example I found in that
in 2005 you also need to be very bored
to find this example so I wish you
recall Governator so example is that if
you want to find this in the
intersections of these four words like
Astra and governor's walk and a
terminator so what is the optimal order
well people say there's no brainer so
now let's start with the shortest -
that's the that's the that's the usually
it's a good guess and but for this
example that's not necessarily good
because if you start with the shortest -
which is shortened and Terminator
you got a half a million over two
intersections because she walk on a
terminator they're basically the same
person
but if you start with the show up and a
strand you got a lot a lot less a lot
less intersections so in this case
actually is good idea to start with the
show up in a nostril and but you don't
know this answer you don't know this
until you do it and if I you don't want
to do it because so that's why you want
you want to do some kind sampling and if
you keep a sample the data in the memory
then you can calculate this intersection
very quickly then it will help you to do
database query optimizations so I
finished all my motivation examples so
just repeat my problem statement is that
you have assumed that I have data matrix
and no matter what it is and the but the
bottom row both number number rows and
number of columns very large in the
order of million or billion or something
I don't know
and so I need to store the data uses
some more memory like a using a small
sample and then I need to recover the
original distances so the techniques are
passed for sparse data we have a
conditional random sampling for non
sparse data we have stable random
projections but before I present this
techniques people may ask what's wrong
with the conventional methods what's
traditional methods we should we call
random coordinates mpany so this is a
model recall what I caught condition
traditional methods or conventional
methods for sampling is that if you have
a huge matrix data matrix and you goes
the go to compute the distances between
the rows so say or let's let's around
early Pig comments and all random pig
components then used to then use a
sample to approximate the original data
matrix by simple scanning it so you pick
K columns out of these so your best so
the skinny factors B over K so this is a
perfect reasonable way is often the
first tire to do sampling that's obvious
right and there so you want to get this
random sample of the comments then you
can compute like any distance or like a
or two distances or l1 distances by
simple skinny you get estimates of the
originals and okay
any questions for this yeah it's very
simple and works for any norm so that's
that's good so people like it because
the same point works for any norm the
disadvantage is that the variance may be
a very large you some of the entries are
large if you rent and pay comments
you're gonna miss it
shingles or things like that yeah the
question is at the Commons that I assume
is something about the Commons
well I assumed that the distance you
want to compute a reasonable so so the
task is not to make make them my task is
not to make them a reasonable so I think
so
so you for the engineer present me a
data matrix they say it's a reasonable
data may choose to use and that they
need to compute the distances between
the rows so then then then then my
presentation is to how to compute the
distances efficiently but it's the it's
very important to make sure that
distance is I'm reasonable in the first
place yeah so yeah well I so so that's a
different task yeah thank you no that's
not but although for next stable random
projections they don't really care
whatever that you give me I can compute
so yeah so so so that's our two
disadvantages so larger variance means
if there's some big big entries you can
miss it and also that another
disadvantage is that you have lots of
zeros in the sparse data so that's
that's actually more serious of
heartburn because when when the data
sparse and your interest in the
intersection with sparse vectors you if
you randomly pick fix the columns you
don't really get anything so so that's a
problem and and a stable and condition
of understand funny actually works very
well on the sparse data so I just repeat
so you start with sparse data matrix you
do a random permutation in the comments
and there may be millions or ways or
many ways to do this random permutation
efficiently so what's it done a random
permutation you squeezed our order now
order zeros on look at among only look
at
Zero's which is called inverted index by
the you know stole all of them is still
a fraction of them and by the way it
doesn't have it doesn't have to be a
straight line cutter it can be any
zigzag form but this makes simple
the exposition yeah for the spot yeah so
the question is what I store
well first was for those boolean that I
only need to store the locations right
for the non point in that how you store
the location and the values
disproportionately yeah so the other the
acronym present is that it doesn't have
to be a straight line so you can ask you
a sample proportional to this hospice it
is but usually we care bus bus items
other than the a and these non sparse
terms right so sometimes it's a such a
good idea to send for two more
proportion from the sports terms and the
in the extreme case they can eat you can
you catch your sample do everything from
the sports from the pharmacy bosphoros
yeah so just giving example so for so
suppose we have what this boolean data
matrix and the app suppose is already
permitted can already permutate so I
don't need to do another matrix and of
course we only stored we only look at
the non zeros which is convert the index
so they're equivalent because the first
row means the location one four five are
non zeros so there you covenant and the
sketching algorithm the sketches are
nothing but the frame of the non zeros
so this is the green areas the green
area is where I call sketches now I will
only store the green areas so I don't
know anything about the rest of them so
now the so this reduces the storage but
now nest ASCII is a how to how to
recover the original distances or the
inner product or any distances it's
actually very very hard statistical
problem because yeah I haven't figured
out of the main I haven't figured the
exact answers yet but there's a trick
the trick is that because all they need
is a pairwise distances so so therefore
we can actually look at them one p at a
time we should make it much simpler so
there's a very simple trick that so now
you notice show my tray so I put the
screen area back to the original
permuting matrix so they are the same
right it's clear it's the two green
areas are the same clear right okay
now one thing we see that using a small
sketch correspond to many many comments
because all the non zeros are neglected
and the more sparse
the data the most at most parts the role
is and the more Collins they actually
represent so that's the that's the whole
idea
so so now if we agree that the both are
the same
and we are supposed our goals to compute
the distances between the first two rows
and that we don't have to look at the
older data so let's only look at the
first two rows so now it's kind of clear
that usually now this is a given that
this is the sketch in the green area and
we have to we have the choice of not
looking at this this element there's 11
elements if we don't look at it it's
equivalent to take the first 10 comments
over the original permuting matrix
because we just don't look at it so this
is me will take the minimum of the last
elements which is 10 here and we get a
sample size of the 10 columns as far as
this two rows are concerned so and I
claims a random sample of size of 10 why
is that because I do random permutation
in the beginning so therefore any chunk
is considered a random sample so in
particular the first ten columns is also
random sample now I'm using sketch the
size of a suggestion of size of five to
get a sample size of 10 Tikaram and
encoding the sample size of 10 and the
ratio is like a so it's a ratio T which
is a 10 I mean these are the 15 PS is 10
here so we get this ratio is so sampling
ratio and then we can estimate the
original for example in the products by
simple scaling so the sample inner
product is 2 y is 2 because 4 and a 7 in
the intersections so the sample
enterprise 2 and that original estimates
it's just simple scheming so which is
which which is very simple of course for
the for the boolean case and the
pairwise boolean case it's equivalent to
study the contingency tables like so for
sales if it's so a means those are
be means why the first row is one the
second was zero these both are zeros so
it's covenant learn to study the
contingency tables so we can actually
compute every cell of this contingency
tables from the sample tables or
pertaining by this trick so is that that
trick clearer okay yeah I know people
here very smart so yeah actually yeah it
took me like a whole summer to find this
trick I mean just stare and just to
stare at this green arrow green bars for
the whole summer
so at the end of the summer I found this
trick so retrospect it is a very simple
but now I say bonuses or conditional and
a sampling is that it doesn't have to be
restricted by a cure to pairwise for
example for for pure white amines only
consider two pairs like a wood one worth
three in this case the sample in the
product is three so it's a 1 4 and a 5
so for what Row 1 Row 3 and the sample
size is 6 Y is 6 because 11 a minimal of
ax and a 6 to 6 so it is 6 and it's very
simple so the reason that I called
conditional random sampling is that I
don't know the sample size until and do
the estimations but that doesn't mean in
the matter except it gives some trouble
you if one do a theoretical analysis
it's some kind of troublesome but
practically it doesn't matter and that
you can do this for three rows at one
time and at one time for example if we
one just in a product for between am
under one row 1 2 &amp;amp; 3 so the sample in
the product is 1 Y is only 1 because
only 4 is in the intercessions and it's
emphasized to your 6 because minimum of
this 3 numbers are 6 so it works for
motorways and as someone point out that
you can actually a sample more from the
from the less sparse ones because all
you care about this there's minimum
operations so it doesn't really matter
it doesn't have to be a rectangular
samples and doesn't have to be boolean
because it can store the values in
addition to the IDs
so you actually works for general data
and works for any norm and yeah because
just to get random cold in a sample so
you can to compute any norms and not
advantage is that because you did this
random permutations of the non-zeros you
know how many non-zeroes are there so
that means you know that you know that
you know the margins so once you know
the margins you can actually get a
better estimate so I'm a firm statistics
so making actually so it's kind of
interesting problem is that if we have a
sample have a contingency table ABCD you
want to ask you have a sample table and
they also know the margins so in this
case margins are number of non-zeros
every row suppose so this'll you know
the margins didn't have to get a better
estimate by solving mlu equations I just
showed this complicated equation just to
just impress people but we actually
proposed a very simple quadratic
approximations which it works as good at
this one and we can study the
theoretical variance is that if we're
using the margin and the vs. without
using the margin so the arrow or arrow
square in terms of earrings is always
less than 1 by could be less than like
pointy or somewhere something at that so
this different curve depends on
different values a B and C D so so the
algorithm is like has so the main
advantages are using the sparsity and
they actually also take advantage of the
margins yeah just you get an additional
storage right yeah so thanks yeah so the
summary of there is yes we just get
started but now summaries because it's
very simple it's as so summary of this
album that you do one random permutation
on the data matrix currents and it store
the front of the non-zeros which is
called sketches and if you reconstruct
the random corner in the samples
pairwise or group wise on the fly and
the small sketch correspond to many many
comments depend on sparsity and you can
do further statistics on top of the
conditional random sampling but not as
your data your algorithm or your
application only concerns one pair at a
time or a group of data at one time
which is often which is almost always
the case but now the the question is if
the data mark sparse the conditioner man
assembly is not going to help you you
actually work to the into actual work
but it doesn't help if it's the data
announce pass so therefore you want to
do in a conditional stable random
projections but before I go to that
algorithm that one I want I want to know
you for any any any confusions
particular role is sparse
haha you will be doing something
differently with it in the patching is
in our daily bias si is almost no
berries and but I need to be careful but
because like if you if you just the if
you just sample a few comments right if
your sample of your comments they need
to compute the estimators and then you
ask me the distances which is this is
not it's not unbiased because this is
just simple coordinate samples however
so pairwise is cannon on bias so and
conditional is unbiased but
unconditionally it's also a mayest if
you Mikan if my trick is a hole exactly
what you know is that could sure and
because in the extreme case when Ichiro
who only has one elements you may find
out this trick has a has a has a has a
bug in it but that doesn't matter
because you know everything if he only
has a one Ichiro has only one elements
then it up because you also keep track
of the margins you know everything about
the Ichiro's so you don't make any
mistake so so that's the case i don't
worry about and so it's the same simple
answer to your question is practically
there's no bias thank you
and so it seems like you're getting back
to what he was just saying if you do the
normalization off through Tory and then
you do this
normalisation you're doing
but it seems like you're assuming that
all of your data remains the raw data
and the difference is you have to make
normalization
the question is do I have to do
romanization well the answer is I don't
really care the algorithm they just
compute the distance for you yeah
regardless of what what better you
present to to the algorithms so so the
output as far the algorithm is concerned
it doesn't matter of course you should
have people yet the normalization that
if you do usually when you do the
normalization you can't you kind of
assume if snacker
using our two distances you already
assumed using our two distances so but
that does not really affect the
algorithm if we only well if you
consider the data or the raw data
yeah it doesn't affect the algorithms
yeah I think I know I mean because you
use node because a margin or me because
the normalization factor you so
normalizations kind of like is similar
to is similar to not computing the
margin so you say you do normalization
you actually know the margins sometimes
I'm on just one right here nigga yeah so
well we can talk about move about that
yeah thank you
just like the comics my friend then you
still have the triangle in your RTC if
you terrorize distances estimate
pairwise distances given various
maintenance of your sketches I don't the
question is do I have to us to have
triangle inequalities well if you just
select a Collins you do have triangle
inequalities and for this for this game
I don't know my guess is that strictly
speaking is not but practically
practically maybe may not may not may
not may not be matter because you know
the margins and you had to do some
restrictions on the neck hop on your
estimators so so so I just say
practically maybe not maybe not problem
and I haven't done any research on the
other on the magical properties on that
yeah thank you it's a a very good point
here so I say how much time we have
okay yeah I only need about ten minutes
so so good so stable render projections
more like theoretical or peening because
what the whole what they do is that they
do Mitchell notifications regarding this
what the data is and they have some
theorem to say something about the the
about the about the arrows they could
yeah Jo lemma Johnson in shows lemma so
you do the matrix of applications but
the matrix you multiplied with depends
on which norm you like to work with if
you like usually people like our two
norms so example from normal or normal
like and there's some often people like
l1 norm too because everyone knows more
robust so you stand by from Koshi but in
general example from stable
distributions and so for the l2 lone
case this can obvious why it works
because when R has sample from normal 0
1 so you take the it takes the the the
the pairwise inner products P times B
transpose takes expectations which is
the same as the a times a transpose up
to normalizing constant because this R
times R transpose takes expectation just
one I mean just identity matrix so it
works so normal and a projection
preserves all pairwise l2 distance in
the expectations but it's not obvious
for L 1 and L alpha exactly not true so
just in case people not familiar with
stable and put stable distributions so
stable distribution
they don't have expressed a density
functions an S is l1 or l2 and I you'll
always use s and the offer this is one
just scale parameter so they have the
Fourier transform of the density
function have this form which can be you
can take inverse Fourier transform if
alpha is 1 or 2 otherwise you have to
use a numerical simulations for the debt
density function so this gives the
trouble when you do the estimations
after the projections and the nice thing
about stable distribution is that it
trying to form the unknown data in the
stable data so for example so X has
stable de turbulence iid so C is any
constants so the way to the some of
which is a more stable distributions
they are also stable with
indexing parameter but the scale
parameter now does l alpha norm of the
data so if you all you care about the L
alpha norm so you just you do not need
the original data matrix
so for example in the air to case so
it's a very obvious because a way to the
salmon or most enormous with the
variance now is the square of every
every components and the l1 is also also
it's also true but now the scale Prem
there's a l1 is the l1 norm of the data
so after I do the random projections you
end up with the parameter estimation
problem of the for the scale parameter
in the l2 case which is very very simple
for the our two case and so let me just
skip this so for the for the normal case
which has been well very well studied
and this is a very famous lemma it's
called GL lemma is that if you if you do
the l2 projections and the you have a B
as you projection then you compute the
distance using the distance in B to
approximate distance in a and then you
get very good estimates but you need K
the number of projections which is
logarithm to the number of rows so this
is a very profound lemma and the people
liked it because in this case k is not
affected by D so seems that no matter
how large is the dimension is it doesn't
matter so this in a way that removes
removes the
curse of dimensionality of course you do
the horrible matrix multiplication in
the front and that the proof is actually
not not too complicated and recently
people like charcoal they prove the
impossibility result is that if you do
the L 1 projections there you cannot use
the L 1 distance in B to approximate the
L 1 distance in a yeah so this is the
impossibility results and well lots of
applications in database a to
approximate set intersections for for
computing from newest neighbor searching
in motion Ernie
they try to learn Mitchell Gaussian for
example in high dimensions the the
mixtures of gaussian high dimensions are
also mitchell gaussians in low
dimensions after the projections but low
dimension is much easier to work with
and all
so computationally you can use no more
and a projection to approximate the
grande matrix or the kernels so just
sure that so that there lots of very
good work has done for the normal case
now recent Oneiric Donohue and the
candles and Tao Tao is a recent
celebrity who wins or feels the price
last year they also work on this so so I
decided to do something different so I'm
actually working like after less than
two so but first people may ask why do
you want to work with L alpha norm other
than L 2 well the the answer is that
well you should win the data the
original data the raw data if you don't
transform the data or don't process the
data then we really have mini foyer two
moments so usually people do the they
take logarithm I'll take some power
transformations and people like to you
the L 1 norm because much more robust
and like C HIV this vapnik C's in Yahoo
I think HP is doing it AT&amp;amp;T when when
they they use SVM kernel of this form
which is the L alpha distances between
the data points they they turn the Alpha
parameters and the show that a
four-point fire that works the best for
no particular reason and Indic and maybe
some others this suggests using alpha
versus more to approximate Hamming
distances which is the basic which is
number not zeros because when alpha goes
very small this becomes the number of
non-zeros in this vector so their
applications so I'm not doing nonsense
things so for the for the coachee case
after the projections after the
projection for them if you only look at
the first two rows in a and the first
two rows in B then after the projections
you know the goal is to estimate the
scale parameter over the Koshi so so the
the problem is after the Cauchy random
projection the projected that I also
call she distributed but with different
scale parameter so now the goals they
asked me the scale parameter and and the
you file if
so the problem now is a statistical
problems is - you have K samples it was
the K projections and they want to ask
me the scale parameter but the sample
mean which is which is metric some
people care about is it match it does
not work because the co she does not
have fan expectations but the median
actually works and it's it's know in
statistics and also in computer science
recently but the median estimator
does not work too well it's not very
accurate and difficult to derive
theoretical bounds or things and
actually we suggest that geometric mean
in this form or this kind estimator
which works better more accurate and
much easier to analyze easier in terms
of deriving the make bounds like this so
we can actually get it explains your
tail bounds and without with all
constant explicitly given and therefore
from this bounds you can get it to sir
Jo temple Emma and then well this is
this is not contradicting the
impossibility results because this this
thing is not a metric like the the
geometric mean in two dimensions
negative this type of form they neglect
this so it's not a metric so does not
satisfy triangle inequality and well but
there's nothing else we can do
yeah because the impossibility results
so so I actually generalize this
estimators to two general alpha so
between 0 and 2 so this so this
estimated that the geometric mean
estimator also works but with a
different difficult crashing factor and
asymptotically has this form so and we
can actually study how good at this
estimator by studying the variance that
theoretical where asymptotic variance by
comparing the ML Yi the maximum
likelihood estimators because maximum
likelihood estimator is the best and I
it's difficult to do because of the
because of numerical problems and the
way to show that we actually showed that
this is geometric mean estimator work so
what sir very well when alpha is between
is a1 and phonetically 6 to 1 point 1 so
this value will go all the way to 1 can
only be one
so when alpha is between point six and
and one point one so it's not 80% as
good so it's a very good estimator but
in this stage I mean when alpha e to you
don't want to use that estimator in what
you want to of course you want to use
the arithmetic mean when alpha 2 2 &amp;amp; 1
at this point you want to use in a
harmonica me so it's very very
interesting in the middle you want to
use do match with me so I see people
bored so this is a can tell bounds for
people who care about the pair bounds we
actually get the tail bounce and this
constant and the reason I don't have
plot the constant instead of uniform is
that the constant would take a half of
the page and so that so that's all I
want to say about stable and projections
and I do a simple comparison for the two
matters conditional random sampling and
stable and projections so so compare
those two methods now people naturally
equipment asks which one works better
well when the data hiding sparse and
when the data boolean condition when a
sample works much better you can show
theoretically what buna data is nothing
but is also sparse data otherwise you're
not Boonen and so so this is a so this
is the comparisons for state conditional
random sample versus stable random
projections so in this case the
conditioner - Tampa is overwhelmingly
better than stable and projections so
this is the meanest NSF abstract data
and this numbers means for this data set
a computer so they're under points so
unsquare over two pairs of data
convinces sir compute estimate all the
distances and compare the condition of
and a sampling with stable random
projections so 80% of the time
conditioner and assembly works better
for the l1 and for the inner products
and also for they are two so so for this
case condition of an assembly works much
better because you dead how sparse so
Fernando Collor summary of the talk is
that we have lots of data
but people still want more and the
distance based methods are very popular
and are necessary to consider distances
in L alpha norm other than L 2 norms
because of the data heavy-tailed and
reducing the data storage size often
critical like in web search and exact
computation the distance is often not
necessary or feasible or - time comes to
me so if you do the random quote in a
sampling by taking random samples of
Commons it does not work too well
because that I have to tell our hardest
boss and a conditional random sampling
it's a simple trick which it takes
advantage of data sparsity but
generating the random code and the
sample pairwise on the fly and a stable
random projection works really well in
heavy-tailed data but does not take
advantage that is positive so this oh I
want to see say so thank you very much
so any questions
relax
that's prayer this is one yeah just so
happy if this is the question what what
happens with this well it's for this one
I assume that both conditional - Anthony
and the same grand projection take a ban
is a modest the margin l2 norm which is
done you need extra storage for that and
if you both came on n is the margins
then you do the then you can estimate
improve estimators but conditional and
send was much better
well yeah yes over on you yeah it's
almost one so this yeah this number
means condition nineteen ninety nine
point nine eight percent of the time
condition and a sampling works better
then stable random projections yeah
what's better
yeah depend on the data point so so that
these two matters all depends on those
paucity and the heavy tailed nests of
the data yeah so
packing sense
so this is a 99% of time is better right
so it's a practical this conditioner -
and is much better yeah this father 100
put the L 2 distances well for the l2
distance L if we don't take organism
organs so the L traditional associ
manner coordinate sampling is very
sensitive to the heck to the heavy
tennis of the data and this isn't that
sensitive in the air one case because
for in happy but have Motorola so we
have mother various so if we do the
random coding examples their variances
are affected by the for the L few cases
is affected by the fourth movements of
the data which is much much sensitive to
outliers but for the L one case is less
sensitive so therefore so therefore the
L 2 K so we show that much difference
however because in the L 2 K so we can
take advantage of margins you know the L
2 distance it can be decomposed as
margins and the end of the inner
products right so that's why it doesn't
really matter because yeah yes
okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>