<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hacking the brain by predicting the future and inverting... | Coder Coacher - Coaching Coders</title><meta content="Hacking the brain by predicting the future and inverting... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hacking the brain by predicting the future and inverting...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G7-pH-LMGRo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to what I'm going to be talking
about and glad so many people seem to be
interested in it at least so far curious
have any of you seen Jeff Hawkins talked
and talked here a while ago okay a few
he's a guy I worked with that red who
Neuroscience Institute it's better known
for having invented the Palm Pilot and
there's a certain degree of overlap and
what I'm talking about know what he did
I'm going to get way more technical and
mathematical but we have a lot of common
approaches so something might seem
familiar what I'm interested in is how
the brain works I think a lot of us are
the brain is arguably the coolest
mystery left in science um it's very
compelling because we each have one and
it's who we are and it's what we live
through but it's also a much more
approachable topic than you think of it
doesn't depend on quantum things or you
know gravitational physics or anything
its macroscopic thermodynamic it's in
the right scale to investigate it's just
electrochemical it's a bunch of circuit
elements really um it's cool if we
understand it because of who we are and
because it's got so many practical
applications if we figure out what it is
is which is going on inside our heads or
even inside the heads of Mike's or cats
then we will be a long way toward the
holy grail of real machine learning and
toward robotics you know being able to
hook up all the sensors in the compute
power and the actuators to be able to do
something coordinated and sensible so
this is a really cool thing to approach
and I think we're on the cusp of having
the grand unified theory the e equals MC
squared of brain algorithms and when we
get it I think it will be a generic
algorithm it's not going to be one
little piece for color vision in one
piece for binocular vision in one piece
for speech and one for music and so
forth it's going to be a single thing
which covers all of those a sort of a
generic algorithm probably the mother of
all algorithms in terms of its
usefulness and its impact and we have an
existence proof because we're all
carrying around something we are capable
of learning practically anything we
don't even really pay that much
attention to how we get our cues
from our visual system or our auditory
system or our balance and so forth we
seamlessly put them together and we can
fly fighter planes and write computer
programs and all kinds of other crazy
things which you wouldn't think of for a
mammal with a bunch of straight forward
nerves coming in and a bunch of other
motor nerves going out so there is
something cool going on I'd like to talk
about how to get there from here there
are really two questions people tend to
ask about how the brain works one is a
question of circuitry and one is a
question of algorithms the circuitry
part is by far the most traditional way
of doing theoretical neurobiology brain
science whatever what is a neuron how
our neurons connected how do they talk
to each other how do they send their
signals what is a group of them do arm
and what happens inside a neuron what's
its computation ion channels sodium
potassium dendrites second messengers
all kinds of electrochemical stuff
inside all of those together are the
traditional way of approaching it and
I'd say it's the wrong way of
approaching in great part because the
data is just so multifarious ambiguous
hard to interpret tangled nasty and just
not worth building a theory on it's
interesting stuff I spent almost all of
my academic career doing that um I
discovered some really cool things about
what happens inside neurons and I'll
talk your ear off if you want to talk
about how difficult it is to really
understand the brain from the circuit
level but for me that's not where I want
to push things um the other part is the
algorithms that's where I think
interesting stuff is going to happen and
that's where I spent you know my last
ten years as an algorithm hacker
programmer software engineer software
architect biophysicist things like that
and that's the mathematical part it's
not what the circum it's how what
mathematical operations do they do how
do they represent things store them
efficiently trance
information do the computation what's
the statistical inference going on is it
Bayesian is it quantized continuous
manifolds metrics all that kind of stuff
the mathematical part is the part which
doesn't have to wait on experiment it
doesn't have the same ambiguities as
experiment and it's something which if
you get it right has applications no
this afternoon rather than ten years
hence so I wanted to approach this more
as a algorithm problem with the three
pieces I'm only going to talk about one
of them but the three pieces of how to
approach the brain as an algorithm our
first of all what's the problem space
you don't hear that much about it but if
a brain is going to make sense of
sensory data of a million pixels coming
in or sound coming in at you know 10
kilohertz resolution or something
tactile from all over the body
proprioception whatever if the brain is
going to mend sets of this stuff what is
a pattern we say brain does patterns but
is a pattern a linear manifold and
nonlinear manifold what kind of
dimensionality does it have what kind of
continuation in time we need to
understand the input space before we
talk about what to deal with it second
question of course as a software
architect is what is the modularity of
this we can't solve it all in a go we
have to break it down into pieces so
what would be a module for a chunk of
the brain to deal with and what is the
API between modules how do you make sure
that they're robust when they're hooked
together that the whole thing is
scalable um so that's the connection
between modules which is a really good
question the piece I'm going to talk
about today is a subset of that it's
what happens inside a module you've got
one piece of the brain you know this
square centimeter of cerebral cortex say
what does it do with its inputs what
cues does it have to do inference what
are its outputs going to be how can it
stitch them together generalize properly
that kind of thing that's where I want
to get into some excruciating detail or
at least hacked them today but before
getting there one thing about the method
which distinguishes this approach from a
lot of academic approaches even though
I'm speaking of this as a mathematical
problem I don't think the answer is a
handful of equations in a paper
somewhere I don't think the answer is
some sort of architecture which looks
like the brain or some set of outputs
which looks like how brains work I don't
think looks like a sufficient I think
the answer is more like something which
works we need to build a system which
functions which does something proper
perceptually which generalizes in the
right ways and then we'll have a hunch
that we're on the right track because
nature can't let us do that unless we're
doing something right and of course
we'll have a working product whatever
that means um but in order to get there
in this kind of the nasty mother of all
algorithms involves understanding
something about algorithm development
algorithms are hacked their fiddly they
evolve they iterate they don't just come
out born whole on the page um there's a
process involved with the process which
involves sometimes open source involves
teamwork and benchmarks and revisions
and so forth um and I mean I shouldn't
be saying this to you google has
arguably the world's most sophisticated
and valuable algorithm and you know how
to develop it you know that it's more
than just page rank now um I don't know
how much more but I know it's more than
that and it involves questions of
sorting and scaling and clustering and
data access and representation and
storage making something work has a lot
of engineering questions in it all of
which are theoretically interesting and
that's the aspect of brain algorithm
development which I know is lurking in
the background even though I can only
talk about a tiny chunk of it now where
we are in this process is I think in
1903 um we don't have airplanes yet and
I'm not interested in writing a journal
article about the theory of airplanes
I'm interested in going to the sand
dunes of North Carolina and building one
I think that after you have built
something which gets off the ground
however poorly everybody can join the
team and make it better but what we need
is a reference implementation we don't
need a theory really so I will
I to hack my way through a little piece
of this and maybe convince you that even
if this particular hack isn't a very
good one maybe it's on the right path
there are lots of other people who have
done pieces of this I'm not going to go
into them Lauren Swiss Scott did a large
chunk i'll be talking about people like
geoff hinton have done some amazing work
in this field and with a lot of these
hierarchical modules over time rouwen
ballard have done some of the work for
temporal prediction on which they built
on something i did and i'm building on
stuff they did and so forth I'm not
going into name-dropping I just want to
see if we can focus on the content for
now um so let me see if I can make this
work let's start with a few simple
observations I'd like to just make the
claim at the level of mice and cats and
people that all cortex is alike this
cortex is the outside part of the brain
here it turns out that every chunk that
looks the same you know a mouse looks a
bit different from human but the part of
the human which does body awareness
looks the same as a part which does
vision or verbal memory or action
emotional planning speech recognition
it's all the same circuit and their
heart wired together in a reasonably
common way hierarchically some parts are
sort of low-level they deal with the raw
sensory inputs eyes ears and so forth
other layers stitch together some of
those the ones up at the top are sort of
the frontal cortex the part we claim
does the thinking and the planning
they're really wired up that way these
guys talk to each other in a stereotype
way just as they have stereotyped
internal structure um the other
generalization I'll make is essentially
all of this part of the brain all of the
cortex learns from scratch now of course
you know babies start out with no
coordination and they don't understand
speech and they can't really makes too
much sense of what's around them but
there are a couple other even more
powerful indications one is that in
someone whose eyesight is gone from
birth they've got this chunk of the
brain the occipital cortex which
normally in most of us does vision well
it's lying dormant these poor people
don't have vision
well what does it do it winds up doing
something else it not only deals with
acoustics it winds up dealing with a
high level of abstraction it does verbal
memory instead of low-level vision same
circuitry completely repurposed likewise
or maybe contrariwise if you find some
poor soul who at birth is missing the
left temporal lobe which usually does
speech they will speak some other part
of the brain will take over that task so
they're missing ten percent of their
tissue maybe they're kind of ten percent
worse on everything but it seems a
highly malleable nonspecific substance
whatever it is that these pieces of
cerebral cortex do so let me condense
this into a car sort of cartoon and then
I will do cartoons with in cartoons the
cartoon here is a hierarchy in which at
the bottom level you have a very high
dimensional input stream which is
changing pretty quickly each one of
these things think of it as a pixel but
it could equally be from the ears is
changing very fast no one of them is
constant even all of them together or
not constant or even slow so it's high
dimensional and high bandwidth and each
one of these modules I'm drawing in this
faction takes a bunch of
high-dimensional stuff and if compresses
it into a smaller dimension and now you
stitch together that first row and
you've got another high dimensional
signal which perhaps has correlations
and structure in it that now gets put
together by the same kind of algorithm
so you basically compress your data all
the way up a hierarchy until at the top
you have something which is in principle
less structured more independent more
representative of the outside world and
a kind of abstraction you know you've
thinking of a dog or a car rather than
pixel 27 um and it's something which is
a little more persistent Pixum 27 may go
on or off but presumably the dog hangs
around for a few seconds so you are
basically finding things which are more
which are invariant over space and over
time and that's the purpose of the whole
thing and it sends back signals so I've
drawn arrows in both directions so if
you have some idea of Bayesian prior
about what sort of thing one module is
going to expect that module can accept
that and perhaps expand it out to a more
particular representation and expand it
down so you've got a kind of a bayesian
belief tree here except that it's
running across time and it's having to
learn a lot of its own stuff but this is
sort of the the overarching view of a
sensory brain I am completely leaving a
hot aside motor motion I'm even leaving
aside the notion of surprise detection
and novelty just to make the scaffolding
where we can start to get something done
those other things matter but I think we
can do something without doing those
first today I want to talk about just
one piece of it what is inside a single
one of these modules now I'm calling
this my own notation i'm calling it a
compressor it compresses data it takes
something like pixels at the bottom and
puts out something like jpeg at the top
or takes in pixel streaming and puts out
mpeg-4 or it takes an audio and puts out
mp3 the trick is it has to learn by
itself you don't even get to tell it
whether it's going to be an mp3 recorder
or an mpeg-4 encoder it has to look at
its input streams figure out what the
patterns are figure out how to represent
them make the abstraction the compressed
representation and in the process it
needs to be able to go backwards to take
the compressed version and re-inflate it
into something which makes sense at the
high dimensional pixel level and that
last part that blue part is is what I'm
talking about today so the questions are
first of all letting pixels be a
stand-in for any high dimensional
sensory signal how do you turn the
pixels into the abstractions just how do
you do it second how do you learn to do
it without anybody telling you what
you're supposed to do um and most
importantly how can you take that
compressed version and reflate it again
without supervision so let me jump start
us with a technique which was published
a couple years ago I think it's
brilliant I did not invent it and I'm
not going to really tell you how it
works just take my word that it does
work it's called slow feature analysis
and what it does is it
looks at a bunch of different quickly
fluctuating high dimensional signals
such as pixels in an image and it looks
in the space of all second order
polynomials of those and tries to find
those polynomial functions which change
slowly so if your pixels are doing this
it tries to find a polynomial which does
this it involves principal component
analysis variance reduction minimum
eigenvectors and stuff like that I can
go into it later if you really want just
trust that it works because that's all
we really need it finds a handful of
cool polynomials as a function of a much
larger dimensional input space and these
polynomials tend to be calm or constant
over time than the individually
fluctuating inputs so that's what we're
starting with let's just assume that's
already worked those are the s functions
problem is and this is a very deep
problem using ultimately one of the
hardest problems in this branch of brain
science you can't invert that first of
all it's a bunch of arbitrary
polynomials you can't just run around
inverting these polynomials of umpteen
functions second it's ill-posed you've
got one function of a hundred inputs you
can't determine every one of those
hundred numbers from this one number
that's impossible and yet we need to do
it this whole hierarchy I'm talking
about requires that we invert the
unenviable we go from a low dimensional
signal and create a high-dimensional one
in general in the generic sense how are
we going to do that we hack or at least
I hack you can do the math I'll just
plug along I'll do it with a separate
map instead of trying to take the actual
polynomial and mathematically reduce it
which I don't know how to do I'm not
good with polynomials I'll just stitch
together a bunch of different clues and
tricks and hacks and whatever I can put
together somehow to try to best make it
work you know getting something which
works now is better than getting a grand
idea ten years from now so I want to
list essentially six different tricks
with six different clues each clue
inspires a little trick and put these
together and it seems to work it's not a
proof I've only tested it on one kind of
data set was a toy data set to start
with
but that's as far as I've gotten maybe
you can improve on it for me so this is
odd this is very odd because I don't see
the other part of this oh okay I think I
know what's going on um the I'm going to
start with a very simple example which
is just a two-dimensional case so you
know I illustrated a hundred pixels
going up to ten dimensions I can't graph
100 x 10 I can't graph a thousand
dimensional space I'm going to do the
absolute minimum which is a two
dimensional input space graphing up to
one dimension of output because I can
barely fit three dimensions on a slide I
hope you can grant me the literary
license for that because this way I at
least get to draw pictures and make them
understandable even if it's nowhere near
the complexity of the real problem so
let us suppose that we've got this
two-dimensional input space and we've
got something approximating a
one-dimensional motion in it you know
some little pixel is wandering or a
little somebody's little light
flashlight image is wandering around on
this too it's not even a pixel array
it's just two dimensions and you've got
something which is kind of going in
Proxima tlie a circle and that's our
input and we want to learn how to figure
out where the input is how the input is
changing how to predict what's going to
happen next um we basically want to make
a prediction and then somebody else can
use a prediction we're just going to try
to figure out how to learn the pattern
and how to predict the pattern so it's a
graph from two dimensions into one
dimension so here we are it has found it
according to this recipe somebody else
invented it has discovered sort of the
contours and is found there is this
function which you and I happen to know
is x squared plus y squared or x 1
square plus X 2 squared and that
function happens to be nearly constant
you know so even though x and y are
flailing about x squared plus y squared
is kind of on a circle that's good
that's what it's supposed to do so what
we have at this point is we have the map
up from two dimensions to one we have
the forward mapping we don't have the
backward mapping we don't know how to
find x and y given x squared plus y
squared and that's what we want so this
in a nutshell is what I'm trying to work
um so I've got the six different tricks
some clues about what we've got and how
to use them and let me just walk through
those so the first clue the input data
we have is not filling all possible
values it's on what we'll call a
manifold a locally low dimensional
subspace in this case let's call it
approximately a circle so most possible
data points are not used this is good
news we can use that so the way to use
that is to find some way of representing
those now we happen to know what's a
circle but in general it may not be a
circle it may be a pretzel it may be a
line or a triangle or something else so
since we can't designate the shape all
we know is something low dimensional
let's take a bunch of sample points the
red ones i'm going to call anchor points
and plop them around think of them as
cluster centers we're going to do a
cluster analysis when the input space
drop a bunch of clusters down i happen
to be fond of k media it's it's better
than k-means leaves out outliers better
so we find some cluster centers on the
input space and that's where we're going
to focus our attention okay this was a
first trick second trick kind of like
the first trick you notice it about in
the region of any one of these cluster
centers the data is still 0 dimensional
which means it is not a given fog in all
dimensions around that cluster Center
but it stretched out in one in this case
one dimension ideally just a few so how
do we represent that I would take the
covariance matrix the eigenvectors of
the covariance matrix of the data about
that cluster Center and notice that it's
stretched in a particular way um so I'll
just write those eigenvectors down
somewhere and that will help remind us
in what way the data is oriented about
each cluster Center so now we've got a
reasonable representation of the low
dimensional the locally low
dimensionality of our input we're
getting there clue number three um we've
got the smooth function it's a pollen
it's in this case a second-order
polynomial but it's not a really wrinkly
function nice thing about smooth
functions is our locally linear so
around any one of these cluster centers
week
do a reasonably good job of
approximating the function by saying its
linear Taylor series um so that's what
we'll do about each one of these things
will look at the polynomial will find
the gradients about however many
dimensions it has and will write them
down so we'll have calling S sub L it's
going to be the linear approximation
about that cluster Center um so we're
getting there so we not only know where
we care about the data we know something
generic about the function in the
neighborhood of each one of those points
halfway there were through clue three
through six let's go to four well we
can't go to four yet the good news is
that we have this locally linear
function and linear functions can be
inverted the polynomial we were talking
about can't that sucks um and if it were
an arbitrary function that can't either
the pump but but linear functions yeah
you can invert those just a matrix well
pseudo inverted anyway since it's going
from a low dimension to a higher
dimension so when you sudo invert it you
basically get the the best guess linear
inverse subject to like a mentor norm
constraint the problem here that is a
good news bad news story the bad news is
that when you get that pseudo inverse it
does indeed tell you a point on the high
dimensional manifold or the high
dimensional space which solves this
equation to solve this inference
equation it does not tell you a point on
the manifold you care about remember
this linear function doesn't know about
the fact that these these little points
are in this little stripe through the
red point the linear function just tries
to sudo invert and so it's going to aim
somewhere off to the side it's not going
to put a point back or you want it and
so we've got to get a hack around that
so this is where we use that covariance
matrix and if somebody's really clever
they can find out how this is a hack and
not the mathematically perfect thing to
do but I'm not going to tell you um the
the trick is we have the covariance
matrix which tells us how the manifold
is stretched where the data actually is
in the neighborhood of that red point
and so the trick here is first you get
what singular value decomposition tells
you you get the pseudo-inverse so you're
now down somewhere
error on the original data space and you
use that covariance matrix to stretch it
out so if the linear inverse gives you
that little horizontal green line there
at the left you stretch it and it's
going to become much more like the
diagonal green line on the right it's
not perfect but it's a pretty good job
and it's numerically fairly stable so
this is a way of at least approximately
getting your point back on to the
manifold you care about obviously you
have to throw in a normalization term
here so it doesn't get too far away but
that's not too hard so now we've got
away of actually starting with the low
dimensional thing and getting our way
back down this is good almost there's
one thing I left out another good news
bad news the good news is that trick I
just explained works of taking the
locally linear function going back down
to the manifold stretching that all
works if you know which point you're
starting with but we don't we just know
how high we are on that sheet we have in
this case one dimension of input we know
we are on this one dimensional space we
want to get in the two-dimensional space
there are lots of different points on
that one dimensional space and many of
them share the same value even though
they're far apart you can imagine like
below perhaps all three of those boxed
in anchor points happen to share the
same elevation on this hill they share
the same value of the number we're
starting with how do you know which one
to choose this is a big problem this is
it where it becomes an ill-posed problem
because there can be many such points
and you have to somehow choose which of
these red points to start with if you
have any hope of getting to the right
neighborhood when you're going backwards
so here's the clue um well first a
restatement of the problem we certainly
need one of these red anchor points
which shares the right low dimensional
numbers it has to be the right height on
this arm so we're starting with that so
that's that's necessary but not
sufficient but we somehow need something
to break break the logjam to break the
tie if there are many such candidates um
we need a hint of which ones to use and
the good news is the
we're dealing with some sort of input
space which is supposed to be continuous
which means whichever redpoint roughly
it means whichever red point we use last
time is probably close to the red point
we're going to use this time it may not
be the same but it's not going to be on
the other side of the circle so we've
got two complementary hints one which
tells you how high the red point ought
to be and the other tells you where it
ought to be in what neighborhood it
ought to be in XY space so we've got
even though neither one of these alone
is sufficient there at least sort of
orthogonal they're not the same thing so
maybe we can combine those hints sort of
do a best of both worlds approach a
compromise and maybe that will narrow
down which one of these red points to
start with the myths which linear
function to invert and so forth um okay
oh yeah this problem with the losses
get one of the problems okay here's a
digression very good one about one of
the problems with this method analysis
and so I'm borrowing this thing called
slow feature analysis which looks for
quickly changing things and find some
constant which or some not constant but
some less quickly changing function of
them now that's a very good idea because
in the long run you want to get away
from things which are changing quickly
and find things about the world which
are more constant but the extreme
version you don't want to find something
which is so constant as the hairy have
no information so that is not I would
say even a fault of his approach it's a
fault with our understanding of what
we're trying to accomplish here you know
you do not truly want to find things
which have no information contact
whatsoever you want to find things which
have a little information content which
is really useful like is this a dog or
not is it going to eat me or not but not
something which flails minute by minute
so that is an extremely deep question
and because it's a deep question I can't
answer it with an algorithmic answer
that's why I said at the beginning let's
just assume this thing is approximately
a circle I am literally trying to
sidestep the extremely deep question of
what constitutes a useful invariance
what can't what is the huge difference
between slowly changing and not change
involved and I'm just trying to give an
example which illustrates the process
here if you're dealing with real pixels
it isn't the problem nothing in the real
world is going to be that constant for
you um okay so does that answer your
questions that good night alright um
feel free to follow it up later there
are a lot of Pete questions buried in
this um okay a final clue trick pair
which is not essential to solving the
problem that is very useful in
evaluating your solution is you still
got a sense of where the data is
supposed to be and you've got a putative
data point and you can tell if you're
doing a good job in part by whether the
data point you get one of these green
stars whether that data point happens to
be close to the points you've gotten
historically and one way of doing that
is to take these lilies covariance
matrices which tell you roughly
speaking where the data is near to the
red circles and using it to create
Gaussian hyper lip sides which is
basically a way of making little
probability clouds and now you can tell
if the point you just got the one you
are claiming is your prediction of the
future if that point is in fact arm
inside one of these little probability
zones or not so it's a hacked way of
getting a kind of probability number as
a sanity check worst case you at least
know that you suck you know sucking
isn't that bad if you know you suck the
worst thing is if you suck and you don't
know you suck so this is kind of halfway
there and it helps correct the problem
um so here let me just walk through this
whole thing of the whole bunch of these
steps here is kind of a cartoon reminder
and then I can show you a very simple
test case so what we want is we want to
find the Greenstar we've got a history
of these the blast blew black spots they
seem to be moving in some pattern we
want to know where they're going to be
next we want t plus one which we have
not seen yet so the first thing you do
is you find this slow feature this
invariant or nearly invariant polynomial
of the exes and so now you've converted
it from a two-dimensional thing which
changes quickly to a one-dimensional
thing you hope changes less quickly as a
function of time now in that low
dimensional space you extrapolate so
because it's smooth you just hope that
you can just do some kind of a linear
extrapolation and guess my new value of
this invariant thing is going to be up
here that's the Greenstar down on number
two the one you expect to get now you
look over all of your your anchor points
in that space and you find out which
ones are close to that so down here in
the bottom all the dotted lines are all
the different values you could have you
know the one value you want so you look
at things whose values are near to that
and I've just at the bottom enumerated
candidates 116 3 and 7 so out of maybe
20 anchor points those are the ones
which are close in that space so just
write those down hang on to those those
few integers
now you go back to the loader meant to
the the space below the high dimensional
space and taking your most recent point
there as a hint you now look for the
other anchor points which are near to it
so this is the complementary part you're
looking at nearness in the input space
rather than nearness in the output space
you have a different list of candidates
let me just enumerate them eight three
five and 17 so now you've got two lists
in curly brackets this list from the
input space and the other list from the
output space which I've got in the table
the ones where the output space that's
on the left that was 116 three and six
and on the input space eight three five
and 17 and you do some kind of a best
compromise and say guess what three is
on both lists exactly how you do this is
just another hack I even forgot how I
did this one I think I just did
something we're not so much the some of
them are the same but it's like the sum
squared is minimum or something so you
basically try to find things which are
which are high on both lists and the
first thing which is high on both lists
wins so you take them both into account
somehow now you have an anchor point now
you know which red circle and it's all
home from there so you use that
particular red circle so you've got now
some input point which is close to the
red circle but not on it um or no you
don't have the input point you have
historical inputs but you have that
thing you try to find how far away from
the red circle you are up in an output
space you project it back with the
pseudo-inverse you project it back and
now stretch it out along the manifold so
it's now presumably where you've always
seen your data and then you take that
stretched bit and actually tack it on to
the where the red point is so it's now
stretched relative to their at redpoint
this is all taylor series basically so
we're now back to our input space except
now we have a prediction of a new point
which we've never seen before we think
this is where it's going to be based on
historically where we've seen that
points what our recent trend
extrapolation has been and then all this
pseudo inverse stuff this is basically
it that's what we've done um
and I wish I could say that I tested
this on real-time video images or
something like that but this research
prob I was working at an institute which
closed and this research problem got
just about to this point when the
Institute closed um so it's been sort of
staying in stasis for the last few
months but let me show you the amount i
was able to prove at least to my
satisfaction that the concept seems to
work um there are two parts to the test
the first is testing the part that I
didn't do and that's the slow feature
analysis does this system for finding
stable patterns in an unstable line of
inputs does that actually accomplish
anything sensible which means we need to
start out with an input space which is
simple enough that we know if it makes
sense if I was doing real-time images we
don't know what the answer is so it's
hard to check I want something better
than that I want to know what the answer
is so it's again an absurdly simple
space and it's pretty good for
illustration um the second part after
proving that I've learned the upward map
properly is to see whether the downward
map actually is a good method of
prediction and my answer is it's
ninety-five percent good so let me show
you that the first part look at the
bottom first on those kind of black
lines the input space is twenty
dimensional reasonably high more than
the two I showed you and all we've got
on there is blurs a little Gaussian blur
moving sideways with circular boundary
conditions so it goes off here and it
shows up over here and it just goes
round and round and round in one
direction and so it just keeps moving
sideways um the I think it's 3.3 third
of a pixel per step um and there are
three different widths so there are
three different patterns we can have a
narrow Gaussian or a medium Gaussian or
a wide Gaussian just like Goldilocks and
the three bears you've only got you know
big medium and small and they just keep
spinning around here um this is a truly
two-dimensional problem so even though
we've got 20 dimensions of input retinal
input the only underlying parameters
generating this are where it is that's
the one dimension side to side and how
wide it is so the true intrinsic
dimensionality of the problems
bases two dimensions and I want my
system to discover that two
dimensionality and if it does then I am
confident that the forward system worked
and I can worry about inverting it so
let's see how did these two graphs are
the two main dimensions that it found
the two main polynomials it found the
one on the top seems to correspond to
what because I was running the same time
and again and I changed the width of the
Gaussian and the top parameter has three
discrete values for the three discrete
widths of Gaussian I found now this top
parameter is a polynomial of twenty
inputs so it's got twenty inputs and
it's a second-order polynomial I do not
know what the coefficients on this are
but there are about 400 of them um so it
is it is not some puny little thing it's
actually a big ugly function um and but
whatever that polynomial is it seems to
be reasonably constant at three
reasonably different values for the
three different widths of the Gaussian
so in some sense it has discovered as
much as you could hope for about the
intrinsic nature of which one is net of
wideness it turns out i varied wideness
i didn't tell it it figured it out the
bottom graph seems to have discovered
the one other thing which changes slowly
and that is the position of this moving
Gaussian blur because you notice oh by
the way the the red points are the data
of the polynomial the green points are
the generating function so on the top
graph the three step functions of green
are the generating parameter I gave it
in Gaussian width which I think were
about you know fifty percent off each I
didn't scale it right I'm sorry um so
you can tell that the three on the top
graph the wire or the what the Gaussian
blur widths in red which are bouncing
around or at least changing at the same
time as the actual width of the Gaussian
given in green the bottom one the actual
position is again given in green but
it's now the sawtooth pattern and the
polynomial discovered something which is
a one-to-one correspondence to that
sawtooth pattern which is a sine wave
you wouldn't really expect the
polynomial to get a sawtooth it's hard
to get polynomials which changed is
continuously like that so I'm quite
happy with the sine wave because the
sine wave is something like telling you
the cosine of the position of this thing
so roughly speaking the red which came
out of the system are the two parameters
I learned out of the 20 parameters I
could have learned and they correspond
very well to the actual generating
parameters I built into the system um so
this is supposed to stand as if not
proof at least plausibility that the
system for discovering intrinsic slowly
varying features works that's part one
now I'm going to show that my system for
going backwards also works so that's the
next step does my little hacked up map
of these six different clues and tricks
and covariances and anchors and all that
does that actually learn to predict what
the next picture is going to be so this
system has just learned it's seeing a
bunch of Gaussian blur is moving by now
I'm setting it up to try to figure out
where these pixels are going to be next
based only on its lone own learning no
supervision or interference or hacking
of any kind um so let's see what happens
let me have restate the problem we now
have a lot of history of learning things
back and forth and we have a handful of
recent points recent points in this high
dimensional space which correspond to a
blur moving sideways and where those
question marks are is where we want the
system to come up with an answer we want
it to say where the white black is going
to be in place of those question marks
but what the success look like I mean is
it success relative to zeros or ones or
what I'm going to use the weather man's
prediction which is the the null
hypothesis is that you're merely
predicting the future is like today
you're merely taking as a reference
whatever your most recent actual data
was it would be really stupid to do
worse than that so I've gotten the
metric where you get a 1.0 if you merely
make as your prediction what your most
recent data point was anything below 1.0
is good so the error is going
to be normalized such that one is the
reference prediction and zero is
infinitely perfect and so who imagine
i'm going to show you a graph with a
bunch of things below one um here they
are reference prediction is a dotted
line some of the points are really bad
there are three times worse than the
reference prediction um most of them are
not most of them are at about five
percent six percent of the reference
prediction which is i would say 95% good
um and that's without using my little
trick of figuring out which point suck
and which points don't if you use that
probability estimate with the gaussian
hyper lip sides to figure out whether a
prediction makes sense based on your
priors you can now decide to exclude the
ones which on that basis don't look good
so this is not post hoc exclusion it's
not looking at whether it worked and
then saying oh no I'm going to throw out
a way to the side that's cheating it's
in advance deciding whether you're going
to throw it away and if you filter off
by the median there you've now got twice
as good so now it's 97% good instead of
ninety-four percent good which is as far
as I'm concerned for a first iteration
of a hacked up algorithm as much well as
I would expect an approximation to be
but again this is supposed to be a sort
of a wright brothers biplane to the
problem of learning data compression and
unfolding it again it is not the last
word I'm sure there are a million other
ways to improve on it so oh by the way
it wasn't tossing median I wasn't
tossing fifty percent i was only tossing
twenty percent so it's much better than
I thought how about that so here's his
sort of the punchline first of all it is
possible to not only find but to invert
these slow features from high
dimensional space they can be used to
make specific predictions in time
predictions of the future not merely
reconstructions of the present or the
past the prediction works well at the
ninety-five percent level and it can do
even better than that if you use
built-in confidence estimates which come
with the territory so that's really all
there is i started off this was kind of
a bait and switch i started off with
grand
Gio's proposals about how our brains
work and how great this will be for sort
of society and philosophy and stuff and
I've ended not even with mathematics but
with a string of mathematical and
programming hacks um but that's the way
i know how of getting from here to there
and i'm hoping maybe some other people
will have some better ideas or be able
to help out that's it thanks yeah motion
and time this model change the desperate
I mean really woman if I saw somebody
will be at office through that I've used
to sing then I would notice how does
your body let me try to repeat that the
second microphone how do you predict
motion in time because in the real world
you see things actually moving and you
sort of know where they're going to be
is that your question yeah but if things
move faster or slower than I expect
though I have a sense of time you'll
hear one of them I don't know how it all
very good oh this is for some wonderful
question um how do you predict motion
and time because he observes that this
model does not really have a sense of
time well in some sense it does when I
said you're extrapolating you take the
most recent values of the pixels and
then at the next time step you try to
put a slope through that so given the
most recent history you actually can in
this case you do make a prediction of
where the thing is going to be so in
that sense it does but I there's a
deeper sense in which you're absolutely
right and you've discovered one of the
very weak points of this whole thing I
told you that this slow feature
principle not the one which i'm
borrowing wholesale finds a polynomial
function of the inputs at one time and
it creates a polynomial which happens to
change slowly based on historical
statistics you're right that polynomial
is not a function of multiple time steps
it is a function of now only it happens
to be a similar polynomial to than the
one here and one here but the tall no
meal is not multiplying x of t times x
of t minus 1 times x of t minus 2 or
something it is not capable of non
linear interactions across time I would
regard that as a weak point and one of
the things I would like to do is fix it
an obvious way is to just expand the
dimensionality it's capable of learning
from to a handful of time steps at a
time so you're right this polynomial
system does not detect motion it the
polynomial system by itself has exactly
the same value whether you're moving to
the left or to the right and that's a
weak point but it's I think easily
hackable it's there's nothing intrinsic
about it I've fortunately my inverse
process does not depend on that
particular trick of the polynomials my
inverse process a pretty generic one um
so I'm I'm impressed I'm glad you
reminded me of that because i forgot to
say it
but yeah so a lot of the problem you
decide to attack is really good right
inverting the thing is really important
the inversion doesn't always happen when
you're observing time digging what's
about to happen the brain you also need
to invert the typical stop but you never
see the top down or some higher profit
so if I tell you it looked it up your
mother you know that is up the auditory
cortex the fact a visual cortex with no
prior fish of your mother that you
wonder about the match is only a slight
chain so I mean is that outside
developers hope of what you've done how
would you have a presented at least the
ones that explicitly made use of the
previous point although I didn't seem
like you could is able to reply other
ways to that stuff the typical fire
they're just the most echo
some experiments check is there any
chance I can get to this question before
we expand it but it's a very good
question and I want to focus only before
because I bike yet otherwise than wants
of it it's an extremely good point and I
just want to get my look at that um to
rephrase the point in case Mike's didn't
pick it up in case I misunderstood it um
the system I've outlined makes its
prediction both by appealing to the
abstraction the sort of the low
dimensional version up here but also by
appealing to the recent past in the high
dimensional space in the real brain or
in a real machine learning system you
may not have access to a reliable form
of the recent past you may not have
access to a high-dimensional hint you
may only have the abstraction to work
from is that a fair condensation of it
absolutely true um part of my plea to
this is that I was trying to only work
of what happens within one module so I
was explicitly not talking about how
modules talk to each other and how one
of these compressor modules would take
advantage of some prior would have
certainly in an actual brain including
the hierarchical thing I outlined these
guys are talking to each other all the
time each one of these things is not
only looking at its most recent version
of the abstract variable and the past
it's also looking at all the votes
coming down raining on it from above
from all the people at a higher abstract
level who are now giving him their
opinions their Bayesian priors of what
that abstract variable is going to be so
the good news is that you have a much
better version in such a system like a
real brain it by us in favor of a
picture of your mother as opposed to
george bush or bicycle um the bad news
is that since you don't know how any of
the pixels are lined up you don't have
the high dimensional hint so first of
all I can't claim I've solved that I've
got some ideas on it but you can see
that at least you've got half the hints
if you've got some prediction of the
system it won't give you the exact
specific prediction if you know you're
going to see a picture of your mother
but you don't know where it's going to
be in pixel space obviously
you can't say you know which pixels are
going to be black and white however you
are now primed to much more quickly
distinguish whether it's your mother or
not because you have now in some sense
restricted your search space to those
which are consistent with mother versus
those which are not I know that's a kind
of a hand wavy approach but does it at
least address your question even if it
doesn't answer it okay okay all right
that's as good as I think I can do with
that question now but it is a very good
yeah oh okay cool also I while answering
the question which wasn't asked better
than I answer the question which wasn't
lucky yes so going back to the question
of computing the English I really like
your Anatomy oh you come here the
function and that sort of you have it
and then you need to reinvest it and you
need to discover this country the
question is how is that a movie so if
you take the example of you drawing a
circle my looking at and my building a
mental model of a cell phone and then I
copy yourself for something simple like
a circle I'm a perfidious a perfect copy
but if you draw a picture of a dog the
picture of a dog that I draw might look
significant need is a good up okay so
how at what level how accurate does that
inverse have to be the inverse function
that much our brain is computer all that
the inverse real letsa go to rephrase a
question um I've drawn a very simple
mapping system from very few dimensions
up to even fewer and back again so you
can reasonably expect that the two are
going to line up fairly well but if the
problem is much more unconstrained much
higher dimension how can you tell
whether you're doing a good job and how
can you even expect that you might be
doing a good job is that a fair how good
probably to be able to really
effectively about the picture let's see
how good does the inverse need to be in
order for the brain to reason
effectively about the picture being a
dog or something like that um it depends
on which I'm of course dealing at the
level that might apply for a mouse so
when I say reason I'm really thinking of
just being able to fill in the blanks
and kind of recognize what it is but the
short answer is to the degree that you
have sufficiently narrowed the
probability space to the degree that you
now have a prior which is going to help
you quickly figure out whether this is a
predator or not you know if you've
weeded out ninety percent of the
possibilities your principal ninety
percent fat or you know nine times
faster at evading the dog something like
that rather than having to sit down and
wait and gather more data to figure out
if it really is gone so any improvement
is good relative to a baseline of
nothing so it's not a system which will
presumably catastrophic we fail if
you've got more error than you're
accustomed to having it's merely going
to have to gather a bit more data or
process it a bit more but then again I'm
completely leaving aside from here the
questions of how these different modules
in the hierarchy work and whether this
system has now got fifty percent of a
dog so it passes its fifty percent up
here in this system is passes it fifty
percent they go up and now you've got a
hundred percent and that guy goes now
these guys are at 70 and now these are
at 90 or something you can imagine
although I'm not specifically outlining
you can imagine a system in which the
reverberation up and down is a settling
process so even though each individual
piece has very noisy data and by itself
does not have a very good estimate each
one is iteratively improving the
estimate passing it on to other levels
which have access to more information
for other places and that collectively
they might be able to settle toward or
zero in on an answer that would
hopeless for any one of those modules
independent okay i'm seeing a time
marked here so it looks like i've i've
run out of questions just as they're
getting hard takes an awful lot for your
time and I hope you guys solve this one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>