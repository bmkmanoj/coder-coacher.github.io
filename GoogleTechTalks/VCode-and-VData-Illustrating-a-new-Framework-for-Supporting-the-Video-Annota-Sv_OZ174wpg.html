<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>VCode and VData: Illustrating a new Framework for Supporting the Video Annota... | Coder Coacher - Coaching Coders</title><meta content="VCode and VData: Illustrating a new Framework for Supporting the Video Annota... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>VCode and VData: Illustrating a new Framework for Supporting the Video Annota...</b></h2><h5 class="post__date">2008-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Sv_OZ174wpg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon thank you for coming
my talk today my name is Joshua Halpern
I'm from the University of Illinois at
urbana-champaign and I'm an intern here
at Google and today I'm going to be
talking to you about video annotation
the current uses of video annotation the
state of software and techniques that
are currently in the research
environment and a framework that we
researched and designed at Illinois to
help facilitate the video annotation
process using technology and then a set
of tools V code and V data that we built
to demonstrate the feasibility of such a
toolkit
following this framework but before I
can even really talk about what video
annotations are and how they're used we
sort of need to take a step backwards
and start thinking about the study of
human behavior if we go back hundreds of
years people have been studying human
behavior in the form of anthropologists
during the time of the West exploring
and undercovering the world around them
they sent out anthropologists to
different tribes different communities
and cities to immerse themselves to find
out how the different cultures work to
understand the roles of different people
in those societies and these
anthropologists would take detailed
notes in their sketchbooks they would
draw pictures and then write about these
these experiences they that they had to
help give other people an understanding
of these different cultures these
different communities and these
different people all over the world
people and behavioral sciences have also
studied human behavior for hundreds of
years we can look at people in speech
pathology special education even the
psychology v'n psychology people have
looked and studied the way humans
interact and used note-taking to try to
understand and convey the behavior of
different people to others who are not
there at the time at the nog rafi has
also been a vital tool in understanding
human behavior in the workplace death
Onaga furs have gone into work
environments to understand the workflow
how where there are shortcomings where
there are problems where errors can
occur and suggest ways to improve those
work environments one paper that that
comes to mind is the paper of McKay
where he he looked at the the roll of
paper in airplane air-traffic
controllers
and how paper was used as a redundant
tool to help prevent planes from
crashing in the sky and so that all the
different controllers could be aware of
what each other are doing now the role
the study of human behavior has also
come to computer science in the study of
how people interact with software and
webpages many many universities and also
many companies have start to employ
people in human-computer interaction CSC
W to come and work with them to try to
understand how to make tools more usable
provide the features and interfaces that
individuals want to interact with to
allow them to complete their jobs faster
and also collaborate with each other now
the normal process in these in these
domains exists by a researcher conducts
conducting an experiment whether that is
going out into the field or in a
situation at a university or maybe here
at Google you would take some
individuals bring them into a room and
videotape a session coders would then
and use video to perform some kind of
analysis on that video making marks
about when different types of events
occur the now I'm talking here about
annotations the difference between an
annotation and person per say a
transcription is a transcription is what
a stenographer does in a courtroom
they're writing down the different words
that are being said something that could
show up as subtitles whereas annotations
is trying to capture specific marks and
vents that occurred throughout a session
or an experiment they would then perform
some type of check for reliability to
make sure that the marks that one person
made actually occurred during that
session and then you would hopefully try
to publish that data in a reputable
journal or conference
now the role of video has evolved over
time and rather if we look back at early
types of study of human behavior and
making those types of annotations it's
very different than what we find
ourselves in today at the beginning
people were using paper think about the
anthropologists from hundreds of years
ago who would go out all they had was a
pad of paper a notebook and a pen or a
pencil and they would make these marks
and make sketches and then based off of
the trust and strength of the the
observation that they made they would be
able to convey their observations to
their peers in the scientific community
with the advent of video this
revolutionized the way that people
approach approach annotations and
understanding human behavior what they
were able to do is now videotape a
session and show it to every other peer
researcher that they had out there so
now everyone could actually see the same
events that we're going on if you
provided timestamps for this video then
you can actually have people mark when
events occur at a specific time and then
compare those marks to see what the
agreement of how many of these events
whether it's time somebody smiled time
somebody looked at a computer screen
time somebody said a specific word and
they can actually count how often they'd
agree that these events occur now with
VCRs that moved the entire world of
looking at video a hundred light years
forward because suddenly it became
really easy for you to pause video
rewind fast forward slowed it down it
made it much easier for people to do
multiple passes consider the the older
way which was using a video rule using
celluloid film you would have to rewind
the film slow it up play it again this
was a very difficult process but with
VCRs things became much easier in
addition VCRs had actual timestamps that
were actually displayed now on LED
screens of when what time in the video
things occurred so suddenly it was not
an extra effort to actually include
timestamps with the video suddenly we're
now able to look directly at the VCR the
playing device and look at and actually
be able to say aha this person smiled at
five minutes 43 seconds into the video
now when the advent of spreadsheets
digital spreadsheets came out this moved
everything forward once again whereas
before people had to rely on pen and
paper to make these notes and that was
one copy now people could make these
these annotations in a spreadsheet in a
log file where they could put dates they
could put the type of event that they
observed and maybe even some comments
suddenly backup was made much easier and
some of them the basic things that came
out come out of using digital technology
like copy paste undo made their lives
much easier
imagine having to write the word smile
thousands of times over a video
well what I'm save you could just press
paste every single time well that just
makes everything to move faster makes
them that much more easy but when for
checking for reliability for to see
whether or not two people agree that
events occur but people still had to
print out these reams of paper with all
of these notes and still go sit down in
a room and go line by line to see on
whether or not people agreed that a
certain event happened at a specific
time computerized systems is the wave of
the future it is what we are all
striving to get to because if you think
about it
computers have this amazing ability to
shortcut a lot of these problems that
we're currently going on by using paper
and VCRs by using computerized systems
we can present synchronous secondary
data so imagine if we had let's just say
the volume if we're looking at the the
way people are yelling or talking in a
room we could present a waveform to
discuss to show somebody the volume
that's going on in a conversation or if
we're looking at a technological domain
we could provide secondary data recorded
or logged by let's say eclipse when
people are coding you can then take that
logged Eclipse data of interaction with
button presses or key strokes and
present that to the user so suddenly not
only do they have the video but they
also have this secondary data that they
can also make understanding and make
conclusions based off of so how many
times when people click a button is
somebody actually smiling so now we can
look at the smiles and the button
presses together in addition research
has shown that by presenting this
secondary data the quality of the result
is higher the speed that people are able
to mend the confidence that they're able
to make for these marks is that much
higher as well in addition can computers
provide a control mechanism for us to do
easy play pause rewind some of the basic
things that come together when you're
using a VCR but we can now get it much
more accurate with a VCR you need to
toggle with you know maybe hitting the
spacebar or the the pause key at the
right time or slowly go forward one
frame or at a time with computers it's
become very easy for you to skip one
frame forward one frame back in addition
computers have the potential to take
care of these reliability calculations
for us consider when you're using paper
somebody has to go line by line to see
whether or not you agree well if this is
all digitally stored data the computer
can all read automatically calculate
well that you agree 40% of the time or
90% of the time as we would hope for in
addition computer streams also allow us
to do multiple streams of data multiple
video streams so whereas in the
traditional VCR mode you would either
have to have a lot of monitors synced up
at the same time or pre process the
video so that you had two different
video streams going at the same time on
the screen with the computer you can
take multiple let's say QuickTime files
sync them up together and then alternate
between them computers have an amazing
potential to increase the quality and
speed of this process so I now would
like to talk about some of the existing
software tools that are out there to
facilitate this video annotation process
now I'm not going to give a full
description of all the tools or all the
different features that they have this
is just an overview and what I would
like to discuss is some of the trouble
spots that these existing tools have
have run aground on the first trouble
spot is the use of the timeline now the
timeline is a common tool for video
editing if you look at video editing
software from iMovie Final Cut Pro they
all have a timeline it's an easy way
that you can show different elements in
a chronological fashion as you can see
in the first example on the Left Vacca
the timeline is condensed where all the
different marks the annotations all
exist on one uniform timeline now this
is
great for us to get an understanding of
seeing the relationship between those
different marks it's great for screen
real estate but if you've got many
different types of annotations possibly
occurring at the same time it gets very
confusing to actually differentiate
between them now visita or mac visito
depending on which operating system
you're running is an alternative
solution that goes in the exact opposite
direction they say every single
annotation line every single type of
variable that you might want to look at
should occur on a separate line now this
is really great for us seeing the trends
that occur within one one specific
variable however if you've got anything
more than let's say 15 variables you're
gonna have to start to scroll up and
down and the relationship between
different different variables measured
will be very hard to tell yes it might
be easy for the variable right above or
right below that current track that
you're looking at but when you start
looking for variables away five
variables away
can you really easily compare the the
marks at the top of that line to the
marks at the very bottom of that window
it's very very difficult another problem
that many tools have is an overly
complex interface this is a naja less
observer this is the what's considered
like the premiere commercial grade video
annotation tool that's out there as you
can tell there are a lot of windows up
there that somebody who's making these
video annotations have to have to
mitigate but the question that I
immediately have is where are the
annotation marks turns out it is that
top of that window in the top right and
they use video annotation marks as a
list instead of viewing them on a
timeline which makes it very difficult
for us to see the relationship between
the chronological events in the video
and the actual marks or annotations that
need to occur now they did a great job
here by having multiple video streams
that solves a lot of our problems but
there are so many windows you become
overwhelmed with all the possible
options that you have another downside
of the nahji list system is that the
admin windows the functionality that
allows you to set the configuration for
the video that you're annotating is
right up there coexisting with the mark
with the the areas that the annotator
has to go through to make their marks
now this is a big big problem because if
you have if a researcher has set up a
set of variable
that you want to look at you want to
make sure that all of the annotators are
looking at the exact same set of
variable so if somebody tweaks that
control that admin feature in the main
window that can have detrimental effects
to the data that you're trying to
collect one of the best tools that's out
there that's for free is called anvil
animal has done a really nice job with
many aspects of their interface as you
can see here but they fall into the same
trap that all of the tools that I have
discussed have fallen into which is
actually meeting the needs of the
researchers if you actually spoke to
researchers who all actively use video
data and video annotation there's some
major problems that all of these tools
have run into and we call this sort of
flow the sort of organization this
process at these video and that these
researchers use the video annotation
workflow so step one is collecting the
video that's very simple you run a
research study you set up a video camera
and you record it part two is creating
the segments of the video to code and
also creating the list of variables that
you want to annotate the dependent
variables if you will
number three is training the coders now
this is a critical part of the process
you need to make sure that all of the
video annotators that you are using to
gather data all I have the same mindset
of what the different variables means
for example let's let's say that we're
looking at smiles how many times
somebody smile is when they're using a
piece of software that we wrote well
what is a smile I might view a smile as
you know I've got my teeth showing and
my cheeks are really high and you know
I'm I'm give you like a schoolgirl
but somebody else might say well you
know if you're if you the corner of your
mouth is just slightly raised I'm going
to consider that a smile well that's a
huge spectrum and everything in between
so you need to make sure that all of the
different video annotators have the same
understanding of what those variables
are and you can do that by checking the
reliability showing them both the set of
video asking them to annotate a certain
variable and seeing how many times they
agree and if they've got a certain
percent agreement you can say that
they're reliable they have an agreement
level that is reliable so that I can
assume that the data that they produced
is actually the events that are
occurring
now that reliability variable is usually
set somewhere between 75 and 80 percent
agreement for my own personal research I
shoot for 85 so that when there's
fluctuations they can fluctuate to about
80 so that you can still have a fairly
reliable set of data
number four is gathering the data this
is after you've demonstrated a
reliability across all your variables
you sort of release your coders to to
annotate all the videos that they can
and that immediately leads into number
five which is the weekly reliability
sessions every week researchers come
together with their video annotators
have a video that they boat that all the
annotators have looked at together and
checks and make sure that the
reliability is still there we want to
make sure during the entire research
process the entire annotation process
that we are still having a certain
degree of reliability in the data that
we produce and when there's a problem
when let's say a variable is that 50
percent agreement we want to go back to
the video and look at and see how these
where these differences and
discrepancies occur and lastly we want
to perform data analysis so after all of
the data has been collected we want to
be able to look at this data and figure
out trends and actually perform the the
research analysis that would get
published now if we look at the existing
tools that are out there basically they
gather data and they're pretty good at
that but when it comes to doing anything
else they're very very difficult now we
can probably give them one and two we
can probably say well yes these tools
allow an easy mechanism for you to
import video that has already been
collected and maybe if you've got
secondary data some of the tools allow
you to import that but they have no
methods for you to train coders to
demonstrate reliability and they kind of
deal with allowing you to export the
data to a secondary piece of software
but what you're basically left with
right now for doing this reliability is
still exporting the data to a text file
printing out the text file and going
line by line with the people in a room
trying to figure out where these
discrepancies occur and much like the
situation with the VCR if we find a
discrepancy we now need to queue up a
videotape find out exactly the timestamp
that both you and I were looking at and
see and determine who is correct
so based off of this in discussions with
researchers who actively use video
annotation we came up with a set of
requirements of what an ideal video
annotation tool should really do to
fully facilitate research step one is
facilitating the coding workflow which I
just discussed
number two is that the video the
annotations and the guidelines the
descriptions of the variables should be
completely in sync because we're dealing
in a digital domain this should be
really easy we should be able to go and
look from the video directly to the
marks that were made about that video to
the description of what those variables
are and go back and forth really easily
capturing appropriate data is something
that a lot of tools actually already do
this means that sometimes we're looking
at events let's say every time somebody
says the word banana that's an event it
occurs and then it's over so that's a
momentary event but then we have
something let's say that is how often is
somebody looking at the screen and for
how long
well that's a ranged event there's a
start and a stop with a stopping point
so we should allow whatever tool that we
build to have events that are momentary
and ranged events but in addition we can
also say well there are some times where
we want to let's say look at the
language that somebody is using so
somebody who's in speech pathology wants
to look at the phonemes that are being
produced in terms of their sound or you
want to perhaps put a rating so you say
well we're looking at smiles but I'm
gonna put a 1 value if it's a you know
just your the corner of your cheeks
going up and I'm gonna give it a 10 if
your teeth are showing and you're giddy
like a schoolgirl
so what if you wanted to put those kind
of ranking systems and associate those
with your marks so you need to
facilitate comments and these ranking
systems ranged events and momentary
events
as I said earlier secondary data should
be able to be displayed this improves
reliability and quality of the results
many of the tools that I'd already
discussed do have a form of showing
secondary data but this is a critical
part in the digital world that we're
working in today this one
this requirement allowing multiple forms
of playback is one requirement that is
not met by any of the tools out there
through conversations with researchers
that we that we had and with coders we
discovered that it's not enough to just
simply play a video from start to end
pause and rewind and fast-forward we
found out that sometimes there's
variables that are hard to sort of say
when a variable actually starts and ends
for example let's go back to that smile
example when does a smile actually start
do your lips have to move a certain
degree to your cheeks have to be raised
a certain amount it's like so many
inches so many millimeters so what
they've come up with is a system where
you play a video for let's say three
seconds maybe five seconds and then it's
then you pause it then you say in the
past three or five seconds did this
event occur and then you make a mark and
then you watch the next three or five
seconds well with the VCR this is very
troublesome because I'm gonna try to
pause three seconds later but I might
not be exactly right let's say I'm on
half a second off and as I continue to
get half a second off with every single
three-second increment and by the time I
get to an end of let's say a two-minute
video well that's a major problem now
the sections that I'm looking at in the
sections of my coding partner are
looking at are going to be completely
different so we should be able to
facilitate both standard play and this
interval playback mode so that we should
make sure that the coders when they
watch at three second or five second
increments that they're always watching
the same chunks of video
the sixth requirement should be that a
tool should facilitate agreement
calculations this should be so obvious
to anyone who's doing research is that
you want to go immediately from making
these annotation marks to calculating
what the agreement is between the coders
and because we're working with digital
tools here digital videos Digital marks
calculating that agreement automatically
by the computer should be very very easy
and lastly when we're reviewing the
marks we won't want to easily go from
the discrepancy that occurs back to the
actual video and we want that process to
be so seamless that the researcher
doesn't even have to think about cueing
up the videotape that they can
immediately say aha we had a problem
with the variable smiles so we want to
go back to the video and see where kotor
a encoder be line up and where they
differ and then we can have discussions
about why that occurred to demonstrate
that this framework could actually be
built we created a suite of tools called
V code and V data V code is the main
interface that video annotators utilize
we try to simplify it as much as we can
and provide them the functionality that
they directly need as you can see here
in the main part of the video of the V
code interface we have multiple multiple
streams of video the main video as well
as two secondary video sources for
example if we have multiple camera
angles on the research session possibly
if we're doing a screen grab if we're
capturing all the screen information
this'll and then the coder can switch
between these different video angles
simply by clicking from one of the the
streams at to another and the main video
stream will be replaced
this allows the coder to have direct
access to the best video source for them
to make the most intelligent annotation
possible if they can't see the action
that's going on in video angle a they
can easily switch to video angle B as I
mentioned before facilitating multiple
modes of playback is critical and so we
wanted to make sure that this
functionality was in the hands of the
coders and if you look at the bottom
corner of the screen
we provided a toggle switch to allow for
video for interval playback mode in
addition there's also a text box that
allows you to set the time interval
whether it's three seconds or five
seconds whatever it is this allows the
coder to have different set different
variables to be different interval
lengths so that they can go back and
forth between continuous playback or
standard playback and this can this
interval playback mode the timeline is
the heart of the V code interface and as
you can see here this is where we
present a lot of information at the top
of the interface we have ticker marks
this allows people to have a numerical
count in seconds of where they are in
the video process in the main part of
the screen we have all of the different
marks now you can see the momentary
events are the individual diamonds on
the top half of the screen and in the
lower half of the highlighted area you
see the ranged events now we found out
that by combining and overlapping the
momentary events we can still provide
enough physical area for a mouse
interaction for moving the events around
while still conserving screen real
estate
but because the ranged events how are a
little bit more complex that we're not
just dealing with a with a binary event
a binary diamond that we have the actual
range to consider when you needed to
prote each of those on a separate track
but this is very efficient for screen
real estate usage while still providing
optimal opportunity to manipulate the
marks at the bottom of the screen we
also have secondary data this can be
this is imported in through a text file
with a comma comma separated text file
these marks can be displayed as a bar
graph as you can see in purple a line
graph which is shows up green on my
monitor but on the projecting screen
shows up as blue and also as a scatter
plot which is shown up in red now in
this example all of these secondary data
points are overlapped but you have the
option to actually have these separated
into different tracks so we keep the
data right in line with the annotations
and right in line with the video now one
of the most important things here is the
difference between being able to make a
mark that has the most important thing
here is being able to attach those marks
to the actual variable that those marks
are trying to represent and in the
right-hand corner of the screen we have
created an entire list an entire panel
of all the different dependent variables
that we're trying to measure on the left
hand side you can see the different
keyboard shortcuts in the middle of the
variable name and on the right you see
two different toggle buttons the first
button is allows you to make a mark at
the current playhead and the second
button allows you to put that additional
note that I was talking about whether or
not it's a ranking or it's the
phonetical transcriptions and if you
choose to put a note a window pops up
which gives you a freeform text area as
well as the entire phonetic repertoire
to facilitate any type of phonetic
transcription that needs to occur during
this process
and when you put a mark when you have a
mark that actually has a comment on it
or an annotate or an additional piece of
information attached to it the border
around that mark changes from white to
black and as you can see when I go back
to the timeline here you can see that
some of the marks have black outlines
representing that they have a comment
associated with them and some of them
have white outlines which means that
they do not and here's a highlight of
nothing now moving on to the video
annotation window admin window this is
the window and the interface that
researchers actually use to set up the
decoding environment for their
annotators as you can see on the Left we
have the list of all the variables that
a researcher can put in they can set the
keyboard shortcut the name the color and
whether or not hood event is range or
it's a momentary event researchers are
also able to drag and drop in multiple
video streams so that you can have
multiple angles where or screen captures
going on simultaneously and in the
bottom right hand corner we have a list
of all the secondary data that was
imported this is could be sensor data
this could be log data logged by a
computer piece of software and as you
can see you can set the color whether or
not you want that variable to be shown
you can also set whether or not you want
that variable to be shown as a bar graph
a line graph or a scatter plot and you
can also have a checkbox right above
there that says whether or not you want
these variables to be stacked one on top
of one another
viii data is our tool for doing
agreement calculations this is a quick
and dirty tool but it meets the needs X
greatly for researchers the main area
gives you all the data a lists every
single variable that that researcher had
to ask their coders to annotate and we
calculate agreement by doing something
called point by point agreement we
designate one coder as the primary coder
and one coder is a secondary coder every
mote
every mark that the primary coder makes
we call that an opportunity for
agreement for every mark that the
secondary coder makes we call that
actually is a mark that the primary
coder made as well we call those actual
agreements and we do a simple percentage
by taking the agreements over the
opportunities and as you can see we
provide the act of all of the individual
steps all of the detail there in three
different columns now because we're
looking at a lot of different data here
we're talking about momentary events as
well as ranged events the next three
columns are the agreement calculations
for ranged events following the exact
same formula and the last three columns
are for comments or for the ranking
systems that we asked the coders to make
as well
so we get agreement calculations for all
the different aspects of the annotations
that we made now you might be asking
yourself hey Josh it's not inconceivable
that two different people both observe
the same thing but their marks were off
by a little bit so would they be
actually considered to have agreement
and this is why we added in a tolerance
feature this allows us to have some
wiggle room between the two different
marks that the that the coders made so
right now we have a mark tolerance of
one second which means the secondary
coders mark must be within half a second
on either side of the primary coders
mark now this number can easily be
adjusted this there's up arrows down
arrows and this is an editor editable
text box so the researchers can say well
this is the the tolerance this is the
wiggle room that we want to use for our
research whether it's one second two
seconds three seconds and in real time
the agreement calculations are adjusted
researchers also have the opportunity to
calculate overall agreement across all
of the variables looked at in this
individual session what they can do is
use the checkboxes on the left-hand side
of the main window and select which
variables they want to consider for
total agreement and as you can see down
at the bottom this field is dynamically
updated so we can see that other
variables that we're looking at we had a
ninety-eight present one ninety one
percent agreement and a hundred percent
agreement for calculating their duration
now let's say that we asked the coders
to annotate a variable but lots of
variables of a small DotA so let's
consider that Smiles example that I keep
on going back to let's say we asked them
to and put one mark down if they thought
somebody was smirking another mark down
whether or not they thought they were
just sort of grinning and a third mark
down for whether or not they had a big
full grin and you could see their teeth
but let's say the agreement for each of
those variables was something like 25%
well it's not inconceivable that coder a
would categorize a certain smile as a
grin and coder be thought it as to be a
full big open tooth smile so what we had
what we facilitate here is a merge
function that allows you to take all
these variables condense them into one
and treat all the marks is the same
thing and this allows you to say well
maybe we were going to fine grain with
this but with all these different
variables but when we look at at a more
coarse level a higher level we actually
might have a higher agreement percentage
and when I discussed the requirements
for a tool one of the most important
things was being able to resolve
conflicts right away and so we created
this compare selected tracks button what
this does is it allows the researcher to
select a bunch of tracks that may or may
not have low agreement level and go
directly into looking at the video with
those marks side-by-side
and that's the next picture and as you
can see here what we've done is we've
selected two different tracks and they
get put it right back into V code you
can see coder a and coder B both use the
same color but different brightness
levels and we can see the marks
side-by-side and if you look carefully
you can see where coder a and coder B
differ and because this is directly
synced with the video when having these
resolution discussions researchers can
say well coder a you didn't put a mark
here why is that and then you can say to
coder B well you did what did you see in
the video here and we can immediately go
from the poor Agreement calculations to
going directly back to the marks and the
video so that it becomes a seamless
process a seamless cycle of making the
annotations reviewing the marks and then
checking out where these discrepancies
occur so when we look at the video
annotation workflow that I discussed
earlier and we sort of hold V code and V
data up to the annotation workflow where
we can find out that V code and V data
actually handle very well points three
four five and actually part of six if
you want to count agreement calculations
in there because sometimes you have to
publish that data as well and when we
make the same concession that we did for
all the other tools which is that we
have an interface for collecting and
segmenting video we we take that data in
V code and V data actually hit almost
all six of those agreement points and
one of the exciting parts about V code
and V data is we allow for export at
every stage of the process so if you
want to export data from V code you can
do it into a comma separated file if you
want to export data from V data you can
do it there so no matter what at any
part of this process if you have a
better data analysis tool if you haven't
a better agreement tool we allow you to
take the data right out of our software
and put it into someone else's so you
can perform the analysis that you need
to get done so when we look back at the
set of design requirements that we said
an ideal tool should have we find out
that V code and V data do all of them
well mostly all of the coding workflow
but it matches every other thing we sync
up our video annotations and guidelines
we capture all the different types of
data from moment
to arranged events to allowing for
comments or rankings we say that we have
facilitate additional data to be
displayed and in our case in line with
the actual video we have multiple forms
of playback the continuous mode as well
as play for three seconds pause and make
a mark mode we allow for easy agreement
calculations and when there's
discrepancies and you need to review
them you can go right back to the video
right back to the marks between the
different coders and find out why those
discrepancies occurred via code and
video were released for free on our
website and this has been about out for
about two and a half months right now
and as you can see we've had an amazing
success with the number of downloads the
right-hand side is the download history
as of today we have 6000 for over 6,400
downloads of our software and in the
next week or so we're hoping to be
releasing version 1.2 and as you can see
every time that no position that mark
number 2 and mark number 3 were when we
had releases of new versions of the
software so we're hoping that with the
next release which is going to allow for
Cowen's kappa calculations for agreement
which is another standard used for
determining whether or not the quality
the reliability of the data we're hoping
to see another bump in our downloads as
well but what does this mean in terms of
future work where can we go from here
because I'm sure you're at saying V code
and V data seemed like the perfect tool
that I want to use in my own lab well
there's actually a little bit of room
left to to improve as I mentioned we're
planning to include Cohen's Kappa in the
next release version 1.2 also allowing
for network distribution of this system
would be very very nice this means that
an administrator the researcher could
set up a database of all the different
videos that need to be annotated and
when a coder logs into the system it
could just give them the next video that
they need to work on and they don't need
to worry about finding which where the
file is making a copy of it putting
their own marks in there as well now
with the advent of Android and the
iPhone everyone always asked well maybe
you could do video annotation on your
bus to work so we're wondering maybe we
could actually have a web interface or a
phone interface to allow you to do these
kind of annotations while you're driving
while you're driving somewhere please
don't do this while you're driving while
you're on a plane while you're on a bus
or taking a cruise you know because you
really don't leave Google behind when
you go on a cruise and lastly
researchers that we've talked to and
started to ask for marking up on the
screen so actually being able to
highlight areas with potentially a
stylus or with their finger so that they
can say well not only did I see somebody
smile but it was this person in the
picture and I put a circle around their
face or this person when they were
looking at the screen only their right
eye was looking at the screen so I could
circle their right eye or write a little
note so this adds in a whole nother
element of the video annotation process
something that's not necessarily there
for reliability but allows marks to be
made so that researchers when they go
back and re-watch the video they can say
oh yes I observed this very interesting
thing happening at this point I would
like to thank Joie Hagedorn who was
essential in getting this software to be
built he is one of the most talented
programmers that I have ever known
and I just need to thank him because
he's absolutely fantastic I also need to
thank my advisor Kerri Kalahari oz I
would like to thank NSF whose grant
facilitated this research and all of our
subjects collaborators and researchers
that we talk to to help design our set
of requirements and workflow and at this
point I'll take any questions and I
encourage everyone to go and download
our software at the very least to help
bump our numbers if not to use it in
your own lab thank you very much
yes
so the question is whether or not the
data that gets exported comes out in
standardized form we were debating when
we were doing the export function on
whether or not we wanted to embed it
inside of the QuickTime files so they
would be QuickTime tracks but we decided
to end up going with comma separated
files as as the method because Susan
right so the reason we ended up going
with comma separated though easily that
could be replaced with tab separated
with a small tweak to the code is
because many of the researchers that we
spoke to after they get the annotations
they end up having to import them into
something like Stata SPSS or some kind
of statistical package to look at
overarching trends and therefore we
wanted to make that as easy as possible
that's a very good point as in terms of
either allowing it to be tab separated
ended in instead of commas or as an
option so we might look to trying to get
that option to be put into the next
iteration of our software
what is making this face
right
and we completely ignored it
yeah we didn't even look at going in
that direction but I will definitely
pass those comments along to Joey and if
he and/or I are so inclined in the
middle of our respective work we might
try to get that it might probably won't
be in 1.2 but am I make it into 1.3
light
sure so the comment that was made was
that very often what people might want
to do is to chop up a video by putting
these chapter markers in and that allows
you to sort of break up a video into
easy segments we actually decided not to
facilitate that function in our current
tool though we highly recommend doing
that if you're going to do a large
distributed management system because if
you have QuickTime Pro you can break
down a video file into its small
segments save them as referenced movies
so that it takes up minimal extra space
and then use those reference movies in
their own work but like you said that
that breaking down function is is
definitely a desired feature and if we
were to make the distributed management
system that would definitely be
something that we would have to include
because that's sort of like the missing
link in in in step 1 and step 2 of the
annotation workflow
any other questions if not thank you
very much and once again download view
code and view data thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>