<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reducing the Risk of Shallow Information Analysis | Coder Coacher - Coaching Coders</title><meta content="Reducing the Risk of Shallow Information Analysis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reducing the Risk of Shallow Information Analysis</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KIlHuWAOSC8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone good afternoon
my name is Han myself and right normally
the guy who give me production to David
woods and I would mention we work with
unclear control panels and we mentioned
the studies of NASA with human error
Bailey systems and building Brazilian
Pema new zoning boise state university
ok a just a little prologue to this a
little satire rather than a conventional
since you're not a conventional
information organization I was
interviewed for Japanese TV a number of
years ago on aviation safety on a near
miss and it was dubbed into Japanese so
I hid it from my students figuring they
would do terrible things to this tape
and to me well this year was 50th
anniversary of the human factors society
and I had been involved in the human
factor society and someone wanted to put
on a show of all the past presidents
which sounded like something that we get
very pretentious pretentious very fast
so I pulled the old videotape that's how
old this was walked in and handed it to
the students and said you have a
three-minute time budget make as much
fun of me as you can and so by
introduction of who I am and what I do
we will play their there re-editing
that's a nice sensory so confining
Pradesh modem I could let us leave
suicide notes
oh good in this league logic
chula secret
baby
yup
josepho Demento local consent a goggle
at a co Coochieness in caselma today
technology meet I serial citizen laga
mando martinez in co kangana Connery
Kenta mo Korea's cana ricardas computa
Tony mega yo I can kill moose latina
investor cheerio hi de ho de nada con
los dos aguas tea odo cerrito miss nepal
hallo Senga topic on arenas nara yoked a
don don kangaroo kataoka watch this way
to eat time on a sahaja yoga lesson
computer died 03 technology cuddle isn't
supported a chile china nashua look so
you can eat the Cavalli wanna cook Darla
the mother Emanuel bajo de moi tu puta
monitors
so we'll stop there but that was the
satire needless to say the human factor
society is so conservative they got mad
at me for putting this together and
playing it at the annual meeting I
figured at Google that would go a little
differently today's topic though is
something that you enable people around
the world to do and that is to do new
forms of information analysis and we
work a lot with professionals who do
information analysis mostly because
we're very expensive and they're the
ones with enough money to pay us to work
on their problems so we work for some of
the three-letter agencies and open
research to try to understand how people
find relevant information now in my
career he mentioned control rooms and
accidents so in my career we started out
with data overload but usually in fairly
exotic worlds like running a power plant
at very exotic times when everything's
going to hell in a handbasket today
successful companies like you have made
everybody's experience every day like a
nuclear control room during an emergency
so thank you very much the other thing
we've noticed about data overload
problems over the years is everyone
complains about the difficulty in
interpreting massive amounts or today
hyper massive amounts of data so when
you say to them oh well let's take some
of that access to data away you think
they go oh sure I can't interpret it all
anyway and of course they all go no no
no no don't take I we worked hard we
waited for a long time to have that
access to all of those forms of data and
we use them some of the time please help
us with interpreting all of that so our
work with the professionals moving away
from cockpits and control rooms and
operating rooms where we have tended to
work happened one day when somebody came
to visit me from one of these agencies
of a decade ago and said we should work
on data overload
and we said oh sure we'll be happy to
work on it as long as you let us work on
it two generations ahead all right
because you were constantly being
trapped in band-aids and repair
processes so we got involved and we've
been doing a variety of kinds of studies
with professionals so we don't go off
and just ask them how do they do it
there's a lot of books out there where a
so-called expert at information analysis
will write a book on his own or her own
psychology call it the psychology of
intelligence intelligence analysis and
they do their own sort of folk
psychiatry on themselves what we do is
we go off and study how real people
solve real problems so we work with real
pilots real physicians real professional
analysts and we do that because we
obviously in this case have a big
barrier the wall of classification for
many of these people so we create
difficult scenarios and they're modeled
on real-world cases so one of the real
world cases that occurred that made us
think a little differently about some of
the results we had been getting was the
Columbia accident how many of you have
looked at the Columbia space shuttle
accident how many of you looked at it in
detail how many of you read the return
to flight report how many of you have
read the seven-page descent seven or
eight page dissent within the return to
flight report and will notice the
descent they buried it as an appendix
because they didn't like some of the
people saying they were doing a bad job
during the return to flight decision
making well this is the situation this
is one of the PowerPoint slides how many
of you heard tuff T's critique of
PowerPoint okay all right so this is one
of the slides he refers to in the run-up
to Columbia where someone presents this
analysis and the question I would ask
you is this a rigorous can you tell from
the slide is this a rigorous analysis or
is this a shallow analysis and
the ambition and so there are a variety
of things here that look like someone
has done a detailed assessment there's
been no changes to mentions there's a
probability that it hasn't changed it's
now safe to fly with no concerns and no
added risk it implies there has been an
updated risk analysis it implies there
was a risk analysis hmmm it implies that
they've done a careful understanding of
the boundary conditions right now most
of you know that this is all fake this
was one person's opinion all right put
on a PowerPoint slide to create a
convincing argument there was nothing we
needed to do in other words these foam
events that were occurring regularly pre
Columbia didn't add safety issues and
therefore it wouldn't slow down the
schedule they were operating under
faster better cheaper pressure and so
their concern about foam events was
about how to refit the orbiter to be
ready to launch again so they were all
orchestrated around schedule and if you
go through the Columbia report there's a
couple episodes of this where people
couldn't tell that they were making
decisions on a very shallow basis so
here we have some leading engineers some
highly specialized people doing a very
specialized and risky task falling into
the trap of shallow analysis um do you
ever fall into the trap of shallow
analysis um anybody you know teach
younger kids all right which is right if
you type in a query and you get
something that's on topic that seems to
support your need to write a short paper
for your teacher then you're done if you
ask the professional analysts they are
very very concerned fact we think that
our definition of a professional analyst
is someone who is who is afraid that the
analysis product they're going to create
will turn out to be wrong I mean if
they're very very afraid of shallow and
that the risk
shallow analysis that's what we think
makes a true professional versus the all
of us being able to grab on topic
material and quickly throw out a result
how do we tell what's rigorous so we
went back after the Challenger case oh
and by the way in the return to flight
descent does anyone know the definition
of rigor in the return to flight return
to flight descent they say strict
adherence to a standard any Bay's Ian's
here bayesian what would you call a
rigorous decision analysis following
Bayes rule well some of the
statisticians or the decision people
this formal decision modelers would say
rigor means you follow the rule we think
applies or the algorithm that applies to
that situation well we decided that that
looking at the risk of shallow analysis
and how would someone interpret the
whether or not an analysis was rigor was
an important leverage point in improving
information analysis generally whether
it's specialized for one community or
for any kind of task that matters to
people so what we did yeah
yeah I will fact we will show you
it takes a minute it's a big thing so
what we did was we went off and we
started working with the professionals
and so what's our basic method create a
realistic case launching point way so
our expert exploration said how do you
assess the quality of a information
analysis process and how do you
communicate that to the consumer of the
product so we started with this
definition and let me turn off the sound
because this is something we use as a
multimedia tool oops so we started out
using words that were used by
professional analyst and we just started
asking them what's a rigorous analysis
and we use synonyms for a variety of the
things to say a broad disciplined
exploration of alternative explorations
for the findings so that was our
starting point based on knowledge
elicitation studies and this work is
from Emily Patterson and Dan zelich and
a little bit of contribution from me so
that's where we started so we said okay
let's put this together into a into a
situation and so we created a situation
we call the supervisors dilemma you're
in charge of an information analysis
shop and you have to decide whether the
analysis your people have done is
sufficient to go forward sufficiently
rigorous is it ready to go forward to
the consumer decision maker planner re
planner the customer so you're the inner
ear in the interplay and in some ways
your decision is doesn't need to go back
for more analysis and what kind of
investment given that you're always
under limited resources you have limited
time you have limited forms of expertise
right limited access to different people
and if they're working on this analysis
they're probably not working on another
analysis you have opportunity costs so
we created a situation where we could do
this supervisors to love
now to do this we had to create a case
in which people how to do could do a
realistic form of analysis so we picked
an energy safety / security case on the
other side of the country liquefied
natural gas is a growing new form of
energy basically you can get natural gas
from other parts of the world and ship
it in a liquefied cooled form but that
means you have tankers with this stuff
and they have to come into some kind of
terminal and this is particularly
interesting for energy supplies in the
Northeast us that doesn't have good
access to alternative supplies so
liquefied natural gas is a new energy
source there are safety issues with it
why is liquefied natural gas a safety
issue because it has a lot of energy if
it's very efficient a lot of energy so
if it gets out and ignited it can create
a very intense very quick spreading fire
under just the right conditions so you
have your classic safety stuff so it's a
great topic for analysis you've got
energy you've got nimby not in my
backyard issues where's the terminal
going to be you've got cascading
failures potentially so we've got a lot
of different factors and then you throw
in the wild card security what does
security do is it changes all the
assumptions in your old safety analyses
and in fact the adoption of liquefied
natural gas slowed down because of
security concerns that someone would
deliberately try to attack one of these
tankers and create an intense big fire
and secondary fires that were from the
other facilities that would be engulfed
so it is a great case because it changes
the whole basis of your old analysis and
you have to rethink things so the
opportunity for revision getting trapped
in old ways is there so we created a
liquefied natural gas scenario we've
been using this into several different
studies we've been doing looking at the
actual performance and behavior of
analysts so there's the liquefied
natural gas scenario and then we brought
it together with the supervisors dilemma
with a
walk through process that we developed
in a previous study of information and
analysts which was we used it
elicitation by critiquing so the
situation we created for studying and
getting information was to say you're a
senior analyst you're going to critique
a junior analyst result so we give you
the product and in this case will also
then give you a process and your job is
to critique it in this context of the
supervisors dilemma the specific
decision you have to make is is it ready
to go on to the customer if it isn't
ready what would you invest it right so
the the walkthrough in this case we
created two junior analyst reports one
we worked on to create a low rigor
example but realistic analysis of the
liquefied natural gas situation energy
safety / security um it's also a nice
example because you can play with
different perspectives from the local
responders energy regulators safety
local government environmental energy
business and so the different
perspectives makes us a nice case to use
in our studies as well so the second
analysis was based on one that was
actually done by a security company the
former counterintelligence our
counterterrorism czar Richard Clark's
company did an analysis of this case for
the Boston in Massachusetts and we built
on their analysis to creating what we
thought of as a high rigor example okay
so we have a low rigor high rigor
example the method that is you're the
supervisor you've got to make this
judgment first critiquing pass we give
you the product the second critiquing
pass we give you a summary of the
process by which the analyst created
that product so these are effective
junior analysts they're not looking at
them directly they're looking at first
their products that can pass their
process and they have to make judgments
and we build in a variety of probes into
the situation and into the contents in
order to be able to get elicitation
elicit more information and be able to
orchestrate the information now what's
interesting from this is on
a couple things and we actually just
finished the papers on this so you can
actually get this topic landscapes a
little bit old when we produced it so
you can I can leave a copyist the
summary of the results if you're
interested in this so if you go back to
tuff T's critique of analysis by
PowerPoint right in some ways this is
empirical data that might support or
revise or overturn some of the claims of
different people in that discussion of
the weaknesses of analysis by PowerPoint
so we've got some real data well the
first thing in the findings is is that
in the first pass people made a judgment
about was it rigorous enough based on
just the product information and we got
some of the cues they used what was
interesting is once they got the process
information they changed their mind
about what was rigorous so knowing the
details of how you got to the end result
mattered and if we went into the detail
some of those the issues become
important in terms of revealing or
making observable the analytic process
since supporting a judgment of rigor
what are some of the things they would
be looking for well we're going to come
back to this with a new definition of
rigor or shallowness low rigor versus
high rigor deep vs shallow analysis so a
good a simple example would be the
breath of exploration of alternative
interpretations did they generate
alternative interpretations did they
explore them another would be critiquing
did they show it to other other peers or
other groups who would have an
opportunity to comment on or critique
the inapt the analysis results of the
junior analyst so they were relating to
some of these cues and I'll integrate
those findings in the in the new model
the multi attribute model of rigor
shallow versus deep we also had a big
surprise which is after they saw the
product some of the six of the nine
analysts were ready to send it on I
believe that's the correct number once
they saw the process none of the
analysts none of the supervisor people
acting in the supervisors world
we're ready to stand on the animal
either of the analytical reports once
they saw the process they decided
neither were ready which surprised us
because we had worked extremely hard all
right to try to make these both
realistic and different in level of
rigor so there was a simple example
would be the low rigor involved a much
smaller set of documents that were
examined and ordered as a base for the
analysis and there was a much broader
exploration of document sources
documentary sources for the high rigor
example but in the end they said that
both of them were were inadequate to go
forward and that was a surprise to us
and then we went back and reanalyzed R&amp;amp;R
the the the fake analyses we did and
i'll show you those results in a minute
so let's jump back into this issue of
what's rigor what we're trying to do
here is we went back through this study
other studies we had done in some other
groups including the down the road at
Park who are also looking at information
analysts and how they do things and we
started to pull all of these things
together and say we identified eight
different kinds of attributes that would
fold into a meta judgment about the
rigor of an analysis and so let's run
through some of these indicators of
these dimensions and how they play into
assessing what's higher low rigor so
here's the dimensions information
synthesis what's that well a low rigor
report let me jump in here too it's this
one so information synthesis so we have
examples here low synthesis would be
data reporting fact reporting here's a
collection of facts right reading the
news is sometimes said by the
professionals all right or are you
starting to add some level of insight
that goes beyond a simple compilation of
apparently
of information right and then was it
reconsidered from diverse points of
views and maybe leading to a rican
sexualization explanation critiquing is
another category so at low explanation
critiquing we would come back and say I
just did my job I threw together an
analysis within my resource and
expertise base and its up to other
people in the organization to see if
they're going to critique it or comment
on it a medium level would be I went and
showed it to one of my peers to do a
quick check on my chain of reasoning or
story construction and they might
critique or augment the process think of
this like a physician with a hall side
hallway consultation when they run into
one of their physician partners in their
in their service and a high would be a
series of reputable peers and
specialists kaif carefully examining
this identifying inferences that might
be wrong what's an example of this the
Columbia accident investigation board
they didn't just form a reputable book
panel right the reputable panel had
diverse backgrounds related to the
accident but they convened a series of
symposia and brought in a series of
consultants myself being one of them to
check their reasoning and logic expand
on it reinterpret it so here you have a
high rigor examination in the context of
the Columbia accident investigation we
can go around hypothesis exploration
alright so this is the how broadly do we
generate and search and evaluate
multiple explanations low is quickly
generate a favorite hypothesis and this
tends to lead us to look for certain
kinds of fixation or what Paul felt a
bitch calls knowledge shields these are
shields you put up to prevent discrepant
evidence from revising or changing your
story alright so this has been much
studied we've studied it a lot over the
years how people get trapped in a single
hypothesis alright
here we start to consider others but we
don't have a balanced interpretation
versus we have a process that's
carefully considered and waited the
different the different possibilities
and often that returns to the issue of
did you generate new possibilities from
the examination of the initial
possibilities so the hypothesis
generation process is important we can
keep going around the the actual search
process did we quickly find something on
topic in my information analysis class I
give people back while I'm here they're
working on this back in Ohio I give them
a surprise in the history of
intelligence analysis all right and I
give them a seed article on it and then
tomorrow when the co teacher does the
exercise the first exercises and why did
the surprise happen they the first thing
is to say what other sources did you did
you consult and I'm guarantee you that
several people will say I went to
Wikipedia and looked up that accident
and it seemed like that what you said in
this article was correct so I just used
the article you gave us obviously you
gave us the article for a reason right
and then somebody else will say I
googled it I googled the accident I
googled yom kippur war where yom kippur
war surprise and I'll come back and I'll
look at the first page all right these
are graduate students I guarantee you
two or three will have done this they'll
look at the first page they'll click on
a couple they'll say oh the information
is consistent with what you gave us I'll
just read the chapter you gave us um
right I mean it's about a 15 page
chapter even for graduate students
that's their quota for at least a week
these days alright in actual text form
right as opposed to multiple Facebook
interactions you know that's why we're
moving to multimedia resource sets here
with video clips and things because we
don't find people read anymore so
writing a long paper on this is only for
me to support people with greater beards
than I have who still want to evaluate
professors by academic publications
um and and then you'll get somebody who
will go wait a minute this is in the
defense this is military history and
they'll go look it up in a military
history a guide like Jane's all right
and they'll go yep you gave us a good
source and they'll still read the one
chapter and so that's the information
search kind of issue one of the things
we've seen even with professional
analysts is they will often simply
funnel down on a relevant set of
material that's manageable for the
resource window they have and they will
then adopt a few of those as key
resources in the analysis and of course
if those turn out not to be complete
accurate assessments of the events in
question they're their explanations or
projections about the future will turn
out to have flaws in them and again we
have empirical studies with
professionals who have 10 to 20 years
experience indicating these traps of
shallow analysis can get any of us not
just high school students notice the
difficulty in the information search and
to some degree it's a difficulty the
whole shallow versus deep low versus
high rigor problem has and it comes up
particularly here how do you know how do
you get a sense whether you're going to
find stuff because if you already knew
what to ask for or to look for you would
have already included in your search so
there's this sense of have I really
looked for other material would anything
be promising if I put extra effort in
this means that our one of the risks
were worried about is premature closure
and the concept that we originally came
up was it was in observing expert
analysts was they usually include some
kind of broadening check they have some
kind of heuristic they use to look a
little fart further afield to look an
extra time to generate a different topic
or a different twist on the search in
order to reduce the risk of getting
trapped in too narrow a set of resources
to build their case their explanation or
projection we can keep going around
I'm information validation this refers
to what the analysts usually talk about
the conflict and cooperation process all
right did you just accept a report in
the liquefied natural gas is it easily
ignited some sources will say it's
easily ignited others will say it's hard
to ignite that's a vague interpretation
how do you push behind that did you
identify conflicts like there are
sources that from the industry we say
it's hard to ignite and they're from
environmental groups would say it's easy
to ignite and how do you notice that
conflict and then pursue that to get
some collaboration or better technical
detail I found one that sounds good the
issue here is sort of a suspicious
attitude remember I said I think our
working definition of a professional is
one who's afraid they're going to give
you the wrong answer in their
information analysis and that comes out
very much here in the validation very
much a suspicious attitude and the the
journeyman or new professional analysts
who come into these organizations often
end up doing a kind of cut and paste
when they find on topic materials cut
and paste them together to create a kind
of composite storyline but you don't see
the active what they think of as the
analysis process is how do you detect
conflicts how do you find corroborating
sources to build confidence Stan's
analysis remember i said in the
liquefied natural gas example as you
work through the analysis there's
multiple groups that are relevant to it
and they have different stances there's
the obvious energy company versus
environmental group which we tend to
think of as opposing stances but a
national perspective it can be quite
different than a local responder
perspective a regulatory perspective can
be different than the business
perspective and so what are the stance
of different sources right and why would
they emphasize certain things so the
Sandia lab safety analysis is based
mostly on old fault trees honey which
tends to say it's a very safe form of
energy transportation well what's the
problem security issues change the fault
pass and can they get out of the old
mindset and sort of see new ignition
sources new ways that things could
spread new ways that things could go
wrong in these situations so stance
analysis refers to are you understanding
the perspective and background if you
get a source that for example an
environmental source that opposes every
energy development that's carbon-based
all right well then it's not much in
there's not much information if they
oppose this so you'd be looking for
variations against the stance an example
in international affairs is what does a
leader from another part of the world
say in English versus what do they say
in their native language and a stance
analysis would look at the difference
between what they were saying in the
different languages to the different
communities in order to understand their
actual position sensitivity analysis
this refers to a lot of the things we do
in mathematical studies and simulations
and analytical codes a lot of times
they're used for sensitivity analysis
basically in the what the professionals
do is they look at if they're high rigor
on this what they do is they look to say
if something is wrong in the base for my
story will the story fall apart right if
one of the key if one fact is key and
the whole story hinges on that one fact
being true right I don't have a very
rigorous analysis it's got its poor on a
sensitivity dimension okay so i'm
looking for an analysis robust because
there's always uncertainty in the core
underlying facts and interpretations um
specialist collaboration this is
something that NASA is expert on
normally it didn't happen in Columbia
which is one of the mysteries why it
never got to Mission Control because
Mission Control is fat in fact a expert
organization at doing specialist
collaboration remember the front room
you see on TV those aren't the real
operators those are the supervisors each
person in the front room right is in
charge of a back room and each back room
is potentially connected to specialists
people all over the world who have
expert knowledge on detailed aspects of
the processes and components of a space
shuttle in the payload and the mission
planning so specialist collaboration is
how do I get the right kind of expertise
and bring it to bear given the situation
i'm dealing with often information
analysts or thought think of themselves
as kind of generalists and the
generalist the mark of a good generalist
is knowing how to get the right
specialist and integrate the specialist
knowledge in the overall interpretation
so here we've gone through eight
categories that try to take this thing
and say what's the definition of a
shallow versus a deep or a rigger low
rigger versus a high rigger analysis now
what's interesting here is notice we're
not saying adherence to a particular
ideal process we're not coming back and
saying you should do it in this
particular way mathematically there's a
particular decision analysis process
there's a specific cognitive
psychological process that should be
followed to do good information analysis
in part because information analysis is
such a broad and diverse set of
activities in so many different
technical and human areas we can vary in
purposes context resources access to
expertise and so it seems to have a
quite a different set of flavors in fact
if you ask the professional analyst what
are they always saying to each other my
analysis is different than your analysis
hey well uh you know um you know I do
country studies you know and you'll come
back and say but I I actually come up
with a plan to what to do next
and somebody else will say but I look at
health care and somebody else will say
but I look at financial information and
each area just like Boeing vs Airbus in
cockpits emphasizes the detail
differences of their special home base
of information analysis rather than
looking for the general underlying
commonalities as a form of work so we
think that the rigger multi-attribute
metric gives us a mechanism by which we
can quickly score and understand our
portfolio of information analyses what
are we doing are we tending to be
shallow alright are we over investing
could come up in some places let's take
a place where this matters to you well
probably not for 15 years most of you
and that's health care one of the
studies were doing right now is we're
analyzing accident reports adverse
events in health care and what we think
we're going to find when we finish the
study is that most of the adverse event
analyses done in health care to try to
avoid injuring you in the process of
giving you health care when you need it
all right back highly shallow analyses
when you compare does anybody remember
the Duke transplant death at exactly the
same month as the Columbia accident
Jessica santillÃ¡n died in duke hospital
due to a botched liver heart Tran
heart-lung transplant it was multi-organ
transplant which by the way means at
least two other people unnamed people
died because they tried to transplant
her again and given the lack of organs
that means to other people waited longer
on the list it didn't get Oregon so at
least three people died in that event
the accident report for Columbia
involved a prestigious committee
orchestrating a year's worth of intense
resources around the world hi rigor on
almost all of these dimensions Jessica
Santilli in case we had press releases
vetted by the lawyers for Dukat
university hospitals all right the
opposing position was a news news news
reports trying to get behind the veil of
secrecy the wall
all of protection that was being put up
by the hospital well it was this one cut
one small mistake and it wasn't really a
bad person who did it and so in one case
we have a very shallow analysis with low
confidence the organization learned and
changed in the other case we have a very
deep analysis but a very expensive one
what do we come back to the fact that an
information analysis making this
judgment of is it sufficiently rigorous
am I done can I stop should I keep going
is a constant background decision at
least implicitly in all information
analysis not only is it an is it a
critical decision and information
analysis it's really an iterative
decision right because as we're going
through this we're constantly assessing
have we do we have sufficient rigor all
right and if we don't have sufficient
rigor it's acting to guide us into
looking at other kinds of sources
getting other kinds of critiques
integrating other kinds of specialist
knowledge in fact a good information
analysis should help us always ask
answer the question if I had a little
more time expertise or money what would
I do with it that's one of the kinds of
broadening checks right that would help
us avoid the era of prematurely closing
an analysis in a place where we would be
incomplete or inaccurate in our
explanation or projection of future
events so let's look at how we did in
the study we set up to fictive junior
analysts with a low rigor and a high
rigor case after the study when we were
surprised that we that was harder to do
than we had thought we went back used
this this was the third revision of our
of our rigor multi-attribute metric and
we we went back and based on the results
what we were told by the analysts in the
study we went back and rescored the two
in this metric form so this is the
loaf and interestingly enough the low
one in order to make it realistic we had
actually made it high on one dimension
and it was medium on a couple others in
order for it to be a realistically
meaningful case and not just a cartoon
of a professional analyst analysis
here's the one we thought was a high
rigor process if you notice it's mostly
medium right with one hi they saw a lot
more documents in this process but one
in the end in terms of giving indicators
of a broad hypothesis exploration in
generation it actually was pretty low
based on what the analysts had told us
about it so all of a sudden we can look
back and say here's an example of how
this provides us a quick graphic kind of
metric that we could compile and say
where are we given the kind of analysis
that we do in many situations remember
the question sufficient rigor maybe may
be answered with a relatively low rigor
process because it's fairly easy to come
up with an on topic definitive answer or
that a answer that is just what's a good
enough answer has very low consequences
relative to resource expenditure or we
may find that in some cases being high
on only one aspect of this is critical
again given the nature of the resource
to criticality of the analysis so this
gives you a tool that's
context-sensitive it doesn't impose and
say this is the right answer it allows
you to assess what are your information
analyses looking like and are they
appropriately match to the context of
the decisions you have to support yes
one process could be compared to another
but I'm not so clear how the greater
process on the absolute um that's right
so we're doing these relative and that's
why we used as anchors if I go back for
the scaling process that we're working
on we use these kinds of anchors which
are based on example statements from
actual analysts from different studies
we've done so if you ask the analysts to
describe their process all right if we
have a relatively low one on the
synthesis these are the kinds of
statements we would see and these are
some of the kinds of indicators if you
look at the process they went through
you would see lots of lists you would
see lots of facts being taken in
isolation rather than embedded in a
narrative structure so we're trying to
build up a set of cues that you could
actually use as a pragmatic tool so for
example one of our plans now is to go
back to NASA safety of a space shuttle
safety group and come back and say we
could compile a set of these indicators
that would work for you to monitor your
own analyses and make a organizationally
specific commitment as to what's level
of rigor is appropriate what should
trigger additional investigation because
remember again pre Columbia the foam
events all right the problem was
pre-existing conclusions were supported
by an individual orchestrating a set of
facts instead of an engineering analysis
process safety analysis process which
would examine the evidence to come to a
conclusion we can now go back and say
okay what's tuff T's comment in some
ways tuff T's comment by the way I
forgot to mention in one of the two
processes we used a figure we use
graphics and one had no graphics
and of course one with graphics the
graphics attracted a lot of attention in
the the senior analyst acting as
supervisor making the supervisors
judgment as a cue to perhaps a more
rigorous process so what come what is
tuff T's real remark tuff T's real
remark is that of course the
representation of the process will
influence the confidence you have in
basing a decision on the results of that
process we can either write trick people
as in the Columbia case where someone
had a conclusion they wanted to support
nothing is different it's not a safety
issue and therefore it doesn't require
delays and extra work or we can join
orchestrate the image of the analysis to
make the process observable so that
someone who can see into critical
aspects of the analytic process that are
related to a more effective analysis a
more trustworthy a higher confidence a
more rigorous analysis the making that
process observable interestingly enough
in the example we've used by making it
observable relative to these eight
dimensions so these eight attributes
right doesn't doesn't lock you in but
instead forces you to debate what is
good analysis for your purposes and the
role of your particular analysis shop or
activity so that you can start to come
back and that you can change over time
your answer to what defines low medium
or high as a relative process tailored
to your context images we've always
known that representations can change
the evaluation of the underlying
information it's so old I call it a y
and yet another demonstration of
something we've sown over and over and
over again the representation affect how
you represent something influences the
cognitive processing to use that
interpretation to do cognitive work is
it is as old a finding as there is
cognitive psychology all right so tuff
T's critique is really reminding us of
that and suggesting that we need to go
back all right and re-examine how do we
reveal or make observable the analytic
process without having someone repeat
the analytic process in deciding to base
a decision on it remember what Tufte
says and think about the rigor metric
tufty says we should give them a report
what do you think the answer from a
professional analyst is when Tufte says
that I barely get a minute or two with
some of these people all right or if I
don't impress them in the beginning that
I have something critical to their
decisions they've moved on to the next
the next refeed the next document the
next slide the next topic the next plan
I have a very brief window given the
nature of this and many of the certainly
the geopolitical but also in companies
and executives and I would suggest that
the rigor analysis is a way to balance
and the supervisors dilemma is a way to
balance that interaction between
analysis and decision making or we
prefer to call it replanning you have an
analysis perspective and a replanting
perspective and the rigor metric starts
to tell you about how are you balancing
what you're doing an analysis relative
to supporting replanting judgments
without getting trapped in giving them
long technical reports and turning them
into an analyst when they want to be a
policy maker or planner on the other
hand avoiding the mistake where the
planner makes a decision without
understanding the basis for the
analytical recommendations as in the
pre-colombian judgments about the foam
events being no threat to safety so we
would suggest to you that the rigor
metric is a basis that we could explore
with you a variety of kinds of
applications just as we can use this
with some of the security oriented shops
we can also use it as a basis for what's
a rigorous enough safety analysis how
would we use this in various kinds of
debates about energy development around
tree we can use it in in the NASA
context of managing critical missions
okay so we think this has a fair power
of generality and it can be customized
to specific settings and becomes again a
learning tool that can be updated and
changed as new information comes in so
that's the kind of stuff we wanted to
tell you about about the current state
of our information analysis work I would
also comment if there's questions I'm
happy if people want to discuss some
things I would mention that these
studies by Dan zelich and Emily
Patterson are part of the Institute for
collaborative innovation we run every
summer where we launch interdisciplinary
teams of people at varying educational
levels with a diagnosis of some of the
aspects that make information and Alice
is hard examples would be updating or
stale information was used one summer
last summer was the question of how do
you tell whether an analysis is rigorous
or not we also look at how do you build
collaborations across different analysts
that one analyst can take advantage of
another analyst work without getting
trapped in that analyst perspective or
result so we take a variety of diagnosis
like this we launch interdisciplinary
teams and we put on a show ten weeks
later where the show is to demonstrate
promising directions to solve that
problem and by show we mean more like a
art show where our performance art where
we're trying to stimulate the audience
so we bring in people from a variety of
analytic customers and organizations
that do analysis professionally and we
use it as a way to stimulate debate
interchange reconceptualization of their
own work and information analysis so
thank you
you
yeah yeah you have anything to say about
how you decide what you've got enough or
is there a way to get high rigor you
know better faster cheaper one of the
things we're very worried about is the
issue that lies in the Columbia example
that it's just long and expensive to get
a high rigor and if you spend a lot of
time and money you will have a high
rigor result and so we're looking at a
variety of poor a good resource for us
as a variety of poor accident analyses
so we're looking at a radio a
radiological event in Scotland that had
a very elaborate long winded accident
analysis is not very good as an example
something that spent a lot of time and
money but it wasn't very rigorous
relative to these dimensions so we're
exploring the sensitivity of the rigor
metric to discriminate the quality or
the depth of the analysis independent of
the easy correlates they spent a lot of
time they spent a lot of money they had
a lot of people on it on the other hand
certainly a shallow and out it's hard to
do more than a very shallow analysis if
you don't spend some resources so our
view is this is a tool to calibrate your
resource expenditures to this context
you work in on the answer will be
different for different shops so a space
shuttle mission might be very different
than someone making investment decisions
though if it's your money that's being
invested I think it might be about as
critical as if you were on the shuttle
right the the other issue we bring up in
this is is that there are other criteria
that go beyond this to decide whether or
not you've made the right resource
investment so our recommendation to
safety organizations is to have a
balanced portfolio right if you don't
have some highly rigorous analysis
you're probably not learning as much as
you could or should to prevent events
spat events from occurring if you're all
relatively shallow or if they're all
shallow and high in the same on the same
dimensions you're an increased risk of
scene cues that could have warned you
about early signs of trouble or partial
events that might be indicators a
full-blown accidents and disasters that
might be ahead so our view is it's a
tool in that sort of cost-benefit
resource investment versus learning
decision that every organization has to
make this tool doesn't help you make
that learning investment trade-off but I
think it's a critical tool in an
organization making that trade-off
contribute
well you create the opportunity for
information analysis by people who have
less experience so the issue is can we
orchestrate the tools in the artifacts
they would use in the methods they would
use to encourage people to do a broader
set of of explorations of higher rigor
so we do have suggestions for you all
right and one of these is something
we're working on which I wasn't really
going to talk about but one of the
examples that come up from this is a
kind of diversity search and so we're
playing around with different
definitions of what would be how would
you do a diversity search where you're
trying to identify that from different
different perspectives would define
relevant to your topic differently and
therefore you would come back and now
have a here are a couple different
interpretations of the query and people
by sampling from those different stacks
or less hi relevant because they are
still only going to sample a couple
things at the top we know people are
going to be trying to minimize effort
right and as they try to minimize effort
we're trying to make sure they get a
broader or at least the possibility of a
broader return of different perspectives
so that would play into the hypothesis
generation hypothesis exploration it may
also lead them to expand the search and
to do a little more validation because
they start to see that they might start
recognizing something's in conflict so
several of these dimensions might
naturally get better if we could come up
with mechanisms that would implement a
kind of diversity what we've been
calling it is tuned diversity search
because we're tuning it to the nature of
the query and the underlying topic and
purpose and the question is can we build
that into search tools or do we need
other kinds of information about the
purpose of a search in order to do this
kind of diversity mapping that would
encourage a broader a broader
exploration the other side of this is I
think whether we like it or not and I'm
an old fogey
I you know truth in advertising right no
hiding it here gray beard um you know
I'm in shock every day from my high
school or at home who does a quick
Google search on her homework finds on
topic material and is done two graduate
students in in you know we're going to
be professionals dealing with usability
and cognitive eight cognitive decision
support systems and stuff how easy it is
for everybody because we have so much
access to stop too early and to not use
discipline mechanisms to broaden the
search the by the way the professional
analyst organizations tell us they have
the same concern about their new hires
that their new hires fair amount of
experience intellectually and
academically the difference between them
and a seasoned and analyst is the new
person it they're not new in age they're
not new and academic or topical
experience but what they are is way over
confident that what they initially find
is a complete basis to build a
reasonable and acceptable story and
projection for the topic they're
assigned and they're constantly worried
that these guys are and ladies are
prematurely closing and have have
inappropriate confidence they're
overconfident that they have a good
analysis and they're insensitive to some
of the cues that would tell them they
have a they're in danger of a shallow
analysis so at all levels I think the
danger of shallow versus deep analysis
is the side effect of the easy access to
massive quantities of data you are so I
guess
like that but typically there's a cost
involved in things that the cost for
your high school high schooler who needs
to turn that paper the cost of being
wrong may not be so great in some cases
right well that first off we think
that's the point of using these you can
use these tailored to the consequences
in your context right now for the for
the student I think there's a different
consequence which is if you learn
shallow analysis techniques you'll
always be satisfied with shallow
analysis down the road so there's a
different consequence with respect to do
you do get an A or a C on your paper
relative to this being a general form of
literacy for the public but the second
thing is one of this is one of the
comments we always get about our
research right which is well you talk
about planes falling from the sky and
things going wrong in the operating room
I don't deal with high consequence
things like that well what do you mean
wait a minute consequences are in the
eye the series since the consequences
are in the eyes of the beholder if it's
your investment portfolio that goes down
the tubes when the financial landscape
changes because people reacted to the
trans too slowly that's your house foot
in California right that's a pretty
serious thing it's you know it's the you
know I'm just past the college the
college investment but it would have
been the college savings for the kids it
could be there you know under the under
some of the political initiatives it
could be your retirement those are
pretty serious consequences to people
and they go to great lengths and they
get very mad I mean look at people when
they have to write a taxi a check for
taxes at this time of the year they're
not happy campers when things don't work
out so I think that the issue I think it
becomes easy to study when they're very
high direct consequences it's very easy
going to show people that these things
make a difference but I think the
results apply across the board they just
get scaled a little differ
in terms of the total resources to the
total consequences I'll be happy to stay
and talk to other people too we need to
do okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>