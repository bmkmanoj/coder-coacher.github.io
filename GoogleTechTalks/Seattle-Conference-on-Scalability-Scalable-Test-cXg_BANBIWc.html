<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability: Scalable Test... | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability: Scalable Test... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability: Scalable Test...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cXg_BANBIWc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey everyone we're going to get started
again and I would like to welcome Ryan
Gerard who's an sqa engineer from
Symantec and he's going to be talking
about scalable test selection all right
my honor all right Thank You Amanda you
may have to put up with a little bit of
sniffling today I have a nice like virus
cocktail of fun going on right now so
pardon me if it gets in the way of the
talk so I may so as Amanda said my name
is Ryan Gerard I'm a QA engineer with
semantics with the security 20 team
which is a relatively new team as
working on some web security engines at
the end and also in the online identity
space and I'm here to talk to you today
about a project that we've been working
on in my team an idea and a prototype
that we created for to do a little bit
more intelligent test selection first
often just some logistics feel free to
ask questions unless it's going to be
followed by I don't know like a tomato
to the face but otherwise Savior you can
save your questions to the end I'm sure
there will be time so today we'll be
talking about sort of the basic problem
and a suggested solution suggested
meaning it's going to be a high-level
look at the solution with some details
on how we implemented it internally a
prototype did some ideas on
implementation other ways on measuring
effectiveness and some lessons learned
also I should preface this was saying
you know there's been some great talks
here so far today most of them are
relatively low-level machine and
operations and whatnot this is going to
be a little bit higher up the stack so
before we start a little bit of
background I can't assume you all know
what regression testing is or what
regression bugs are so a regression bug
is essentially a feature that used to
work the now no longer works and you'll
see this in mature products that maybe
have 20 30 features out there and a new
release comes out maybe a couple of the
features don't work as well as they used
to this is known as a regression bug so
hence you do regression testing which is
essentially a nice cachet of tests that
you go out and verify that the old
function
he still works correctly the basic
problem though is that regression tests
only increase especially on mature
products I'm sure many of you have seen
this right its product starts out you'll
have a few few tests maybe new build
comes out you'll run a few automated
manual tests eventually though it's
turns into something unmanageable at
least within a certain amount of time
right you can't test everything all the
time so this is essentially a problem
that doesn't scale well so when you run
into this situation how do you determine
what you should be testing and another
facet of the problem a lot of times
regression testing is testing all
components when in reality maybe only a
few are changing so this is essentially
wasted time and effort testing things
that may not need to be tested certainly
there may be dependencies between
components so if component one changes
you don't want to just change component
one if there's a dependency on component
to but I think these dependencies are
things that can be learned at least in
terms of test selection
lastly so typically all tests are not
automated but you know may not be true
in some projects but i would say for the
most part that's the case you will have
some manual testing so in time
constrained situations reducing your
manual test load becomes incredibly
important let me give you the situation
that is near and dear to my heart let's
say you have a release that's going out
Friday no matter what happens management
is determined Friday this thing is going
out and thursday night a major change
goes into effect how do you decide what
to test at that point in time you have
literally hours maybe the test you can
you literally you just do not have
enough time to test everything so how do
you decide what to test so what are you
doing now is what I asked you I mean I
would say most likely testing as much as
possible right staying late but even in
that even and even in that situation you
can't test everything and you're
probably going to miss things so
hopefully will be a better way to test
to select test these should be running
so here's the basic high-level overview
of what what we're trying to do given a
set of source code changes determine the
set of tests that need to be run now in
order to implement this what we've
decided to do is do a little bit of data
mining on some of the test artifacts
that are all interrelated in testing
things like your defect management
system the bugs that are associated with
test cases and source code your
requirements test case execution history
and build information and from this
hopefully you'll have some sort of tool
that can adequately determine what tests
need to be run this is a very
non-scientific graph that is really just
meant to visualize an idea that being
your risk of releasing a regression bug
back into the field is going to be much
lower if you can execute every
that you have right but that's not
realistic you can't execute everything
all the time so we believe that if you
have a better method of test selection
you can reduce your risk of releasing
regression defects back into the field
if you can select the regression tests
that you're that are more likely to fail
then those failures won't be seen in the
field so the way that we've attacked
this problem is to look at three
different ways to associate source code
with test cases source code Delta source
code changes with test cases the first
two requirements and defects are a
little bit more straightforward we can
make associations between the source
code changes and what defects are and
what test cases are affected by those
source code changes by doing a little
bit of data binding on those two
management tools the requirements
management and maybe the defect
management tool and then we do some more
direct data mining in doing some build
correlation so here's the basic overview
of how one would associate source code
and test cases based on requirements so
first off you imagine that requirements
are going to be defined probably by
product managers or you know lead
engineers something like that you'll
have source code that's checked in to
fulfill those requirements and then test
cases checked in to fulfill those
requirements as well correct so given
that you can assume that there's going
to be some sort of relationship between
those two pieces of information so that
later on when source codes checked in
you can say oh well this source code
based on this relationship we found most
likely we should be executing these test
cases right because these test cases are
associated with the source code through
these requirements
very similar situation with defects
let's say in the given situation you
have a new build out QA starts testing
and a test case fails a defect will be
created and source code will be checked
in and from there again you can say well
if a test case fails and the source
codes checked in to fix that failure you
probably have a relationship there where
late the next time the source code is
checked in you can say well last time
this was checked in these test cases
failed it's probably a good idea to run
these test cases again the build
correlation method starts on day one so
build one day one and hopefully you'll
have all this information saved in a
database for mining when you're
implementing this unbilled 200 you have
source code changes that are checked in
and then test cases that are failed you
can make a set Association from this and
say the set of source code changes that
are checked in at failed this set of
test cases you can continue this with
successive builds more test case more
source code check-ins more test case
failures and more set associations so
here source code one and three are
associated with test case failures three
and five so now later on build n your
source code that's checked in right
let's hope by now that we have enough
historical data to make some sort of
intelligent predictions so source code
one in three are checked in well based
on our you know to build history right
we can make some some kind of
intelligent selection you can say well
it looks like we should probably be
running these four tests that have
failed in the past source code one
failed in both cases those four tasks
source code three failed those two cases
in that one test so hence the set of
test cases is can be selected
intelligently
so now given that the build history is
method is only going to be accumulating
test cases over time you probably want
to put in some kind of prioritization
methods so what we've done very simple
prioritization is just look at frequency
so over the build history test case one
has failed more times it's probably a
better test case to run in this
situation there are other prioritization
techniques you could use waiting recent
failures things like that it's really up
to your context whatever it is that
works best for you this is how we view
method usefulness over time in the
beginning the requirements as you can
see so the three icons here on the side
is supposed to be your guide for the
methods so the requirements method at
the top there is going to be useful
early because that's all you're going to
have at the very beginning you won't
have defects you won't have many builds
and hence that will be essentially the
only one you can use it first however as
you gain more data with more builds more
defects we think the other two methods
will surpass requirements significantly
especially the build correlation method
so implementation so this is this is the
basic metadata that we think you should
probably pull in to make these set
associations to make these associations
and I don't think you should change any
of the tools you're already using you
should integrate them so for instance
spring in your change control systems
your defect tracking systems your
requirements management systems test
case management and build system I have
to say it is not necessarily easy to
integrate all these systems together
especially if you're using third-party
software or something that's not
developed in-house maybe something
that's hosted somewhere else you may not
have direct programmatic access to it
which makes this obviously a more
difficult problem but if you do more the
more data you have to mine the better
test selections you can make so some
very basic overview of how this would be
implemented so let's go over the
requirements Association implementation
first so keep in mind we're looking for
a relationship between source code
checked in and test cases checked in for
per requirement so an easy way to do it
is to parse your requirements again
assuming you have programmatic access to
these to these databases look for the
change numbers look for the test cases
that were hopefully commented or somehow
put in there to be associated with these
requirements so assuming you can you
have the change lists or commit numbers
or whatever what have you for your
source control system you can grab the
file list associated for that
requirement and say with some amount of
certainty these test cases one and two
are associated with these two source
files now when you go back to actually
use this information let's say that data
has a change there's a source change on
the Parsee file you can look back into
your tables and say with some amount of
certainty well these source files are
associated with these test cases based
on base
on the requirements we have checked in
hence those are probably good test cases
to run for the defect Association
implementation so again in this case
we're looking for test case and source
code associations relationships based on
the defects that have been checked in a
good way to do this is again parse your
defects there's going to be very similar
to your requirements look for the change
numbers and look for the test cases that
have failed so assuming that it's you
know hopefully in the comments there in
some field that's associated with your
defect you can get the file list for
that specific change your set of changes
and associate well this test case failed
with these source code changes so it's
probably some relationship the way this
will actually be used in the tool is
very similar again source changes will
be coming in and it will go down into
that Association table and say well
according to these with these source
changes what test cases do I need to run
and based on history of your defects
that it'll it can hopefully tell you
which test cases it suggests you should
run test cases that have failed in the
past that probably have a relationship
with this source code
so for the build correlation method the
first thing that you probably want to do
that we did is integrate with your build
system so that it parses each check in
and records the changes that are being
made in some table some database that
you have access to so you can say with
certainty okay with build X we have
three changes and you know five
different source code changes the next
thing you want to do is integrate with
your test case management system let's
get more metadata definitely so then you
can say again with some certainty on
build X we had these three test cases
run and two of them failed so we can say
with some certainty these two test case
failures are associated with the last
set of source code changes so how this
would actually look in the tool itself
is with some source code change that's
checked in you can go down and try to
look at the build history and say well
based on all of these n number of builds
with this piece of source code how many
of them have test cases that have failed
how many times have test cases failed
with this source code checked in so it
looks like with these two we have you
know let's say this for instance there's
only two builds that have been found
with test cases four or five and six
that have failed when this source has
been checked in in the past was a
probably good test to run this time
around as well
so a quick summary in comparison so
we'll look at the first two right now so
requirements are definitely useful early
on however there's a main problem with
management and processes that have to be
in place for this to be really useful
meaning let's say you have test cases
that are checked in they have to somehow
reference these requirements either in
the test case or in the requirement
however you do it that may not always be
the case not everyone does this and at
the same time the source code when it's
checked in has to say somewhere somehow
this references you know requirement X
right well that's not as easy as sending
out an email and telling people to start
doing it takes a little bit of time
change beating people over the head
things like that with defects again
it'll be it's useful later on once you
have a nice set of defects that you can
you can mine through but very similarly
to requirements corey process changes
are necessary you have to have the data
in the defects which test cases failed
which source code was checked in to fix
this defect somehow it has to be
referenced within your change control
system I know a lot of times we do it
when when tests when source codes
checked in it'll say within the comments
within the description fixing defect you
know one two three and in that way we
can later go on and mine all this
information the build correlation date
method is much useful later on however
it's a little bit more work a little a
few more systems need to be integrated
and it's going to be returning more test
cases to you so it's not nor error-prone
it's not necessarily going to be useful
method if it's returning a thousand test
cases to you and you have you know an
hour which is why you'll want to have
some prioritization in there to
determine which test cases to run but
it's still at least it does narrow down
the field
so our methodology is to use all three
methods so if we can find associations
using all three approaches each one will
return a different set of tests for us
to run and from there we can then
determine make some another set of
another prioritization another waiting
technique to determine okay have the
three methods that return tests which
test should we be running I also have to
say in our methodology this isn't a
replacement for any of our current
processes this is in addition to so in
the past let's say when the situation
comes up when source codes checked in at
the last minute we have to determine
which tests to run you know maybe you'll
you'll depend on your lead engineers or
managers whoever is familiar most
familiar with this product or feature to
say oh here the test cases I suggest we
run based on my own history and
knowledge so that is a great technique
it's very useful technique and it's good
to complement that with a system such as
this that also says okay well also based
on some historical data maybe here's a
few you missed here's a couple to look
at as well so in the end for your
resultant test selection tool you want
to answer the question what tests should
I run for this change so given a change
that comes in you can use each method to
get a different set of tests that it
suggests all right so here we have three
methods returning three sets of tests
that it suggests and now you have a nice
set of test cases that may or may not be
useful more useful than whatever you
were doing previously probably staying
late testing as much as possible
so we've done some very simple
prioritization on this tool internally
on our prototype so the first easiest
most obvious way is by frequency which
tests have come up the most what's
failed most frequently so we can see you
know looks like test seven in terms of
frequency it's come up the most so that
should probably be a higher priority
than ones below it you can also assign
some simple weights so here we've
assigned some simple weights to defects
and requirements and build correlations
this can be based off trust or where you
are in your project for instance if
you're early in your project you
probably are going to trust requirements
the requirements test set a little bit
more because you're not going to have a
lot of defect data you're not going to
have a lot of build data so hence the
test sets the tests returned by that
method are probably more useful so you
can wait those accordingly it may also
be possible that you just find just in
general for whatever reason the defect
method returns is is more consistent in
returning useful tests tests that fail
tests that show problems and if that's
the case you find that to be the case
then for whatever reason that's true you
can wait those accordingly so I want to
talk about effectiveness how do you
measure a tool such as how do you
measure effectiveness what metric would
you use well the first thing we're going
on the presumption that we want to run a
tests likely to fail that's what we're
looking for this is a bit of like test
philosophy so it's debatable certainly
you know a lot of people believe running
a thousand tests and having them all
pass you know makes you feel good right
things are working hopefully probably
and that's definitely good philosophy to
have and we're looking but we are
looking for tests that are likely to
fail because it's not something that we
want to re-release back into the field
so in order to do in order to track this
we want to we're looking at some using
some classic sort of security lingo I
was thinking later on this and I was
thinking that's probably not security
lingo but in
I have more of a security background so
that that's a little bit more prominent
for me so so given that we want to
select tests that are likely to fail
what is the worst case situation that
could happen that a test case is not
selected that would have failed had it
been selected so that's our false
positive meaning that it was rejected
but it should have been accepted now I
want to I want to keep it I want you to
keep in mind that this is a false
positive on the tool not the test if the
test itself is a false positive that's
no matter to us that's that's the
testers problem whoever whoever
implemented it in the first place we're
trying to measure effectiveness of the
tool itself so if your test case is
selected and did fail true positive
that's something that was accepted that
should have been accepted and in the
other two cases the true negative and
false negative we're not so interested
in because they pass either would have
passed or did pass in the test selection
it's not data that we are since we're
not looking for test cases that pass
that's not something we really want to
track so in this case we want to track
our false positive rate on this tool so
now in measuring effectiveness there are
a few different methods that we use we
recommend at first certainly a training
period meaning you run all of your tests
and compare you compare what happened
the result to what your tool suggested
so if you run all of your tests &amp;amp; test
cases one two and three failed and your
tool just fails to return test one two
and three that's bad that's a false
negative those are tests that should
have been selected but weren't for
whatever reason so you want to put it
through training period in order to
track your false positive rate getting
back to risk you're going to have to
find a level a false positive rate that
you're comfortable with I think we're
going somewhere between five and ten
percent on our team but whatever it is
you're comfortable with whatever you
to be realistic I guess is something
you're going to have to determine we
found a random sampling to be very
useful with the results set so the
results said right now the tests
returned are going to be directly from
whatever data mining method is is is
implemented however usually
supplementing that with a few extra
tests randomly selected just to see what
see what's happening make sure that
keeping you on your toes really is
useful in particular we find that if you
have a randomly sampled test that then
fails and obviously wasn't selected
originally by the test at that point
it's good to stop and do a full run
again of all your tests to track your
false positive rate to sort of rerun it
and and going in you know sort of
following on top of that you should do
regular audits meaning every once in a
while you know however however
frequently you need to run all of your
tests manual and automated and track
your current false positive rate so this
is again a nice little made-up graph to
help illustrate an idea meaning we
believe that the risk of releasing a
regression bug back into the field is
going to be much lower if you have if
you can achieve a low false positive
rate so it's just something to keep in
mind if you're going to implement a tool
like this question
so he asked would it be useful to
intentionally inject failures to see
what if the tests come up yes absolutely
that'd be awesome that's a good idea no
where's my pen and paper though i'm
going to write that down yes i think
that'd be actually that'd be a great
method determine effectiveness right
right great yeah well first off let me
get to the first part of that if you
have no failures that's pretty amazing
maybe two thumbs up we in that with that
in mind you can still mine let's say
requirements right requirements doesn't
depend the requirements method doesn't
depend on test failures it's going to
look at strictly here's a requirement
here the test case is associated with it
here are the source code changes
associated with it so that isn't
dependent upon test failures but yes so
moving forward have we looked at source
code so say that again however we looked
at source code coverage Yeah Yeah right
sure great great
my question if you go through birds
jewel now I can ask you to file okay
great so so he's suggesting that using a
code coverage tool you can associate
source code of tests absolutely yeah
that's a good idea no we're not doing
that at the moment again pen and paper
maybe I I was there another question
over here great yeah so exactly so it
suggested that if you have a basic block
that changes you want to run the test
cases associated with those basic blocks
that change right exactly that's that's
what that's what we're trying to achieve
so just a suggested use in the beginning
so assuming that this is is your basic
sort of build test system a build system
that then leads into some automated or
manual or both kind of testing and then
hopefully that information saved in some
sort of test management system we
suggest first just integrating it
directly into your build system and
seeing what task cases come out this is
this is going to be for your initial
training period and you'll probably want
to do this for a fair amount of time
before you feel comfortable with with
using these tests normally for the build
so sort of complementing your current
system with this with this extra system
you know basically I'm suggesting you
add more work on top of your current
work
so getting to some lessons learned so
the prototype that we built is still
relatively new relatively young we've
implemented two of the three systems of
the three methods the defect in the
build correlation method and we don't
have the scale of tests yet that this
will be very useful for in the scale of
hundreds now we're anticipating a scale
of ten thousands so that hopefully when
we get to that point in time a year or
two from now and we have thousands of
tests to run when we get to that
situation when we r you know have an
hour to test before you know some sort
of critical security bug needs to be
fixed immediately we'll have this in
place that we can go out and sit and see
what the suggested tests are that we
need to run through our thousands of
tests process changes are can be
difficult it kind of depends on your
environment and atmosphere and your
company if the processes are already in
place that's awesome if they're not it's
always a little bit harder to get those
in place and to enforce them and meaning
these process changes being in
particular let's say with new defects
you have to say in the defect which test
case failed and when source codes
checked in the fix that fixes that
defect it has to say I'm fixing this
defect if that information isn't present
somewhere it's going to be hard to
detect and again with the same thing
with requirements when source codes
checked in the fulfills requirements it
has to say somewhere oh I'm fulfilling
requirement X so that later on we can
say haha now a relationship can be made
random sampling is useful ah setting us
all false positive goal is useful
otherwise it's sort of like trying to
achieve that that enth degree of quality
it's impossible you have to at some
point find what you're comfortable with
and say well we're at six percent false
positive rate it's not perfect but good
enough random sampling I think is very
useful keeps you on your toes and
integrating these systems can be very
difficult if you you may not have access
to all the data you need i would say
work with whatever data you can get
access to
sort of schmooze the team's you need to
get access to these databases if
possible and mine the data for your own
usefulness so to conclude in those time
constrained situations that last-minute
change on Thursday night and you have to
release Friday we've it whether you know
for the desktop or the web even if it's
just like an internal milestone you
can't literally test everything it's
impossible even if it's not time
constraint maybe it's resource
constraint or system constraint if
you're build test system takes 12 hours
we only have six that probably means
your test system is going to have less
time so given that you need to we've
suggested methods to help reduce your
risk in these crunch time periods and
not only reduce your risk but help save
some of those human cycles I mean in the
end hopefully the the automated tests
you have aren't taking as long as the
manual tests the manual tests are going
to be much more expensive your human
time is much more expensive than your
computing time and so what we're hoping
is that you can save your human time for
more important problems like catching
butterflies for instance with that I
conclude my formal presentation and I am
here to take any questions that you have
No so the question was I suggested
12-hour build times but we only have six
hours to release those the time periods
I was working with no those are time
periods I've heard about though from
other companies people come and tell me
that o our build test cycle takes like
16 hours or a day or eight hours and
it's ours isn't there yet ours is
probably about two and a half hours but
it's still a younger project so it's
only going to go up let's things
generally don't decrease
environmental text but nine hours crane
but not like we don't break it on our
own but sometimes just the network
issues great okay Kennedy IBM verse
connoisseur control an inverter so
things are great so the question is how
do we help how do we work with
environmental problems things outside of
your control or outside the control of
the test let's say someone installed a
new GCC package that now is breaking all
of your tests well I guess in essence
I'll say that we don't and I don't think
this tool is trying to solve that
problem it has to assume it's some level
that the tests are correct write the
tests were written correctly and if and
that includes environmental problems
okay ah right okay yeah no that's sure
okay so he he said we probably want to
undo the association if it's breaking
for a reason that's that's not normal
that's not a part of the test yeah
that's true so I guess that's another
thing in terms of let's say a web
management console or something like
that that can help with these
associations there has to be a way to go
and backtrack and say oh this
association is actually false this is a
bad association to make I mean hopefully
if you have enough data over a long
period of time you'll be able to filter
that out or at least prioritize it lower
and say you know well it's you know
these test cases have failed a lot more
than the one random one that is that's
failed hopefully those are run before
the other one question
being integrated into the entire testing
process just for prioritizing what tests
to run first even if you're running the
entire suite into a usage do you use it
when you have sort of that crunch period
that you're talking about or this thing
with your developers so the question was
uh this will be useful in the be to
suggest some first set of tests to run
so is it run so let me get this right is
it run in only by us or in the full
testing cycle right ah right now we're
still you know maintain a nice level of
skepticism with this tool so we look at
the suggested tests and you know sort of
compare them to what we're going to test
and perhaps integrate them with our
current tests but it's certainly not a
replacement and it's it's it's in
addition to of what we're currently
doing yeah what we do use it I mean at
the very beginning yeah we see what it
suggests and then we run what we want
and then we also look at what it's yes
right so the question was what tools are
we using to plug into these the
requirements management's and defect
management tools well well so that's
actually we use an internal tool that
was built internally so because of that
we have full access to it to the
database to all the data I think we're
hoping that those dependencies will be
learned in in sort of an underlying
manner right if if this if this source
code if this source code change has a
dependency on another source code change
but they both break the same test
hopefully that that can be mined out we
aren't doing anything specifically for
that situation though well so I want to
mention something I forgot to mention
this idea of selecting better tests to
run in crunch periods is is definitely
becoming more and more important in this
sort of moving software-as-a-service
world where release times are getting
shorter and shorter I went to a talk
once by katarina fake who's one of the
Flickr people and she mentioned that at
one point they're releasing their tests
they're building testing and releasing
every half an hour now that was pretty
amazing I didn't want to stand up on cul
shenanigans on her in the middle of the
talk but it seems pretty like amazing if
you know if that's valid all right if
there aren't any more questions I'll be
out in the hall and available if you
have any more questions for me thank you
for coming
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>