<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Organizing the World's Scientific Knowledge to make it Universally Accessible and Powerful: | Coder Coacher - Coaching Coders</title><meta content="Organizing the World's Scientific Knowledge to make it Universally Accessible and Powerful: - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Organizing the World's Scientific Knowledge to make it Universally Accessible and Powerful:</b></h2><h5 class="post__date">2013-05-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DU5HRck4bn4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">basic the basic idea of what with the
underlying model that I'm trying to kind
of promote is this notion that science
scientific work is a cyclic process the
you start off with scientific knowledge
that is contained within the scientists
head most of the time they then analyze
and think about the work that they're
trying to do they come up with new ideas
new theories and they come up with good
scientific questions that are relevant
and important and and actually make a
difference in in in in in the subject
though asking in order for this to be
scientific work those questions need to
be addressable in an experimental way so
you have to actually have experimental
designs and protocols that test and
execute the processes for that and then
once you actually have your scientific
protocol you execute an experimental
design you actually populate the the day
you acquire data and you you gather the
data you get hold of it and then you
interpret it to present to add to your
scientific knowledge and you kind of and
so the whole purpose of what we're
trying to do is to figure out how we can
go around the cycle more quickly how can
we build tools and systems to accelerate
the process around this so called
knowledge turn and and the kind of key
question is how do you make scientific
discoveries what is the crucial element
of of making scientific discoveries well
in some ways we want to be able to
analyze existing knowledge that comes up
with important questions that then our
experiment a testable scientifically if
you think about it the kind of this is
the absolute crux of what science is
really revolving around and it comes
down to how do you find the right
questions okay how do you actually do
that now if you're someone who can weigh
the universe on the back of an envelope
this is not really a problem you're
someone like Einstein you can figure
this stuff out for yourself without too
much difficulty you could also be
someone like rosalind Franklin who is
the kind of poster child of someone who
did really serious diligent work really
solved the problem and that was
important and unfortunately wasn't
actually recognized for him she didn't
get the Nobel Prize for her work because
they don't offer those things
posthumously but instead these guys
Watson and Crick
able to come up with exactly the right
question at exactly the right time and
solve the problem in an in a really kind
of innovative and creative and
interesting way okay and in a way again
this is what computers can do perhaps
computers and computational technology
and knowledge engineering technology can
help us find those questions and answer
them in an inappropriate way which is
kind of cool so a notion that underlies
all of this work is this that is
actually interestingly missing from most
people's accounts of most people's
accounts within bioinformatics of how we
should address these kind of questions
is this notion of scientific paradigms
now I don't know if you guys are
familiar with the work of Thomas Kuhn
but in 1962 he wrote the seminal work
called the structure of scientific
revolutions in which he talks about
paradigms as being some accepted
examples of scientific practice examples
which include laws applications and
instrumentation together to provide
models from which spring coherent
traditions of scientific research and so
you know we have this kind of underlying
idea of what a paradigm is but what's
really interesting is how do we actually
transform the knowledge within a
paradigm shift within the framework of a
paradigm shift now so let's just go
through a kind of simple example to
illustrate what we mean by that paradigm
shift from elements to the elements to
the chemical elements and in fact this
is this is a kind of illustrative
example so typically what happens when
people come across a new phenomena such
as fire they they try and kind of
categorize the different types of fire
that they have they they they do stamp
collecting they kind of figure out okay
this type of fire is very green this
kind of fire is blue this kind of fire
is very bright that kind of thing and
and of course a lot of science actually
falls into this category the moment
people come across phenomena they don't
understand very well and they they try
and kind of explore it and figure out
how the different phenomena work and of
course when they do that scientists
typically will make up a theory and I'll
figure out they'll postulate some
mechanism that may or not be accurate so
in the case of fire of course the
original idea was that fire is
stuff called phlogiston that that has
some kind of properties like a fluid in
such like but of course it was the wrong
theory it didn't work so what happened
there is that the the paradigm can't
kind of got stuck it got got in a little
loop there the predictions that were
being made weren't accurate they weren't
the predictions that were being made
weren't consistent with the data that
was being gathered until of course
Lavoisier came along with the brilliant
idea and in fact the correct mechanism
was that the process of combustion
involves oxygen and carbon and produces
water and releases energy which is what
fire is all about now of course what's
interesting about that is that it's then
from this discovery then formed the
basis of a whole theory the theory of
kept modern chemistry and and once they
were able to do that then we were able
to export the theory as a kind of
structured reasoning framework something
that scientists in other fields could
use for their own purposes and of course
the periodic table is a great example of
that kind of thing and that's actually
the kind of thing that we're trying to
address within the knowledge engineering
work that I'm going to be talking about
today and of course once you've actually
got a theoretical framework which you
can operate within and work within then
you can develop new technology you can
do anything you like
you can actually build combustion
engines you can cure disease you can do
all sorts of stuff so one of the
important distinctions that I'd like to
make about this cycle this cyclic
process is this distinction between
interpretations on the left-hand side
and observations on the right now what's
interesting about this is that the
people who are kind of great at dealing
with these different processes are
different people right on the left you
have professor honeydew who is an expert
in the field he steeped in the knowledge
acquired over many generations
he knows the details of exactly how to
interpret everything and he's the guy
comes up with new experiments whereas on
the other hand you have beaker on the
other side who is able to reap who's
capable of reproducing the experimental
data he's able to execute a well-defined
scientific protocol but he doesn't
necessarily understand why and that's
okay that's actually that's actually in
our case is a good thing now the
distinctions of interpretations versus
observations within this context
is that the interpretations are
semantically complicated whereas the
observations involve only really data
semantics which is actually quite
tractable on the left-hand side you need
a human in the loop on the right hand
side you don't you can automate stuff on
the left hand side you have knowledge
that has to be specific to the
individual paradigm like if you're an
expert in neuroanatomy and you know how
the structure of the brain is is
functioned when you're trying to design
and develop experiments within that
field you're going to be pulling on that
kind of theory whereas a
neurophysiologist doesn't understand the
details that you would be using okay
whereas if you're just dealing with if
you're just trying to process in your
anatomical data you don't need to know
that stuff it's just data that's it's
nice and coherent and so the idea is
that the way in which we look at these
two different situations has to be a
little different in order to make
interpretations tractable you want to
scoped representation of the paradigm
that allows you to actually kind of
reason within the structure of that
paradigm effectively whereas if you're
working with observations as long as you
keep it restricted to data and
measurements then it becomes then it's
tractable you can calculate all sorts of
statistics and score effects quite
happily okay so that's an important
distinction to make and because we are
trying to keep it simple and because
we're trying to build this within a
computational framework we are going to
tackle first the observations and we're
going to address that kind of question
so the work that we do is based around
this kind of notion called
knowledge engineering from experimental
design I'll get to in a second okay the
idea is that this is a slide that you'll
probably see in scientific talks about
biology all over the place at the top is
this scientific statement that's just
oops that's just here that would be an
interpretive a statement saying
something like rats eat cheese when
they're hungry or something fairly
straightforward and it's usually based
upon data and the data is going to be
based upon measurements of a dependent
variable and the settings that you set
within your experiment before the
experiment starts to test your effects
as independent variables across the
board so here the the kind of important
criteria that you're looking for
is there is a difference between
conditions D and E and the inn and it's
illustrated by the measurements of the
dependent variable okay so so that's
actually how scientists think about
their data most of the time this is the
underlying conceptual model that they
use to design and build experiments and
to actually do though technical work and
and what we've done is we've tried to
extrapolate away from that to provide a
simplified framework for scientists to
be able to describe what they're up to
using these representational elements
using processes material entities
parameters constants measurements -
brought by two points and four points
and given this kind of vocabulary of
conceptual structures it's actually not
too difficult to get scientists to
describe their protocol and and to draw
out their protocol in the way that I've
just shown you okay so and actually I
gave a talk at a neuro science
undergraduate lecture and I showed them
this slide may ask them to put their
hands up and I said how many of you feel
comfortable that you could do that and
most of them did which I was like great
that's awesome okay so now what's
interesting about this if we're in if
we're trying to extract the
relationships between dependent
variables and independent variables
measurements and parameters we get that
for free from this kind of diagram
because in fact we're able to trace back
through the structure of the the
measurements to see which are the
variables that actually parameterize the
process upon which the the the
measurements were taken so in fact you
can see that measurement one is
dependent upon parameter one and
parameter 3 but not dependent upon
parameter two because it lies on a
different arc of the of the protocol
okay now that gives us our relationships
between independent dependent variables
that can form the basis of a data
representation which is kind of cool we
actually have a system that we've built
called bio SCARA that in fact provides a
drawing palette that scientists can use
to draw out the the protocol that
they're actually using so here this is a
very simple experiment you take my OC
measures blood pressure nothing
complicated about that and the system
allows you to
describe the various different variables
of different stages with different
datatypes and suchlike very happily we
were able to publish this as a paper can
I get this some reason Li here we go
so you see on the right hand side the
reference to a paper in BMC
bioinformatics and I just like to point
out the first author of this is sitting
right here this is Tom Russ thank you
who was working with me on this work
previously so and this is a highly cited
paper within the community so we
actually got some impact people were
reading and it was quite quite good to
see okay so okay so
so having kind of set up the structure
of this knowledge representation where
we're trying to capture the primary
measurement see by using provenance
relationships through the protocol we
then confront the the unfortunate fact
that data processing transforms data
well obviously that's not surprising um
but but if we're able to actually
capture that as well by seeing if you if
you look at this representation on the
graph the idea is this is an
experimental protocol that consists of
taking the MMSE questionnaire which is
the mini-mental State Exam which is a
test of cognitive ability that people
will take if they if they're suffering
from Alzheimer's and you can actually
see that the the data structure is
relatively simple you have your score
followed by your ID number for the
subject and then the group that you're
in and if you want to calculate the mean
values of this specific measurement
which is an important step to take you
then have to the process of taking the
of calculating the mean will average out
the IDs
no longer the individuals will be
measured there so so by mapping the
inputs and outputs of the each
individual data processing we can
construct a representation for four
actually
that will support a knowledge
representation for the following
elements for primary measurements for
mean values for statistic effects
between groups which we're showing at
the bottom here for the MMSE Zenzi score
and of course correlations which in not
here so the idea is that all of the kind
of statistical relationships that you
might see in a paper if we adhere to
this kind of modeling approach we should
be able to capture and represent quite
cleanly and easily within this approach
now how expressive is this how good is
this as a methodology now you can see
this is a paper that's concerned this is
a bigger data figure from paper
concerned with looking at vaccine
responses to HIV to an HIV vaccine and
what you're looking at here is a it's a
slide that shows kind of a horrible data
slide right but this is not completely
atypical of the kind of things you see
in scientific papers the thing that
you're looking at is that the black dots
are supposed to be higher than the white
dots the black dot shows an immune
response under conditions of the vaccine
and the white are a control okay now if
you draw out the entire protocol using
the kind of conceptual framework that we
described previously you get this
complex representation and in fact the
the measurement that we're looking at is
shown by the number five the circled
measurement in the diagram and if you
trace back through the protocol through
every single individual parameter that
that indexes that measurement they
correspond exactly to the measurements
shown in the correspond exactly to the
axes of this diagram shown here which is
very encouraging okay so so we think
that given the complexity of this data
so we were actually able to go through
this paper and curate all of the data
points every individual data points from
this paper into a database
representation which is actually really
really cool because that's actually very
very difficult to do using other
database kin methodologies and data
modeling approaches okay so so that's
the kind of core that some of our core
technology that's the kind of approach
that we're trying to take now we're also
trying to think about okay how do we
apply this approach how do we apply this
approach across a whole field now given
the complexity of the different types of
research that people do we don't know
the number of paradigms that people are
working with them there are all sorts of
different ways in which people doing I
do
data data analysis and and three
examples of the kind of data formats
that people use a shown here human
neuroimaging
protein proteins interactions gene
expression all of them involve different
types of data but they also also involve
different kind of hidden intrinsic
methodologies and assumptions that
people make when they're driving the
kind of research that they're doing okay
so we want to know how do the how do
these different well let's ask the
question how do we figure out how many
paradigms there are well you guys are
all very familiar with the topic
modeling as a methodology maybe we could
take all that all of the text within a
specific corpus of of scientific papers
and try and automatically discover the
kinds of paradigms that are women this
is an explanatory slide that I think
after talking to Brian it's probably
completely unnecessary the idea is
obviously topic modeling involves taking
documents like this this is just an
abstract from the New York Society for
Neuroscience meeting a few years ago and
what topic modeling allows you to do is
to take is to cap is to identify papers
I'm sorry words that tend to reoccur
together in the same document again and
again and again so if you're if you're
working on Parkinson's disease for
example you'll see these kinds of words
like nigra duck and Minaj ik substantial
nigra all co-occurring within the same
document and so by doing topic modeling
you can then model the structure of the
document itself as a mixture of
different topics distributed over the
entire corpus and that gives you a
framework within which you can then
perform statistical analyses on the
papers that much more easily and what we
did was we wanted to kind of provide a
graphical representation of the layout
of a subject that scientists could
eyeball and examine and so what we did
was we took a literature corpus we
generate a topic model from him will
then calculate document document
similarity scores based upon comparing
the different topics themselves for each
document and then we actually embedded
this in a Google Maps application to
provide a kind of a layout that you can
then zoom into and look at and
Simon and this is actually quite old
work we did this back in 2007 and this
is a cluster map of the 2006 Society for
Neuroscience meeting which is actually
kind of interesting semantically as well
this is a conference that about three
hundred thirty thousand different
scientists come to every year it is the
primary main conference for all of
neuroscientists across the world and and
in a way it's kind of like a snapshot of
the state of the art at that point which
is kind of interesting and all we're
showing here is the is this layout that
you can kind of zoom into that the nodes
are colored by the different the
different themes that they were
classified and they were manually
classified into within the conference
description and so you can see that the
it forms these clusters and and it's an
interesting idea to think can these
clusters of different nodes be used as
the basis of defining or investigating
or looking for paradigms and this is
what the stone going we haven't really
developed this a great deal but this is
an example of the kind of work that
Google could in fact do obviously at
scale rather than dealing with the
11,000 or 12,000 documents that I'm
looking at here the Google could
certainly examine for example the 20
million documents that occurred within
pet within PubMed
these are things that that that you know
I'm trying to kind of throw into your
space as possible avenues and ideas for
research that you might be interested in
picking up okay another example of this
is the NIH Maps org project which was
work that we which was derived from our
original idea we work with Natalia and
with a company called chalk labs to
develop this application and actually I
should I definitely want to make it very
clear that we had no real part in the
development work here this is all
chopped labs development work but
nonetheless is kind of inspired by our
original idea what you're looking at
here is a representation of the 80,000
or so grounds that were actually funded
last year by NIH across the entire by
the whole US government and what we're
able to do is query and search for
grants that contain 30% or more of their
words classified as Parkinson's disease
relevant words and you can see that the
you zoom in and you actually get a small
cluster of papers that are concerned
with deep brain stimulation which is a
specific methodology so in theory
everything within this kind of cluster
may be relevant to the same kind of
thing that could define a paradigm kind
of interesting and it's also relevant
because these guys are all funded right
so it's not just a simple theoretical
question about ok who's doing this work
these guys are all actively processing
and developing the same kind of ideas
here's a this is kind of a little hard
to see I apologize but this is a
breakdown of the different categories of
topics that occur within this corpus and
they they cover the gamut and of course
this is quite interesting because then
you can actually see what the literature
is telling you as are the most prominent
words that are being used by scientists
rather than ontology x' that are defined
by a by informatica sees the actual
words that are in the documents so
that's another interesting Avenue of
research that people that we could all
kind of think about and work on so so
now I want to talk about something evil
dark data don't die right and of course
if you've not heard the term before dark
data is an allusion to the concept of
dark matter which is data that is re
matter that exists but the matter that
we can't see it's kind of out there
somewhere we know it's out there we have
no idea what it is which is kind of cool
but it's present now when you're looking
about when you're thinking about data
and dark data this is data that exists
we know that it's in people's labs it's
in people's notebooks it's in people's
filing cabinets in proprietary file
formats and all these different places
but we can't get hold of it and we can't
see it and the fact is that so much data
is kind of hidden away and the
infrastructure that we currently work
within doesn't actually promote the
sharing of data very effectively it does
so a little bit but not very effectively
um it has a big impact last month I went
to a meeting in Oxford where
we're talking about scientific integrity
and scientific rigor and the whole
process of data sharing and this guy has
bouncer kind of gave up stood up and
talked for about 15 minutes without any
slide present without any slides at all
and he gave this wonderful impassioned
speech about how destructive the process
of concealing data from each other is
within the pharmaceutical industry and
so he heads the Oxford division of the
structural genomics consortium which
tries to provide a pre-competitive space
for drug discovery one of the and he's
actually getting a lot of really
supportive money from pharmaceutical
companies who are willing to chip in to
try and basically support data sharing
at the at the stage before which drugs
become competitive so there's all sorts
of preliminary trials that people want
to do to make sure that molecules are
valuable targets for actually developing
drugs for and he's trying to kind of
develop the idea of being able to save
some money for doing this and and and so
obviously the quote is it's harder to
get a working treatments by Alzheimer's
disease and putting a man on Mars the
reason he says this is because according
to his estimates as far as I remember I
could be wrong about this he said that
it costs thirty billion dollars or
thirty billion pounds a time in which
one it was for all the different
researchers gone into trying to
understand Alzheimer's disease and they
still don't have a working drug like all
of the stage two pharmaceutical trials
that have taken place have all failed
which if you think about it is
horrifying okay we should be able to do
better than that
really but specifically for Alzheimer's
disease okay okay so there was
apparently there are some more there are
recent failures in the last couple of
days for Alzheimer's disease and drug
development so okay this is a very
topical question as well a very serious
one and so you know the people in the
knowledge engineering community and the
Semantic Web community have have put
together the linked open data cloud it's
a little blurry
I'm afraid I'm sorry you can't see it
very well and within that space there
are a number of
different projects and their life
sciences and essentially what this is
all about is they these guys take these
large on online open databases and they
dump them into IDF and they look for
ways in which you can make connections
between them which i think is a very
very valuable starting point to
demonstrate how data can be linked and
data can be made interoperable in a very
open way however having said that the
representation that they use is not
doesn't make any distinctions of the
kind that I just described at the
beginning of the talk if there are
interpretations and observational data
they tend to be mushed together and you
need to know the underlying schema of
the database to be able to make sense of
them so in fact even though and of
course the the the different datasets
that are contained within the link to
open data cloud are all basically
representing that the data that's
contained within the databases is stance
so you have all of the problems of
linking together terminology and trying
to understand how the different data
schemas are representing their data and
suchlike so we haven't solved some of
the major conceptual problems about how
this kind of representation should be
constructed and should be framed in a
paradigm specific way and they would
actually allow us to reason over and
build representations for specific
scientific problems but but I think that
this is a good start and this is an
interesting way this is kind of a
something this is the way things should
be in the longer term like sign this is
a framework within which we could
publish our data and actually this could
make a difference in the longer term at
the moment this hasn't necessarily
really proven to be a game-changing
technology but nonetheless you know the
promise is there at some point in the
future and now I'd like to also mention
an approach that's being put forward by
Perrin Monts and an extended community
of people which is called nano
publications so the idea is that you can
represent you can find a representation
in RDF that contains a named graph that
stores a scientific assertion or a
scientific data set some form of
representation
that could be fairly general purpose and
could be from lots of different types of
format that you can include there but
you have a standard representation for
the provenance and for the supporting
data that allows you to link together
different data types and I think this is
a really valuable and interesting idea
and is actually a way in which we can
try and think about transforming the way
in which scientific data is published
and shared as part of the whole
publishing cycle and some examples of
this is you could have a data point
linked to us data paper citation you
could have a representation of a
statistical effect or you can have an
interpretation based upon k-fed data and
these are all obviously conceptual ideas
they're not we haven't gotten worked
examples and nano publications to show
you yet but watch this space this is
something that we want to work on okay
so having talked about this and having
talked about the underlying k-fed
representation and all of these various
different other components I wanted to
kind of bring it home by talking about a
real scientific problem and actually I'm
talking about how one might be able to
encapsulate and really represent a
specific scientific paradigm in a
coherent way okay so talking about
biomarkers who here has heard of
biomarkers as a representation one guy
one person great thank you
okay so so the idea is and actually
biomarkers is a very well understood
methodology within biology the idea is
that given an individual in some kind of
disease state over time optional can is
this okay okay so in some biomarkers the
idea is that you have various different
types of measurement that you make on
that individual over time that actually
allows you to track various different
aspects of the disease and so here's an
example of a numeric biomarker of course
this is all just cartoons
or a binary biomarker that shows a state
of either being off or on and imaging
biomarkers that actually indicates you
know the MRI images that reflect a
person's structure of the brain and the
idea is that essentially this whole the
the the way in which the biologists
think about this is they're looking for
an indicator test they want to find
something that indicates the transition
from a normal to a preclinical state and
they want to find a test that you can
deliver as a blood test or a
questionnaire or something that's
indicative that this person may have a
disease of some kind okay and so the
what's interesting about that is that if
you think about this the you have an
individual who state changes in discrete
stages over time and you have a whole
bunch of features that indicate
measurements of some aspect of that
individuals in measurable quantities
over time as well now it doesn't that
sound like a hidden Markov model to you
isn't that something that computer
computer science can actually address in
a very tractable and easy and and
straightforward way well not necessarily
easy and straightforward because the
features you're looking at are very
complicated but nonetheless it actually
falls within the domain of computer
science you can dress that kind of
question so so the kind of the goal in a
way of what we're trying to do by
shaping and representing the data that
we get access to is to make it available
and accessible to people like you to
computer scientists to experts who are
really really really good at data data
analysis at scale and can actually
address these kind of questions
okay so let's then see where this is
apply applicable to maybe so there are
these two projects called the
Alzheimer's disease nearly informatics
neuroimaging initiative or Adney and a
more recent attempt to analyze and look
at Parkinson's disease data in the same
way called the PMI the parkinson's
progressive markers initiative now these
two projects
kind of interesting their large-scale
multi-site projects didn't involve
clinical sites across the world and lots
and lots of different places where they
have to coordinate very carefully the
data acquisition and the representation
of different data from different places
though they're actually exemplar really
really good at LAN and the idea is that
they simply capture and track 200
patients in an in a disease group and a
non disease group over time over a
period of five years and they capture
imaging data they captured blood they
actually do spinal taps and and and take
cerebrospinal fluid and freeze it so
that people can do subsidiary secondary
backup studies to look for specific
things within within the tissue they do
questionnaires they do clinical assays
they look for various different any kind
of clues that they can find that may
suggest a transition and the progression
of the disease and importantly they
actually provide access to all the data
and they provide access to the samples
if you look you have to go through a
rigorous application procedure to get
access to this stuff but it's available
for anybody so these guys are really
setting the stage and pushing these
things forward and really kind of having
a big impact and of course surprisingly
enough all and rather not surprisingly
enough these projects are really
successful they actually are there's a
paper out by the guys who run
Adney Abney is now and its second
findings it's a more international
consortium now it's it's globalizing its
various keling itself up quite well and
they published a paper that shows
someone like 200 papers were published
around this individual study originally
and and and and of course the impact
they have on secondary studies is even
greater so i'm exactly yeah exactly
they're saying look we want you to
publish we want you to leverage this
stuff we want you to succeed okay so by
taking this kind of process the the the
people who developed a denie really
pioneered the whole kind of data sharing
framework and they did so in a way
that's actually kind of interesting
because 200 patients per group is not
massively scalable it's not at a
level of you know 10 million the
population of China or something like
that it really but nonetheless it's
actually able to solve these problems
and actually really make some progress
about trying to detect out sinus disease
in people um
the PPM eye project is a little earlier
is a little younger it's only been
around for a short period of time it's
it's um again it's a large-scale
international consortium of 24 separate
clinical groups all from working
together it's sponsored by the Michael J
Fox foundation and actually is kind of
and this is the the main presentation of
the from the website the P PMI and so
they have a whole complex governance
structure where they have different
working groups developing and building
these things and of course what's
interesting about this too is that all
the researchers who are working on this
problem are working on their own
problems in their labs so you have these
top researchers from these different
clinical labs all contributing to the
centralized project but that's not all
they're doing and so the the the
community built by this project itself
is able to kind of share data and
knock-on effect as well so I'm I think
that's something that we kind of
underestimate the impact of and it's
actually one of the reasons why I'm very
excited to try and work with these guys
now what's interesting about this as
well is they're sharing the data but the
data is all shared as CSV files text
readable CSV files usually with there's
a data dictionary that you can look
stuff up and and all of the the data
descriptions are written down in PDF
files so that you can actually learn and
understand the data if you're an expert
and if you take the time but for a
machine readable approach using Semantic
Web knowledge engineering it's actually
not enough they need to innovate we need
to bring our technology into this domain
and we need to be able to kind of
actually model the process using the
kind of tools that we've done so we've
started this work this is a k-fed
knowledge engineering from experimental
design model of PMI this is completely
unreadable so let's make it a little bit
bigger
and then you have that actually and then
you have a whole bunch of the various
but nonetheless we're able to capture
for example the structure of the data
that's being measured and represented
with an original data set so I'm very
encouraged about this and I think that
this actually provides us with a
framework that then we could publish
nano publications based upon P PMI data
according based assuming of course that
we get permission to do so we have to
apply through their internal processes
to make sure that that's okay then
excitingly enough what I'd like to do is
to present all of this data in a machine
readable form the people like you can
then process and so we could actually
present this as a framed data set that
anyone could have a crack at with
respect to using hidden Markov models or
CRFs or any kind of data modeling kind
of approach that we have so so in a way
again this this illustrates the role
that I think the work that we're doing
actually provides by being a bridge but
between by being a framework for
capturing the semantics of the
biomedical processing in in enough
detail so that we're able to kind of
actually capture the underlying data and
representation in order to solve
problems but not so much detail that we
start trying to invoke all the knowledge
that these guys carry around in their
heads because there's no way we can do
that but then we want to package and
present it to you guys so in a way then
then exposes all of the data for all the
sorts of kind of life scale
complex and interesting machine learning
kinds of analysis that then that are in
your toolbox so that's kind of part of
the the mission of what we're trying to
do and how we can actively pursue this
and actually get some make some progress
and maybe even cure some of these
diseases okay so going a little bit
further that's the the main body of the
talk but looking further on off into the
future a couple of ideas for you to
consider so first of all and this is a
slide that describes this is genetics
you have very different proteins
channels in cell surfaces Sciences
microcircuits neurons it's whole brain
structures then in the havior and of
course your disease like Parkinson's
disease actually affects all levels of
this framework you have to be able to
look at the research at all different
levels and in fact the kinds of
questions if we're interested in
studying not just the correlative
effects of these different symptoms
which is what biomarkers is really doing
if we want to start digging into the
causes of how the disease works then we
have to start looking at these various
different elements so look to is a gene
and in fact I think the gene sergey brin
has a mutation form that is part of the
reason why he's actually interested in
studying parkinson's disease and trying
to find a cure himself there is also
work done part of the reason why
Parkinson's disease causes damage is
it's based upon aggregations of these
various different malformed proteins
that they grow and and and cause
problems and and and basically kill
ourselves we don't know how we don't
know why but the processes that underlie
that is somehow related to the way in
which the proteins fault again we don't
understand exactly how that works
there's a whole that itself is a
paradigm people work within that field
their entire lives trying to understand
that whole process then the process of
but let me get this right or toughie gee
I can never pronounce that toughie G is
the process by which your body actually
consumes and eats up the proteins that
get made and that's broken in some way
with respect to Parkinson's disease
there is evidence that that suggests
that mitochondria might be implicated in
this kind of process there's evidence
that inflammation the process of
information within the brain actually
have a role to play within the causes of
Parkinson's disease but we don't exactly
know how and of course we do know that
at the level of brain regions and cell
populations as a specific set of neurons
that are damaged within Parkinson's
patients that causes movement disorders
and other sorts of long-term devastating
horrifying effects that we want to try
and understand we want to try and stop
okay so in order for us to go further we
have to start building representations
and knowledge engineering approaches of
these various different aspects of the
of the these different research fields
and we have to find connections between
them we have to try and synthesize their
knowledge so that we can actually
understand and cure this disease that's
kind of what we're up to in the longer
term so coming here I was wondering how
would Google kind of tackle this I
thought you know my question was is like
if we were gonna go Google on this
problem what would what would that
entail what might that look like
and so forgive me for being a bit
fanciful but but um I thought well what
it would be cool if we had a super
massive repository of all that
scientific data in the world like we
found a place where we could shove
everything and we put all the different
scientific observations from every
single scientific experiment work that
has ever been done we put it in first of
all question would be how would we
populate it well since this is Google
you guys would develop really beautiful
data management and analysis tools that
would deliver scientific expertise to
the to the scientists working in the lab
and in a way that would transform the
subject so I was thinking about this is
like I remember the days when I was
driving around La with my Thomas guide
on my lap and and and and and not that I
would do that of course but but um but
you know the experience of doing that as
opposed to the experience of using
Google Maps to kind of examine how one
guesstimate to be was transformative
right perhaps maybe there's something
that we can do within the scientific
within a scientists experience of doing
work within a lab that would be
analogous that would be cool
of course less a lot of data in
databases already perhaps you know you
guys can configure out ways of doing the
information integration sucking
information from these open open access
databases anyone can do anything with
the data from these things maybe there's
some interesting and interest
interesting and really fantastic ways in
which we can pull data from them and of
course the scientific literature which
is you know effectively a 16th thing a
16th century artifact right these things
are PDF files they're accessible way
electronically but it's still just text
on a page now maybe there's ways in
which we can transform the way in which
these things get written maybe in
there's ways in which we can inform we
can pull the information out more
aggressive more interestingly more
powerfully there's all sorts of things
that we could do that and then of course
once we gotten this system what would
then be possible well you could do
terminology analysis you could actually
develop ontology that reflects
scientists use so that scientists would
actually be willing to use the
terminologies that and usually
standardized terms in an easy and
straightforward very natural way perhaps
those ways in which we can do meta
analyses across studies which still
remains a very very difficult and
unsolved problem in general for
scientists to do you know trying to
actually get the data from the on paper
and compare it to the data from another
paper and being able to make the
connections between them not an easy
problem you know with with this kind of
repository that would be easy
let me straight forward then imagine for
example oh yeah we could scale up a
representation of data massively so if
you want to do systems wide large-scale
systems biology analyses of all the
proteins involved in a specific disease
bang you'd be able to do it in no time
at all and then I'm kind of excited
about this idea judea pearl is a giant
in the field he's he's someone who
developed
probabilistic graphical models and and
and really made it made a huge
difference now some of his work is based
upon the idea of representing causality
in a computer being able to actually
look for causal relationships between
within the structures that he's
developed now maybe we could use that
and we could leverage that in studying
how things would really work
maybe there's actually some some
underlying long term kind of interesting
research that we could really leverage
there and then of course we could build
a breakthrough machine okay I don't I
don't really know what that looks like
but that's kind of what I'm up to that's
what we're trying to do maybe there's a
way in which we can automate the process
of doing scientific breakthroughs we can
actually turn that into an engineering
problem okay so in that in that in that
vein I'm organizing I'm one of the
organizers of this workshop the
discovery informatics workshop well
there's a workshop at the triple AI
seminar at the triple AI autumn sorry
fall symposium series coming up in
November deadlines for this arena are in
one month we're very very excited to try
and get people involved and interested
and we're also looking for people who
might want to act as keynote speakers as
well so if there are people who could be
potential speakers for this we would be
very interested in talking to you and
we're kind of excited about this whole
thing so just to summarize I've gone
through a lot of stuff and I've talked
quite quickly but I just wanted to kind
of summarize wrap it all up in a bow for
you guys so first of all I've talked
about constructive developments in terms
of this cycle of thinking about
scientific scientific process as a cycle
and we want to try and represent and
think about paradigms explicitly and
this is I think an important
breakthrough that no one really is doing
in an explicit way we have our own
methodology knowledge engineering from
experimental design which is the whole
way in which we can act accurately
capture data from scientific papers and
scientific studies in the way that they
were originally intended
we have we talked a little bit about
topic models of the scientific
literature as a methodology for
identifying examining paradigms there's
also obviously other types of
information extraction work and all
sorts of in natural language processing
that can be done in this in this realm
that I think are very interesting
avenues of research we want to try and
advocate open standards to help
alleviate the problem of dark data and
then finally I'm proposing this specific
project as a target kind of goal for
actually trying to address and support
the parkinson's research community in
their in their work this is thanks to a
lot of people we are obviously I have a
lot of people to thank this is just a
small sub sample and thank you for your
attention here's my email address my
website and my blog and feel free to get
in contact with me if you have any
questions or if you want to if you have
any ideas
thank you I can repeat the question -
yeah absolutely
so Marty Hearst at I think at Stanford
is actually developing she came up with
the notion of a Satan's a citation
sentence as being a kind of unit of
representation you can do this kind of
work with I'm not sure if she's
specifically
looking at the kind of questions that
your you've just described but I think
it's kind of in her wheelhouse there's
also an ontology of citations called
Sato
being developed by David Shattuck at
Oxford and he's trying to provide a
language for describing the different
roles of a citation when one is
supporting another but what you're
saying is specifically just calculating
the distance metrics within the text as
a way of looking at these things I
haven't heard that before interesting I
did thank aware its concept versus the
knowledge graph and it seems as though
insight the citation graph seems to be
traditionally associated with some kind
of knowledge graph when actually maybe
more power is simply a physical
proximity of citations if you want to
scale yeah I haven't heard about the
physical location but I think that's an
interesting idea I think personally the
way my citations are used is is really
undistinguished
I mean obviously you just point to a
paper and you have some underlying
theory of why it is you're pointing to
there but there's no way of really
telling why or what the purpose of that
is so I think yeah I mean and also the
other the other aspect of this is that
doing text mining of a full text is
actually difficult because of licensing
concerns so we have to be able to forget
the pub we have to be able to get the
text we have to be able to extract the
text from the documents that that it's
contained within we have to be able to
recognize citation sentences and then we
can actually start addressing the kind
of questions yeah how I I don't feel as
though I've really effectively answered
your question
okay I'm sorry the description for data
can be discrete actually depends on the
instrument it was taken and so on and so
on so increasingly we're taking data
which make sense without right so how do
you think right that's a good question
so um I think that one size does not fit
all and that you have to tailor the
representation of the data that you're
capturing in a way that's appropriate
for the task that you need the data at
that time because frankly one of the
problems that we have is that you know
the process of knowledge acquisition
from these things is a bit of a rabbit
hole you can go down as deep as you like
into the details of things and unless
it's going to be useful for you in the
immediate future you're not really going
to be able to get people to do that so
um my answer is that I don't I don't
know what level you have to take it out
but I think you want to aim for a kind
of process of just getting just enough
as as the as the way of doing this
because if you don't do that if you dive
down into as deep as you possibly can
get then the process of curating the
information becomes pejoratively
difficult
and no one will do it so you have to
kind of tailor your approach and and of
course what you was what you need as
well is a kind of multi-level
representation of the workflow so in
fact for example take an example let's
say that we're doing a secret genetic
sequencing process you could put that in
a single node in the process graph as
we've described or you could explode
that into all the different sub steps
that you need to take within that
process so from our perspective the
representation should be able to say
okay this lot this single process is
actually made up of all these sub
processes and have it linked together so
that then you can retrieve you can you
could you could in theory curate both
first when you when you first make the
analysis you want to curate it to the
maximum the maximum of your ability at
that time but then let's say that you
want it to do it all down and for the
data later on you would want to explode
out that process into the different
subsections and do that so I don't
really have a good answer to that
because but in theory I mean the thing
that in theory if you're automating this
process and automating the processes of
data acquisition just shove it all you
want to capture everything right I mean
that would be the ideal process but then
you'd need a representation that allows
you to make sense of that and as you say
10 years down the line there are some
experiments that you can't repeat so in
those experiments you have to try and
capture as much data as you possibly can
because it's incredibly valuable to be
able to do so so you know that that's a
that's a very important question and
something that I think scientists in the
future are gonna have to deal with yeah
a quick question from VC
can you hear me so so you described you
know representing the data acquisition
process it sounded like a essentially a
manual effort to do this to create these
flow diagrams and you mentioned it could
be multi scale and so on so once you've
created this by hand then what do you
have algorithms that process this graph
representation that you've created or
does it somehow add in what way does it
had been add value to the raw data
okay so we're currently in the protein
so all this is development work and we
are we have okay so at the moment what
we've done is we were able to actually
capture the organization of the protocol
and we're trying to develop the
underlying infrastructure for being able
to represent data at each stage it's
still very nascent we're still trying to
look for funding to be able to develop
this further and make this work more
coherently um the way I think about this
in the future so this is again a future
direction kind of argument is that
imagine for example that the processes
that we're dealing with are kind of like
a grammar so you could a compositional
grammar to build experimental designs
for a whole class of different
experiments and so the kind of
underlying theoretical model that I have
in my mind about how we can then process
this data is that that you can look at
the different kind of fragments of an
experiment that of dealing with
measuring specific things and see how
they combine which is essentially the
kind of underlying this is the reason
why this is a complicated problem and
that normal data modeling methods don't
work because each experimental design
when taken as a whole involves composing
different pieces of the same underlying
design right and so you know if we are
able to construct a grammar based
approach to represent this type of data
then perhaps we can use that to analyze
the underlying data and the underlying
touches and then maybe run I have a
bunch of graphs of work that we've done
for example and automating the
comparison of different studies in
studies of HIV vaccines and and so for
example instead of a lay out the kind of
graph based layout of papers based upon
their the similarities in topics we
could for example drive analysis and a
visualization of the different types of
experiments because of similarities in
their in their underlying design so
that's one aspect of how I think this
kind of work could be used to help
evaluate and understand the data within
a field but it is still very early days
and we're still looking to try and
develop the ideas base okay make sense
Thanks thank you any other questions
yes I think that that's a really
interesting question we did some work on
looking of a specific class of
experiments called track racing
experiments which study connections in
the brain and we found good results when
we knew what the independent and the
dependent variables were so if you look
at the results sections of papers and
you have a predefined idea of what it is
the measurements that are needed to be
used in this type of experiment so for
example labeling in brain tissue or the
location of an injection or that kind of
specific type of chemical that's being
used within the protocol that boils down
to a simple information extraction
problem so you get the same kind of
performance that you have when you're
trying to pull out names of companies
for example it's the same kind of
process so we think that we can do that
quite efficiently but we need to have
the underlying models defined for an
experiment for a specific experimental
type and of course that's an interesting
question because some experimental types
are much more complicated than others
for example I mean I've just been so the
work that we were I've started looking
at cell biology studies of cellular
processes to try and get at ways in
which we can look at for example how
look to is how luck 2 is influencing
different aspects of cell biology is
part of the kind of trying to study the
causes of Parkinson's disease
now their representation of experiments
in cell biology is completely different
from the kind of thing that you see in
in in vaccine studies vaccine studies
you basically run an experiment and over
uh you have a very complex design for a
single process whereas in cell biology
you have twenty five separate
experiments or very simple ones where
you basically get us guess themselves
you incubate them
you you run a gel you see what it looks
like you then do the same thing again by
tweaking some of the parameters and and
and so the Underwood's ink so that it's
interesting and it's encouraging because
we think that k-fed the k-fed modeling
approach could easily handle that
without too much difficulty so you have
a lightweight framework for capturing
those kinds of experiments but again if
you're trying to model that if you're
trying to automate the process of a quiz
of pulling that information from the
literature it's a whole different kind
of ball of wax you then have to identify
you have to identify experiment
boundaries within the paper you have to
look for different things so I think it
really comes but that will also come
down to how you prepare the background
knowledge of what experiment types
you're looking for and how you would use
that to see the process of developing an
information extraction engine across the
paper but but I think that that what's
interesting about that is that looking
for the variables and the measurements
is a really powerful method</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>