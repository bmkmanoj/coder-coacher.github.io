<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Viewfinder: How to Seamlessly &quot;Flickrize&quot; Google Earth / a collaboration betw... | Coder Coacher - Coaching Coders</title><meta content="Viewfinder: How to Seamlessly &quot;Flickrize&quot; Google Earth / a collaboration betw... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Viewfinder: How to Seamlessly &quot;Flickrize&quot; Google Earth / a collaboration betw...</b></h2><h5 class="post__date">2008-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LRAj21y23r8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's my privilege this afternoon to
introduce my good friend Michael named
arc who is responsible for a lot of the
work
see he's been in the city digitizing
business for about 30 years when he
worked on the Aspen project since that
time he's digitized things with various
apparatus including a backpack from
ancora to Timbuktu and he's now on the
faculty of USC's Interactive Media
Studies program Michael named art thank
you very much now are we am I gonna have
to hold this I don't need this because
you're making me good I don't need to
turn this off either okay yeah sorry
about the AV stuff you'd think we'd be
better by now um Thank You Lance ah it's
a pleasure to be working with you and to
be here at Google presenting this kind
of crazy project that we've been doing
for the past five months intensively it
really is a collaborative project
between the primarily scientists and
engineers at the Institute for Creative
Technologies and more art artists and
designers in the film school basically
interactive media division my old boss
at interval research David Liddell used
to joke that you know how you get
artists designers and computer
scientists to work together you put them
in the same building and it turns out
that we had pizza lunches every Friday
and it helped we just it was a cultural
thing that I hope you all appreciate
just getting to know each other's
priorities and languages and things like
that this was also of course produced
with the support of a Google research
award I'd like to thank Jeff Wilson John
Hanke for that opportunity when I had a
meeting here a while ago and I had a
very short amount of time and I didn't
want to introduce myself through the
normal bio I thought I put together a
quote feeling lucky list and they said
what's that and I said you know when you
are number when your stuff is number one
on Google searches and they said never
heard of that before so um the one
that's not on here now is that if you
google feeling lucky
it's also number one for the work I do
but I do think that this is sort of a
more sincere unless egocentric
alternative to sort of you know BIOS and
stuff you find her his novel method for
users to spatially situate or find the
pose of their photographs and then to
view these photographs along with others
as perfectly aligned overlays in a 3d
world models such as Google Earth our
objective is to provide a
straightforward procedure for
geo-locating photos of any kind and our
approaches to engage a community of
users for a little bit of human help two
big points here any kind human help we
specify that a ten-year-old should be
able to find the pose of the photo in
less than a minute we're convinced his
goal as possible um early on one of our
colleagues will carter was walking
through his a bank parking lot in the at
Water District of LA and I don't know
what compelled him to do this but he
went down and took a picture of the
parking lot with his foot in it now he
did know that this area is pretty high
res in Google Earth and I don't know why
I think this is an absolutely amazing
cosmic photo that there's something
going on here that's very very difficult
to explain that it's it's an alignment
between something shot very high up if
not in a satellite certainly in a high
res airplane posed with something very
personal and so eccentric that we don't
even know why he would have done this so
I two years ago asked audiences in
particularly the art and technology
world have you thought about after you
die if you've decided to be buried where
that location would be I'll spare the
embarrassment here but I've done that to
a bunch of audiences and what always
happens is some percentage of hands
shoot up there's no hesitation and you
know I can look around the room you know
some of you are sort of nodding you
asked it so much lower percentage than
Europe and whatnot but the point is that
place runs deep and there's a statistic
that I'm going to blow but you'll get
the idea
it's something like prior to a hundred
years ago something like ninety-eight
percent of everyone on the planet never
ventured more than 60 miles from the
place they were born in their entire
life so this resonance and one could
even say magic that we associate with
with place is a very deep one and one of
my favorite little applications
unsecured area but it's made wikipedia
sri photography and the photos here kind
of don't do justice the magic of being
able to actually see them over late and
you know it when they're like dead-on
right one that i do want to show down at
the bottom of the third view are some
people that have been refunded sequence
and I heard these guys talk and I can
assure you these guys don't know pose
from beans they've never heard of sifter
ransack or you know they were hiking
boots all the time and they go out there
and they just have developed it's hard
you know to know when to move laterally
versus when to pan your camera when to
zoom versus when to dolly in but one can
develop an intuition for doing this and
if you think that these are the sort of
people that might have you know worked
it a little bit in photoshop I really
don't think so they're very pure about
what they're doing another example
similar is what happens when you spray
paint a living room white after filming
it and then project the film back on the
white painted objects except the people
of course so this was an installation at
the Art Center College of Design in 2005
and you can see personally a lot of
artifacts that it's not always working
but I hope you can see at least that
what's going on is when the reprojection
is properly posed with the background
physical geometry things work you get
that magic
so the basics of how we began really was
around finding the pose of photographs
with respect to a 3d model and a little
bit of human help and forgive me in
advance I'm be talking more in maybe
cinematography terms than computer
science terms so I'm not going to talk
about intrinsic and extrinsic camera
parameters as much as what's used in for
example the special effects industries
so yes we know that there are more
parameters needed and deep in the
special effects industry where they know
that glass is very analog they'll do
incredible extremes to sort of match
lenses and get you know the fundamentals
of the optics of a particular lens to
match with 3d graphics in terms of
posing but for our purposes these eight
parameters are pretty much enough and
how how accurate to get that magic of
rifa tog raphy or reprojection as a
starting point we're saying laterally
about a meter and angularly about a
degree now if you're shooting Mount
Rushmore the lateral tolerances for rifa
tog raphy or posing is going to be
really different than standing in front
of the ferry building the angular won't
be which is to give an idea consumer
technologies are off by not quite an
order of it about an order of magnitude
and going in the other direction the
motion control the special effects world
has much much much tighter tolerances so
for the sort of things that we're
interested in doing it falls somewhere
in between cheap consumer tech and very
very expensive Hollywood technology
related work I gotta admit this has been
a little obsession so indulge me in kind
of a story I heard about witness point
tracking probably twenty years ago
something about putting tennis balls in
the scene and
than garbage matting them out making
sure that they were non coplanar at
known relationship from each other about
10 or 12 years ago I remember having a
conversation with Glen entus who's now
at electronic arts but back then he was
at pacific data PDI right um and he was
saying that they had developed a system
for putting a thing in the frame and he
wouldn't tell me exactly what it looked
like but it the way he waved his hands
it sounded like a large tinker toy you
know kind of thing that they would
garbage matte out and the phrase there's
a way before match moving which is now a
deal in Hollywood the phrase witness
point tracking was used so I was
bewildered before our project to find
only five hits and other than the the
blue ones that all refer to as some
obscure thing the bottom one is a bio
for Brad de Graaff in German for ARS
electronica and lens 1988 and the middle
one goes to a paper by Gary demos that
you can get the PDF but it's not on
searchable easily on Google indeed in
1977 for Close Encounters working with
Bill most zigman Spielberg and Doug
Trumbull they did a test using four-inch
light bulbs that they put through and
they pretty much proved that it could
work and of course there is human
intervention in something like this but
they didn't get the contract Doug
Trumbull had the year to earlier develop
this absolutely incredible instrumented
motion-control camera that had all of
these parameters very precisely sensed
um and if you look at match moving today
I don't know if you can read the bottom
of the first and some of the best
examples of match moving were used in
the film Jurassic Park colored tennis
balls 1993 this is clearly incorrect and
I hope that our paper and research might
get this fixed the guy who did a smell
McMahon turned 81 in February and was
helpful with this um related work movie
maps I have a lot to say about movie
maps I humbly can say that I don't think
anyone has put more camera contraptions
and shot more places over more years I
don't even know who's number two but in
in terms of this context a lot of it has
to do with the relationship between the
interval that you're shooting what kind
of transition you use from one to the
other it's all look up right and and how
smooth this is so to me the magic
question for any form of movie map
whether it's Aspen or Street View is how
credible are the in-between frames what
are they there for a street view now
take a look i don't have to say this to
you all but this is a combination
dissolve and scale and does it help the
i adjust while i suppose it does but are
any of the in-between frames usable on
their own no every scape does something
more interesting and the interval
therefore seems to be greater because
they can do these longer transitions but
i would still challenge anyone to answer
the question are any of these
in-betweens usable Photosynth as you all
know it's a point cloud based it also
has the transition issue going from one
frame to another so there's no
underlying model behind it but the more
significant difference between it and
our intention deals with what gets
ransacked so in their paper and again I
apologize not sure how much you can read
this but when they did their automated
search
is using flickr for example for Notre
Dame and Paris they got 2635 photos of
which 597 made it as in liars 325 out of
1882 and so Hannon's about twenty two
percent and we very very very strongly
feel that our little contribution in
this world is to emphasize creativity
driving technology rather than the other
way around and I think that's why we
were so mesmerised by my colleague will
Carter's picture of his feet in the
parking lot panoramio of course you all
know about what fascinates us is how
panoramio does by editorial Fiat what
photosynth does by technological
limitations and again there I have
nothing against you no no people posing
cars planes pets flowers close-ups
underwater and events but if this were
applied to Flickr you'd still have a
bunch of good images you have only 1% of
what you started with um we have been
tracking panoramio images entering
Google Earth because in July I can
barely read this is 1.2 million and in
February there's 4.5 million presumably
selected by hand because of the
editorial policy somebody's working
pretty hard over there looking at a lot
of pictures we noticed certainly after
our project started that the gigapixel
images looked really nice in terms of
how they were positioned and you know
assumed that this is not a coincidence
that this just happens to be perfectly
posed very nicely posed and this is
leaving the pose
flickr i went to internet archive just
for fun and this was the first homepage
and I think 2004 for flickr and i also
found on the internet archive a charming
little blog post by katerina one of the
cofounders this is what june july august
three months after flickr started that
she found four lovely tags graffiti rust
neon and frog frog ok today frog has
over a hundred thousand hits on flickr
and flickr according to wikipedia itself
has more than two billion images on it
so what does that tell us that the risk
of being a little presumptuous google
earth has posed several hundred photos
has potentially four and a half million
more geo-located photos to go photosynth
is impressive but it has problems with
no underlying 3d model and most
importantly it's a low success rate at
handling arbitrary photos and yahoo is
this crazy free-for-all database where
something's going on in terms of
community involvement so our approach is
first and foremost that we can deal with
any and all photos number two that we
use a little bit of human help and you
probably know the word phrase
crowdsourcing um and that we're not
saying this is an end-all solution we do
want cameras to have good GPS in the
future we do on cameras to have accurate
angular sensors and we do want vision
technologies to be able to make
virtually all of this automated but it's
not there yet and the cost for that
having the cost for doing it right now
is snipping off the creativity and
that's what we're trying not to do so we
developed two solutions for pose finding
that both begin with the first
yep we didn't know what to call it so we
just refer to it as step one it's
basically like panoramio it's putting a
dot on a map anecdotally think about
this yourself when you look at a map
particularly tight aerial overlay and
you think about when you actually press
the trigger anecdotally we think that
people their memory of where they were
standing without gps is probably within
a meter depending on landmarks we're
assuming human height for now and we ask
the user to specify a direction because
the goal at the end of step one is to
have your photo here and a screen grab
from a 3d world model that's a rough
approximation there our two solutions
are what we call the 2d to 2d solution
and the 2d to 3d solution the 2d to 2d
solution takes those initial position
three parameters as a given and locks
them down and when you think about if I
was trying if I was standing in what's
that inverted globe it's at the
Christian Science Center in Boston you
know and I had a projector mounted in a
fixed X Y Z position and an image that I
wanted to correspond it should be
obvious that I could match it by panning
tilting rotating and zooming it's a
fundamentally 2d and clean problem and
something that humans can do really
pretty easy the problem is that if you
didn't nail that initial point properly
you have to go back to go the 2d 3d
solution is more in the spirit of match
moving and witness point tracking and
it's a much more challenging solution
but the end game would be the 10 year
old saying this point is this point this
point is this point this point is this
point and having the stuff under the
hood do the correlation
um so we just put these all out the pros
for 2d the 2d is that it is
computationally easy and it's easy and
intuitive for a human helper to just do
the little bit of registering and by the
way since it's only fighting for
parameters we initially started by
nailing one point and then doing a
rotate scale to get things to match now
our designers more into you'll see in a
second but slopping around but you know
in the end you really only need to
nonlinear points to get these four
dimensions lock down it's a pretty
simple problem the bad news is that the
initial position must be accurate yeah
the 2d to 3d is again more in the spirit
of computer vision something that we're
interested in pursuing there are a
couple problems with it though one is
that the human helper will need to
develop some kind of intuition about
what non coplanar means and whether that
can be intuitive or not is an open
question probably yes and then the more
functional question is that we would
need the cooperation of folks like you
or another 3d world builder because we
would need access to the z data in order
to do that so we would have our photo
here a rough approximation screengrab
here based on the initial point and when
we start matching points the z data from
the 3d model is required for the 2d to
3d approach and I should also point out
that this is non semantic 3d data as
opposed to semantic 3d data and de
bewick and I had these long discussions
about trying to like understand so for
example I they had a tendency to want to
go for the vertices and I would ask
let's say you had a good model
and it was photographically textured and
on the roof of the model the tiled roof
there was a missing tile could I use
that as a potential feature point it's
in the middle of a plane and he said the
answer is yes and it's just really you
know it's it's 2d to 3d so it really
doesn't involve semantics as much as it
involves just having access to that
information so that's posed finding hos
viewing we had to rely a lot on photo
overlay thank you very much we could not
have done the six months ago um and the
place to start with viewing posed photos
is the truism that if you're on the
nodal point and the photo is posed and
let's say it's transparent fifty-fifty
it will always be perfectly aligned
always the image can be close and small
placed in the 3d model it could be far
and big if it's too far it might
intersect some of the objects we
actually did a little bit of work on
having it intercept prominent objects to
see if that's interesting sort of it
could be a task you dangle doesn't
matter from the nodal point it's always
always going to appear properly aligned
but just get these all out but there are
some variables and I'm calling them
designed variables because it's kind of
a little bit a matter of taste as I
mentioned I feel very very strongly to
only show the photo when you're on the
nodal point and to use line three some
kind of non photographic indicator when
you're not on the nodal point and for
that matter to have a snap to feature um
let's see Photosynth has empty frames
that glow very nice and frustum lines
which you can turn on and off as
indicators and then the
photograph fades up as you approach it
where's the gigapixel gigapan images
that you do our persistent and there's
no indicator other than the image itself
I don't know any way to get good at how
these variables interact except to just
do a lot of them and for the video that
I'll show you at the end five-minute
video you'll see little examples of all
of these and again I think that we've
developed a little bit of experience in
this and that there's a long wait this
is a sort of a community effort and
design um so this is the step one thing
and what's going on here is very simple
you stipulate where and at what angle
and if you go a little under the hood
you can control tilt and we built a fake
google earth frame grabs server using
the com API it's running in the
background it's not what we wanted to do
but we had no choice it's a hack but
what's kind of cool about this is that
you stay on the ground and that's hard
to do with Google Earth anyway and you
kind of move around and try to get as
close an initial match between your
image and the point of view and again if
you're doing a 2d to 2d solution this is
critical which is a bummer if you're
doing a 2d to 3d solution just need to
get it close the current incarnation of
the 2d to 2d and frankly we're making
more work than we have to for to do this
and we don't want to sugarcoat something
that no human should really have to do
and try to make it fun we want to make
it easy and again you know if you have
to nonlinear it's not nonlinear
if you have two points you should be
able to nail all four coordinates and
you're done and the ICT people have been
working on more ambitious 2d the 3d
approaches there's something like five
hundred iterations going on for every
match that's made here and of course
there's still a long way to go
again proof of concept only
um if we pursue this the question of
whether to run right now with a useable
lon chable web app that's to tita 2d
that doesn't frankly require any you
know connection with google it doesn't
require access to information that we
don't already have or whether to pursue
deeper 2d 3d solutions is something
we're undecided about you know we think
if we get a 2d the 2d solution out there
there's enough people that would pose
images the real research is more in the
2d to 3d as i mentioned understanding
the design issue design issues around
posing when we started this project we
said there are three phases pose finding
pose viewing and modeling and we haven't
addressed the much larger and more
important issue of how can you
incorporate photographs into the actual
model making process we chose to focus
very narrowly if you will on post
findings as we know that post finding is
an important step in anything that
you'll be possibly doing with model
making John Markoff claimed that I said
that we were rabble-rousing we do want
to in our own small way have impact on
companies like Google and camera
companies to integrate things like this
to make it easier and we also strongly
believe that the more community
engagement in the modeling process the
happier everyone's going to be um let's
look at the video and then be delighted
to take questions
there is audio
so on one level what we wanted to do
here was understand what the outliers
were in photosynthetic torial policy was
and do everything to violate both them
in the in-between
it's also important to point out that
the 3d World models like Google Earth
have three different types of geometry
Todd terrain only which is very coarse
what we call shoeboxes like all of Japan
untested and then the sketched up richer
buildings so we wanted to show samples
from
there's from the hill miracle
and these could you recognize me
discreet you so you kind of wonder
whether people would pose there
this is from vertigo
yeah I know
that's actually an image depositors
thank you and I have to emphasize that
I'm really just a messenger for a very
eclectic and talented team so thank you
very much um that's all I really have to
say and you know we wanted to do
something fast and lean and get it out
there as quick as possible to provoke
discussion and what's next because this
really is a hot interesting timely area
are there any questions yes
and i'm i'm not sure where I was to
within one meter for any of those
pictures I know I was standing out in
front of my house back and forth and
took whole bunch of pictures can you
think of a way to use that group of
pictures to pose the whole group yeah
that's a good question it you know in
terms of the big big scheme of what
we're proposing is that humans that what
we're trying to do is solve a
multi-dimensional search problem and the
more we can pound constraints into this
the more manageable it is for a
computational solution humans are not
very good at solving a dimensional
search problems but they're good at
getting a close so it might just be that
for the next couple years if you get
them in the general vicinity something
like sift can help you know if you take
them at the same time and they're near
each other pardon with the same camera
right and I think the bigger picture is
that what you're saying is what a lot of
people are doing now it's not like 35
millimeter film where you know you care
yes
yeah so first of all to say that this
video is really impressive so and so I
had to look at your web page and at some
point it said that you were considering
to find the vanishing points of the
pictures because of those pictures with
love I mean buildings and you can have a
lot of parallel lines and I wanted to
know how how far are you on this track
and um we're uh we want to do it you
know it's funny cuz part of the healthy
tension between us from the film school
and them from you know computer science
is around the issue of various
automation schemes that might help but
act as a fairly strong filtering
mechanisms for what images work better
than others again those pizza lunches
have helped a lot we all sort of get
along and try to figure out and
negotiate if we're going to have use
vanishing points we push them to go okay
but can you back off of like rectangular
buildings only you know we know you can
do that so we you know sort of iterate
back and forth so that's a long answer
to the question where the real answer is
we'd like to continue
um in your examples of the movie were
there any pictures where you didn't know
the location or couldn't guess it yeah
that's that's a really good question um
let me break that down because
mathematically there are a whole class
of pictures that have non unique
solutions if I take a photograph of a
mural on a wall I could pose it along a
non-unique line anywhere right and then
there are pictures of devils tower where
you could be especially with terrain
turned off sorry about that but where
you could be off by a long long way and
it would still be credible and then
their pictures like the foot in the bank
parking lot where you know there are a
lot of white painted lines on concrete
where it really could be anywhere in
terms of a photograph and I'm assuming
you mean an outdoor photograph where we
have some idea where it is but we just
couldn't pose it why more talking about
the case where you don't know the
location where the photograph was taken
and so therefore you have to have a much
bigger solve our problem I mean you have
to so alpha male routers um we have not
been dealing with that if you have a
picture of a you know a tri-level green
and orange home with trim you know lawn
in front of it an a you know lexus
hybrid and you know i know we have not
been dealing with anything no I'm
actually sorry I'm not making myself
clear was more thinking of where you
can't put that initial dot on the map I
mean basically you know you were there
but you can't localize it enough well
yeah like exactly okay that's a good
question um they started with the
runways and that's actually an
interesting one because I that was a
certain amount of witchcraft you know to
pose that one also when you look
carefully at the video and of course
it's on YouTube the poses are far from
perfect and if you get the foreground
right it's a lot better than getting
before ground round in the background
right so you know we know at least where
to start in terms of making it look good
um but if what you're asking is a little
bit like what you're asking your at
Golden Gate Park or you're walking
around the Ferry Building you took a ton
of pictures and you look at this and you
go well you know it's kind of pointing
toward the bay bridge and part of the
ferry buildings in the foreground and I
don't you know it's somewhere around
here this is why the 2d the 2d to 3d
solution would fix that it would be an
adaptive system by which all you need is
enough non coplanar feature overlap
between your photo and a 3d model and as
you match points and it could be a
broken tile on a roof non semantics with
3d points it would just automatically
adjust so that that's really the more
interesting approach and the answer to
your question is if you can get it in
the ballpark and you have an adaptive
pose finding human you know okay yes
you're absolutely right and then it only
failed in absence of good 3d data and so
that was corolla lab question correct
how do you deal with inconsistent 3d
data all 3d data or silhouette edges
where you can't really localize points
but you basic you have to guess how do
you deal with let's say that i live in a
neighborhood of currently shoe boxes in
my neighborhood you know in LA but i
really want to pose a picture that i
took so I'm confront
with taking my photograph and matching
it up on a rectilinear surface which is
a little bit like step one and sketchup
right can I find a best match for that
that satisfactory well kind of um sure
we secretly hope that through the
massive accumulation of posed
photographs this is valuable information
that can be used to refine the models
which are far from perfect right now yes
about supposing video oh yeah um the
person was about posing video um well we
certainly thought a lot about it and
there is that one camera on tripod three
progressive image to show a little bit
of motion that's in our video in our
demo video um the nice thing about video
is that if you pose the first frame you
know if you pose the enth frame the n
plus 1 frame is so similar that that
should be able to be done
computationally right um so it doesn't
seem like that harder problem there's a
lot of problems with storage and
retrieval we just think that if we can
get some tools like this out there to me
a metric of success is I want to see
surprises I want to see people doing
stuff creatively that's gonna make us
all like wake up and go wow I never
thought of that before yes so the photo
overlay tag was very handy for this what
one or two features would you add to
google earth to to make people pose more
photos now to make this easy thank you
that's thank you for asking um control
of field of view for photo overlay um
control of transparency I mean you guys
know that we did a lot of after effects
and stuff to you know artificially make
dissolves happen that they
that don't happen in photo overlay right
now um I don't you pardon um
controllable okay I apologize um and
that's accessible well I mean we can
find it okay um there's a question
related question that deals with whether
we and it meaning the community meaning
including Photosynth and everyone can
agree on some posing protocol so I don't
know how whoever's posing gigapixel
gigapan images are determining how big
the virtual photograph should be how far
back from the nodal point and I equally
haven't a clue when you go into photo
sent where you see lots of glowing
rectangles it's clear that they're all
normalized so that whatever they're
doing to one they're doing to all the
rest but I haven't any sense uh what
that is I mean can you just you know you
can think well okay Wow let's find the
first solid object and go midway between
but you know we're in the frame again I
don't know how photosynthesis at all but
if we can agree on protocols that makes
the whole posing pose viewing process a
lot lot easier for everyone
a bit of a general question it seems
that a lot of your best demos were when
you actually have that all the 3d models
in the region available and you know and
sometimes most of the earth you don't
have 3d models available right and if
you want this thing to really be global
like you need to do something mean
pictures to pictures alignment or like
what do you do then right i mean that's
maybe not satisfactory because then
you're in photosynthesis 3d models of
everything before you place it or I
think that what we're doing at its best
is contributing a small piece to a very
very big problem and the big problem is
modeling out the world and which clearly
does need some level of community help
and yeah we've we've always sort of been
against the photo to photo thing we've
always been assuming that our thing is
about photo to model the posing photos
with respect to models not photos with
respect to other photos and we've
speculated so what happens if you're
standing in the middle of the Sahara
Desert where there's not a feature other
than brown plane and you pose a photo um
it's a good question o part of the
answer is ok there's photo to photo like
Photosynth but there's also photos to
model which we which that's the other
thing for photo overlay is the ability
to not only see the multiple photos
which you can do now more or less but to
have some kind of snap to or other means
of like Photosynth traveling from one to
the other so you're in the middle Darfur
is a great example because you guys have
been doing work there so what you might
have in there for initially are a bunch
of photos posed with respect to each
other on the model and the model is flat
and the minute somebody puts a little
shoebox there you have
anchor to start building up more I don't
know how else to begin yes one
alternative is to have this you know
point clouds the spar Swanson's try to
align things to them right have you
tried that is alternative to actual 3d
models and how much worse it is we
haven't done anything with shift and
point clouds at all and I have a lot of
respect for how Microsoft has been
dealing with that and as you can try to
do the manual solution to 3d point cloud
world right and try to do some kind of
robust matching that sense where you
point clouds could be more wrong it's
not as good as the 3d model but still
there's three information right and and
frankly you know we're assuming that
there's so much activity going on in
that area between you know the cvpr and
3d pvt communities and whatnot that
we're sort of positioning our role maybe
it is rabble-rousing as not so much what
does it take to build out the world and
models but what does it take to get the
broadest diversity of photos somehow
integrated that's our niche yes
um this is mostly a user interface
problem if I see that correctly and have
you looked at other paradigms to place
pictures of other kinds like let's say
the sahara picture you see the horizon
so in that case did you think of special
tools for horizons or indoor scenes or
things like that um that's a good
question and as someone who has
sometimes very unsuccessfully tried to
project images on white painted you know
volumes that there are sort of
heuristics that you can use to some
extent but know that that's a very good
question how to make the UI easier and
you know again I we think that if we get
something out there ah that there's
enough of a community again you look at
the numbers with Flickr and you look at
the number of people doing historical
photographs you know and we photography
my experience with you I is that it's
extremely counterintuitive and every
time I think I that's the right answer
on like so wrong so the lesson for me is
you really just have to test test test
but you think there's a single solution
sort of I don't know no I don't know um
I mean again in a perfect world I
suppose there wouldn't be any human
interaction you take a picture and it
would end up in a model and in a less
than perfect world you'd want something
that is like no more labor-intensive
than what people do for Flickr right now
but by the way one other little point on
that and again with no disrespect to
Photosynth taking Flickr images by the
batch is not the way the flickr
community works where you have a vested
interest in
ownership picture by picture in tagging
it and sharing it and if you have to do
a minutes worth of other stuff and
there's some satisfaction this all of a
sudden like clicked it locks you know
you get a little payback that doesn't
seem so bad yes I think instead of
thinking this is a problem we're in the
perfect world that would all be
automated I think you should always
think that no matter how great your
computer vision is it's going to have
failure cases there's gonna be that guy
holding up the picture of war in Iraq
and there may be like a little tiny
sliver of the picture that right that
you want to place and everything else
doesn't fit right there's always gonna
be cases that don't work right and so
you want to start with a manual system
that you always fall back to when it
doesn't work and then it becomes a
problem of just how to make the manual
system more helpful or the automatic
tools in there like a verizon finder or
a or a you know parallel line detector
or something like that assist you and if
you can have it snap to and you know as
soon as the computer understands the
picture well enough you know he resolves
all 11 parameters perfectly frank that
would be helpful but underneath it all
you always have to have a great UI to
let the people fix the problems when it
doesn't work you have done but it needs
to be quick you know small cycle time so
you don't have to wait a minute for it
to calculate i need to be able to you
know hit escape or something undo that
back up and try again you know let me
give you one more hint and so you can
get it right this time and i totally
agree and again I without any hint of
denigration self denigration here how
can I put this um we don't see this as
some big thing that we're ready to hype
as being slices dices change the world
as much as focusing on a very particular
piece of the puzzle and I want to be
careful with my words here because I
don't want to hype it too much because
it is a transitionary state and I do
think that if we can get something out
there like this
and I see the need you know on a bunch
of levels and the timeliness that a year
from now and two years from now we'll
have learned so much because of
community involvement that then we can
sort of reconsider what the next step is
because the the really really big issues
are huge issues by the way you know
maybe the huge issue just issue of all
is that the difference between Earth
models and fictitious models like second
life and warcraft is that for every X Y
Z and T there's a ground truth and
without getting into postmodernist
rhetoric about what is reality for any
given X Y or Rashomon well that's I saw
something else and you if I had a camera
and you had a camera and we took a
picture aimed at the same place at the
same time we're going to get the same
thing and what that means is that all
earth models are aggregated ball and I
don't know if you folks here are
considering this but that's a huge the
implications behind what that means I
mean right now there's a website you can
go to and it aggregates because you can
sort of like link from one to the other
but it's a unique property of the real
world that doesn't exist in fictitious
worlds and Lance is smiling on this one
getting back on your objectives you were
saying that you want you would like to
attend you'll being able to correctly
put the photo in a minute or so so I
wanted to know in with this because
right now you're using the 2d 2d
interface to post online yeah in Google
Earth so what is the average time to do
that successfully and have your test
with random user and some kind of
usability test and um no no we haven't
tested with random users and we're
certainly not ready for prime time i
will say that will carter and eric
lawyer who are the two contractors that
have been doing most of the post finding
in the past three months have increased
their speed by probably a factor of five
and that step one interface is now
really pretty quick so that that that's
probably a minute right there and then
the question of registering from there's
probably about five minutes if it's an
easy one so right now you can move
horizontally like that and vertically
yango we've done yes and i think that
you can rotate but you have to go a
little deeper it's not like an easy
interface similarly you can change the
height for step one for your Xyz
position but it's not a clear and easy
thing to do
party assuming it's in jail but they
like you know they're pretty something
to tickle um yeah you know we want to be
careful not to reinvent Sketchup I mean
you know we could come up with two dozen
easy-to-use features that deal with
vanishing points and assumptions about
edges and corners we're seeing how far
and fast and how do we can run with a
very simple idea and that's why we're
keeping focused on the 10-year old one
minute thing you know
good good questions
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>