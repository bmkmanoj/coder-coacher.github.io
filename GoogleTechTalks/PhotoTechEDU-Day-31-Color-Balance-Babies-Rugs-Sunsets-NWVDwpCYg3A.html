<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 31 - Color Balance: Babies, Rugs &amp; Sunsets | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 31 - Color Balance: Babies, Rugs &amp; Sunsets - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 31 - Color Balance: Babies, Rugs &amp; Sunsets</b></h2><h5 class="post__date">2007-12-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NWVDwpCYg3A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome everybody to photo
analogy lecture number 31 or really
pleased today to have Paul Huebel he's
the chief image scientist at fobian he
got his education undergraduate in
optics at University of Rochester grad
school at Oxford and postdoc at MIT
Media labs before going off to HP Labs
for 10 years for the last five years
he's been at fo beyond where I had the
pleasure to spend some of those years
working with him he's responsible for
the color balancing and other image
processing algorithms and at least three
brands of cameras that I know about so
he's gonna tell us some of the some of
the tricks of that business and why it's
interesting Paul take it away so is this
working you can hear me can hear me in
Santa Monica or wherever you are so okay
so thanks dick so I'm gonna talk about
white balance today a color balance I'm
not going to talk specifically about
fovea on technology or or anything of
that nature if you're curious about that
just by dick a cup of coffee or
something like that he can tell you all
about it and I'm gonna talk mainly about
color balance in image capture if I was
gonna talk about color balance in
projection displays I'd point out that
we have two completely different white
points of the two projectors here I
don't know if that's intentional or not
but so just an overview color balance
problem there's a couple different kinds
of what we call white point and I'm
gonna talk about what the distinctions
are between those I'll talk about there
so this white point color balance color
constancy illuminant estimation they all
mean slightly different things and
they're all useful for slightly
different things so I'll try to
distinguish between them and talk about
what the differences are Oh
outline the problem that specific to our
application which is photography and
talk about some of the some of the
methods that are used I'll explain some
of the basic methods like gray world and
I'll talk about some of the more
advanced method
it's you know going going through a
bunch of a bunch of different methods
adding more and more complexity as we go
along then I'll talk about some hard
situations specifically some work that I
did in the past on on dawn and dusk
scenes sunset scenes which are one of
the most interesting and then I'll tuck
up tuck finally at the end talk about
methods of testing these algorithms so
let's see adapted white point and
adopted white point so this is this is
an important distinction between
different kinds of this is actually
cropped from a standard so there's au in
color in in some situations sometimes
there isn't the UI apology so adapted
white point is the color stimulus that
an observer who's adapted to the viewing
environment would judge to be perfectly
a chromatic so this is what sort of like
what our brain sees as neutral in the
scene that's the way I think of it is
it's what the visual system has adapted
to in the scene the adopted white point
is what a camera or some kind of device
is adapting adopting from the scene and
and neutralizing the picture too so
ideally in photography you want adopted
to equal adapted and so I it's it's kind
of confusing but it basically it's what
the Machine does versus what the visual
system is doing unfortunately we don't
know what the visual system is doing
except by asking observers and things
like that there is no you know there's
no synapse that you can probe to try to
figure out what is white it's something
that there's a lot of research done on
that and there's a lot of theories but
we really don't know the mechanisms of
how these things really work so color
constancy comes about and says a camera
courts different colors depending on the
scene illuminant so obviously we have
daylight
two flavors of daylight tungsten and
fluorescent and the camera sees those
all as different you know the the
tungsten is going to be yellowish the
d65 is going to be bluish and
fluorescent comes out slightly greenish
our visual systems somewhat sees these
things as neutral regardless of the
illumination so I'll have more on that
later because it isn't quite true but to
first approximation it is so what the
alumina estimation problem is is you
have the raw camera data from that raw
data you try to estimate the aluminum
from that estimate of the aluminum you
try to put that into the camera pipeline
into your rendering model in order to
get an image that comes out neutral and
doesn't come out yellow so this is
something that all digital cameras do
analog cameras used to do it with their
choice of film and the photo finishing
systems also you also do them
unfortunately sometimes you know
especially in the early days of digital
photography the ability to do this hurt
rather than helped there are a lot of
times where it just screwed it up
and you'd end up getting a worse result
than you would have if you had taken it
with film so the ability and the the
sort of digital capabilities was hurting
things in the beginning because it you
know it wasn't doing a very good job so
one thing that's important is to look at
possible scene illuminance and so this
is a diagram this is a chromaticity
diagram so we're looking at red over
green channel against the blue over
green channel and the axis are not
really important I'm just saying it's a
chromaticity diagram it's a
two-dimensional cross-section of of
color space and here you can see
different alumina it's just Tungsten's
down here the 120 will be up here that
would be a shade and from from there you
can see a kind of a you can make a line
sort of a curved line and that is the
planking radiator and then the daylight
locus and then off to the side here are
where the fluorescent lights would
appear in this particular camera system
so are that red over green
blue evergreen is defined by the camera
system itself it's not necessarily a
visual visual system how do images
change with illumination if you have a
patch in under one illuminant and a
patch under another illuminant and then
another patch under one versus the other
they change basically by these these
factors alpha beta and gamma and so as
the illuminant changes image RG and be
changed by three independent scale
factors so this is what we call the
diagonal model of aluminum change and
its approximate and but it usually works
depends on the on a number of factors
like the spectral sensitivities and the
specific color space and in fact in most
of these situations you do this white
point correction in a different color
space than the original data so you go
through a linear transformation to get
into a space where you do the the the
diagonal correction and then you go back
out you so this is what's invented about
a hundred years ago by von crease and
it's still in use use today and it's one
of the most important things so if you
try to do a diagonal correction in just
in raw camera space it would the the
quality of the of the white point
adjustment would very dependent on the
camera in our camera the fovea on camera
where the spectral sensitivities are
broad it doesn't work at all in
something where you've got sharper
spectrally sharper sensitivities it
works okay but in all of them they do
better by doing this going in to this
different color space so implications of
the model we simply need to recover
these three values to take the the
illuminant from one illuminant to to the
reference light and usually that's srgb
which is d65
for them for almost almost all
applications these days so we want to go
from the the
seeing illuminant - this too - srgb
equivalently we just once so one way we
can do it is we just need to determine
the RG and B value for some reference
surface in the scene or a white patch
under the unknown life so that there
that or the scene white point in fact
it's usually enough to estimate a 2d
position so this in this chromaticity
space if you can determine the
chromaticity of or the color of that of
the white point in the scene that's
enough to get you where you need to go
in terms of color balance now white
balance or white balance can also
include intensity balance and tone
adjustment and I'm not going to talk
about that today it's a fixed step
that's usually controlled by exposure
and the tone curve that's used so and
that that can depend a lot on how bright
or how dark how dim the white point is
we're really just going to be talking
about the color color balance properties
of the white so why is it hard let's
look at what we know when we get data
from a scene and what we don't know we
know the RGB values of all the surfaces
in the scene we don't know the color of
these surfaces we don't know their RGB
values and under the reference light
that we're trying to transform to we
don't know the color of the scene
illuminant and so we have more unknowns
than we have known so this is the the
basic problem we don't we don't have
enough information if you take it miss a
simple way so we can't uniquely identify
the scene illuminant with mount out
making additional assumptions about the
scene and the camera by the way if you
want to interrupt me or just you know
feel free try to keep this less formal
some white balance approaches so i'm not
going to go through all these but these
are a few of the major milestones
in the literature of white balance gray
world which I'll talk about first is the
most basic max RGB which is often
attributed to Edwin land but I'm sure it
was done before that it's part of Lynn's
rat next model which is much more
complicated than just taking the maximum
of the scene Minkowski spectral models
are trying to take some of something
make some assumptions about
dimensionality of the spectra of the
surfaces and the illuminance let's see
what else we'll talk a little bit about
constrained gray world and then getting
down into these models we'll talk about
some of those towards the end
specifically some of the gamut based and
probabilistic approaches which have been
have been quite successful and these
these basically increase in complexity
and constraints and also in performance
in a mildly I mean it's not performance
is very hard to measure because some of
the more complicated methods can do very
well with very hard scenes some of the
simple methods can do very well with
some of the very easy scenes but it's
not necessarily vice-versa one key thing
is that the methods up at the top don't
need any calibration and if you're
working on really inexpensive cameras
where you're making upwards it's it's
getting upwards towards a billion a year
cell phone cameras that are gonna be
generated by the end of this decade I
mean think about a billion a year it's
just mind boggling there's no way you
can calibrate them it just takes too
long even the second that would take the
flash one picture is often not done in
these really cheap cameras so you have
to rely more on uncalibrated sensors as
a result you don't get as good
performance in terms of the color
balance as you can add more and more
calibration getting up towards digital
SLRs like the sigma camera that uses our
sensor those you can spend a fair amount
of time being really
that every camera works as its performed
so that color space that I showed before
is anchored correctly you know you know
where illuminant a is specifically for
that camera really accurately because
you have it well calibrated so excuse me
so the gamut mapping and the
probabilistic approaches down at the
bottom of that list are well justified
in terms of theory some comparisons that
have been done in a few universities
jest that these are the best-performing
also some of the cameras that use these
methods tend to do quite well in terms
of comparisons in the in the industry
however good performance is a lot of
good engineering as well as a good
theory so you can have a good theory of
how to do your white balance but if you
don't calibrate well or if you don't if
you don't figure the system out and go
through all the details it it's never
gonna work all that well and there's a
big gap between you know the theoretical
literature and explaining how something
is done to actually putting something in
a camera and getting it to work well so
the combination of the good theory and
the good engineering ought to give the
best results so let's start with the
gray world gray world simply says that
if you take the mean of the of the data
coming in from the camera and you set
that to gray then under a different
colored light the scene average will
change by the three independent scale
factors factors that we showed before
and if you take the mean of the other
the image under a different light you
can scale by those scale factors and end
up neutralizing the image so basically
if the world is gray then this system
works perfectly so if you're taking a
picture of a white board or a gray piece
of paper or something where the average
of the scene that you're taking a
picture of is neutral then this this the
system works works perfectly however
course that's that's never quite the
case unless you're taking a picture of a
whiteboard and I came up with this
really complicated algorithm at HP and
someone took a picture of a of a
whiteboard and it just it didn't work it
was pretty embarrassing because here we
are comparing it to the most basic
algorithm and it failed so it's it's a
yeah course it's wide spectral sensor
that averages the spectrum for the whole
image you had a spectrum for the whole
image and the RGB values for all the
pixels it seems like you could always
recover the one
that's a wide spectrum but then you then
you get one value okay sure you could
put in extra extra sensors into the
device and some early cameras the very
high-end cameras actually do have
special white point sensors is that what
you mean sure yeah you could do that in
commercial photography it's too
expensive minutes to put an extra system
in except on the very highest end
cameras it's not a practical situation
for other reasons it doesn't work as
well because when you're measuring that
usually you have a diffuser in front of
the of the thing that's measuring that
white point you get the light that's
incident on the camera coming down onto
the camera not necessarily the light in
the scene so if you have a telephoto
lens and you're looking at something
half a mile away you're not balancing
for the white point of that scene you're
balancing for the white point of the
light coming in you you're right you
could put a beam splitter in and have an
extra Spectrals but then you need a
spectral sensor you couldn't just use a
broad sensor
but you need more than one you need more
than one value you need 31 values to
measure the spectrum
sure fluorescent lights have 12 basis
functions they're spikey they don't it
doesn't work the the spectral
decomposition of light sources you need
a lot of basis functions to really be
able to determine illuminant accurately
especially fluorescent light yeah
so the idea is that if you so if we go
back what shade of yellow that I can see
not exactly I'll get to that later on it
that towards the end of the talk
especially there are some situations
where it simply doesn't work that if you
measure spectrally what for the
illuminant is and you try to correct the
picture it looks wrong I'll give you an
example at the end it's a it's a good
question though so but that's one thing
I mean we don't have spectrophotometers
coming out of our foreheads the visual
system doesn't Mac measure the spectral
characteristics of the illumination we
do we do we take the the all the signals
like a camera we take this information
in our visual system is actually making
an estimate of the aluminum so and
that's what we're trying to mimic we're
trying to mimic what the visual system
is doing not we're not trying to
recreate we're not trying to reproduce
spectra but I'll get to that and I'll
give an example of that later on ok so
we go through the steps I don't need to
talk about that we're just so we're just
taking the average so here's an example
we have a raw data so this is a raw
image under a blueish colored daylight
and this is the properly corrected image
this is if I do actually measure the
spectra and do the correction and this
is what we get under gray world so here
this is the standard failure situation
where you take something like a
poinsettia poinsettias are really hard
for lots of different algorithms not
just white points you don't work on des
mosaicing or something like that they
got they give all kinds of fits to never
make a camera around this time of year
so the so the gray world here you can
see it's turning the the background
color green because what's happening is
that the average of the scene is no
longer gray here it is gray here it's
grayer and here it's not great so as you
can see as you add the red points etta
into the scene it throws the algorithm
off so the classic the classic example
of this is the baby on the red rug it's
called so here you have a picture of a
baby newborn baby on a red rug
if you take the average of the scene and
balance it the baby turns green or goes
kind of jaundice in some situations and
of course is the classic failure and
something you really want to avoid and I
actually had this happen it wasn't on a
red rug it was on some purple IKEA toys
he's now ten so this was a while ago but
it's a great example of how you know in
in this camera I took took the picture
and let the algorithm go on its own on a
on a similar camera had the raw data and
I couldn't write it through a more
complicated algorithm to get the right
answer so here's a that just the classic
kind of gray world failure problem so
the the next approach would be to take
the maximum of the scene and set that
equal to white also called the white
patch if you have a white patch in the
scene and it is the maximum then you can
correct that it's popular in video
systems so you're just replacing the
mean that we used before with the
maximum it assumes that none of the
channels are saturating or anything like
that and it works reasonably well but it
gives problems that the brightest
objects aren't white so often you'll get
a yellow ish surface or a fluorescent
color or something like that that if
writer than white you know if you washed
your clothes and in tied the max RGB
algorithm doesn't work quite so well
this Minkowski norm technique is a
pretty cool theoretical work and what
that does is basically it's it goes
between the mean and the maximum and the
if you look at the Minkowski norm and
you take the P is equal to one in in the
in the Minkowski norm exploited
mathematics that gives you exactly the
mean of the scene and if you go to
infinity where you're so you're putting
the data to the infinite power and and
doing the infinite
root of it you get the maximum and you
can do there's a family of of properties
in between and the some work done in
England founded for some reason at least
in the data if you use the sixth root
you get the best results at least for
that particular dataset we found that
something between cube cube root works
works better than the mean and works
better than the maximum kind of gives
you something in between these two
things but you're still you're still
you're still going to get situations
where it doesn't work you're still going
to get statistical situations where the
image just doesn't look right and so
then so now we're going to put forward
some some smarter algorithms and these
so these simple assumptions for gray
world max RGB and the all the things in
between simply don't hold for some
images so one one thing is that for some
scene estimates the some scenes the
estimate of the scene illuminant that
you'd use isn't even realistic and I'll
explain what I mean by that in a sec in
fact the the range of illuminance that
you see is quite restricted and this is
what we refer to as an illumination
constraint so if you take this
chromaticity space and you look at the
illuminance a d65
all the different ones that you might
see in a scene so the squares are
standard sources the dots here are
various measured lights so we went
around and measured a whole bunch of
different lights and they're they're
clustered around in there now this this
forms a set and so it's useful when
you're doing any of these algorithms to
make sure that the answer that you get
at least lies inside that that polygon
so what we can do is you can take the
gray world the simple gray world
algorithm get the result of that
now in this kind of situation where you
specifically where you have a failure
what can happen is that your estimate is
not an illuminant it's not possible it's
not physically possible so what you can
do is you can just kind of map that in
into the aluminum polygon so you're
constraining your answer to possible
illuminance and this helps this helps
quite a lot so these are the same
pictures but here we've constrained the
illuminant you still don't get quite the
right answer but it's a lot better than
the results I showed before so here
we're just kind of pushing it into that
into that polygon additional constraints
well one one thing that you can do is
you can say you can try to predict
something about the scene so all
possible lights are here all possible
results under shade conditions might be
this green polygon and under bright
conditions might be this blue polygon
one one interesting fact is that the Sun
is a lot more powerful than any
illuminant we know so if you're under
sunlight you're usually around d 53
pretty pretty pretty regularly I mean
it's a even even late in the day or
really early in the morning you've got a
lot less light than you do at noon so if
you've got that really strong signal if
your BV value from your camera is really
powerful
you can constrain down to this pretty
small polygon and just from that you can
get a pretty good result so there are
tricks you can play with this kind of
thing and this helps a lot so on to the
next so this blue
I guess you can see it better over here
so so the next the next level of
complexity that I'll add here is that
instead of just looking at the average
value or the maximum value or some basic
value we're going to take a histogram of
color space so here we've just bend it
we've got eight bins in either dimension
and we're looking at how often a
particular chromaticity occurs in the
image and unfortunately you can't see it
that well on this this projection but
you can see value there are some values
in here so there are a few colors in the
test image that occur in this
chromaticity there are a couple couple
colors that occur a lot and a few more
up here that don't occur very often so
basically by using this histogram we can
avoid some of the gray world problems
and actually there's a little cross here
which is so once you get this histogram
you can take statistics you can take a
centroid or you can do all kinds of
different kind of statistical
calculations to give yourself an answer
and so that's the particular answer that
a centroid calculation would you give
you in this case you can weight that
histogram so you can say to yourself
okay colors in the center of this
histogram which I know are neutral
because I've calibrated the camera I
find more important than the others I
think that neutral colors are telling me
more about the illuminant in the scene
so I'm going to consider them more so I
take this and I just multiply it by some
kind of distribution Gaussian
distribution or something like that and
use that to help you with this answer so
it it helps you avoid outliers out here
because you're not counting them as much
so in this kind of algorithm these
saturated colors are not contributing as
much to the answer as the neutral colors
and this this this improves it's still
basically a gray world but it improves
on it a lot because it's it's if you
have if you have a single surface
that that is contributing a lot it's not
it's not weighted as highly as something
that's neutral another example is to go
to go to a binary histogram so here
we're just asking the question is the
color in the image or not and so this
this here we ignore the problems of
greater well completely we ignore the
fact that there's a lot of contribution
from from one particular surface and
maybe a lot from another surface we're
saying that okay the image has all of
these colors in it therefore they're all
possible so I want to make my estimate
just from that information and if you
take a convex hull of this buying the
histogram you have what we call it a
gamete of the image so you you get your
image in you find your colors that exist
you take a convex hull and that
determines the gamut of the picture
that's coming in and from that gamete
you can do some tricks to try to figure
out what the illuminant is so so we want
to so this is this the so-called gamut
mapping techniques for a limited
estimation and just explaining that if
you so again we're still in the similar
color space chromaticity space if you
have an image taken under a bluer light
d65 or dat or something you get a
certain gamut corresponding to that if
you take a picture under tungsten you
get a different gamut so the same
surfaces just under a different
illuminant the gamut shifts from one
position to another so if you take so
what you can do is you can do a little
test of consistency so if you have say
say you're calibrating your camera with
these surfaces you know where these
gamuts are and a test color comes in
this color one here is consistent it's
not consistent with your daylight gamut
but
consistent with your sorry it is
consistent with the daylight is not
consistent with tungsten if you add
another surface this surface is
consistent with either of those
illuminance and this third one is
consistent only with the tungsten not
with daylight so if you get an image
coming in with these two colors it's
consistent both of those colors are
consistent with tungsten but only the
second second patch is consistent with
with daylight so for both of these
colors to exist under the same
illuminant in this simple case it has to
be tungsten so that's that's how you
know to choose tungsten because both of
these surfaces are consistent with the
gamut of tungsten so this is what's led
to this color by correlation method
basically what we do is we take a set of
reference surfaces under an illuminant
we take the convex hull of those
surfaces and then we just make a binary
correlation vector a binary vector
corresponding to whether those colors
inside that convex hull exist or not so
ones where it exists zeros so we're just
rasterizing that space putting ones
where we have the colored surfaces and
zeros where we don't so we do this for a
whole bunch of different illuminance so
here we have eight different illuminance
each one is going to give you a slightly
different polygon and so each one of
these vectors is going to correspond to
a different aluminum so once we build
this correlation matrix and we do this
when we calibrate the camera then we get
and we take our picture so we've built
the camera built the correlation matrix
put it in the in the memory of the
camera then along comes a picture we
take a picture of that then we just take
that image vector rasterized in the same
way multiply it by the correlation
matrix M
so we just do the calculation and in
this simple case we just get a whole
bunch of numbers that corresponds to
consistency of this data with these
different illuminants so we can see the
illuminant to in this case is the most
consistent with the image that comes
through so we use that as our estimate
of the aluminum so again we have the
image data coming through our ones and 0
so your correlation table you you you do
the calculation of the correlation
matrix you do a threshold to determine
what's the most consistent instead of
using a binary histogram you could use
likelihoods or other statistics so in
that case under the likelihoods here
you've got what what surfaces are most
likely under all the different
illuminance and here instead of just
having binary numbers you've got a more
complicated correlation table and you
end up with with again you have a an
illuminant with your threshold that's
the the most likely to have been the
illuminance of the scene so this is an
example of a typical grey world failure
kind of a scene and here you can see the
max RGB case has probably taken her
forehead and treated that as the
illuminant so her face kind of goes a
little bit gray and the the the green
gets you this kind of Chemlawn look to
it whereas the correlation gives you
something more closely related to the
perfect result perfect result being a
spectral measurement of the scene
corrected perfectly so this is just an
example of it
that that was binary so these these are
very well justified theoretically they
perform well one interesting point is
that saturated colors help a lot if you
have a really saturated red in the scene
you know that the light source could not
have been really blue on the on the same
token if you have a if you have a scene
coming through for a blue chair and you
get a really strong blue chromaticity
from your camera you know it cannot
possibly have been taken under tungsten
it's just physically impossible
so those saturated colors are what help
you again these methods can fail on very
simple scenes like like a white board
so you should be able to quantify the
confidence that you have in color
correction in these matters
how much information is in the picture
yeah you can do that in fact in fact
that's a good way that there that so
when I talk about good engineering and
what actually goes inside a camera
usually what happens is there's a lot of
different methods and you produce a lot
of different answers potential answers I
need to look at statistics of the scene
to see for example how saturated what's
your distribution of colors because your
this this algorithm works better with
saturated with when you have a large
distribution of color so you're right I
mean it's for sure but there are still
going to be situations where you get a
strong dominant color in the background
and if you were to use a simpler
algorithm it's it's gonna fail but if
you just have so say you have completely
red background and you just have one or
two other colors in there just tiny
little things in the center and
algorithm like this can get to get the
right answer get a lot closer to the
right answer then just take doing a
simple statistical album flash so flash
is an idea interesting idea that came
along and in fact going back many years
ago this is actually what got me started
on this work at the beginning because
Canon has a patent that says that if the
flash fires you can't use that
information to correct to the color of
the image so I had to try to work around
that patent and that's got me started on
it in the right in the beginning but you
know aside from you know a lot of
patents and they in this field and but
we won't worry ourselves with that at
the moment but one interesting thing is
did the flash fire you know you can you
can answer that easily electronically
look at the metadata of the file and see
if the flash fired you know that there
flash obviously often has variable
strength you can tell how how how how
much voltage was applied to the flash
and that can actually give you different
flash colors what's the focus distance
right if you're infinity and the flash
fires it doesn't do you much good
but if you're at ten feet and the flash
fires you can do pretty well just with
that information without going to
anything else just correct for the color
of the flash this flash no flash pairs
is an interesting idea that came along
and so here you have the idea is that
you take an image with the flash and you
you take an image without the flash and
you subtract the the first image the
image with the flash to get the ambient
light so you have an image taken with
the flash an image taken without the
flash so the image with the flash has
both a mixture of the flash illumination
and the ambient and then you take
picture under the ambient subtract them
and you end up with an image of just on
of the ambient so you can determine do a
pretty good job of determining the
illuminant color by using that and this
is some work done at Stanford where they
they did that they take a pure ambient
image and an image with both the flash
and the ambient and they can get a
pretty good estimate of the spectral
sensitivity the spectral this is going
through a surface reflectance model to
determine the the spectral powders to be
standing aluminum from this kind of
flash no flash pair so color constancy
color constancy at the extreme gives a
surface color so it's like mapping
spectral signals from a single point in
the scene to color now color constancy
so if you assume that the visual system
is color constant that is what I've said
so far is true that the visual system
balances the that the ambient the
adopted equals the adapted that you
balance completely for the color of the
light coming through then color
constancy is valid and really what your
visual system is recovering is surface
color right your your compensating for
the spectral power distribution of the
aluminum and you look at a scene and no
matter what the color of the light is
everything neutral is neutral they're
all exactly the same unfortunately
it doesn't doesn't hold true because you
have situations where you have multiple
illuminance and they don't look the same
so this begs this question is color
perception color constant and oh this is
this is an example of an advanced
algorithm done at Simon Fraser
University where they're actually
balancing different parts of the image
for different illuminance so here this
is the the raw data this is one
correction method another global
correction method and then this this is
a a correction method where they correct
differently for one part of the image
that they do for another and so as a
result if you're trying if you've got a
machine vision problem where you trying
to determine surface color surface
reflectance or surface color this works
really well because you're getting rid
of the effects of the illumination
now unfortunately aesthetically it
doesn't look like the perception if you
look around this scene and all these
different illuminance they all look a
little bit different so I'm a more
yellow of another summer some are bluer
some are even greener the greener you
can't see quite as well but certainly
between blue and yellow they all look
different
some are warmer than others they may not
look yellow but they're warm right so ok
so this is this is so this is the
interesting sunset case I want to talk
about so here's the scene taken under
tungsten illumination if you measure the
spectral power database distribution of
the source it looks like that
it's 2800 kelvin something like that and
if you balance that image for that color
of the source you get it looking looking
pretty good like like the scene looked
when the picture was taken so here
you're compensating you're correcting
the image for the color of the UNAM
ination and everything's everything's
great everything's happy this scene
taken under sunset illumination if you
measure the spectral power distribution
of that source it's very similar to the
other one under tungsten but
now depending on the monitor this may
look pretty bad because here we're
balancing for the color of the
illumination just as we did in the
previous image but the image looks blue
it looks terrible
why so if you look at the spectral power
distribution of these two light sources
they're almost exactly the same not only
that they have almost exactly the same
color temperature any ideas
so under tungsten we need to correct for
the illumination color but under sunset
we don't want to correct for the
illumination color so is the visual
system corrects for tungsten not sunsets
kind of weird so the answer is in the
shadows you know alumina on the Left
balance for the element color and here
we hear we're balancing for the
illuminant estimate not the illuminant
color so this one's measured with
respective photometer this one is going
through one of these algorithms that
estimates the aluminum here it looks it
looks like you'd want it to look like
the you'd want the photograph to be like
the scene looked when I saw when we when
we took the picture on the left it looks
way too blue now the issue is that this
is actually two illuminance you've got
the direct illumination from the Sun and
on the horizon which is very low color
temperature but you've also got the
illumination in the shadows which
because of Rayleigh scattering is very
blue at that time of day so you've got
these two completely different colors of
illumination illuminating the scene and
if you were to take the maximum or the
mean you would basically be ignoring the
shadow light and it turns out that the
shadow light is really very important in
these kinds of scenes because we adapt
to the shadow light it gives a lot of
indication a lot of clues into our
visual system about where where we want
to put color balance so basically the
estimate in this case is better than the
measurement because the estimate
sitter's the direct illumination and the
shadows not just the direct illumination
itself so we're using both the direct
illumination and the shadows any scheme
that assumes the single elimination
measurement will fail in these kinds of
situations now the key here is that
photographers get up early in the
morning to take these pictures and if
you make them neutral they're not going
to be happy
you can't take you don't want to take
away that warm character of the picture
and this happened a lot in the early
days of digital photography you get a
camera people take pictures to sunset it
would look cold and horrible and in fact
some companies even went back and said
we're not gonna ask them at the
illuminate at all we instructed to
superimpose the primaries of the camera
sensors on those photographs because I
can easily see how I could get a very
distorted estimate in three primaries if
respecting has those big spikes and
that's actually but but you still even
even if you do do that you'd still get
if you just considered the direct
elimination and looked at the
chromaticity of the camera if you if you
just balance the camera to that data
from the direct illumination you'd get
the same result you would still get a
cold picture
yeah but the important point is that the
estimate considers all of those
statistics and doesn't limit it to just
the dominant ones just the brightest
ones or a single measurement oh I'll
have it I have another example of that
coming up so the interesting thing about
the sunset is that it gives this kind of
warmth it also gives an expanded color
gamut if you look if you remember that
first picture the the plot I showed
earlier of the tungsten gamut and the
daylight gamut of the two pictures so
each one of those independently has a
certain size to the gamut but if both
those illuminants are in the scene and
you take a picture with both of those or
you look at the scene that has both of
those illuminance the actual actual
gamut that's coming in to the visual
system is the whole thing it's it's
bigger and that's you know I'm just hand
waving now but that to me
explains why photographers do take
pictures that sunrise and sunset because
the gamut of the whole experience is
larger the the kyer the colors kind of
glowing they're kind of on fire the Reds
that you get on their sunset are
beautiful because you've got that
contrasting blue shadows also the
shadows a darker which leads to more
dynamic range which gives you more
chroma so here's here's the case sunset
scene balanced to tooth the tungsten
3,000 Kelvin and here we have it
balanced to 5000 here it looks like it
did here it's way too blue tungsten
scene studio shot on the other hand here
we do want to balance it and if we don't
we get something way too yellow but
again the the color of the shadows in
this scene is this are the same usually
the color of the shadows indoors in this
kind of low light situation or a
tungsten scene are going to be pretty
similar to the direct illumination this
walls and ceilings are usually neutral
so here's a scene taken under sunset and
this was going through this correlation
algorithm and
just go whoops so here the estimate is
telling us to balance it at D 55 if I
look at the just the brighter areas the
estimate D 45 wins out if I look at the
shadow areas D 75 wins out but the
estimate up at the beginning that's
considering everything gives us
something in between and that's that's
the one that actually gives us the
better answer so here's something about
the the question of do we want to
balance all of these illuminance out do
we want to balance because there's a lot
of debate about color constancy and
whether we want to balance each
illuminant independently and my point on
this is that a hundred years of history
we've always had a single white balance
in the images that we that we that are
done and in photography and they there
aren't scenes that just don't work there
aren't many at least that I know of and
because that works to me using a global
white balance is works very well and
going to to locally varying ones doesn't
make as much sense so I'm running out of
time so it's five minutes so let's see
I'll just go here's a few examples of
scenes these are hard scenes where
you've got very blue light from the
flash here's a white point a gray world
failure with a sunset see so you've got
two illuminance the sunset and the shade
in the background and you've got a large
dominant color to one side again here's
a tricky situation you've got tungsten
light and you've got a monitor this is
where these will throw the maximum
algorithms way off because the monitor
you know will set to D 95 or something
like that gives you Fitz
testing color balance so there are I'm
in this camera phone image quality
standards group it's part of the
international imaging Association and
here we do methods of testing white
balance so we have standard methods
where we take a color checker and we put
next to it a bunch of colored folders
and we use this we take a bunch of
pictures to determine how well from this
you can determine how well the algorithm
is is balancing you know we're looking
for these kind of gray world failures so
this is what and yeah so there's an
example of that kind of an image that
we're trying to mimic basically when
you're testing these nothing nothing
helps more than just taking a lot of
pictures a lot different kinds of seeds
you can do some synthetic tests like
these folder tests and you can do some
quantity quantitative analysis of this
but you're always going to run into the
problems like the sunset problem so I
want to thank Steve horribly Graeme
Finlayson big lion soo much Avila ken
Doyle
Pal paradise and other colleagues from
vivillon and HP over the years and so
here's a final image and if you just so
this is using sort of a grey world kind
of correction whereas if you do a better
job of the Illumina you get the richness
of the blue and also last week my talk
was replaced by this stereo talk and
they they actually talked about the fact
that the as you go further back the the
atmosphere can make things bluer in the
background so this is a good example of
of that kind of depth cue coming from
that so it's just linking those two
talks to so that's it
any other questions
yeah
so question is how much does it vary
between humans about what is white I I
don't know I have never I've never seen
any anything that has that has discussed
differences between what is white there
are obviously differences between color
and comb sensitivities and color space
preference there's a lot of difference
in preference but as far as any kind of
measured thing I'm not aware of any
certainly preference there are
definitely differences what's the what's
my opinion of seeing content approaches
to doing white balance and and so it
recently new cameras have kind of come
on the market that do face detection and
what can do flesh tone correction and
and that's it's it's exciting I don't
know if it's gonna solve have a better
solution to the problem so I'm
optimistic that it'll help for sure I
mean if you can if you can use your the
the fact that there's a face to limit
the same way you did we did the aluminum
constraint if you could use that to
constrain the possible results to be
something that would be within the realm
of skin tone colors then that would be a
powerful tool you got to be careful
because different cultures prefer
different skin tones to be rendered
different ways and so there's you got to
be careful but I think it could be quite
helpful yeah
because possible pajama game I see two
polygons one for direct illumination one
for the roll a scattering sure I
definitely you so ultimately if you can
determine what the color of the
illumination was from every point in the
scene it's useful information whether
what I'm what I'm arguing against is
correcting it out of every pixel in the
scene you'd be better off taking that
information and choosing something in
between so that that's my argument for
photography you know machine vision is
very different completely different yeah
so so the difference between first the
difference between the the fobian three
stacked versus a general general bear
there the biggest difference is the
spectral sensitivity bandwidth the
Foveon senses are broader so the the
correction the space you need to the
transformation you do into the space is
more important and it's a it's the broad
sensitivities basically illuminant
determination works better the more
orthogonal look the colors are so you
need to do transformation into something
that's sharper it's more important to do
that in the case of the broader
sensitivities then it would be in the
bear now as far as having two greens
that are that are actually different
from each other which some cameras some
newer cameras do they put it like an
emerald in or a white or something like
that in place of two greens that it
gives you more information for sure they
they don't the at least Sony's emerald
didn't I don't think helped all that
much but a fourth channel will always
give you more information and yeah for
sure for a luminance determination that
would be helpful to have another
dimension for sure the slight
differences that you would get between
two greens that are just only very
slightly different from each other the
potential that could also help because
the the difference between illuminant
colors from one illuminant to the next
very slight in itself and actually
there's some I can tell you there's a
paper on this chromogenic camera it's
called which uses that kind of idea
where you have two slightly different
sets of sensitivities and that's a it's
a very powerful technique and actually
gives you much better results but there
you have two different sets of RGB so
you have RGB and then RGB gone that's
going through another filter so you
actually have six dimensions coming in
but it gives you more than just what you
would gain from the added dimensionality
that's a it's a
show you the reference to that it's
pretty any other questions
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>