<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DocEng 2011: An Efficient Language-Independent Method to Extract Content from News Webpages | Coder Coacher - Coaching Coders</title><meta content="DocEng 2011: An Efficient Language-Independent Method to Extract Content from News Webpages - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DocEng 2011: An Efficient Language-Independent Method to Extract Content from News Webpages</b></h2><h5 class="post__date">2011-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4xc2VmaI4dI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this afternoon's first session is the
title of it is visual analysis it might
conventionally in the past been called
document analysis but I think this is a
nice distinction it's that it's looking
at documents as visual objects very
various papers doing this and trying to
extract information from the visual side
of what is in the document our first our
first talk today is from Brazil it's
when he presented by Eduardo Cardozo and
they're looking at getting information
from news web pages which is obviously
an important class of web pages so and
this is a this is a group that some of
the people in this group have a long
history with doc enjoji rioja driguez
worked with luis fernando gomez Suarez
in at pookie he oh and and has been a
contributor at various times in the
document engineering community thank you
can everyone hear me okay good so I'm
going to present to you today an
efficient language independent method to
extract content from news web pages it's
a rather long title but it's very simple
so I'm going to start with a quick task
overview given a news web page we want
to start by detecting the relevant
content which is the the middle portion
where the actual news story is and we
want to go one step further and extract
structured data out of it so we're
particularly interested in title
publication date and the news body why
is this important our scenario is search
engines so we want to index documents
for textual search and it can also be
used as a basis to apply natural
language processing and other tasks on
top of the of the text there are other
uses but they're not our primary focus
such as screen readers for
the visual impaired and reef lowing for
small screens a quick recap of the state
of the art page level approaches are
very common there are very good results
for most labels by labels I'm entitled 8
body some some works talk about author
advertisement etc but most of them rely
on page rendering which for our scenario
is too slow so our desirable
characteristics are broad applicability
which means a page level approach and we
add one restriction that we want to be
able to apply this to a document written
in any language english portuguese
spanish even Chinese we want a high
extraction quality that's very important
and performance we're dealing with a
high volume of documents so page ran
range is low and we need something
faster I'm going to talk a little about
our approach we use a machine learning
model to classify down tax notes and we
use visual features but since we don't
use page rendering we wrote our own CSS
parser and I'm going to talk a little
about that ahead and again our focus is
on large-scale document processing so I
will start I didn't find the relevant
content and then we proceed with title
detection data tection and body
detection in this order to detect the
relevant content we use the NCE
algorithm which is a previous work of
ours it was published it in 2009 I'm not
going to go into the tales of how this
algorithm works but basically we look
for a massive portion
text in the page and then we proceed by
refining it and other algorithms could
be used here such as readability which
is well known and once we detect the
relevant content we will be working on a
subset of the Dom nodes in the document
and the subset is based on the relevant
content we expand it a little bit but
the focus is on the relevant content so
from now on everything happens on this
subset of nodes we have a small set of
features and I'm going to highlight the
ones we feel are most important for each
step for title detection the edit
distance from the text node to the
documents title tag is often very very
useful because they often overlap the
font size is very important the title is
usually displayed in bigger font sizes
and we use the number of similarly
styled nodes in the document this
deserves a little explanation when you
look at a news page we can often
identify the title very quickly even if
it's in the language we don't know how
to read or speak that is because it's
usually presented uniquely it stands out
from the rest so we look for a
combination of font size font color and
both text and we count how many nodes
present this combination in the document
and titles are usually unique in this
regard so this feature usually has value
of one or two for titles and if we can
find a suitable title we just go with
the documents title the title tag
for detection we usually look for small
notes with a small amount of text and we
look for the presence of digits this is
because usually we have a timestamp such
as 2 p.m. or a date such as September
21st and we see numbers there most of
the time so it's it's usually a good
signal and we look at the distance to
the identified title from the previous
step usually when you look at the title
and the date in the Dom tree they're
usually close so this is a good signal
to detect dates and we look for nodes
close to the title this has a small
issue that our model becomes title
dependent so if we happen to miss the
title or data tection won't be as good
we performed some experimentation and
this didn't didn't show to be very the
results were still good it wasn't as bad
as we thought and we just kept it
because he was looking good and for body
detection we were going to use the
relevant content that the algorithm
provided us not the expanded subset and
we're going to remove the title and the
date if they are contained in the
relevant content that's going to be our
body we tried some other approaches but
they didn't really show much improvement
so I'm going to talk a little bit about
the quality and applicability of our
method
we use two corpora the first one we
created ourselves it has 200 pages
written in English Portuguese and
Spanish and then we used another corpus
which has works published in the
literature i believe it was presented
last year document engineering as well
it's the new 600 and it has about 600
pages all written in English so we
started with an intra site
cross-validation what we call inter-site
is actually we train the model on some
pages for instance pages from CNN and
when we apply it we apply to other pages
also from CNN so the model is able to
capture some characteristics from that
website and the results we have our
about ninety two percent of f1 4 title
84 for dates and 87 for body and we also
did what we called an exercise
cross-validation which is the opposite
we trained using CNN but we will only
apply to BBC Fox News and other websites
so this this shows how well our approach
can be used for pages that we didn't
intend originally free to be used on
unseen scenarios so we still have 91 %
for title 77 for date and 87 for body I
have a slide here with the difference
from one to the other and we can see
that title and body have very stable
results we can capture these two labels
with good results and we see some
fluctuation on date and we attribute
this to the fact that we don't use
textual features to detect date so we
don't have a dictionary of days of the
week or months because we're interested
in applying this to any language so we
didn't use that we also try to apply it
to some pages in Chinese but we didn't
have annotations for it so we don't have
any numbers but the the visual the
visual inspection that we conducted
showed some consistent results and then
we apply to new 600 which we didn't use
until the approach was finalized so this
is this is good to check if we're bias
it by our choice of pages and it seems
we're not because the results were so
good we have ninety-three percent for
title 86 for date and 88 for x so i
talked about quality i talked about how
our method can be applied to several
several pages i'm going to talk a little
bit about the performance which is the
part where most interested we use the
subset of downloads a subset of notes
from a document and we were very
conservative when using the subset we
didn't discard as many nodes as we could
and we still managed to discard about
half of the nodes in the page and when
we look at the time impact that this has
it cuts are or evaluation of nodes about
by half which is looks good it's
proportional to the amount of nodes we
discarded so if if we were more
aggressive we could get better running
times since we wrote our own CSS parser
we were able to optimize it as well not
all rules affected or features so we
were able to focus only of those that
actually had any impact and those were
about thirty percent of the rules we
found and when we look at the time
impact we could also reduce it
proportionally
we could then combine this bull this to
optimizations and or total execution
time dropped to about half what we had
it's the left graph it only dropped
about half because we have some some
tasks that couldn't be improved here
such as I oh and the HTML parsing which
happens in both both methods and that
data structure initialization etc and on
the right we can see our method compared
to an equivalent method that instead of
our HTML parser and our CSS parser uses
a rendering engine such as WebKit and
this is a very optimistic approach
because we're using web kit without
images without JavaScript and without
plugins sometimes these are important
but in a best-case scenario you have all
of these turned off and we can see we
can run in about sixteen percent of that
time which is very good when you have a
large number of documents so to conclude
we have stable results in a variety of
pages we have a good extraction quality
some methods have better numbers than
ours but they don't have the same
performance that we do and it's a
trade-off and we feel we have good
results given given the constraints and
we were able to use visual features with
a much faster running time than when
using a full rendering approach such as
using web kit and we plan on extending
this work in the future to other tasks
on the web and that's about it thank you
very much and if you have any questions
are there questions for Eduardo
some curious you talk about performance
and so both in my PhD research and then
in my first job afterwards we did
content analysis and PDF document class
seems very dirty words right around I
will I will speak louder okay so yes in
my past research and in my actual work
I've done sort of content analysis and
content recognition we've had much much
higher percentages of accuracy are much
more complicated document forms
including newspapers and magazines with
train models that analyze sort of
indexing and positioning and things so
you your reasoning behind this was
performance the you said that you can
basically discard a lot of this
information and just look at certain
characteristics and try to determine
that they use that information to build
properties about the document one thing
I would ask is when you don't have
something like HTML and CSS why it's
reasonably trivial to calculate font
properties if you look at say a
postscript model or something in a
different front document category where
you have to basically render the
document to get a lot of the analysis
out of it where do you get benefit from
this algorithm and how does this scale
to other problems is what I'm trying to
get that you're giving the reduction in
accuracy how we could apply this method
to other documents such as PDF I'm
trying but I'm trying to understand how
the scales in terms of performance as it
moves to file formats that aren't is
amenable to the sort of analysis you're
doing and the you lose quite a bit of
accuracy on a lot of the algorithms I've
written in the past on analyzing
documents and I'm trying to understand
why you why you made that trade off why
you feel the performance is such a key
characteristic okay it's actually I
think a two-part answer the first one we
had some technical limitations on the
project so we were using decision trees
as our machine learning method and we
did some quick xparent a shin later and
changing the method to something like an
SVM and maybe tweaking some parameters
we were able to achieve better results
this would be that does room for
improvement here just changing the
method
but I'm not really sure you mean scaling
it to other documents I didn't really
get this part yes that's / that's a
better term this this HTML and CSS is
very amenable to this type of analysis a
postscript based document for example
probably wouldn't be I would be harder
to produce and so the traditional sort
of glyph analysis and and measurements
and the styling characteristics of those
and sort of blood blackboard techniques
are bringing sort of multiple sources of
data to produce sort of very high levels
of content recognition but they are a
lot slower I'm trying to understand why
the performance was such a and why you
fellows a need for that and just
generally understanding what you try to
achieve overall yeah so we need the
performance because we're dealing with a
lot of documents and we need you to
process them quick and I think that the
performance is very particular to to the
scenario of HTML and CSS since we're
able to discard most of the document we
were able to look at smaller portions of
it and only the bits that interested us
so that's where the performance comes
from the approach probably works if we
calculate same features for for instance
on a postscript PDF file it probably
works we haven't experimented with it
but yeah the performance wouldn't be the
same we will have to do some research on
that specific to type of document does
that answer your question yes we can
talk later other questions
okay well let's think Edward o for his
prestige</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>