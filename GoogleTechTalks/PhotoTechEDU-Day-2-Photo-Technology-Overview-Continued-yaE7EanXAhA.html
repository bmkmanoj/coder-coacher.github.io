<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 2:  Photo Technology Overview Continued | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 2:  Photo Technology Overview Continued - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 2:  Photo Technology Overview Continued</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yaE7EanXAhA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for coming I I think we
got at least half the people we had last
week so that's a half-life of about a
week maybe probably some wandering in
late stuff yeah so as you notice we
released last week's lecture externally
because we didn't talk about anything
secret so I'd like to keep it that way
when we have more Google specific topics
we'll have we'll have a private session
on that so if you have questions that
relate to anything relevant to your job
here save them to the end and we'll
we'll get at them then so let's get
started we we left off in the middle of
a bunch of random topics last week so
we're still sort of doing overview of
the front end part of photography of
image capture and optics and sensors and
color and stuff like that not not so
much about what to do with digital
images once you get them that'll all
come later there's a there's a few
optics topics I want to introduce in
this and next week we'll have ROM
Clements going to give us a lecture on
optics and ray tracing and stuff we'll
get into more depth on some of these
things but there's there's a lot of
things that come up in in lenses when
when you have something more than a
simple lens so when you don't have just
a single element but for example here's
a just a drawing of what you can do with
two elements and there's some dotted
lines in there that are trying to show
you that from the from the back side of
this lens if you look in there it looks
as if the light is all coming from a
pupil or an opening that's that's here
that is there's a there's a virtual
image of the stop the stop is this this
line a that represents an aperture that
blocks rays of light any further out
from that so that's the aperture stop
when you when you look into the lens you
see a typically a magnified image of the
aperture stop or in some wide-angle
lenses if you look in the front you'll
see a diminished image of the aperture
stop and that that virtual image what
you see when you look into it is called
the pupil so in the front of the lens
you can see the infant's pupil in the
back you can see the exit pupil and
sometimes they're about the same size as
in this picture they're not they're not
quite the same size the entrance pupil
smaller than the exit pupil and some
lenses they're very different in size
and so there's
lenses have what's called a pupil
magnification factor which can be quite
large in some cases if you if you pull a
lens off a camera that you have if you
have a wide angle lens in particular and
look in the front when it's stopped down
a little bit you'll see a little tiny
opening turn it around look in the back
it's got a great large opening those are
the pupils you're looking at the pupils
are relevant for various things that
you'll be doing in in digital
photography for example in the the the
exit pupil is relevant because it
controls the the directions of the light
rays hitting the sensor we're having
another earthquake I'm going to walk in
on the roof don't know if you stop down
that aperture stop really small to limit
the amount of light that comes through
the lens towards almost just a pinhole
and you look in the back of the lens and
you see where that pin hole is that that
determines where all the light is coming
from that's the center of the of the
pupil that's illuminating your sensor
and basically determines what's called
the chief ray angle the chief ray is the
ray that comes through the center of
your aperture so when that with that
that chief ray the middle of your
aperture is incident on your sensor and
it's not coming in perpendicular to the
surface of the sensor some sensors are
kind of particular about those angles
and so this knowing how far the exit
pupil is from the focal plane lets you
compute as you get out to the edges of
your sensor lets you compute the angles
that those rays are coming in at and you
want to make sure that your lens is
compatible with the acceptance angles of
your sensors so that's why the that's
why the exit pupil is a relevant thing
in digital photography it was much less
important in the film days because film
is has got a pretty broad acceptance
angle and take light from any angle but
digital centers tend to have little
micro lenses on them that concentrate
the light into the sensitive photodiode
areas and those are those make it pretty
directional yeah
right so the question is what does it
mean when people say lenses designed for
digital part of it is that they're
designed for the smaller field of a
digital camera but does it also mean
that they have lowered chief right
angles for example and and yes in
general it does for example here's my
camera I have the Sigma sd10 some
coincidence there again they came out
with a really nice normal ends of fast
normal lens for the smaller sensor size
normal lens is about a 30 millimeter
focal length for this camera they have a
1.4 f 1.4 lens for it is quite fast if
you look at where the exit pupil is on
that thing there was a simple lens to
single element lens the entrance pupil
exit pupil an aperture stop are all in
the same place and the distance from the
focal plane is equal to the focal length
so it would be 30 millimeters from the
focal length but in fact for that that
normal lens that's optimized for digital
cameras the exit pupil is about 300
millimeters from the focal plane which
means it has a pupil magnification
factor of something like 10 so it's huge
and most of them are not that extreme
but the lenses that work well with
digital cameras usually have a distance
to the exit pupil of say between 80 and
150 millimeters whereas there even when
their focal lengths are quite a bit less
than 50 so that's important when you
look at the other side of the entrance
pupil what is that good for and that's
what what that's essentially good for is
making panoramas if if I've got a camera
here I want to take a take a couple of
pictures over here I want to take
another one here and get it to to stitch
together while I want the view point for
these guys in the view point for these
guys to be the same in particular for
Jeff in the middle here when he's in
both pictures I want to be shooting him
from the same position so I want to
rotate the camera about the entrance
pupil because the entrance pupil is
where its what he would see if he was
looking at the camera and I turned it
and it didn't move that's a good thing
that means that his
things behind him aren't going to shift
relative to him so you won't have
distance parallax if you shoot
everything from the same position as
defined by where the light goes into the
camera which is the entrance pupil so
those are optically important concepts
there's other concepts about lenses
about various cardinal points and so on
the that will mention later how many'd
people in here have a single lens reflex
camera of one sort or another that's
probably about half okay so you probably
know something about how those cameras
work single lens reflex means that
there's a reflex mirror that you're
viewing the scene when you when you look
through your camera you look through
your viewfinder you're viewing the scene
through a mirror and the same lens that
you use for taking the picture with so
there are these complicated optical
systems where the the mirror here has to
get out of the way to take a picture so
it flips up when you push the shutter
release that's why it makes all those
clunky noises but you might not know is
that there's actually a secondary mirror
in most of these cameras that's behind
the main mirror and there's a half
silvered region in the center where some
of the light goes through bounces off
the secondary mirror and down to the
bottom where there's a little out of
focus sensor assembly this is because
you need you need some while you're
looking at the scene through the
viewfinder the camera wants to be
looking at the scene through a mechanism
so I can figure out how to focus ahead
of time so when you push the button it's
done focusing and and it can take the
picture in a hurry sometimes there's
also secondary optical paths up in the
top for an auto exposure sensor there's
all kinds of interesting mechanisms in
there and well well perhaps talk about
about some of those but when you have a
camera that's not a single lens reflex
and there's no reflex mirror in there
and the the main imaging lens is always
focusing the image on to the main sensor
you can use the image on the main sensor
to focus with but it's harder to get a
good enough it's harder to get a good
fast focus response when what you have
to work with is
the actual sensor image these autofocus
sensors are very clever because they
actually look at the scene through the
through two different halves of your
main taking lens and try to compare the
the images from the two halves to figure
out which way it's out of focus we might
have another look at that later but so
ramble through a few topics quickly here
as a question what's a pentaprism
pentaprism is the thing on the top there
that has five sides to it you see the
Pentagon it's actually it's it's drawn
in cross-section here as a Pentagon it's
got total internal reflection at two of
those faces and it's like going in and
out of two of those faces and another
one that looks like it doesn't matter
it's actually more complicated than that
because it's a it's a solid that usually
it's a roof prism with some interesting
structure and the other dimension to get
the image flipped around the right way
question over here
Wow a good question what controls the
exposure time when you set it to
something like a hundredth of a second
is it the mirror or is it the aperture
it's neither of those things it's in
fact this line here represents a
focal-plane shutter this is the sensor
back here and shortly in front of it is
a an arrangement that's usually made
with two curtains the curtain will be
closed and then first curtain will drop
to start the exposure second curtain
will drop to terminate it and if you set
it for a really short exposure time the
first curtain starts to drop and the
second one starts to drop right behind
it and you get a slip going across the
sensor to expose it for two thousandth
of a second both film and digital
cameras use essentially equivalent
shutter mechanisms mechanisms there
pardon me not point to what oh not point
and shoot digital snow ssl ours use this
kind of mechanism point and shoot
digital 'he's typically whether the lens
is not interchangeable they integrate
the shutter into the lens assembly
somewhere even some let's see even so
you know almost all non SLR cameras use
a in the lens shutter in particular if
the lens is not interchangeable you can
put your shutter in the lens some SLRs
may also use an in the lens shutter for
example the hasselblad medium format
SLRs they have a complicated mechanism
where they the the shutter and the lens
is normally open so you can look through
it and when you want to take a picture
it closes the shutter raises the mirror
opens something to expose the film then
it opens the shutter again closes it
again and when you Reese when you wind
the camera resets it all back to normal
yeah
yeah the question is why not do it
electronically so you you can make
sensors that have what's called
electronic shuttering where they they'll
be sensitive to light for some interval
of time and then under electronic
control they stop being sensitive to
light and you read out the signal it's
hard to do that well some very low end
cameras do that cell phone cameras
generally work that way it's hard to
make a image sensor be not sensitive to
light and it's hard to store the charge
signal during that interval when you're
trying to read it out in a way that's
separate from storing it on the
photodiode that is sensitive to light so
I've made cameras at work that way I'll
show you one but in general it's if you
can use a shutter you're way better off
okay what questions on that one some use
a combination yeah you can do like a
half shutter where you you
electronically can reset the thing and
then start integrating photo charge at a
certain time under electronic control
and then terminate it by closing a
shutter that's called a half shutter and
things like that so the the point of
this slide is just to say that when you
when you're doing photography you need
some processing infrastructure you need
to be able to take the images out of
your camera and have some kind of
standard ways of processing the images
and making them into good pictures you
also need some flexibility you need to
be able to tune and adapt your process
to your needs over time and fortunately
we don't have to do that in the field by
standing under a black cloth anymore
although if you have a typical laptop
display you might still want to use a
black cloth but you don't have to use
the nasty chemicals that we used in the
old days I don't have a good public
domain picture of someone someone using
a laptop doing this but I thought this
would be a better illustration of the
whole idea of a processing chain you
also need good automated back end
processing and Kodak figured this out a
long time ago when they sold a camera
with the with the slogan you press the
button we do the rest so that's that
means a they've implemented a processing
pipeline and once you once you've
captured the image it's up
to them to do something something
standard to get a good image out of it
when you're working with images you
really need both of these things you
need a personal infrastructure for
processing images and you need a sort of
a standardized high-volume
infrastructure for processing images and
this again just to introduce that
concept i'm not going to say anything
anymore about it today i'd like to go
back into vision optics and color a bit
the human eye as i mentioned last time
is the is the ultimate arbiter and
consumer of these images that we want to
make so you need to understand a little
bit about how it works and it works a
lot like a camera it has a lens it has
an entrance pupil which is what you see
when you look in somebody's eye in the
middle of the iris there's a little dark
area that's where the light goes in and
in fact you're seeing a slightly
magnified view of what the actual
opening of the iris is because of the
curved surface in front of it so what
you're what you're seeing the apparent
size of the opening and the pupil is is
in fact literally the entrance pupil it
has this variable iris the iris
diaphragm the in the eye it's called an
iris and a camera it's called an iris
diaphragm when it's a variable thing
that works like the eye and an image
forms back here and it doesn't have a
flat film plane or sensor plane the way
cameras tend to tend to have but it has
a spherical one and it it works well
enough in the in fact probably works
better than flat in the center there's a
there's a thin area where where you
don't have so many nerve fibers sitting
on top of the photoreceptor cells that's
called the fovea centralis and that's
that's when you look right at something
and you see a lot of detail you're
you're aiming your eye to put the image
right into the phobia here it's only
about a degree or two wide and that's
the region that's dedicated to high
resolution color vision it's also the
reason why if you look at a dim star
tonight you can't see it because there
are no there are no high sensitivity
cells in there there's no rods so you
see with rods and cones the cones come
in three types and they they mediate
color vision and the rods are for low
light level vision since there's no rods
in this area
night when you look at the sky and you
see stars all over you're seeing them
all all out here when you look right at
a demo and you can't see it because you
don't have anything sensitive enough
there to pick up really dim light so
it's a pretty interesting structure if
you look in the phobia you see the cones
tightly packed there these these kind of
long skinny tapered cells sort of
cone-shaped and they're they're packed
in columns and if you probe individually
through a bunch of those for the
microscope microprobe of some sort and
see how they respond to different
wavelengths of light you can get all
these these plots of response versus
wavelength and you can classify the
cones into three different types
depending on whether they peak up at a
at a longer wavelength the medium wave
length or a shorter wavelength and we
might refer to those as red green and
blue although they're more properly
called long medium and short and then
that defines a color space called LMS
color space long medium short tone color
space there's very few blue ones in the
fovea and some sources say there aren't
any but this guy says he found a few
that means if you if you look right at
something like yellow text on white
paper the only difference between yellow
and white is how much blue light is
being reflected there if you look at
yellow text on white paper it's very
hard to read you can see that it's
yellow and if you look at it you know
perfectly it's yellow makes a great
highlighter for marking large areas
because you can detect that outside the
fovea but the yellow text on white paper
the detail is all in the blue and you
have very little sensitivity or detail
to the blue in here that's why with
digital images you can blur the blue
channel or the blue yellow direction the
B channel of lab space it's called you
can blur that a lot and not notice it so
we're going to talk a lot about color
color vision and color sensing like to
go back to James Clark Maxwell in in
1860 he did some experiments and and
came up with what we now call color
matching functions and this has to do
with how to how to mix different lights
to get to match a certain color say a
single wavelength of light or some other
some other different mixture
and he he figured out what it meant to
have three primary colors and there was
a theory around for some time due to
Thomas Young from about 1800 about the
about the I having really three
different dimensions of color perception
which are these these three primaries
these three sensations and Maxwell
figured out that this is right that
there are these three separate
sensations in the eye that correspond to
these three cone types we were talking
about but that you can't stimulate these
three sensations independently of each
other so for example if you pick
different wavelengths of life and I'm
not sure what the units are that he's
numbered in here but I think it was kind
of an arbitrary scale on his prism or
something that he's plotted these curves
blue green and red that correspond to
his idea of what might be going on with
these three sensations that he's
stimulating and in general there's
there's always more than one being
stimulated he may have already done some
some processing here because I see he's
got curves that go negative which is
yeah so these are done in terms of the
mixtures of certain colors of lights and
sometimes the to match a certain
wavelength you need negative amounts of
some primary color which seems
unintended ray tube color triangle
relates to negative values of color
matching functions and colors that are
outside the reproducible gamut and so
forth so Maxwell pretty much figured
this stuff out back in eighteen sixty
and had this notion of a of a color
triangle and understood what it meant
for a color to be in the triangle or
outside the triangle and he also made
the first color photograph and this this
theory of trichromatic color vision came
to be known as the young Helmholtz
theory because Thomas Young had
originated it and Herman Helmholtz did
some work on it but Maxwell is really
the guy that figured it out and made it
work and he made the first color
photograph by taking a picture of this
tartan ribbon through three different
jars of colored liquid he dissolved
different chemicals in water to make
filters if you read his paper he'll tell
you how much of what chemical to
dissolve and what volume of water if you
want to be pretty
experiment and he took the three
pictures through that and then he took
the he developed the negatives and
turned them back to positives and he
projected him back through three jars of
liquid and did this demo in front of the
Royal Society in London so he was able
to produce on a screen a color image
about like this it's not a great color
image it was is enough to show the kind
of the three dimensionality of color but
it wasn't a great color image because at
that time none of the photographic
materials that were in existence for
sensitive to red light and so to get to
get anything resembling trichromatic
color he had to kind of fake it and he
knew that in his paper he mentioned that
it would work a heck of a lot better if
they had materials that were sensitive
to red light they could do blue light
they could do green light but for the
red channel I think they actually used
some ultraviolet and so they got they
got three color channels but they got
kind of a false color yeah let's
was it a revolutionary development yeah
I think so I'm not sure what the
reaction was I wasn't there but it seems
revolutionary when you read about it I
have I brought a few books today one of
the books I have called sources of color
science has three of Maxwell's papers in
it and you know they were important
enough that in retrospect they were
selected among the world's top dozen
papers in color science it was quite a
big deal i mean the first color
photograph i think that's quite a big
deal i'll show you some more early color
photos in a minute here yeah okay so a
NS if you have four different color
receptors that is three cones and the
rods why don't we have four dimensional
color perception and the answer is that
actually we do but there's only kind of
a narrow range of intensities over which
that matters and what what happens is
when when you're in kind of normal room
light like this where you see a lot of
color the rods are pretty much all
saturated out they're not doing much for
you when you're in dim light you know
under starlight or something the cones
have no response at all you have sort of
monochromatic vision but somewhere in
between like if you're sitting around a
campfire where you've got you've got
kind of enough light to see maybe some
moonlight starlight and so on you've
also got the the red glow of the
campfire you actually have kind of a
kind of a limited color space there
where you you're sort of trading off the
long wavelength cones illuminated by the
red glow of the campfire against the
response of the rods from the other
shorter wavelengths but there's only a
very narrow range where all four of
these things come into play and it
people have studied it and they've tried
to make a sort of a modified four
dimensional color space but it doesn't
buy you much it's not worth a lot
because people normally don't want to be
looking at colors in such dim lights and
so if you're if you're doing photography
and trying to produce stuff to look at
it doesn't really buy you anything yeah
Lance
yeah Lance point out some people have
for cone types that's pretty rare a lot
of people have two cone types and some
only have one but very few have for
question the back yeah question is do
pigeons have four I don't know about
that I hadn't heard that but they they
animals have a wide variety of different
kinds of color vision it's quite
fascinating so if we were going to be
designing photography for the pleasure
of pigeons and other animals we'd have a
kind of a different game but the same
the same kind of theory would apply I
think yeah so he says we had four
evolutionarily at some point and we'd
lost two of those and split one of the
others into the to the medium and short
medium and long yeah the question here
okay so birds have four types with
little beads of oil acting as micro
lenses over the cones very clever those
guys wow I'm learning a lot here today
thanks
so this is just a representation of a
that you'll see a lot this this space
this is what we call XY space it's
called a chromaticity space which means
it represents the the hue and saturation
of a color without regard to brightness
intensity or luminance so it's only a
it's a two-dimensional color space that
lets you distinguish nominally different
colors without worrying about the white
grey black dimension this space and the
the precursors that it depends on we're
all defined in 1931 at a standards
meeting of this CIE this is I don't do
French but this is some Commission
illumination stuff Franklin someone can
tell me what that means they define
these what are called color matching
functions or try stimulus curves and
basically these can be used to measure a
color if you have if you represent a
color by a spectrum of light say you
have a surface that has some reflectance
spectrum and you illuminate it by a
light that has some emission spectrum
the product of those gives you the
reflected light spectrum and that's like
an infinite dimensional thing you don't
really want to deal with that you want
to reduce it to a three dimensional
subspace that represents what the three
cones and the human eye would detect and
you could do that by using the the three
response curves of the three types of
human cones but what they did instead
and the CIA was to define these other
color matching functions that are a
linear transformation of the three
that'll get you into the same subspace
but with different axes and they define
them in such a way that this one in the
middle which is called the Y curve has a
has a value that's that at every
wavelength is proportional to the the
extent to which light of that wavelength
will give you a perception of brightness
so what this is telling you is that
stuff in the 500 to 600 and some region
is a lot brighter than stuff out near
400 nanometers
700 nanometers so in fact there's a
concept of luminance luminance is a is a
defined photometric term that means the
the dot product of this Y curve with the
spectrum of a light falling on a surface
or not falling on coming from a surface
so the the Y curve was picked to define
luminance and then the other curves were
picked to to give you to color
dimensions that are they're not exactly
orthogonal to luminance but they give
you the extra information and and they
were picked with some certain properties
as well so the Z curve was picked to be
a narrow compact blue channel to be as
low as possible over here but not
negative anywhere and then the red curve
was picked to to give you the there was
nothing left to pick at that point it's
it's determined to give you the right
three dimensional subspace that one's
determined at that point when you when
you take a spectrum and you use these
three curves to project it down into a
three dimensional subspace you get a
three vector called XYZ that represents
the color of that light the subspace
should have the property that if two
different spectra look the same to a
human that is the same color then they
give you the same X Y Z value and if
they look different to a human they give
you different X Y Z values so that
doesn't mean that the human internally
represents x y&amp;amp;z anywhere but it means
that the what happens when light goes
into the eye is these long medium and
short cones respond with their three
color matching functions and they give
you they reduce the spectrum to this
three dimensional subspace if we can
with curves like this get to that same
subspace even though it's on different
axes then we should have this property
that points that look the same to the
human are the same XYZ coordinates and
things that look different to the human
are different XYZ coordinates that's
that's kind of the idea on which
colorimetry and color matching is based
if you take the x and y coordinates that
is well take all three coordinates the
the X the Y and the Z that you get from
these three curves you take each of X
and take the sum of those three X plus y
plus Z and divide that into the X and
divide it into the Y that's how you get
the little X and a little y that are
plotted here so this is just the big
exit little y big x and big y divided by
the sum of the three and that's how you
get rid of the the overall level
dependence and that defines these
coordinates this these x and y which
can't be greater than one obviously
because you're dividing x by x plus some
other things that can't be greater than
one so there's a there's a triangle here
that goes from zero zero out here to 10
and up here 201 and here's the other
edge of the triangle and all colors have
to sit within that triangle and you can
define other color spaces I should
mention what Orban Schroedinger did it's
a little bit out of sequence so by show
this here just to show that before the
CIE standardized all this stuff in 1931
a lot of good scientists were
experimenting with it and trying to
figure out how to measure and represent
color Schroedinger had figured out that
there's this colors basically a well
it's a three dimensional vector space
Maxwell knew that and people knew that
for a long time but he characterized the
shape of that space and he showed that
you could you could kind of slice it and
get this curve which so here's a a cut
where he's cut this this
three-dimensional solid that has black
at the apex over here and the shape of
this thing here is the same as what we
saw in the previous side the shape of
the boundary of this region is the the
coordinates that you go through when you
when you use a monochromatic light
source that varies from long-wave here
through shorter and you know all the way
from red back to violet here so that
shape was already known sure under had
mapped it out pretty well here he showed
how to make color triangles around it by
projecting this red green edge and this
violet indigo edge out and this edge
between red and violet and making a
triangle that would fit as close as
possible to that
space of colors when the the CIE didn't
choose that triangle they could have
they chose a bigger triangle that didn't
fit as tight he also showed this thing
the spectral curve where if you take a
given intensity of light and you start
out at start out at red and you bring
the wavelength down it gets to be more
and more visible and it goes around this
curve and comes back to back to black
again after violent so he had pretty
well mapped out everything but he didn't
reduce it to kind of standard standard
vectors and these guys did in 1931 and
more recently there's a there's a new
standard that everyone uses here's a
question okay how does the closeness of
the triangle matter let me say a few
words about that now because the
standard we all use now for RGB color on
computers with some exceptions but
mostly we use this thing called srgb
which is a the little s was meant to
stand for standard because
hewlett-packard and Microsoft had the
power to make it a standard when they
defined it and it's now an international
standard adopted by the IEC and the CIA
and the I Triple E and all these other
guys I so it has a color triangle too
and in this case the triangle just like
we have this big triangle out here that
has all the colors inside of it the srgb
space has a smaller triangle and these
are all the colors that you can
represent by taking values of r g and b
that are positive or non-negative and
the values of r g and b are what you get
by taking these color matching functions
and doing a dot product with the
spectrum of the light you notice all of
these functions have negative regions
which means that if you have a
monochromatic light say here at the
where the red curve is most negative
that means that the the red value that
you get for that color of light is quite
negative the blue value is zero and the
green value is positive and if you work
out what color that is in in XY space
you can show it on this picture and it's
it's out here somewhere where the the
blue was zero which means you're
over on this edge and the route but what
it means you're out here somewhere and
what that means is the because red is
over here red is kind of measured from
this line positive this way- red is
anything to the left of that edge of the
color color triangle so this is an
example of where the color triangles
made quite small if you make the color
triangle sort of as tight as you can to
this space and you can see the XYZ
triangle is pretty tight it's tight on
this side it's it hits the bottom and it
hits over here but what what
Schroedinger had proposed was to put an
edge parallel to the the blue the violet
indigo edge here and that would make a
triangle is kind of long as skinny out
there and then another edge right here
so as long as the triangle contains all
of the colors you can represent with
positive numbers any color but on
computers we don't normally want to do
that because what we what we really want
to represent our colors that we can make
on a monitor usually that's where the
argue be spaces on computers come from
to produce a color on a monitor you have
to mix three primary color sources and
when you do that you can you can pick
three primaries anywhere you want in
this space when you do that the triangle
defined by those three corners is not
going to be able to get you all colors
and that's what Maxwell was able to
figure out there the notion of three
primary colors giving you all colors
only sort of works it only works for
colors inside the triangle defined by
those three colors unless you have a way
to make negative amounts of a primary
which we normally don't would like you
can match negative amounts by adding
neutral white light to one of the colors
you're matching and you can use that to
experimentally measure curves like this
but you can't produce those colors by
mixing those three primaries yeah lanson
yeah so Lance says if you have a
technology lets you absorb some
wavelengths from ambient light you might
be able to simulate that effect maybe I
don't know of any technologies that work
that way Peter yeah so what's what
prevents optimizing this curve to to
give you more of that space in fact that
you can do that there's a lot of
different color spaces srgb was kind of
a poor man's standard in that it was
defined to represent what your average
monitor does as opposed to some ideal
that you'd like to approach a lot of
other color spaces have been defined for
example adobe RGB puts the green primary
right up around here and it gets you a
lot more saturated yellows and a lot
more of these Bluegreen colors the CIE
defined an X and RGB space the same time
they defined XYZ that had monochromatic
primaries so they used 700 red I think
420 violet and 5 45 green it still left
out some colors but they picked they did
it for more metric purposes than
reproduction purposes and they picked
this wavelength of green to be something
that was easy to produce or the mercury
light source instead of something that
would give you all the colors you want
these guys were very practical now Peter
why can we see them yeah we we simulate
colors out here so you can see something
the kind of colors that you get in this
space that you can't really get in srgb
are like the deep blue green color of a
pool table felt there aren't a lot of
things around that are that saturated
that that you have a problem with them
and this this looks like a large part of
the color space but the probability of
any surface reflectance being out here
is extremely low it's hard to make
colors out there so you're not really
losing nearly as much as it looks like
what you look at what what colors you
can can you produce by mixing the
magenta yellow and cyan colorants that
are used in printing colored images you
get kind of a blob in the middle you
can't get out near these primaries at
all but you can get a little bit outside
this edge so there's some colors in here
that you can print that you can't
represent an srgb and that's why Adobe
defined adobe RGB for people that are in
the the print industry that want to be
able to print print everything they can
with their inks that question here yeah
why would you not want to always use a
broader color space excellent excellent
question so you can use Adobe RGB say
which is it's a it's a very commonly
widely supported larger color space
that'll get you more colors and what
happens if you use it it's on most
digital cameras it's an option most raw
converters it's an option well supported
in Photoshop and other programs and so
on said if you save your image in Adobe
RGB colour space and you put it up on
your web server and say look at my
pretty pictures what happens is people
download your pictures and most browsers
are not smart enough to look at the
profile and realize it's adobe RGB and
convert it back to the srgb that their
monitor can display they just treat the
numbers and display them just as they
would srgb and the result is that if you
had a if you had a fairly saturated
green in your picture that would have
looked like like you know 02
under zero in srgb space and just turn
on the green primary on your monitor
because the the adobe primary is way up
here now that that looks relatively
unsaturated relative to what they think
the primary is they'll put out a number
that's maybe fifty 250 or something when
you display it it's greatly unsaturated
and flesh tones move toward a kind of a
gray ash in color and so on so if you
put images out in a great wide gamut
color space and people view them in srgb
they look awful so that's why you don't
use it you only use it if you have
complete end-to-end control over what's
going to happen with those color numbers
if you don't you better you stick with
srgb which is really not much penalty
like I said all you lose is a few of
these deep blue green colors that you
could print and that's about it yeah Bob
there
okay I'm not sure I understand Bob's
question which is does the rarity the
rarity of these colors have anything to
do with its application and high
visibility applications oh hi vis yellow
so high vis yellow is really kind of
over here the wavelength that's most
luminous is about 550 and that's kind of
a limey greenish yellow so like tennis
balls and so on or the kind of as
reflective as possible in the red and
green parts of the spectrum and
absorptive in the blue part so those are
those are captured pretty nearly in most
color spaces it's really only these kind
of deep blue green felt color things
that you're going to miss oh yeah Lance
ass aren't those tennis balls actually
fluorescent and yeah they probably are
yeah Daniel seems like another reason
you want to keep the smaller
right Daniel points out another reason
to use the color the smaller color
triangle is that when you quantize to
eight bits you get more color resolution
you resolve the colors more finally when
the color triangle is smaller people
cite that a lot I think it's a pretty
small effect but it is it Israel yeah in
the back oh yeah excellent question
that'll that'll that'll lead me on to
some of my next slide share question is
how how do the primaries of srgb relate
to the primaries of the ccd or the image
sensor and that gets a little bit
complicated but that's where we're going
so let's let's let's move ahead tell
okay another excellent question tell us
if you're going to do a chain of
processing is there any value to having
intermediate values outside this color
triangle absolutely yes that becomes
especially important when you look at
white point adaptation so for example in
if you illuminate a scene with an
incandescent light most of your colors
in the XY space are going to be all over
here but when you reproduce that you
really want to move all those colors
over and make the scene bluer even
though there was very little blue light
in the incandescent illumination and in
particular there will be colors here
that are outside the gamut that when you
do white point adaptation you want to
move in gamut so you don't want to clip
the yellow edge before you've done the
white point adaptation I'll say word in
a minute about the the question about
what the what the spectral sensitivities
or primaries of the sensor have to do
with all this but let's look at
different methods of sensing color Wow
times going so fast this this may have
to spill over into a third introductory
lecture with it that's good I'm glad to
have the questions and a chance to
ramble on about it this guy Louis do
goes to her own some kind of genius in
my book he wrote a paper in which he
kind of he laid out all the different
ways you could detect color he talked
about using beam splitters and he made
himself a camera that works a lot like
this somewhat more modern Devon tricolor
camera where you have three different
plates you have internal pellicle
mirrors and filters to separate the
incoming light into three red green and
blue images on three different
photographic emulsions and take three
pictures this actually worked really
well from moving for getting color
images that could go to print because
you could take each of those negatives
and make a magenta yellow cyan printing
plate from it and print stuff that way
it worked less well for making
affordable snapshots because it's quite
a process to realign and reassemble
these things but it worked he he laid
out some of the other methods of doing
color this is well this is one of the
early photos he made with with a three
shot camera you can sort of see at the
edges the three things that he's lined
up and he's
his name on it lui de Costa her own 1877
so he was a definite pioneer of color
photography these guys auguste and louis
lumière who are often remembered for
inventing moviemaking they were among
the guys who figured out how to how to
do cinema they also did a method of
shooting color that they called
autochrome and they they took little
grains of potato starch and they made
vats three different vats with three
different dyes they died some of them
green some of them violet colored and
some oranges red color and then they
mixed them together and put a slurry of
this on a photographic plate with some
black in between so light doesn't leak
through so each of these is like a
little color pixel light goes through
the filter and exposes the plate behind
it then when they they would develop the
plate with a reversal process which is
like when you're making slides where you
take the areas that got exposed to light
when you develop it they turn black with
silver you bleach that away Yuri expose
it redevelop it now you've got a
positive put light back through it the
starch grains are intact through this
whole process so wherever you exposed to
the orange filter you reproduce an image
if you got like there you reproduce was
like coming back through that same
orange filter that same bit of potato
starch and you can make color images
this way and they sold plates for people
to do this and other companies sold
plates that were prepared for doing this
and it's like the slide film of the day
the development process is a lot like
ektachrome with a reversal double
exposure double developer re-exposure
bleach all that kind of stuff and they
made color pictures that are quite nice
they're kind of noisy splotchy because
you have a random selection of these
color pixels all mixed together you
don't have a nice regular bear pattern
like we have in the digital sensors
nowadays Lance's itching to ask me
something
right so Lance says you can make grains
with any sensitivity you like you're not
limited to three or four primaries
that's true however I want to answer the
question that we had in the back now how
to how does how do the spectra of these
these little color filters relate to the
color matching functions and color
spaces and so on and the stuff
autochrome the auto here refers to the
the essentially using the same filters
for taking and for reproducing color so
and theoretically basically it doesn't
work because there are no sets of filter
transfer functions filter you know
spectral transmission curves that you
can use that will give you accurate
color reproduction this way because if
you if you pick three sets of of
transmission functions for your filters
say go back and pick these three not
that you would pick the XYZ curves but
you could the the spectral transmission
of a little piece of filter is something
that's going to be non-negative when you
when you measure light with non-negative
functions like that if it's if the
curves you choose are going to give you
accurate color that is that they are a
linear combination of the three
responses of the human cones then you
can map colors this way and you can put
onto this XY picture you can put the
triangle defined by the three functions
you chose the triangle defined by these
three functions is the one we we talked
about here that goes through 0 0 1 0 and
0 1 but if you if you pick any other set
of functions that are non-negative and
our and our truly color matching
functions they're going to give you a
triangle that that correctly maps all
colors inside that triangle which means
they're going to give you a huge color
triangle to reproduce those colors you
have to mix corresponding amounts of
light that correspond to the vertices of
those triangles but the vertices of
these triangles don't correspond to any
actual light source they're outside the
region of possible colors
so this is always true that if you
measure three channels with non-negative
transmission curves and those are color
matching functions then you get a color
triangle whose primaries are imaginary
colors they're not real colors so you
cannot there's no spectrum of light that
you can use see if this is the right way
to say it yeah there's there's no
spectrum of light that you can use such
that you drive the amounts by those
three measurements and get back the
right color so if I've said that then
you're wondering well how the heck does
color photography work then and
basically then the way it works is you
you either have to tolerate a lot of non
ideality in your color which is what
they did with autochrome it's kind of
pale and it's Reds go orange and all
kinds of weird things happen or you have
to put a processing step in between
which is what everyone does in digital
cameras called matrixing and you even in
film you have some form of matrix sing
with dye couplers and things that's
fancy chemistry to try to get the colors
to come out right we'll talk about this
matrix in a minute or or in a week or a
year or something so here's another
method that louis de costa her own
talked about then was probably best
implemented by sergei mikhailovich
protein Gorski who was the photographer
to the Tsar in Russia these are some
pictures he made of the austro-hungarian
prisoners of World War one lined up
outside their camp he has these glass
negatives with 33 images on each glass
plate and these are now in the
collection of the u.s. Library of
Congress and they've they've gone to a
lot of trouble to take these negatives
and reproduce good color images from
them doing the right matrix in to try to
get the colors right and so on and he
has a lot of pictures of people standing
still because to do this you have to
move the plate and move a filter and
expose it three times so some of the
pictures you can see where somebody
moved and made great color artifacts
most of the pictures he's got people
just standing still of course the the
thing that worked really well in color
raphy was the introduction of these
three layer color films where you could
measure red green and blue in
registration all at one time so with the
you had the the beamsplitter one-shot
cameras and you had the three plate
three shot cameras he had the autochrome
mosaic type cameras but color film which
measured all three colors in all
locations all the time with one shot was
the real solution and it was really
invented by these guys these two
leopold's Madison Gadowsky who were
musicians and get this crazy idea in
their head and convinced George Eastman
and Kenneth meas to support them in
codec research to develop this film so
it's a it's a great a great advance and
I'm not going to try to explain in any
detail how how kodachrome works and it's
really complicated and a real problem to
process the the film technology that
really took off is a little bit
different called ektachrome and agfa
chrome and so on that really dominated
that field when you when you run forward
nearly a hundred years and look at what
happened in electronic photography it
goes through all the same steps so here
for example is a three shot camera that
we landed on the moon in 1966 and it's
got its it's a little color test chart
in the picture so it can calibrate its
images and makes it's got a little color
wheel inside the camera here and it
rotates and takes three pictures and
sends them back on slow scan video so
there's a there's a digital camera
actually it was analog but an electronic
color camera in the 60s I always tell
people I think it seems likely that we
did three shot electronic photography on
the earth before we did it on the moon
but I can't find any evidence for that
so if you see any let me know we did the
beamsplitter approach this was used in
television cameras throughout the 60s
and 70s this is a assembly out of a TV
camera in which they're in each of these
cylinders is a little glass tube a
little plum akan or highly evolved
vidicon tube that picks up images
there's a prism assembly in here that
splits light off for the blue one the
green one and a red one by the way when
you when you get into the digital field
and they talk about different format
sizes
have things like half-inch format
cameras that derives from this
technology if you pull the tube out of
here and you measure the outside
diameter the glass envelope it's a
half-inch that defines the half-inch
format it's it's the size of the
rectangular image that you can pick up
on the front of a glass tube with an
outside envelope diameter of a half-inch
so keep that in mind it's important we
did a beamsplitter prism camera at pho
beyond that used a slightly different
prism geometry and we use CMOS image
sensors on the three faces looks like
we're gonna have to finish up in a
minute here and that was a few years ago
and I think we're just going to have to
well here this is this is sort of the
end of this line of stuff and I'll pick
up after that next time or time after
basically what's in almost all digital
cameras these days is a mosaic sensor
sort of like the autochrome thing except
you get a chance to do some processing
in between to do some matrix in and get
the colors right and what's in a few
cameras like mine is this three layer
three photodiodes at every location so
you can measure red green and blue
everywhere and takes more more
aggressive matrix thing to make that
work out so let's cut it there we'll see
you next time and we'll have to work out
whether to do the new topic or continue
this topic but i'll let you know thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>