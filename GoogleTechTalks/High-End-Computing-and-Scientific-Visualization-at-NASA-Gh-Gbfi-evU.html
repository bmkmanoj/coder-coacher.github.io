<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>High End Computing and Scientific Visualization at NASA | Coder Coacher - Coaching Coders</title><meta content="High End Computing and Scientific Visualization at NASA - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>High End Computing and Scientific Visualization at NASA</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Gh-Gbfi-evU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I
welcome everyone this is the kickoff
meeting of the Joint Google NASA seminar
series so we're going to have one of
these things every two weeks alternating
between here and ames campus and we're
really looking forward to the
collaboration that will come out of that
today we're going to have a tag team
presentation and roop outfits was going
to lead it off well I'm glad to be here
there's a lot of effort on both sides of
the of the ditch or whatever you call to
get this thing going so again I don't
know what what all of you are interested
in specifically so I'll keep it at a
high level and then as we go to the talk
and then maybe later during discussions
we can focus on certain specific areas
I'm going to talk about high-end
computing I'm computing with the
Columbia machine that we have at Ames
even though the title says at NASA I'm
really going to talk about the Columbia
stuff and and then feel free to
interrupt me or if you want to get into
certain other areas we can talk I'll go
for about half an hour or so I give you
an overview of what high-end computing
is the effort at NASA and then I'll and
then I'll attracted turn it over to my
to Chris henzy who is going to talk more
about the visualization you will see
some of those visualization in the talk
but he'll talk about more about the
visualization techniques so i'll start
off by a very basic slide because many
of you and if this goes to the next
slide of course which basically tells
you a little bit about how NASA is
organized many of you may not know this
or if you do then just sort of ignore
this slide for a minute so basically the
NASA is a large federal agency it spread
across the continent and it basically
has about 13 centers three of these are
research centers so these are basic
research centers like Ames Langley and
Glenn some of these have very specific
missions like Johnson and Kennedy there
are other facilities that focus on
certain areas like Goddard is more info
sciences so this sort of gives you an
idea that there is a challenge in across
the agency to trying to get all the
high-end computing resources or all of
the expertise or all of the vast
resources and infrastructure that nASA
has that is spread across the country
and at the bottom there are these sort
of thumbnail pictures that show you the
various kinds of things that NASA is
interested in come ranging all the way
from aeronautics to earth sciences space
science with an exploration I'll talk a
little bit more about that but this is
how the various centers within the
agency are organized but the agencies
also organized as mission directorates
and the next few slides I'll give you an
overview of what these four mission
directorates are and each of these
centers sort of contribute to each of
these mission records certain centers
might be doing more in certain areas and
less in others but generally speaking
the focus of the various centers across
the across the agency a sort of tied to
all of these mission directorates and
I'm not going to read all of this and
you can read for yourself but basically
the the four main areas one of them is
aeronautics that is the core area that
NASA's always been involved in basically
trying to identify the next generation
of aerospace vehicles aerospace systems
for both space transportation civil
transport as well as space
transportation exploration is the big
thing within NASA in fact this is
exploration as the Mission Directorate
and then exploration as the agency
mission and the focus of this mission
directorate is basically to sort of
develop technologies for both
long-duration spaceflight both human and
robotic and as part of that also to
understand what the effect of
long-duration flights are on human
beings so if you're going to go on a
Mars mission Mars mission you're talking
about a three year round trip so how
would such long-duration spaceflights
affect human physiology science Mission
Directorate is something that doesn't
come out automatically out of the things
that NASA does but and it's broken up
into two major parts one is Earth
Sciences and other space sciences earth
sciences NASA is into that and you would
think that other agencies would be doing
that but because NASA is a space
organization it gives NASA a special
perspective on earth so
fact that we fly outside other lets us
do certain things about the earth and
science about the earth that other
agencies that are more sort of with
their feet to the ground cannot do and
then of course there is space science
and then finally is the Space Operations
mission directorate this is the mission
directorate that's mainly focused on
human spaceflight and right now the
three major areas are international
space station the space shuttle program
and flight support a lot of the work
that is going on in this mission will
sort of get evolved into more of this
exploration vision and might get merged
with with the exploration Sciences
mission exploration systems Mission
Directorate and then finally there is
this across the agency sort of a center
called the NASA engineering and Safety
Center that was established in response
to the Columbia accident to make sure
that all procedures in terms of safety
as well as processes as well as well as
how scientists and engineers work within
the agency interact with the projects
that come do the reporting and all that
sort of thing is is sort of you sort of
try to break that dependency with that
safety issues with the rest of the
agency so this gives you a little
background so that in case you don't
know exactly how NASA it organized gives
you a little give detail about that so
let me now talk about high-end computing
and specifically about Columbia there
are various things as sort of the stars
were aligned in some sense that this
happened they're both external and
internal factors so some of the external
factors is you may know was the Japanese
erk simulator this is what this is the
fastest computer in the world for two
and a half years running and it was the
first time that any one computer
actually held that number one ranked for
such a long time and the fact that that
the Japanese had actually talked about
it and they actually were able to pull
it off it was not a surprise but it's
not a secret it took the u.s. somewhat
by surprise and in response to that the
US government among the various agencies
through the National Coordination office
put together this high-end computing
revitalization task force the main idea
was to understand how we could pet put
the u.s. back and the leadership
position in high-end computing there
were other things that happened as well
there was a panel that was put together
and I was part of both the heck RTF and
this expert panel that when
japan and try to understand how the
Japanese government was able to do this
and how it worked with the Japanese
industry NEC in particular because
that's the machine of their simulator is
and how they pull this off and then
finally as far as Colombia itself is
concerned there was of course interest
from from the vendor side as well
internal SGI particularly were very
interested in in playing a major role in
putting together a system that would
sort of put us in a leadership position
there are internal factors within NASA
these are some of these are most of
these are very NASA specific so for
example the Columbia accident happened
at about the same time and then it was
clear that iron computing would play a
major role in trying to understand why
the accident happened and how we could
prevent such accidents in the future
there were critical requirements for the
next generation of modeling and
simulation for spaceflight and that we
had a long history of working together
with SGI with the single system image
shared memory machines and we had sort
of established the success of such
architectures as a viable platform for
doing high productivity high performance
supercomputing and the fact that before
we had Columbia all of NASA had a total
capacity of about six teraflops for the
system that we had at that time with
about maybe three teraflops I serve and
that machine would be completely
overbooked just to do one mission and
NASA being a multi-mission agency that
was not acceptable so I'll go through
this slide real quick but you have to
understand also that NASA is not in the
business of building machines so we we
are sort of a mission agency we're
trying to get to Mars get moon get gone
and all those kinds of things try to
understand where we are going where we
are from and so we there was a it was
imperative that there was some support
and some partnerships and I'll go
through this real quick to give you an
idea so there's obviously partnership at
the national level and hear you talk
about the heck are ya and that how the
various agencies worked among themselves
to sort of come up with a resource that
was of national value and as you may all
sort of guess that all architectures are
not good for all sorts of applications
and there are at least four different
architectures that are available at this
time and it was not possible for any one
agency to sort of
large machines of a single architecture
and be able to be experts in that
architecture and worked out fine for
example we at NASA were more intrude
into these shared memory systems with
all takes Livermore when the Blue Gene
route so they have a big machine which
is right now the number one machine in
the world in terms of sustained
performance on a benchmark oakridge was
more interested in power vector systems
they were in the crowd and then Sandhya
was more interested in clusters and they
have the AMD opteron cluster but with
cooperation among the agencies all of
the major bases are covered all of the
major architectures are covered in they
could be sort of cooperation and trading
cycles to sort of make sure that that
this right architectures were used for
the right applications but then
independent of that you have to be able
to or we have to be able to satisfy an
ass or requirements in NASA as I as you
can now understand because of the
various mission directorates have
various very different requirements
signs for example is very interested in
large computational power they can they
can really scale up their problems when
you brought any size they want if you're
doing earth sciences you can make it a
tenth of a degree a twentieth of a
degree a thousandth of a degree and you
can keep going forever however there are
also engineering requirements and
engineering requirements those are not
quite the requirements because there are
limits on how fine you can make an
engineering instrument or a thing for
example however they are more interested
in multiple scenarios they're more
interested in parameter studies what if
scenarios so they might be running
hundreds of thousands of the same thing
but with different input conditions to
see what what what what the what the
what the optimizations are and where the
payoffs are so that was also important
so we we must we must have an
architecture that sort of satisfied
these these diverse requirements and
then finally like I said it was a it was
really a sterling example of cooperation
between government and industry so in
dell and SGI obviously were the prime
partners Intel provided all these chips
and I'll talk a little bit about that
SGI it was the integrator they put
together these all things boxes the
single system image 512 nodes and then
mellanox and voltaire sort of stepped in
and did more of the most of the inter
connection and the infiniband and the
fabric between these notes and then
finally when you talk about high-end
computing a lot of people have this sort
of impression that we are talking about
big machine settings
we're crunching out numbers doing
floating-point multiplications but it's
not that it is an integrated environment
anytime we talk about high-end computing
we are not talking about the system
itself is also the storage is the high
speed networks that bill with it but as
well as the chords and the applications
that run on that machine and a complete
sort of cast of support activities like
post-processing visualization data
analysis or optimization and all that
sort of thing and so that is when we
talk about Columbia this is the
environment you're talking about and
that is kind of the environment that we
have it at NASA Ames to sort of go on
that a little furred that this shows you
an integrated support for this
high-performance modeling and simulation
so we as part of a supercomputing Center
do not make the decisions what herbs and
applications need to be run there are
NASA scientists and engineers sitting
across the agency at various places
making their call as to what is
important for the agency they sort of
work together with our computer
scientists experts on programming models
or data structures as well as experts on
the system side people who know the
baltics architecture titanium platform
the interconnects and make sure that
those applications run efficiently on
this machine and like I said it's not
just the machine itself is the storage
and the network and then finally all the
results that come out these are reams
and reams of floating point numbers it
transferred over to a data analysis and
visualization and Chris henzy will talk
a little bit about that about the
techniques that we that we use and then
turn that over turn those results over
the visualization the analysis the
feature detection and all that sort of
thing back to the scientists and
engineers so that they can either
understand what the next step they need
to do what are the what are the problems
they have in the model or in the physics
or the chemistry and then sort of move
forward so this this is how it all
started this is the building block of
the columbia system this is the SGI all
things that we first had is called
kalpana in in in memory of astronaut
kalpana chawla who actually worked at
NASA Ames in our division and so this is
the first SGI all things is a 512
processor Intel itanium to chip and at
1.5 gigahertz clock this sort of gives
you the specifics this is how it started
each of these this is one node so this
is a single system image shared memory
system so basically all of the 512
processors can see all of the memory
one terabyte of memory and it had 20
terabytes of rate and and then we end it
rounds the Linux operating system and so
we took one of these and then we sort of
replicated it 20 times so that is the
picture of columbia system and some of
you i understand are going to be at ames
tomorrow so i'm sure you're coming over
to the NASA building and you'll see
Columbia so this is 20 of those systems
just to give you an idea it has a big
performance of 62 teraflops so if you
can't do it in your head then you can
probably search on google or you can see
what is red in the yellow box so it
tells you sort of if you can multiply
two floating point numbers every second
you take two million years what Columbia
can do in one second the goal of course
was not to build that system like this
you have to be like I keep saying that
you have to be showing the impact on the
missions provide super computing and
storage for the for the for the agency
the whole thing sort of was done in four
months that was a record in some sense
and it took pains from all sides all
parts of the government as well as the
industry partners as well as OMB and
Congress and everything so that we were
able to actually have this system up and
running in four months but it's even
more impressive than that because each
of these five twelves a single system
image stand alone production machines
themselves and so each of these machines
would come in to be on a monday or
something and by tuesday or by thursday
or friday they would be up and running
actual nasa code truth be told most of
the applications still run on each of
these fibrils by themselves there are
few applications have better run across
multiple boxes there are at least two
cores that have run on 4096 processors
and of course we ran the Linpack
benchmark which is the benchmark that is
used to rank all the supercomputers in
the world and add an linpack ran at 52
care of locks which put columbia the
number two position in november two
thousand four we still believe that it
is it is the fastest operational
supercomputer in the sense that is
actually doing work right now it's
ranked fourth in the world is the two
blue jean machines ahead of of columbia
and number three position is an ASCII
purple machine at livermore right now at
any instant of time we have about 150
so simultaneous logins there are 900
accounts 160 projects running running
running on the machine and that sort of
covers all kinds of applications the
next few slides short sort of show you a
little overview of what the architecture
looks like and Bob co t is here also and
you can talk to you a little bit more in
detail so these are the 512 modes
they're not all the same as you can see
they're basically three kinds they're
the 3700 is that at the top there 1.5
gigahertz six megabyte cash nodes and
then there are certain 512 that are 1.6
gigahertz 9 megabyte cash and then there
are double density so that there's more
closely tightly packed together they are
all connected through several Giggy
interconnect which I use more of the
system side then there's the infiniband
fabric and then the 10 gige tcp/ip
interconnect to connect all these up
then there is also a front end which is
128 processor all Thicke's that is
called the channel here and that is
connected directly to the graphics array
and I'm not going to spend too much time
on this but when Chris talks about this
you will tell you a little bit more
about how we can do real-time
visualization sort of getting results of
Columbia and visualizing them and that
is some of the things that we did for
hurricane prediction last year and we
will show you more about that and then
finally there is the the storage we have
440 terabytes of storage these are eight
disks and they are connected through
these 2 128 or brocade fibre channel and
dates and at this time the problem that
we have at Columbia is that the
computational power is not really
balanced with this rate is we need much
more storage to be able to sort of store
and process all the data that's being
generated as you can imagine like I've
said since NASA is a distributed agency
you can't have a Faton or sitting
somewhere you must have fat pipes
flowing into the node and so there is
this effort also to try to get
high-speed access high bandwidth
connectivity to Columbia from across
across the various centers we are in
that process of doing that there's
various issues with that in terms of
what the backbone could be there are
several things that we are using at this
time the National lambda rail we see the
consortium of various industry as
academic partners as well as other other
agencies and then that currently we have
high bandwidth connection between our
can
Ames and Godard there are other
connections to JPL but it also depends
upon what the the internal local loops
are within each of these centers but
again the primary focus for doing this
is that you have end-to-end distributed
data access to and from Colombia across
the agency and at the right it gives you
some numbers so that you can see that it
makes a huge difference if you want to
transfer one terabyte of data at five
gigabits per second it only takes about
30 minutes but you could take 500 hours
at five megabits per second so so it's
important that that access to this
machine is sort of high across across
the across the various centers so the
next maybe five minutes or so i'll just
give you an overview we had debated
about giving you a certain flavor of
kind of applications that are going on
to really do justice to this you would
really need many of these applications
scientists to come out here and talk to
you in depth about what those
applications are what I'm going to do is
part of an overview and weaken as all on
seminars on this series we can think
about what we can do but what I wanted
to give you here is give you an overview
of kind of applications that are running
here and Chris will also talk a little
bit more about some of these
applications mainly from the
visualization side but also give you an
idea so so the machine that you can
understand is 20 of these five twelves
it's a capability machine as the
definite definition of a term capability
is that you're actually running large
process account jobs across the machine
but in a sense each of these five
twelves are also capability machine
because each of them are three teraflop
machines by themselves and the fact that
you have 20 of them and you run an
application on 16 of them simultaneously
doesn't necessarily make it into a
capacity machine but nevertheless these
are some of the applications that are
going on now for example the I'll talk
some of these recently we contributed to
this New Horizons mission that you just
heard about in the papers and the news
for Pluto mission we've been working on
some of the return to flight stuff which
is the Space Shuttle main engine
redesign and trying to understand the
flaws in that we'll talk about some of
the earth sciences work and then four of
these nodes like I said are connected
maybe I didn't say but four of the nodes
are connected to a more high bandwidth
interconnect that allows you to have a
2048 more of a leadership class system
that allows you to do more great through
problems because it is also has four
terabytes of shared memory so you can
run large process account jobs on that
so then so the next few slides I'll show
you a little sampling of the various
thing that is going up that are going on
so this is gives you an idea of the
space shuttle program the return to
flight and this was part of the work
that was done in a part of the
investigation process of the Columbia
accident normally we run various
calculations on various sort of models
on this we have lower fidelity fidelity
models that are shown on the left which
are solving the Euler equations doesn't
have the boundary layers in the
viscosity all in there you can run more
of the navier-stokes calculations that
are in the shown in the second picture
there's a sort of an animation that
shows that the tile and where it could
have hit the leading edge of the orbiter
the problem of course is that you have
to build many of these models of this of
this debris they're not regular shaped
stop their various odd shapes and no one
had actually thought about that such a
child would have fought could have
fallen off so a lot of new work had to
be done in terms of developing models
there are questions about impact
velocities and locations and we have
scientists at aims at with polar with
cooperation with God with Johnson have
developed both ballistic and
probabilistic models so those show those
are shown at the top right over there to
understand both where which does would
have fallen off where they could have
impacted and what damage they could have
caused work is also going on in for that
external flow this is for the Space
Shuttle main engine that's the picture
of the Space Shuttle main engine gives
you a little bit more close-up view and
Chris will show in his stock an
animation of the of the complex airflow
and the unsteady nature of the flow
within the inducer in the in the pump
and how that causes back flow and causes
vibration and fatigue and damage to the
Space Shuttle flow liners and again this
is required both for
returning the shuttle to fly as well as
to understand what designs or redesign
or re-engineering have to be done for
for for future shuttle engine designs as
well as the CEB nclb design this is a
switching gears this is more of
cosmology and astrophysics this is a
mission that will go up in 2015 this is
a collaboration between NASA and the
European Space Agency this is a laser
interferometric space antenna and the
idea of this mission is to try to
understand the origin of the universe
and how this all started so as you can
see there is a little picture over there
this is a 3 spacecraft in a perfect
equilateral triangle arm length about 20
25 kilometers and they will sort of
follow the earth and there's a little
animation here they'll follow the earth
at 20 degrees behind the earth and try
to measure these very small vibrations
in the gravitational waves that
performed during when black massive
black holes merged this is an animation
of massive black hole binaries merging
so they go through this in spiral stage
and then they go through this merger
space and that is where you need to
solve these full-on Stein equations in
3d and as well as in time requires a lot
of computational effort and the idea of
of the simulations that are going on is
to try to understand when the data comes
back you would be able to model that and
be able to sort of scientists will be
able to understand where how universal
performed and galaxies happen
and again like I said I'm trying to show
you a sort of a sampling of the various
things that are going on this is again
from last year this is a hurricane
prediction this is from 2004 and will CH
in the next part of the prop will show
you results from 2005 but this is
hurricane ivan which was in I believe
September of 2004 the lower right
picture shows you 33 tracks so the the
blue track is what was originally
predicted by the National Hurricane
Center and the other two prac tracks was
what was predicted by this FV GCM code
and the actual track of the hurricane as
you can see those huge discrepancies
between what actually happened and what
was predicted in 2005 because of the
work that happened in 2004 a lot of the
one of the Kurds that is used for
hurricane prediction is run on Columbia
four times a day and then that would get
sent to the University of Florida and
get money with other models it's an
ensemble model so they would take
various codes some are good at
prediction predicting the tracks on my
viet precipitation wind speeds and all
that and that manage data would then get
ordered to the National Hurricane Center
to make the predictions the picture the
top shows you wind velocities and
everything below 100 km/h I believe our
zapped out so your it's only color coded
by by altitudes and wind speed and it
shows shows you kind of kind of the
things that work that we are doing on
Columbians some of the expertise that we
have with visualization and finally I
will I think this is my last slide there
is also a community effort to really
predict climate and weather in as you
can imagine the four major components
are ocean and atmosphere of course or
the major components but then sea ice
and land and this was one work that was
done last year where we put together the
ocean model and the sea ice mortal and
as soon as you try to put the sea ice
model if you use the latitude longitude
rid then you have singularities at the
poles and so the scientists had to move
over as well as the visualization
techniques moreover to a cubic feet and
and so this was a de cattle run that was
done on on the on the all tix nodes and
scientists were able to see new features
things like vortices of the Cape of the
tip of South Africa that scientists had
never seen before as well as we develop
new visualization techniques to be able
to do these kinds of things so I I'll
sort of stop there if anyone has any
specific questions or comments I'll be
happy to take those and then as I as you
can imagine there are bunches many more
of these things we can talk about any
one of these could be and 45 minutes all
by itself if you can talk more about the
details of our designs yes say a little
bit about the programming model for the
machine so right now if you think each
of these five twelves our single system
image you can write openmp code for
instance of course naive openmp code is
not going to scale after that because of
all the thread management overhead but
if you get it in a domain decomposition
way and we have examples of actual
production code production code means
codes that are more than 100,000 lines
of code written in OpenMP running on a
512 typically people run it in a hybrid
mode so you'd have an mmpi plus an open
em give mode you also have developed a
model called MLP which instead of using
them VI calls sort of leverages the
shared memory architecture and uses
unique pork processes to do the coarse
grain parallelism and then coupled with
the fine-grained open empty if you want
to run across mean boxes of course you
have to run it in hybrid mode or at
least pure MPI so and then people have
also done a nested OpenMP for example so
there you could also do like three
levels of paralyzation MPI and then
nesco community what's the point to
point latency for mpi other machine fog
what is the point-to-point latency
any other questions comments flames
ok so I'll turn this over to Chris and
Chris will give you a little overview of
the visualization work and data analysis
and I don't know how you want
all right well good start for the
visualization until we're not Mac
people alright so I'm representing the
visualization group at the
supercomputing division at Ames there's
five people in the group all of whom are
here so if there's any questions about
any of this and get the appropriate
person to answer that question so what I
want to do is just give a brief overview
about the kinds of things that we do and
some of the strategies that we employ to
do those things like I don't have enough
time to go to go into detail okay more
into the microphone so I don't have
enough time to really go into detail
about anything so I just want to give a
broad overview and then if there's
interest in any of this we can go into
it either in the question period or at a
later date so basically what we do is
scientific visualization support
research and development for projects
running on NASA supercomputers primarily
that's physics simulations of all kinds
and roopak showed some examples of that
in his talk the characteristics of the
data that we see their large large today
means on the order of terabytes we're
seeing datasets that are 10 terabytes
and a bigger is coming soon the domains
that we see typically have a spatial
interpretation they're trying to model
some chunk of the world either very big
or very small or somewhere in between
and I point that out because our common
distinction is made between scientific
visualization and information
visualization and commonly that
distinction is made on the basis of
whether there's an inherent spatial
attribute to the data or not most of our
data is spatial commonly but not always
the data represents some time varying
phenomenon
and so that means that we rely on
animations quite a bit the time-varying
stuff and the data probably most of the
data come from partial differential
equation evolution equations that are
discretized onto some mesh system we do
see some data that our particle-based
for example molecular dynamics data and
some of the some other end body
simulations some of the astrophysicists
and cosmologists use n-body simulations
so those a lagrangian they're not
they're not on a mesh and some of the
data we see comes from basis function
models in some abstract space for
example quantum chemistry calculations
use some basis plane waves or gaussians
in a configuration space our emphasis is
on interactive graphics and exploratory
analysis and more and more it's
necessary to visualize the data as
they're being computed and we refer to
this as concurrent visualization and
that allows us to capture every time
step of a lengthy integration without
inordinate amounts of i/o and finally
I'll say a little bit about our work in
tile a systems okay so generally we're
application driven pretty obvious and
the model our approach is basically we
work in close collaboration with the
scientists that are the domain
scientists that are resorting to numerix
we work in an iterative fashion but over
the course of many of these iterations
we've tried to extract several generic
components that we can reuse on the next
go around and so the bulk of his talk
will discuss three of these our data
model the field model our visualization
technique library biztech and our
distributed framework growler and like I
just mentioned I'll say a little bit
about one of our display systems the
hyper
just to provide a little context here's
a very high level view of the kinds of
things that scientific visualization
involves in the center I have a data
source this can be running calculation
some files living in the file system or
maybe an instrument and there's pretty
much three things that we need to do one
we have to at the bottom we pretty much
have to get a handle on what what the
data are how to interpret the bites at
the lowest level once we have a kind of
have the data in hand we can apply
various transformation techniques this
is where the graphics comes in where we
translate floating point data into
intermediate geometry and ultimately
pixels and then finally we have
typically the data source the display
systems analysis module and so forth
aren't co-located so we need to harness
these things together that's the upper
right box kind of controlling all these
things providing interactive
environments and so forth in magenta
here I have kind of these three things
if you're familiar with semiotics study
of signs signs in the sense of
signifying things that's typically
divided into three disciplines syntax
the formal aspects of signs semantics
the meeting or what they represent and
pragmatics or use and it's a fairly
abstract way of viewing visualization
but I think it's a pretty close match
and in blue you can see the 3fm biztech
and growler the three kind of reusable
components and then also the hyper wall
and where they live in this abstract
view so just to motivate things here's
an example here's a typical data set we
might see this is part of the
fuel-injection system of the shuttle so
on the left this big pipe coming down
this is the pipe that delivers liquid
height liquid hydrogen from the external
tank is the giant tank that the shuttle
is bolted onto when it lifts off it's
about 16 inches across just to give you
some scale and that brings the liquid
hydrogen down at try for kate's and each
one of those three subtypes feeds one of
the engines and at the end of each one
and we have one of them surrounded
in a box there's a relatively low speed
impeller there's a close up on the right
and that moves the fuel into a high
speed turbine that actually blows it
into the engine ok so this data set
consists of 264 zones or sub meshes so
the whole space including this complex
geometry is discretized into these kind
of lattices in this case they're
structured meshes and these meshes are
kind of warped and wrapped around the
geometry and they overlap and basically
fill all space but they can't be warped
too much or the numerix break down so we
get this pile of meshes with a bunch of
flow primitives assigned to each node
and we want to do something like this so
here's a typical animation that will do
the one major point of this study was to
try to understand the causes of cracks
that are seen in these two rings of
holes these are actually ports that are
put into this flow liner to retrieve
welding debris after the joint is formed
so they're not are they initially
weren't designed with flow in mind but
in use the liquid hydrogen actually
flows in both directions it turns out
through these holes the color coding you
see what we're doing here is seeding
particles into the flow and advective
them these are massless particles so
they're showing the direction of the
flow the flow is and now it's away from
us it's toward that impeller the blue
color signifies flow in the correct
direction it's going downstream and red
signifies back flow and when it turns so
in general you can see that the flow the
impeller kind of pulls the flow forward
and in and you can see around the
outside we get reverse flow and in in
general terms what's happening is that
the
hello has a lot of momentum and it's
actually kind of bouncing back off the
outsides of the blades you can see these
reverse flow in these pods around the
outside and what that gives rise to is a
kind of back-and-forth motion and a
resultant pressure fluctuation around
these holes and that was fatiguing the
metal in some cases that was even
leading to cracks okay so we want to
create this visualization we're given
this giant pile of mesh data so how do
we do it okay so the first thing I want
to talk about is our data model and this
pretty much summarizes what it does the
data model is called the field model or
FM and it basically gives us a uniform
abstract interface to underlying
discrete data it allows us to treat the
entire computational domain as a
continuous field and we can ask for
values either primitive values or values
that are derived from the primitive
quantities at arbitrary locations and
the library does all the dirty work of
figuring out what sub mesh the physical
point we're asking for corresponds to
what cell in that sub mash contains the
point pulls all the values at the
vertices of so does the appropriate
interpolation and perhaps derivation
from the primitive quantities to the
derived quantities that we want and
along the way we can run the values
through various filters for example
differential operators we want might
want the gradient of some scalar
quantity or the curl of the vector
quantity and so forth and the really
nice thing about this is that it kind of
is able to deal to provide this same API
for a number of underlying mesh types
and we see a lot of different kind of
mesh types in the example I showed it's
over set curvilinear meshes it's a whole
bunch of curvilinear meshes filling
space but we might see tetrahedral
meshes which are unstructured and
various other kinds of discretization
schemes
when I briefly mentioned a few
optimizations we put into this interface
one is to exploit symmetries frequently
and in this case this running example
the entire collection of meshes will be
transformed versions of a smaller set
what we call reference meshes so we will
actually only load into memory the set
of reference meshes and then generate
the entire set of meshes through
applying the various transformations
this greatly lessens the memory load and
the i/o and this benefit is especially
prominent in time series so we can load
the set of reference meshes for the
first time series for the first time
step and then use that for all
subsequent time steps and get even a
bigger a bigger win a second
optimization is paging so many
visualization techniques will only touch
a small amount of the data so in this
example we're doing particle tracing
we're trying to show the path of one
massless particle released from one of
these ports and you'll notice it only
passes through a very very small
fraction volumetric fraction of the data
we don't need to pull data from the
other side of this ring in order to add
Veck this particle so what we do is
bring in the data we need it in page
size chunks and then maintain a working
working set of those cached pages and
this also works remotely it works over
the network which allows us to just pull
the data over the network that we need
for a particular visualization technique
I mentioned earlier that we also support
derived fields so typical computational
fluid dynamics will calculate a momentum
variable in to thermodynamic state
variables there's lots of other
variables that scientists are interested
in for example pressure or temperature
these are all derived quantities so you
have to pull the appropriate primitive
variables and then run them through a
transformation formula and we'll only do
that where needed we won't frequently
many data stream or data flow
Oh utilities will actually calculate
these drive quantities across the whole
domain just in case you need them
somewhere we'll only do that where
needed this is an example of a derived
field pressure it's actually a function
of e is energy Emma's momentum and D is
density in the expression and so will we
can define this quantity in this case
this is a Python front end that allows
us to type in these formulas on the fly
this actually doesn't evaluate anything
anywhere it just creates this virtual a
little derived field machine that gets
invoked when and where needed depending
on what the visualization technique
requests okay so that's our field model
once we have the data in hand using the
field model we want to apply a number of
different visualization techniques this
is just a laundry list of some of the
visualization techniques that we've
encapsulated into a library interface
and there's basically four big groups
here sometimes we want to visualize the
mesh itself the geometry of the mesh and
then most commonly we get scalar and
vector fields defined on those meshes we
see some data that are tensor data and I
don't have those listed here and then
there's kind of more abstract techniques
that I have under this heading of
feature detection we might want to find
features like vortex cores especially of
interest to the fluid dynamicists
separation and attachment lines and
various topological features of the
field so I'm basically just going to
show a bunch of examples of this of the
vids techniques here's a standard scalar
field technique this is showing pressure
on the surface of the shuttle in the
ascent configuration just mapped onto a
simple rainbow color map this is from a
simulation of the dynamics of the solar
atmosphere this is showing current
sheets in the magnetic field as a scalar
technique and then also showing some of
the magnetic field lines simple vector
field technique
here's an animation showing large-scale
structure evolution in the early
universe the ticking number in the lower
left is billions of years before present
the simulation starts at about 13 and a
half billion years that's a redshift of
about 1,200 and what you're seeing the
domain here is about 500 million light
years across and it's it starts with
almost a uniform distribution of matter
in this case nonbaryonic dark matter
there's slight fluctuations from
uniformity they're given by the Cosmic
Microwave Background statistics and then
the only force that nonbaryonic dark
matter feels is gravity so all the
structure you see developing here is
purely a result of attractive forces and
you see the structure resolved into
these sheets and filaments and kind of
dense clots this is very close to what
we see from the deep sky surveys today
and just for some scale one of the kind
of medium-sized yellow blobs there is a
cluster of galaxies that might contain
somewhere between a hundred or thousand
galaxies like the Milky Way so this is a
n-body a particle-based simulation
technique and actually a particle based
rendering technique by the way okay this
is showing surface winds uh on the earth
obviously so this is from a five day
forecast hurricanes this is extra from
2004 this is hurricane frances crossing
florida just now you can also see at the
same time this is looping there's
significant typhoon this is type of and
tsonga is happening at the same time and
this is and you can see various fronts
kind of at the low and high latitudes
this is using a technique called lick or
line integral convolution which
basically combs a noise filled with the
flow field so the high frequency
modulation you're seeing here the kind
of moving streaks are showing the
direction of the flow the low frequency
modulation the kind of bright white
areas and the not so white areas that's
proportional to velocity magnitude so
you can see that the big cyclonic
motions
we're quite a bit faster and there's
kind of some doldrums down the on the
equator and so forth you can also see by
the way if you look across the land
across South America in particular you
can see this kind of right-to-left wave
that's the Terminator that's morning and
I'm coming across and you can get after
you see some afternoon convective
activity the thunderstorms over the
Amazon basin and so forth ok many
visualization techniques we like to
think of as queries and they're commonly
location queries want to know what's
going on at a particular location in a
computational domain there's another
kind of query which are conditioned
queries and what you want to know is
where in the domain certain conditions
obtained and so many of our feature
detection algorithms can be thought of
profitably as condition queries we've
developed many of them for CFD for
computational fluid dynamics for example
those that are designed to find flow
reversals wakes shocks various
topological features of vector fields
like critical points critical lines
critical surfaces vortex cores and so
forth but we found that these techniques
are generally generally and usefully
applicable to many different kinds of
vector and scalar fields for example
we've applied our vortex core finder to
neural map data and found some very
interesting features I'll show one
example feature detection this is
showing the results of our vortex core
finder the vortex cores are actually the
white lines a little bit of noise this
is showing flow over a Harrier jet I'm
the main vortex is actually down by the
tail and and they're actually seeing
buffeting by this vortex against the
tail which is why they're doing this
partly why they're doing this
investigation locating that vortex core
guides us in the placement of seat
seating the particles for the particle
tracing method here and in particular
that yellow trajectory that you see
coming from upstream you can see it it
almost goes under the tail get
recirculated back over the tail almost
escapes out the back it's gets pulled
into the vortex gets spirals in and then
the red trajectory is actually leaving
the vortex core you see it's a very very
complex float okay so that's
visualization techniques so the third
component I want to describe as our
distributed component framework the
motivation is that our data sources are
compute engines and our display devices
and environments are typically not
co-located so we need to functionally
rig them together is very rich
functional interfaces so in order to do
this we need a means of expressing rich
communication semantics we have to deal
with the fact that the different
components that we're wiring together
have very different characteristic time
scales and what I mean by that can be
illustrated by an example in the lower
right there's a picture a kind of mocked
up picture of an environment we've
created for computational steering and
interaction with the molecular dynamics
simulation so this allows you to kind of
push atoms around while the molecular
dynamics are running get an idea of how
they behave and we have that rigged up
with head track stereo glasses and a
haptic of force feedback device and so
forth the molecular dynamics simulation
behind this might be updating the atomic
positions and velocities at a small
number of times a second one to one to
10 Hertz say the stereo glasses need to
be refreshed at 120 Hertz 60 for each
eye the hand tracking which allows you
to grab an atom or a group of atoms and
move them around you need to give kind
of reasonably snappy feedback to the
user or you lose the interactive or the
immersive environment that needs to be
updated at least 10 Hertz and the haptic
device the force feedback device has to
be updated at a kilohertz turns out
that's about flicker fusion frequency
for touch if it's if it's updated any
slower than that you feel
is kind of a buzzing instead of a solid
force so we have 2 couple and integrate
all these different event loops which
are running at different times so we
have to interleave these things and be
clever about that and that's in part
what this our framework is designed to
support and of course all this stuff is
happening over the network so we have to
deal with failures of the network
security issues and so forth so I'm
pretty I'm just going to give this one
slide a list of features that our
framework has in my backup slides I can
kind of go into more detail about any of
these if there's interest so first of
all we have our own interface definition
language that allows us to express these
complex functional interfaces it's very
C++ centric basically allows us to work
in the same classes that we use in our
in our local applications we have an rmi
or remote method invocation service this
allows us to make method or function
calls on remote objects those remote
objects are reference counted so in
their reference count goes to zero
they're cleaned up helps immensely and
resource management many of our
distributed systems rely crucially on
events so typically these events have
large data payloads these are typically
updates and we're running simulation
that's that's a very common case and
when those events show up you need to
intercalate them into your interactive
graphics loop that's a common situation
and so we have a very sophisticated
signal selector mechanism that allows us
to assign the incoming data to a thread
for maybe some local processing maybe it
generates a graphics display list and
then at the last minute that's handed
off to the thread that's servicing the
interactive user interface and a lot of
that handoff mechanism relies on
read-write buffers which is a were
developed in the group to kind of help
the synchronization and to minimize data
copying and then finally the all of this
machinery allows us to kind of create
this distributed component framework we
need to manage that and so the namespace
we have a distributed namespace that
basically we can publish interfaces that
are available and then hook them
together on demand so this functionality
allows us to create something like this
don't expect you to be able to read this
but it gives you an idea the complexity
of the digitalization pipeline we set up
this year for the hurricane forecasting
effort and so using the system we're
actually able to interface with a
hurricane forecast running live and his
roopak mentioned these were running four
times a day in production mode every day
during the atlantic hurricane season so
from june through november was actually
extended this year into a into december
for a couple weeks and as the simulation
is running we reach in and through the
shared memory of the alt Ock's we're
able to copy out the data that we're
interested in them move that over the
infiniband to the front end of columbia
send that over ethernet down to our
rendering engines generate frames and
code those as mpegs and then stream
those wherever we want including back to
goddard where NASA Goddard the center at
which this code is being primarily
developed here's an example of one of
the five day forecasts and this actually
shows Hurricane Rita you can see it in
the Gulf it's just taking landfall now
and you can see it in this global view
this is a global model these hurricanes
is big and this destructively as they
are they're actually only a minor
feature in the whole global circulation
and they're being pushed around by these
very very large-scale dynamics gives you
an idea of the scale of the difficulty
of the prediction problem
okay using our framework we've also
rigged a live interface to Colombia
itself hoping this will I'll be able to
connect to that right now works okay it
appears to have gone just gone to sleep
so probably our SSH tunnels have timed
out we can fire that up after the talk
but you can see the static version here
we basically have three views of each of
the 512 nodes on Columbia so the kind of
upper left this tiled section that's
showing the 512 processors and it's
color-coded by their state of activity
the red lines show memory usage used in
free pages and the kind of graph the
network graph shows the interconnect the
topology of each of the 512 and shows
activity on that apologize for that
timing out
that's
okay and finally I want to say just show
a few slides about our tile display
system we call the hyper wall it's a
seven by seven array of flat panel
displays gives us about 65 million
pixels over about 55 square feet a
viewing surface so we can run this in in
several modes one of them is standard
power wall mode where we show a single
large image like this modis composite
more interesting is to use the separate
displays to show parameterize layouts of
related images so here we're showing the
design of a mars airplane and in this
case it's laid out by airspeed left to
right and attitude up and down so this
shows a lot of different flight
conditions and you can compare them
side-by-side you can interact you can
kind of roll these around just like a
standard your standard desktop 3d viewer
but now you're interacting with the
whole set that's laid out in a
systematic fashion and another mode is
to have visualization and analysis
utilities that are functionally
interconnected so in this case we have a
bunch of linked scatter plots these gray
blobs and they're actually linked to a
bunch of 3d point plots and volume
renderings so this allows you to do
things like select a range of points on
one of the displays and have that update
in all the others this allows you to
probe high dimensional dependencies and
so forth here's a little more readable
version of that in the upper right you
can see this is a data set this is a
section through an airfoil and so
there's a mesh wrapped around this
airfoil and the flow now if you're
stokes equations are being solved on
that mesh so at each point in that mesh
there's a whole bunch of other variables
like velocities pressures enterpise and
so forth so we can take that mesh and
actually embed it in these other spaces
so for example in the lower left we've
embedded it in velocity component space
the free scream or the undisturbed flow
in this data set maps to a single point
here on the x-axis
everything else is a perturbation of
this flow caused by the wing so things
regions of the flow farther away from
the origin which is here our accelerated
flow and so we can select a range of
those with this blue annulus tool and
you can see those correspond to the top
of the wing and flow which is slower
relative to the free stream comes in
green you can see that's below the wing
and see you nice to actually see in this
pressure versus temperature plot that
low speed is high pressure and high
speed is low pressure so this is kind of
visualization of live and so forth you
can see all sorts of other features to
these little kind of wing like things
this corresponds to this flow reversal
so forth so this is an example of kind
of a visual representation of condition
queries and we can composite these
arbitrarily using boolean expressions
and so forth so this is my last slide I
would the group just wrote down a few
interests and challenges that we thought
might be relevant to the folks around
here pertaining to metadata you can read
these but the gist of it is that we deal
with a lot of large binary objects we
extract a lot of data around them and we
want to kind of retain that data and be
able to attach it to these objects and
search over it we be very receptive to
any assistance on that right out there
I'm taking questions
making essentially an ontology of all
the different variables that you have
and how they can be related to one
another what the goal
unless like that Victor Valley go for
you guys
feel free to chime in under peplum the
group I'm not sure how realistic that is
like I mentioned about that we're very
demand reven so the scientists typically
have very specific questions that they
believe it internally weekly conceivably
constructing alive
oh how do you do but this is just let me
make all next second I'm sure I mean not
here without would you be the I don't
think what your partner on that well
it's it's fairly modular so we kind of
debug it piece by piece don't know what
to say beyond that it's the usual divide
and conquer typically the failures we
see are typically in the large
distributed systems or failures of
integration so one part will drop out
the whole thing will kind of either just
die or limp along</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>