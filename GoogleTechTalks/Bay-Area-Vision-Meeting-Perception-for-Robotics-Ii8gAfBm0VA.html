<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bay Area Vision Meeting: Perception for Robotics | Coder Coacher - Coaching Coders</title><meta content="Bay Area Vision Meeting: Perception for Robotics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Bay Area Vision Meeting: Perception for Robotics</b></h2><h5 class="post__date">2011-04-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ii8gAfBm0VA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have Caroline Ponte Faro she earned
her PhD degree from CMU in 2008 working
with Marshall a bear there she join
Willow Garage in 2008 as a computer
vision research scientist her current
focus is on perceiving people from
detection to tracking to activity
recognition we also have Radu rousu uh
yes ah Teresa here in his PhD from the
technique technische universität
münchen my German is horrible and you
join Willow Garage in 2009 as a research
scientist his curtin research interests
include real-time perception and 3d
semantic mapping and object recognition
for mobile manipulation platforms help
me welcome our speakers payal thanks for
having us here today
so today Rudra and I are going to be tag
teaming this talk a little bit we're
gonna start off by talking about what
Willow Garage is for those of you who
don't know and then I'm going to go over
a fairly random but hopefully
interesting set of perception projects
at Willow Garage and I'll pass it off to
Otto who's going to deep dive into the
world of perception in ND so let's start
off with what is Willow Garage
well Willow Garage is a privately funded
personal robotics research and
development company in Menlo Park we
have about 60 people working with us
right now with a wide array of
specialties as you can think of a
robotics company from researchers to
software engineers to end to engineers
of various sorts to psychologists and we
also have a ton of interns and I'm gonna
use this opportunity to a very very
early intern plug for those professors
and students in the room we have about
20 interns a summer our summer this
summer is actually full but we're taking
interns for the fall so please come see
us afterwards very early plug so what is
personal robotics is specifically indoor
personal bias that we focus on well we
define it as
robots that operate in the same
environment as people without using
cages or other restrictive devices like
industrial robots - so our main personal
robot is the PR - the personal robot -
that you see in this image it's got a
base that's about the size of a
wheelchair so can go anywhere that's a
DEA compliant that's American
Disabilities Act compliant it's about
four and a half feet tall and actually
stretches to about five and a half feet
tall when its torso lifts that changes
things a lot for makes it shorter than
me and goes to taller than me
and when it's nice and tall it can reach
tabletops and shelves and other such
things it drys around on casters so it's
very stable and it has two arms there
are actually counterbalance they feel a
lot lighter than they are which is great
when you're operating around people it's
much more appropriate for human
environments mostly what this robot is
for is research and prototyping it's
meant to be a general purpose platform
on which you can turn a hardware problem
so robotics traditionally if you want to
start a lab you had to build a robot
first and that took a really long time
the startup cost - robotics lab was huge
and what you'd really rather be if
you're a software person is someone
hands you a robot or you buy a robot and
then you can work on a software problem
well before there really weren't any
robot platforms that kind of fit this
personal robots paradigm and the pr2 is
meant to fill that void it's meant to be
a robot on which you can prototype and
do research there right now about 15
peer two's out in the world and
hopefully that number is growing but
harder is actually only a small part of
what we do most of us actually work on
software and on research what we want to
do is write higher level software
perception algorithms manipulation
algorithms all that type of stuff but in
order to do that you need a standardized
robot architecture and for us that's the
robot operating system or Ross which
right now is in its third sort of big
release that's called diamond back and
dime
turtle actually which is what that great
pictures for Ross at its core is a
message passing system if you have a
distributed system of nodes where
different nodes do different kinds of
processing for your robot like camera
drivers or perception algorithms or
controllers and it lets them pass
messages between each other so they can
talk in a distributed way so you can
have some of them running on board some
of them running off board próxima them
even running in the cloud but the most
important thing about Ross is that it
standardizes data structures and some of
the basic processing so that when we
write our higher-level software we can
exchange it work with other people and
in general build a community that can
actually work together so oh and most
importantly I can't forget this ross is
open-source it's released as bsd and
what that means is that if you build on
top of it you can use your work for
whatever you want including the ross
component set below it including if
you'd like to start your own company
nothing's preventing you from using any
of these components in the same spirit
we have a couple of perception libraries
that are also open source you've
probably heard about open CV and I'll
give you a bit of an update on that
later and there's also the newer point
cloud library which rod is going to talk
about more index and these are
perception building blocks also open
source that you can use in your code
build upon and hopefully share your
algorithms and hopefully one day soon
these will merge into an open perception
library but really robots and software
are not actually what we want to do what
we want to do is to build a community a
community of robot assists are working
on this personal robotics problem if you
think about personal robots today
they're kind of the Roomba and that's
that's about it that's because they're
it's really hard to do personal robotics
and no one has expertise to do all of it
so the hope is if we can work together
as a community all doing the open-source
bits then
get to the point where there can be a
personal robotics industry and then we
can all have companies and there'll be
plenty of pieces of the pie to share
they'll be plenty to go around at that
point so right now as you can see on
this map there's actually a ton of
places that have Ross repositories
they're helping to develop Ross right
now and we hope this number will grow
so to summarize that Willow Garage is
goals are really to create an industry
and we know that in order to do that we
need to share the engineering burden we
know need to avoid duplicating work so
that's a lot of what our open source
policy is about share the work don't
duplicate it we need testing standards
and in general we need to do better
science so it's not just do demo do demo
you can actually get numerical results
and compare algorithms and in order to
do that we need to build a community so
I've been going on about Willow Garage
for a little while let me bring this
back into the world of perception for a
bit and well for the rest of talk and
first of all talk about sort of why you
should care as a computer vision
community about robotics right now so
computer vision especially recently has
been a lot about web imagery has been a
bit of large datasets of single images
from single cameras all disparate taking
from different cameras at different
times all over the place
well you might say that a robot well it
has cameras it actually has video
cameras it has a number of them it has
two pairs of stereo cameras in what you
would consider its head plus a higher
resolution camera it also has a camera
on each forearm so I can see what it's
holding in his hand well all these
cameras produce video that's actually
really relevant right now because
there's a ton of video online now so
even for those who are interested in
mostly in the online space video is
becoming a huge huge issue at the
yeah okay so that's not much of a
stretch computer vision has always
looked at video that's fine okay well
let's look at something else something
that computer vision has kind of
considered cheating and recently let's
look at 3d information so I told you
that our robot has a couple of pairs of
stereo cameras on its head well it also
has a texture projector that I just
highlighted and what that does is
projects texture to the scene to improve
your stereo move information
this might sound familiar some of you it
also has a couple of laser rangefinders
one in its base that just as a 2d scan
and that's mainly for obstacle avoidance
and one in I guess you would call its
neck that tilts so gives you a nice 3d
perception of the scene so this kind of
3d data especially laser range data has
it sort of makes your achieving of the
computer vision community but with the
recent introduction of 3d consumer
cameras sorry stereo consumer cameras
and especially with the introduction of
the Kinect this is suddenly extremely
relevant for the computer vision
community as a whole there's going to be
more and more 3d data online out in the
world and it's time we all deal with it
finally a source of data that's really
been considered cheating in the
community community and that's
situational awareness data so I've
highlighted one of the robots caster
sort of down there below to represent
the robots Adamo tree some robots may
have GPS our robot doesn't because
indoors robots may have maps and a lot
of this data hasn't been traditionally
used in the computer vision community
but suddenly our cell phones have GPS
and there are tons of maps online so
they all the images online a lot of them
are labeled with GPS data and we tell
have situational awareness for the vast
quantity of visual data out there so
this is a great time for sort of
robotics perception and the rest of the
computer vision community to really come
together and focus on the same problems
because the sensors have really been
spread out over both fields
okay so there are a number of components
to perception and this is just a few in
sort of a a reasonable ordering although
you could definitely consider reordering
them calibration data collection feature
extraction and matching geometric and
scene context evaluation and then
integration to bigger systems the rest
of my section of talk I'm going to give
an overview of projects at Willow Garage
in these areas it's to a random
assortment but it fits into this kind of
structure to give you a taste of the
things that we do so let's start with
calibration so calibrating sensors
calibrating single cameras bad enough we
need to calibrate all the sensors on a
robot it's the worst task possible
nobody wants to do it which is why it's
it's really awful and people put it off
as long as possible but calibrating a
robot is extremely important for
instance if you want to do manipulation
and you have bad calibration at best
you're going to drop what you have in
your hand at worse you're going to
really hurt somebody so it's a very
important that you have accurate
calibration and one of the great things
about having a standard platform and a
community is that someone will write the
calibration algorithms once and then
everybody else can use them so right now
there's a standard calibration algorithm
for many of the pr2 sensors and you can
see it calibrates itself for near range
cameras which is great means we don't
have to go around and calibrate our
robots
there's also calibration data available
calibration algorithms available for the
Kinect and there are people out there
writing calibration algorithms for their
own robots so if you have other robots
you can run their calibration in Ross as
well it's a fantastic thing about having
community just write things once and
then use them so once you have
calibration you might think of trying to
get data now there are obviously a lot
of possible different data sets you can
have for
friend kinds of applications if you want
do object action you need objects if you
want to do scene recognition need scenes
I'm gonna focus on one that's kind of
interesting from a personal robotics
point of view and that's getting data of
people so the motivating the motivation
behind this project was the fact that we
wanted to detect and tracked people in
indoor environments the catch was we
want to do it from a robot so our
platform was going to move and we knew
people would interact with their
environment naturally so there was going
to be a ton of occlusion and we want to
do all of this in 3d so out of that came
the moving people moving platform data
set the static set was collected from
multiple sensors including stereo
cameras and you can see example of one
of the images in the data set on the top
right and you can see it in the colored
point cloud in the 3d scene at the
bottom as well as the data from both
lasers the data from the base lasers
that red line you've seen the scan and a
number of um pieces of data from the
tilting laser are the blue scans and
unfortunately it was collected before
the Kinect came out but you could think
about adding that kind of data to this
data stand we welcome new data whenever
possible so we went around and collected
as data at four different companies
offices and including our own how we
have about 80 people in the dataset and
there's about four hours of non
continuous data so it's a fairly sizable
chunk of data and then we went online
and had it annotated using Mechanical
Turk sure a lot of you are familiar with
Mechanical Turk it's an online
marketplace where you can set up micro
tasks to people and they'll do small
things like label your data for you so
we had them do two kinds of labels for
us the first one was an accurate polygon
around a person which is the red line
you see in the images and you can also
see a black box that's a tight bounding
box around that outline the second kind
of annotation we got them to do which I
think is really interesting was this
green
Vox which is where they predict the
whole person is including occlusions now
obviously it isn't going to be nearly as
accurate or precise as an accurate
polygon but it does show you is where
they believe the rest of the person is
whether it be the person's legs behind
their desk or the person's body behind a
door and at about a hundred out of about
a hundred and eight total images we had
about a little over a third of them
annotated which is which is great so
what are some of the issues that come up
I said there are some interesting issues
about collecting this kind of data well
what you're trying to do is you're
trying to collect natural data of people
in their environments but you are
intruding on their environment so the
first issue that you come across is
privacy you're entering workplaces and
you're taking video of people and we're
so I trying to make this into a public
data set so luckily actually people were
fairly willing to do this how about of
about well I guess there were eighty
about eighty people our data set so
let's have about eighty three people
actually in the environments could about
three people opted out a couple of
people just closed her office doors so
we wouldn't take video of them which is
fine and one person opted out after they
reviewed the data and said I want my
data taken out this data set which is no
big deal when you have a fairly sizable
data set but more importantly than well
not more importantly but in addition to
privacy you have this issue that you're
taking this four hundred fifty-pound or
robots for under fifty pounds robot
that's about four and a half feet tall
into someone's environment and trying to
get them to act naturally
the robots not settled it's actually
fairly hard to do so first of all there
are safety concerns and because of that
we had someone tella operating the robot
at all times and what that looks like
well I'm in the picture doing that with
a joystick you basically walk behind the
robot and drive it around so while it
would be nice to collect autonomous data
that's just not realistic when you're
trying to get access to other people's
offices it's not going to happen so what
happens you collect this kind of data
well
at the beginning of the day you have two
kinds of responses that you don't really
want one response is people pull out
their cell phone cameras and start
taking pictures of this robot that's in
their environment and it's okay to have
a few piece of data like that because
that's pretty much what happens at a
demo so it's not bad to have some of
that data but you don't want your whole
dataset full of people taking cell phone
pictures the other reaction is that
people start talking to the operator um
and this got a picture on the top right
it is actually using hand gestures and
speaking and doing things he wouldn't do
if the robot was there alone and this
data is meant to be used for a later
time of the robots autonomous in this
environment so that's not ideal either
what you want is this very small sweet
spot in the middle of the day when
people basically ignore you in the robot
but it's a very short time because as
you hang out a little bit longer you
start to annoy people so you have this
slightly noisy robot spying on you and
people start to close their doors and
then you're done for the day and you
have to leave so I think this is these
are going to be issues that come up more
and more as we try to collect data in
natural environments people are gonna
know we're spying on them and we're
gonna be intrusive in a way and trying
to figure out how to do that and still
collect natural data is something that
maybe we can learn from anthropologists
or other social scientists who are used
to doing this kind of thing ok so once
we have data so the next kind of natural
step in our perception pipeline is to
collect features so I'm gonna move on to
another project called the textured
object attack etre so the textured
object detection is something that's
been studied a lot in the computer
vision community and there are actually
very good detectors out there
unfortunately they're closed source or
patented and that's really really
unfortunate for those of us who want to
build on top of them so there's a
project going on at Willow Garage right
now which I hope you saw the poster for
at the poster session by Gary Brad
Stevens this our bow and Ethan rouille
to do the texture
open-source textured object detector
very much built on this premise that you
can match features vote for an object
and then confirm the objects pose by
projecting it to 3d once you have
objects detected you can think about
trying to refine your search or trying
to refine your detection by using
context or geometric information so
let's consider the face detection
problem there are very good face
detectors out there for instance the one
OpenCV is excellent but you can't get
around the fact that they still have
false positives but if you know
something about your task or your
environment you can really constrain
where you expect to find a face so for
instance if I know the size of face I
want to find of course you do face
detection at multiple scales well let's
just pick one and I know I'm looking for
people who are sitting down then I can
immediately eliminate for instance all
the blue pixels in this picture by
saying they're too high or they're too
low in the scene they're not where I
expect to find face I can also eliminate
the red pixels by saying given their
depth in the scene because I've read the
information there's no way a face of a
realistic size could be there and
interesting enough I can also eliminate
all the green pixels now the green
pixels are places where you don't
actually have depth information from
stereo but a really interesting fact is
that if I don't have depth information
it's because I do have texture and if I
don't have texture I'll never detect a
face so truth is if I don't have stereo
information I probably don't need to
look for a face there either
so that leaves only the white pixels as
actual places where you could find a
face of a particular size and that's a
very very constrained set it really
helps you get rid of false positives and
speed up detection so another way you
could look at features and geometries as
I play this features and geometry is by
looking at the visual slam problem or
the simultaneous localization and
mapping
this is work done by how K straws dad
who is in the video right now and Kirk
our knowledge so there's a traditional V
slam problem of you have a moving camera
through an environment you're trying to
match features and try to figure out
where you went and what your map looks
like so they've done a great job of
doing this it's much more efficient it's
a general framework and it does useful
things like closing loops properly
let's speed up the video a little not
stop the video you can also if you
happen to have RGB and depth information
like from the Kinect do your sparse
matching and then come up with density
models which is very useful with the
environment so while you're mapping you
can also have these density models
finally this is skip doesn't listen to
me another useful thing you can do is
build object models by spinning your
objects around the circle doing feature
matching and getting these dense
reconstructions you get fairly detailed
object models so when you've gone
through the whole process through
calibration collecting data finding
features using context you still need to
do evaluation and that's actually a big
deal in robotics because getting close
when you do recognition it's not good
enough you need to be right a hundred
percent of the time and there's very few
computer vision algorithms that are
meant to be right a hundred percent of
the time so there's a new solutions and
perception challenge being organized by
lavarse Gary Brad ski that's going to
look at actually solving problems and
having those out in the public domain so
other people can build upon them and
rely upon them the first challenge is
actually at Achra this year and it's
going to look at textured objects
detecting them
and figuring out their pose with data
from the Kinect if you're interested
there's the wiki page you can go and
take a look and there'll be future
challenges in the future there'll be
future challenges that look at other
types of data as algorithms evolved
perhaps personal tracking perhaps other
things
and finally perception is interesting
for the sake of perception but
ultimately if you want to be useful you
need to integrate it into a larger
system and this is still very much an
open research question so will garage
helping to organize a number of
workshops on integrating perception into
larger systems such as manipulation and
figure out how to do that effectively so
I'll just take a minute before I pass
this off to Raju because I would be
remiss in not talking about open CV for
a minute at a computer vision conference
the open computer vision library has a
huge number of algorithms it was started
long before Willow Garage came around
but is now mostly sponsored at will
garage but is a community development so
some of the some of the new things in
open CV include full support for Android
which is great an ever-growing GPU port
has direct connect support a full Python
interface new 2d features calibration
patterns and fast approximate nearest
neighbors and very soon you can hope to
see actually a lot of the things I've
talked about here today integrating into
OpenCV including textured object
detection and a lot of things that take
us out of the 2d realm into 3d object
pepp object capture and six degree of
freedom poses and finally hopefully
there'll be a perception app store so we
can work a lot work around a lot of the
issues that have stood in the way of
actually licensing out computer vision
algorithms for use and now I'll pass it
over to Raju
okay so we're doing a bit you know
good-cop bad-cop white background black
background thingy here I actually wanted
to pick a little bit more on the 2d
computer vision community but I'm you
know Caroline has advised me not to do
that so hi my name is Raul
thank you for being here what I'm going
to talk about today is a bit of what we
call nd perception and you know I'll
start with explaining what nd actually
means so at the very least N equals
three so we come from a geometric
background here where you know we have
scenes with point clouds right pointers
are natural representations for
describing collections of points which
are three-dimensional at least now
naturally if you extend this to you know
you add RGB or normal information and so
on basically oops your your space
becomes nd easily and here's an example
this is a point cloud recording using
the Kinect sensor garage this your pr2
there as you rotate around basically get
much more than just geometry you also
get a texture attached to them so let's
see it's actually very easy to switch
between it so it's easy to get the image
from the point cloud back so in our
opinion and Caroline I think studies
already 3d has arrived I mean I don't
know what we're waiting for more you
know Microsoft came up with this great
sensor made by originally by primeSense
now they're doing you know there are
other things probably working on connect
to as far as we know there's also 3d
pocket cameras which are which are
coming out and I want to mention this
because they have a high resolution so
if there's anyone out there actually
interested in doing like high quality
stereo reconstruction here's your chance
right everybody's going to have this
cameras in within a few months or a few
years there's a real potential for good
close range 3d from these devices
sensors like the Kinect actually are
very helpful in robotics and they enable
not only you know let's say more
expensive projects such as a pr2 but
also cheaper robotics so here you see an
example of a room by with a Kinect on
top and then this is a quadrotor here
from UC Berkeley which actually I'm
going to I'm going to show a video of it
later the nice thing is that you know
being cheap and lightweight it's very
easy for people to actually start
working on 3d problems because robot
actually needs 3d you can't do 2-liter
bottles it just doesn't work so as if
said I'm gonna talk a lil bit about a
point out library which is one of our
efforts at Willow Garage and as you will
see it's not only a real garage project
it actually started at Willow but it
very very rapidly spread it out across
across the globe actually and you have
to think about PCL as OpenCV 3d sister
for now just bear with me so as I've
said it's a we really like Google Maps
so I you know you're gonna see that
they're all over our presentations we
actually have core developers from over
15 I would say like 16 I think start
starting last week institutions so it's
not product we have four universities in
Europe contributing there's universities
in Japan and and universities in the
u.s. basically what we're trying to do
is kind of focus our attention from from
so this is kind of how we see how we see
the picture right now at least that we
know or how we saw the picture right we
have this huge field Cod robotics
there's personal robotics in it and very
interested in that and then we have
industrial robotics and other types of
robotics that we're not going to tackle
too much because we have limited
resources and in this field of personal
robotics we have things like navigation
manipulation and perception right and
perception is also split again into 2d
and 3d right and so we really want to
shift from this mentality to something
like this where we think they're equally
important and we think that there's a
lot of work being done out there in you
know not only fields like computer
graphics but also other fields and we
can bring that knowledge into robotics
right so we think that the complexity of
you know n dimensional perception if you
want and personal robotics are equally
high and there's many things that we
don't think of or unnatural so if we
could leverage the broader 3d perception
community I think we could easily
advance our specific goals in personal
robotics this is an interesting slide is
one of our projections actually for the
next few years and this is actually
growth so it's not necessarily the size
of the library or anything like that but
because of the Kinect so suddenly we
have a sense of
you know millions and millions of people
and sure from that from that you know a
number a small a small percentage will
actual connector to a PC
but when they do they're gonna need
something like this they're learning
something that takes the data and does
something useful for it and that's how
we anticipate anticipated growth of this
project and Caroline already said this
in her talk Gary and I actually joke all
the time about when we actually merge
Open City and PCL together and I think
that's going to happen soon we just have
to iterate a little bit over the api's
and see exactly what's the right way to
you know get ND structures in and not
just two dimensional structures so to be
a bit specific on on PCL for those
interested it's reason c++ template a
library completely open sources you
might have seen earlier BSD licensed
basically everything that video thank
we're using the SSE for optimizations
and openmp and TB for multi-core
paralyzation and it's actually ongoing
word I didn't want to talk too much
about it but we do have a project
starting with with in v1 actually
paralyzing it to on GPUs so we want to
tackle GPUs and clusters of GPUs to
solve these problems like faster because
in some sense we are overusing
exciting's for some problems and our
methods are embarrassingly parallel I
think he's really split into a variety
of tiny libraries so if you're only
interested in 3d feature estimation you
don't need to get that library in theory
same for surface construction there's
there's a lot of code in there
segmentation registration and so on
basically if you're interested in this
aspect I can talk to you about
afterwards one of the concepts that we
brought with PCL so coming from a
robotics perspective you realize that a
lot of roboticists
that are not perception folks and they
want to solve problems but we can't
teach everyone you know about ransack
and about feature detectors so my call
is that the motion planning they're just
one like building blocks that they could
use so we we thought about a way of
basically building modules that are self
sustainable and you know we have open
interfaces between them such as like you
know for segmentation you don't need to
worry about some things
interests omission other things and then
put them together in is like processing
graphs and have our colleagues test them
out so it turns out that actually work
really
well we came up with this nice
dynamically loadable plug-in
architecture our colleagues are using
every day and whenever you see videos
with you know grass and perception you
will think about these processing grafts
that go under and the reason why we
actually created this is that you notice
that we had a lot of projects that we're
dealing with you know planar
segmentation for one reason or another
and we saw our colleagues basically copy
pasting the same bits of code all over
again in different in different packages
and what happened in the end is that
when once we were figuring out what what
our bodies are you know we're trying to
fix the original code those changes were
not getting propagated so obviously we
had to build something that would help
the most and in terms of you know we
have a good API we think we have a good
idea actually and you know doing this
this bring is carefully with with some
great software engineers in house we
actually simplify development
interesting I think quite a lot some
statistics here we're only dependent on
three other libraries eigen for matrix
computations I've seen impact for labor
market optimizations and and flan
Caroline mentioned this earlier which is
a library from David Lowe on fast
approximate nearest neighbors and just
roughly weave their library still we
consider Reed young because it's only
it's only been out there for like one
year now but the code the beats that we
have there were we're out there for a
lot of for a long time so just a few
examples a few code examples because I
don't want to really go into the math I
think it's gonna bore a lot of people
uh-huh but just to show you how easy it
is to use it so here's an example of a
pass through filter all this does is
basically saves data and then you know
basically cuts it on a certain width a
certain field or certain dimension and
and some limits you can do down sampling
which is also very important in 3d
because a lot of times you know again
like we're over using 36 we have huge
amounts of data and if you're trying to
move towards you know real time
processing you money to downsample your
your cloud also for certain applications
like for instance finding a plane you
don't really need
high density of points you can you can
just go with the lower density there's a
lot of out layers in 3d data because
three sensing is still not as as good as
2d sensing or same so you might need you
know methods to remove all players from
the data and here's an example with a
statistical outlier removal filter as we
call it
you know the classical moving this
squares problem from computer graphics
applied to real data sets in robotics
again for smoothing data and again by
cleaning out layers very easy to use you
don't have to be a computer graphics
expert to use this feature estimation is
also very important an important topic
not only for registration but also for
surface learning and whatnot and here's
examples of doing normal estimation you
can find curvatures in a cloud you can
find boundary points which could be of
interest for our application or as we
call them principle curvatures so you
get like tiny coordinate systems at each
point which again could help you
depending on what you're working on
there's classification examples with
here's an example of a fast point
feature histogram classification so you
basically train data for surfaces convex
and concave and this is what a simple
classifier can give you out this is
something newer called a three-point
feature histogram where you will look at
partial views of objects and then given
the fact that you already have them in a
database like as CAD models you can
recreate some views by doing virtual ray
tracing and then you compare the objects
in a histogram space and the results are
really really impressive this is the
only prior here's it's like there's some
segmentation existing there so it's not
gonna work in high color other things
like I'm not gonna have time to show
everything here but 2d all truly / 3d
features we can call them mostly because
they're in range images that are
actually in 3d and these are very useful
for for segmental sorry for segmentation
and for registration and speaking about
registration here's an example Carolina
showed this earlier in fact this time is
not gonna give you the perfect model
that you're actually looking for in
applications such as grass being so you
need to do refinement and here's an
example of running a simple wall I call
it
it's it's a it's a classical ICP with
some bells and whistles on top that
gives you an acronym it's nicer models
and I get another example for a larger
scene we're just basically moving your
camera around and reading is like nice
to be panoramas but here using only 3d
information so you can turn the lights
off it's the vertical moron segmentation
getting planes out computing holes out
of them getting the objects supported by
them so polygonal prisons and so on
clustering segmenting differences
between them there's really a lot of
functions there I'm not gonna have time
unfortunately to say well maybe I can
show you one example maybe I can pick
one let's see this is part of what we
call the rust Woody contest and we are
so we basically encourage people out
there to try our software try PCL try
the connect drivers the people and so on
and come up with cool applications and
we had some prizes behind them so they
were like a lot of students but also
like just engineers all around the world
and we're really like you know dazzled
by by their creativity I'm not really
sure exactly which one is the best to
show because I have a few here this is
actually the one that one so it doesn't
have a lot to do with robotics but we
thought that the idea is just like so
simple and so you know already in some
sense this is one of our key
co-developers Gallagher from MIT and I'm
gonna skip through it but basically what
he does is he just draws things on a
surface and does like some simple
segmentation and then creates like his
own virtual whatever piano it's a really
simple stuff again like it has nothing
to do with robotics but if activity is
so fast and he documented properly and
released the code in in PSD open source
form that was just really something that
we liked about and like more complex
things again you know doing green visual
slam again just using three data here
and not using 2d so we think there's
some advantages to
and finally I was about to talk more
about their uc-berkeley one but um so
all our colleagues from UC Berkeley they
they basically got a quadrotor with a
Kinect on top and they're doing some
really using PCL for segmentation for
real-time obstacle avoidance and
segmentation of certain areas it's
really impressive that you know we
managed for PCL to Android to you
networks in Windows Linux and on Macs
and it's just great to see people use it
use it for applications that we never
thought of and there's other examples
too but unfortunately I think I ran out
of time thank thank you for your time
thank you and we have five minutes for
questions we're running about five
minutes late but that's more my fault
than theirs that's a fantastic question
we actually have an ongoing project
right now I didn't have time to talk
about it it's it's a project on point
cloud compression it's very interesting
it's both spatial and temporal so we're
basically looking at weight so the
Kinect is actually overwhelming us with
data you have 30 frames per second 640
by 480
you know our USB path just goes like CPU
spikes and so on so want to see whether
we can actually do something to reduce
the data so if you're not moving right
if you're seen it static or even if it's
not why is it from that scene that
you're trying to actually investigate
you to try to do every frame or just you
know every key frame I can talk to you
about afterwards are you interested
so the sensor on the robot is actually
an LED projector oh sorry the question
was whether the IR capture projectors
are problem when you've got multiple
robots running around the scene um so
the projector on the robot is actually
it's visible lights LED and you can see
the texture being projected onto the
scene
um and it's really only meant to be work
about a meter away from the robot for
like tabletop manipulation kind of stuff
so you're unlikely to have multiple
robots really interfering in that space
now I'm gonna hand the Kinect which is
IR basically if you have a lot of them
they do start to interfere a certain
amount exactly how much they interfere
I'm not sure but they definitely do
interfere so we did do some tests well I
don't think they're official right but
uh you know you can get up to four or
something and it's absolutely no problem
personally me or Caroline she's she's 2d
I can show the slide that I want not
arguing well I do think the love to the
wall classical computer vision
researchers have been stuck in a local
minima to some extent and it's very hard
to pull them out of it
there's Caroline actually mentioned it's
a lot of sensing modalities that could
help us tremendously in robotics and a
lot of people are just plain refusing to
use those things and I think because of
that we're actually progressing slowly
we would like to
so there's definitely something to be
said about that all right
let's thank our speakers again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>