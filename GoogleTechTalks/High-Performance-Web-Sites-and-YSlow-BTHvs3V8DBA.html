<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>High Performance Web Sites and YSlow | Coder Coacher - Coaching Coders</title><meta content="High Performance Web Sites and YSlow - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>High Performance Web Sites and YSlow</b></h2><h5 class="post__date">2007-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BTHvs3V8DBA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yes I do now
the Vicodin jackson.com
ate gaps and that's where I first kind
of met Steve I didn't come out for this
conference called the Ajax experience to
talk about only this great staff he was
doing a yahoo with respect to
performance that then when I become to
become his book and the yslow
application Firebug extension is going
to talk about and he's got some fun
anecdotes about how he came up with the
name and some other fun stories to share
with us so let's say Rock and Steve from
Rahu
I just wanted to say every once while in
your career you get to meet people who
are incredibly smart and incredibly nice
people too and Dion's one of those guys
and so I wanted to present you with a
copy of my book the honor all right
thanks for having me here I'm sorry I'm
sentimental I'm emotional that's okay
so my name's Steve Souders I'm the chief
performance Yahoo and I understand Doug
Crockford from Yahoo was here a month or
so ago so it's maybe not the first time
that someone from Yahoo has come and
done a presentation at Google but I do
have to say I will you know was
wondering how this was going to go and
and you know was trying to get myself
prepared for this and and so I just
wanted to start off with this note and I
don't know if anyone remembers
concentration Ed McMahon was one of the
hosts for it yeah and so anyone from the
audience one thing I guess
Sissy Spacek and someone who looks like
Peter Frampton in the movie Carrie
and how many people here actually know
what this expression means where has
ever used it
so preaching to the choir might have
been another good one to use but in
revisit that I didn't think that was
this interesting so so right my life
said no is going to know what that means
so I thought I would look it up in
Wikipedia and bring that as well so so
it kind of means something that's so
it's selling coal or or carrying coal to
Newcastle it's something that is maybe a
foolhardy or pointless and so given how
fast google sites are and the work that
you're doing on performance maybe
there's not a lot of motivation or need
to come and talk more about performance
but as was shown with with this
expression there can even be cases where
someone comes and surprises people by
carrying coal to Newcastle and actually
being able to sell it so I'm hoping that
there are some takeaways that you can
get from these performance best
practices that we found at Yahoo and so
let's let's dig into that so I've been
at Yahoo for about eight years working
on various things and about three years
ago some of the folks there asked me to
start a group to focus on performance
and and so I called the group the
exceptional performance group and the
Charter is very simple we're supposed to
quantify and improve the performance of
all Yahoo products worldwide so that's a
really big charter we scoped it down a
little bit I kind of break performance
into two areas response times there's
kind of a black box perspective of it
and efficiency so efficiency actually is
easier to correlate to two dollars if
you can do what you're doing with half
the hardware that's a lot of hardware
costs that you've saved power
consumption Rackspace but actually the
area that I focused on for the last
three years is the response time and the
main reason for that is at Yahoo of
course we want to reduce our hardware
cost and our power costs but really it's
really important for us to have a very
good user experience very engaging
products that
increase stickiness and and user
adoption so that's where we've been
focusing and we've also narrowed it down
a little bit by focusing almost
exclusively on Web Apps so we're not
like trying to optimize Yahoo Messenger
and we're kind of like a consulting
group within yahoo we've we've done this
at Yahoo and other areas like security
and you know redundancy we can have a
small group of people in my case our
groups about 5 to 7 people we have a
small group of people that focus
full-time on this area and can really do
a deep dive and then we disseminate we
evangelize those best practices across
the company so we build tools like yslow
I'll be showing later we have lots of
other tools some we've released some are
internal we look at a lot of data we do
research so we'll we'll do what kind of
have on-the-job experience we'll go out
and we'll do consulting with groups and
we'll go oh wow this is what really made
this site go a lot faster let's store
that away as a best practice let's see
if we can generalize it and make it
applicable see if it's applicable to 80%
of the properties that's what we call
them at Yahoo 80% of the properties at
Yahoo so we tried to identify these best
practices sometimes we have to research
the best practices so no one's actually
doing it yet but we think there's a way
to navigate through this the set of
constraints to find something that's
going to accelerate the user experience
so we do research and when we find these
best practices we evangelize them out to
the to the company so when I started
this my background is more on back-end
engineering so some of the first
projects I did at Yahoo I ran to my
Yahoo team for 3 years I built an
architecture that pushed all of our
content all our our sports scores and
movie listings and TV listings worldwide
I wrote a caching layer between all of
our properties so if you you had a
property that needed personal
information like someone's calendar for
that day they
and have to constantly bombard the
calendar service with web service calls
we could cash that in a way of updating
the cache and expiring the cache and
flushing the cache so when I was
approached to start this performance
team I thought okay well this is girl
work well you know I've worked on
large-scale systems trying to make them
as efficient as possible but I said
before I start digging into this if the
we identified early on that the goal was
really to make the user experience
faster I said let me look at that let me
analyze that profile that and see what
the long tenth in the poll is and so I
found something that was kind of
surprising I'm glad I found it right
away because it completely flipped the
approach to looking at performance at
Yahoo so this is this is from a packet
sniffer called IBM page detailer each of
the bars is an HTTP request the first
you can see they're labeled and the
first one is the HTML document and in
this case this is dub dub wo comm with
an empty cache and the thing that's was
very surprising to me was only 5% of the
overall user wait time was getting that
HTML document and that includes not just
the web server stitching the content
together but the time for the request to
go up and all of the packets to come
back all of that was only 5% and in my
previous life working on these large
websites that's the part I was always
focusing on how can I build a better
database or cache data in mint in memory
or change my compiler options anything
to squeeze out a couple more
milliseconds and it turns out that I was
working on the short tenth in the poll
so really the long tenth is this I call
it the front end I think most of the
time when someone says the front end
part they might be thinking about like
JavaScript execution so this is bigger
than that it's really I call front end
everything after the HTML document has
arrived to the browser once the browser
has that delivery of the of the page
what does the browser have to do from
that point forward I call that the front
end part so there's certainly JavaScript
HTML Javascript CSS parsing JavaScript
execution but there also is a lot of
other Network time involved therefore
all of these other HTTP requests most of
the time there isn't for this other part
this front-end part there usually isn't
a lot of back-end time web server time
because most of these are static assets
that are just read off the disk but some
of them could be HX requests or
something that take a little longer but
in this case we found that 95 percent of
time for a empty cache is spent on
everything after the HTML document so I
thought okay well what is it with a
prime cache so even in that case it's
only 12% there's a little white gap here
in the middle where the browser is
reading those cached assets off the disk
and having to reparse the CSS and
JavaScript and execute the JavaScript
and at the end there are still a handful
of requests for images that have changed
or ads or beacons or something but still
only 12% was was that back-end part
getting the HTML document so this you
know really surprised me and I said well
maybe this is something peculiar to
dub-dub-dub but as I looked at more and
more sites I found that this pattern
held true that only 10 to 20% of the
total end-user experience was spanking
the HTML document down so these are the
top as of about 6 months ago these are
the top 10 sites in the US there's only
one that breaks that kind of guideline
and that's Google in a prime cache so
there's only two HTTP requests for
Google with a prime cache just
dub-dub-dub the google calm and but even
here the HTML you think like I think the
other one is a beacon maybe I was in
like a test bucket or something like
that I don't know if it happens all the
time but here the HTML document was
still only 36% but this is the exception
almost every site you go to you'll find
what we call the performance golden rule
that 80 to 90% of the end-user response
time the time the user is waiting is
spent on this part after the HTML
document arrives and so that's if you
really want to improve response times
that's where you have to focus I've got
three good reasons why you
you believe that one just the opry
probability of making an improvement is
greater if you focus on that front-end
part and your wildest dreams maybe you
could cut the backend performance in
half
well you'd put a five to ten percent
dent in the response time in the user
experience but if you could cut that
front-end part in half you're going to
make a forty to forty five percent debt
and that's going to be huge users are
actually going to notice that so just a
priori you have a better chance of
making a big difference the changes are
simpler so you know if you want to
change cut half off the back end
response time you have to come up with a
new database schema optimize your code
replicate your architecture across
multiple data centers worldwide huge
huge complex tasks where as you'll see
in a minute I'll talk about some of
these guidelines in more detail most of
them are hours or days of work change
your web server configuration rearrange
the page a little bit nothing that's
really that complex so the changes are
simpler and they're proven to work so my
group has worked with probably a hundred
properties at Yahoo it's pretty easy for
us in fact there's only one exception I
can think of where we've haven't been
able to go in and with just a few days
work cut 25% off the response time of
any website and what's also cool is now
that the book is out and yslow is out
I'm getting emails from people at small
and large companies everywhere that
they've tried rules you know 2 3 &amp;amp; 7 and
they've cut 20% for 40% off their
response times so this doesn't just
happen at Yahoo and it doesn't just
happen at gigantic websites these rules
really apply to almost any webpage that
you're building so I wanted to talk a
little bit I'm gonna have just a few
slides about some research and then I'm
going to go through the bulk of the talk
is the rules that the guidelines we have
and then at the end I'll run yslow and
we could like look at some Google Sites
or any other sites and analyze them to
do kinds kind of some live analysis so
my coworker at any toy or blogs about
most of our research
on Yui blog comm and I'm going to talk
about one of these experiments that we
wrote up the browser cache experiment
because a lot of our our best practices
hinge on increasing the use of the
browser's cache but before we really
could know how valuable that was we had
to answer the question how many users
come in with a prime cache we didn't
know no one knew and I couldn't find any
research about that out there in the
world so we made up this experiment
where we put a little 1x1 pixel in a
page but we had to be kind of careful
about these two response headers we put
the expires in the past and we made sure
that on all the servers the file stamp
was identical so the last modified
timestamp for no matter which server you
went to for this image would always be
the same and so we know that there's
going to be two possible HTTP status
codes returned for this either a 200
which tells us the user had an empty
cache or 304 which tells us that the
user had downloaded this image
previously they have it in their cache
with this last modified header so when
they requested when they went to the
page again and requested that image
again they made a IMS if modified since
request or conditional get requests they
said I have a copy of this image on my
disk that was last modified at this time
and the web server says oh we'll just
use that one 304 not modified just use
that one but those two different status
codes 200 and 304 are written into the
web server logs so we can just go
through the web server logs and find the
ratios of these two hundreds two three
or fours and answer these two questions
what percentage of users come in every
day with an empty cache and what
percentage of pageviews happen every day
with users with an empty cache and we'll
see that those are two different numbers
so on the first day no one has seen this
image before so a hundred percent of the
users come in at least once a day with
out having this image they come in at
least once a day
considered having an empty cache and
100% of the pageviews have an empty
cache but then over time more and more
users are going to get this pixel this
image written into their cache and as
they go to the pages
that image is going to get a 304 status
code response so after and we've run
this on various sites at Yahoo it always
happens at for about 15 days we hit a
steady state and it always comes out to
these numbers pretty much no matter what
website we're looking at about 80% of
the pageviews are done with a prime
cache or full cache and 20% with an
empty cache and for users it varies
between forty to sixty and I don't mean
that we run it on property X and it'll
be 40 than 60 40 than 60 what I mean is
if property X is really sticky then
maybe only 40% of the users are coming
in with an empty cache every day
whereas if property X is not very sticky
60% but it ranges in in those values in
that range so what does this tell us
unfortunately it tells us our job is
really hard because both of those
numbers are really high you can't ignore
these users who are coming in with an
empty cache every day you know 50
percent of your users about are coming
in with an empty cache and that's going
to be their first page view that's
really going to set their expectations
their impression of what the site's
performance is going to be like so you
have to optimize for that you have to
make sure that when people come in with
an empty cache for that first page view
the page still really fires fast but
then you also have to think about 80% of
the time people are coming in with a
prime cache so you don't want to do
things that really optimize the empty
cache but then kind of penalize that 80%
of page views that happen over time so
that was one experiment we wrote up and
you can go there and you can read the
other ones so now we're diving to the 14
rules and I'm actually going to for the
sake of time so I want to try to wrap up
in under an hour
I'm going to skip four of them and a lot
of these you guys are already you
already know you're already practicing
but certainly these four avoid CSS
expressions reduce DNS lookups remove
duplicate scripts and configure e-tags
are things that I haven't seen Google
ever be a concern for Google Sites
so we're skip those when we go through
the others as fast as I can so the most
important one and these are in
priority order so we saw from that
packet sniffer plot that there's a lot
of HTTP requests that happened after the
HTML document comes down and that's
getting even more so our sites are
becoming richer there's more JavaScript
on our pages so a obvious way to improve
performance is to reduce the number of
HTTP requests but the constraint I set
myself in I set for myself was how do
you do that without changing the content
on the page because I'm not a designer I
don't want to go back and tell the
designers that they should maybe not
have so many rounded corners or not so
many images given the content on the
page what the designers have come up
with how can you reduce the number of
HTTP requests in that design so one is I
would go into CSS sprites in the next
slide but let me just do these other
three if you have six JavaScript files
just combine them into one so your
instead of six HTTP requests you're just
going to have one the overhead is going
to make that page faster even if you
have keepalive on same thing if you have
four CSS files combine them into one
image maps are kind of old-school but if
it works for you if you have four icons
that are next to each other and they
could just be one image map do an image
map inline images are very very cool
unfortunately they're not supported in
IE but if you have this is where you can
actually take like the contents of an
image or JavaScript file or anything
else and actually inline it in the page
and so for us there are some cases where
if this has been so important we've
actually forked our back-end code to
just deliver data URLs for browsers that
support it but the most important one to
have a big gain in performance is CSS
sprites how many people here have ever
built a sprite cool the Google front
page uses a CSS sprite so the idea is
how many people here have ever played
with a Ouija board Wow more people have
built sprites them use the Ouija board
I've never had that happen before that's
very cool
so the idea is you know with a Ouija
board there's that glass thing that
everyone puts their fingers on the
planchette
and so think of think of any box that
you have in your page a div or a span or
whatever as that plan shed and the Ouija
board is really all these images that
you've combined into a single image so
in this case these were 60 icons I don't
think the Yahoo frontpage ever had all
60 of these on the page but they had a
lot of them and we said wow it's a lot
of HTTP requests let's combine those
into a sprite and when we did that they
said oh well if it's just one image
let's add these other icons that we
might want to use in the future so we
have 60 icons here and we just combined
them into one image with a little bit of
white space separating them and now we
can take our div like the planchette and
we can slide it over the background
image using the background position CSS
styling and and the size of the div will
dictate how much of that background
shows through so now we can get sixty
icons available on that page without
with only one HTTP request so this is
really powerful if you have a lot of
background images combining them into
one images is a way to really cut down
on HTTP requests there are some cases
where depending on if they're being used
for corners you might not be able to fit
all of your CSS background images into
one sprite but if you can go from 20
background images to just four images
that's still a huge savings something
that's kind of interesting is you would
think that the overall combined size
would actually be bigger than the sum of
the individual files because of the
extra whitespace but each individual
file has some color table and formatting
overhead in it so when you add them up
the the combined file the sprite file is
actually smaller than the sum of all the
individual files so you save download
size to yeah oh and please ask questions
in the middle
just so that your your box might be a
little bigger than the actual image like
like my my icon might be a 16 by 16 size
but maybe I have one that's just a
candle and it's very narrow and I might
have if I said well let me just try to
squeeze them together as close as
possible I might have gone too far
inside that 16 by 16 space so you and
then you just add that extra white space
just kind of for a little bit of safety
I think it makes it a little more
flexible okay so use a CDN so I don't
know what I just did some NS lookups on
on these popular sites with CDNs they
used and yeah actually you guys host
everything on the same domain so so
maybe we could talk a lit later you
could describe your hardware topology to
me
but you can see that you know Akamai is
kind of the industry leader and we made
this this change on Yahoo shopping about
two and a half years ago they were still
serving their content off shopping that
yahoo.com we made this one change moving
all the static stuff to our CDN and it
cut 25 percent off the response time
just this one change alone and and the
point I try to emphasize especially to
kind of startup companies is make this
step before you try replicating your
architecture because like I was saying
before splitting your back-end
application across multiple data centers
can be very complex and time-consuming
and this is pretty easy there are some
costs involved paying for a service
Akamai or whatever there's a new one
Panther Panther Express I think which is
very very reasonable I just heard about
them this weekend but make this step
first before you ever decide to split
your version at a far future expires
header so I want to mention here I
wasn't I received a very nice compliment
on my book that they thought being that
I was from Yahoo that I was very
even-handed in my analysis of different
sites including Google in the book I
really appreciated that I tried you know
very sincerely to be objective so I just
want to point out here I didn't pick
frugal because I was trying to find a
Google site that was bad it's just that
dub dub dub google.com doesn't have
really any content on and it's not very
rich and so it wasn't a very interesting
one to analyze so I looked around and I
picked frugal as the kind of Google
example to analyze for things like
expires headers and I should also
mention that's why Craigslist isn't on
here Craigslist is still in the top ten
but there's nothing on it so I switched
it out for AOL but anyway what we see
here is that depending on the site
you're looking at they they more or less
believe in in making assets cashable on
the browser and so the idea here just
really quick is if you put a far future
expires header now the browser has that
on the disk the next time the user goes
to the page if it hasn't been flushed
from the cat disk cache the browser says
oh there's the thing I
oh and look it's still valid it's still
fresh it doesn't expire until 2010 or
2013 so it can just use it off the disk
whereas if you don't have an expires
header the browser will see it on disk
if it hasn't been flushed but it'll say
oh it's not fresh anymore let me make
that if modified since conditional get
request and the web server can still
luckily return to 304 if the asset
hasn't changed at all but that's still a
round trip if you're an Idaho on a slow
internet connection let's go to slow
down the page so it's really good if you
have static assets to put a far future
expires header on it and that way the
browser the next time the user goes to
that page can just read it off the disk
without any network traffic at all so
the challenge about this is suppose you
have an asset a JavaScript file or an
image and it changes well for years
we've had the policy at Yahoo that once
you put pushed something out to a large
user base on the internet you can't
change it because there are so many
misconfigured proxies or overly
aggressive caching technologies that
they might not pick up that change and
when we do make a change especially if
it's like a bug fix to a javascript file
we want to make sure that every user
gets that new file so for years we've
had the policy of putting a timestamp or
a version number in our URLs of our
static assets so if you're doing that if
you've already swallowed the pill that
you can never change an asset once it's
pushed the only way to do that is to
change the filename you might as well
make that asset cashable forever so so
sometimes people say like like CNN only
two out of a hundred and fifty one
assets have a far future expires header
well that's a new site maybe a lot of
these things were changing a lot their
photos and they're constantly changing
and they it was just easier for them so
then what I do is I look at the median
age this is the the number of days
between when I ran this test and how far
back the last the asset was modified
based on the last modified header so I
can count the number of days that has
been since this asset was modified
and if I look at the median of those on
CNN it's been seven
50% of the assets on this page that are
not cacheable have not been touched in
seven months and so we can see that
value for other sites so we kind of had
this same attitude especially with
JavaScript and CSS at Yahoo well you
know those are changing a lot and we
looked at the last modified header we
saw that they really weren't changing as
much as we thought so it's a good
practice to look at that and and figure
out if it would make sense to give
everything a far future expires header
so it's kind of interesting to look at
these rules from the perspective of do
they help empty cache so first page
viewers only prime cache or subsequent
page viewers or both so this is one that
and the ones that help both are really
really key so this is one that helps
both especially first people who visit
the site for the very first time so you
can just compress anything that's not
already binary you don't want to
compress images or flash or PDFs but not
just HTML documents but JavaScript files
CSS files JSON requests Ajax requests
all of those can be can be compressed
and typically you'll cut about 70% off
the size of what's sent over the wire
and so everything's very get there a lot
faster and there's always some edge case
browsers that might have problems but
that number is getting smaller and
smaller
and you can do different approach
approaches like a whitelist approach to
turning on gzipping or not yeah and
here's the point I was making it's
pretty popular to gzip the HTML document
but it's less well known or less
practice to gzip the CSS and JavaScript
so that would be good to do so this is a
interesting one this actually doesn't
make the response time the mechanical
instrumented response time any faster
but makes the perceived response time
faster and that's really what we're
after trying to make that user
experience feel as fast as it can so the
thing that happens here is it's a little
different in ie and Firefox in IE it
gets the HTML page parses
finds all the assets that have to be
downloaded all the components or
resources and maybe at the bottom
there's a CSS file right well ie says
well that CSS file might change the way
that I draw elements in the page tables
or anchors so what I'm gonna do is I'm
not going to draw anything in the page
until I download that CSS file
well since it's the last thing in the
page it's one of the last things most
likely to get downloaded so the IE will
have all the static HTML text in the
page it might also download images and
other things in the page it's going to
hold all of those it's just going to
leave the page white until it downloads
that final CSS file and then all of a
sudden it'll draw the draw the page so
this isn't what you want you want to get
the CSS files the stylesheet
declarations inclusions up in the head
and that's also what the spec says to do
so it's a good thing to do Firefox is
different Firefox will render things
when it has them and so if you had this
same scenario where you had a stylesheet
at the bottom it would render the HTML
to render the images finally it would
download this stylesheet which would say
draw everything differently change the
font change the way anchors look and now
you have what's called the flash of
unstyled content the page gets redrawn
and it's a flash experience to the user
which isn't pleasant either so the key
is to put the style sheets in the head
another small change to try to follow is
don't use the add import rule because in
IE that will cause the stylesheet to
actually be deferred later in the page
and since it's deferred you'll have this
non rendering no progressive rendering
behavior in IE so use the link tag for
pulling in style sheets and kind of the
other side of the coin is scripts
scripts have two bad behaviors one is
they block all parallel downloads so
with HTTP 1.1 you can download two
components per hostname in parallel so
if everything was on one hostname you'd
see a stair step pattern like this but
maybe you're using two or three host
names so you can actually get some
things in parallel but still on any
given host name no more than two I've
actually gotten ie two
load 114 things in parallel but as soon
as the browser hits a script and this is
in both ie and Firefox and won't
download start any other downloads no
matter what the host name is until that
script is returned so one thing you want
to be careful of is putting the scripts
higher than they need to be if we could
move this script maybe we couldn't move
it all the way down maybe it actually is
doing a document riot or something like
that but if we can move it just a little
ways down some of those images would
actually be drawn once they got
downloaded the browser would go ahead
and render them and all of the text
above the script would be rendered but
not only do scripts block parallel
downloads they also block rendering
anything that's below the script will
not be rendered until the script is
downloaded so it's not always possible
you know you might have scoping issues
that require it to be higher in the page
but if not move the scripts as low in
the page as possible or better yet
load them with an onload event handler
and defer doesn't help it's only works
in IE and it's not supported by Firefox
and even in IE it doesn't defer it to
the end it just defers it a couple
resources so it'll still have this
blocking behavior well skipper rule 7
rule eight so far I've kind of talked a
lot about scripts and stylesheets being
external but should you really make them
external or not and so that really kind
of depends on the way users interact
with the site so if this is a site for
example that users only come two three
times a month and they only have one
page view you should make your
JavaScript and CSS external inline it
because the advantage of making an
external is it will be cached and the
next time the user comes though they
won't have to download that 10k or 40k
of JavaScript but the user is only doing
one page view their next page view might
not be for eight or ten days and by that
time especially when we look at how many
users come in with an empty cache the
that asset might have been purged from
their cache so it might be the fastest
experience for that type of site to
inline everything but then if you're a
site that has multiple page views per
session or a high revisitation
rate you might want to make those that
javascript and CSS external components
with a far future expires header make
them cacheable and now when the user
comes in they might have to do a extra
HTTP request to on their first page view
but now for the next four page views
that will be a faster experience and
it'll be less bandwidth costs there's a
couple extra credit things you can do
here post onload download is okay
maybe like mail might be a good example
on the first page of mail the mail
launch page I want that to be really
fast so I'm going to put all my
JavaScript and CSS in the page itself
then in the onload event I'm gonna
download that javascript and CSS again
but I'm go download them in their
external file format and they'll get
written to cache so now when the user
actually goes to the next page you can
include those external assets and
they'll already be in the cache and that
page will be very fast because it will
read the JavaScript and CSS from cache
rather than having to download it over
the wire again the problem with that is
that first page that launch page is
always going to have that JavaScript and
CSS in it even if they have the external
assets so then you can do something like
when you download those external assets
set a cookie a pretty short-lived cookie
maybe session base maybe just a day or a
week and now on the backend server when
you're serving the launch page look for
the presence of that cookie if you see
the cookie it's a good indicator they
have the external assets so use script
source but if you don't see the cookie
inline it and do the post onload
download rule 10 min of a JavaScript
Google certainly does this you'll see
not most of the top 10 sites don't do it
minification just removing whitespace
comments but you could also minify
inline scripts too and there's even
fewer sites that do this and I wouldn't
even argue it might be easier to do that
because you just have to hook in click
all your JavaScript script insertions
into a function and J s min is kind of
the most popular one and it is written
in almost every language written by Doug
Crockford
at Yahoo and it's available in a lot of
different languages so it'd be easy to
hook into your back-end system but I
wanted to point out just recently Julien
Lecomte
has come out with the Yui compressor
that's available as of about a month or
two ago and it works on JavaScript and
CSS so that's nice but it's more of an
obfuscator typically obfuscators have so
they have greater save as you see here
minification cut 20% obfuscation where
we take long function variable name
symbol names and make them shorter has
even greater savings as you would expect
but they can also introduce bugs I've
seen that happen but the nice thing
about the yui compressor Julianne's
taking a little different approach there
it's very safe it's almost as say safe
as GS men now it's not as fast as
jasmine so if you're doing real time
cleanup of your Java Script I recommend
using jasmine but if you're doing that
as part of a build process the yui
compressor will actually have greater
savings and it works on CSS as well
avoiding redirect so I kind of call this
the the worst form of blocking I talked
about how scripts can block downloads
but if you put a redirect in front of
the HTML response
everything is delayed so we can do
things in our HTML page that I've been
talking about to increase
parallelization to increase progressive
rendering but if you put a redirect in
front of the HTML document
none of that hard work can be taken
advantage of so try not to put redirects
in front your HTML documents
last rule and we were talking about is
making Ajax cacheable so Ajax requests
are dynamic and a lot of times we think
like our HTML documents we almost never
want to make cacheable because they're
very dynamic
okay so Ajax responses are dynamically
you know so maybe we shouldn't make them
cacheable and a lot of times Ajax
responses are personalized they have
parts of the of them that are only
appropriate for that single individual
user so you would kind of think well
maybe I should make that cashable I mean
it's not a static image for example with
the scenario I always bring bring up is
something like a male web app
maybe the mail application on launch is
doing an AJAX request to get your
addresses right and so those addresses
maybe you change your addresses a lot
for me maybe I add an address once a
week so if I go to mail three times a
day seven days a week
twenty-one times I'm going to that page
and it's making twenty-one downloads of
my Ajax address book
whereas if instead we just put something
in the URL that indicated the timestamp
of that address book when was the last
time I edited my address book and just
on the backend server when you're
stitching together the URL for that Ajax
request make sure that you embed that in
the URL if I haven't edited my address
book the URLs the same the Ajax request
will just read from disk but if I have
changed it the ajax URL will change and
now i'll make another request but I'll
make it cashable so look at your Ajax
request you might be able to make those
cashable as well so I'm really quick I'm
going to talk about the second edition
of the book and we're give a little
prelude to that I've got five new rules
one is split dominant content domains so
this is Google News and you see that
stair step pattern tu-tu-tu-tu-tu right
and that's because all of those images
are using the same host name so and we
can see that kind of two to two right
you can see that whenever you know a
dark blue one ends the next one starts
the next one starts the next one starts
the light blue one so on and so on so if
we actually use two host names there
instead of one we could get four
downloads in parallel and look what it
does to the overall response time of the
page cuts about a third off the response
time right so we've done studies and we
wrote them up there about well you know
how far should I take this should I use
three host names for Hosting's five host
names we found that once you go to 4 and
above if the benefits start degrading
because of DNS lookups and I think
thrashing on the browser side on the CPU
side but definitely splitting things
across more than one domain is something
to investigate just be careful of cookie
wait the expiration model is different
than HTTP so
a lot of times I think when people are
creating cookies so they don't think
about aggressively cleaning up that
cookie wait so try not to set your
cookie expiration dates too far out or
you might see those cookies lingering
for a while now Yahoo we host our static
content on a different domain that's not
on yahoo.com
and that is to avoid that cookie wait
for our static content it doesn't change
based on the user's cookie state so we
serve it on a different domain and those
HTTP headers are much smaller minify CSS
now with Yui compressor I'll start doing
more research on that and we also want
to do maybe not obfuscation but
simplification like change ffffff to
just FFF 0px to just zero use iframes
wisely especially for ads putting
third-party content inside an iframe has
some benefits especially sandbox in
JavaScript but just be careful about how
much you do that don't go creating
iframes willy-nilly there are very
expensive Dom event just a blank iframe
can add 20 to 50 milliseconds to the
load time of the page and so just be
careful about that I've got some more
details about that they'll be coming and
look at optimizing images I haven't
talked about this too much in my
previous work but we've seen sites where
we could save 80% on the size of images
by changing the format or just
optimizing the format that we're using
without any loss in image quality so
I'll be writing about that too so I'm
almost done
Yahoo searches our poster child we
started working with them about a year
and a half ago we recommended these kind
of front-end deep performance changes
and over the year and a half we've cut
the response time by 40% for broadband
users you can see I always make the
analogy it's like closet space at home
right you clean out the closet and the
next week it's full again so you can see
they're like you know we drive things
down and then the people go oh it's a
lot faster now now we can add features
so you're always toeing the line right
you've got a fight fight the battle to
keep everyone focused on performance but
sometimes it does allow you a little
to add had features that users really
want that improved the experience so my
book is out has been out for about two
months
high-performance websites 10 D and I do
a lot of talking at conferences
I mostly blog on YT and she's on Yui
blog and we've also released yslow how
many people here have used yslow very
cool how many people here use Firebug
great so there's the URL you can
download it it's a performance lint tool
it gives you a grade against these 13
actually the first 13 rules it doesn't
recognize Ajax right now working on that
it's an extension to Firebug so it's an
extension to an extension and it's open
source so what I wanted to do now was
just really quick look at a couple sites
so here's dub dub dub Google com oh yes
do we have any kind of ongoing analysis
or monitoring that helps us make sure
that people aren't making the
performance worse yeah we do we have
ways of running yslow there's this
little option to run yslow and auto run
mode which means it will kick off
automatically and if you I'll give you
my card if you send me an email there's
a couple open-source technologies where
you can actually from like a Perl script
reach inside and touch the Dom of the
browser and so you could pull out you
could build some kind of harness that
could run things in an automated fashion
with a set of scripted URLs but really
the the biggest thing that we've done
and the reason why why we did why I did
yslow and Firebug I had I first wrote
yslow about two two and half years ago
as a bookmarklet than a greasemonkey
script and then Firebug came out and it
really took off at Yahoo most of these
rules some of them have to do with CD
ends and web server configuration but
really
a lot of these rules are targeted
towards front-end developers and Firebug
is the tool of choice at least at Yahoo
for front-end developers and so we
wanted to try to keep this focus on
performance during the product
development cycle and the way to do that
that has really worked out is put it in
the tool that the Ute that the
developers the front-end developers are
already using so that everyday is
they're doing stuff they'll just run
yslow every once a while and they'll
know what their grade is for the thing
that's out right now live and they can
see if they're getting better or worse
it kind of keeps people on their toes or
at least it makes them aware that
there's a if they're adding a new
feature there might be a response time
penalty for that a trade-off to consider
so we see here Google got a 99 it's not
possible to get a hundred so this is the
highest grade you can get but I just
wanted to point out so we can see here
that that it took about 170 I'm on about
4 megabits per second right now
it took about 170 milliseconds and it's
about 11 K so now I wanted to do this
how am i doing on time pretty good right
so very different styles to front pages
right yahoo has a lot more content and
again I'm not a designer I don't want to
talk about the trade-offs there I just
want to point out something about the
way yslow works so we see here this page
took one second and was a hundred and
fifty three K and yet it still got an A
how is that why slow as I mentioned I
try not to make recommendations about
changing design why so looks at the
quality with which a site was built so
different sites are going to have
different design requirements some might
need more images some might need less
some might need more JavaScript some
might need less so what yslow does is it
says given what you've done in this page
have you done it the best way possible
so you might have a lot of JavaScript
but did
you might have a lot of CSS background
images but did you do them with sprites
or not so even with so yslow doesn't
take into consideration at all the size
or response time of any of the assets in
the page it's just looking at the way
the page was built so I'll come back to
that in a minute but I just wanted to
you know what actually is we're running
out of time a little bit so I'm going to
wrap up and then I'll do some questions
so on that point here I have the top 10
sites they're told page weight response
time yslow grade I did a little
correlation coefficient calculation so
just a reminder correlation coefficients
run from minus 1 to 1 minus 1 means no
correlation zero means inverse
correlation 0 means no correlation 1
means highly correlated anything above
0.5 is considered strong or high
correlation typically so we're not too
surprised to see that there's a very
high point 9 for correlation between
response time and page weight so if you
have a bloated page it's typically
typically going to be slower but what I
was very satisfied to see was this high
correlation between response time and
yslow grade so yslow was built to
measure these best practices that if you
follow them will make your page faster
and what we found and we've looked at
much much larger number of sites than
just these what we found is during
development where sometimes it's
difficult to gather a lot of response
time measurements on your page as you're
building it you can have a pretty good
idea of how the page is going to respond
based on your wife's low grade if it's
getting better than what's out there
you're probably going to have a faster
page if it's getting worse or yslow
grade you're probably gonna have a
slower page so I think that's very
powerful so I wanted to wrap up two
takeaways the main thing that I
emphasize is looking at performance from
a different perspective I don't want to
say don't optimize back-end performance
especially for hardware cost power
consumption but if you really want to
put a dent in response times you got to
look at this front-end part of it a lot
of these things are not that hard to
implement and it's not that development
from this point forward is going to be
ten percents
lower because of these practices a lot
of them are get this mechanism in place
and then it's behind you it's a gift
that keeps on giving
so harvest this low-hanging fruit early
on and then you'll just reap the
benefits of it on an ongoing basis so
that's that one make the investment and
you should feel empowered that you
control response times so it used to be
I think that we felt the bandwidth speed
was actually the most critical
controller or variable for the end-user
response time but we've been able to cut
some response times by 50% by making
these engineering changes so there's
really a lot that you do control in how
fast your pages will appear the users
and finally look out for number one
Yahoo users are number one we're always
focused on doing harder work us taking
on harder work to make their experience
better we kind of feel that we're the
last line of defense before the pages
get out to the users and we want to do
everything we can to make that
experience as fast as it can be we think
that's critical at Yahoo and they hope
all of us do in front-end work feel that
same way and we're all looking out for
users on the internet and that's it
thank you
I'll take questions that's the Verrazano
Bridge in New York City
the question was what bridges that yes
well you can use we have various ways of
measuring response times you can use
services like keynote or Gomez you can
use tools like faster Fox the definition
I have for response time is the unload
event to the onload event and excuse me
the main reason that that I've defined
it that way is because it's a
measurement that we can apply across all
pages and it's absolutely true that the
more important thing is not to optimize
this this instrumentation that's been
put in place it's to optimize the user
experience so if users engage with your
page after the first you know everything
above the fold is rendered or liked in
my Yahoo the first modules across the
top are rendered then that's great and
if you can try to figure out a way to
measure to that point in the page where
users feel the page is engaging and they
feel it's done loading but if you're not
sure we fall back to that definition
unload to unload and also you can play
games you can move a lot of stuff that's
critical to the user experience to the
onload event and make that mechanical
time shorter and so we try to emphasize
you know whatever time you're measuring
try to understand how that
instrumentation works and really
understand how it reflects what the user
perceives the response time to be yes
the question was we observe that there's
a lot more web to OSD HTML which web
apps that have a lot of JavaScript that
runs on the client side do I see why
slow helping to measure that or make
performance suggestions there I'm not an
envisioning that right now we're
starting to at Yahoo especially with
people like Julian and Doug there we're
starting to try to more formally gather
our JavaScript performance best
practices similar to what we've done
with kind of overall front-end browser
web server interaction and I also feel
that Firebug does a pretty good job of
profiling in fact as an incredibly good
job profiling JavaScript code on the
client and so I don't feel that there's
such a lack of tools to help with that
right now
yes this gentleman pointed out that that
performance will also vary browser to
browser so you have to pay attention to
that you know I will say that that the
14th rule came out from looking at more
ajaxy apps where there's so much
JavaScript you know hundreds of K of
JavaScript in some cases that all of a
sudden the amount of HTTP traffic was
not really the issue it was that
JavaScript code executing now we did
kind of we did come up with rule 14
which is okay well how much of that
hundreds of K of JavaScript that's in it
may be in an Ajax response or Jason
responds could that be cached and that
will help but there's still this whole
other world where even if even if
everything was in cache just reading
that JavaScript off disk and having to
execute it could take seconds and
there's some exciting things coming up
in JavaScript performance out of
Microsoft and I forget who the other
folks are so stay tuned to that though
you know some ways of improving
JavaScript performance by an order of
magnitude or more any other questions
yes in the back
stress
question was why do the browser
developers stall things when they
download scripts so the idea is that
suppose you downloaded scripts in so I
want to preface this by saying I've
talked with the IE team and I'm hoping
to I've passed on the suggestion to the
Mozilla team there's no reason you
couldn't download scripts in parallel
and get everything to work correctly but
it would be a fair amount of work and
here's the reason they don't do it now
they didn't do it initially suppose you
have two scripts a and B a is 100k B is
1k but B requires a and you have them in
that order and you decide to download
them in parallel well guess what B's
going to come back first it's go be
executed and it's going to generate
errors because the code it relies on has
not been downloaded yet so that's one
reason there's also the document.write
reason so that's one reason why they
only download one thing at a time
to make sure that scripts will be
downloaded and executed in order but
clearly we can all think of ways that
you could achieve that without having to
have this blocking behavior I will say
that opera six does do download of
images in parallel with scripts and
that's kind of nice but still it will
not download more than one script at a
time okay two more question okay three
more questions
you can get my pending on my planning
yeah the question was have we looked at
how much pipe lighting helps with
performance it helps a lot I don't have
numbers that are firmer than that
because we feel that's moot I forget
what it is like Firefox has pipelining
support but it's turned off by default
and ie even ie 7 doesn't support
pipelining so it's gonna be so long
before we could really take advantage of
that we we've looked at it a little bit
but we haven't spent too much time
quantifying it do you mean parse time
just separated from execution time no
best practice the question was have we
looked at best practices for making
javascript parse time faster nothing
comes to mind
how are you
well the grades we measure with yslow do
you mean the response times oh well a
lot of times so you don't mean
mechanically how do we measure the time
just what's our process for you yeah so
you know sometimes there's nothing you
can do you make a change and it just
goes out and you just have to measure
the sight that's out there and there's
lots of variables that could make that a
that comparison invalid so what we try
to do is we try to run things in
parallel so we'll have a subset of users
some that are exposed to the benefit and
some that are the control and then among
those users we can see how fast or slow
the page is so that's the best way to do
side-by-side comparisons yeah or we can
use scripted means as well Kino Gomez
things and maybe last question and then
I'll wrap up was there one more yes you
received contributions to yslow from
outside of Yahoo and so what kind the
question was have I received
contributions to yslow from outside and
if so what kind now the it's an open
source license but it's not open code we
don't have the code published we get I
won't say lots maybe one or two emails a
day of bugs or suggestions and we're you
know working on a new release we try to
do monthly releases but certainly going
to an open code like Firebug has is
something that we've talked about and I
imagine will come in sometime but it's
not on the roadmap right now okay so I
wanted to wrap up and let everyone get
back to work thank you for having me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>