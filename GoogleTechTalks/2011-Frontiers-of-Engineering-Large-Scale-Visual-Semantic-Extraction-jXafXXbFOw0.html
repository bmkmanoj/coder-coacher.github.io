<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2011 Frontiers of Engineering: Large Scale Visual Semantic Extraction | Coder Coacher - Coaching Coders</title><meta content="2011 Frontiers of Engineering: Large Scale Visual Semantic Extraction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2011 Frontiers of Engineering: Large Scale Visual Semantic Extraction</b></h2><h5 class="post__date">2011-09-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jXafXXbFOw0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today we're going to sort of switch
gears a little and look at another
modality that is images and plots and
diagrams and so on so with that let me
go ahead and introduce the first speaker
dr. sami Benji Oh who's a research
scientist here at Google Oh too-sami
Thank You amar so working yes good so
yesterday there was a question about the
weather we were only using a web
document to infer some semantic about
the world so today I'm going to show you
an example of where we can actually use
images to infer text semantics if you
have enough data so let's start with a
single simple task or apparently simple
task which actually is difficult which
is I'm given a lot of images like these
ones they are actually coming from the
web and I'd like to know what they are
about now what they are about that can
be a very large statement it depends on
your are in your dictionary how you will
express what they are about so I call
that image annotation I want to annotate
each image attach some texts to each of
these images in there's a related task
in computer vision which is called image
classification where usually you will
have something like 20 classes are 100
classes or some small number like that
and you'd have to say whether this image
is a dog or cat or whether it's a
bicycle or car that's nice and there's a
lot of research into that but I want to
go further and try to go to a much
larger type of notation number of labels
you could assign to each image in
particular I'm going to talk about the
case where we looked at whether a way to
render images from a label set of about
a hundred thousand labels now 100,000 is
much more than what we usually see in
the literature in the very very recently
like last year there was the competition
on on trying to label images with
addiction
one thousand images labels and that was
already much bigger than the literature
and only a few months ago there was a
paper on how to label with 10,000 images
and I'm going to go a bit further and
we'll see that when we go there first of
course the task becomes much more
difficult but the but you'll see that
you can actually infer something more
interesting in terms of the relation
between the labels that you learn so
first yes sometimes size matters and it
matters in the fact that it gets more
and more difficult to label images when
the dictionary the number of different
labels you have in your hand is bigger
so that graph says that if if we only
had about a few hundred labels from
which we can pick to label a given image
on average we would make about
thirty-five percent of the time we would
be right that seems low but it's going
to be worse true as that has the task
becomes more difficult when you have
about the thousand that was the
competition you get to about twenty
percent accuracy and and when you have
about 10,000 well you're about six
percent accuracy these numbers are
really low there that's scary but we'll
see that it's not that bad but can we
can we improve that and what happens
here are we going to get to zero or
negative so here's the setting into
which I'm going to present some result
today I've been working on two databases
one of them is public it's called
imagenet it's actually built over
another database called wordnet which is
a ontology of the world that has been
built by humans and if so it's like this
tree of things like humans animals cars
and it grows it goes into very precise
semantics and some people have been
putting attaching images to each of the
nodes of that tree and that's imagenet
forward net
the total number of images at the time
we took their database was about 4
million it now grew to about 10 million
so I divided these into a training set
where I could actually train a model and
some test and validation set to verify
whether my motto was good the total
number of labels in that database is
about fifteen sixteen thousand which is
bigger than whatever has been published
up to now even though that database is
public and has been ready for two years
it's still too big for most people to
handle in parallel I've been having
access here too much bigger data set of
course we have access to all the images
on the web I took a very tiny portion of
them but that was still about 15 million
and and these 15 millions were taken
actually from our image search engine so
people using it search the type queries
they get images then sometimes they
click on them and I'm going to aggregate
that behavior if many many people type
this query and click on that image I'm
going to assume that this image is about
that query and of course if you do it
only once if only one person does it it
doesn't mean anything but if hundreds of
people do that there must be something
if everybody agrees that this image is
the interesting image to click for that
query there must be something so that's
how I got that data that means I have
about 100,000 label that was again a
tiny data compared to what we have
access to but it's still an order of
magnitude bigger than the rest and
you'll see that it's already very
difficult so how do how do we do image
annotation or image classification in
the literature how is this done so there
are two steps and actually most people
in the in the computer vision new
charger have focused on the first step
which I call feature extraction someone
gives you an image okay that's jpg
whatever comes from your camera what do
you do with that so you actually have to
extract some relevant information
from that image so that you can pass it
to some model that will try to classify
or annotate the image and the app
various ways to do so the most popular
one these the recent years has been the
following broadly speaking first you
look for places in the image that are
interesting so an example of a place in
the image that is not interesting is in
the middle of a blue sky well nothing
moves it's blue blue blue blue that's
not interesting so there are plenty of
algorithms to decide which places in the
image might be interesting then for
these places you try to represent them
so to extract something that explains
what it is you could extract for
instance the color around that place are
how the edges were so there are plenty
of techniques for doing that once you've
done that you have a variable number of
these points per image and now you need
to add Wregget that into a single vector
of information that will represent your
image that's the aggregation part the
most popular way to do so is to create a
dictionary so a dictionary of visual
features that you will create from a
very very large set of images so you'll
note for instance that out of million
images or 10 million images there's
always this type of color and that type
of car are very very popular so these
are your building blocks most images
should be built using these what I call
a dictionary word but leads are feature
words so for a given image now you're
just going to count how many times
you've seen that color and this edge and
that's going to be your presentation so
your presentation is going to be a count
of how many times I've seen that kind of
feature and this kind of feature in an
image and if you if you think of it
that's very similar to the bag of word
representation we talked about yesterday
while you were to count the number of
times you've seen that word and this
word in a sentence that means basically
that we forgot about the whole structure
like in text and we just count how many
times event happened in a given image so
that's the figure extraction part once
this is done you need to transform that
into a decision
this a car and most people do this in
the following way they they would have
access to a training set so a large
number of images for which they know
this is a car this is a bicycle etc
that's what i call the training set they
will extract features like this for each
of them then they will train a
classifier so you may have heard about
support vector machines for instance
that's one classifier they will train
you support vector machine for car and
another one for bicycle etc so that
that's next when you have 20 classes it
actually doesn't work that well when you
have a hundred thousand classes so
that's not the way to go if you really
want to scale so here is a solution
proposal for doing so which actually
scales much better we call that was ugly
don't ask and here is how it works we
have images and first we're going to
extract features from them exactly as I
said here no change and then we
transform these features so these
features were basically like a vector of
information of some size 1000 features
are 2,000 features we're going to
transform them linearly so I'm going to
map them multiply them by a matrix that
will give me another representation a
smaller more compact representation of
saying in this case 100 dimensional
vectors so I'm going to transform this
into a 100 dimensional vector I'm going
to explain later how I do that and this
is my space I call that an embedding
space i'm going to embed the images into
that space so just multiply it by a
matrix that gives me the new
representation but what's interesting
into that space is that i'm also going
to put each and every label into that
space so dolphin is a label it's just
the text dolphin but i'm going to
represent dolphin as a position in that
space so dolphin will be also a vector
of 100 values which will be somewhere
into the space same for obama and eiffel
tower and anything I have so each and
every label I have will have a position
in that space and every image that
I'm given can be transformed into that
space now that's cool because if I can
do that I can take any new image put it
into the space and then into that space
I can look at the nearest label so for
instance here i have my image dolphin i
put it into the space and I can look
what is the nearest label oh it's dalton
so i'm going to assert this image is a
dolphin that all looks good but of
course how do i do that i don't know
where I should put my labels and I don't
know how I should transfer my images
into that space that's the learning
algorithm and I'm going to to try to
learn to do that jointly now I want to
propose you a solution to do so and and
I think the solution to do is to do so
is it a try to learn to rank so you know
that that Google we're good at ranking
you give us a query we rank document and
we give you the top 5 10 whatever and
hopefully these ones are good well i'm
going to use the same technique except
that now the query is an image instead
of being a text and the documents that i
want to rank are going to be labels
instead of web documents but otherwise
it's exactly the same problem I want I
have access to a hundred thousand labels
and you won't look at all of them you
have something else to do so i'm going
to show you only 5 of 10 of them like
the top 5 according to my feeling
according to my model and and either
you're going to be happy or not so i
think our problem of image labeling is
actually a problem of Labor ranking it's
the same thing and if I can solve that
if I can order all the labels for a
given image such that the top ones are
relevant for that image I think I have
solved the image labeling problem now
how do we do that well we're going to
use a very stochastic approach I have
access to this very very large data set
10 millions of images each of them may
have one or two or sometimes five but
very rarely correct label attached to it
so an image can have more than one label
attached to it simply because for
instance in the case of
the way I took my images for image
search the same image could have been
clicked for the same for two different
queries that happens sometimes so here
is how I'm going to have I consider all
my training set I'm going to sample
randomly one image from it that's this
line then this image is attached in my
training set with some labels one two
three depends i'm going to sample one of
these labels randomly and then i'm going
to sample another label and that will be
a random label and it's very easy to
find such random labels because only one
or two are good and the hundred thousand
of them are wrong so I just pick any one
it's going to be wrong so I sample this
and now i use my models so i use this
thing I take my image I embed it into
the space I take the correct label I
look at its position I take the wrong
label that is sampled say Eiffel Tower I
take its position and i compute the
distances so the distance between all
right yeah the distance between the
image and the correct table and the
distance between the image and the
incorrect table and that's what I do
here if the distances are properly
ordered that is the distance to the good
label is smaller than the distance to
the wrong label then things are okay
don't move fine actually I want them to
be not only properly ordered by properly
order plus some margin so I want to be
safe and if that happens I don't move
anything if it doesn't happen then I use
technical gradient descent which can
compute impute for each parameter of my
model how I should modify it such that
next time it's going to be better the
distances will be better adapted and if
I do that very very often it will the
system will converge to the best local
solution and I'm just going to repeat
that for a while usually takes like one
or two days or three days up to one week
on a single machine now that's
interesting it works
a single mission I don't need the the
gazillion machines we have at Google I
just need one machine to do that but a
good one of course anyway now so
learning to rank is good it has been
used in various places not just image
annotation but sometimes that's not
exactly what you want let me give you an
example what what you really want
actually is to be to be to be able to
rank the label such that at the top of
the ranking so the first five you're
going to show are good you don't care
how it happens what happens at the
position 1,000 or 10,000 because nobody
will ever go there that's what happens
in google search and Google Image Search
never anybody goes very far after the
first page so you really want to be good
at the top of the ranking and the
problem is that if I just apply this
technique I'm going to end up with
problems such as this one suppose I'm
given to solution and for the two
solution I have an image and and two
good labels and in the first solution
the two labels are ranked position 1 and
100 and the second one they are ranked
position 50 and 52 I just made this
number up but the problem is that with
the type of loss and method is described
these two solutions would actually be
equal in terms of the loss i guess and
that's not really what we humans what we
really want to prefer out of these two
we really want to prepare this one we're
okay to get rid of one label as long as
one of them is well ranked while here
they're both already too far and we're
done not good so how can we do that so
the trait to do that is to wait when we
do this gradient descent is to say some
triplets are more important than others
and which ones they are they are those
for which there's the good label is
already highly ranked and the higher
rank it is the more emphasis I'm going
to put in it and that's what this thing
says ok I'm going to give you some
numbers and you're going to be horrified
but you'll see afterwards some examples
so numbers are very low here
on the image net so there was about
fifteen or sixteen thousand labels these
are different methods I'm not going to
explain all of them but basically this
would be like the state-of-the-art
techniques that people would use to
classify so not to extract features
everybody uses the same feature X Factor
just how you classify the wild vs. rest
would be the support vector machine
approach precision at one corresponds to
classification error how many times the
first ranked image label I have for an
image was the correct one so we get
about three percent on the best
competitor three percent of the time I
rang the correct label at the first
position so ninety-seven percent of the
time I missed okay in case it's not
clear so wasabi is a bit better still
ninety-six percent now the good point
with these models that you actually can
create more of them so if you have more
than one machine you can train more than
one because it's a stochastic approach
when you train the second model it's
actually different model and you can
merge them and you get better and I and
we did that and if you do it properly
you get up to ten person on the prophecy
so that's start to be better so I need
to explain what is precision at ten so
precision at ten if if you were to be
given ten labels so the top ten of your
ranking per image how many of them would
be good / 10 it's divided by 10 so that
if there's more than one image you never
go up to more than one hundred percent
and so the numbers are already quite bad
so that the end change it looks better
so that would mean that thirty percent
of the images I have I get one good
label out of 10 that's roughly what that
means so that one's on fifteen thousand
on the hundred thousand label everything
is even worse so any competitor is less
than one percent we get up to three per
person so it's much better still looks
very bad let's look first
numbers so numbers only it looked bad
but now we put these two numbers in the
in the curve and it looks much better so
the problem is still very difficult but
we've upped a bit the quality it's their
value it looks very low so I need to
explain now what it actually does in
reality these are numbers let's look at
examples here are some examples so one
thing that very interesting in this was
a be embedding approach is that after
I've trained I can just look at the
space so this 100 dimensional space and
I can take one label say Barack Obama
and I can look around what are the other
labels out of the hundred thousand
labels I've trained and here is what I
find so that's interesting what I found
was spelling mistakes other ways to talk
about the same person as we speak
spelling mistakes here David Beckham is
uses football player so you see other
ways to write David Beckham you see
other football player now you see that
we actually captured some semantic
because that's very in terms of number
of character they have in common it's
very small but it's actually the same
kind of people that you would see why
because both of them were in images on a
soccer field and they look very similar
same far so Santa you have other ways to
say it in different languages this is in
French dolphin you have different
languages different type of of animals
that are actually often seen in the same
kind of conditions same for original
ipod you have various type of ipod to
see the kind of planes you'll see so now
you have to understand that I've never
ever said that an f-18 is a similar to a
fighter the only thing I had were images
and labels that's it and this was
inferred from that setting only I have
to be fascinated only five minutes so
very quickly if we talk about the actual
task this was an image here is what we
get not too bad not too bad I have to go
quick this is Barack Obama
is Eiffel Tower something interesting
you see things like Tokyo Tower it's
actually very similar I've seen you if
you've ever been there or last vegas
trip they actually have an eiffel tower
there who knew okay let me switch for
the last five minute to the second more
evolved topic I have these 100,000
labels now there are in this space
that's already great can I do better
well I think one interesting thing we
could do is organize this label into
some structure so what is the best
tracker we can think of a tree of course
why a tree because if I had a tree in
the label space whenever I'm given an
image instead of having to compute the
score of every of the 100,000 labels i
have in my hand i could go down the tree
and say oh is it on the left on the
middle on the right okay it's here I
would have actually to compute only log
of the number of labels scores to decide
that this image is about a car or not so
it would be very efficient now such a 3
doesn't exist in some cases like forward
net it is given where net is the tree
that's fine but one that doesn't really
correspond to real life and it doesn't
generalize to anything else than Burnett
what happens when you like us we just
have this collection a huge collection
of labels that are completely
unstructured you want to learn that tree
how can you do that so here is a very
simple idea there's no way you are going
to be able to separate two things that
are too similar to each other if you
don't have the features to do so so if i
don't have the features to distinguish
between toyota and honda well let's not
even try to but I don't know that so I'm
going to try this for an EV image of the
Toyota I'm going to try to to see how
good I'm faring at asking whether it's a
honda and and for this i call that the
confusion matrix i'm going to look at
how many times I confused Obama with
dolphin and number of times i confuse a
dolphin with whale and things like that
out of my current model I'm going I'm
going to make out of my training set our
serial validation set a lot of mistakes
it turns out that the mistakes i'm going
to do most are things that are very
me out to each other that makes sense
otherwise I wouldn't make these mistakes
with my training error step set so I'm
going to use that hint I'm going to try
to put two separate things that are very
difficult to separate I'm going to put
them into different bucket and there are
algorithms to do so there's something
called spectral clustering that does it
for you so you can take all your label
in the current model separate into some
bucket then take those of a given label
separate into some bucket and do that
recursively a bunch if you have enough
fluid levels and you're done examples of
what happens okay these are for instance
labels that were taken from the same
bucket so you see things like sharks sea
otter things that seem to happen inside
see here you see things that seem to be
related to phones here you see things
that are looking like trucks etc and
these clustering have been done
automatically using the model i just
showed now the good thing about that is
that you doing so you have actually not
lost any precision you still get about
the same performance but now you have
this nice tree which could be using
various conditions and you're faster at
the actually label in one image so
that's going to conclude my presentation
so image annotation as such i think is
an interesting tax there's a lot of
things we can do inside Google if we had
a good image generator and there's a lot
of things outside Google you guys could
do as well like just annotate your own
images but of course as the number of
labels grows as as the task becomes
interesting it becomes much more
difficult so we need to to join
information because some labels are more
difficult to separate than others but it
turns out that when the number of labels
grows actually the semantic space gets
more fuzzy things are very similar and
if you make a mistake by saying it's a
shark instead of it that kind of onion
sea animal that it's not as bad as if
you were saying it too it's a car so so
we can use that information and we can
sure try to learn some semantic roundup
and I think that was the point of the
paper one thing way I'd like to be able
is to scale even better but we product
we need more power
ilysm and it's not that simple in that
case thank you questions is very nice
talk harmonica shava Draper laboratory
my question is why do you limit yourself
to the same set of features no matter
what the search happens to be I mean
faces will have the best kind of
features vehicles will have strong
geometries landscapes will have
different kind of textures shouldn't you
be empowering your search by using the
right kind of features for the right
kind of search item well in general
you're right of course we should use
more specific features within but in the
butt in that case it's actually more
difficult first when you get the image
you don't know whether it's a car or a
face so you don't have to extract
features for all of these type of labels
that will make things a bit slower but
its feasible you'll have to just to
handle that maybe you talked about three
types of features but I have a hundred
thousand labels so how many types of
feature do I need for Reese all via
large task there's nothing that prevents
you to actually have an an Obama
embedding for for using face features
and an Obama bidding using car features
and let the best one wins that's all so
there's nothing that prevents do that I
haven't tried thank you this is adam
fantasio from jackson university i
noticed that some of the answers that
came up for example with President Obama
weren't President Obama there was other
singers there's jay-z in there yeah you
have a feeling for if that is the search
engine not working or if they're
mislabeled images that actually would be
a print a picture of President Obama
that was mislabeled by somebody in the
in the ground truth system and how do
you make that determination well so I
haven't actually looked at that but both
can be true of course there's a my
training set is highly noisy because of
the way I got it as I told you it's just
people clicking on images if they do
that often so if men
people clicked on jay-z on this image
when they search for jay-z then I'm
going to assume that this this person is
JC because I don't know anything else
and I have no way to actually really
look into all image like labels there's
too many of them and that's only a tiny
portion of what i actually have as i
want to remind you so it's too difficult
to do so so I'm just going to rely on
that and the but one important thing is
that actually I'm sure I don't have good
features like this gentleman was saying
I don't have very good face features so
I'm not that good at separating between
people so even though I had even if I
had a perfect training set I would still
make that kind of mistake so the two
things are not true hot lips and Cornell
have a question about what happens when
you have a label that matches two
different areas of the space how would
you algorithm handle them okay so let me
give you an example like tiger no sorry
jaguar actually tango would work as well
but yeah jaguar is a car it's also this
animal it also happens to be a version
of OS X Macintosh so it has very
different meanings what happens in that
case so two things happen first if you
only have one so in the setting i showed
it will work we were actually very
surprised the reason why it will still
work is that 100 dimension is actually
large enough to have one of the view
being the animal version and the other
view being the car version but there are
other ways to do so and we tried
actually we experimented using this
trick instead of attributing for each
label one point in the space we're going
to attribute three points in the space
okay it's going to be three times slower
to train fine we can handle that and at
the beginning they are going to be
random now a training time whenever I
get an image of a Jaguar I don't know
which Jaguar it is of course I'm just
going to pick the nearest one randomly
that's all and I'm going to train using
the nearest one it turns out that when
you do that each of the three points
will specialize to three different tie
jaguar that that actually works so I
haven't presented any original done that
but it does solve that problem the
problem is that when you show numbers to
like these numbers to people the number
of such images that have actually
multiple meanings is tiny compared to
the millions of images that I only have
one clear meaning so numbers don't
change but if you look at some examples
it does work so that's how I we love it
we have another question if there's no
what in machine learning the biggest
challenge is finding the features if you
have the right features then you can
then there's lots of algorithms how do
you so in the in terms of you know the
future hug is there a way you can
automate the feature detection in the
first place it's a hard problem the
machine learning techniques to do so
like boosting is a technique that does
so so you propose a very large set of
features it has to be very large and
boosting is this technique that would
that can be used to to evaluate each of
these features and and take those that
are most important those that better
solve the problem at hand and you can do
that repetitively and you get the top k
features as much as you can so this is
one technique the others but in general
it's an np-complete problem to find the
right figures and it's also an art to
craft them before and it's not assault
by far not problem yeah Brent stuck your
University of Louisville I'm just
curious about the flip problem what
happens if you have Barack Obama at the
Eiffel Tower how does how do you then
label that that image I don't get it you
have Barack Obama and I felt I was out
to label the same picture Oh in the same
picture oh sure so that that picture
ends and you're assuming that it for the
other training center is a test as it
tested but in general yeah so in general
both feet so you remember that the type
of feature we have is a bag of word type
of feature so most probably for that
image that have Barack Obama and like
actually this one I think it has Barrack
Obama and this thing in the in the back
that you may have seen once in a while
the features that we're going to extract
will extract things everywhere in the
image some of the features will be
mostly representing Barack Obama and
others will be mostly representing the
the Eiffel Tower so in your case so most
probably what would happen in that case
is that you're going to see in the top
return labels both of them and with some
probability I'm probably hear it would
be mostly Barack Obama and and some
others that Willie represents a Eiffel
Tower because it's a bag of all its will
respond to both of them as much as it
can anyone hear from Purdue so you
shared some of the accuracies returning
your research made a point that they
were very low but can you give us some
idea of where these Precision's need to
be to be market useful is it now I need
to tell you about another experiment
I've done which is not shown here we
actually asked humans after we saw that
and we were depressed we asked real
humans the following we did the
following experiment we showed the human
that image say and we showed them the
top I forgot see the top five labels
that we were junk and for each of them
we asked humans is it a correct label or
not and the result of this experiment
was that something like between twenty
and thirty percent of the labels that we
return were actually correct so the
human perception was much higher than
the numbers the exact number of course
it's a small-scale experiment it was not
done on 20 million images but on a few
thousand images so we have to be careful
but it's a hint that as I said people
are more happy than the numbers shows
so have you tried to use the auto
correct thing from from search spelling
correction yeah so I haven't yeah I've
been asked already that I haven't there
are reasons because sometimes it's it's
delicate I'll give you an example things
like dolphin and dolphins seem to be
very similar in with it's a pity that we
have to give keep that into two separate
labels but actually in the US dolphins
also means a sports team but not
elsewhere in the world so these words we
don't often sometimes we don't really
know if that was the real intent or if
it was just would be easier if it was
just the canonical version of the word
so because of that we've decided not to
treat them in a particular way and that
might be further research to try to do
so but it's not that simple to do it
Chad mountain from Mattel very nice talk
so so you do your training initially and
you're starting now to present you know
these possible images for someone typing
a label how are you now looking at an
iterative approach where you can name
them refine what you've done by looking
at the next round and people picking
from the subset that you initially
present yeah if I if I had a bigger
second step training said that so I
would need to ask humans so I could
design a word game like the Lewis
vanaheim type of game where I would show
if you know what this is this is a game
where you you show an image say on the
web and and two people at the same time
try to annotate them and often when they
agree it's a good label so it's a it's a
game to trick people to annotate images
for free so so you could use that kind
of trick to do so it's nice and you
could change the settings such that the
only pic into the set that I've proposed
and it many people use that set then it
makes sense it's still going to be very
slow to get to enough training images to
to to have maybe enough to train but it
it's
worth considering right thank you I
Oliver Williams microsoft research so
you talked about having the two
functions that go into embedding space
one yeah maps images one that maps
labels could you say a little bit more
about what form those functions yeah so
later I actually had any question that
explained that and I decided to remove
it so that it would be simpler but it we
use the simplest version of that
technique which is a linear embedding so
suppose you have an image where you
extracted and it suppose this is your
person that has vector X and we're going
to have a matrix W which is and so x
times WRX transpose time w will give you
another vector which is the
representation of that image in this
case so it's just a linear mapping from
the input features the feature that you
got from the image to the embedding
space that's one part the second part is
the label representation and that's the
direct representation so we have a
single matrix where each line is the
embedding so the position in the space
of each label and and the two matrices
corresponding to size such that they all
have the show in that case 100 dimension
by use you could try much more complex
version but this was the simplest the
linear mapping basically it still is not
a convex problem because that tool in
our mappings and there's the there's
more than one solution that would solve
the obtained oh we don't know how to
solve it at one quick follow-up so if
there is a hot topic in the news are you
finding that sometimes that's a
distraction people may actually jump
over to a different picture because it's
a hot you know issue ah it can you and
can you actually use google knows what's
hot in the news it actually use the time
information and what's it's a good point
i haven't in other places at Google we
often see that offers when something
happens all the queries go to that
direction and then we have to react
quickly but how that experiment I
haven't taken that into account
so it might be that why i collected the
data is something like that happened and
i even don't know about it but that's a
good point yes so is it Angela from
Purdue then for these pictures so have
you ever considered whether there is a
many more other representation of the
original image so say or you put down
here 100 dimension so these many images
what would it be the most compact
representation have you so we played
with that of course we if that's what
you mean we get this is a hyper
parameter that we tune we tried the 20
50 100 200 400 and the more the better
but the longer to train so actually what
we found was the best was to try
multiple models of say fifty are 100 and
then to concatenate them at the end and
that's what I presented as the best
results work three models of 100 I think
concatenated so it was faster and better
than a single 300 dimensional vector so
so yeah have you see erratically started
as a lower bump in other also tie it
together with the error so given this
accuracy requirement what would it be
the minimum order to represent the
original system I don't think no I
haven't I we don't know how to do so
because these this number and the
performance really depends on the on the
type of data I didn't say anything about
that but I used that similar type of
model with different type of objects I
tried with music and you'll hurt hear
about that later this afternoon I try I
tried with videos I tried with text you
can put anything here and for each of
them you get very different hyper
parameter setting and performance of
course it really depends on the
complexity of the task it's more like a
general to to put two things that are
similar they have the same semantics but
they have a different way to be
expressed like here its images and and
label that's very generic and and it's
such a generate task that there's no
single solution the number here really
depends on how difficult the task is
okay thank you hi I'm Steve Johnson from
SAS see um I was wondering with all
these labels associated with every image
that does that cause you to have a lot
of metadata you have to carry around
with the different images and also if
because there's so much redundancy in
the lists of labels that they're
compressible okay first the median
number of label / image is one so most
image only have one label and then like
everything on the web there's this
exponential curve so there's a few image
that have 20 labels but most of them
have only one and then two three but
it's going to be very rare it really
depends on indeed on the popularity so
things like Britney Spears we probably
have multiple labels which usually will
correspond to mutable ways people like
to call her our stellar so but there's
not much to carry and as up to compress
labels that's a good point I just don't
know how to do so because I don't know
when to labels are actually the same
thing for everyone in the world gym
while and university of southern
california do you take the the intent of
the photographer into account at all i
could a senator buys phenomena where the
interesting thing will be in the center
of the photo so in that case i'm obama
will let the obama vs the capital
building because obama's obviously this
so you're right at the tog refer and
there's a so this i put that into the
realm of feature extraction there's a
lot of research doing in that field and
many people one of the dominant approach
in the last 23 years have been this
pyramid technique where you actually
look at broad images and then part of
the images and in such a way that it's
going to concentrate on roughly the
center of the image and things that are
around so you can craft your feature
such that you put more attention in the
center or wherever you want actually and
that works better so I haven't done that
in that experiment I think but
it's it's in the feature extraction part
so it's not in the modeling part so
these two IDs could be joined without
any problem all right let's thank the
speaker once again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>