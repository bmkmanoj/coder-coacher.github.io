<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Keynote | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Keynote - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Keynote</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xJtNp7fwn-U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">And with that, I'm going to actually hand
over the mic -- well, the clicker really -- to
Juergen Allgayer of YouTube to talk about
testing on the tube.
[ Applause ]
&amp;gt;&amp;gt;Juergen Allgayer: Whoever designed this
needs to hand out -- boy, there's an echo,
too.
Not only do I need sunglasses; I need to get
used to my new voice here.
Good morning, everyone.
Juergen Allgayer.
That's me.
I run the test group in YouTube.
And over the next -- what's left?
-- 50 minutes, I'm going to give you an overview
of what challenges we've tackled over the
last years and our success story, at least
a few of them.
I joined Google eight years ago with a job
to build up the engineering productivity function
in Europe.
Probably most of you have heard about engineering
productivity in Google.
It's an umbrella function for lots of disciplines
all around testing, automation, in general,
speeding things up, from speed up the compilers
all the way up to building tools to help find
maybe those bugs that take two weeks to find
manually.
When I heard about this idea of building an
organization really only focused on that,
I was very intrigued, applied for the job,
got the job, moved to Zurich in Switzerland.
And for the next three years, started to build
up this group.
And then I got a repeat offer to do the same
thing for YouTube about three, three and a
half years ago.
And, again, it was pretty interesting to see,
okay, for coming from the world of trying
to do this operation across all of the different
product groups within Google that happen to
be in Europe, in one of the Europe offices,
kind of pivot around to say, okay, wherever
a YouTube team works, which is Zurich, here,
San Bruno in California, Mountain View, of
course, the home base, L.A., Paris, wherever
we have people in YouTube, we build -- I help
build up this organization that hopefully
helps us to get YouTube and the operations
behind it clicking on all cylinders.
So what I'm going to do in the next time here,
in the next hour, is to lead you through a
couple of those challenges that we were facing.
But let me first give you a bit of an introduction
into what this system under test that we're
talking about actually looks like.
Yes, we're going to talk about YouTube and
pretty much only about YouTube.
So I will not mention a lot of other of the
product areas and tools built there.
I'll just give you the story of the smaller
scope.
As it's not -- not really surprising, being
a video service, a lot of the -- of the stack
is built up with proper video services, how
to upload, how to transform, how to get all
the different formats in, how to stream out,
how to distribute over the data centers, et
cetera, et cetera.
On top of that, we have a set of very unique
features that differentiate YouTube from the
other services that are out there.
When I joined, we had our main site, YouTube.com.
And we were just starting to build the main
YouTube app for Android and iPhone.
But over time, in the last three years, this
has changed as well.
There's many more apps.
We call them verticals, which we will talk
about a bit more.
And, of course, we have all the platform metrics.
So for the main site, all the browser combinations
you are all familiar with.
And then with the -- with the apps, then we
got a lot of interesting platforms that are
not in the typical mix.
So we -- YouTube runs on TVs, on set-top boxes,
on PlayStations; right?
So, unfortunately, in some of those, we are
not as far along in automation terms.
And I really would want to reach out to you.
Anyone familiar with that type of problems
and working on that, please reach out to me
after this.
I'm really interested to learn about how to
automate on a TV, for example.
One step deeper into the details, as I said,
video service, so we are concerned with getting
your videos up, be that from the desktop or
from mobile.
We started streaming live when -- from game
stations, where the gamers can stream up live.
We have live streaming, you might remember
the big Olympics streaming through YouTube
a few years ago.
So we get the stuff in, lots and lots of pixels
and bytes.
We start processing.
The processing typically happens within three
minutes.
And in that time, we get the formats converted.
We are still in the mode of preconverting,
so we convert an upload so that when you request
a video, you actually -- we are actually ready
to serve out the format that you need.
We store it, obviously, and distribute it
through the centers.
Later on, the streaming component helps to
get all of these previously generated formats
out in the various platforms and device formats.
You probably heard about our adaptive streaming,
where depending on the network speed and the
quality of the network you're on, you might
change the resolution and the quality of the
stream that comes in so that you on the device
get the best possible experience without seeing
the spinner too much, hopefully.
So around all this, on top of that, we have
some very special things that not everyone
knows about or recognizes right away when
you hear &quot;YouTube.&quot;
The biggest unique differentiator between
YouTube as a video platform and most of the
others is that we have this feature called
ContentID, a way to allow creators to protect
their content on YouTube by using or utilizing
the information about the copyrights they
already have; right?
So you upload your video.
You do something we call claim.
So you call out that this is yours and you
don't want to share this with anyone and that
any usage rights you should be in control
of, et cetera, et cetera.
Interesting part of this is that these copyright
rights are handed out by territories, so meaning
if you own a video that has four or five copyrights
for lyrics and live performance and digital
streaming, et cetera, et cetera, if you have
those, say, in the U.S., it's not necessarily
true that you also have those in other countries.
If you travel a lot and you use YouTube while
you're traveling, you may have seen, &quot;This
video not available&quot; in whatever country you
happen to be in, although you just yesterday
watched it before you entered the plane; right?
And this is why.
So the owner of the video has managed to get
the copyright in the U.S., has not managed
to get it somewhere else.
But overall, the content is protected, so
when we look at this and try to stream this,
we say, okay, until this is resolved, sorry,
you can't -- we can't help you on.
So that is, I think, the biggest differentiator
to others and brings a lot of people that
create content, like musicians, artists, to
stick with our platform.
Next to that, we have social components.
You can comment on the videos.
You can interact with your fans, for example,
if you're a musician.
And it's a pretty high-traffic social component
in that.
You have personalization.
You may recognize that you get recommended
videos.
If you watch a certain set of videos, you
get others that we think you might be interested
in.
That is based on personalization engine that
learns from your behavior.
We have one of the larger search components
that are out there.
When I checked this beginning of the year,
we were number two.
So we still are probably pretty high up there.
The search is, obviously, optimized for video
media.
So if you put the same search into google.com
and do YouTube.com, you will get different
results, 'cause on our side, you get videos
and not news reports and other things; right?
I mentioned the platforms we are on.
It's pretty much anywhere you can think of,
with some trouble spots like I mentioned the
TVs and set-top boxes.
A variety of browsers, a variety of versions
of Android.
You might have heard that we have more than
one out there.
A bit fewer on iOS, but still a variety there
as well.
So we have the usual matrix of testing and
compatibility.
There's also an aspect -- we move on to, really,
the user interface, the client side, there
is a -- the challenge to keep in sync those
clients with all the changes that happen on
the mid-tier that are pushed on a pretty frequent
basis, et cetera.
We started out with just browser, as I mentioned.
And by now, we not only have what was on the
browser before on the mobile platforms in
our YouTube app, but we also diversified and
have now more apps next to each other; right?
We call them the verticals.
One vertical that was pretty popular through
this year was music.
We launched a Music app, where you get an
optimized access through the interface, optimized
access to the subset of the corpus that is
music-related.
We have a special app for kids that has a
very unique UI.
If you ever looked at this, it's very different,
obviously targeted for kids, for young people.
A lot of parental control in that.
A lot of restrictions.
A lot of scrutiny.
Just recently, we launched our gaming app
that, again, has -- it's the same idea; right?
Just provide a very unique access to a subset
of the corpus.
And the gamers seem to love it.
So talking a bit about what this engine actually
does, what this machine actually does, if
you have -- right now we're looking about
300 hours of uploads per minute.
So all of these now in a huge amount coming
from mobile, all of these videos come in in
about a dozen formats.
We transpose them, we store them, et cetera.
We have acquired quite a bit of corpus by
now.
It's in the billions of videos.
And, of course, there is this -- if you say,
you know -- talk of the long tail.
If you say users over how many videos they
actually upload, right, there's only a few
that have thousands of them.
Then there's lots and lots who have one or
two or three.
Some of them uploaded, never watched.
But, still, they're in the corpus.
We store them, we replicate them, we move
them.
Lots of fun.
Streaming.
You have heard maybe about some of the highlights.
We had the Olympics a few years ago.
We had the space jump.
That was our peak outbound stream of several
terabytes per second.
So we have lots of users that use -- that,
you know, consume the media.
We bridged the 50% mark or we stepped over
the 50% mark of mobile usage, something I
personally couldn't imagine to happen.
But we have half of the traffic now coming
from mobile devices; right?
So half of all videos are watched and consumed
on -- not on the desktop.
What also is interesting is that kind of a
little subsystem that's humming out there
is that as you go to the site and you see
the videos, you always get these little thumbnails
next to the videos; right?
Which is basically an image server that serves
about 2 million images per second.
Just noteworthy, I thought.
I talked a bit about the content system.
So we have lots of people who use that.
We call them our partners.
More than 8,000 individuals or groups that
make use of that, make quite a bit of money
with that.
And we paid out -- this is the number of last
year.
We paid out over a billion dollars to those
owners and rightsholders.
So this is the system I'm living in.
And what we are going to talk about a bit
in the next half hour probably is some of
the things that we -- that I ran in the last
three years and that we tackled and actually
solved.
Or, let's say, our -- some of them we are
still in the phase of solving them, in the
process of solving them.
I don't think any of these is not familiar
-- you're not familiar with.
You've all heard those.
Everyone has their own solutions to this.
I hope this will be some insight in this.
The first topic is what do you do in an area
where your launches are just not clicking
along the clock as you want them to.
Right?
Some take longer.
Some go okay.
But you can't really for sure predict, okay,
tomorrow or in three days or whatever your
schedule is that you will actually hit the
clock and your launch is ready.
The second is a bit about our mobile device
landscape.
I mentioned that already.
So a bit more detail there.
And the third one is not a technical or tooling
question, although we also try to solve this
in a tool-based approach.
How do you change a team's development culture
from very fast, almost startup-like, into
a more mature, process-oriented, quality-oriented
team, without losing the speed and iteration
speed that you have in the startup phase.
All right.
Up first.
So here we were a few years ago, having a
very simple idea, or at least an idea that
sounded pretty simple on paper.
After we upload and -- the initial store of
the video that you send us, we create lots
and lots of formats, and then we store it.
And I already mentioned, some of them are
never watched.
Right?
So pretty straightforward idea is to say,
well, why don't we transform them when we
stream them?
And we know, actually, at least someone, there's
at least one request for it; right?
And then we start transforming.
And then we start storing, and you go on from
there.
You could save probably lots of storage space.
We could probably save lots of processing
time during upload, so your video might be
ready earlier.
Sounded like there is not that many negative
points to this, negative things to this.
So we came up with an idea that we call, on-the-fly
transcoding, which means exactly that.
On request, you start, and then you stream
out.
Lots of changes, some really video-specific
technical challenges.
But what I want to talk about is this.
Don't worry.
You don't -- There's no test on this.
Just trying to give you an idea of a system
that has lots of components, some dependencies
in between them, nothing new.
What made it hard was, these components were
distributed over a set of offices.
And you see these little lines showing through
the boxes.
So there was -- there were dependencies between
components owned in different sites.
So communication was a problem.
Complete independence of ownership of the
component was a bit of a problem.
That was one dimension.
The second dimension was, when we looked at
how these components are actually built and
made ready to launch, so how the corresponding
binaries were created, we found a picture
that, again, had, okay, some locally, at least,
contained releases, but there's more than
one, which means, again, now you see the lines
between the release binaries.
We have dependencies on not only content,
but now also on schedule.
Right?
So when our front-end wants to push and the
mid-tier, the storage, say, is not quite ready,
then what do we do?
Right?
No surprise in what that resulted in; right?
Everyone has seen these pictures.
We start coding.
We get a release finally ready and put it
in an area where others can access it; right?
So the other guy comes in, puts in his code,
starts to bring it together.
And, oh, didn't think of that.
Okay.
Have to go back.
While I'm going back and do some fixes, the
third component comes in; right?
So we're getting through the typical waterfall
model of -- of building large systems; right?
And I think there is one more explosion there.
Yeah.
[ Laughter ]
This is when we wanted to launch, or a few
days after; right?
So everyone has seen these pictures; right?
The problem was, okay, how do we do this?
Right?
We have the need, obviously, for something
that can be used in all the environments that
we use in development.
So that is my own workstation, kind of my
local development environment that is the
-- any staging testing environment that we
build up, where others are already participating
where I build up.
And, obviously, finally, it has to work in
real life, what we call Borg out there.
We wanted to make sure that each individual
environment is hermetic and has no dependencies
on others.
So that was the biggest problem.
We wanted to be able to build it from head,
so no delays.
Whatever current code is should be moved forward.
We wanted zero run-time dependencies.
We also wanted to make sure to take care of
one area of -- One source of problems we ran
into quite a bit is when we moved from our
staging areas into the production area.
There was a short, small configuration file
that described exactly how the different servers
and the different CPUs were configured and
what priorities they have, et cetera.
For live production; right?
And in way too many cases, we got into a problem
where everything worked fine on staging; right?
But, essentially, no one believed that we
are really done until we observe the same
behavior really in live, because there were
some I'm not quite sure how we map this.
So when we started out, we thought, let's
try and get rid of that problem, get rid of
that insecurity.
Let's -- let's see if we can build a system
that is able to look exactly the same in our
test environment, at least in the larger ones,
and then when we swap over, we do not touch
the configuration.
One second.
And, of course, you know, coming from where
we're coming from, we wanted this to be test
oriented, right?
So we wanted to be consistently designed for
testability from the get-go.
After almost two years where we created the
system, what we have right now really is a
system where we're all these various boxes
that I had on the initial pictures, that's
about 200 different services owned by different
groups.
Some of them in YouTube.
Some of them in Google that we make use of.
We have one environment where we actually
can develop and test all of these components
together.
A nice side effect was that the engineers
were -- are now so involved in working on
this system that there is really not that
big a difference anymore between developing
and testing, right?
And it was almost a side effect.
We said, yes, we wanted the system to be -- to
be targeted for testability, wanted to be
easy to be tested on.
But the way it looks right now is working
in this environment is -- equals to testing,
which is great.
Also, there is no separate integration phase
anymore.
As soon as you commit your code, it's deployed
in this infrastructure and you live in that
big system.
So we save the whole stack.
And we actually hit the no-surprise objective
that we set ourselves to say when we switch
over we don't have to care.
We can use exactly the configuration from
the live system.
If you remember the little explosion chart
I had before, right, so we had a honestly
unpredictable release calendar.
Because of so many dependencies, we were basically
continuously pushing.
Whenever something was ready, we started -- whenever
the push was ready, we started over the next
one and worked our way through.
And we went up to sometimes three, four, sometimes
five weeks in really bad cases.
And what we got today is a system where we
have nightly builds that are ready for consumption,
if we choose to do so.
And the main reason for this is really point
two and three.
There is almost no difference between development
environment and test environment.
And whenever you are done developing, you
also know that it's actually working and we
can push this out.
So we are on a mostly daily -- we don't hit
every day; but most of the days we got a successful
build, not that it's pushed necessarily into
life every day, but it is something ready
which makes -- has the usual positive impact,
consequences, right, small deltas in code
changes from day to day.
If something comes up, there is a small scope
to look for bugs, et cetera, all the well-known
things.
So that was pretty -- we were pretty happy
about that and still working now to get this
completely 100% for all the 200-plus services.
A few stragglers are out there, take a bit
longer.
But we plan to have this really ready and
available.
So that was a pretty happy story, and it took
us about two years to get through this, right,
to build this whole infrastructure into which
to dump these components.
But well worth it because now, you know, it's
unbelievable the amount of time everyone saves
by not going to wait for something or to look
in a pile of code that is old for changes
that may or may not impact the current thing.
Sorry.
On to the next topic, mobile.
So if you recall, we have this, quote-unquote,
matrix that I have to care about on the client
tier.
While the mid-tier and the back-ends are pretty
well moving on a daily click with new pushes
for the live site, if you look at the state
of apps that we now have, it's a slightly
different story.
One, we have to make sure that any of the
mid-tier and back-end changes keep alive all
the variants we have out there.
So on the browser side, that's easy because
we push that as a whole stack and they're
in sync by definition.
But for Android apps, iPhone apps, for the
TV apps where we have no control of when they
are actually shipped out for Xbox or PS2,
they have really long development cycles -- or
launch cycles, rather, we have to make sure
that whatever we shipped a year and a half
ago is still working with tomorrow's update
of the live site.
That brings with it a whole new dimension
of test matrix, if you want.
We call it compatibility of what's out there
to what will be available tomorrow in the
servers.
The way we tackled that was like this.
So here's a stack of various development and/or
test environments including the production
system.
Most of them we have in multiple variants.
And the challenge was how to get the apps
platforms connected to these integration environments
or test environments on a not manual and not
necessarily hardware-device base ideally.
You don't want just -- we have maybe 200 developers
in that group running around with ten devices
each to figure out what works and what not.
So what we did, usual approach, two parts,
right?
We have a hardware lab that has a few hundred
devices that are centrally managed, created,
kept up and running that you can as a developer
can log into and exercise your test or see
you change.
And they can connect to one or more of those
development environments or test environments.
Next to that -- so these are as close as taking
a tablet and start tapping, right?
Next to that we have the emulator set for
-- at least for those where we have emulators,
so for Android, iOS.
We have machines that do exactly the same.
That's mainly the basis for all of the automated
tests.
And then what we are still working on, made
a bit of progress but have quite a big spot
on the map to still cover, is what I here
call the dedicated hardware for, call them,
non-standard devices, right?
So TVs, for example, is hard because I haven't
found a good answer to automate on a TV for
some Linux derivative that is reproducible.
Sure, you could automate it, right?
It's a huge effort.
But then what do you do with the next TV?
And the TVs seem to pop up every holiday season.
So open question.
We have one for the subset of all the devices
where we have to run that are -- that have
a video-out card, where we intercept that
and do comparison -- golden snapshot comparison.
That works pretty well.
Brought us to the point where, for example,
for HTML5 browsers on TVs we can for every
of the daily builds we run a smoke test that
does this, right?
And although Xbox is kind of the same story
underneath, Xbox has an extremely long launch
cycle.
So we could -- we have said before, well,
we care about all of this when it comes closer
to the holidays, right, when there's an update.
But in January, February, March no one really
cared about this yet.
Where we are now is that with everyone cycle
of the back-end, we create the corresponding
build and we have through these dedicated
hardware setups some way to smoke test those
and say at least everything is still running
with the version we shipped, which is nice.
But, as I said, this is a topic I'm really
interested in talking to some of you hopefully.
All right.
And then lastly -- so, no, this is a summary
of where we got here.
All platforms including those weird ones,
we have found a way to validate at least every
daily push.
So at least know we are not breaking.
Have automated smoke tests from those.
And because we now have the automation platforms,
the amount of testing that was manual was
reduced dramatically.
And I'm not an advocate for not having -- too
many &quot;nots.&quot;
I'm an advocate for having manual tests.
[ Laughter ]
Too many negations.
I'm an advocate for having manual testing.
It should be exploratory testing, not scripted
regression tests.
So I don't want to spend every day when I
create a new driver X numbers of hours to
just make sure everything works, right?
I want to spend that effort and energy on
the new features that we developed, make sure
they're okay.
And with the platforms we have now, we were
able -- and we got a bit more of that later.
We were able to cut the amount of manual tests
needed on those devices to 50% while -- compared
to a year ago while we were adding three different
apps.
So it was a pretty dramatic reduction.
The last topic I want to talk a bit about
is what I call the cultural change.
So with all this automation -- so step back.
Three, four years ago when we talked about
testing at YouTube, it was a huge amount of
manual work.
Right?
Even when we had only the live site -- the
site, the browser-based clients, it was a
huge amount of manual testing.
We did unit tests, yes.
We did some integration tests.
But most of the bug was end-to-end and most
of the end-to-end was done manually.
So one question is as you introduce these
platforms, you also want to introduce a culture
where everyone knows these platforms and knows
what to do of them.
And it's as easy as possible for someone to
pick those up and learn that so that every
developer actually has an easy way to get
to the platforms and do the job, the testing
job.
So as an organization, we decided, okay, quality
is a thing now.
We need to somehow get to a better level of
where we are.
We want to reduce manual; but we also at the
same time do not want to increase bugs obviously,
rather reduce them.
We looked at where we are.
And this was about 18 months ago or so when
we -- 18 months ago when we set out to do
this.
And we took a snapshot of where are we today,
number of P0, P1 bugs out in life.
How many do we find in the staging process?
How many do we find here?
The typical analysis, we go there.
We looked at it and came up with metrics of
what we actually want to look at on a consistent
basis and what SLAs are we setting ourselves.
What's our goal?
If we now have 20 P0s happening in a week,
is that bad?
Is that good?
What do you want to do about it?
Is that the new goal?
We started pretty simple.
Where we are, we cut everything in half within
two quarters and see where we come with this.
Have to have a goal.
So we set the SLAs.
The next problem then was: How do we talk
about these?
How does everyone know where we are and specifically
where the team is that I'm part of, right?
Am I doing okay?
Am I improving?
Am I slacking?
What?
So we wanted in order -- we thought in order
to get these goals right, we need a tool that
allows us to look at our metrics across the
board and be able to drill up -- to drill
down, go as big scope as you want or as defined
-- as granular as you want and say, Okay,
the organization itself is like this.
This team is doing here.
And find out who is doing well, where can
we learn, how can we transfer best practices.
We also thought about the different types
of what quality could mean, right?
There is obviously the product quality, right?
Does the product do what it wants?
Is it easy to navigate?
Is it responsive?
Is it intuitive?
If you click here, this will happen.
Especially if you want -- as we went out to
the platforms where the UI is so far away
from what we were used to in the browser world
-- if you look at a TV or the Kids app as
an extreme, you have very different UI methodologies.
And we wanted to have a statement on that
level, Are we okay?
Where are we not as good we think we should?
The second topic was operations.
How are we with development excellence, if
you want?
The first is product excellence.
The second one was developer excellence.
How many bugs do we have?
Where do we find them?
How is our coverage?
How many manual tests does it still need?
That was my favorite.
Do we still have manual stuff in regression
happening and where, and why is it not moving?
And the last test that was more specific for
the test organization to go after are we doing
the right things to help the larger organization
to do their job, right?
And quite obviously out of all this, we wanted
to have automation wherever possible.
It's pretty obvious when we talk about this,
everyone thinks of test automation.
But there's lots and lots of other areas where
automation helps.
So if you think about the push process itself,
how we release a product, we had lots of manual
steps in between that slowed things down,
natural sync points but single points of failure
if something didn't work out.
So across the board, look for opportunities
to automate and one by one take them down.
My personal favorite goal: No manual regression.
Not quite there yet, but we made some headways
there.
I already mentioned the goals -- I was a bit
of myself here.
So we picked where we were.
We cut it in half.
We said go.
And then we tracked.
And the tracking, I think, is more important
than the actual goal because you can always
reset the goal when you get closer and you
think it's still -- you should still improve,
right?
Then we mapped this back to -- you probably
heard about the OKRs in Google.
So the commitment of every individual to the
team, okay, I'm going to do this and we set
kind of both downstream and from bottom-up
objectives for the quarter.
These were kind of enforced objectives for
the team.
You have to have this state in the metrics
that we give you from the top, right?
And those were the typical, okay, no P0s in
production and kind of the obvious things.
But we built the tool in a way that the teams
themselves could come up and say, Well, I
have something that's really important to
me for my interpretation of what quality means
for my subset of the system that no one else
really cares about, right?
But for me it's important so we allowed this
to be incorporated.
And, finally, we had also to do some work
on who we had on board.
That's specifically for me.
I had a team mix three years ago that is dramatically
different from what we have now.
Now we have much more software engineer types
that can help build the tools and automation
and help understand the systems and less core
test engineers concerned about larger system
consistency tests plans.
We still have them.
We still need them.
But my problem is I didn't have the other
as well.
So we built that up, and now I have a mix
that is better equipped to tackle this.
The single -- the most used -- I can't remember
the American phrase.
The tool that was most useful in this that
we used to get to change in the larger organization
was this.
You don't have time for manual.
Because of that, you have to think about how
to automate and what to automate and let everyone
else know what you need to do to help you
get there.
So we said, Okay, three years ago, on domain
side even, we had every few weeks we had a
push.
And we said, okay, we go to daily builds -- sorry,
daily launches, daily pushes every day.
If you think from three weeks down to one
day, there's a lot of time you don't have
anymore to do all kinds of things or you have
to stop doing all kinds of things.
This was the single most important decision
we made from an organization to get to that.
This is a current snapshot of where we are.
You can see top left -- top left the little
downtrend.
That's the amount of manual regression tests
we still have in the system.
We are 34, -5%.
So a third of all tests -- we classified our
tests.
Our P0 tests, we called them, that have to
run on each release before we say you can
go, right?
And 2/3 of those are now automated.
And, as I said, keep in mind which platforms
we talk about.
In some I have no idea how to ever get really
down.
And we added a couple of interfaces and apps.
So I'm pretty proud of the take-down but obviously
not complete yet.
Unit tests, we kept pretty stable which is
also good because of, again, of these new
verticals that we launched, right?
The standard, the default discussion at every
point when decision is made that an additional
app will be launched is, &quot;Oh, we have to go
to market tomorrow.
We don't have time for anything.&quot;
And by experience everyone knows that, too,
the first thing that gets cut back is work
on automation, right?
I'll do this when we launch.
I don't know how the system looks like, how
the interface looks like.
I don't want to spend any cycles on this quite
yet.
I'll do this later.
So looking at a coverage curve that stayed
flat over those additional new verticals is,
I think, a pretty good result out of this.
And on the right side is our current snapshot
over the last -- how far back does this go?
-- about a year, I think.
Yeah, about a year back on our release plans.
So we set out that we want daily pushes, we
want daily launches.
And so this shows how often we hit -- so daily
in our world means four, actually.
We have a four-day week.
We didn't want to push on Fridays, because
we didn't want people to work on Saturday
and Sunday to fix the bugs that we introduced
Friday potentially.
So we said, okay, we want to go Monday through
Thursday.
So we have a target of four per week.
And here you see that we get three, three
and a half out of that most of the times.
There are some weeks that have holidays in
them where we didn't need four really.
And as I said before this, in my mind, this
was the best decision we ever made, to push
for this.
Because that squeezed everyone's allowance
for what can happen.
We have right now a honest test window on
the release level of three, four hours, where
testers actually go and manually do something.
And even that on the apps we have reduced
to exploratory.
So we are not going through disrupted tests,
but we spend the time to say, you know the
app, we know it should work.
Let me know whether we are good to go or something
is off.
What remains was the question of how to talk
about this; right?
How do you let the organization know how we're
doing and how do you find out where we're
not doing so well and we should refocus and
change things that are not working.
So this is an eye chart.
You're not supposed to read this.
Just give you an overview.
This is how the tracking site looks like.
This is an organizational chart of YouTube
as the top organization, and then you go through
the different we call them functional areas
and investment areas and teams.
So we picked three levels down, split the
world up into a tree of depth three, projected
it, and said, okay, so here's who we look
at.
And then we track, for everyone, a mix of
a fixed set of matrices that are predescribed
and you can't opt out of and an arbitrary
long list of matrices that are team-specific,
we use to say, okay, for the video delivery
team, they have a few matrices that the Android
app doesn't really care about.
Okay.
You put it in, you leave it out.
Fine.
But once you put it in, you will be tracked.
All right?
And then you can drill down, and you get a
slightly different view of the various aspects.
And I mentioned, we have -- I mentioned before,
we look at product, we look at release, we
look at test.
And further down -- this is the lowest level
-- my group actually could go and say, how
are we doing?
We had some production bugs in P1.
This is probably this tool we're talking about
that had a bug.
And you see kind of very easily where are
hot spots, where did we not meet the SLAs.
We still think in six-week sprints.
So this little bar chart you see up top gives
you the history of the last eight or nine
six-week sprints next to each other.
So you also get a trend, whether you're actually
improving or you're degrading, you keep your
level.
You see pretty quickly where you should be
concerned about and where you're doing fine.
And then the next step, obviously, after that
is to think about ongoing improvement of this.
But what we got out of this is now a vocabulary
to talk about quality in this organization
that everyone understands.
Everyone knows where to go and look.
All these matrices we defined, even if it's
in a team that, you know, is next to you,
you learn a bit about what they care about.
It is transparent.
Everyone can look at this.
There's a bit of public shaming going on,
obviously.
You don't want to be the one red spot in a
long list of green, all positive things.
And mapping this down to the teams allowed
also to have it very easy to know who to talk
to about a problem; right?
It's this team, and I don't care which component
in the code and blah, blah, blah.
You do this yourself; right?
This team shows up as red or as declining.
We need to talk.
We need to do something about it.
It's really cool.
And what's left here?
So we want to think about how do we -- if
we are at green and some of those have some
history where there are three, four, five
sprints on green, what does that mean?
Does that mean we're done?
Or does that mean we should look at the goal
we set and whether we can improve that?
Right?
There is an ongoing discussion, especially
with the aforementioned fast-to-market needs
of teams, which is a reality.
Look at our gaming app.
We wanted the gaming app out there as fast
as possible.
But the question is, what is &quot;as possible&quot;?
What does that really mean?
What are you willing to jeopardize in order
to get there?
And, obviously, in the way of getting out
there as fast as possible is all the process,
I would call this process work, around it
to actually keep everyone from running too
fast and introducing too many problems; right?
So that's an ongoing discussion that on the
plus side is actually on the surface.
Right?
It's not some hidden just one team has this
problem.
It's on the surface, because we probably see
a lot of red area in this -- of red color
in these areas, and we are forced to think
about what we do.
Another slightly hot topic in the existing
culture is how far you go with asking people
to do things a certain way.
Right?
We typically try to stay way back from that
statement; right?
You can choose your tools.
You can choose your -- your development process.
You can sprint or scrum or do whatever you
want, as long as you deliver; right?
But we are now at a point where we start seeing
teams who do really well and can see the -- the
positive trends and impact on that, and think
about whether we want to turn this around
and not say, okay, we can use this or this
or this tool, but too prescribe one tool set
across a larger part of the organization.
And if you're not in that, you need to have
really good reasons; right?
But we're not quite there yet.
It's on the table.
My biggest problem: We still have platforms
that are hard to automate that we haven't
cracked, that I'm just without a good solution.
We are almost on push on -- push every day,
every workday, or every push day.
So pretty consistent for probably 80% of the
codebase or the component set.
And we're now starting to think if and how
we push this forward; right?
So daily?
Should we have two a day?
Should we have four a day?
What's the right number?
Should we push every time code is ready?
What we call push on green?
Right, you submit your code.
It goes through all kinds of tests.
And at the end of that process, you're live;
right?
How far on that dimension do we want to push?
Not quite obvious.
Huh.
Team --
Let's talk about my other big problem of getting
the right people for this type of work, right,
which I found rather harder than I expected.
Hiring is a real struggle, both in terms of
numbers and in terms of the right skill level,
or -- level and skill set.
People who are really passionate of this topic
of not working on the front, you know, &quot;Look,
Mom, I built a new app,&quot; but more in the back
office.
This room is probably more -- it's much more
easier to convince people here to do this
type of work.
But if you go to the general SWE, software
developer population, they're hard to find.
Very valuable and hard to find.
Ah, now I remember what this other was.
Another organizational problem on the team
is, if you recall the -- I try to give a flavor
of how diverse and how broad the technologies
are, the set of technology is that is used
to build this YouTube offering; right?
And almost in every area of technology, there
is a specialty for how do you actually validate
this and make sure you have improved; right?
So you think about machine learning and model-based
testing on one -- as one example.
You go down and you have hard-core video imaging
processing things; right?
You go up to the apps, and you have UI, human-computer
interaction topics.
Right?
So it's a broad set.
It's a broad spectrum.
And we have not that many people.
So most of them are specialized in some of
those areas; right?
And that's what I meant here with the -- with
the team identity.
How do you make these hundred people that
do this feel as one team that solves this
problem for YouTube, versus, oh, I'm in the
search group.
Oh, and I'm in the personalization group.
And I do this other thing.
Right?
That is, you know, an ongoing struggle.
And then the hiring.
So thanks for your time.
I hope it was a bit interesting.
I tried to not get into too much of the, this
is the tool we use, and this is the language
we do it in, right, but more on a higher level
to see whether we can have some discussion.
I'm here for the next two days, very eager
to talk to you guys about how can we do this
better, what are your experiences.
So take it as one of those experience drops
in the back and see what it can do for you.
With that, thank you.
I'm not sure how we're doing on time.
Do we have time for questions?
&amp;gt;&amp;gt;Yvette Nameth: We are good for questions.
&amp;gt;&amp;gt;Juergen Allgayer: Oh, excellent.
Is -- will this show me the --
&amp;gt;&amp;gt;Yvette Nameth: In theory.
&amp;gt;&amp;gt;Juergen Allgayer: -- the online?
&amp;gt;&amp;gt;Yvette Nameth: Because it wasn't showing
up on my --
Can you click so it's showing the top tab,
please.
&amp;gt;&amp;gt;Juergen Allgayer: You moderate; right?
&amp;gt;&amp;gt;Yvette Nameth: Yeah.
&amp;gt;&amp;gt;Juergen Allgayer: There's a negative one.
What does that mean?
Don't ask that question.
&amp;gt;&amp;gt;Yvette Nameth: Yeah.
Apparently, I'm not allowed to ask you that.
I feel like I want to, though.
[ Laughter ]
&amp;gt;&amp;gt;Juergen Allgayer: I didn't know you had
that.
&amp;gt;&amp;gt;Yvette Nameth: That red button.
Could you describe the transition to a test
environment that matches production.
&amp;gt;&amp;gt;Juergen Allgayer: I'll try.
So the problem was that in our production
system, the -- the configuration information
is stored in config files.
Right?
Old Linux tradition.
So we have information about which data centers
to deploy to, how many CPUs, what priority
level of the CPUs, who talks to whom, what
is the backup philosophy, et cetera.
So there's a lengthy description of all these
configuration flags and what values, et cetera.
Typically, in your development environment,
you don't have or need that detail.
Right?
So if you ignore that and just think about
the development environment, you probably
also come up with some configuration.
But it doesn't have all the intricacies, and
especially not all the tuning part, that we
have in the live world; right?
So the problem was that you basically have
two different system configuration descriptions,
right, and while you know in your development
environment everything works fine, there was
a pretty good level of uncertainty about what
happens if I deploy into this one with this
set of configurations, to say, you know, we
don't have to redo all our testing again.
That's where we started out.
We were done here, and then we deployed.
And as we slowly rolled out, we rerun lots
of our tests just to make sure, just to make
sure, quote, unquote.
It's a quote.
Just to make sure.
Right?
And what we now have done is, to start out
with the configuration files of the live system,
use that as input to the development system.
Right?
Ignore what can be ignored.
Really, that we don't care about.
But make sure all the rest is exactly like
it is in life.
Even if it's overkill, and even if it's too
much and we don't need this, who cares; right?
It makes the step from one to the other so
much less error-prone.
So that was the problem I tried to describe.
Sorry if I wasn't too -- too -- too clear
on that.
&amp;gt;&amp;gt;Yvette Nameth: If you accomplish the goal
of having no manual regression, how will you
validate the look and feel?
&amp;gt;&amp;gt;Juergen Allgayer: So I think I mentioned
that as well.
I don't want the regression -- the manual
regression to be scripted, first of all.
Right?
So we do right now allow a time window of
these three hours that we have when the new
level is ready and it's not really to get
rolled out, we use that for free-form testing,
right, for exploratory testing.
What I don't want is to say, oh, I have these
scripts, and these hundreds of scripts have
to go through.
And the only way to scale this is to get more
bodies in front of keyboards.
Right?
That's what I don't want.
I want these scripts to be in the other world,
where they're run automatically, so that exactly
this can happen in the time we have.
You shouldn't worry about, okay, does this
result in that; right?
You should worry about does it look right,
did we break some of the usual things in internationalization
and -- right?
Does it look right?
Is the integration of the new feature into
yesterday's system actually looking out there
as it should?
Things like that.
So it's exactly that, I think, that I want.
No manual scripted regression, but use the
time for exploratory.
&amp;gt;&amp;gt;Yvette Nameth: And I will tell people in
the local audience, I am primarily taking
questions off of the Web site.
So if you want to ask one, feel free to add
yours there, too.
This one might be hard to answer.
&amp;gt;&amp;gt;Juergen Allgayer: Yeah.
Yeah.
&amp;gt;&amp;gt;Yvette Nameth: Because --
&amp;gt;&amp;gt;Juergen Allgayer: I think we should have
a conference on tools.
&amp;gt;&amp;gt;Yvette Nameth: Yeah, what tools do we use,
Juergen?
Can I share some of yours?
Well, we can share.
But can we share it with these guys what tools
we use?
&amp;gt;&amp;gt;Juergen Allgayer: So we could share.
Usually what is behind the question is, can
I use that tool; right?
&amp;gt;&amp;gt;Yvette Nameth: No.
&amp;gt;&amp;gt;Juergen Allgayer: And this is where it gets
a bit tricky, because a lot of our tools are
specially made for the whole proprietary stack
that we build.
So it starts -- everyone knows that, too.
It starts with a low-level, slightly tuned
Linux system that has its own compiler and
its own -- and then you walk up the tree.
And once you get to the test environments,
a lot of them are very finely integrated with
25 other systems.
Although, on the other hand, we have a few
that we open sourced.
WebDriver has been a very prominent one.
There are some.
But I honestly don't know -- I mean, if you
have maybe a list of 25 to 30 different tools
that make up our environment, 20 -- if it's
25, 23 of those are not really very usable
outside.
&amp;gt;&amp;gt;Yvette Nameth: And one of them is Google
Drive for sharing test documents.
&amp;gt;&amp;gt;Juergen Allgayer: That is true, yeah.
But not in spreadsheets anymore.
&amp;gt;&amp;gt;Yvette Nameth: No.
No test case management and spreadsheets.
&amp;gt;&amp;gt;Juergen Allgayer: We don't do that.
But maybe also want to mention that one of
our big work -- our, Google -- big work items
since a few quarters is to bring a lot of
these development tools and testing tools
out into the cloud for developers to use and
share; right?
So while right now the only path for us, really,
is to go through the work, to open source
something that we use, if you think it really
can be abstracted away and is still useful,
right, by itself, to say, forget this.
A whole different approach.
Here's a stack of tools to use.
If you develop on this platform, then you
get all these tools for free, build tools,
integration tools, roll out to deploy, blah,
blah, blah, blah, blah, including the platforms
to run the tests on; right?
So including emulators for Android platforms,
or even labs that run real physical machines
that you can, as a developer, reach.
That's still a bit out there, I hear.
Maybe another week or two.
But it's coming.
&amp;gt;&amp;gt;Yvette Nameth: Become friends with a Google
Cloud tester and find out what you can about
what is currently available in that space.
Thank you, Juergen.
&amp;gt;&amp;gt;Juergen Allgayer: All right.
[ Applause ]
I'm around.
I'm around and really would appreciate your
reaching out.
Thank you.
&amp;gt;&amp;gt;Yvette Nameth: He keeps trying to steal
my clicker.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>