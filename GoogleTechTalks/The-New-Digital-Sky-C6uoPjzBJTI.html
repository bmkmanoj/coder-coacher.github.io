<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The New Digital Sky | Coder Coacher - Coaching Coders</title><meta content="The New Digital Sky - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The New Digital Sky</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C6uoPjzBJTI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's my great privilege today to
engineers Tony Tyson who is at the
University of California Davis from the
physics department and also the director
of the LSST project the large area
synoptic survey telescope which is
always going to be talking about today
and the interest of brevity I'll limit
the introduction of that if you have any
questions this is going to go up on
Google Video publicly so if you have any
questions relevant to Google internal
stuff by all means feel free to ask them
but please do so after the videotape is
over thanks Tony thanks Rob it's a
pleasure to be here again I'm going to
talk to you today about a new project to
digitize all of this guy that's visible
from Chile which is the large part of
the sky very deeply to the edge of the
optical universe and 4d and deliver up
that data first starting a year after it
begins operation about 2013 it's a big
project you can see various
collaborating institutions here they're
growing their three more now that are
being added to that it's a big project
it started in two thousand the first
meeting we had this is sort of by
comparison to the rest of my colleagues
in astronomy where the first meetings on
new projects are usually hardware
meetings the first meeting we had was a
software meeting and you'll see why so
that's what the machine looks like and
just for scale that's a person standing
there it's eight and a half meter
diameter primary mirror and it's a new
kind of optics
and I'll explain how that works in a
minute so 20 years ago even 15 years ago
we couldn't even dream of doing this and
that's because of some breakthroughs
that have occurred since then and the
first and obvious breakthrough is in
micro electronics that comes in twice in
this machine it comes in the focal plane
with really big wide area detectors
billions of pixels very low noise and it
also comes in the many teraflops that
one has to have on site and elsewhere to
process and and search the data needless
to say in this building a very very
important part of this probably the most
important part of it is the software and
then we couldn't even think of doing
this without some breakthroughs in the
engineering of large a spherical optics
fabrication that have come down the pike
in the last 15 years you mix all these
three together and you can do something
new so in exploration in astronomy
anyway it's a huge three dimensional
universe space is very big you've heard
of explorations as a function of
wavelength think of x-ray radio optical
explorations in angular resolution when
normally thinks of the Hubble very good
angular resolution area survey this is
something that most of my colleagues in
astronomy these days don't and can't
boast about because as we've been
building larger and larger telescopes
they've been looking at smaller and
smaller fields of you you take a look at
this typical field of view of a very
large state-of-the-art telescope it's an
arc minute or a few arc minutes and here
we're talking about ten square degrees
and depth go to the edge of the universe
edge of the detectable universe now
you can do any one of these things
usually in some one facility and that's
the tradition and that's because of
Technology what we can do because of
these breakthroughs is we can do Sarah
Sarah them including time resolution all
at the same time you can go wide deep
and fast you can choose any one of these
and and and come up with examples of
what people have done but we have to do
all three in order to achieve our
mission and I'll show I'll show you how
we're going to do that so this is a
cartoon of the of the telescope it's
been designed we have some funds from
the National Science Foundation that
started a year ago to do the design and
we're ready for a construction proposal
that will be handed in in a couple of
months this is the telescope the light
comes in i'll show you in a minute
raytrace comes in and it bounces off
this huge primary mirror back up and
bounces off a secondary mirror up here
and then down off of a tertiary mirror
in the back into the camera which is
shown on the right and the camera has
some optic sent it to this is an optical
rate trace so light comes in now it's
looking horizontal light comes in
bounces off m1 the big primary mirror
bounces off the secondary mirror down
off the tertiary mirror into the camera
and this particular design allows you to
have an extremely large field of view
and crisp images independent of
wavelength all at the same time and it
it works really well we we can also make
huge mirrors here's an example of some
folks that made this base this eight and
a half meters across it's made in a
rotating oven you take chunks of Pyrex
glass and put them in the oven and heat
it up rotate it and heat it up and it
becomes a parabola and then you
gradually cool it off and then polish
polish the mirror so we think we can do
this we're waiting for some upfront
a little bit more upfront private
funding to do this very long lead item
has to start really this year this is a
focal plane in the camera and this is a
hundred-million-dollar camera just the
camera alone the focal plane looks like
this it's made out of all of these rafts
there's some approximately 190 separate
detectors each 4k by 4k for a full focal
plane of 3.2 Giga pixels and that sent
that's about 65 centimetres across here
and lots of electronics this whole thing
gets read out in one to two seconds and
the image images have to be processed
before the next exposure is taken the
exposures are very short they're about
15 seconds long the pixels are 10
microns and 10 microns of 10.2 arc
seconds in the focal plane which is
about the rights and praying for the
best sites on the earth deliver about
point 6 2.5 hour seconds full width at
half maximum point spread functions
16 bits deep Roth very very soon gets
turned into 32 is immediately when you
do math on them and that gets done right
up front so a lot of data
pixels little bee
so so we jiggered the pixels this way so
we can have some extra things in between
them to measure the wavefront curvature
so we know precisely exactly where the
where the wave front is where the Tocco
focal plane is relative to the detector
plane we have to know this stuff to
about a micron precision at all times
no so this was very interesting I would
have expected so with all of these
demands
the way to look at it is that we have
six tries to get it right you have the
three reflecting theirs and then three
aspherical optics in the camera and
Lynne seppala at Lawrence Livermore who
gave the optical design told me that it
just was an accident that it turned out
to be plan I still think he's lying but
it's a flat focal plane and so the thing
that the currency in this business of
doing very wide very wide and deep all
at the same time is the French call it a
tendu and some American engineers call
it through foot but most of us call it a
tendu and it is the product you might
imagine this should be it's a product of
the light-collecting area times the
field of view so units are meters
squared degrees squared that product is
what drives your ability to go wide fast
and d fall at the same time basically
the survey as much sky as possible and
so here's by example is the Keck
telescope that you've probably heard of
its a 10 meter primary mirror and here's
the field of view of the Keck telescope
and there's the moon by for comparison
in terms of angular scale here's LSST
here is the primary mirror and here's
the field of view and you calculate the
a tendu in your head with these two
beasts and there's no competition so
this is a very different kind of
telescope it allows you to go wide and
fast and deep
with good angular resolution other way
of looking at it is to plot the a tendu
in meter square degree squared of all of
the facilities that we know about the do
imaging both space-based and
ground-based and um except HST is the
Hubble Space Telescope it's way over on
the right here because it is tiny tiny a
tendu little tiny field of view and
small telescope also so it's not even
here but these are all the telescopes
that either exist or are planned for the
next ten years and here's LSST in terms
of attack a tendu your ability to go to
a certain flux limit and at the same
time cover a certain area in the sky is
proportional to the eight-ton group are
you going to talk to
clear of the amount
versus the amount of light
that's a very good question so so
there's a trade naturally between the
amount of time that you take to survey
the entire sky and the light collection
so you might say well I can get away
with a you know a one meter 2 meter
telescope if I if you give me 50 years
to do this and then the Aton do that
product comes into the a tendu and you
you can almost get away with it the
problem is that a small telescope can't
go very faint very fast and therefore he
can't do the fast part of the white fast
deep you could do the widened the deep
maybe turns out you can't do the deep
with a small telescope because of the
noise in detectors and detectors were
totally noise-free you could do that but
you can't do white fast and deep all at
the same time in order to do fast and
deep you need to collect a lot of light
for a second and and so you need a big
mirror
is what helped you see how it works
behind the Space Telescope yeah so the
way the tell Hubble telescope gets deep
is they take really really long
exposures a day long I mean last data I
took on the Hubble telescope with
several days of exposure just to go deep
in a tiny field that is about an arc
minute in size so you have to know what
you're going to look for you go there
point there and integrate like hell for
several days and and that's a different
kind of astronomy than this so there's a
there was a great competition there were
ten sites really really good sites
worldwide and it was narrowed down
recently to one site Sarah patron shown
here in northern Chile in the Andes
incredibly good site and this is our
timeline i'll show you a better picture
of it later
so we're now clearing ground there this
is supported by the national science
foundation and do ii but but department
of energy but really this is a
public-private partnership we've done a
whole lot of simulations and it's
necessary when you build a really big
new facility to do simulations to
convince yourself first of all in others
that you can actually do what you say
you can do and we've done simulations of
how it how it tiles the sky how rapidly
can tell the sky using actual real
weather data from from that site and
then simulations starting at the high
redshift universe introducing a
cosmology following all of those photons
all the way through as they pass by
intervening over densities of dark
matter through some warped cosmology
through our atmosphere which further
works images into the imperfect
telescope and the imperfect detector and
through our pipelines and so we've done
end-to-end simulations to show that we
can do what we claim in terms of science
the survey itself is six optical bands
from the ultraviolet to the
near-infrared and we're going to visit
the sky each little piece each little
patch of the sky will be revisited two
thousand times so in addition to going
very very deep we will also be able to
make a movie of everything that happens
in the sky and so
and also because of the L show you in a
minute because we cover so many
different weight bends we can actually
have a poor man spectrometer in some
sense and because of the Hubble
expansion and redshift get a pretty good
idea of the red chip or the distance to
every galaxy that we find about 5
billion that's wrong it should be 5
billion galaxies for 25 billion galaxies
with red shifts time resolution going
from seconds two years so we can think
we can look at things move and those are
sort of interesting especially if they
were to hit us in 20 or hundred years
and things that go bump in the night at
the edge of the optical universe is very
interesting science they're also the
time domain is interesting
over 10 years
so something else happens with this
particular design of a telescope but
you're probably all used to well maybe
not these days but when I was a kid your
use used to Tekra torial mounts where
you have a telescope and it just rotates
around an axis and this guy doesn't
rotate on the focal plane you can take a
long time exposure and you're all set
this is an out asmath mount so it moves
up and down and then it moves around and
when you have an alt azimuth mount
here's an image of a piece of the sky
and two things to watch for here one is
the relative orientation of the focal
plane relative to the sky and the other
is the orientation of the pupil plane
which is up near the spider up here and
you can see the diffraction spikes on
bright stars do to those the spiders the
pupil plane actually rotates at a
different rate and so if I go through
this little amateur movie here of
following appealed on the sky as we go
forward you can see that the pupil plane
rotates at a different rate on the
detector than the sky and they both rate
rotate relative to the detector and this
gives us superb control over systematic
errors and a way of getting rid of these
annoying diffraction spikes in the
reconstructed image they're just not
there how do we get the distance to
distant galaxies here's an example
here's these six bands in the optical
going from the ultraviolet to the near
infrared and over plotted on these
transmission curves of the six bands is
a typical elliptical galaxy spectrum in
red starting here at redshift zero right
next door and I move that guy further
and further away and you can see on over
on the right if you want to follow it on
the color color plot where that moves as
i move that elliptical galaxy a little
further away it gets more and more red
shifted and you can see that just by the
relative colors and across these six
bands one can reproduce maybe how far
away it is to some accuracy it turns out
the accuracy that with which
you can do that is about five percent
which is pretty good
different filters in front
not simultaneous it turns out the best
just not possible we're very greedy
concern agreed we want 90 in excess of
ninety percent quantum efficiency on
every image that we take so we can split
up colors that way so there have been a
lot of science drivers for this in the
bunch bunch of reports that have put
this is a high priority for the nation
and I thought it would be good just to
very quickly review for you of what
those science drivers are and then talk
more about the data and how it might be
served because there are some you know
everybody it's not it's just not
astronomers are physicists that are
interested in this data we're all I mean
there is a nice whiff from one branch of
physics into astronomy is because I'm
really curious about the sky you know
not so much as a scientific level
initially but I just it's interesting
it's fun and i would like to look at
this guy oh so this movie isn't going to
run because we don't have this movie up
here on this machine but basically
back in the 30s an astronomer at Caltech
by the name of Fritz Zwicky
found something and people figure he was
crazy and they didn't realize that he
was correct he was also crazy his rock
points out that sometimes a necessary
condition oh I digress anyway he looked
at clusters of galaxies with the
spectrograph and he found that the
galaxies and the clusters were zipping
around in excess of the local speed
limit set by saying that ok I'm going to
assign one solar mass per solar
luminosity and that's how much mass is
in that cluster and therefore I can
figure out how fast things should be
moving around and it was off by factor
of 10 and we called it dark matter and
people thought he was crazy the turns
out he was right the universe actually
is made up mostly the matter in the
universe is made up mostly of dark
matter and we are not dark matter which
comprises virtually all of the matter in
the universe is made up something other
than the so-called baryons protons
electrons neutrons that were made out of
weird there's something even weirder
which I'll tell you about in a minute
but it would be nice to know about dark
matter sharp its development and also
really nice to know about something else
called dark energy
so if you look from the earth either
with an earth-based telescopes like LSST
or with an orbiting radio telescope like
w map at the universe you can detect
different parts of the universe
depending upon health how far out you go
at the very beginning of course there
was presumably a very very hot time
which was everywhere in space called the
Big Bang but the earliest moment that we
can actually see is the moment when the
hot Big Bang plasma had cooled to the
point where electrons and protons could
recombine into hydrogen neutral hydrogen
letting free those photons the photons
then weren't hopelessly scattered and
contained in that plasma and those
photons came to us and have been red
shifted stretched down in wavelength by
the expansion of the universe and appear
to us now to be a microwave yes in the
microwave called the microwave
background and you can take pictures of
the microwave background and have people
have done that that's our earliest
vision or earliest picture of the
universe and it shows the regions where
this more or less dark matter so that's
where our first measurement of the
spectrum of dark matter comes from there
it eventually and that was about 300,000
years after the Big
by the way if actually about a half a
billion to a billion years later stars
and galaxies started forming through a
mechanism by the way which we don't
we're not very sure of anyway they
started for me and those those galaxies
are visible from here on the earth
thanks to a coincidence that our
atmosphere is actually able to transmit
in the optical really well you can see
to the edge of the optical universe from
down here throughout the optical and so
you can you can probe the universe in
various ways and one of the very great
puzzle months came starting 85 but
really we weren't really shocked until
around the mid 90s mainly there's so
much dark matter invisible dark matter
in the universe but it really slows this
huge expansion of this explosion sort of
like if I toss a ball up in the air it's
going to come back down it's the gravity
of the dark matter slow is the expansion
of the universe this slowing this
deceleration of the expansion was seen
and detected and we all believe that the
universe was slowing down however
recently there's been a couple of lines
of evidence that have shown us that the
universe has changed its mind in the
last few billion years and is beginning
to accelerate its expansion again and
this is this is what we call dark energy
which means we don't understand it so we
had to give a name but we don't know
what it is it's something in the nature
of space-time so new field which at this
late stage in the universe this very
dilute universal we have now compared to
where it was
at this late stage in the universe
dominates the energy budget and somehow
has negative pressure and negative
gravity and causes the universe to fly
apart that blows my mind in a lot of
others and that's for many of us a
driving motivation but there are a lot
of other there's a lot of other cool
astronomy with this stuff so what about
dark energy this is our resident
theorists so their various scenarios for
the scale factor of the universe as a
function of time they all start with
some kind of huge big bang about 13.8
years a billion years ago but they all
have different time scenarios one of the
so-called re collapsing universe has
been ruled out but that was very much
alive up until the 90s and think then
there was the critical universe we all
believed we all believed in the critical
universe we're just kept just very very
finely tuned kept on coasting then
there's a different kind of coasting
universe and some of us believed in for
a while and now we're forced to believe
in this wacky universe that starts by
decelerating and that changes its mind
and so if you wait long enough in our
current cosmology everything will be
infinitely far away and protons will
decay and will be nothingness so this
you know sort of a lonely prospect
so how do you actually measure this
stuff really one way is to see how light
from these distant galaxies is moved
around on the sky so for example imagine
that you're over on the right over here
somewhere you're looking out through a
universe composed of lumpy dark matter
and as time proceeded from the distant
past to the more recent past the
lumpiness becomes greater structure
forms as a function of cosmic time
simply by self gravitational buildup
dartmoor dark matter attracts more Dark
Matter gets bigger density with time and
so you imagine look the light from one
of those distant galaxies passing by
these over densities of dark matter
getting deflected by gravity Einstein
taught us light is deflected by gravity
and so you use the general theory of
relativity as a tool to try to
understand what the distribution of dark
matter is and what it's doing is a
function would cost me kept up and this
is called 3d tomography 3d mesh
tomography so I'm not going to show you
this movie this is all on our website
which will show you at the end LS STI
org but basically this is the effect
that you see you have a over density of
dark matter invisible in this picture
and invisible in reality and what it
does is it it it causes this funny kind
of distortion of distant of this a
universal around the center of the over
density of dark matter
and images of galaxies that would have
appeared here actually is moved out to
here and appears out here instead and it
gets distorted in this way and I can
show you in the next image here really a
very simple cartoon of how that happens
it's really simple idea so you're down
here there's a huge over density of dark
matter here and here's this law abiding
nice little ground distant blue galaxy
out there and each one of its pixels
appears at a new place in the sky by
Einstein's like bending formula this
little pixel gets moved out here this
little pixel move gets moved out here
and you get this banana this
characteristic shear is a diagnostic of
the distribution of the dark matter you
take this away you see the galaxy here
if you put this matter here you see a
distorted image of the galaxy here and
the amount of distortion is a function
of two things you consistently you can
do this just simply by dimensional
analysis you don't even have to know
relativity it's a function of the
distances if I if I put a lens right up
here next to my eye it's not very
effective I put it right up next to the
candle or the source it's not very
effective the best place to have a
lenses in the middle and so it's
proportional the amount of shear here
distortion is proportional to the ratio
of distances here but it's also
apportionable to the amount of mass but
at the National 20 there's no distortion
and so it's from this term here it's
proportional to a product of cosmology
and gravity
gravity comes in big G and this one's is
that this is a function of the cosmology
and so as a mix of large-scale dark
matter structure evolving with cosmic
time and cosmology and these
measurements and there are a lot of
other things you can measure but this is
this is this is fun you probably used to
see images like this that are basically
basically our images of light or
radiation so I could have told you this
as well this is an x-ray picture of the
sky but this is not radiation this is a
mass picture this is a real data that we
have now from an existing experiment a
pilot experiment 20 square degrees doing
this 3d mass tomography this is a slice
in cosmic time of the mass distribution
looking out in this little part of the
universe 2 by 2 degrees on the side so
the moon is about be a big and these
over Denton these are over densities of
dark matter in that little distant slice
and we've confirmed them by looking with
x-ray telescopes at them and seeing the
very hot the X rays coming from the hot
electrons at the bottom of this deep
gravitational potential
so we can map you can map the mass
density of the universe in different
different distance slices and therefore
chart the development of cosmic dark
matter structure this question that was
that color red shift which I went
through all too fast for mass
spectrograph so for each one of those
billions of distant galaxies you have a
lot of color information across these
six bands and you can reconstruct with
some accuracy what its distance is if
you have the right if you have the right
template for its rest-frame spectral
distribution like a Jewish were looking
at a dark matter in between ah so so how
do you reconstruct okay let me go back
it's good question so
here is where the center of this pile of
dark matter is so the way I developed
this algorithm back in nineteen eighty
or so is it's really a very simple idea
mainly if there's no dark matter here
you won't have this characteristic
alignment if there is you will and the
amount of this alignment or shear is
directly proportional to how much dark
matter you have to her so imagine
imagine splitting the piece of the sky
up into a little grid of dots around
each dot you ask the question how much
how much tangential alignment of distant
galaxies images do I have and it's a
simple integral of the electricity and
you just do that calculation and you do
that calculate and it returns a scalar
number it's the number for each one of
those points and that's an image and
that's an image it turns out calibrated
wheeler image of this dark matter that's
how to make a very simple forward
integral no inverse problem with
regularization requirements
so it's a lot of fun when it happens
it's a very fast calculation
so this also probes the physics of this
mysterious dark energy because if we are
able to trace this development of cosmic
structure all the way from the
decoupling in the microwave background
only three hundred thousand years after
the Big Bang all the way up to the
present that's a huge number of
constraints on the cosmology in a
particular the physics of dark energy
here's a better way of looking at it
imagine that you have two very distant
galaxies not necessarily physically
associated but close an angle on the sky
and you follow the light from them as it
passes by and through intervening over
densities of dark matter to you down
here for galaxies that are close by an
angle of the deflections are the light
from them suffers very similar to
flexions and therefore they're induced
ellipticity due to these deflections are
correlated and are uncorrelated with the
same sort of thing somewhere else on the
sky and so you might imagine that the
shear shear or lip teste ellipticity
two-point correlation functions can tell
you a lot about the intervening Dark
Matter spectrum and then if in addition
to all of that you can do this as a
function of redshift of distance which
is what we do using this poor man's
spectrometer of color red shifts then
you can untangle all of the effects and
have a very precise measure of the
physics of both dark energy and dark
matter so this is just one of the things
that's beast will do there are a lot of
them but we will serve a 20 to 30,000
square degrees of the sky do this for 4d
reconstruction of about 4 to 5 billion
galaxies which means that in principle
we can render have a have a have a 4d
rendering a movie that you could swim
through of the of the 3d universe as a
function of cosmic time in the time
domain we will detect and measure the
orbits of a hundred thousand asteroids
this is a public service it's also some
science in there
some of them are have our name on them I
think a million supernova a lot of
science there and probably more
important than any of these things we
can't even questions we can't even ask
because we're looking we're opening a
new window in the sky over current
facilities by about a factor of a
thousand in the time domain so here is a
near-earth objects with a threatening
picture of one here our solar system
inventories is woefully incomplete this
will finish the inventory all the way
down to 150 meter rocks below that size
they burn up in the atmosphere pretty
much but it demands LSST or something
like it you want to have short exposures
because if it's longer than about 15
seconds they they trail during the
exposure you can't gain depth by
integrating longer you want you have to
go very faint in that period of time so
that's the high the high a tendu
requirements needed for this strangely
enough also so how do you find things
that go bump in the night well you take
an image and then you compare what you
found in that image with a archival
image of the same piece of the sky and
you take the simplest and dumbest things
is you register them work them onto the
same coordinate system and then subtract
them and something that is new is new
and so that's how it works here's an
example from our current survey before
during and difference and bang you have
a visitor object we've discovered these
things recently they're not super novae
we don't know what they are LSST will
find lots of them maybe we'll find out
what they are here's some more before
during different
a new class of objects probably
a laser pointer yeah another another
civilization so maybe they're trying to
tell us something so LSST extrapolating
from what we have now found should find
about 10,000 of these every night so
that's cool so we've done these
simulations which i think i showed you
before so massively parallel
astrophysics is the key word take-home
message scientifically from this
probably the most important thing that
it will be remembered for is not on this
list but here are some things that
people would like to do with this
machine going all the way from the earth
to the solar system to the history of
the Assembly of our galaxy and its local
group two galaxies in general how
galaxies formed to the mysteries of dark
matter and dark energy so data a small
word for a big problem six bands five at
a time that is to say we can get five of
these big filters which you have about
the size and weight of a manhole cover
inside of the camera all at the same
time so in 30 seconds we can move one of
these filters in place we observe in
pairs of 15 second exposures to second
readout separated by a five-second slew
this thing is busy in it and it tiles
the sky every few nights and this
relentlessly continues to do this for 10
years in different color bands it goes
very faint if any of you are astronomers
you'll notice that that is pretty faint
and it goes faint in 15 seconds he goes
to the edge of the optical universe in
15 seconds
so
I was debating whether to show this
slide here actually because you know
astronomers look at the word petabytes
in their eyes you know they don't know
how to grab grapple with these concepts
this is probably not under pro Prius
slide to show at Google but I mean this
is this is so much more data than we
folks are used to handling it's it's
it's not even funny actually the
high-energy physics community is
beginning to contemplate handling
petabytes a year and and so it's it
turns out not to be all that scary but
one of the things that we do have to do
on the fly which is a little scary is we
actually have to extract knowledge in
real time from this much data that's an
interesting software assignment
especially if you don't know what
question to ask you don't want to design
an experiment to exclude new science and
so I mean the most obvious thing is the
most obvious thing is to do one of those
science experiments it was on that list
of massively parallel astrophysics you
could put blinders on design your
pipeline software pipelines do that and
go home but how do you find something
that you didn't expect how do you design
software that will do that you'll be
monitoring roughly 10 billion objects
for variations it's a nice assignment
this is our concept of what we think we
have to do we have to have a lot of
compute capability on the mountain to
get rid of the instrumental instrumental
signature and also to look at the health
of the of the machine we actually have
to come up with alerts on the fly within
seconds so we have to do this image
differencing stuff in seconds and then
we have to do a lot of this very much
more complicated analysis for all these
other science drivers and particularly
the cosmology part in a very big
facility somewhere then we have to serve
it to the world
US scientists but I think more
importantly to the world in general here
are some estimates of data volume so in
terms of raw data 16-bit data
unprocessed about 15 terabytes a day and
here's how you do that calculation we
actually have to do some instrumentals
signature removal so that increases the
that increases the size event and it
very quickly gets turned into 32-bit
images of course and then we can
compress it a bit there's also a
metadata catalog there's information
that you learn from these images that
you put into some kind of flat file
catalog system or something where an
object is what its shape is what its
colors are did it change in time
something like two kilobytes of
information per object information that
you get and you can put it in the
catalog which has a size roughly ten
percent of the size of the images and
it's from such a catalog that you might
actually at at Google if you really
wanted to render a a four dimensional
universe that people could swim through
it would be from that catalog probably
not the images themselves you can
actually reconstruct images if you have
enough information in your catalog
perfectly decent looking images from the
catalog alone at the highest application
layer and the software there's of course
the data acquisition image processing
pipelines to get rid of the instrumental
signature difference imaging that goes
on a whole lot of different steps have
to happen pretty much all at the same
time
so we have a requirement which probably
will get modified in the wrong direction
mainly shorter that within 60 seconds we
actually have to deliver a data product
to the world a believable data product
which is this object changed its
luminosity or color or moved on the sky
by such and such at a certain time
within 60 seconds it'll probably go down
to 30
Oh what would be so suppose that you're
interested in the science of gamma-ray
bursters which are weird things that pop
off in the gamma-ray and then you see an
optical signature of this thing they're
not really understood but if you have a
lot of statistical data on the optical
signature of gamma-ray bursters you
could get some science out of it and
particularly if you had it fresh enough
that you could point a gamma ray or an
x-ray telescope at that place this is
sort of the reverse of what is normally
done now very inefficiently if you were
interested in getting the orbits of
asteroids you have to actually get lots
of observations of this rock so that you
can pick it out of the haystack which is
the main belt asteroid population which
is huge so we have a significant noise
background in earth-crossing asteroid
you're threatening asteroids detection
from the main belt asteroids and you
need to you need to you need to run that
we call it the disambiguate er it's an
inverse problem you need to run that
image two orbits calculation really
quickly so and you don't want to lose
the thing so at an infrastructure level
the telescope will be down here but
we've got to get the data back up here
in some way on some time scale so the
mountain will have to the camera reads
out in two seconds maybe one second
we'll see but certainly less than two
seconds that data has to go down to the
base station through a ten give it
gigabit per second plank which is here
this is probably in la serena seacoast
town
there there's more compute capability
here in this building this is boring but
it's something that we have to have and
this is where some of the transient
alerts will get created then we have to
get information all of that data back
stateside and then we have to serve it
in some way to the scientific community
and more importantly to everybody seven
petabytes per year and some folks not
your average high school student but
some folks will be doing sufficiently
complicated calculations on this data
that they don't they're not going to be
happy with just the catalog they're
going to have the catalog and then
they're going to want to do a
calculation based back on the individual
images of it they're going to do some
calculation using the seven petabytes of
data and it probably is going to be a
non-local calculation so lots of
teraflops have to be available for that
activity but that's a very small on the
other hand that's a very small part of
the total user base most people the most
of the most of the hits are going to be
from people that are just curious about
the sky and want to swim through the sky
so how do you do that this is a very
amateurish way of doing this so a bunch
of us sat down at the Hayden Planetarium
last year try to figure out how how we
could through their so-called digital
universe how we could serve this data to
the public and we took the sort of the
worst case scenario we degrade the
resolution so that you just have a
barely acceptable image compress it in a
lossy fashion and then serve this this
is not very much fun too
even many casual users but you can do it
and you could do it with about a hundred
and 80 gigabytes database x 3 by the way
because there's so many there's six
bands and so you have basically a RGB
color three times so say three times 200
gigabytes in JPEGs and maybe in twenty
ten or twenty thirteen you could
download that for billion galaxies but
the real photometric catalog that a lot
of people are going to want to use is
this guy here 10 to 100 terabytes
depending upon whether it's the first
year or the tenth year of operation so
clearly however the right way to do this
is to do it professionally and have
people gain access to the sky through
this google sky capability so basically
this machine you can view this machine
as a transformer it's basically
transforming the sky to disk farms and
then it's then the fun starts so
nowadays we have telescope assignment
committees and we apply for our three
nights on some you know Keck telescope
and have ten percent chance of success
per attempt and there's a committee that
assigns time there presumably will be a
floor space assignment committee for
some of the computation that will
high-end computation that will be done
on this data so different the other
thing that's different about about this
project and any other astronomy project
that i'm aware of at the moment is that
the data is not proprietary this gives
me is director a hell of a time raising
private funding for this from
participating universities because the
dean will always ask well what's in it
for our professors if everybody could
get this data if our competition in
germany can get this data
what's in it of course is familiarity
and being close to the close to the
Beast itself
but it is public and it will be it's
sort of a different idea sort of open
source open data all of our software can
be sort of the Linux model so here's
something fun that I thought I'd finish
with mainly a little piece of the sky
this is eight arc minutes across this is
a piece of the sky that just by chance
overlaps between some photographic
plates that were taken some 50 years ago
the Sloan Digital Sky Survey and our
current survey the deep blend survey and
it just gives you a feel for how rich in
depth and and features and information
the sky can become if you go deeper so
here's digitized photographic plates
these were taken with the Schmidt
telescope on Mount Palomar in the 60s
and two bands so you can form a sort of
color from it color image jpg image and
in the next image is the Sloan Digital
Sky Survey a data released for showing
these same galaxies and a few other
bright stars and galaxies and whatnot
and by the way it's sort of fun if you
blink between them take a look at this
star here this is 60 years
hey guys moving
stars do move particularly nearby sub
luminous red stars are trucking right
along and then if you take a look at
that same patch of the sky with the
d-plan survey this is what we have with
our existing facility not much exposure
only 20 exposures and you're looking out
nearly to the edge of the optical
universe there are some artifacts in
these in these state-of-the-art images
with existing facilities that won't be
present as I mentioned in the LSST
imaging because of the fact the focal
plane because of the fact that the sky
rotates in the focal plane between your
hundreds of images that in any one band
and that is two things the diffraction
spikes which actually in this light you
cannot see very well from the stars and
of something called blooming which is
the star so bright that electrons are
flowing out of the potential well of
that pixel and just happily going down
the column and both of those effects
will be suppressed extremely well by
having a whole collection of images that
are rotated now this does not go quite
as deep as LSST it's about a factor of 4
less deep than LSST will do and it's
also a factor of roughly too poor or
angular resolution so imagine you know
this the difference between going from
sloan to deep land survey imagine that
difference again pretty much with LSST
data and then on top of that sort of
two-dimensional color information you
add another dimension mainly mainly time
or distance so you can make a movie you
can do celestial cinematography you can
have overlays of things that move like
the star there are other things that are
changing in brightness popping off but
you can also reconstruct the full
three-dimensional arrangement of all of
these galaxies and swim through them
if you want you can you can you can
reconstruct that from the redshift
information and that would be a fun tool
so here's our timeline I'll finish now
we are here it's going to cost us about
40 million dollars to do our design and
development and we have some of it a
little over half of it in place through
a combination of private and federal
grants then construction will be 300
million and then operations about 20
million so far two volunteers for that
dle and NAT D OE and NSF but NASA is
also interested because of the asteroid
problem so we're nearing the end of the
first R&amp;amp;D phase where we are prepared
now we know how to build this beast we
think we've done a lot of tests
simulations we are putting in a
construction proposal in the next couple
of months to both agencies invited
construction proposal to both agencies
and we expect to see some light through
this machine and somewhere around 2012
science operations probably in 2013 in
just the first year of operations the
entire visible sky will be done to this
depth plus the distance information plus
the time information and it'll just get
better and deeper as the years marks
Bonnie so it'll be fun from the point of
view really of turning people on to
science
so here's where you can go for more
information Thanks questions
what proportion of the science will
hey
do by itself to
first six months yeah it's a good
question so that's a really good so
everyone's asking in the scientific
community what what portion each one of
those lists of science topics last
massively parallel has to physics lists
that I showed how much of each one of
those scientific missions will this
machine do as a function of time in the
first six months in the first year etc
and it varies but a lot of the science a
lot of the science gets done in the
first few years so it's sort of a an
exponential kind of thing some of it
depends on on the full sum of some of
the questions that you might ask depend
on the full ten years of data but a lot
of it a lot of it gets done in the first
year too
24 Oh what happens when we're I think
we'll see you know what word something
that we don't expect now and that will
drive our decisions in 2024</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>