<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Recognizing Opportunities for Thread Level Parallelism... | Coder Coacher - Coaching Coders</title><meta content="Recognizing Opportunities for Thread Level Parallelism... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Recognizing Opportunities for Thread Level Parallelism...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/H2qXweg9gW4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Brad Chen in the system
infrastructure group and I'm pleased to
introduce Manish fahara Johnny who's
going to tell us today about work
happening in his research program in
Colorado on on finding opportunities for
threaded level parallelism okay great
thanks for the introduction Brad so
hopefully I have something interesting
to tell you guys I know Google has a ton
of smart people working internally on
lots of interesting stuff and you're
very secretive so we don't actually know
exactly what's going on on the outside
so it's hard to tailor a talk for the
audience since it's not as I said it's
not entirely clear exactly where your
problems are but hopefully I have
something interesting of interest to say
to you so today I'm going to talk about
finding parallelism from finding
opportunities for paralyzing otherwise
sequential applications and so this is
work that's done in my research group
generally I would have my students
present but a student whose work on
focusing on today isn't isn't here and
so I apologize for that but business
work that's done in my group and with my
students
Brian Bushnell John Giovanni Graham
price and Rohan omlie at the University
of Colorado at Boulder so let's get
started all right so everybody every I
come from an architecture background and
so every architecture talk has to have a
Moore's Law slide and so we'll put up
the Moore's Law slide but I do have it
but there's something interesting right
that's happening in our community and so
most people when they think about
Moore's Law think well you know
computers get faster at some exponential
rate but the truth is Moore's Law is a
statement of it's an economic statement
that says that the economics of the
semiconductor industry will force force
fabricators to integrate more and more
transistors at an exponential rate and
the rule of thumb has been about every
18 months you can expect the number of
transistors on a die to double and
that's held remarkably true if you look
at this map of say Intel microprocessors
from here to 2000 and you'll have to
take my word for it but this exponential
trend continues oh and it's exponential
because our vertical axis is logarithmic
so then if you look there
what we're used to is what I'll call
Moore's corollary which is that
performance also doubles every 18 months
or so so those transistors are converted
directly to performance and this is held
remarkably true this is the fastest spec
int performance reported in each quarter
starting from 99 the red line is the the
red line that you see up there is the
ideal curve and the other lines are the
actual reported for spec int and spec FP
right the green is specint and the gray
of spec FP so if you look at this what's
happened is sometime around Q 3 of 2003
we fall off this Moore's corollary curve
and so there's a gap here right so we
see that there's a substantial gap and
so this is the gap that we're trying to
fill so why is why have we fallen off
the performance the predicted
performance curve well the reason is
because designers around that same time
frame stop building uniprocessor systems
and basically if you look at high
performance systems today uniprocessor
systems are at the end of their life so
we're now everything that you buy for
your desktop is basically a chip
multiprocessor my laptop has two
processors in it right 10 years ago
you'd say oh you'll have a dual process
or a laptop what right but now I have a
dual processor laptop you can buy a4
processor workstation from Apple for
under $3,000 so so multi cores are here
and we're talking about a large number
of cores right we're at four cores now
MIT group has fabricated a 16 core
machine and Intel us has an 80 core
prototype so we're talking about
hundreds to maybe thousands of course so
going from multi-core to many core so
the problem for us unfortunately is that
everything that we do in architecture to
support the application on top of on the
silicon is based around the von Neumann
architecture so we have a von Neumann
programming model we have a von Neumann
compiler the operating system multitask
is designed to really multitask and
timeshare a von Neumann processor von
Lohmann applications on a von Neumann
processor though it is a little bit more
sophisticated than that today but
the architectures Fontenoy min so on and
so forth and what we've done is the
architecture said oh yeah well we no
longer know how to make that von Neumann
thing go any faster so now we're gonna
build these shared memory
multiprocessors go deal with them well
this is a difficult problem because
everything else in our stack is centered
around von know I'm in architectures and
so you can say well alright I'll just
keep the von Neumann programming model
and I'll just build a new compiler and
everything will be good but as we know
there's been about you know 30 40 years
of maybe 30 years of research in this
area and we've not been able to
successfully produce a compiler that can
paralyze these applications right so we
break this chain right we have no way to
get our von Lohmann applications under
these shared memory multiprocessors
furthermore operating systems are not
very good at dealing with 400 Corpse you
know even a hundred cores 64 if you look
at a lot of things in linux for example
right now at about 16 or 32 cores you
start to run into some scalability
problems on these systems and so we have
to adapt the operating systems as well
some of these things we know how to fix
we just haven't fixed them other things
we don't know how to deal with okay so
the von Neumann programming model is not
an option and we have to replace it and
the best thing that we know what to do
so far is say well okay now you have a
shared memory multiprocessor programming
model great the problem that we have
with that if the next slide will come
out the basic problem with the shared
memory multiprocessor programming model
is well programmers are really bad at
explicitly parallel programming okay now
and you get you've seen this internally
right so there's internally there was
that famous MapReduce paper that you
guys published right and so this is a
way to sort of structure parallel
programming so that people can do it
even though they're bad at the problem
in general so if we look at this
programmers are much better at
sequential programming they're not it's
not perfect as we know a lot of these
things have bugs and our current
solutions haven't worked right the
solution to this was typically been
let's hide the parallelism and build a
compiler and as I mentioned earlier this
hasn't worked okay so what do we do all
right let me move to these multi-course
well you say well Google
have parallel apps and we have MapReduce
and we do these things right and so
right now each of these boxes which is
one of the functions apply to one of the
datums in in the map process you'd map
it to say a cluster node and you say
well now what happens when our nodes
become multi-core oh well I'll do is
I'll say two cores okay I'll put two of
my map nodes I'll take two of those Maps
and put them on two cores for no problem
right but you know if this gets bigger
this isn't really gonna work very well
you still only have one disk you still
only have one memory Channel so on and
so forth and so as these things scale
this may not be the best way to take
advantage of these applications second
of all this doesn't necessarily help you
with the latency of anything right it
might it might allow you to reduce your
utilization or something like this which
is useful but it doesn't necessarily
accelerate the amount of timing to those
map nodes takes so when you look at
using multi-core systems and
parallelizing things there are different
ways to think about parallelism right
there's the classic task parallelism
I'll run two different applications in
parallel on the machine no data
communication whatsoever right so that's
the classic task based parallelism then
there is data parallelism which is
exemplified by web serving and MapReduce
is in some ways an instance of this
where you fan out your data to a whole
bunch of nodes you do a whole bunch of
computation and you recombine the
results at the end right so this is like
the split join and these types of
semantics right and then there's another
type of parallelism which we often don't
think of and it's been a large portion
of focus in our research which is
pipeline parallelism and we don't really
think of this as much in software though
it can it can be used to advantage and
this is where you have a sequence of
serial operations to perform and you can
decompose them into per say data or per
video or per frame that you need to
process and you can break things into
serial stages okay
and so video decoding and network
processing for example on a per packet
basis fits nicely into this and I can
take each of those pipe stages say and
implement them onto a separate core so
it's another way of taking advantage of
these multi-core systems right and then
you can combine these approaches and say
it's some type of cluster system so you
can inside of each of these
elements that you've mapped you can put
in an entire multi-threaded application
for example okay so in order to do this
though even building pipeline which
parallel structures and general parallel
structures programmers are still not
very good at it and they need tools to
help them and we envision building an
entire tool chain that starts from
identification of parallelism and ends
with verification and runtime system
support right so you can look at it so
here we join this minority chorus that
Steve along with Steve lameta and Winn
mate who and a few others say look maybe
the best way to write these parallel
apps is to really first write it
sequentially make sure it works get it
right then let's look at where the
performance bottlenecks really are and
try and get those portions of the
application to parallel lies okay now
this is not always the right way to go
obviously certain scalability can be
hard to achieve in these types of
environments but nevertheless it's
something that's definitely worth
considering right the sequential app
also teaches you a lot how many how many
people here have not learned something
about the application they were writing
as they are writing the application
right and so hopefully you don't to
learn those things upfront when it's
easy in the sequential space before you
go ahead and parallelize everything so
what I'm going to focus in all of these
areas the most mature research is on
identification and implementation what
I'm gonna focus on today given some
feedback I got earlier is on the
identification of parallelism and really
focus on this parameter tool right so
and talked a little bit about the
prototype that we're developing for this
tool and some of the technology that
underlies it okay so what observation is
that when you're writing a parallel
program identifying concurrency is hard
right finding out exactly what can go in
parallel and what can't is difficult you
might have some intuition as to what
these things might be but knowing what
they are for sure is non-trivial
implementing a parallel algorithm is
somewhat easier than this though getting
all the corner cases right is the hard
part right so getting something that
sort of works is not bad but then
getting it so that you have no race
conditions and none of these other
problems that
hard thing right so what we're trying to
do with parameters we're trying to help
find large-scale parallelism in existing
programs now we're not proposing an
automatic approach so I'm not saying
we're gonna tell you a compiler we're
gonna analyze your program and extract
the threads for you right we're trying
to help programmers not replace them so
we're gonna try and point out hey here
are some areas where you might have some
parallelism you should go investigate it
and then produce the tools that will
help them investigate it and maybe prove
to themselves that yes I can paralyze
this application and then go ahead and
write the tools that help them extract
these programs but we're envisioning it
as an interactive process okay so we're
going to start by looking at parallelism
in sequential applications and people
often have misconceptions about
sequential applications and I don't know
how many people in this room have these
misconceptions but people say oh well
what are you gonna do with sequential
applications clearly 25 years 30 years
of compiler research has failed these
applications have no parallelism to
speak of beyond the fine-grain ILP but
that's not true under ideal
circumstances there's been plenty of
prior work that have shown that these
sequential applications have a lot of
parallelism to offer right executing
hundreds of in fact being able to
execute hundreds of instructions at a
time over sustained durations the
problem is that none of these this
parallelism can really be exploited by
instruction level parallel and that was
shown by wall in the digital western
research laboratory back in 91 where as
soon as you impose limitations on your
branch prediction accuracy you impose
limitations on your instruction window
size and so on most of the parallelism
is gone
right you'll never see it so but the
issue is maybe we can extract that
concurrency into threads the problem is
that there's no study to even identify
what code is responsible for this stuff
right and there's a lot of it so what's
going on there and that's was the that's
some of the motivation for building up
on this parameter tool so what we're
going to do with parameters we're gonna
try and visualize concurrency and
dynamic trace data okay and we're gonna
look for the basic sources of
parallelism using the results of dynamic
analysis and hopefully going forward
we'll be able to integrate with the
dynamic
analysis the static analysis to say well
here are things that I can prove
statically will always happen and here
are the things that I can and this helps
programmers find out well let me see
what can't the compiled the static
analysis proved oh yeah I can't prove
this okay but I know that this happens
to be true and therefore I'm going to
fix it in this fairly simple way or oh
this is not true this is not real
parallelism and it can inform the tool
and then you can take that out of the
search space okay but the key insiders
will see is that in order to do this
analysis we're going to need an
analyzable compressed representation
because the biggest problem that you
face here is the size of the data sets
that you have to analyze okay so the
basic plot that we use in parameter is
what we call the din vs. ready plot and
so what what is the Dan Wheldon is
that's just a dynamic instruction number
given a trace
what is its position in the trace so on
the left side here we see a subset of a
trace of a program that computes the
Fibonacci sequence so it's just a toy a
toy example okay and so because of how
we plot this the first instruction in
the trace is down here and the later
instructions in the trace go up okay so
what we're gonna do if you look under
non ideal circumstances on the
horizontal axis of the plot we are going
to plot the earth at the ready time
right when does the instruction execute
in clock cycles okay and in the naive
case every instruction in executes in
its own individual clock cycle and so
you just see this dying a line going for
however under ideal circumstances no
branch worries no memory delays no no
processor with limitations you can see
that you're limited by your data
dependences and so these first four
instructions have no data dependences
and so it can all be scheduled in cycle
one right the next set of instructions
are all scheduled in cycle two because
they somehow depend on this instructions
in cycle one notice in these types of
plots by the way instructions are as
pushed as far to the left as you can
because if there were no data
dependences then it would have moved
over
even further okay and so we go on like
this and we plot this thing and if you
look generally if we were to zoom out on
this you see that these instructions
this depend this forms a diagonally I'm
from lower left upper right which
corresponds to a chain of dependent
instructions or a collection of
instruction that are all dependent on
each other as you progress through time
all right so this isn't by itself very
interesting but now let's zoom out and
so I apologize if this is a little light
we've fought with what the best way to
represent this data is and so in a
presentation but this hopefully you can
see enough to see that this is a plot
it's the same plot that you saw before
but for 254 gap from the spec benchmark
suite which is a sequential application
right it's it's not one of these loopy
scientific codes
it's a sequential serialized application
for the most part and so what you get
here is again the first instruction in
the trace starts here and instructions
are numbered as they go up and then on
the horizontal axis you have time if you
look at this graph you'll notice that
there are these spidering lines that pop
out here right these are multiple
dependence chains and they're
overlapping in time which means that
they are potentially threads that you
could extract and exploit there may be
dependences from the left thread to the
right thread for example and that's fine
because that forms a pipeline structure
right if you extract these threads
they'll form a pipeline structure now
these are under ideal circumstances so
not all of these things are gonna be
extractables threads so we want to ask
the question okay well what are these
things what instructions are responsible
for this what part of my static code is
actually executing in parallel here I
don't know and today no one can tell you
right and the reason it is because you
have nine billion instructions here on
the vertical axis you have a hundred and
twenty five a hundred and fifty million
cycles right now how do you keep this
data around and do a data analysis write
the full 56 gigabytes of data or
required just to list the program
counter locations of all the
instructions forget what addresses they
access right forget about what
directions the branches went in and so
on and so forth right and any summary of
the dependents data is gonna be huge -
okay so how do you store these and how
do you analyze this data and this is the
problem that we're focusing on a
parameter so if you look at what happens
today we have a trace compression
routines for example by Berkshire Martin
Bertram at Cornell and if you look at
that work they get really good
compression results I can take that
terabyte trace and I think oppress it
maybe 250 gigabytes or less so I have a
terabyte of data with all this stuff and
I can compress it to 50 gigabytes the
problem is to do any analysis I have to
decompress the data I don't have to keep
it on disk but I have to stream through
the entire decompressed representation
50 gigs of data it'll take me you know a
minute or so to reach the billion
construction maybe a little under right
to reach the two millionth like 15
seconds or so just to get there and then
I have to summarize all the information
for what came before if I want to do any
type of data analysis on this
you have better approaches like sequitur
that was originally proposed for
compressing gene sequences and Jim lares
at Microsoft came up with a using it for
representing program paths the problem
is it's grammar based and it's really
only good for sequence matching so I
want to find sub sequences of code okay
now using these types of techniques this
compression and then using a special
plot that shows the distance between how
far is your ILP so notice the vertical
distance between those two threads on
that original DIMM plot tell you how far
part of far regions where things ran far
apart in the trace but are actually
capable of being executed in parallel so
looking at that distance plot we came up
and we said hey there's this place in
this propagate block function in GCC
where I can execute a lot of stuff in
parallel and this goes on for a while
there's some cross dependences that you
can implement as pipeline forwarding
through pipeline stages and then there's
a recurrence there's a lot of this stuff
okay so that's great you say well
there's no problem look I can identify
parallel structures in this code well
guess what Craig Zyliss and
papers on speculative threading the same
example Princeton folks to coupled
software pipelining the same example
right Guri so he I think in a paper
after Craig Zyliss graduated same
example alright so why is everybody
using this particular example in this
particular function well the answer is
it's really easy to find right and the
the fact that everybody uses it it's
likely the only example you can find
because we looked hard it was the only
one that we could find and he said look
we have a problem here this is all we
can find based on all the state that
we're able to collect we really have a
data mining problem here how do we
actually look at this data and explore
what the heck is going on okay so all we
said is well we need a better way to
represent the data right and if you look
at it with the we want to represent the
data and compress it in a way where we
can still analyze it well what are the
types of analysis we want for finding
parallelism what we want trace data flow
slicing right so we want to be able to
look at here's a set of instructions who
needs their values show me who needs
their values right that's a forward data
slice show me where these values come
from that's a backward data slice
because this is going to give you
insights under which dependence is you
need to break to improve the amount of
parallelism available in your program ok
you want to do control flow slicing to
find out where the control flow is
really limiting what you're trying to do
here right you want to also be able to
visualize things like do the trace
visualization you want to visualize
these slices you might want liveness
information to find out well if i
paralyze these two things how many data
values do i need to keep live before i
can so that the later computations can
use them that's important if I need 50
gigabytes of RAM to keep my datasets
live in this parallelization that's
probably not a good idea I'm gonna have
to swap two discs right on most systems
now maybe in five years I won't have to
because RAM density will double a bit
fine there are so other applications
where you can think of this so what
we're going to do is we're going to
represent traces using a set based
representation and in particular use a
compressed form of BD of binary decision
we're gonna take boolean functions we're
gonna take traces represent them as sets
take those sets represent them as
boolean functions and then take those
boolean functions and represent them as
binary decision
that's right so that's the change so
let's start in this chain and this will
see we get good compression in the
analysis on those decision diagrams is
relatively fast so let's start so first
we need to understand how do we
represent a trace assets this is based
on the idea of characteristic or
indicator functions so I have this
function here Phi V a of X and we say
that this function is equal to 1 only if
the element ax is in the set a so if we
have a set a and you know has dog cat
snake ferret whatever and we say fee a
sub a of cat well that will be 1 because
cat is in the set but if I were to
choose say iguana fee of a of iguana
would be 0 and we see that this function
is called the characteristic function of
the set a because it tells you what's in
and what's not the set it uniquely
characterizes the set ok so now let's
take an example of how we do this with
trace data well we have our dynamic
instruction number which is just a
number from 0 to say 2 to the 64 minus 1
and we have a program counter location
in the most basic sense what is a trace
well it's a copy it's a list of program
counter locations that you executed ok
so all we do here is we say well if we
have the set of tuples of all program
dynamic instruction numbers and program
counter locations we have a trace so
program counter value 0 right PC value 4
1 5 and we can extend this as far as we
need to go well in this case now this is
a set this set a corresponds to a very
very simple trace ok great what's the
indicator function for this well fee of
0 comma 4 is going to be equal to 1 fee
of 0 1 comma 5 is going to be 1 and fee
a of anything else is going to be 0
because there's nothing else in our set
right so is this clear because if I move
on and it's this not clear then then all
is lost so hopefully this is clear ok
great so excuse me so now how do we
represent sets as boolean functions well
this is pretty straightforward for
people who haven't seen it before but
I'll take a little bit of time to
explain it if you haven't right so if
you're in the pl space I suspect that
there are a lot of things that
look a bit like this because people have
done work on using binary decision
diagrams for analysis and so you've seen
it but if not so if we have a set a
that's the same set that we had before
and we know what its indicator function
V sub a looks like we can represent it
as a boolean function by encoding the
elements using a boolean representation
so let's say that my dynamic instruction
numbers can range from 0 2 through 8
well then I need 0 through 7 then I need
3 bits for my dynamic instruction number
and I'll call those bits I'll call that
vector X right and then my program
counter I'll have a 3 bit vector called
Y okay so now I can encode 0 4 as a
tuple as a tuple on these variables as 0
0 0 1 0 0 right
pretty straightforward any questions so
far on this alright so pretty
straightforward 1 5 I can do the same
thing
now I take fee a X Y well what does this
look like well I just encode as a
boolean function right the fact that X 2
is 0 so this first term encodes 0 comma
4 well X 2 X 1 and X 0 or all 0 so it's
not X 2 not x1 not X 0 right and I do
the same thing for the other things now
if I plug in into this equation 0 comma
4 what would I get that's 0 so this is a
1 0 nodded 1 0 nodded 1 1 not nodded
that's a 1 0 nodded is 1 0 or not it is
1 okay so these are all ones this
conjunction turns to 1 and therefore the
characteristic function evaluates to 1
so I know 0 comma 4 is in my set
pretty straightforward okay so we can do
other trace data in this way too right
we want the data dependence graph to do
slicing no problem right that's just a
set of tuples from egos V cross V where
V are the vertices
well the vertices what are they they're
the instructions which we can represent
uniquely by their dynamic instruction
number right so darn dean versus ready
time that graph that we saw earlier same
thing the ready time is just another
number okay and pretty much any
arbitrary data can be encoded in this
way now we want to compress this data
into a compact representation for which
we use a graph data structure called the
binary decision diagram
and the binary decision diagram works as
follows the best way to think about it
these are proposed by Randy Bryant in
the mid 80s and have been used
extensively in Hardware verification as
compact representation for boolean
functions that represent transition
systems and graphs right which is
exactly what we're trying to represent
here so now I have a tree here ok and
this is just a boolean tree for some
arbitrary function in three variables XY
and Z the way I read this is I say well
what is the value of the function for X
equal to 0 y equal to 0 Z equal to 0
well I just traverse the left edge of
this tree write the open circles
represent the arcs that correspond to
when that variable is false right so now
I just traverse the left edge and I find
Oh
f of 0 0 0 well that's 0 right but if I
change the value of Z to 1 I get 1 ok so
this is a graph structure but the
problem is we all know this is
exponential in the number of variables
so we want to have we want to compress
this representation down so what we're
going to do is we're gonna convert these
trees into what are called reduced
ordered binary decision diagrams by
ordered I mean that the for every level
in this in the final graph that level
always corresponds to the same variable
so in this case the top level
corresponds to X the middle level Y in
the bottom level Z and that's called the
ordering so the ordering here from top
to bottom would be X Y Z now let's
compress this into a binary decision
diagram first thing we notice is there's
no reason to store a node that
represents the same function twice one
the constant function one we only need
one node to represent one so we're going
to combine all the nodes that used to go
to one into a single node one right and
all the arcs are going to be redirected
there so we can do the same thing for 0
and we get this particular graph second
observation is there's no need to have a
node if both the true and the false arcs
go to the same function in this case
notice that this true and false are go
to this function 1 so it doesn't matter
what the value of Z is once you get to
this node right X 0
why equal to zero and I'm missing a
circle I apologize y equal to zero goes
here well they both go to one so who
cares what the value of Z is I can
remove those notes so I'm gonna
eliminate them okay
so eliminate those nodes and I end up
with this particular graph here the
third optimization that you perform is
to observe that if there's a function in
its complement there's no reason to keep
both those things around either I can
just mark an edge as an inverting edge
right one way to look at this is that
this is a multiplexer these are the
control signals on the multiplexer and
so I just multiply it's one of these
signals up and I go through the tree and
at the output I get one value right well
I can also just stick in an inverter and
not have a multiplexer tree that just
has the bottom elements flipped around
so if I do that observe that this
function and this function are opposites
of each other right they're inverses so
I can eliminate one of the nodes I can
eliminate one of the nodes there we so I
can eliminate one of the notes and then
I mark the false edges inverting now
just like you can form just because you
have things like and and not or complete
and therefore I can represent or using
nots and ands similarly here we have a
problem that there's not no once we
answer to introduce these inverting
edges the graph for a particular
function is not unique but we want to
keep this representation canonical which
means that because it's important for
our complexity results and so what's
been proposed in the literature is to
say well only the false edges can have
these bubbles on them and if you do that
and you think about it it turns out that
then every graph is unique and and you
can represent every function so notice
that 0 and 1 are inverses of each other
as well and so we'll eliminate them and
then redrawing the graph a little bit we
end up with this BDD now because BD DS
are canonical it turns out that say I
have the concept function 0 it always
has the same representation which is
just the constant it's just an inverting
point
to the constant node one okay so what
does this tell you well if you're
painting if you're paying close
attention what this tells you is it must
not always be possible to build these
BDDs efficiently because if it were I
could solve the satisfiability problem
simply by encoding my sad problem as a
BDD finding out that it's a zero node
and saying its onset right and then P
would equal NP NP right and I'd be
accepting my touring word right so or
Randy Bryant whatever whoever proved
that this were the case so turns out
that not all data has good BDD
representations okay and in particular
these can blow up exponentially so what
we want to see is in practice for our
trace dated you are they a good fit okay
and it turns out that this problem is
complicated because the ordering heavily
controls the size of your resulting BDD
and you have to come up with a good
variable ordering for the problem at
hand so here if you notice this is an
order where Z is on the top and X is on
the bottom and this BDD is the same
function as this BDD but it only has
three nodes in general as you have more
variables the amount of compression that
you can get can be exponential relative
to the truth table or you can be on this
linear in size of the truth table and so
you want to be on that exponential
compression side and not on the linear
side okay so what we've done is we've
done a lot of work to come up with what
is a clever ordering and I won't get
into the details I'll be glad to answer
questions on to what the clever ordering
is and what it's not in this case but
with this particular ordering we're able
to store in about inning so what we see
on the graph the vertical axis shows how
many tuples of the data dependence graph
for a trace we can store right because
that's interesting information the dim
comma PC okay that's less interesting we
can store a lot of those too but this is
an interesting piece of data because
it's derived data that's hard to come by
normally and the vertical axis shows how
many of those tuples we can store we're
storing a 64-bit value for each side of
the tuple X comma Y so each of these
naively is 16 bytes large ok but in a
gig of ram what we see in the black bar
is how much we can store in a gig of ram
how many of these can we store you can
see in the peak case we can store almost
for bill
instructions in one gig of ram okay on
average you're getting around a billion
instructions per gig of ram right this
is a 16 x to 60 X compression ratio it's
pretty good right
it's not great there are compression
algorithms that do better but remember
the purpose here the purpose here is not
to get the best possible compression at
any cost
the purpose here is to come up with a
compress representation that's also
analyzable okay and so that brings us to
the next result so we said slice
analysis is an interesting analysis so
let's look at slicing if we look at
slicing here's a fragment of the data
flow graph okay and here is a pseudo
code for slice analysis as performed on
BDDs and let's go over this and see how
it works and I'll highlight the
important complexity results okay so
what a blue node is a node so what I'm
going to do is I'm going to build up the
dependent slice for a set ID so ID is my
initial set and these are a collection
of instructions and I'm gonna find out
what are all the instructions dependent
on this instruction okay that's the goal
that's forward slice analysis so ideas
an initial set and so if the node is
been asses the set is the slice set so
it's the forward slice as I compute it
and at the end as is all the
instructions that depend on ID including
ID okay so let's start we start at the
algorithm and ass hold is equal to 0
fine we don't we're not really worried
about that right now so it's the empty
set right and we start s as ID so what
is ID well I'm gonna pick a node here
arbitrarily to be ID and so s starts out
as the singleton node right here notice
that it can be a set of nodes however
and the algorithm functions just fine
internally will see that we create sets
of nodes how do we represent set so we
use BDDs so don't forget every one of
these variables ID it's represented
using a BDD s it's represented using a
BDD everything we can show all the
variables we construct in here our BDD
variables okay so now what I do is I
need to compute the output edges so I
have the set of tuples and I have the
set of states and it turns out that if I
intersect them the set of edges with the
set of
initial I call them States because
that's what they always in the harbor
verification world they're called States
but they're their instructions here if I
take the intersection of e with the set
of instructions that I'm worried about
it turns out that I get the set of
outgoing edges from that okay and so I
get the set of outgoing edges well these
are on BDDs how long does that take
turns out the operation is polynomial
time in the size of the binary decision
diagrams and can be reasonably fast now
saying that it's polynomial time is a
little bit deceptive because these BD DS
are large and so it can take some time
but it's still much much better than
streaming through the decompressed
version of the trace right and I'll give
you an example I'll show you some
results where it shows that the actual
times that these takes are reasonable
okay so that's polynomial time in the
size of the BDD
next thing I have to do is well I have
the edges but I want to find the output
nodes so this offer this existential
quantification here what it does is it
strips out the first part of the tuple
right if I have the set of edges that
lead to where I want to go and I strip
out the source the destinations are the
next the deep head the first set of
dependent instructions all right so I
strip those away and I add to ask those
nodes so I get these new nodes in my set
apps right my forward slice after this I
do a rename operation for tÃªte which you
need for technical reasons because I
have the second half of the tuple and I
need to rename everything to get the
first half of the tuple so that I can
repeat the computation right so I do
that rename then I say did I add more
nodes to my set yes okay try again right
so now I compute the next frontier slice
so I do it again
I find the outgoing edge set but now
notice that s is a set of nodes he is
still my total edge said I do the
intersection and I'm gonna get a whole
bunch more edges okay in the note the
complexity of the operation is the same
doesn't change as complicated as it was
before so now I do that and I'm gonna
get a whole bunch more nodes okay and
I'm gonna say did I add more nodes yes
it did so you go again go again
and this time I didn't have any knee
nodes so I'm done and I returned ass
this is the Ford slice computation right
so this is pretty simple all these
operations are polynomial on the BDD
some of them are really cheap this is
linear in the number of variables I have
this operation can be and the other
operations can be pretty fast the check
here are things equal because BDDs are
construction on achill it's constant
time I just compare the two pointers the
same know then they must point to
different functions which means the sets
are different so it's really really
simple
okay so I do this and the comparison to
make here is that doing slice analysis
on BD DS takes a few seconds for a gig
of ram doing them on the compressed race
and streaming through to find the slice
can take minutes that's a big big
difference right in particular for a
particular case if you look to say take
the two millionth instruction right and
to compute the forward slice of two
thousand instructions takes under two
seconds using our approach just
streaming and finding the two millionth
instruction and in normal trace takes
over fifteen seconds okay and so if you
do the two billion beasting skip the old
approach is linear so now I have to take
ten times longer 150 seconds our
approach two seconds right so we're
really good for the analysis once you
can build the BDD alright so in summary
sequential programs have a lot of
parallelism
despite misconceptions but no one knows
what that parallelism really is and
we're trying to figure that out it's a
parallel parameter gives you
visualization and analysis of large
traces to help us find where that
parallelism is now I really wanted to
have a demo of the tool here and but you
know how things go on software
development we have bugs and it's a
little clunky right now so we're working
on it I'm hoping in the next month or so
we'll be able to have a good demo that
shows interactive visualization of
traces and then get some really cool
results on how to parallelize
applications and I said I really wanted
the demo here and I apologize that well
you know how these things go right how
long is Gmail been in beta
all right so but in summary we get
sixteen 260x compression and we get
analysis without decompression that's a
key factor right
no decompression for analysis the
complexity of the analysis in terms of
the compressed size not the decompressed
size okay so this translates the second
versus minutes for slice analysis so in
general where's this going right how can
you use this these I told you these
threads look like pipeline parallelism
what does pipeline parallelism get you
anyway so let me give you an example of
some work that John is done we're
looking at Network processing and we're
trying to process say gigabit ethernet
frames and you're trying to subvert
denial of service attacks or something
like this well in these scenarios you
need to process every Gigabit Ethernet
frame and you want to do it at line rate
with no drops right that's ideal that
means you got to process one point 488
million frames per second or right
that's a lot of frames that's coming in
they're small there's 64 byte frames
you're not bandwidth limited in this
process you're but you're limited by is
the overhead of processing each frame
stripping the header finding out where
it comes from so on and so forth right
you have at that rate 672 nanoseconds
per frame 1200 clock cycles main memory
how long does that take one access to
main memory right hundred cycles right
200 cycles a system call will cost you
200 cycles or more right 600 cycles in
some cases thousand and others okay so
so it's higher and higher right so
that's not a lot of time and in fact
it's not possible as far as we can tell
with one processor to achieve this but
you can build a pipeline structure and
really accelerate this so what you do is
you break up the application in three
stages which is what we've done you have
the input phase which deals with the net
the network controller and pulls packets
in frames in the app phase which
actually does work on them and then the
output phase which blasts them back out
okay now cool thing about this and I
don't have time to talk about this does
the input stage runs in kernel this
application stage runs and you can run
user space it doesn't have to but it can
in any case by doing this now each of
these stages gets dedicated
processor and then I just run the
process run frame after frame notice
once the pipeline gets going so this is
time on this axis I forget I'm in
architects I think everybody knows how
to read these diagrams this is time on
this axis on the vertical axis frames
that were processing on the frame axis
on this axis notice that this frame
still takes T time time T it doesn't get
any shorter the latency is the same but
our throughput is tripled and the reason
is because once this pipeline gets going
right notice I can overlap a pan IP here
I'm overlapping the input reading one
packet while I'm processing the previous
one
while I'm outputting the one before that
okay this is built in software we have a
working implementation hard thing here
is that you have to transmit data from
these guys to each other really fast we
have a mechanism with no special
hardware support on commodity hardware
can do that transmission in 35 to 40
nanoseconds that's really fast right I
mean I can say it but think about it 35
to 40 nanoseconds that's less than the
time it takes to access main memory
right and we have a clever way of
arranging the caches and using
prefetching to overlap the transfer of
data with other useful work okay and so
with this as far as we know on a dual
core dual processor and the Opteron
system we can hit the full well 95% of
the full frame rate we can probably do a
hundred percent the reason we don't know
is because our network cards can't
generate a hundred percent so all you do
is you attach a kernel module pin it on
to a process and tell it blast packets
out of this network interface we can't
get up to a gigabit so we're at 95% I
don't see why we shouldn't be able to go
to 100% right really good this is
pipeline parallelism and it's a
mechanism of using pipeline parallelism
for performance benefit so in conclusion
you know processors are at the end of
their life and we need to utilize
multiprocessor systems we happen to
think that pipeline parallel systems are
a good way to do this but there may be
other types of parallelism but
programmers in any case need a tool
chain to help them identify parallelism
implement verify and verify and finally
you need runtime system support for
example we need to be able to have a
user space application have a pipeline
stage in the kernel that's
pulling things from the network that's
one example of runtime system support
and we're working on all of these areas
you've seen parameter I briefly
described fast forward which is that
high-speed communication mechanism right
and then we have work in these areas
I'll be glad to talk about these are
much less mature than the first two so
so that's all I have to say are there
any questions I think I went a little
over time and I apologize so the
pipeline parallelism example if if you
took a bus bushel of smart engineers
you've probably two or three of them
would have tried that themselves so can
you explain a little bit more detail
what the Parra Meador system would how
it would help them in in improving that
above and beyond the the basic intuition
of tried pipeline trail isn't oh you
mean the network application or the
pipeline example you showed yeah so
Network application so I think that
there are applications for which you
it's a no brainer to figure out the
pipeline structure and in that case why
run parameter right there's no point
there are other cases where it's a lot
less clear how you can build pipelines
so one of these applications that we're
looking at and we're not doing with
parameter actually but is looking at
certain scientific codes and seeing if
we can speed up the core kernels that
are not parallelizable or hard to
paralyze and see if we can parallel wise
them using a pipeline structure but
there are the best work to point to is
work out of say Princeton University
where they are automatically pipelining
code in tight loops and it's really hard
to see they build the data flow graph
they partition it in a strongly
connected components and each of those
strongly connected components becomes a
threat it's really hard to see what that
parallelism parameter will show you that
parallelism and it'll show you
parallelism that's coarser grained than
what they're able to find in that
particular scale for example this
propagate block and it escapes me now
what GCC is doing there but if you look
at propagate block and what's going on
you wouldn't you're looking at the code
you wouldn't say oh yeah duh I can
parallelize it but when you look at it
after the fact and you see what it's
done like oh yeah
that's pretty clever I could paralyze it
in this particular way so it's hard to
give examples until we have a working
tool and we'll gladly publish a paper
but certainly our hand inspection has
shown that there are structures that are
paralyzed in there that you think what
how is this going in parallel and then
you guys oh now there are shortcomings
two-parameter right one of the reasons
parameter won't show you the networking
examples parallelism and the reason is
because it's an application-level tool
but notice what we've done we've
overlapped OS services along with the
application so it's something that we
would like to extend so notice that
parameter has its limitations but it
also has a strength it's going to show
you these things internally that are
difficult to find yes how do you handle
the memory dependences in the trace oh
we when we record the trace we log the
addresses that are actually read and
written by every instruction and then
when we build the dynamic data
dependence graph we keep that into
account to build the dynamic data
dependence graph we run through the
trace and as we do that I have this
inverted page table implementation that
cluttered so that it can keep around all
the dependences in some type of
reasonable way without having you know
what I can't have a two to the 64 bit
flattered right so using an inverted
page table and some clever algorithmic
hashes in there I have two levels of
hash and it'll help me track the
dependences as I go along so that's how
it's done
I can even ship you the code if you if
you're that curious about it it's not
that bad so it appears to me that the
compiler the compiler should be able to
resolve most of your dependencies
anyways for example if you have regular
loops or there's no loop carry
dependence ease you get like really long
traces but the compiler knows exactly
what's going on there so any plans of
incorporating that kind of information
well okay so there are two questions in
one there right so the question was the
compiler knows a lot about the
dependencies you get these really long
traces and the compiler knows 90% of the
time maybe not 90% but for a bulk of the
time knows what's gonna happen there
so the one question is can you use this
information for compression and the
answer to that is yes but it's not clear
to me yet how that would affect the
analysis routines right so if I use that
information in compression will to
complicate the analysis I don't know
would it lose certain granularity to be
able to pull apart the structures I
summarized I don't know
so for compression we've thought about
it but so far I've stayed away from it
because everything I showed you in terms
of these representations is naive it's
the first cut and there's a lot of
research and how to make these
representations even more efficient that
come from the harbor of the world that
we want to map in so on the compression
side it might be a while on the help the
programmer figure out what he what what
to look at what he or she should look at
on the other hand we are planning to use
static analyses the biggest obstacle is
because of the scale that we're looking
at pointer disambiguation is really
really important and the existing point
pointer analysis techniques are decent
so we have something coming online on
that front hopefully soon we have the
point or analysis working and everything
but we don't have it the two tools
integrated together so once we integrate
them hopefully we'll get some data going
forward then you need better pointer
analysis and we're looking at combining
points to analysis with abstract
interpretation to help resolve some of
these details that's still in its
infancy what's that good luck with that
yeah but but well if you look so so the
reason why we think it's promising is
because abstract pointer analysis can be
cast as abstract interpretation problems
and that's why we think well maybe you
can actually combine different abstract
domains and actually do things with them
doing it in the largest hard part
another small question is enjoying your
experience how do you see any
correlation between ILP and TLP such
that you know if you have applications a
lot of ILP those other your prime
candidates for TLP or anything okay
there is a tight relationship in certain
cases between so the question was is
there a relationship between ILP and TLP
and in certain cases there's a tight
relationship
so some anecdotes about people doing
automated compiler analysis to extract
some of these pipeline parallel
structures and it turns out you have to
very carefully balance what you consider
ILP and what you consider TLP because
otherwise you can do a threading where
you steal all your ILP into TLP and then
you're not really utilizing the
processor core itself very well and you
gain almost nothing and you have a much
more complicated program to boot and
you're using two processors so you had
to be careful not to take away things
that the processor is good at executing
as ILP and then turning into TLP that's
useless you have to make sure that
you're grabbing the things that the
program compiler wouldn't be able to get
from ILP parameter will help you do that
because it shows you this distance in
the trace and it can show you the
control dependences and so you can say
well what is likely to be extracted as
ILP and what is not and then you can
help pull those things apart in that way
but it is it is a point of concern and
if you're not careful you end up doing
these useless parallelization zhh can
the tool be used to show me that I may
have some parallelism if I can break
this particular dependency we don't have
a good visualization for that yet but
yes and in particular what you can see
by doing the slice analysis and slice
visualization as you can see that
there's this very narrow thread that all
converges on one point in fact if you
zoom in onto some of these graphs and if
you recall the paper we presented at
epoch 3 maybe or epoch 4 we show that
there are these critical dependences and
in that din plot you can see there are
these critical dependences where all
these computations converge to compute
this one value and nothing else can
happen till you compute these like one
to ten values and then everything
diverges so you can already see that
information what we want to do is have
these what-if scenarios and a
visualization for it you say well what
if I were to break it what would happen
and then if it just backs up by one
cycle because there's now some other
critical dependence well then you know
ok maybe that's not so good on the other
hand if you could break it for real you
might get a lot more parallelism out of
it so we're working on that but first we
had to get the initial stuff up and
running so yes but it's
area okay one last question I've worked
on kernel tracing so we have a tool I
call the Linux trace toolkit viewer
which is basically using this tape
format uncompressed data but well we we
do time-based indexing of this that time
periodically dump well in that kind of
scenario that would be lets say for
politically we would dump for all the
variables we would say we we keep track
of the last accesses and this helps us
seeking quickly in the in the data the
come the comparison you've done between
your compressed format and the the tape
based approach as I think it's based on
the fact that you have to stream along
all all this data before you can get to
the actual data that you need so what
are your thoughts about well having this
kind of fast seek in in the raw data
would that diminish the difference
between so so the question is you can
store these traces on using time indexes
so that you can randomly index into sub
parts of the trace right there are these
fixed points sort of like that what are
they the P frames or the eye frames or
something in MPEG video right you can
jump to any of them okay so my feeling
on that is as far as as far as the
example I gave it would make the example
less compelling certainly and so in some
sense it's not a fair comparison that I
gave on the other hand the general
principle so there are two issues with
having these time index databases one is
that there's a limit to how much you can
compress them okay because you can't
compress across that time index because
if you do you can jump to the time index
but you can't decompress from that point
forward right so you can't use previous
things so it limits the rate of
compression and it's really important
for interactive analysis to get the bulk
of your data in RAM or you just have a
giant cluster behind you and I knew
click on it right which is the case that
you guys have the problem with the giant
cluster scenario is that I want to often
do analysis of my analysis results and
in this case what we what when we build
we build the you do a slice analysis we
build a BDD of that and you can just
treat that like another trace and now do
all your analysis inside there
recursively which is is a useful thing
and the time indexing doesn't really
help you do any of these types of things
right and then the big cluster behind
you also once I have a slice and I want
to do things really fast on the slice or
work on a lot of instructions at once to
figure out what's going on the time
indexing thing even though the big
cluster behind you is not going to be
competitive with something that's an
in-memory approach on one workstation
now having said that their analysis were
the time index database and having the
cluster behind you is actually better
than what we're proposing so you know I
you'll get I'm not gonna say that it's
the be-all and end-all of everything but
it is it is very good for a large class
of analysis that we want to do right
because we do have we do have some
clusters that see you where we can we
can you know get maybe not hundreds but
not thousands but hundreds of you know
hundred machines or hundred processors
working on the problem and it's not
clear to me that that's gonna really get
you what you need all the time so does
that answer your question
well basically with the time index what
we can do is we whenever you seek to a
precise location then you have well all
the data about the state of the system
at this moment is supposed to fit in
memory for quite large traces if you
have a limited set of variables right
and then the information you're
interested into is located between two
consecutive time intervals so basically
there's no real need to either cluster
behind you to deal with that so that's
that's an example of analysis where your
time window you have time windowed
analyses right where you don't even need
a cluster and the time index database is
pretty darn good we're not bad in that
case but if that's all you're doing the
time index database is actually better
right if you're trying to do what we're
trying to do is look at the program as a
whole
because we're trying to pull different
pieces from very far apart in the trace
and then try and paralyze those and in
that case we've tried the time index
database as I said and in that case they
leave a lot to be desired and they don't
they don't scale very well with let me
look at a whole set of things let me
look at even more and more and you
eventually start pulling in you need
little chunks of tons of the little
pieces and and that's where cluster will
help you right but it's still not as
competitive as this approach so I said
they're good their pros and cons to all
of these things if people have other
questions I believe Manisha's around
this afternoon and has some time to talk
with people if you're interested just
come up and let us know otherwise let's
think
thank Manish for the interesting
presentations thank you guys for</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>