<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bay Area Vision Meeting: Learning Representations for Real-world Recognition | Coder Coacher - Coaching Coders</title><meta content="Bay Area Vision Meeting: Learning Representations for Real-world Recognition - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bay Area Vision Meeting: Learning Representations for Real-world Recognition</b></h2><h5 class="post__date">2011-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/L4WqSYbcbH8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our next speaker is a UC Berkeley
representative go bears that's where I
hail from Trevor Darrell is on the
faculty of UC Berkeley eeks department
and leads the computer vision group at
the International computer science
institute in Berkeley before that from
nineteen ninety nine to two thousand
eight he served on the faculty at MIT in
the EAC's department where he led the
csail vision in her faces group since he
joined us at Cal well them at Cal yeah
he's been working there is a visual
perception machine learning multimodal
interfaces and he's also doing some
entrepreneurial activity in Berkeley on
the side please help me welcome Trevor
Dale results in the last month this is
my one month old daughter and her older
brother so that's been her name is
linnea so that's been our happiness at
home and I think today I'd like to be
maybe a bit of the reconciler after this
3d versus 2d debate that we've seen
unfold and tell you a little bit about
the work that we're doing at Berkeley
and our perspective on using machine
learning to really bridge the gap
between sort of robotic vision
perspective that we've heard and some of
the more traditional computer vision
perspective so one thing to say is even
though we have seen a lot of progress in
computer vision there is still a long
way to go I don't think we're close yet
to having broad category level
recognition even in relatively simple
indoor environments or maybe we're close
may only be a year or two away but it's
it still can require some significant
advances and maybe it'll come from
bridging the gaps that we've seen here
today right so it's almost a bit of a
set up with it was just the discussion
we just had which is nice because I do
think there are divergent perspectives
in terms of how vision people
traditionally would see these problems
and robotics people would see these
problems so what's the vision
perspective well at least from them from
the morning it's really a machine
learning philosophy and that can often
be boiled down to who has the largest
data set wins right I mean algorithms
matter but data really does
there's a downside of that and we see in
the flip side when we look at how
roboticists approach the problem it
leads to the least common denominator in
terms of the features that one uses you
don't really know what color means on
the web so you're probably not going to
use it there's not a lot of 3d data on
the web yet so we're probably not going
to use it but there's a lot of data and
we can learn categories but with week
features what's the robotic point of
view well it's a sensing paradigm
sensors are cheap they're getting
cheaper why not just throw multispectral
imaging for different connects have
everything on there 3d models it's great
whoever has the most sensors win right
well it's sort of this is the flip side
of that coin if with so many sensors you
really can only get training data in the
particular environment that you're you
built your rig it's very hard to
generalize at least with conventional
methods so we have strong features but
really an instant recognition focus so
far so where do we go from here so which
one is right I'd like a show of hands so
how many people go for computer vision
how many people go for robotic vision
well no one's wants to commit here which
is probably the right answer neither one
is entirely right both are probably
right and the philosophy that I'd like
to advocate really combines many of the
different themes we've seen today which
including a machine learning foundation
which is let's learn to use both of them
right we're really not going to simply
adopt a 3d paradigm or a 2d paradigm and
so here are the kinds of what I mean by
that right we'd like to learn how to
leverage robotic and vision perspectives
together for example detect develop rich
low-level features that can handle the
kind of complicated sensing that happens
in the real world exploit machine
learning to learn mappings between
modalities so we may have a certain set
of modalities at training time and a
different set of modalities a test time
and still without any manual tuning or
engineering get something done and
finally learn the shift between domains
we may be trying to run a robot in this
environment recognizing objects that are
found on the fly but using training data
from an environment completely external
to this environment with different types
of imaging conditions and and whatnot
and we'd like to do better than a naive
approach would at that so there's a
sampling of different projects that i
could tell you about today ranging from
including hallucination and learning
these two point wendy representations
i'm not really going to have time to
talk about those two and I'm instead
going to focus on these three the first
is learning low and mid level features
using sort of a hierarchical
probabilistic sparse coding really
actually echoing many of the themes that
Andrew presented earlier today then I'll
talk about our work on domain adaptation
how we learn in one domain and test in
another and then I'll briefly also sing
the praises of the Kinect and related 3d
sensors and mention our effort to
collect a new data set for 3d category
recognition so let's dive right in so
the first let me tell you about this
work which is a probabilistic model for
recursive factorize image features this
is a cvpr 11 paper by Sergei carrier of
Mario Fritz Sonya feet learned myself
and it actually follows much of the same
philosophy that we saw this morning so I
really can I can go on both sides here I
can I can tell the Willow Garage story
or I can tell the it's all just you know
semi-supervised learning or were sparse
coding story we'd like to have a
distributed coating of local image
features especially the kinds of
representations that are useful for for
vision historically they've been coded
using relatively naive vector
quantization approaches to find these
visual words and as we saw this morning
approach is based on additive models
especially sparse coding have been quite
successful we've also looked at this
ourselves in terms of a probabilistic
topic model approach to an additive
decomposition of
for example sift descriptors so you can
see how you might take a local
descriptor and factor it into
constituent parts that might capture the
fact that there's multiple different
things happening locally and in an image
or an image patch so so we're also on
this bandwagon of hierarchical models
hierarchies are very important I think
everyone had a slide like this today
it's interesting you should didn't have
one in the Willows in the willow talk
but you yeah good so it's probably not
doesn't require much motivation to to
say that hierarchies have a strong
inspiration from biology that if you
look at psychophysics there must be
top-down influences in perception and so
we'd like and also that hierarchies
enable effective sharing of visual
features and that there's been a long
tradition of models in computational
neuroscience and computer vision that
have chipped away at this idea of
learning a representation and learning
in a hierarchical fashion but one limit
it and I won't have time to go into all
of these models one limitation of the of
many of the existing methods is that
they do so only in a feed-forward
fashion they learn they approach
learning and inference only from the
bottom up so if you have a
representation here where you've
apologized if you can't see my pointer
where you learn a set of sparse codes or
in our case probabilistic topic model
over image patches and then you go to
then explore a hierarchy of that or a
recursion on this representation where
you take the stacked output of those
activations and use them as inputs in
another layer well first you you learn a
representation for the first layer and
then you attack the entire problem again
and learn representation for the second
layer instead what we've explored in our
recent work is what's the what is the
benefit of jointly on optimizing across
the hierarchy so that you you don't in
fact rely on a fixed representation from
from the bottom layer when you're
learning the top layer but you can
optimize all aspects of the represent
asian jointly so this is our our goal
and our result in our recent cvpr 11
paper a distributed coating of local
features in a hierarchical model that
allows full inference and full recursion
so we derive our method based on
probabilistic topic models using latent
dursley allocation and we call it
alternately recursive lda or sometimes
hierarchical lda so I won't go into this
in detail and many of you have seen
these models before they've been used
envisioned in many places over the years
most famously they are used to describe
topics which are composed of quantized
sift descriptors or visual words and
that's not how we're using it here we're
actually built we're actually building
topics over the descriptor itself not
topics over quantized descriptors so we
have a representation for example of
sift and we form topics over the
individual cells in the in the gradient
histogram so our topics can be excuse me
our topics can be thought of as finding
structures that are additively combined
or transparently combined and we can
find visual is it so we can find these
topics which are similar to what we've
seen before in sparse coding but now
they're evaluated over local sift
descriptors or they're forming the
Constituent basis of histogram based
descriptors and here here's a
visualization of average images that
might give ride give rise to each topic
so our baseline is just the feed-forward
method we can express here an instance
of the recursive model and its full
glory and here's the visualization of
how you might jointly be estimating
topics over a patch and then topics on
top on top of topics for a patch we've
approached this with a gibbs sampler for
inference and
do use a few schemes for efficient
initialization and so far in our tests
we're able to show that this recursive
model that jointly optimizes both layers
and we'll go beyond two layers
eventually but so far that's what we've
explored is better than just
feed-forward initialization and it's
better than just a single flat model you
and when you control for dimensionality
and it compares very favorably to all
the other published hierarchical models
I think leading all that we've
considered to date and still does very
well compared even the best of the best
such as the work that Kai and others
have shown or earlier today and the
saline see methods from UCSD so these
are the kinds of visualizations we get
so the first part of the story is how we
can learn good models from the bottom up
we can learn them in a hierarchical
fashion from data and when we we find
that when we do so we get better
performance jointly optimizing the
representation rather than just doing a
feed-forward and there are many
different future directions we're
considering including pushing the
hierarchy all the way up to the object
level looking at a spatio-temporal
volume and training it discriminative li
and considering non parametric models
you still hear me if I walk away from
the podium oh good good so the other
okay so that was the first the first
theme today the second theme is domain
adaptation which is what happens when
you train on one thing but you test on
another what you see is not always what
you get envisioned and this is the work
of Kate senko and Brian coulis who are
postdocs in my lab also with Mario fritz
and kate is jointly appointed at Harvard
and at Berkeley now that's a commute so
people are very good at a wide range of
visual category problems how'd you can
recognize these mice in many different
domains
even if I show you examples of Mickey
Mouse you could then find the matching
mouse in a real image but humid machines
often suffer even with very simple
transformations so even if you just go
from one type of image sensor to another
image sensor you can find yourself in a
very different feature space and you can
also find very different domains just by
looking at objects at different scales
or in different poses illuminations or
backgrounds or when you're moving from
one domain which is more artistic to
another domain which is more
surveillance for example or seasons so
we'd like to overcome this problem of
training in one domain and testing
another domain so in most object
recognition paradigms we assume that we
have essentially the same distribution
of training data and testing data so we
assume that we have a lot of label data
and we get a you know nice image of some
instance and we just go back and compare
what does this look like using some
fancy machine learning algorithm and
that's fine this is what most all of
existing computer vision research has
been exploring recently but in the real
world you go out to take a picture of
this cup which is taken in a completely
different domain the temperature just
changed about 10 degrees I apologize for
the video which will now suffer as I
stand over here so so we'd like to be
able to recognize the cup on the bottom
which is taken from a completely
different domain and the sad truth is
the existing methods don't work right
out of the box in fact if you just try
and train on one domain such as objects
in an office but train on images that
were collected outside of that domain
such as Amazon or the web you know if
you train on Amazon and test on Amazon
life is pretty good but if you then try
and train on webcam images all hell
breaks loose and
you lose anything close to good
performance so what's the solution to
this well one solution is just to ask
everyone to label everything in every
environment that's that's a maybe the
conventional approach but it's awfully
imposing and that's not what we'd like
to do it's too expensive even with
Mechanical Turk you could try and
engineer features such that in that
feature space everything is invariant to
everything that could go wrong and
that's actually probably the
conventional approach if you think of
what the representations we use today in
fact do they've been hand engineer to
try and be invariant to illumination and
and the kinds of pose variation that
don't seem to matter but they don't seem
to solve the whole problem so there's
obviously some part of the problem that
we can't hand engineer and we should
probably turn to machine learning to
solve that part of the problem so that's
our idea try and learn some notion of
domain shift from from the data right so
here we have an example where we have
instances of different categories in
different than for example here are
three different categories but they're
in different domains so for example I
have circles both taken in the green
domain and in the blue domain and
squares in the blue domain and squares
in the green domain and and and stars
and here for the stars maybe I actually
have labeled data in both domains rather
stars I have no label data in both
domains but for the others i do so i can
i can actually give i can do something
with the stars I can take advantage of
the fact that I know that that these
stars that the circles and the squares
had corresponding elements so I know
that that there were squares over here
in the blue domain and over here in the
green domain and circles over here in
the red and the green domain and over
here in the blue domain so it looks like
there's some sort of domain shift that's
going on in this space and much as I can
optimize representations for specific
categories here I can optimize a
representation for a specific domain
shift so I know I don't really want to
change things in this direction but it
does seem like changing things in this
direction is useful for this for
whatever transformation is happening
between feature spaces between these two
domains and so that's the crux of our
idea we'd like to have it some sort of
feature space transformation that maps
from a raw domain to a domain in which
door to it from a raw space to a space
in which domain shift is minimized so we
can frame this as a transformation
learning problem given pairs of similar
examples much like in traditional metric
learning we can say let's find a
transformation of a space such that the
distance between examples that are in
fact the same but in different domains
is is small using our learned
representation but with a raw
representation it would be large so how
do we learn w well this is where we put
our machine learning hats on and take
advantage of a variety of interesting
approaches that have been applied
prometric learning and other forms of
transformation learning the work of been
geo here google has inspired us in the
past but to our knowledge no one had
really applied this for domain
transformation where you try and solve
an OP an optimization problem that
balances a regularization against
constraints formed from those similarity
links so there's two ways you can think
of forming these constraints one is
based on category where you say these
these are two cups so I'm going to form
these links between these cups these are
not cups this is a cleaner so the
important thing is that you don't if
you're going to do to learn a domain
transformation you actually don't want
to do the traditional metric learning
thing which is to have a intraclass
similarity constraints and and
dissimilarity constraints across
different classes here we only want to
learn the domain shift reacts you don't
want to learn anything related to a
specific class so that's meant much of
the art of this
idea is and how you form those
constraints you can also form them
across instances if you happen to know
that this is exactly the same
corresponding object that's the best the
best case for this domain shift idea
okay so and there are ways to learn
these transformations efficiently even
with Colonel eyes or Colonel eyes
version of these transformations and we
both looked at a symmetric
transformations and most that was an ECC
V paper and our cvpr paper is a
symmetric case so there is a stream of
related work in domain transformation
modeling domain shift especially in the
natural language and and text processing
communities some of which have been
applied to vision there and there's a
sort of classic techniques that look at
pre processing the data how you can
create domain and sort of source and
domain-specific versions there are ways
that adapt SVM parameters for for video
transformation and there are also ways
to model match readjust the training
data to match the the test data
distribution none of these can handle
the specific case that we were looking
at which is when you have training data
for some number of categories but a
category for in both domains but you
have a new category where you have no no
training data at all so I come into this
room I want to recognize these chairs I
have no label data for this chair but i
do have label data for a few other
objects that may both be on the web and
be in this domain and i use that to
learn a transformation that can then
help me learn these chairs so we've done
some work evaluating this on a data set
that we collected for the main
transformation so the here for example
are is a category of keyboard that we
collected from Amazon high-resolution
camera and a low resolution camera we've
shown how the methods we've developed
improve over various baselines we've
also shown that the asymmetric method
improves over the symmetric method and
we have a few really egregious examples
which are you know somewhat unfair which
are that if you take completely
different feature spaces baseline
techniques will give you garbage so if
you simply change the level of
quantization in a normal visual word
model and then try and do nearest
neighbors of course you find garbage you
find that this pad matches these all
these other different types of objects
but our method can easily learn that
transformation and M you could you could
engineer that transformation as well but
you can also learn it directly ok so
we're excited about this idea of domain
transformation we think it's an
important one in computer vision and
we've shown a variety of cool things ok
any questions about this so far so last
but not least I want to go and you know
throw my hat back in with the 3d folks
and say that I think there is the time
is ripe again to really take a serious
look at 3d for many of the vision
problems that recently had been tackled
mostly with 2d and in looking back at my
original slides the reason or the first
slides that I presented at the beginning
of the talk the reason I think vision
researchers looked at 2d features is
they wanted category level variation and
wanted to look at things that could be
found from the web and so then you're
going to match images from the web you
really have to rely on some of the
techniques that were developed for wide
base line matching and and not really 3d
object descriptors but the a and every
several years there's a new
revolutionary 3d sensor right I think we
both of us who've been in the community
for
quite a while can remember these there
are real-time stereo hardware is still
exciting my friends if the tix company
or continue to pioneer in that realm the
time-of-flight sensors that came out
three or four or five years ago lidar
you know sort of the success of robotic
vision made everyone think that a lidar
sensor was going to be the the
impressive thing until recently but now
the Kinect has come on the scene and
provides images that really do have a
quality and I guess ubiquity that seems
quite exciting and so we think the time
is right now to attack category level
vision the kind of vision challenges
that we've seen in Pascal and Catholic
101 and so on and so forth forth in
contrast to instance-based challenges
using this type of sensor we looked in
the literature and we couldn't find many
existing many existing 3d category level
data sets they all had limited seen
limited a limited seen complexity
limited number of categories limited
pose variation and and many had just
explicitly an instance focus so we've
begun a collection effort that will be
open source to the community in the in
the spirit of the label me data set that
has complicated real-world scenes where
we've collected registered depth and
intensity data here's an example of the
size distribution of categories that we
have here's and here are some samples
from the chair category so you can see
there's just a lot of the Kinect does
give you a lot of signal there and I
think the vision community had have
waited tackling the problem of category
level representations with with explicit
3d descriptors and now is the time to
come back and get to it one of the most
obvious things when you look at 3d data
is the distribution of size priors
so if you compare the distribution of
categories in 2d in a scene and you can
and of course you get a huge range of
variation if you look at the 3d
variation it's in many cases much
tighter so that's the first thing that
one can exploit with 3d and the other
obvious thing one can do that that
several groups had proposed recently is
to directly use a orientation histogram
descriptor on a depth signal and so
we've considered both of these as
baseline methods on this data set and
it's a mixed bag I think the simple
things are not necessarily going to
immediately be what wins surprisingly
even with this really beautiful clean
data quote-unquote you have problems
there are so monitors glasses they still
don't necessarily show up and connect
flat objects still you may not find a
connect signature so if you just use
depth alone you're not going to have
necessarily a signature from those
objects but you can often improve the
search complexity by pruning so this is
a project we've just started and are
interested in collaborating with other
groups who were connect hackers so
please contact us if you'd like to join
forces okay so those are the three
themes that I wanted to talk about today
how to take a hierarchical approach to
learning low-level and mid-level image
features how to learn transformations
between domains so you can train in one
environment and test in another and our
first look at 3d category level
recognition I think we're going to see
an evolution of visual object
recognition read research move towards
3d and robotics domains where you have
the whole problem being considered at
the same time including interaction with
people I think having a PR to that can
recognize everything in the environment
and and have a conversation with a
person about the objects is really an
exciting topic
have time to mention any of the ideas we
have on how you how language and vision
might work together but that is one for
the future and so I'd like to end with
just some of these themes that I think
are important for the for the coming
year and coming years how we can
effectively bridge category level in
instance level learning fully integrate
the richness of scene and task content
context that the these robotic domains
are going to provide us and explain
explicitly model user interaction and
leverage data from multiple sources but
I put in a plug for two things that
really do work right now these are two
local companies that are looking at
optimal fusion of crowdsourcing and
computer vision in the case of IQ
engines and in an old interest of mine
that I think is coming back now with
Kinect how you use vision for interface
in gestures that can control lightweight
user interfaces and I do advise both of
these companies so it's a bit of a plug
so I'd like to thank everyone who helped
with these projects including my
students and postdocs who are listed
here and they deserve all the credit and
I'll take all of the criticism thanks
very much
they're fast so we have lots of time for
questions and still before the
refreshments arrive I could put them
back in they all have equations on them
so you might you might prefer to just
ask me a few questions
you
well I think we use transparency for our
early work and additive probabilistic
models because we thought it was a fun
and entertaining example but it's
certainly not true that all objects are
transparent but it is true that these
additive models are useful for for
general objects and that's what the
results that we showed that i showed
earlier confirmed for just Caltech 101
or any kind of object and you get and
also it's similar to what Andrew and
others and and and Caillou have shown
with sparse coding that even for non
transparent objects objects that you
think of as being traditionally a single
process often aren't perhaps because of
background effects or occlusion effects
or because of unmod 'old illumination
effects even for non transparent objects
so I think this the set of objects that
are truly modeled by sort of a constant
albedo patch that's you know homogeneous
within a rectangle it's pretty small I
should have repeated the question what
yeah the question was well you could
figure out the answer the question from
the answer right this is jeopardy for
you all one of you guys that aren't
under which is first so your first or a
part of the work um so if it up it's a
hierarchical model right and CZ already
working on learning the hierarchical
filtering simultaneously so why don't
you directly working from the pixels so
as they learn everything from the pixels
that'll be cool right yeah that would be
cool and I think that's although that is
what many people already had done so we
wanted to sort of it attack part of the
space that had not yet really been
addressed which is applying these
probabilistic topic models were sparse
coding models directly to sift and
seeing what we could get out of that but
time permitting I think we would like to
go back and have a model that was both
which we're in which the different
layers were tuned to the different
underlying noise models of the raw
pixels and you should be able to get the
whole thing out of a three-top a three
model when you learn your domain
transformation matrix W do you actually
need to put in feature correspondences
or and and if you don't why not we don't
put in feature correspondences we do put
on put in instance correspondences so we
say here given a particular feature
representation let's say we're just
doing a traditional bag-of-words model
at this point here's here's what a cup
looks like in my particular
representation and here's what the same
cup looks like in a different domain or
even just here our cups in one domain
and here our cups in another domain so
we're learning a transformation on the
entire feature space not per individual
features you were thinking of them
any other questions sure so it's very
beginning you mentioned in a training
time you probably can use of multi
modality of signals and they're in test
time maybe some of the modalities are
missing right so but that's how in your
following worker you didn't particularly
mention or this big apart yeah topic
well that was one of the the topics that
I didn't cover today in the interest of
time where we do and this was a cvpr 10
paper we had where we do hallucinate
modalities in essence if you have a
certain set of modalities that are
present in your test data but not
present in your training data you can
essentially apply semi-supervised
learning learn a model that maps from
one modality to another from this pile
of unlabeled data that you have a test
time and you know it's counterintuitive
but it actually helps if you go back and
hallucinate more training data at least
it helped in our experiments and use
that to build a model there I'm not sure
we've I think they're a variety of ways
one can approach that problem and we
approached it from a supervised learning
regression problem a paradigm using
Gaussian processes but there are other
their other approaches
the texts claim
you
I haven't and that's a topic that many
folks have investigated envisioned in
the flat in the last few years and I
think often it's frustrating that's hard
to build good co-occurrence models at
the level of visual words certainly
there have been recent efforts that have
started to show success and I think
feifei if she's still in the room has a
model that does touch on that so I might
defer to her anyone else but I haven't
done it and I don't have a good summary
of the recent literature does it work
didn't you do feature co-occurrences in
a topic model
you
there are many ways to do that any last
questions with that maybe we should
return some time to the to those of us
who have to drive back to Berkeley all
right let's thank the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>