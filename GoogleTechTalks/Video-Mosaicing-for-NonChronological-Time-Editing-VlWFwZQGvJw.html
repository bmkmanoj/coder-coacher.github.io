<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Video Mosaicing for Non-Chronological Time Editing | Coder Coacher - Coaching Coders</title><meta content="Video Mosaicing for Non-Chronological Time Editing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Video Mosaicing for Non-Chronological Time Editing</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VlWFwZQGvJw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming it's my pleasure to
introduce professor shmuel Peleg from
the Hebrew University in Israel I've
known his work almost since i started in
computer vision way back in the middle
of the 90s and even today when I'm going
through the Proceedings of a vision
conference I look to see if I can see
some of his papers because he has ideas
that are really simple very creative and
yield very impressive results as you'll
see in his talk today so okay thank you
so where I will discuss some work that
we've done recently and the main idea in
the work is that time should not be
chronological so it's funny notion of
time but before that we see what what
can we work on we want to be effective
so I want to use on very popular video
so the occasion what is the most common
video the question is not defined well
Sasquatch the most popular video camera
so we all know what's the most popular
video camera it's your cell phone what
did trouble if the trouble is that no
one uses it it is in the pocket it's
rarely used as a camera so then ask what
is the most actually video what is the
most video captured around us and and
this is the most video captured rounders
I'm sure you fund some cameras around
here each camera takes 24 hours a day
seven days a week look tons of video
there's no more video none captured by
this camera what the trouble with this
video no one watches this video so we
see that kind of two deficiencies in to
kind of very extreme a lot of video a
lot of cameras are not being used well
so first I'll see can we make this video
infinite basically that's infinite
stream of india more usable and also
take make some use of this very popular
cameras around that are not being used
well now the real question is what is
the most popular video watched now i do
not know how to answer this oh I'm sure
that people around the arrogant Google
at least would know better to tell what
is the most popular video watched but
I am NOT going to address this year okay
so let's look at this what they call
webcams its surveillance camera mounted
somewhere looking and these are just off
the web where we record some this is a
camera in stupid girl airport and it's a
camera I don't know where in some
billiard club club notice there's no
pockets here you should ask to replace
the the table you have outside with the
table in no pockets it's more
interesting anyway so so we have this
video we look at this video and
basically not not much is happening
there not much is happening one person
here why go so many tables and Airport
usually are quite not very active when
you look on the rod Mary side but still
the idea is can we take this video and
don't look at it to see what activity
goes and stood at airport what activity
goes in this billiard club do we really
have to spend 24 hours to look for full
day or do after seven at one hour and
this is no so we would like to do the
following widdle we would like to answer
queries like show me in one minute the
last two hours so show me in 15 minutes
at last two weeks so basically where
this system they'll take this webcam and
summarize for us what's going on there a
and the way we do this is it is the
following we take we look at the video
in three-dimensional space so this is a
picture one frame in the video and then
we have a lot of frames right now I'll
address the issue of a finite video so
assume we have a video of few seconds so
this video a few seconds is basically a
three-dimensional volume and in this
volume we have activities and if we
think that what's interesting is
activities in video usually activities
are important
we can shift activities from original
time to a shorter a shorter volume so
activity that originally happened a
different time will now appear at the
same time so if each black snake like
this represent what collectivity tube we
pick them into a shorter time and in
this shorter tag we show a shorter video
with all activities and skip how we
compute them to so an example so this is
a short video outside my office with a
water dispenser and we have three people
coming and the activity tube is likely
someone is coming on the left is living
then another one coming from the Sun for
the left hand for the right and then
another one for the left so this is the
video the person coming from the left a
person coming from the right sorry
person coming from the left person
coming from the right and then someone
got slower
my computer is doing something
background or no idea why let's see how
can revive the video
just one minute
what happens here
Oh
okay so we see this activity now it's a
long video so we have these three
activities we pack them in a shorter
time and we get basically we see all
three of them concurrently like here in
the shorter volume so this is basically
we see all three together we get much
shorter video and we see all people
together within show a shorter video and
we do not miss activity so we see is
this effect of different activities a
different time we pack them into a
shorter time what happen if there is one
long activity happening all the time so
if this one long activity have no time
if we know what is the length of our
output video we can segment the I can
segment the output video into segments
and pack them into a shorter video and
this is the example so we can have this
video this girl on the monkey bars is
occupying the entire video if you want
to shorten the video by one to one third
of its length without losing content we
can basically show different periods of
the girl that the peeled originally
differenter we can show them
simultaneously so we get a shorter video
without losing activity it works also
with the pending camera if we take a
camera weep and track this lioness we
summarize the motion of the camera by
creating this mosaic and the lines in
itself showing five different segments
at 5pm concurrently okay so what we like
to do we remember the webcam with a
webcam what we show now is a finite
video but the webcam is infinite it
never starts it never ends it going
infinitely and what we do we always as
the camera is recording we analyze the
video and we put moving object aside we
list them in the database when when the
database fields up usually the data
object
is infinite our database is not infinite
well maybe this is not true in Google
people say me that Google database is
infinite but the University it's not we
have to throw out the object the
pressure is how do we throw out object
that's another issue which I'd skip and
we've got to to bring the query service
the query says show me the last day in
10 seconds for example so what I have to
do first to collect all the activities
that happen in the last day out of the
database pick them and in the packing
guy do not have to keep the original
chronological order because the karoo
order prevents me from using the time
compactly and then generate the output
synopsis so what will that 24 hours in
10 20 seconds of this Airport so this is
the airport that's usually shown here is
summary 24 hours in 20 seconds so we say
I'll take object from all different
times and basically show them
simultaneously you see their airplanes
and cars and think together they didn't
have together but I push them from
different times you shown together
there's one object which is special
object i call the background so the
background object it is there i treated
separately for the background object i
just do what called time lapse so i take
one frame every a few second so this is
this the airport and this is the
billiard club so this is what i saw was
wondering is it only one table why do
you need the entire club so i came next
morning and so i'll show me the last
night 10 hours in 10 seconds and this is
the last night in 10 second so the day
is that once with your webcams you don't
only can see the boring activities are
shown on the left but the colossus
summaries of past time people will be
interesting let's go to certain web come
and see what happened there assume there
is a webcam up there looking at this
area here or in this room know sometimes
it's boring
but maybe there's some activity in the
last thing okay yeah
overlap well I when I pack of course I
can do overlap but then the video will
be incomprehensible because object we
want one on top of the other on the
other end i can assume i don't want any
overlap this will create inefficient
summary so there's a parameter sir how
much overlap I allow in this case for
example I allows I think ten percent or
twenty percent overlap so for executive
two buses that can go through each other
because they each time but they cannot
sit on each other so I do lobular
because we know it's summer it's no one
can be confused that this is the real
any of these summaries the real thing so
we know it somebody all we need to do is
to have enough or small enough overlap
so we can distinguish object for one
another and it'd be comprehensible so
there's a parameter that we can adjust
so the way of packing rather than I'll
skip this one okay the one question what
is good for for us it was a good for fun
it's really i think fun but there is an
any other usage for it other than fun
cider i imagine that we have on the web
there's tens of thousands of web comes
that you can see everywhere on Earth I'm
sure that when people will be aware that
there's ability to put the cab on the
web more people will put a camera on the
web but these I have to provide access
to this Cameron so maybe Google is a
company that likes to collect all the
information on earth this is one thing
you have google map every location you
see a camera you can see the camera live
but not only live but you can also see
the camera summarized over whatever
period of time you wish to summarize so
maybe this can be one use of this
synopsis other users of course are for
surveillance security cameras but that's
not interesting applications okay
remember time we have a problem with
time we have a problem with time
especially when the time is
chronological why do you have a problem
if for example do mosaicing mosaic is a
the camera
scan the scene and you stitch the images
together and and here we have trouble
because if we stitch them normally we
have we have wind in the trees here and
the me wind is missing from the panorama
if we want to put the wind in the leaves
as it was originally a we have a problem
because when do we have a problem
because we have a shadow here of the
branch on the top we've seen the shadow
in the original video ten second before
the cedar branch and now we see them
concurrently but they never they did not
appear we didn't see them concurrently
but now we want to show them
concurrently so what we do have to shift
object in time and therefore this is
similar object the shifting in time you
say no one to see the synopsis object do
not appear in the original time they
appear in different time a so how do we
do this all the approach is based on
first taking the video and making it
like seen before making it into a volume
a three dimensional space-time volume to
do this we need to adjust the motion we
need to do other things we may talk
about this later and we create this
space type volume and and and we create
new images out of this based on volume
by running some surfaces really so we
take a surface and displace the volume
the surface cuts pixel in the space and
volume and whatever pixel it cuts this
is the new image if I want to create
another image icon the video frame after
frame I move the surface it cuts new set
of pixels and so so for example if it
was the orange line and the frame of
regional video the black line of frame
of the new video and I progresses a new
frame is basically each frame in the new
video takes a collection of pixels it
cuts and create a new image so what kind
of this stupid scheme can do so this
stupid thing to do the following for
example take this video
that's a demolition of the Kingdome in
Seattle what's 78 years ago se we did it
for for a movie and we did the
demolition and then the director comes
as a we did a mistake actually we wanted
the center of the don't call up first
and the sides later and it happened at
all collapse the same time or maybe we
want the side to call up first on the
dome at the center later we cannot do it
again because the structure is not there
but we can play with the time how do we
play with time we take a surface so this
is the space that volume looking for me
above so each we see each each row is
the top one top line in some way a cut
here in the image so where we take it
time front that the surface that the
sides of the surface are in the past and
the center is in the future and we run
this through the space-time volume so we
get the secrets of images each frame
generated the center was taken from an
image who is ahead of time than the
sides so we need to do we separate the
shape of the time front and we get a new
video in this new video the center
collapses first if you want the side to
collab first we create another surface
this surface and we create the surface
where the sides are from the past sent
Surak for future and the center from the
past and then the sides co-op first so
once you have this three-dimensional
very easy to recreate new things with
different time ordering yes
oh of course when when I'm saying I'm
cutting through the volume I'm putting a
surface through a volume so the surface
goes in between pixels so I must have to
me no no the surface just knows what you
see here just take the volume it moves
in the volume we do interpolation the
interpolation due to the fact that the
surface is continuous and the pixels are
in discrete location so we need to
interpolate but we don't use optical
flow here for for this we'll use optical
flow interpolation for other things
later but not those losses we have
well yeah maybe I don't have to say that
there's some time you do need about get
on without it but here we don't need
okay well do something more interesting
racing you know there is this race you
want your kid to win but it doesn't what
do you do well not problem the memory
you know only the computer has the
memory so we can change change the
membrane do the following take the
space-time volume take initial image
original time the last image your kid is
ahead of all the others your kid is
ahead of all the others you can
interpolate kind of these time so
everyone is going in one speed your kid
his time goes faster he stops go faster
meaning that he is going to win so he's
not winning come is his best friend to
say I want to win his word well you can
go take the same thing go to these best
friend let him his time run faster as
well and then he is winning so so
everyone can show to his friends
whatever we do he watched I I don't
think there's any way to tell the
original video from the remanufactured
videos then come again trips you you're
going on a trip you see wonderful things
I have not been an equal Sioux Falls
people tell me that the magnificant
falls very wide everyone everyone takes
this step of ways are so wide this is
the way you take everyone takes the
panorama of the of the falls and of
course you can stitch them in the
panorama and you get this nice picture
but the water turns into ice so what are
frozen so what you want to do we want
the water to fall as well so where we do
this with the same space-time volume we
run the surface this way and then we get
both panorama and the water is falling
and again remember there's a Time Shift
key because we never
all these two part flowing together we
showed a different times but this the
fact that this surface is diagonal
basically turns time into space and we
turn show different thing that appeared
different times at different look a
location and this is another use we said
we use look for useful cameras and
cellular phones this is an example of
you see a scene you take your camera
take a picture and create this dynamic
panorama of course once we have this
tool every time I go to interesting
places like that taking panoramas this
was in Saloniki again you see this image
from the camera and this is the panorama
that is dynamic of all the region ok so
so this is a very stupid idea it's so
not stupid a very simple idea you take
the video organize it in a tube and then
run some kind of slice through the tube
and generate new videos but it can
generate something very unintuitive
thing for example if we take this is the
space not volume of the waterfall so it
goes from left to right we see one frame
after another that's why it's diagonal
because the camera is scanning the scene
so as we progress the time we go to the
right now look at this black tank fronts
time from this is the surface we are
running through this whoa we are running
this surface we'll move it down so this
is the intersection and this is the out
will be the auto show you moving down we
see what happened to the output frame
the upper trim will move to the left so
basically we got an input video that
poof are left to right and now we got an
output video they'll go for right to
left why should we do that we have an
input video that go from left to right
if you want an upvote video that gopher
on to left what we need to do just to
play the frame backward right so played
plane backward
and we have a video that go from right
to left but something is wrong they
write the gravity's inverted if you want
to keep gravity this is our method in
our method this diagonal line we get the
camera going from right to left but the
water continue to go down so we inverted
the motion of the camera but we did not
invert the motion of the drops of water
so this thing it's a very powerful tool
can be done by scanning this space-time
volume now we look on cameras again and
we notice that when we have a camera a
location the image mean different way in
space so if we take say a column in the
center of the camera we have rain space
they go perpendicular to the image
surface and if we take a column in the
left side we have raised going in in
different direction so when we actually
take strip in the image means direction
of phrase so how can we use that so if
in the camera that is translating the
world and it's collecting a set of
phrase in all directions if we just take
center column for every image and stitch
it together it means that we create what
called a push broom panorama we have
arrays that are parallel horizontally
vertically that perspective but
horizontally they're parallel because
are all taking for the center of the
translating camera but if we rather than
stitching column from the from the
center will stitch column from the left
side or from the right side we get
panoramas that also parallel rays but
parallel rays are tilted in different
ways so we see an object one time we see
an order from the left from the center
or from the right so if you go to our
space and volume presentation this slice
is stitching together of columns from
the right set of image so this is one
direction view of object so when you
move the slice each time with a colorful
different location each time we look at
object from different direction so if we
just view what we see take this is the
input video for example
on the bottom we see they're slicing the
right we take the slice and move it from
the right to the left and back we get
this effect so we see that all we did
was just taking the space-time volume
and moving the / from left to right
that's all we can create views that
represent three-dimensional images and
without actually computing
three-dimensional image we just kind of
different slices of of the the
space-time volume again here we build
space top volume you see the camera is
jumping on the helicopter so we need to
compute the motion but after building
the space-time volume running the slice
we can see 3d something strange here the
camera isn't running you notice it's
standing it's it's a derailed camera
actually 2004 very strong earthquake the
red leashing concern think about sitting
in a camera in a train at 270 kilometer
to unsettle I don't know how much is
pedo Shinkansen got sounded very fast
and then there is a earthquake nine on
the Richter scale and throw the train up
and down so it derails but no one was
hurt anyway so off you go trip this is
case Aria in Israel you take your camera
scan around and not only get the
panorama but you can get the 3d of of
the thing okay a nice house you can show
it in 3d
ok so we've seen we have the space the
volume we see that we can change order
of event we can look left right what
happen if we take this surface the plane
that we take in a space and volume but
rather than shifted left none are we
rotate it so we take column from the
center of the image this is one thing
but then we rotate it so we tried to
take the haagen I'll slice so this lies
start on the left column I left image to
the right column on the right image what
happens when we do that one ever
everything's rotated so so what happen
is is the following so we again I shall
camera that's translating left to right
translating left to right we look at the
diagonal slice so we take the left slice
from the left image and then the center
slice from the center image and the
right flight from the right image so
basically these are the race that will
be collecting from as the camera goes
the command the left looks at the left
trade the cup of the right look at the
right ray it's set right so if you
collect them we see raise that in
addition to the right passing through
the camera part because all the way were
taken all the images were taken from the
camera going in the same path they
intersect all the planes intersect
somewhere in in a vertical line
elsewhere so basically this creates a
model of the camera called cross with
camera crossett camera is a general
camera more general the pinhole camera
were the keraits go both through
horizontal line and a vertical line a
different location so when the red and
blue plane are glued together this is a
pinhole camera but when they are taken
apart we call the cross lead which is a
generalized generalization of a pinhole
camera and basically with mosaicing we
can play around with the location of the
vertical slit auras honestly that's a
camera path we cannot touch because it's
a physical line generated by the camera
but the work at the line
is ritual I generated by the mosaicing
and we can play with it as we want so so
we can kind of generate for example
forward motion if we change the angle of
this slice we can generate change the
location and we move as if we move
forward in the scene by rotating the
slicer if for example we take this
horizontal video this horizontal video
this is one mosaic generate them from
the left side of the left image to the
right side of the right image and we
take this and rotate this slice and when
rotate we move forward and you can see
that move forward you can see object
hiding its own zooming object are behind
the table are showing up there is a lot
of power lock so as if we know the 3d of
the object is there we know the
occlusion but we don't all we did was
just rotating that's part of image based
rendering but very simple image by
surrendering because all we do was kind
of rotating a plane in the space-time
volume we can have funny shape skip that
ok now assume with it I don't know if
you have heard but some companies are
trying to mosaic straight city streets
and show them on the web so this is an
example of an attempt this is my my
camera driving in a street in Jerusalem
until the the card was filled up this is
a I don't know if you don was acting
before this very difficult it's very
close object very far object the cameras
and stabilized jumping it's difficult to
do anyway so we're trying to do a city
block from this no GPS no laser and skin
father nothing just a regular car
and handheld camera blocking this city
block actually it's number the blockage
half a kilometer so several blocks there
and and indeed wow I didn't realize it
so long
okay so now if you thought about going
to Jerusalem you don't need to you've
been there at least in one Street so
this is a panorama that creates very
very long panorama of this half a
kilometer and I'll describe what we do
in order to get this panorama or these
panoramas and how can you avoid issues
like distortions etc so first thing is
how do you one thing the first task is
to compute the motion of the camera so
we have a camera the camera is moving
taking the pictures and we cannot
actually the original tread regional
mosaicing as was tradition done is to
compute the motion in the image the
trouble is computer motion in the image
is that it is out case we have object of
many different distances so far I object
will move slower close object will move
faster if we compute an image we either
computer image of the motion of the fire
object or the motion of the nearby
object when they change we get a change
of motion even though the K the camera
did not change with our motion capture
tensions a lot of trouble so we need to
do that we need to compute the motion of
the camera a lot of ways to compute the
motion of the camera nothing that we
have tried to work for thousands of
frames without breaking most of them 10
20 hundred and they break and you want
something that will never break so we're
looking for method that will be reliable
basically what we use is two components
that are working for tens of years and
known to work very good one it's called
a two-dimensional lucas kannada motion
two-dimensional not free dimensional and
another one is stereo computation so
what we do we do an iterative process of
compute the motion to the measure motion
and then compute stereo and run back and
forth so most of our motion computation
is in 2d the good old reliable to the
methods and just do the diagram and it
shows that we do get very good this is a
matter of depth map we use
got to do this at this this is the depth
if we call each label is a constant
distance and this if the labels are
planes a they're more accurate actually
we thought that doing depth labels of
planes will be complicated because the
labels are complicated but it seems that
it's simpler because we have much fewer
planes then in the concert so we have
fewer label so it's it's not first so it
shows this is the depth example
iteration this is first iteration second
third iteration we see three iteration
everything the motion and the depth are
computed accurately in three iterations
and now so what we have we have our our
camera we computed the motion so now we
can go and walk around Jerusalem so this
is the cross week walking around so we
can just not only go left and right but
we can so go get inside the side roads
so this is an example of getting inside
the saw so we see we see it's not
zooming in like we see the cars in front
of the cars here it's really kind of a
three-dimensional walk in Jerusalem we
can take another sequence I took in the
conference in Germany create a mosaic
and this is sliding from left to right
just seeing through different
three-dimensional views would you call
it a city street if it's a river street
well I guess it's hard to run your truck
in the river but regular camera is
easier to put in the river we can also
do a walk-through left and right and
okay they one of the problems of making
a very long mosaic like in the city
street is distortion because when you do
very long was I think that the problem
is that tradition was liking is push
pull and push room have distortions that
far away object will be very wide narrow
will be very a close object of
we know a very nice work was done by
some people instead bought some of them
are sitting here here in this room and
the good goes as follows let's take this
the scene break it in two parts and
let's design for each part a different
Crossley the closet projection let do it
different closet projection and indeed
it minimizes distortion but there's one
problem first is it's not sure that the
division into segments is is optimal it
gets some minimal location but we are
not sure that it's mean what we get it
these are the cross week we get
alternating cross slip between different
areas so this actually when we look at
the space-time volume this is the kind
of combination of cross lead it's it's
slicing of the space-time one the one
cross like another cross late so this
different cross later gives minimum
distortion so when we look at this we
figure out why do we need the curve to
have discrete parts each one having
straight line we can have a curve the be
continuous curve with no no
distinguished parts and basically to
find this general curve which is not
segmenting two parts and which have a
different may have a different slope
everywhere much much easier because this
can be done with dynamic programming
optimization is trivial n we get global
optimization other local so this is much
nicer so what we do we basically if this
is the sick of images we find this curve
we take strips or every image some part
we get wide strip we take what strip
especially from parts that are different
to stitch together so rather than
stitching together different part we
just take a wide strip and area that are
easy to stitch together without take
narrow street from them and and
okay the nozzle from the outside a
federal question okay so one thing is
I'll do is teach a strip 11 issues if we
have white strips at the different depth
will call causes a problem this for
example if we stitch street so this may
be 310 feet and the back is hundred feet
from the camera so just sitting sleep is
causes both cutting object here multiply
object in the back so instead of just
stitching strip we take a strip the
strip that we stitch in the panorama
will be rectangle but the strip in the
image will be depth by so this is depth
interpolation that you do here between
the strip in the image and the strip in
the mosaic and and and so so we discover
the cost of of this curve that we want
to run so the coffin first have
distortion cost so what is the cost of
distortion so the very way to define the
distortion we found out from the
simplest cost was the variance of
disparities of the pixel in again in
Steve's overseas trip the cost of the
strip will be just the various of deep
wonders many depth in a strip the
variance of depth is high the strip will
be high cost if there is the strip all
the depth in the stripper uniform like
we see a wall of uniform depth the strip
with a low cost so we can have many
strips that low cost few strip of high
cost so this is just distortion but we
don't need only the minimal distortion
we need that the image or to stitch
right so basically that you can stitch
images and be continuous and then the
staging mean that if we have a strip the
right side of pixels in the image and
the right side in the mosaic should be
similar so basically the two elements of
the cost distortion caused a stingy cost
we create a graph that we represent the
cost between the left column
of a stripping image I in the left
column in image t in the left column of
here J in image Tibbals had this bill
this graph we found a path optimal path
using dynamic programming and we get
mosaic now these are just zooming in on
some part using this push broom and the
what we call med mosaicing met is
minimal aspect distortion was aching so
you can see in regular mosaicing this is
a car which is far away you notice it's
becoming very wide the door is closed
it's it's very narrow you notice how can
people walk in the door and this is this
is a push broom and memos I king is in
the bottom you see both the camera both
sorry the car both the car is the right
size and the door there's no distortion
aspect distortion here this is another
another view of push broom and pop and
med mosaicing on the bottom you see this
was also considered problematic areas
because moving people when it's
problematic when we have a lot of
different that we don't understand what
happened there we put wide strip this is
the width of the strip so you can see
this woman was taking complete strip we
took her this room and another complete
strip so when wonder we don't know when
the difficulty that we take white strip
whites if we mean a large area taken
from a single image when things are
smooth we can take a different street
from different area so this is the the
two mosaicing this is the this just
apart form on top of the variance of
disparity we see that if this is we
bypass variances of disparity and an
interesting thing is here we can see
this is zooming out seeing from distance
this is a city block 1 1 block and we
notice that we see two streets here and
here and each each of them has its own
projection
it's unrealistic projection when we look
up here it may look strange but when you
look close and we walk it look very very
natural you see this block on this side
we look from the left we look from the
Sun better now we look from the right so
so basically if this is a distorted
picture of the block we don't really see
from one point of view the same block
from the center from the right hand from
the left but when we do this very long
mosaic and we just walk along we get the
nature of you every local point to get
the nature of you the only one we zoom
out we get this unnatural perspective
images so so basically here is the app
so this is the street and basically we
can take this half kilometer and walk
along the street you know this company
that has the city block view did you try
walking along the street it jumps so I
hope that one day they have more smooth
so you can really glide along the street
and then jump along the street I guess
the data is there suppose we can get a
very long mosaic and locally everything
is the right aspect N and this was taken
no laser rangefinder is no gps nothing
just the regular camera out of looking
out of colorful window and n if one of
the issues is to to get to get the
universe on the web we take a lot of
time and a lot of trucks to get all the
universe and to update it every time
from the beginning this way everyone can
take a video of his neighborhood upload
it to the web and within 12 weeks you
can get all the universe
yep okay as a compression method for
video compressor our videographer well
we don't we don't guarantee that all the
objects are there so we're in our guests
we know how many object we've thrown out
with enjoy but i think that compression
method eventually does it only use MPEG
what does em big do MPEG compute motion
for every object and background changes
so this compression be very crude
compression be compression but without
all the object you know what I'll do you
have and where well it should be covered
have a database the what I showed you is
is basically a visualization database
the database is the list of object an
object for me is a small page of the
object a long time so a list of object
where they are some kind of feature on
them and the object themselves so this
of course much smaller than the original
video and then we have the background
with the background we take work for a
one one frame every say minute or a kind
of time lapse of the background so yes
there is a way to generate this but I
don't know if it's acceptable to see
that what does the video in video
compression you you measure some kind of
of off losses and I guess for the
measure used in video compression you
have tremendous losses here but of
course you can take from you have object
you're going to reconstruct the new
video from the object I haven from the
time lapse of the background I have and
I can we could start something will be
different originally but actually let's
see let's think about it again usually
now when people do compression they take
usually the mean square error of the
difference so
now we see here is that we don't have
the means well but we have all the
object all the activity so maybe with a
different way of losing so here we
measure they are not by the mean square
error which may be meaningless
especially for example assume that you
take 22 you compare two different frame
rate and resolution everything so maybe
nobody said to be objects and activities
so maybe there's been different way to
look at that compression there was a lot
of distortion
maybe well the distortion is because the
projection we do is crossly it's not
respective its image based rendering we
only have the resultant recession we
don't have the vertical views only the
result one so what we can do is cross it
means we can change the viewing
direction of the Rays left to right but
the way up and down we cannot play with
them because we don't have them so this
cross litany does have this distortion
and now the depth that we use is not
it's below here it's even though it's
surprising more accurate than what we
expected it still is not very accurate
for example there is this rail here so
if we get the general shape but if for
example we want to go around it to to
correct hidden areas it's not accurate
so I don't think it's it's reasonable to
expect that the 3d will be real 3d and
we get a real model of all the scene and
object and trees and it's very difficult
to get the real model of the world so if
we had real model you're correct we can
move in and out left and right and some
places are trying to build models I
think Google is trying to fit models to
buildings in New York or something i'm
not sure but if you have a real model
that's true but there's something we
don't have a model so we use this depth
only for computing the kimono for
stitching but we don't use it the 3d
model of the scene so we can do real
walkthrough in 3d models we know I'm
we're using its image by surrendering we
use the depths only for the stitching
for the interpolation of the stitching
it's only for cosmetics not for the
essence of what we see
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>