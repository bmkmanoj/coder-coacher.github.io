<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Domain Adaptation Workshop: Adaptation without Retraining | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Domain Adaptation Workshop: Adaptation without Retraining - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Domain Adaptation Workshop: Adaptation without Retraining</b></h2><h5 class="post__date">2012-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2cYZ254kpec" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and without further ado I'll get the
floor to Dan for adaptation without
returning okay King John so thank you
for steak his dog so it is slab repealed
in the morning
I could've done that in a magazine
giving the talk one for no
semi-supervised we get a lot of folks
there are several fields on
semi-supervised and indirect supervision
that can easily be related to adaptation
but I would and I thought about it and
people decided to give a new talk on
radio another station so that's why I
probably will see how many more minutes
I need but so so this is a talk
and we get to see the most
so we could think several of my students
that in the local walk view and some of
the funding agencies and and I don't
know so I don't know how many of you
give out any three people well it's a
little bit about the fact that
adaptation is really essential in
anything I mean it's a tough domain and
many many reasons why that domain we see
new trends very different often than the
one we actually care about vocabulary
difference across domains in a world
occurrence many first world usage maybe
third world meanings may be different so
just a simple example the world can his
metal used as a noun in a large
collection of Wall Street Journal
documents that is commonly used to train
a lot of tourists and we all agree that
ten is used quite often in some contexts
as a noun and many many other examples
like this it's not only vocabulary
structural sentences may differ for
example and use this example later on
because of course in the worst food
journal is very specific if you with
other literature fiction blogs other
newspapers the use of course is very
different if you get used one you may
need be confused also TAS definitions of
some of the tasks may differ there's
very little we know to say about this
unfortunately but I'm getting to talk
about so so what are we doing I mean you
try to solve a lot of problems ranging
form with all of them and the
recognition and assuming some of you at
least know this is clear
for me demo that we have on my webpage
basically the idea is to recognize
raises late with people but raises em
name for profit organization the
location and so on and it has to be
clear that even though this is a very
well-defined problem it's actually very
hard for him because entities themselves
are very ambiguous JFK would be both the
location and a person depending on the
context so so just using the list is
insufficient to units half of them we
have to use machine learning techniques
and then after training on a specific
domain you can actually be pretty good
as this shows but moving to blobs or
other things could be a problem
here I show blog about soccer and you
can see that the world Wednesday here
but Wednesdays at the end week and in
this context is actually an organization
in women so so we do make mistakes
another problem which is going to be my
main learning example today is the
pollution ethically thing is the problem
of passing cello passing of sentences at
the level of who did what to whom when
where and why
so given a sentence like I left my first
when I go to Italy will we would like to
pass it at the level of I am the little
a1 my purse is the things left to my
daughter is a place that represents the
benefactor and in my way we call it
location it's an age you can disagree
with this but if some content agent way
so basically you can think about it as
some kind of a chunking of two phrases
and then cuddling each place based on
the world you wanna give and of course
there are many many ways to do this and
here are some but there are constraints
over what you can do so for example this
is illegitimate because you had
overlapping arguments and in this
combination we don't allow related
argument and some other could be to be
violating other constraints
for example say we know that if argument
of the type in two is pleasant a one
must also be played so so this is the
type of for luminous appeal surrending
for this is based on corpus that is
called for Bank that is an
implementation of the triple it's a
relatively large if you want structure
prediction problem with zeroth argument
types and it's one of the tasks that
people have spend a lot of time over the
last few years in nsv and they sense on
the data so and then once you once you
claim you get something that looks quite
nice so this is against potential form
of system and you get identification of
the cumulation in this sentence the
sentence feel that happened exploded up
at the US military base in 11 US
citizens you get the key relations and
you get what's happening in the sense
that this is the key left this is the
corpse this is the bomb and so on again
what happens if you move outside of your
training data and this is where the most
important slide in the talk I'm going to
use this later so here's another example
that seemed quite simple but when you
won this one the devil you see that we
identified peacekeepers as the their own
and in fact they could know this ahead
of time
the key reason this is happening is
because we've never seen abuse before as
a health this fact by itself confuses
that the forces is significantly enough
but we would all agree that I could
leave I the sentence you and these
people's he loves children
if the same sentence as far as you and
unconcern himself
nevertheless here's we get it exactly
right
we get the US peacekeepers how the agent
the entity causing the damage this
health is the predicate children of the
patient entity experiencing damage
beautiful and of course we can now take
these annotations and bring them back to
the original sentence and that's
basically what I want to talk about most
of my talk today so the idea is that I
knew ahead of time without knowing to
think about this that I'm not gonna
succeed here and if we know how to
convert it to something that we are more
likely to succeed on we've done some so
I call it adaptation I don't know if I
can call it domain adaptation because
I'm not sure exactly what's what's the
domain we want to be able to do these
things on the fly without having a
notion of this is the domain of taking
the test example thinking think about
you know real-time processing of the web
data that could be very very dangerous
but you think that don't want to eat the
lemon water I want to use the same order
that they trained on a lot of training
data in the past and now given a test
instance I wanted a little bit so that
it's in it's going to be more like the
training day and then I'm gonna
transform the annotation back to the
students opinions so so before I get to
this part of the talk I want to talk a
little bit about lessons from standard
domain adaptation basically talked a
little bit about interaction between f
of X and F of Y given X that we talked
about the link between the morning and
then I'm going to move to this part
talking about
changing the text rather than the motion
and I'm gonna end just with a few slides
on another of efficient organ that comes
up that the thing is different than what
we're doing and very interesting so
maybe something will stop thinking about
it so so he doesn't perspective on
demand interpretation folder so what I
showed here is is this bill that x-axis
is similar pure X the y axis is similar
pure Y given X so you have the same tax
tax in the bottom right corner and I'm
gonna place points on this on this graph
and bullet pathways on one of John's
papers the examples are gonna be review
so you can think about the point here
which is adapting from English movies to
Chinese movies then in easiest problem
would be a def adaptive adapting from
English bookstore to music and even
better adapting for English movies to
music and a heart problem could be
adapting from Wall Street Journal named
entity recognition to vile biological
dangerous biological articles named
entity recognition really almost a
different task so so just to make sure
we're talking about the same thing when
we talked about P of Y given X we can
please assume that we have a small
amount of label data on the target
domain otherwise we won't be able to say
something of this and typical approaches
that people have developed for this
relate source and target to weight
vectors other than training to separate
vehicles
independently so often this is achieved
by sometimes like utilization tell man
you know similar of the walks that
everyone knows about Champa hey
condiments and pink and out
works in this landfill there's another
land for the talks only about changing P
of X the distribution of the data itself
and in this case we typically don't use
labeled examples in the tabular meant we
just rely on and let the data and the
idea is to try to resolve differences in
the feature space only basically find a
new representation or then the new
representation that brings the source
future space and the domain and the
target piece of space together so what
the open and structured correspondence
of John and several other walks belong
to this class before so so back to this
picture what do we know in fact we know
that most of the pollen that we can
actually solve and this is based on
experiments who is has our algorithm
frustratingly easy really in the bottom
right corner this is what we can do and
in fact if we even normally be further
to the bottom right corner it becomes so
easy that just pulling all the data that
we have together and training it's going
to do so we'll be more specific but not
a lot more specific but yeah well at
least you know the beasts more similar
your eggs and more similar feel for
humans
I don't want to commit and put numbers
so but the bottom line is at least one
of the bottom lines is that if you are
here there's nothing if you really need
to train on target if you want to be
able to succeed on this time
so and in most of the world that people
have done I think actually is in this
bottom religion anyhow where P of Y
given X is quite similar so I'm going to
zoom into this area basically in our and
and what we can see here we can see this
blue region and we can see the red
region and the interesting thing is that
if you take or what we found out is that
if you take points that belong to the
blue region and apply pues excitation on
them and we've done it with simpler much
simpler method Elton John's metal just
bomb clusters actually move points from
here to the red and then you can just
pull all the data together
so so I'm gonna so it brings up the
question you know do we need F of Y
given X at all and and in fact we do
need because if you look more carefully
remember without you see that there are
different regions where you do need it
but sometimes if you do that I think you
can actually move everything to the
origin so I'm going to give you a little
bit more details on what we call the
necessity of combining the rotation
method basically looking positive Y
given X and if ok so what you can see
here on there on the left is a graph the
red is to any money on target so it's
not so good and of course we're and the
blue is prospecting the easy so it's not
doing well when we don't have
similarities to F of Y given X and here
are measuring similarity is the cosine
between the two weight vectors one
optimal weight vector on the target one
another one is optimal weight vector on
the sauce it's not the same Optima that
you described in the morning and
actually there's some interesting things
or possibly
but what you can see is that once you
become close enough then only then for
stability beats doing nothing but if if
you become very close just pulling
together the source and target is
actually good and and this is stuff that
is done without cluster just think about
f of Y given you if in addition you
first run clustering from clusters in
this case on the on the data what you
get is you get better results both with
the post reading be easy and just
pulling source and target and you do
have the region where the blue is better
than the green but at the end actually
surprisingly perhaps just putting the
sauce on top together actually beats
state of the out adaptation so so there
is some theory behind it are not going
to talk about it but we can prove a
mistake bound that show that fe4
studying the easy actually improves when
cosine of W 1 W 2 is greater than 1/2
it's a rather loose condition but it
gives the intuition that it really helps
only on the right side when F of Y given
X is close enough and we've done
experiments on a number of key tasks
including limited recognition the
positional sense which has been defined
here and we show that before any
clusters in these two cases as is in the
best with clusters sometimes we can show
the just putting together source and
target is actually best and leads to
subset of the artists
did you already mention that what is the
proportion of sourcing targets I need to
mention and if they don't remember but
it's the standard data sets I don't know
if you remember so you'll have to go to
the people but generally is their
comfortable size looks like probably ten
times ish for it's a perception based
analysis okay so so basically the lesson
here is that really it's important to
think about both of the patient method
but another lesson is can we get away
without knowing the thing about the
targets and that's really the motivation
for trying to think about on the slides
of tation so I'm gonna focus now on this
wonderful adaptation reading read this
example that I showed before that
illustrates that at least in this
example we can do it I'm going to give
one other example here is in which is
sentence he was discharged from the
hospital after a two-day checkup and
here in his parent blah blah blah and
I'm looking at the powerful other than
semantic role labeling analysis although
I care about semantics 11 charges the
predicate but mistakenly we thought that
all the rest of the sentence of the
passive thought that all the rest of the
sentence is actually a temporal argument
clearly this is wrong I'm gonna rephrase
the sentence by changing the world
checkup to examination and now I get
still this is the predicate but I get it
correct and poor ultimate only this part
after the two-day examination is is the
code so this is another type of analysis
it illustrates another example will be
swoops but it respects another important
idea about natural language processing
some people have moving most important
so so when you walk would you think of
including energy systems it's not just
returning a classified these are
typically big systems pipeline systems
in order to impact the semantic role
labeling I have to start with bounce
each table I have to do passing and if I
want to train I have to learn all of
these so here you can see that basically
the impact I made was on the parsing
that's gonna happen so and that's really
an important port in another incentive
for thinking about minimizing you it's
raining okay so what do we want to do
the question we ask is can this fix
perturbation that I exemplified in these
two examples can it be done in automatic
automatically so that we'll get
eventually better energy analysis the
vide the key question is can we do it
based on training data only so I want to
be able to given test instance modified
based on the training data information
alone so and if there is that I'm going
to collect some statistics on the
training data and this will allow me to
determine what needs to be filtered in
the sentence in half and I'm going to
show some experimental evidence also a
semantical evening where we create on
Wall Street Journal and test on some
future data and from ah okay so here is
the general scheme that were suggesting
volatilization using
missions the idea is as I said without
the text was something that is similar
to what the existing more the lights I'm
going to start with a sentence every
transformation monument at work people
talk about me a little bit more details
the idea is that I'm transforming each
sentence to multiple transformed
sentences because I don't know exactly
what I need to do then I'm running the
existing model that was trained before
needs getting a lot of outputs or
potentially a lot of outputs and then
I'm going to combine their back to an
analysis so that's the scheme and the
question is of course how to do it so
the key step is that we developed a
family of what we call labeled
preserving transformations and by that I
mean a transformation that maps an
instance the target instance into a set
of instances and has the following two
properties the output instance has the
property that it's more like that it's
more likely to appeal in the training
corpus and this is a measurable thing
then the existing instance and get doubt
put in surface topography that it's
likely to be labeled preserving so I'm
hoping that I know what's going to be
the output because I'm going to get to
the bed today so examples could be
replacing the world with with synonyms
that are common in the training data or
replacing the structure that I see in
the test sentence with a structure that
is more likely to appear in training so
so we have two types of transformations
and I'm going to briefly go over these
so so one type of information is called
resource bed confirmations basically we
use resources like walnut clusters and
so on and to our knowledge and the
second class of transformational elf
conformation where the learning is done
on the training data so what do I mean
by resource based confirmation so these
could be replacements or infrequent
predicates so I look at valves that
haven't that I haven't seen a lot in
training now notice that there is going
to be some noise because how do I know
that is a welcome and relying on power
speech the girl that I ran on the
instance on the testes so but still
power speech tagger is relatively robust
so my transformations to take contexts
that depend on our speech taking I can
replace animal mode using walnut
building or clusters and so on and we
have some sentence simplification
transformations dealing with quotations
then invisible positions splitting
sentences based on the positions to two
simple sentences simplifying noun
phrases for example we can identify
conjunction and break sentences to do
all these transformation could be noisy
in downloads in general but hopefully
the boolean are so so here is an example
of dealing with quotations we know that
in the Wall Street Journal quotations
are only off the fork or mostly of the
phone someone said something in
quotation that's it that's the form da
why no idea but once we observe this in
train in training we can take this input
sentences and split it to these three
things first of all we can take the
sentence inside the code to the course
it's typically a sentencing in itself so
we can write it in a way that is more
similar to the quotes in The Wall Street
Journal three we can replace this that
could be very long
I mean this is simple example with a
canonical sentence we always replace it
by with this is good and we know how to
parse and S sub L this is good so it
doesn't affect the other one analysis of
the sentence so this is an example of
one day of the transponder we then added
class of transformations
well the idea here is that we identify
the context and the world candidate in
the tar sentence and transform the
candidate argument to a simpler argument
to a simpler context in which we expect
the semantic level to be more but the
model that we already trained to be more
bust and then we met back the world son
so again here is here is an example so
let's say this is my input sentence the
context in this case would be the person
auxiliary verb in this specific
predicate indictment and hey and I know
in testing that in this context a 2 was
the builder notation and volitans
I'm going to transform it to a simple
sentence that they know the analysis for
where the same outlet is an a0 and I
have a rule that these complex are not
gonna go all the details it encodes the
context in the target in the source and
in targets but the key thing is that I
know that if I found a 0 in the
transform sentence I'm going to label
the original one who's a 2
transformation rules you are you come up
with them before looking at this data
before there is this forward it's
supposed to be good good for anyway
yeah is this like a nearest neighbor so
you don't think of each there's a
distant your transformation defines the
distance basically saying that anything
that comes you projected you over the
closest point and that's it's I think
conceptually it's related but it's it's
a very complex way to do nearest
neighbor but basically the idea is that
I take an instance and I try to map it
or filter it to something that along
several dimensions is more similar to
what I've seen so in the sense it's kind
of like the nearest neighbor only that I
don't know how to define it as a metric
this point of Budo may be an interesting
direction to locate so at this point we
all use to any the root level algorithm
is basically a beam search triggered by
in fluent walls and walls that we see in
the training so we say this is it went
but even with old world the world that
we see are in the test so once we see
that we invoke words that we've learned
and do these conic substitutions so at
the end I have to put all this together
into one decisions and the way we do it
basically
it's we formulate it as an ILP problem
we have to make in general when we do
semantic role labeling we have to make
several interdependent decisions we have
to assign walls to all the arguments for
given credit for each predicate we have
multiple wall candidates and we have a
distribution form a model over all these
possibilities in this specific case in
addition to this for the same argument I
may have different proposed sentences
because of
and what we do is we simply Evolet the
scope of these four sentences and then
we plug them into a IP the constraint
see that NetSuite in this specific case
is half compliance just like you doing
the standard subtle modern and cold
different things for like the non
overlapping arguments verbs centers
subcategorize ation constraints you know
different information pieces that you
know for example is this is if this is
the credited in the sentence it cannot
take this argument or if this is then if
this is a type of the veil it cannot
take more than four arguments you know
things like that and I'd like to know
that like in many quarters in I've in
naturally do processing even though I LT
is community our problem it's very very
efficient even for very large problems
because of sparsity you can do it so
here are some yeah so this is just solve
the srl alone right there's no
transformation involved oh no it's the
same thing I basically take all the
outputs that I got from confirmation and
plug them into the same speed the only
you're solving the same my operation
every transformation yeah so basically I
once I identify the predicate the
standardized P is going to do this it's
going to look at each old candidate and
generated the distribution the model
proposes distribution over posters
little holes these are given in pale
what I'm doing is I'm plugging the
output form the transformations into the
same scheme I just have more candidates
and if I have multiple candidates on the
confirmation adjust
let's go that's so absolutely the same
scheme I don't even lose time in this
perspective only money I'll pay more for
that meter once for each not once for
that but we did that before I'm really
the ILP one time for srl is zero minutes
it's if you compare it to you know
future obstruction encoding things and
so on it's do so so the influence time
doesn't post but in your rules you never
change particles or not as you go okay
so you buy in fact we may add a few
particles yeah so it could be that the
problem is going to be slightly longer
we do change but but although of
magnitude it is really enhancing okay a
few experimental results so basically we
compared it to systems formal systems
that are based on journey of paths and
temporal pulse journey passes a little
bit better but in both cases you can see
quite a significant interest using the
algorithm we also compared it to
state-of-the-art systems that actually
are based on multiple paths so so
basically what the system - is there
another level of influence will pass
house with five or seven pulses and then
combine them to support the SLS so this
is one of six Thames
that's what we get so of course they get
diminishes a little bit because you
start with a much larger
significantly larger baseline we still
get about three percent gain and if we
compare it to the best system that
actually does retraining this is the
system so we went about halfway into
retraining on the training day on the
test day so there's still room to go but
at least there is some
okay so we also do analysis on Petrovich
transformation actually I don't wanna
spend time this I wanna okay so so
there's one other thing they could have
talked about and I don't wanna but just
wanna bring up the issue to do is more
to be said about the use of supply or
knowledge in adaptation and we've done
some really very preliminary work on
it's a bit the basic idea is that
sometimes you have knowledge about it
target domain but you're looking data
and basically the declarative knowledge
and the question is how to use it and
for the semantic role labeling and a few
other programs we did it also for many
our viewers taking we can actually go
for it this declarative knowledge is to
construe and so basically the objective
functions gonna have is gonna have the
original model some kind of linear model
in our case for srl this is just a
collection of classifiers will have the
standard constraints that I mentioned
before for the decision tasks
decile and we're gonna have another
component that is gonna import
additional constraints about the target
domain so for example if you move to
biology and you do named entity
recognition you want to know that
hyphenated phrases of legitimate NER and
stuff like that and it turns out that
this in itself will do it very little is
significant because there was something
that you knew about the target domain
and you just be not using it during the
day just knew something about the
expectation and you can also take this
objective function and plug it in in an
interactive learning
Nellie was something that didn't even
get more important so I don't want to
talk about this I want to end with a few
minutes talking about adaptation for
text so so this is really a cool task
that's not enough people in NLP are
walking on yeah so here are some
examples
these are sentences that people can
lights this is an engineer with the
passion tools
what it does should before laziness is
the engine of the progress we don't need
an article there and the question is how
to record this so so beautiful mansion
on perspective it's a relatively easy
task
think about it is a multi-class
classification task either you have a
collection of articles a they're nothing
or collection of foot positions define
features based on the context select a
machine learning algorithm and that is
the question of course is how to be
trained specifically what data do we use
the Train and and then we're gonna make
decisions say one versus all decision so
what is the difficulty the difficulty is
is data or and in by the way I
anticipate the question is the most
common question yes we're doing much
better than language models tend to be
six better and by ten to the six I mean
the ratio of data so so what we train on
the million examples were about the same
as language model of size 10 to the 12
so okay so the key issue I want to focus
on and in basically I'm just gonna give
examples of all general paradigms is how
can we adapt the model at least to the
first language of the item it turns out
that conceptually it's the same Wardens
context-sensitive spelling that you know
people including me walk for me okay
but there was a very interesting twist
whistle elves and the twist is that
non-native speakers make mistakes in a
systematic manner and these mistakes
often depend on the first language of
the speaker and you know many of you on
this audience are familiar with it if
you've read a lot of papers written by
students it will be more familiar with
that so the question is how can we adapt
the model to the first language of the
writer so just to give you an idea about
arrows so this is to position level
statistics by source language look at
Chinese and chef for example a lot more
per position elbows by Chinese writers
then by check objects and they're waving
water this is moderate with the notion
of confusion set so what you can see is
the confusion matrix for sample
positions for Chinese speakers so so all
is what was written in the text and this
is the distribution over what it could
have come from what should have been
there so whose 85 percent probability it
actually is on but the rest of the 15
percent could be something else and we
want to be able to figure out so this is
Lydian adaptation problem in fact we
have a lot more types of mistakes I
don't know if you can read it but you
know there's article position their
forms
now number you know a lot of stuff and
and I'm going to talk only about results
or put position and articles I just want
to point out that the number of
sentences that we have annotated it's
very small it's very difficult to do you
really need to be an expert just making
speaker isn't always enough in order to
do it and it's time-consuming so that
leads to two possible training paradigms
the simple one says you know we have a
lot of co-ed native English data take
what Wikipedia
whenever you're seated position dope the
proposition expert feature from the
and unclassified the other option would
be to use data please annotated whisper
position levels then I can see all they
hoped to tell somebody use the source
tool as part of my features over there I
cannot use the source because it's
always correct and it turns out that
this is a huge difference you want to
use the sauce because 85 90 95 percent
depending on the type of fellow in
Section in Quebec even people that make
a lot of mistakes make only about 10%
slits so you really want to use it so
what are the paradigm so one power that
says well I have a lot of data from
Wikipedia that's what I'm gonna do I'm
gonna tell an incorrect native data
without any knowledge about typical
devil the second one says I'm going to
use some knowledge about typical element
training but then I have very little
data so that attention problem is how to
use one in order to accomplish two and
almost my last slide basically the idea
or the two ideas that we they feel is on
how to use their own statistics on the
few annotated DSL sentences as a way to
push a model that was learned on a lot
native data in the right direction
so basically for each observer position
we have the distribution of a possible
collection this was taken from a very
small set of unaffected sentences but
you know it's very simple statistics
also since it could be very robust so
what can we do that the really to
allocation scheme one of them that you
can apply on generative models for
example on a base in a vase you can
train a single model for each
proposition or native data it doesn't
matter what is the confusion that you
it's going to be the same order and then
once you see a test sentence was it a
specific proposition you're going to
update the model the model trials
based on the software position and
they're all statistics so I'm going to
change files and then make a decision
when we do a discriminative lis were not
so lucky because we mustn't learn
different models for each foot position
and each confusion if you think about it
a little bit that's the key difference
in terms of architecture between naive
Bayes and say perceptron you really
depend on the confusion set what would
be negative examples so we can't do this
simple scheme what we do instead we
actually notify the training data
according to the arrow statistics so we
take Wikipedia whenever we see a
position with some probability we change
it according to the confusion matrix
that we estimated from the little
annotated data we have now we have data
then we can train with the source
feature included and that's very very
important it gives us a lot more
information the bottom line is that we
get very very significant improvements
over training on training data with this
I must sell it for the disputed the
security method there are few other
complications that I don't want to get
into because you will need a very very
small percentage of negative examples as
I said even when people make mistakes
they don't make that many mistakes so
you have to come up deflate one side
that the one side data that you ever
need be but still if you do try it
actually is doing better than the
general okay I'm done really what I
wanted to say that there is no product
ation than just looking at a perfect
minute
given X and the key message well I think
there's more potential to continue is
really it's possible to adapt without
the training so the examples that we've
generated gave us a systematic way of
changing the text to fit an existing
model but I think this is really
preliminary and there's a lot more one
can do in this area and my last point
was that we need additional other
problems and I think the DSL for them is
one very very challenging example and I
will come over thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>