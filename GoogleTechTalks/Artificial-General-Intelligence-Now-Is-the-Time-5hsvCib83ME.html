<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Artificial General Intelligence: Now Is the Time | Coder Coacher - Coaching Coders</title><meta content="Artificial General Intelligence: Now Is the Time - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Artificial General Intelligence: Now Is the Time</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5hsvCib83ME" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yesterday we took a bunch of data
regarding the expression of genes in my
stand under one calorie restriction
diets threw it into the AI system to see
well which genes are most important for
the calorie restriction process which
genes are related to each other in which
ways and you know we found cool stuff
like we have this MRPL 12 this
mitochondrial ribosomal protein gene 12
comes out as being central the calorie
restriction that a I figured that out
statistics couldn't figure it out people
couldn't figure it out you feed the data
into the AI it churns using some
statistical kind of mining machine
learning algorithms and figures out new
stuff but then a human has to go and
take that and cross correlate with
existing research literature figure out
what the hell it means figure out what
extra experiments do not doing mice to
take it to the next level so now I I
programs like this this one we wrote in
the context of my company bio mind
they're incredibly helpful tools but
they always leave it to humans to
interpret what they've done integrate
things with a wider context and port
things to new domains so my my
contention which certainly is not proved
but it's my conjecture based on my own
experience and integration of available
knowledge my contention is that general
intelligence that the human level or
Beyond is not going to come about but
kind of incrementally generalizing
narrow AI applications like making a
better and better search engine or a
better and better biology data mining
engine or a better better car driving
engine my view is it to achieve general
intelligence you're really going to set
out to make a general intelligence and
it probably won't be useful in the very
beginning I mean I've had three children
and none of them were at all useful for
a long time and it's arguable whether
neither were any use right now actually
they they're kind of cost a lot of money
and cause a lot of trouble so I mean AGI
from the only example we have when you
start out is a complete idiot I mean if
you saw a baby and didn't know it was
going to grow up with it lies there goes
wide and makes a mess I think that an
AGI it's likely to be similar in the
beginning it's not going to know very
much that's to learn an awful lot from
scratch and once it's been educated
sufficiently then it can be quite
powerful in a variety of different
domains so I do think the right approach
is to create something like an
artificial baby I don't want to
over-interpret that it doesn't have to
be like a human baby in any detail but
the point is that it may not have useful
functionalities in the beginning and
they have to be taught only once it gets
to a certain level are there going to be
practical applications from it so just
to pursue this a little further let's
say you want to make an AI baby of some
sort how do you do it where do you do it
now you could make it a purely chatbot
type thing this is a textual system I
don't think that's totally infeasible
but I've come to the conclusion it's
probably not the best approach some
folks take a rodney brooks type of
attitude you got to make a physical
robot and i think that's great it's just
a big hassle and i've taken around with
robots and it just you spend all your
time dealing with actuator and then
sensor problems and maybe it's good if
you have a huge budget and a team of
roboticists but i I've become attracted
to a sort of intermediary option which
is embodiment in virtual worlds where
you still have perception action
cognition and social interaction but you
don't have to do with all the
nitty-gritty of moving the robot arm
here there the bump sensor broke and
there's too much glare for the camera
high and so forth the argument against
virtual worlds is it right now virtual
worlds don't
the sensory and motor 'ok richness of
the real world I'm the physics
simulations in virtual worlds are not
that awesome the amount of data in
virtual worlds is not that much so to
the extent that human level general
intelligence just depends on a huge
richness of perception and a huge kind
of flexibility of affordances in
movements virtual worlds right now
aren't as good as the real world but I
think virtual world technology is
improving it a really incredible pace
mostly due to video games being so
popular so I'm I'm fairly optimistic
that virtual worlds are going to get
better and better so it's to support
robust learning for artificial general
intelligence and it this is a screenshot
from a little virtual world that we made
in our own AI project called AGI sim
which is just based on an open-source
game engine crystal space you know Rai
controls that that little guy there and
the human being can control that the
other and in this screenshot we were
just running the AI through some very
basic learning experiments based on the
psychological theories of Piazza where
you basically the teacher takes a little
bunny and hides it in the box then the
baby is just got to figure out that the
bunny's probably going to be in the same
box that was hidden in which is really
simple of course you could program in a
I to know that with one line of code and
in a good programming language and more
in C++ which our system is programmed in
but the point is a human baby actually
doesn't know that objects are permanent
like a human baby doesn't know that if I
put my cell phone behind my back it's
still there and it's likely to reemerge
somewhere eventually somewhere between
six and nine months the human baby
figures that out and so we've done the
experiments making our AI system figured
out based on the body experiences as
well and that's taking things down to a
really primitive level of course we've
also done experiments programming in
knowledge like that and teaching it more
more advanced stuff but you can see that
they
bodied modality lets you kind of take a
very basic primitive approach to
teaching an AI system to understand
itself and the world and its
surroundings there's also existing
commercial virtual worlds like Second
Life which this is a screenshot from so
they're their virtual pets and second
life and the avatars that people control
as well as avatars are controlled by
simple scripts right now and second life
is interesting because it's more rich
than a simple simulation world that we
built just for testing I mean there's
millions of subscribers to second life
there's a huge kind of virtual
topography there with all kinds of stuff
going on so if you put your AI brain
inside this dog or inside this guy and
it goes around in second life there are
people to interact with it there are
things for to do to their people will
chase it around they can chase people
around people will save stuff to Lee
that's to figure out how to react and
this is something that we're actively
working on though I'll talk about a
little laters using our I autism to
control various sorts of agents in in
second life and make use of the richness
of that environment now as I said right
now second life physics does exist but
it's fairly primitive and we've got
Newton's laws in there we've got
friction we don't have fluid mechanics
for example but i think all all that is
going to come because in the ps3 you do
have fluid mechanics and so forth it
just hasn't been ported to this this
domain so so far all I've done is to say
some generalities I think we should work
on artificial general intelligence
directly I think we should do it using
virtual embodiment so I think that the
the best path forward for the AI field
in terms of the grand old time goals of
the field of really making a thinking
machine at the human level or beyond I
think the right thing to do is just to
focus on making programs that control
embodied agents in virtual worlds and
learn to act like a little baby
her dog and so forth and once they've
mastered those simple behaviors teach it
more and more and more and more stuff
until it learns more and more through
interacting with people in a shared
perception and action context and that
that's my best guess for the right
general way to approach the AGI problem
now of course that in itself doesn't
tell you very much and it's not very
original either only people have been
talking about that kind of stuff since
well before I was born I'm I'm 40 years
old so there's knows nothing there's
nothing very new there although I found
it disturbing that so much of the AI
feel has digressed so far from that with
all sorts of of other things because I
do think that focusing on this sort of
stuff it's still the most likely way to
get to the end goal when I'm going to
talk about and for the rest of the time
I have is my own AI architecture which
according to my best guess one complete
is likely to be capable of achieving
this goal and certainly in 20-25 minutes
I wouldn't be able to convince anyone
that the architecture really will work
as I think I will even if someone had
all relevant knowledge and was
emotionally extremely well well disposed
toward it it's just an awful lot of
detail and we're developing the Nova
meant a system within a startup company
no Clemente LLC whose business model is
focused on controlling virtual agents in
second life and in massive multiplayer
online games and training simulations so
that all the description of the system
is not published at this point I mean we
have hundreds of pages of internal
documentation I have published some
overview papers of the architecture in
various AI conference proceedings it's a
triple AI and I Triple E conferences and
so forth so if if you're curious for
looking a little more depth
on the architecture the moment is saying
the next 20 minutes the website Nova
meant a.net has a papers page and you
can look at some of the eight-page
conference papers which of course don't
tell you anywhere near everything but
I'll position the approach of a little
more carefully in your mind so the Nova
meant a system how does it work well it
is as I hinted before an integrative AI
architecture and I started by trying to
come up with a holistic system theoretic
understanding of how cognition work so I
didn't start with a particular algorithm
knowledge representation I didn't start
from computer science at all I started
from systems theory and cognitive
science so what are the parts of the
mind has to have one of the overall high
level dynamics of a mind how do they
interact with each other what are the
emergent structures then I took a step
back and said well how the hell could
these things possibly be achieved using
tractable II implementable computer
science algorithms not necessarily by
imitating the brain because I don't
think we know enough about the brain to
to use the brain as a detailed God but
using computer science algorithms
integrated in an appropriate way to give
rise to the overall structures and
dynamics of the mind and that's a the
high-level approach that was taken right
now we're not done building the thing
there's a detailed design or maybe forty
to fifty percent complete with the
implementation and detailed design of it
what we have now is enough to control
agents doing some cool things in
simulation worlds but nowhere near where
we'd like it to be so we're
progressively building out the system
while applying the system for agent
control either so let's just some of the
overview papers which you can see on the
website so I'm going to I'm going to now
briefly jump through some of the
technical stuff
I'll talk about knowledge representation
a bit about software architecture I'm
going to end up pretty much glossing
over the cognitive processes which is
the most important part but there's not
that much time and then the the
high-level emergence structures that are
hoped to arise within the system once
once it's complete so first just a bit
on the philosophy of mind underlying
this whole thing which i don't think
it's dramatically original but I found
it useful to formulate it in a kind of
precise way so I view an intelligence
system as a system that recognizes
patterns in the world and in itself and
key to this I think is a reflexive
process of a system recognizing patterns
in itself then improving itself based on
those patterns that doesn't entail like
deep source code level self modification
necessarily although it could but it
entails learning and introspective
learning which humans do from a pretty
early age and keep doing throughout
throughout the course of their lives and
a key part of this is the development of
what psychologists and philosophers mind
called the cell for the phenomenal self
the image within the system of the
system itself if an AI can't even
recognize itself as a pattern in the
world and it is bodies interactions with
the world it's not going to have much
grounding for for any other kind of
flexible intelligence so a lot of the
key of getting a GI to emerge it i
believe is getting a system to be good
enough at pattern recognition and it's
its world that can recognize what it is
in terms of how other things react to it
and that's a lot of what a little baby
does in the first year or so of its life
because when the baby is born it doesn't
know the difference between itself it's
mom and the bed it's lying on and at a
certain point it gets this idea like hey
I'm I'm this thing here my mom is that
thing there this pillow is this thing
here this skin like is me but then when
I touch something else I feel something
else this basic understanding of what
yourself is
it's really critical to ongoing learning
and cognition so as noted before I
consider intelligence as the ability to
achieve complex goals and complex
environments and starting to edge toward
the technical side of things one would
look at the achievement of goals is a
system recognizing uncertain patterns of
the form well if I carry out this
procedure in this context I'll achieve
this goal and again that doesn't say
very much because it's pretty obvious if
you could solve that problem in general
you could do anything but in terms of
generally structuring what the problem
is this is not necessarily the way
people would look at things from say a
neural net or a crisp logic theorem
proving approach approach to AI one
thing you know this is I athletes
probability theory and uncertainty that
the core of the approach will see that
as I launch into the into the details so
I had a book published last year called
the hidden pattern which pretty much
just refuse philosophy of mind going
through the holga motive issues and
cognitive science and philosophy of mind
and trying to largely explain what why
they're not major problems and how they
can be resolved pretty simply in the
context of viewing the mount is a big
system of patterns that recognizes
patterns in the world and itself so
getting toward the nitty-gritty how does
our nova monetary system work starting
off with knowledge representation and I
almost hesitate to use a term knowledge
representations it can be misleading
because I think a lot of what an AGI
system has to do is learn how to
represent knowledge so you can almost
think that this is a proto knowledge
representation and the system has to
build its own context specific knowledge
representations on top of that for
dealing with different sorts of things
but the low level kind of pro
knowledge representation of Nova Monta
it's a graph data structure you got
nodes you got links and to look at it
really crudely it's kind of a synthesis
of probabilistic semantic networks with
a tractor and neural networks in the
sense that you have nodes and links and
you have weights on the nodes and links
that represent probabilistic truth
values you also have weights on the
nodes and links that represent what we
call attention values which are sort of
like activations or weights in a neural
network so we're we're putting together
attentional type stuff like an in a NN
with truth value type stuff like in a
probabilistic semantic network it's not
a neural networking that we're not
trying to do low-level brain modeling
it's also not really pure semantic
network because we're not just
representing high-level conceptual
knowledge we can represent procedures to
do stuff percepts that have come in and
so forth as well as high-level
conceptual semantics and in the same
graph so the number is associated with
the nodes and links include numbers
called attention values each node or
link has two numbers a short-term
importance and a long-term importance
attached to them and roughly speaking
the short-term importance of an odor
link dictates how much attention is paid
to it so the kind of short-term memory
the attentional focus our working memory
is the things with the highest
short-term importance the long-term
importance dictates what if something
gets kicked out of RAM onto disk or not
basically and forgetting has been a big
focus of our work because of any system
that is constantly perceiving a
simulation world and generating new
ideas generates way too much knowledge
to keep in rem so you need a fairly
sophisticated system to guess what may
be relevant in the future and we also
have truth values probabilistic truth
values we use a particular system where
we use a two-component truth value each
piece of knowledge has a probability and
all
number we call the weight of evidence
which tells you how much evidence was
gathered to support that probability and
there's a bunch of math underlying there
that actually connects with impress ice
probability theory interval
probabilities if anyone's looked at that
but the P in general pieces of knowledge
and the knowledge base are weighted with
these two valued truth values and the
two valued attention values we also have
a typology of nodes and links and the
name should be taken with a grain of
salt but we have nodes that represent
percepts coming in from the external
world nodes that represent little
procedures for doing stuff like move
joint at this angle and so forth and we
have no jikan represent abstract
concepts and basically nodes that are
just tokens whose only purpose is to be
linked together by links another concept
can be viewed as a kind of sub graph for
linkage structure among a bunch of nodes
and links so that there's a lot of
science to this and actually the
knowledge representation scheme is
described fairly well on some of the
available publications but the the basic
idea is we're using a weighted labelled
hypergraph knowledge representation
where the weights carious semantics
regarding attention on different time
scales and semantics regarding
probabilistic truth value with it within
the same network so this just
graphically depicts it in the same
network we can have nodes representing
stuff about joints and actuators being
on at a certain time nodes with no name
at all in English but they're just
meaningful in terms of their relations
other thing this is a node representing
the at certain particular instance of
raising your arm this is a node
wrestling the general concept of raising
an arm and all this stuff can be in the
same network that's managing in the same
way so you can you can have links to
knowing a generic association between
things and I always hate to give these
examples because they're a bit
misleading because most nodes in the
system wouldn't have any English name
but some of them
corresponds to concepts that could be
represented English and those happen to
be the easiest ones to make slides about
but no no it's gonna have a generic
association between each other just
meaning that they tend to have been
useful at the same time you can have
explicit logic represented say that just
a nodes and links representing a
predicate relationship Coffee is often
in a coffee cup that would have to have
some probabilistic truth value but that
this is really only going to be useful
when embedded in some other link
structure because only in some contexts
is that true right i mean if it's a
coffee bean on the plantation it's not
going to be in the coffee cup so that
they're really relevant Nodin link
structures as in any pragmatic system
using logic are going to be big nodes
and links structures with a lot of
uncertainty associated with them so
people often drink coffee from a coffee
cup again we need a whole bunch of these
with probabilistic lightings and
contextual embeddings to to do any use
but nevertheless logical representation
with appropriate probabilistic waiting
is a big part of what we're doing and
it's important that that's overlaid with
this kind of neural net like
associational representation and I think
you need both of those operating
together effectively to adequately
represent knowledge through a general
intelligence with a graph type knowledge
representation so software architecture
it's important but it's kind of standard
stuff so I'll go through it pretty
quickly Emily we have a big container of
nodes and links which with all the atom
space and a bunch of objects called mind
agents that that act on that carrying
out different cognitive processes then
we make it a distributed system we can
have a whole bunch of those on different
machines that all hooked together with
each other we can draw our kind of boxes
and lines diagram look like anyone else
can and I think all these diagrams
really kind of look the same because
this level cognitive science tells you
you have perception you have language
you have actuation control you have
memory you have goals and feelings and
this looks a lot like the diagrams you'd
see from stan Franklin's lida system or
Aaron's loehmann's cognitive
architecture and not that different from
Minsky Society of mind or a motion
machine if you kind of compressed a
bunch of Minsky's little boxes into the
bigger boxes and I think that it's
important to understand things on a high
level like this but ultimately
intelligence comes down to one of the
dynamic processes going on inside the
boxes and how do they interact with each
other rather than this this kind of of
high-level portrayal a different way of
looking at things as a basic kind of
animal like cycle for for interacting
with the world where in perceptions come
into memory they elicit feelings in the
system where feeling can be thought of
as a kind of internal sensor the system
has a certain goal some of which you may
have supplied some of which it may have
formulated itself by refining the
supplied goals then it figure out
figures out what to do puts a bunch of
procedures and some pool of active
procedures then does something in the
world then perceives it again and this
basic animal interaction type of loop
it's just a different way of looking at
the same diagram and the crux of it is
in the cognitive processes that occur
inside the system which I'm in no way
you're going to be able to come close to
doing justice to in the five minutes and
I'm now allowing to it and at a high
level we can look at three categories of
cognitive processes occurring in the
novel meditate system one is what I call
global processes these are cognitive
processes that go through everything in
the knowledge base and just iterate an
example of that is assigning a long term
importance value periodically just have
to go through and see how important is
this thing upgrade or downgrade the
long-term importance and then kick out
of round the things are unimportant and
you get scotts cycle through and do that
with it with everything we have what are
called
all processes which are kind of
specialized stuff like executing actions
there's a collection of active
procedures who drew called schema and
you just got to go through and execute
them and we we use a kind of action
selection algorithm similar to stand
Franklin's action selection approach
actually which is it's related to pattie
maes behavior nuts then we have the
essence of it is what I call focused
cognitive processes and these are
cognitive processes that get a small set
of nodes and links from the overall
table and do stuff on that small set to
produce more nodes and links in the
table and this includes logical
reasoning it includes some evolutionary
learning type stuff and that's that's
really where the crux of the thinking is
going on this stuff is kind of
mechanical so if i wanted to summarize
it in a phrase the philosophy that we've
used in crafting the set of cognitive
processes i would put it like this from
a cute computer science perspective i I
become convinced that essentially every
cognitive algorithm used with an
intelligence says exponential complexity
pretty much all you're doing is making
uncontrollable insane combinatorial
explosion it's one way or another woman
if use evolutionary learning your
population size that you need just blows
up when you're trying to learn hard
problems if you're doing logical
inference the process of inference tree
pruning and forward and backward
chaining this leads you to horrible
combinatorial explosions that are hard
to prune so that the whole essence of
making an AGR design that can work i
believe is making an integrative system
that combines various purpose specific
AI algorithms in such a way that they
can cooperate and kind of quelf or
ameliorate each other's exponential
combinatorial explosions rather than
making them them worse and worse so I
don't think there's anything one
algorithm that's critical to
intelligence and in fact I think any one
algorithm is just going to blow up in an
unacceptable way which is what you see
you all throughout the history of they
are
the question is whether you can hook
different algorithms together so they
can kind of calm each other down by
decreasing the constant outside the
exponential and the exponential time
complex tin since I don't have that much
time I'm just I'm going to skip some of
the slides and just verbally give what I
think it's a nicest example of that so
our two most critical cognitive
processes in Nova monta are on the one
hand an algorithm for probabilistic
evolutionary learning and on the other
hand a probabilistic logic engine and
these are both critical ways of taking
nodes and links from the atom table and
creating new nodes and links so the
probabilistic logic engine it's
something I spent several years old and
I think is the best existing integration
of kind of theorem proving logic with
quantifiers and variables and all the
nice stuff with probability theory
measuring uncertainty in a fairly
sophisticated way using emphasized
probabilities so getting logic and
probability to work together it's nice
it lets you do logical theorem proving
about stuff like controlling an agent in
the world and throwing balls and playing
fetch and stuff on the other hand when
you try to learn complex stuff with it
what happens is that you're on the same
problem everyone else does an inference
tree pruning and forward and backward
chaining inference just becomes
unsustainable in terms of the
combinatorial explosion you say how do
you control these entrance trees on the
other hand evolutionary learning a lot
of you are probably familiar with
genetic programming what we're using is
something called Moses which was
developed by Nova meant a co-founder
motion looks and his PhD thesis at
Washington University in st. Louis and
what Moses does as compared to genetic
programming is you learn a bunch of
little programs to achieve some fitness
function instead of doing crossover and
mutation to generate new programs from
the pool of existing ones you do some
probabilistic modeling you build a
probabilistic model
which program trees are good in which
program trees aren't then you do
instance generation from that
probabilistic model to generate new
program trees and that works way way
better than genetic programming a lot of
examples and in most of speech d thesis
he just used it for some basic
categorization and symbolic regression
but since that point within nova mente
we've used it true aging control a bunch
of other stuff but nevertheless when you
try to scaled up to do learning of large
programs with programmatic constructs
like loops recursion lists and all this
stuff you still run into a nasty
combinatorial explosion look at the
population size just gets really big
when you make the tree size too big and
the operators are the nodes to advance
so here we have two really nice
algorithms which however considered in
themselves meet the fate of every other
algorithm in the history of AI which is
they do well at toy problems and when
you try to scale them up too big you
just run into unsupportable
combinatorial explosions what we're
trying to do within our architecture is
get these two algorithms to help
ameliorate each other's combinatorial
explosions and this happens in a couple
ways so in the Moses probabilistic
learning thing when you have a bunch of
program tree satisfying some fitness
function when you're probabilistically
making a model of which program trees
are good and which ones are bad what you
can do is use the probabilistic logic
engine to help with that modeling you
can use the probabilistic logic engine
to help do reasoning based on the
system's long-term memory and the
context in which the system is operating
make inferences about which program
trees are good and that way if you have
an effective probabilistic logic system
incorporating long-term memory it can
help a lot with doing the probabilistic
modeling within evolutionary learning on
the other hand within the probabilistic
logic engine when the logic engine hits
a dead end you can then say well we have
all these options to explore in logical
theorem proving we don't know which one
is any good well let's take one of them
let's take the one that seems most
important according to the short-term
important
system and let's use evolutionary
learning to see what patterns we can
mind about about this concept so if if
you're trying to prove something in one
of the one of the nodes that needs
expansion your backward or forward
chaining probabilistic inference tree is
a node representing cats you can then
use probabilistic evolutionary learning
to mine the knowledge base for
interesting patterns about cats you're
then stepping out of the domain of logic
you're doing evolutionary pattern mining
to extract relationships put them back
into the knowledge base then you re
expand your inference tree using the
knowledge gained by evolutionary pattern
mining so you're trying to use
evolutionary learning to bust the
forward and backward chaining
probabilistic inference process out of
dead ends while at the same time trying
to use probabilistic logic inference to
accelerate evolutionary learning to make
its modeling faster and that's that's
just one example of two cognitive
processes and how we're trying to get
them to learn from each other to quality
each other's combinatorial explosions
there's other things that have to be
drawn in as well I haven't even gone
into attention allocation how does the
system decide which things to pay
attention to which one's not to we use
some actually simulated economic stuff
they're modeled on some of their Eric
bombs work in Hayek but I think that the
key point I wanted to get across there
and I just skipped through all the
details on cognitive processes which I
sort of went through verbally but the
the key point I want to get across there
is yes you can integrate a whole bunch
of cool learning algorithms evolutionary
learning to learn procedures to its
Steve stuff in the world probabilistic
logic to reason on existing knowledge
and generate new knowledge unless you
can plug these algorithms all into each
other to get them to improve each
other's performance and stop together
from blowing up combinatorially you're
not going to create an AGI and this is
part of why I think you really have to
be working on a GI to build a GI because
I think that if you're just working on
one application you're going to be able
to find some trick to make someone
algorithm do what you want that's why I
tweaking it and in environment for matix
with that genetics graph I showed before
what we use there is we
is Moses a probabilistic evolutionary
learning thing but because it's a
particular domain we could do some funky
parameter joining them pre-processing to
get moves it Moses just to work well and
that bioinformatics domain we don't need
all this nasty inter process integration
stuff but my contention is it for
embodied agent control and in the
flexible context in a simulated world
you're not going to be able to use
tricks to overcome the combinatorial
explosions intrinsic and one day I
algorithm you're going to need to use
integration in an appropriate
architecture to get various algorithms
to ameliorate each other's combinatorial
explosions and where does that
ultimately lead getting back to their
the high level of things I
I'm going to skip slides and just talk
more it's more fun i'm sick of
powerpoint okay the ultimate crux of
this as i said is getting the system to
recognize recognize its own self as a
pattern in the world and i think we can
get there by integrating a whole bunch
of these different cognitive algorithms
with it within the architecture that
they've outlined and what i think of as
the fundamental dynamic of cognition
it's what I call a loop of combining
followed by explaining followed by
combining followed by explaining
followed by combining and so forth so
that we have cognitive processes in the
system that take knowledge and then
generate new information from that
knowledge using logic inference using
probabilistic evolutionary learning and
other methods and then we have cognitive
processes that they try to explain
what's there and you need an interaction
between the interaction between these
two things and in the dynamics of your
cognitive system and the crux of this is
the ability of the system to understand
it its own self so in terms of an agent
interacting in a 3d simulation world
what you have is a system that it
observes it a bunch of things in the
world it observes itself doing things in
the world and then it draws logical
conclusions from that and it recognizes
inductive patterns in that it puts those
patterns and in its own mind that then
directs its behavior it's based on the
knowledge gathered then it has to model
itself and say well what could I be so
as to act the way that I see myself
acting then it uses reasoning
evolutionary learning and the other
things it's bag of tricks to make a
guess of what could I be in order to
display these behaviors and I
displaying that model that explanation
feeds back into its own mind and is then
used to generate new ideas and new bits
of knowledge which then causes attack
differently and then has to explain how
it acted differently over again and it
just kind of loops around and the the
question is can you get is sophisticated
enough collection of cognitive
algorithms put together to kind of fuel
that loop where the thing studying is
what it does tries to explain what it
does that directs its actions which it
then has to re-explain and you keep
going around and around and around and
my hypothesis is that the collection of
algorithms in the architecture we've put
together in the Nova meant a system are
going to be enough in a simulated world
context to allow the system to observe
what it's doing in the simulation world
gather data about its own actions store
that data about its own actions in its
memory model what must I be in order to
carry out these actions in the
simulation world draw conclusions from
that use those to direct its actions and
keep going around and round no
ultimately I spent some time trying to
validate this idea mathematically I came
out with a nice list of 17 mathematical
conjectures were if I could prove them
they would validate that this approach
to AI can work and then I set them aside
and put them on my hard drive and ever
looked at him since I decided it would
take me 10 years to prove all those
things mathematically and I'd rather
just focus on building the system and
then validating or refuting the
hypothesis empirically and this last
side is just a rundown of what we've
done so far I mean we built the core
system with the atom table nodes and
links and so forth we have a logical
reasoning engine using probability
theory to guide on certain logical
inference we have that the Moses
evolutionary learning algorithm which
you've done a lot of simple things
within a handful of complex things we
can enact learned procedures in a
simulation world and we've been working
with our own simulation world but we're
going to be rolling out some AI
controlled products in second life
during during the next year which should
be pretty interesting and give more
facility for interaction and we've also
done a bit of natural language
processing dis to allow us to
communicate with the system and then
control what it does but those there
certainly is a long path ahead of us to
get this thing to work and as I said in
the very beginning at this high level of
abstraction there's certainly no way I'm
going to convince anyone that this is a
viable approach in detail even if I
hadn't skipped half the slide just due
to the lack of time but I do think
there's a compelling reasons to think
that if we're going to achieve
artificial general intelligence we're
going to have to work on artificial
general intelligence and that embodied
learning is a clear path there than
anything else virtual embodiment is
easier than physical robotics embodiment
none of the existing paradigms of a I
are going to be enough on their own so
we either got to invent something
totally wild ass and new or take an
integrative approach where you put
together the best pieces of existing AI
paradigms and if you are going to put
together pieces of existing AI paradigms
somehow you have to confront all these
combinatorial explosions that exist in
in every one of them and get the pieces
from different paradigms to work
together then that's the framework
within which we've been operating and if
you keep an eye on Nova metate net for
the next couple years hopefully you'll
see progressively more intelligent
systems operating in second life and
game worlds and so forth we're
going to start out with anything
dramatically super intelligent the idea
is to do just like with a human baby
where it starts out of the relatively
limited but flexible autonomous an
exploratory system then gains more and
more functionality through learning and
interaction any questions yeah well I
think yes he said Jeff Hawkins in New
manta has very similar goals and what do
I think of his sister well I would say
that the goals of our project are not
the most unique thing about it because
if these goals have been around since at
least the the 50s I guess and the
Hawkins approach is more biologically
based at least in principle although
when you look at what he has when he
really has is a kind of pyramidal vision
architecture with a kind of hierarchical
Bayes net superpose on top of it and
what what I feel is that is perhaps a
decent qualitative model of visual
cortex and I don't feel accounts very
well for language learning motor control
not level theorem proving dreaming you
know all kinds of other aspects of
cognition done in his book he talks a
lot about these other things I think
it's philosophy of the memory prediction
framework it's fine so far as it goes
and you can probably map it into my
philosophy of mind as pattern
recognition and so forth but when you
look at the detailed stuffies doing it's
pretty much hierarchical pattern
recognition which in my view doesn't
carry that far toward making a thinking
machine i think that's that's more along
the lines of one particular algorithm
which you can tune to do one particular
thing like recognize patterns in streams
of data you can overcome the
combinatorial explosion problem just
right domain specialization rather than
by fundamentally coming to grips with it
yep you mentioned that it's a C++ and
scalable and typically that's usually
holds that you can tell things in
parallel and many machines without too
much enter communication between the
parts I don't know what you put into
scalable meaning in every pocket update
the system needs to be in order to
achieve this in today's resources well
the honest answer is we don't know how
many machines will be needed to achieve
what human baby level intelligence or
even the chipmunk level intelligence say
I mean we've done back in the envelope
calculations which suggested it's not
millions or billions of current machines
like you guys at this company I'm sure
have more than enough computational
resources so we're a small company with
a server farm of a few dozen machines
which I think it is not going to be
enough to make a virtual bun gürtel but
really there's a lot of research to be
done to figure it exactly how many
machines are going to be need to achieve
a given level of functionality but in in
terms of the overall architecture i mean
what i mean by scalable is you know its
runs in a distributed network of
multiprocessor linux boxes and you can
add more machines on to it and the
intelligence what will increase
gracefully rather than the thing getting
overwhelmed and crashing but there's
certainly a layer of complexity there
and that if if you have a note on this
machine a note on that machine and the
guy on this machine links to the guy on
that machine then it's going to be a lot
slower to get a message across from here
to there then if they're on one machine
so you you have an additional annoying
level of complexity of trying to group
nodes that cluster together in terms of
their internal links on the same
machines and we've we have code in place
that handles that kind of thing it
hasn't been stressed and tuned
extensively and we'd sure be a lot
happier that
massive supercomputer a direct processor
to processor interconnect fabrics or
something but I don't think hardware is
really the bottleneck and getting to AGI
though I think that the bottom lock is
getting all the algorithms tuned and
Inter operating correctly and then the
the absolute worst that's going to
happen is once we have the thinking
machine all designed and working then
you got to wait five years for hardware
to catch up so that so you can run it
within within your budget yeah
I am a good friend of you goes and we've
talked a lot about working together we
haven't yet quite not to the point of
doing anything practical together what
what we have discussed doing is the the
Moses probabilistic evolutionary
learning algorithm being run on the FPGA
is according to a design of Hugo's
because he's done a bunch of stuff with
basically genetic programming we have
neural networks but I didn't really have
to be neural networks genetic
programming using FPGAs to massively
accelerate things and we've tossed
around a bunch of ideas for doing the
same thing for the probabilistic
evolutionary learning algorithms but we
we haven't actually done the work but
there's a this is an area where some
advanced hardware design would be useful
actually because FPGAs can do processing
really fast but they don't have that
much on board read what we really need
is an fpga with a shitload of onboard
ram then you can massively speed up the
kind of evolutionary learning we use
which would make it a lot cheaper to to
build a super thinking machine server
phone but we can save that for when we
become a Google scale I accompanying can
invite hardware companies and subvert
them to our purposes yeah
right well actually that there that is a
technical detail I did not go into it
also each thing that comes in
perceptually has a time stamp associated
with it and there's actually a special
index on each server which is called the
time server which allows various
processes to look up Adams bye-bye time
interval so so we actually made a
separate time management system and
there's a bunch of stuff like that which
is just not going into here but an awful
lot of attention is gone into various
kind of indexes into the knowledge for
efficient access which is a pain in the
ass but seems it seems to be necessary
to get things to work in real time and
that's one thing I would say about the
virtual embodiment is needing the
control and agent in real time imposes a
lot of discipline on you as an AI
developer because the things just have
to happen fast and most academic AI
approaches just on aren't up to to
real-time control infeasible
computational resources yeah
Oh a good question what's it what the
question is what's the most interesting
thing we've seen the system do so far
the thing that interested in most on I
would say is when when training the
system to play fetch and to play tag
just simple games the mistakes it makes
and the kind of pathological ways of
playing fetch and tag that it can come
up with are interesting and we we've not
tried to get the system do anything
really advanced so we've done this kind
of simple moving around picking stuff up
and then the simulation will so that the
AG the pro no a GI system has way less
interesting behaviors and our
specialized bioinformatics systems or
something to discover new things about
Parkinson's disease and chronic fatigue
syndrome and so on but it's interesting
in that if if you take a really simple
reinforcement learning and try to teach
it to play fetch it doesn't make the
same kind of mistakes this system does
when you try to teach it to play fetch
and you can you can vary that the kind
of partial reward that you give it for
playing fetch or tag correctly and see
how it will misinterpret the the partial
reward then which pathologies it will
get there going going to pick up the
thing that you through and kind of
coming near you and teasing you but not
not quite giving it to you and so forth
and so you you can see that the Inklings
of a personality emerging them I'm
excited about that in terms of rolling
out of the system for controlling agents
in second life because once we do that
which would happen around the end of
this year then we'll have a whole bunch
of people interacting with it and
playing with it and it should learn a
lot from those interactions
alright well thanks for your time it's
been a interesting interesting place to
give this talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>