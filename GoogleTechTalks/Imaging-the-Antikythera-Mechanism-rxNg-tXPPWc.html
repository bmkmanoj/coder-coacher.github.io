<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Imaging the Antikythera Mechanism | Coder Coacher - Coaching Coders</title><meta content="Imaging the Antikythera Mechanism - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Imaging the Antikythera Mechanism</b></h2><h5 class="post__date">2010-03-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rxNg-tXPPWc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to google techtalks my name is
Donald Tanguay and today we have a
special guest talking to us his name is
Tom Maas bender he's from Hewlett
Packard Laboratories just down the
street in Palo Alto and Tom is a senior
researcher from HP Labs he has worked in
many things that while HP Labs including
one of the earliest tablet devices and
some brain connective kinds of things
before moving into computer graphics
I guess about 23 years ago and he's done
things like volumetric rendering and
probably polynomial texture maps things
like that and today he's going to talk
about one of his really exciting
applications of his techniques about the
etiquette of mechanism and so this is an
intersection of computer vision computer
graphics in archeology and take it away
Tom great thanks al
so I guess I want to start off and say
that if you have questions during the
talk feel free to ask them the best
thing would be to go up to the
microphone and ask the question so I
don't have to repeat all the questions
but that would be ideal
so basically in 2001 a fellow HP Labs
researcher and I developed a method
called polynomial polynomial texture
mapping and it's a an interesting
technique in that it actually allows you
to see more detail on the surface of
objects than you can typically see even
holding them in your hand and in 2005 we
got sent to Greece to apply the
technique to a mechanism called
Antikythera mechanism and this talk will
cover both the the imaging method itself
and the Antikythera mechanism and then
at the end of the talk if there's time
left I'll go through just some more
recent developments that we have on the
imaging method over the last 10 years or
so so it's well known that as you change
lighting direction you can see more or
less detail on the surface of object
this is something that we use all the
time we hold objects relative to a light
source and move them around but what's
not so well understood is that if you
actually change the material properties
of the object itself you can often see
more
detail then then is that as possible
with the original material properties of
the object so the technique got
developed in 2001 at the time there were
two main ways of applying textures to
the surface of objects for computer
graphics and we did develop this as a
method for computer graphics although
nowadays it gets used more in fields
like archeology and forensics than 3d
graphics but it but you know one of
those techniques is texture mapping that
got developed in the mid-70s and texture
mapping you've all seen when you play a
video game the characters on it on it
are all texture mapped meaning that an
image has been applied to the surface of
them to give you a field that there's
more complexity there than then you've
geometrically modeled great technique
the problem with it is is you changed
lighting interactively in on texture
Maps the lighting doesn't do the right
thing to represent the underlying
geometry so Jim Blinn addressed that
problem in the late 70s by a technique
called bump mapping and what you do with
that is you keep around surface normals
for every pixel that you have in the
texture map and then you can use
relighting techniques to use that normal
to produce an image of what the surface
would look like under the new light
source directions that also works very
well the problem with it though is that
it's not image based typically you have
artists draw these these bump maps and
then you know they're applied onto onto
the surface of objects so it's not a
photographic technique the the results
are typically not photorealistic
so we developed polynomial texture maps
which are well I'll just I'll just show
you one jump right into it
this is a PTM or polynomial texture map
here and what we do what we have is is
control interactive control of lighting
here and the way we do this is for every
pixel we keep around a reflectance
function so that's what's shown on the
right here and so every pixel you know
has this simple two-dimensional function
associated with it and you can change
lighting direction by probing in the
circular area on the right and you can
what the reflecting functions are that
we're modeling on the by probing the
image itself for every pixel so this is
done independently for every pixel so
that's a PTM how do you how do you
collect these things well here are two
objects that we've used the first one
was the first thing I built they say in
my garage at home it's just a dowel
wooden dowel assembly with hot melt glue
and you simply put a digital camera on a
tripod on the top of it looking down on
the floor of this thing and put an
object in the on the floor and move a
table lamp to each face of this
icosahedron and take in that case 40
pictures of the same thing under
different lighting directions it's
pretty simple simple technique and on
the right here is just a more elaborate
version that we built a year or two
later that has computer-controlled light
sources this all plugs into a laptop and
you know with one one stroke of a key
you can collect all in this case 50
images of the same thing under a
different lighting direction so no
matter what kind of hardware you used to
collect it what you have is a stack of
images where you've got a measurement of
for each pixel you have a measurement of
the color as a function of lighting
direction and then we just fit quite a
by quadratic polynomial to that
description of values so specifically
what we do is we fit a a by quadratic
polynomial to the luminance to the you
know brightness basically of that pixel
and you'll see that these these L use
and LVS those are just points in that
circular space I was moving around
before they're just projections of the
lighting vector onto the the plane of
the texture map so it's just the
two-dimensional parameterization of the
light source direction and these a 0
through a 5s are what we keep around in
the texture map that coefficients in
addition to the the unscaled color
values so the polynomial is used to
compute the luminance that gets plugged
into this set of equations here where we
take our or unscaled RGB values and just
multiplying by those luminance values
that we computed to recover what the
what the real RGB values are and that's
that's how we do the renderings those
are the RGB values that that are drawn
now
one thing to point out about this
equation is that it's really simple you
know it consists only of multiplies and
adds and therefore you can use parallel
sub word instructions in cpu to evaluate
this very quickly so the P Theon's that
I'll be showing here don't use any
graphics hardware support they just run
on the CPU directly and you know usually
give you real-time performance out so I
should backpedal a little bit yep so you
use quadratic polynomials here but when
you have shadows these are step
functions as a function of light if
somebody you know suddenly falls into
the shadow of some other part of your
scene does the quadratic deal with that
yeah
so that's right so you know for
representing sharp details in lighting
space you know it's not an adequately
high degree enough polynomial to
represent very sharp edges so why winds
up happening is when we do apply these
fitting functions is that we have
smoothing in lighting space not an image
space because it's done per pixel
independently but it's really equivalent
to having lit the object with fairly
large area light sources instead of
point lights so that's that's right so
you have this this low passing operation
because you're you're taking 50 data
points and you're reducing it to two to
five coefficients so the the the format
that is scribed for it before is just
one of the formats that we have and that
is a polynomial description of the
luminance and then fixed RGB values
we've also done this with polynomials
for each of the RGB components
separately and encoded representations
where we apply compression techniques
look-up tables or jpeg-ls to these
representations and they can get quite
compact okay so that's that's a PTM well
it turns out what the interesting part
of this really is though is that you can
extract all this information from PTMs
that's interesting
so if you have a surface at some
orientation if you have a digital camera
that's looking down at that surface and
you ask the question where does the
light source
need to go to maximize the brightness of
that pixel the answer is for a diffused
object perpendicular to the surface
orientation so if you find out if you
find the maximum of the surface
orientation which you can stall for
analytically you can come up with an
estimate pretty good estimate for the
surface normal in this case and again
you can you can drive that analytically
from the function itself so it's very
quickly so in the course of a half a
second we can compute all the surface
normals for for a PTM now let me show
you how we use that we basically have
two enhancement methods both of which
use that surface normal and this is this
is showing one of them so let me show it
off on this 4,000 year old cuneiform
tablet and it certainly helps to be able
to vary your lighting to be able to see
what's on the surface of that but it's
even more helpful to use those surface
normals for display so if I turn this on
you can now see this red cross here here
crosshair here that just corresponds to
the maximum of the PTM for any for any
pixel well if you use that to now
synthesize synthetic specular highlights
you've now changed the material
properties of the object so we've got a
slider here that controls the amount of
diffuse the specter the diffuse
coefficient of the of the object so I've
just turned that all the way down this
slider here control is the degree of
specular reflections off the surface
I'll leave that up high and then this
slider just controls the specular
exponent so in other words how how wide
or how narrow the specular highlights
are off this object and so when you do
this it turns out you can you can now
see a whole bunch of detail that was
difficult to see before so specifically
look at the two columns of text in the
middle here I go back to the the
original rendering it's you know it's
difficult to perceive those and while I
have this up if you would take a look at
some of the the grooves that occur here
on the top here those turn out to be the
fingerprints of the scribe 4000 years
ago that was holding this this tablet
when the tablet was still wet and that
was not a that was not known to be on
the surface of this tablet
time we photographed it so that's
specular enhancement that's that's
probably our most powerful technique for
bringing out more surface detail another
one that's very useful is something we
call diffuse gain and in this case
there's less of a physical analog to
what this is doing there's really no
there's really no physics and real
material objects that does this sort of
thing but if you look at if you probe
the reflectance functions of any object
that's diffuse they're very slowly
varying you know the the brightness of
an object doesn't change much as a
function of lighting direction well it
turns out we can we can leave the the
estimate of the surface normal the same
but increase the curvature the second
derivative of the reflectance function
and when you do that you can basically
increase contrast quite a bit in a
geometric sort of way so we have a
slider here that is tied to this the
amount of gain that we have on the
curvature on this on the second
derivative here and we can you know we
can see more or less detail on the
surface of these of this object these
inscriptions here then you can typically
see so this is this object is a 3000
year old object Egyptian artifact called
a new shop key and it was typically
wrapped up with mummies to help them in
the afterlife yeah thanks
okay so that's all I want to say about
the imaging technique at this point so I
am a member of HP Labs I'm also a member
of what's known as the Antikythera
mechanism research project and that
consists of basically four groups of
people first you've got largely British
astronomers you've got Greek big refers
people that study ancient writing and
then you've got a team of people from a
company called X Tech which is a micro
focused CT imaging company in the UK and
then you have a couple of us from HP
Labs that have done our surface imaging
work and I should say right off the bat
this group was pulled together by these
two gentlemen Tony Freeth and Mike
Edmonds here and a lot of lot of the
breakthroughs that you're really going
to hear about here were done by by Tony
Freeth and he also published the or
wrote the Scientific American article
that appeared in December if you want
more information on this refer you to
that so the story starts with a
shipwreck that occurred off the island
of Antikythera which is right next to
the island of kathira and it was a Roman
boat that was carrying loot pillage from
perhaps the Corinth area perhaps the
Rhodes area it's really not exactly
known where the Antikythera mechanism
originated from it's a matter of dispute
but in any case the ship did get close
to this island of Antikythera it hit a
storm and was sunk and this happened in
in the 1st century BC and the boat was
under water for roughly 2,000 years and
in 1900 another group of another boat
appeared on the scene a group of sponge
divers who were also hiding out from a
similar storm and when they went when
they awoke the next morning and drove
down to look for sponges in that area
they found what they thought were limbs
all over the surface of the of the ocean
floor and what these were were life-size
and larger-than-life sides marble and
bronze statues of from the ship from the
shipwreck and consequently the the Greek
government recovered as much of the the
treasure as they could from this wreck
and much of it now resides in the
National Archaeological Museum in Athens
there was statues there was glassware
that was found and then there was also a
lump of clay that was brought up in a
bucket and basically left in the
courtyard of the museum to to dry and
when it dried out it split open and
revealed gears bronze gears in the
inside of this this lump of clay and it
was at that point they knew they had
something pretty interesting here this
is a micro focus CT study of the largest
fragment in the mechanism fragment a and
you can see that the numerous gears here
there are 30 30 known gears in the
mechanism there probably were more than
that but in this one fragment alone
there there are 27 years that you can
see with with CT so a very complex
device at the time it was known that the
the Greeks used gears but very very
primitive Li for mechanical things like
turning a water wheel the fact that they
did calculations like this asked what
turned out to be astronomical
calculations was definitely not
understood at the time in this discovery
so just to give you a feel for the size
of the scale of these of the artifact
these are the three fragments that are
appear in the National Archaeological
Museum in Athens but there are not just
three or four they're actually 82 of
these fragments and so in 2005 we went
over to Greece and spent a week applying
our imaging technique to the front back
sides of all the artifacts we took four
and a half thousand photographs during
that week and we produced 82 separate
PTMs of all the surfaces and these are
all publicly available now on
an HP Labs website that anyone can get
access to anyone has access to is public
and has been downloaded often by by
people studying the Antikythera
mechanism so it was appreciated in by
German scientists in 1905 that this was
a some sort of an astronomical
calculator that was found but real
breakthroughs were not made until
darkness Ola price came on the scene and
he actually wrote Scientific American
article in the year I was born in 1959
on on the mechanism that's that's also
very informative and one of the most
important things price did was he
involved a Greek radiologist physicist
who took x-rays of the mechanism and
clearly you can see you know the
complexity of the gear assemblies in it
from the x-rays what we can also see is
that one of the the gear teeth had gear
wheels had 127 teeth on it which is kind
of an odd number to put on a gear and so
price you know correctly determined that
this was has something to do with the
metonic cycle which has twice that
number of months sidereal months in the
unit cycle and that this was in fact
some sort of astronomical computer and
price hypothesized a model of the of the
mechanism which was was right overall it
was it was close to correct but has MIT
has details that are incorrect on pretty
much all the gear trains that appear in
it so there as I mentioned there are two
groups that were sent to you apply
imaging techniques to the mechanism the
first was this company X Tech that does
Micro Focus CT work cat scan work they
use energy levels that are much higher
than what you're capable of using for
medical applications and so they can get
very precise fine images out from this
technique unfortunately the the museum
did not allow us to remove any of the
artifacts from the from the museum
including the exterior mechanisms so we
had to bring all the equipment
into the into Athens into the National
Archaeological Museum which wasn't too
bad for us but for these guys it was it
was tricky this is a 12-ton CT machine
and they literally shut down the streets
of athens for a few hours to bring this
into the museum it was brought in on on
truck into into Athens and so let me
just show you one more data set the
sequence you saw earlier was also from
their CT data this is just a cutaway
cutting through the mechanism just take
a look at the complexity of the of the
gears that you see here and and by the
way none of the gears are intact it's
not just a simple matter of counting the
gear teeth establishing how many how
many teeth each gear head is a tricky
process on its own okay and we were the
other group that got sent down there
this is the PTM assembly that we took
down their imaging fragment see so just
a Nikon d70 camera and and 50 light
sources that were programmable so as I
mentioned already we put all our data on
the web at the full resolution let me
show you some of the things that we
captured here and I guess before I pull
it up I should say that that this one
fragment actually tells you enough to
date the mechanism roughly itself but in
for dating it was also helpful that they
found coins on the surface on the ship
itself that date back from 86 to 60 BC
they real to radiocarbon date the the
Timbers the mass of the ship itself
that's from so the ship was from about
200 BC but let me show you this fragment
in a little bit of detail here
so again helpful at the very lighting
direction to study it but when you turn
on the specular enhancement technique
it's it's really quite easy to see the
writing on the surface of it so the
first thing that jumps out on you at you
is that most of the characters are run
together well it turns out that the
ancient Greeks didn't use spaces except
the demarcate numbers so it's pretty
clear to see that this is a number this
is a number they turn have to be both
both the astronomical numbers
you can also date it from the writing
itself so if you'll notice that this
Sigma here is the top and bottom bars
you're somewhat splayed out that's
that's indicative of stuff of the 2nd
century BC writing style as is the fact
that on this PI down here this leg of
the PI is slightly shorter than this leg
of the PI that's again indicative of 2nd
century BC writing so this one little
fragment that's about an inch across was
uh is quite helpful in dating the device
here's just a block of some of the
numbers that occur these are all
astronomical cycles that occur that the
mechanism describes and I've got a slide
to define all these in a minute but
basically there are 76 years and
something known as the khalipa cycle
there's 19 years in the metonic cycle
and 223 months said year old months in a
sarah cycle ok so certainly one of the
things that we did was allow a new
modality for people to see more detail
on the surface of the object but
actually another contribution was just
providing high resolution photographs of
the thing to scholars this turns out to
be the best photograph that was publicly
available before our work to just two
scholars that were studying the
mechanism of that same fragment so it
was just good to update it all this is
one more example of a fragment of the
mechanism let me zoom into a little part
of that and you can certainly see moving
the light source around that you've got
some some detail on the surface of that
but again turn on one of the enhancement
techniques and that detail becomes
pretty obvious that you've got some
ancient writing all over the surface of
this thing much of this has been decoded
now for those of you that can read
ancient Greeks this is actually upside
down so I apologize for that
but we basically were able to go from
being able to read well in combination
with the CT work we were able to go from
reading about 800 characters to being
only read about 3,000 characters and
turns out that jump really does does
contribute a lot to the understanding of
what this thing was and how it worked so
this is what the mechanism probably
looked like front and backside of it it
had a large dial and pointer on the
front that indicated basically the
zodiac it's a calendar dial there's both
an Egyptian and a Greek calendar on the
surface of that and on the backside is
what's known as the metonic dial and the
serous dial these these are both first
of all it's it's the first known
scientific instrument in history these
are the first dials graduated dials that
occur in any in any artifact and
basically what these two in the back
demonstrate is this is showing you the
metonic cycle which has to do with the
correspondence between the Earth's the
moon's rotation and the Earth's rotation
around the Sun and this is showing
what's known as the Seraph cycle which
is useful for predicting eclipses both
solar and lunar eclipses hundreds of
years out the the data that that this
seems to be based on is probably about
500 years worth of astronomical
observations that the Babylonians
actually made before that before the
Greeks so how do we know that it's
hand-cranked we don't strictly know that
it's hand-cranked it could have been
driven by something like a waterwheel
but given that it's a quite small
compact object it probably had a little
crank on the side of it this is just a
you know an axle that connects to the
main drive wheel then it has an obvious
little slot that looks like sure looks
like you would stick a handle into that
and crank it so that's the that's the
evidence for that this is an animation I
want to show you the Tony Freeth
produced it is of the complete mechanism
is it's understood now all well 29 of
the gears are involved in this animation
and you can see that there's a
remarkable complexity of this thing one
of the things I want to point out real
quickly is this little black what looks
black right now is actually a little
sphere that's both black and white and
as it rotates around it's telling you
it's showing you the phase of the Moon
so it's all white it's a full moon when
it's black it's a new moon so this is
this is the mechanism as it operated
without the without the box around the
side of it you know it's a cutaway and
this is how Tony and our group in
general believes how the the mechanism
worked and how that the gears went
together I'm not going to go into any
detail on this at all but you can see
the you know the hand crank your input
drives both the front dials which are
the calendar dial again and the back
which we're both you know the lunisolar
calendar and the eclipse prediction the
Saros cycle okay so let me define some
of these terms I've been using so what
you think of as a full moon - a full
moon cycle takes twenty nine and a half
days
that's called a stenotic month now if
you look at that from the reference
system of the Sun instead of the earth
you know obviously the earth is rotating
around the Sun is this is always
happening so the the in the reference
system of the Sun for the moon to get to
the same position that's called a
sidereal month it's so much shorter
twenty seven and a half days the metonic
cycle well we know that that the orbit
of the moon is not phase-lock with the
orbit the Sun I'm sorry the orbit of the
earth around the Sun but they do
coincide roughly every 19 years ago
every 235 synodic months and that's
called the metonic cycle and that is
displayed by the mechanism if you add
one day to every fourth year of that
you get a cycle now that's 76 years long
that's called the clip exciti that's
also shown likely was shown on the
surface of the of the mechanism the
sarah cycle is interesting so it was
known that eclipses can reoccur every
223 synodic months and that that cycle
is called the sarah cycle and is
definitely one of the more complex
things that the mechanism shows again if
you now that that repeat of eclipses
turns out to be eight hours off and so
you have these eclipses occurring at
different parts of the planet with the
Sarah cycle so if you repeat this cycle
three times you get what's known as the
eggs oligomers cycle which is more
accurate and in that it's it's possible
that an eclipse can happen that happened
in one place on the earth will occur at
a similar spot on the earth 54 years
later and then the last thing that the
that the Antikythera mechanism shows is
what's called the first lunar or anomaly
and that is that the orbit of the of the
of the moon is not perfectly circular
it's it's somewhat elliptical and so the
moon speeds up and slows down that's
that's called the first lunar anomaly
and it believe it or not this mechanism
even even even predicts that or even
demonstrates that that anomaly okay so
let me give you three examples of how
the imaging was used to cover some of
the details of the mechanism here so if
you look at the largest fragment
fragment a and the back side of it on
the right side there you'll see very
faint markings and if I highlight those
you can see where they are that they're
spaced uniformly what you can also see
is that every fifth or sixth spacing
apart either one five or six spacings
apart there are there there's writing
that you can you can see
these little glyphs and if you apply the
PTM techniques to that you can certainly
pull out some of that writing and quite
a bit more visible now this is showing a
spiral where all the known locations of
these markings occur and if you
extrapolate and basically put them back
into the the four cycles of the of the
dial and both metonic and the Sarris
dialogues were spiral they were not
circular you wind up with 223 months as
again we know is the number of months in
a sarah cycle and if you now look at the
location of the known glyphs that are on
that they're located here and we can
certainly pull all those up into one
view so these are the sixteen known
glyphs that are that are visible you
know some of these that are a little
easier to write up here on the surface
and are pulled out by PTMs
some of the other ones that are
problematic or you know you can only see
with with CT renderings so they're not
quite as quite as crisp but you can
highlight some of the text that occurs
on this and what you'll see repeating
over and over are h's which turned out
to stand for helios ancient greek for
sun and these Sigma's are shorthand for
Cellini our Selene which is a word for
the moon and so this is referring to
both lunar and solar eclipses and the
rest of the writing below actually tells
you the hour and the day that these
eclipses could possibly occur at so if
you now go back and you place the the
known glyphs you can make predictions
about where the remaining glyphs must
have occurred on the surface of that
dial
okay the second thing I want to point
out was something known as the the
pointer follower and as I mentioned
already the dials in the in the back are
spiral in shape and so it's not clear
from just a pointer itself which of the
four or five
arms of the spiral you should be reading
off when you use this mechanism well it
turns out we found a you know here's
here's an arm of a pointer and here's a
little nub that sticks down at the CT
renderings and this is a rendering that
Tony produced of you know what it
probably looked like new and that is
this thing is a pointer follower so this
sleeve is free to slide on the surface
of this pointer and this pointer
follower rides in little grooves that
are in the underside of the mechanism
and let me just show you an example of
that a rendering of that again the Tony
Freeth produced you know as these as
these dials rotate around the pointer
follower slides out and tells you which
of the particular four or five rings to
read out at any point in time and
presumably when it gets to the end of
its it's a it's travel you have to
manually reset it back to the original
but we really don't don't know if that's
true or not okay so the last thing I
want to point out I find I find just
remarkable and let me show let me show
that to you on CT or on PTM renderings
again we'll look at the backside of
fragment a and this time let's zoom into
the area around these two sets of gears
here and let me turn on the enhancement
techniques again so you can see the
surface a little bit better so I want
you to keep your eye on this little
notch down here that's taken out of the
the the gear wheel and you know as you
can see it's a little cut out and at
first it was thought that this cut out
had something to do with the fact that
maybe the Antikythera mechanism was
repaired but it was noticed that that
this that in the cutout there's actually
a slight pin there's a tiny little
circular area which you can see better
in the CT renderings so let me pull that
up right here here's the same little
notch and here's the pin that
we're talking about so it turns out what
this was was a little pin slot
arrangement that tied two gears together
that were sitting on top of each other
and this is an epicyclic arrangement
meaning that first of all that the axles
of these two gears were slightly offset
and they were mounted on another gear
that was free to rotate and basically
what this did without going into exact
detail on the mechanism was it predicted
this first lunar anomaly it actually
caused the year that's responsible for
keeping track of the moon's precession
to slow down and speed up by exactly the
amount that was known at the time or
estimated at the time of that you know
epicyclic behavior so this is I mean to
me this is this is one of the more
amazing parts of this mechanism these
guys you know it in around the mechanism
was probably built around 150 BC you
know at a time when North Americans were
living in teepees these guys were
building mechanical calculators using
bronze gears you know hand-cut of this
complexity that modeled the slowdown and
the speed up of the the moon as it went
around its orbit it's just it's just
amazing and there is nothing there's
nothing of comparable mechanical
complexity in any civilization until you
get to about fifteen or sixteen hundred
when Europeans start building clocks so
this thing you know was 1500 years ahead
of its time it clearly was so complex
that there was no way this could have
been the first device of its kind that
was built it's just there are too many
mechanical subtleties to that to allow
that but it's the only one of its kind
that's ever been recovered which is a
bit surprising probably it's explained
though by the fact that bronze
throughout the years has been extremely
valuable metal and things that were
bronze got melted down to form
cannonballs and you know in cannons
typically and the only reason the
Antikythera mechanism survived was that
it was underwater for 2,000 years
so so let me show you an animation of
that last
what was called Hipparchus mechanism for
some time so here are the two pairs of
gear teeth on the back and you can see
the little pin slot arrangement with
ties the the top gear onto the the
bottom gear again the animation was
produced by Tony Freeth okay so that's
really all I want to say about the the
mechanism itself I want to go a little
bit into some of the imaging work that's
been done based on our technique since
then and this first slide is just no
review of some of the devices that have
been built in the meantime this is not
most of these were built not by art
group a couple of them were built by a
nonprofit called cultural heritage
imaging and the basic idea is that you
know you just want to get a collection
of photographs from different lighting
directions of the same thing under under
you know under novel novel than lighting
directions so let me just show you a
couple of those devices this is again
was built by Mark Mudge cultural
heritage imaging very simple device
she's just got a digital camera on the
top of the tripod here he's got a stand
that's holding a light source and he's
just moving that light source to
different locations those locations are
marked on a template a piece of paper
here that's got you know markings of how
high he should have the light and
exactly where it should be so he's just
manually going through collective you
dozen images of the same thing this is
also a very low-cost way to approach it
this is a PTM assembly that was put
together for a couple hundred dollars by
Walter Burris in the Netherlands this is
just a low end digital camera with an
extension on the flash and then he
bought a $3 styrofoam dome and drilled
holes in the right places and and just
moves the the flash assembly itself into
the holes one at a time and it takes a
number of pictures so very simple and
then on the high end you have devices
like this that again was built by
cultural heritage imaging that
that have very carefully controlled
color controlled light sources that are
brought in with optic fibers onto the
surface of this with these kind of
devices you can make assurances of it
you're not exposing any of the museum's
valuable artifacts to in excessive
amounts of radiation you can carefully
quantify exactly how much radiation
you're applying you know what the
frequency distribution is and so on so
this has been picked up by a couple
dozen groups around the world and the
reason is that all of our tools are
pretty easy to use they're on the web
anybody can download them there's no
licensing or anything like that so
basically what you do is you create a
stack of images and then you create with
a text editor what's called an LP file
which just tells you how many images you
have in the data set where each image is
and what the light source and a vector
to the normalized light source direction
so what direction the lighting came from
that's it
and then you feed that into what we call
the PTM fitter and it spits out a PTM
for you and I should I should just make
mention this this figure right here
shows you 50 of the original samples of
luminance and then the function that was
eventually fit to that and you can see
that will low-pass again in this in this
lighting space typically preserving
details quite well even in light space
but anyway the fitter is available also
for download and so you can just produce
your own PTMs
so lots of people have done lots of
things with it we had an interesting
case I just want to mention with the FBI
on an actual serial murder investigation
the FBI will prefer I not show the
original data publicly so this is kind
of a made-up version but basically what
we had was a serial murderer was keeping
very detailed notes on what he was doing
in a spiral bound notebook he
unfortunately administer rip those out
before they arrested him and what they
had with the FBI had were very faint
indentations on blank pieces of paper
underneath those
those papers and we were able to bring
out the indented writing and this just
shows another example of indented
writing that I made up turns out if you
use this diffuse gain technique you can
really increase the contrast this stuff
and eventually start seeing that you've
got some indentations here and even I
mean even with our method you have two
by the way this is the quick brown fox
jumped over the lazy dog
even with our with our method you need
to interactively do this your visual
system turns out to pull out much more
information under motion and looking at
different lighting directions so for
instance you know the the the J ceiling
line this up accurately enough here the
the J here and the letter jumped is best
seen with with lighting that's
perpendicular to it if you if you put
the lighting overhead you really can't
see it that well if you put it off to
the side it's a little finicky see if I
can dial it in here put it off to the
side you can really see that the
perpendicular brushstrokes oh baby so
basically you want to put the light
source perpendicular to the stroke so
you're trying to recover okay so it's
been used in other criminal
investigations this is something that
the California Department of Justice put
together it's a it's a rig that has a an
arm that's free to to rotate and a
digital camera on top and it's very
useful for capturing footprint PTM so
let me show you another feature of the
PTM viewer that I haven't shown off yet
we can actually extrapolate the space
beyond the lighting directions that we
have or that are physically acquirable
in the first place I mean it's basically
like taking the light source and the
object you're moving it below the light
source in some ways so we can place one
light source there we can certainly take
another one place it off to some other
grazing direction we can take a third
light source and and put it somewhere
else and so you know eventually you can
build up wearing these that are that are
pretty indicative of the 3d shape and
what's interesting here is that you know
once you collect a PTM like this you can
now perform this analysis much later or
as new techniques get developed you can
try applying them to the
to the device they've also been used by
the National Gallery in London this is
an arm that we made very simple out of
literally just bought twelve very twelve
flash units low low cost flash units
from a camera store mounted them on an
arm and you know they're manually
manually activated let me show you this
PTM of France hauls painting taken from
the National Gallery in London so if we
move the light source you know off to
the side you can see the vertical
brushstrokes quite well if you move it
to the top you can see the horizontal
brushstrokes and certainly in all cases
you can see the dust on the surface of
the painting quite well okay so a few
years ago this begged the question to me
that okay if you have control
interactive control of lighting like
this is there anything analytic that you
can say about the quality of the images
that come out as a function of different
lighting well can you characterize in
the fact you know how complex the images
are as a function of lighting direction
well there's certainly a well-known
measure of information content and
images an image entropy that does
correlate as you can see quite well with
with how much you can read in lighting
direction that's not analytically
drivable from the PTM equation but
fortunately variants which is for
Gaussian random variables is
monotonically related to entropy is it's
possible to compute that analytically
from a PTM representation basically
meaning that once you have a PTM in you
know half second you can now produce
maps of variance or entropy that tell
you where are good places in the
lighting space to look at this object so
where you going - where you gonna see
more detail where you can see less and
not surprising every every surface has a
different representation of image M
should be indifferent
you know region where where it's better
than to look for detail often it's in
the grazing grazing directions here
okay so this is just a quick slide to
show that that PPM's can be collected
under any frequency of light this is
using infrared lighting some experiments
that the National Gallery did so far we
haven't found a great use for IR PTMs
but it's possible to collect them this
again was produced by Voltaire Heston in
the Netherlands who has an interest in
microscopy and you can see again both
dark field effects and light field
effects off of a single PTM this is a
the wing of a dragonfly at about 200 X
magnification so this could be applied
at very small scales you can certainly
apply it at very high scales as well
this is a PTM that I put together of a
significant part of Arizona working with
digital elevation data from geologist
named John Saul John believes that that
a period in the Earth's time about three
billion years ago called the late heavy
bombardment where the earth was was had
catastrophic large meteors hit the
surface of it formed very large impact
craters that you can still see evidence
for today and basically plate tectonics
makes the prediction that you shouldn't
really be able to see this stuff but you
know sure enough you can see circular
structures like this one here that may
be evidence for late heavy bombardment
timeframe but this is what you're
looking at a significant chunk of
Arizona here so again PPM's can be done
at all kinds of spacial scales I want to
show you that we've now developed an
even easier way to collect these PTMs
that requires nothing but a digital
camera a handheld flash and a black
snicker ball which you can buy for five
bucks so what you do is you put the
black snooker ball into your scene next
to the artifact that you want to capture
and you take pictures moving your light
source around the different locations
and turns out you can recover the
direction of the light source from the
reflection in the black slicker ball
makes a lot of sense well it turns out
what's nice about this is you can do
this fully automatically you can there's
been some software written by a group
from the University of Minho in Portugal
that you can just feed it a directory
worth event of images it automatically
finds the black ball it automatically
finds where the highlights are on the
black ball it automatically extracts the
light source direction from those
highlights and it runs the PTM fitter to
produce the PTM out from your stack of
images all hopefully without manual
intervention now if they're you know if
it fails and it doesn't find the black
ball or you know automatically you can
go in and help it but it's it's
automated the whole process it made it
made it pretty simple to collect this
kind of data this is also downloadable
from our website at HP labs and let me
just show you some renderings that were
collected from data collected in that
fashion this is some stone carvings that
date from about 20 to 30 thousand years
ago of an antelope here here's the head
of the antelope and the feet and the
bodies right here so it's certainly
helpful to to very lighting to see that
sort of detail okay and we've also done
this in real time so one of the
complaints we were getting from
forensics and criminal investigators was
that you know this is a lot of hardware
to take out into the field it'd be nice
if they could just take things back to
their lab and quickly collect images
based on that and so what we did is we
built a real time assembly that consists
of a high speed 500 frames per second
video camera here and some arms that
capture the the lighting information or
that that that provide lighting in
practice we use eight light sources and
in a sixtieth of a second we can collect
all eight images at different lighting
feed all that data down to a GPU compute
surface normals compute reflectance
transformations and render the thing
which means that they see a video rates
you can produce renderings of things
that you hold in front of this assembly
and start seeing more detailed and then
you typically can
so two o'clock is the end for this right
about ten minutes is there a mic yeah
I'll save some time yeah so you can you
can produce renderings in real time with
this technique so this is just a
demonstration of some of those results
here's an original view of the surface
of a basketball here is low frequency
content being brought out from the
basketball and here's higher frequency
content being being displayed than from
the basketball so it turns out you can
run these normal transformations in such
ways that they have frequency effects
spatial frequency effects too so you can
bring out different spatial frequency
components so so far everything I've
shown with PTMs has been covering
variable lighting well it turns out
they're useful for other things besides
this so this is just a simple example
where I took six pictures of a boring
office seen at different focus settings
and integrated them into a PTM as you
can see we've got continuous interactive
control of focus direction once we've
done that so this to me it kind of begs
the question you know why would you ever
want to fix the focus conditions what at
the time you take the picture as opposed
to collecting data like this and
allowing you to play with it after the
fact and the last topics all I'll
mention here is you know everything I've
shown you in terms of lighting with PTMs
is from a specific viewpoint one fixed
viewpoint you'd obviously like to be
able to for Museum applications capture
an object from various viewpoints allow
the user to rotate it around and change
lighting in real time so we've taken
some steps towards being able to do that
specifically we've generated these PTM
object movies which are quite simple you
know conceptually they're just PTMs
taking at different orientations of the
object and you know in this case we
still have interactive control over
lighting real time plus some view
dependence obviously you wouldn't like
this not just to occur static view
locations you'd like to have continuous
control over
rotations so we've been working with
researchers at UC Santa Cruz to do just
that to collect and it turns out one of
the biggest problems here is the amount
of data that you have to collect is your
very view direction and lighting
directions it's just enormous and so
we've done work in terms of you know how
you manage that trade-off where you take
more lighting directions where you think
more of you directions good and that's
really all I had I want to point out
that this is the URL for the the website
that we have for where all the tools are
these are you know it's very easy for
kind of hands-on people like like you
all are to to go ahead and experiment
with so feel free to play around with
these tools and I just want to thank Dan
Gelb's visibly for developing this
technique and the others that were
involved in the research in the
Antikythera mechanism thank you that's
really cool stuff Tom my favorite was
the PTM at the global scale at Arizona
that was pretty cool so I want to point
out two things definitely go check to
his website just Google Tom and you'll
find the PTM viewer I'm sure but
interactively you can change the
lighting on some sample to be TMZ
already has and it's a pretty cool
technique and then if you're more
interested in the attack there a
mechanism
check out the December issue of
Scientific American and so we have time
for a few questions and please use the
microphone
wouldn't make any difference if you were
to use coherent light not not that I've
been able to figure out no I mean use
polarized light use be coherent light I
don't I don't really know how to
introduce that as a you know increase
the functionality of a PTM with that now
I mean we've certainly varying frequency
light sources but I don't know if not
sure if Carnes buys you anything with
this technique have you or anyone else
yeah creates beacons choosing the Sun as
a light source yeah I mean I've looked
at that I've taken objects and
photographed them automatically through
various points of the day and you can
certainly make a nice one-dimensional
PTM out of that and I could probably
pull up an example if you give me enough
time of that but since the Sun travels
in a one-dimensional path you don't have
the full two dimensions of that you need
to fully capture a PTM so you can't make
a 2-dimensional PTM out of one but yeah
we have done one-dimensional PT ends
they look good kind of related to that -
is an obvious thing to do is to take is
to make a PTM of the moon because mostly
the moon is facing the same direction at
the various phases so it'd be very
simple just photograph the moon every
night and combine that into a PTM now
you've got control over lighting on the
surface of the Moon unfortunately the
problem with that is the moon actually
rocks back and forth I think it's like
five degrees or so enough so that the
you know the images would not be
registered well enough in 3d to make
that happen
we would need to be compensated for
which is all doable but has not been
done yet what about the the color the
the number of bits of color depth you
have is there anything you know is 8
bits per you know RGB good enough for
this technique or do you need to do
anything special in terms of the
type of camera that you have and the
depth so this technique really works
well for diffuse objects and and then
adding specular highlights in for that
and for that class of images and
datasets the few shading turns out not
to have enough dynamic range where you
need to really worry about high dynamic
range if you're trying to capture true
synthetic specular highlights and we've
done work on that you know as well you
know since this since the paper it's the
original 2001 paper yeah you need high
dynamic range for doing that certainly
specular highlights are so bright
typically that they blow out the 8 bits
of dynamic range you can't get good data
in both below you know the low floor and
the high water so yeah but one of the
things we've also looked at how much
resolution you need for displaying the
coefficients of the PTM itself and turns
out you know we've got a trick in here
we have a global scale and bias scaling
bias values that are in the PTM file
that allow basically to store all the
PTM coefficients at a bits resolution
that's adequate so there are definitely
resolution tricks you can play have
there been any applications in movie
making in moviemaking there's a there's
a researcher USC nepal Debbie Beck who
has done some very impressive work
throughout the years applying relighting
sorts of techniques to movies so he's
certainly done quite a bit of that and
very successfully and works closely with
Hollywood doing this oh yes absolutely
one of the more interesting things that
Paul's built is a is a large dome with
colored lights on the surface of it so
they can actually simulate the lighting
on an actor from a completely synthetic
environment so they can go ahead and and
capture the lighting environment of a
synthetic environment or a sari of a
real environment and then produces
synthetically to match the lighting that
you're producing on an actor to that of
the of the some other environment
not a question I guess an observation it
seems to remind me a lot of how the
panopticon like field cameras work with
yeah each pixel being sort of a bundle
of incident rays yeah absolutely so you
know we we noticed that parallel as well
we you know even before we develop this
technique you know if you look at you
try to apply PTM techniques or the
polynomial itself to values are taken as
you vary camera location as opposed to
varying lighting it turns out there's
there's just not as much coherence to
the those pixel values and so a really a
low-water representation like this just
doesn't work for a representing light
fields whereas it works beautifully for
this and you know one of the reasons it
works for this is first of all there's a
lot of redundancy in the in the data but
also the human visual system really can
tolerate quite a bit of sloppiness you
know in this so when you make one of
these PTMs if you have errors of you
know even up to like five percent of
your estimation of light source
direction you won't see any artifact in
the in the original or in the in the PTM
that's produced and the reason for that
is quite simply yeah you might have to
move the light source the synthetic
light source to a slightly different
location it might be five degrees off
now in the rendering but in the end
you'll get a very you know a very
similar result so it's very robust and
like the light fields it would not be so
robust
we're all your photographs done in white
light have you ever done like the red
green or the blue separately and then
combine them and I mean we don't
typically do it in white and then we can
separate again the PTM representation
you need to the color channels
independently and then you know we
certainly can have control over color in
that way so yeah it can be done under
under really any any color light source
but typically white we we do for
generality great well then thanks very
much for your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>