<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Distributed Camera: Modeling the World from Online Photos | Coder Coacher - Coaching Coders</title><meta content="The Distributed Camera: Modeling the World from Online Photos - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Distributed Camera: Modeling the World from Online Photos</b></h2><h5 class="post__date">2012-08-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/opRZUMrLJOM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I'm going to introduce our
speaker it's interesting
I sort of vaguely knew of his work over
the course of a number of years and and
then while ago Michael asked pointed
pointed out the building room in a day
paper which I looked up my bread this
was amazed by and and then back in April
I was invited to go up to to Cornell and
give a lecture about the Google Art
Project which I did and I mentioned the
room building Roman today stuff which I
was very impressed with it
it was teetering in the audience and no
it was right there because of course he
was there and I met him then and I said
oh you've got to come and get it block
you it's the stuff that you do this is
really quite amazing he's an assistant
professor at Cornell in the computer
science department working on computer
vision and computer graphics he did his
undergraduate at the University of
Arizona in computer science and math got
his PhD from UW for those of you who
don't know that that's diversity of
Washington a trick question for you
which I won't ask him now is has he ever
seen Mount Ranier from Rainier View
campus because you know even though he
may have spent a while at Washington HP
you rarely ever see him out here from
their view and then the final trick
question is why are there Terry trees on
the campus at University of Washington
so no why are there
well when the emperor of japan gave a
large gift of cherry trees to the New
Republic the United States he sent them
to Washington and the ship captain
delivered them to the Washington he knew
which was state of Washington that's
great and the governor had them
installed on the campus so another set
is set to Washington DC English is just
talk of you giving cool anecdotes but uh
hey everybody thanks for the
introduction like yeah I'm Noah Snavely
from Cornell University and I'm gonna be
talking about what I've been calling the
distributed camera which is sort of that
camera that exists by virtue of millions
of people around the world having
cameras and taking pictures all the time
and uploading them to where all of us
can access it so it's sort of this
virtual camera that you have to use your
imagination but so if you have questions
please feel free to ask them I'll try
and repeat them if there's questions
during the talk but yeah this is
informal so just please feel free to
speak up so you know it's no surprise
we're living in a world where pixels are
very cheap and very available so there's
I'm sure trillions of photos available
online millions more uploaded every hour
these are interconnected how many of us
have access to many of these photos and
so it's really the case that the
Internet is becoming this unprecedented
visual record of our world and it keeps
growing so anytime something interesting
happens there's probably a photo of it
or hundreds of photos that uploaded to
some website Facebook Flickr Picasa
whatnot so you know you can see facts
attesting to this so this is the SEC
filing that Facebook did recently I
don't know if anyone here is Facebook
stock but you know before they had to do
that they had to pass the Securities and
Exchange Commission one thing they put
in there is that over 300 million photos
are uploaded per day on average to
Facebook and the three months ended in
March 31st 2012 so that's a lot but I
think my guess is that the biggest
source of pixels out there is YouTube so
this is a year ago we're evidently 38
hours of video were uploaded to YouTube
every minute so 48 hours to two days
that's amazing I don't know what that is
but you know maybe some people here can
enlighten me but it's a lot of pixel
data it's amazing it's just
mind-boggling and you know recently you
know the year later that was now three
days of video every minute and they're
very proud of that but the
it's just this massive influx of image
data
all right documenting many many things
cities people events it's an amazing
source of data so I'm very interested in
sort of finding ways to make use of this
data to better understand our world this
is very very unorganized data okay so
you know there's many things you can
imagine doing with this some sort of
sneaky and some maybe more positive
things so you know the city of London
has tens of thousands of closed-circuit
TV cameras all over the city taking
pictures of things and you can do things
like if there's a riot as there was in
London few years ago find pictures of
suspects and put them on the internet
for people to look at so that's one sort
of outcome of having images everywhere
in this case it's cameras that were
intentionally put there by the
government but you can imagine that you
can also do this with just random
pictures that people take all the time
so you know me this could we have been
to New Jersey but years ago there was
this news article which you know a thief
was caught because they were stealing a
camera and they were caught in another
camera that was taking a picture of the
scene at the same time when this crime
was happening and so they were able to
you know connect the dots I'm at you
know someone at CSI was probably looking
at all these photos and they happen to
find this one and you know figured out
who was doing this so you know that is
sort of a coincidence that this happened
but you can imagine as more pictures are
taken there's sort of this surveillance
camera that's just out there in the
crowd all you know documenting
everything that's taking place or many
things that are taking place so okay so
you know you can use this potentially
for sort of forensic applications this
distributed camera all right you can use
it to sort of figure out what people are
doing so this is work that was done by
some of my colleagues at Cornell of
years ago including Dan hunt Walker
who's now running the New York City Tech
campus building that but what they were
doing they downloaded millions of geotag
photos from Flickr and just try to
figure out what they could tell about
people from these images and the mostly
you know one of the most basic things is
where people tend to take photos so what
they did is they clustered the images
into
the top 15 our top 20 city scale
clusters and then with each within each
of those city scale clusters they
clustered into landmark scale clusters
and figured out sort of what the most
visited what the most photographed
cities on Flickr are and with within
each city what the most photographed
landmarks are so here's I think the top
20 most photographed cities in the in
North America and within each it shows a
picture of what the most popular
landmark is so you know New York City
whereas Manhattan the most popular
landmark is the Empire State Building
I think that's picture from the Empire
State Building that of the Empire State
Building
so I guess and you know Fenway Park is
the most popular landmark according to
Flickr users in Boston and the city of
Disneyworld
the most popular landmark is the castle
okay so these tags also come from
people's peoples tagged Flickr tags so
this you know it's maybe not completely
accurate you know completely
representative sample of people but this
gives you a sort of insight in how
people and where people take photos I
think the third or fourth most popular
landmark in New York City is the Apple
Store where's where's the Apple store
the the iconic cube one okay yeah that
one and so you know photographs have
always been also a tool for science
so especially measuring how things
change you can do that in a very
compelling way with photographs so
there's two photographs taken many years
apart of a glacier and you can tell you
know something's changed between them
and that communicates a lot of
information so that's what you can do
with two photographs but imagine what
you can do with millions and billions of
photographs taken all the time all over
the world so it here's just a few sample
photos of the city of London a few of
many many millions on Flickr and you can
imagine what could you do if you had
access to all the photos ever taken of
the city of London what kinds of things
could you figure out for those images
well you could possibly create the most
compelling visualization of the city you
know show it from any viewpoint from any
time of day any season you could you
could potentially figure out how the
city
is changing over time you know what
buildings are going up how the buildings
are changing how people move to the city
differently use the city differently
maybe all that information could be
gleaned from these photos perhaps you
could also monitor the environment or do
sort of science with these photos so
tell well answer questions like Oh is
spring getting sooner every year can you
tell that from these photos and in
various parts of the city our flower is
blooming earlier in the year that sort
of thing and as I mentioned print
forensics could you answer other
questions like who stole this thing from
this part of the city at this time these
are all potentially things you could
imagine answering with all these
photographs that are being generated all
the time so this is not you know there's
other domains in which this sort of
thing is taking place there's this
notion of sort of citizen science where
you actively as a citizen get involved
in scientific sort of process so look at
Cornell we have the college born
ethology and you can go out and see
birds and record where you where and
when you saw them and then enter them
into a database and therefore be helping
with tracking birds nasa's a bunch of
these one of the most famous is galaxy
zoo where you can go on and sort of you
know it's sort of a game there you go
and you learn to label galaxies and they
have hundreds of millions of galaxies
that they need to label and so you can
help with that process so more closer to
what i'm going to be talking about
computer vision how can we use computer
vision to solve this problem or get
useful data from these images this has
been going on in astronomy for a while
now so this is really cool project
called astrometry net that was done
partly it with some of the astronomers
at nyu and also Sam rice where they
could take any photo of the night sky
you know through telescope you know
taken by any amateur astronomer and
figure out the exact camera parameters
of that image and figure out exactly
what you're looking at so for instance
there's an astronomy tree net Flickr
group where if you post a photos of that
group the robot will get that photo suck
it up figure out the parameters you know
what field of view
nice guy this is and then label it and
you know circle things of interest in
that image like what galaxy or gobby-o
globular cluster you're looking at and
it does it completely automatically just
by using computer vision to tell what
stars you must be looking at it based on
their configuration doesn't need any
metadata at all and so you know you can
start doing some interesting things you
could sort of take photos that were
never intended for science perhaps and
use them to do interesting things so
some astronomers showed that you could
then use sort of crowd-sourced
just astronomy amateur astronomy photos
posted on Flickr to do things like track
or calculate the path of a comet okay so
that you know it's not quite maybe Nobel
prize-winning science yet but it's sort
of a start and you can sort of imagine
the kinds of things you might be able to
do if you could tap into this huge set
of photos that are being generated so
the question is can you do the same
thing now with sort of astronomical
photos which are sort of constrained
since the night sky is sort of 2d and do
it with photos taken anywhere in the
world
okay that's sort of one question I'm
very interested in so you know there's a
bunch of applications here's another one
I was talking to some of the facilities
folks at Cornell and I Cornell we have
these old buildings you know built to
the 19th century which are moving
apparently by a few millimeters every
year just sort of the walls are starting
to buckle or what that and they're
interested in sort of monitoring that
how much is the building changing all
the time and so you can imagine trying
to do that sort of measurements if you
could do very accurate measurements from
photos that are just being uploaded over
time you could avoid the expense of what
they're doing right now using laser
scanners every so often to do very
precise measurements of these of these
buildings okay so it wouldn't be cool if
we could just use all the photos posted
on the Internet to do these kinds of
things and the big problem of course is
that there's no order to this data at
all it's completely it's just pixels is
van calibrated you don't necessarily
know where a photo is even taken or what
time it was taken and those are sort of
very important things if you want to
sort of use images to make measurements
or tell that something happened in the
world so
from a computer vision standpoint that's
the kind of problem I've been trying to
tackle over the past few years through
the lens of geometry and I'll talk more
about that as we go through the talk so
we'd like to be able to do is for any
photo on the web figure out exactly
where it was taken
figure out the exact camera parameters
so where in the world it was it was the
camera Center what direction was it
looking what was the focal length what
are the radial distortion parameters we
want to map array a pixel in that camera
to array in 3d space so we can say what
exactly would this Ray hit if we
actually cast it in in a world
coordinate system we also want to know
exactly what time we would has taken you
know hopefully with you know second
precision what is visible in the image
what objects are can we see so we can
compare them to other images of the same
object so what we want is something that
will tell us this photo was taken
exactly here looking in this direction
and the time is you know October 8th
2011 and that this is tree number if you
have some global index of trees this is
tree number with something yeah some big
number and you know you'd probably guess
where this is taken listed here and you
know this is that this these pixels
correspond to a particular building so
then you can start you can do queries
like show me all the images that show
this building from this time to that
time or show me the images of this
particular tree in Central Park
ok that's the kind of thing we'd like to
be able to support just with all the
images in the world
so the sort of guiding principle here is
that we're going to this this involves
geometry in a very in a very fundamental
way we want to know the geometry of this
photo very accurately so you know there
are ways of getting approximate
approximation so this information you
can use GPS and time stamps as many
photos have increasingly but you know in
my experience these are very noisy for a
number of reasons you know you it's not
unusual to find a photo that for
instance you know is taking northeast
you can work it out on a map but the GPS
there the compass information says it's
pointing due south which you know
not a useful measurement if you want to
use this if you want to get actual
measurements from this in this image so
you know I don't know timestamps are
also often wrong I don't know if any of
you have looked into this but the
measurement informations we get from GPS
and and it's not always GPS but
localization and and time stamps it's
hard to trust them and I'd love to be
able to but it's just not the case right
now and many images don't have these you
know historical photos paintings we'd
love to handle paintings too we need to
be able to do this from vision as well
so we want pixel accurate position and
heading we want to know like I was just
saying for every pixel in the image what
object does it belong to okay so we can
use this sort of existing metadata like
GPS as a rough guide but we need to be
able to do better okay so I'm going to
sort of sketch out a story of how we're
starting to do this using large-scale
computer vision the sorts of things we
did in Rome in a day that Mark was
mentioning but it'll even larger scale
we want to build 3d models of the entire
world that we can use for this task of
localizing new photos so I'll talk a bit
about this how we can build a global
database of geometry for this task how
we can get people to intentionally
contribute photos rather than just sort
of sneakily downloading them from Flickr
how we can calibrate new images given
this data and starting to get into some
applications using other geo reference
data sources like Google Earth 3d models
or or time-stamped weather data for this
for vision applications okay so I'm
gonna start out by talking about the
sort of chord geometry sort of scalable
geometric reconstruction from internet
photos so when you want to recognize the
location of an image there's a couple
ways you can do it one is you can
compare it to other images sort of 2d
images and see which ones it matches
Google search by image does a sort of
similar thing to that what we're doing
is slightly different where we're taking
images and matching them to 3d models
and that's what's going to sort of form
the core of this technique so
towards that and one of the things we're
doing is trying to take all the photos
in the world and sort of fitting them
together as 3d models and it'll become
sort of clear what that means as I go
through the next few slides we have
we've developed sorry go ahead
yeah question we want to derive so the
question is am I talking about
reconstructing 2d projections of 3d
models that's correct yeah exactly we
want to find the viewpoint of the camera
by finding how it should be situated in
the world such that a projection of that
3d model agrees with the image yes
that's correct
and to start out we're gonna do the
inverse of that actually we're gonna
take a collection of photos and then
recover the 3d geometry of that scene
from that photo collection so here's an
example with the set of photos of the
Coliseum that I got from Flickr we we've
developed algorithms that will take
those photos so turn the crank and then
output geometry okay so that geometry
tells you for each of the photos in that
input collection potes of the Colosseum
on flickr in this case where exactly was
that photo taken so that's what all
these little wireframe pyramids are
camera positions and orientations and
what's a rough
what's the rough geometry of the scene
that's this point cloud that sort of
looks like the Colosseum but a very
pointillist version of the Colosseum
that's what we reconstruct okay we can
also densify this this is sort of a
dense model of the Colosseum produced
from a set of internet photos and again
you can see the reconstructed viewpoints
down here we're not using GPS or any
other sort of sensor information except
for we do use the poke length if we have
it sometimes that's in the exit tags
yeah
very useful how zoomed in is the image
but even though we don't have GPS we can
still produce this pretty nice
reconstruction it's not quite laser
quality reconstruction but it's it's
getting there it's getting closer so
this is produced completely
automatically using these internet
photos
here's another example I'll show a
sparse reconstruction in just a moment
these are photos of Dubrovnik a city in
Croatia sore a beautiful medieval city
old old city of Dubrovnik we run it to
our software and what the output is this
so this is animated again those black
wire frame points are reconstructed
camera view points so where did that
person where was that person standing
when they took their Flickr photo and so
here's sort of a fly through of this 3d
model where you can see this ghostly
version of the city of Dubrovnik flying
past as we move as we just do a fly
through so you can see all sorts of
buildings this is a church we zoom out
and see more of the city you can see a
bunch of photographs taken along city
walls sort of looking down on rooftops
in the city and here's a different view
looking out across the city towards the
mountains in the background so this is
the geometry we've reconstructed
automatically from these camera view
points question yeah these photos were
selected by searching for the tagged
Dubrovnik on Flickr and then downloading
whatever people had tagged Dubrovnik
which which often it the question was
how do we get these images often things
that are not actually Dubrovnik are
tagged Dubrovnik oh and vice-versa so as
part of this process we have to filter
out a lot of noisy images I think
there's question here first yeah
I'll talk about so the question is why
is it a ghostly version of the city it's
it's if it's if there's any aesthetics
to this it's accidental so-so but the
part that's not accidental is that only
parts of the city that were captured in
photos are reconstructed so yeah it's
probably it's it's it's it's stuff that
people have paid attention to that has
gotten reconstructed here so the good
question so the question is why is it
fuzzy it's partly an artifact of how
we're rendering it it's just you know
the OpenGL calls we're making and it's
partly what you're saying which is that
a point is only reconstructed up to a
certain accuracy right depending on sort
of the local appearance of that point in
the images that see the point okay in a
complicated way that's hard to sort of
explain in the short time here but the
other thing is it's not completely dense
because we only reconstruct points that
are sort of salient in an image if there
was a completely white wall with no
features on it we would not reconstruct
any point or if anything just a single
point big point at the center of that
wall but if something is very detailed
like intricate brickwork we would
reconstruct tons of points there all
right question yeah in this image well
so we input I think about 60,000 images
okay
all the images of Dubrovnik we could
find the output was I think about six
seven eight thousand somewhere around
there because the other yeah the others
were things like oh it's somebody's shoe
or here's our plate of food which didn't
match any other image in the dataset
okay yep
one work yeah I'll take one question now
and then move on yet
so the question is is this accessible
and is is it something that will be
updated every so often there's two
things there's a software that what
created this and there's the model
itself so the software is available for
anyone to try it's called bundler I've
released that code
there's also Microsoft has so do their
own version that's called Photosynth
that you can use to create 3d models and
since the this work there's been a few
other products one is called visual sfm
so two free downloads that can create
these models we've released some of
these models I don't think it relates
this particular one but I we we share
sort of static versions of these we
don't we're currently not updating them
but the whole what my dream is to sort
of have these models that are constantly
being updated as new photos are added
that's sort of the eventual goal sort of
have this computer listening to the
world's photos and sort of figuring out
how things are changing
oh this building is not there anymore so
our model should be updated that's who
they okay oh we got one more question
sure it's a lot I'm gonna so the
question is how much computation time is
needed it's it's pretty intensive so we
can process about a hundred thousand
photos in a day on about three hundred
machines so it's you know that gives you
sort of an idea of the scale but we've
been kick we've been constantly trying
to improve that there's a bunch of ways
we can make it better okay so I'm going
to talk a little bit about how we do
this recovery of geometry from these
unstructured photo collections and the
whole point of this is to get back a
much more structured representation so
you put in a bunch of photos and what
you get back is something that has
actual you can make actual measurements
with it's it's 3d information but how do
we get that 3d information so there's a
few important pieces of this puzzle one
is so you have these images how do you
figure out the 3d geometries of the
scene the first thing you need to know
is what
and the images are the same point in 3d
okay so this is correspondence problem
how do you figure out that you know
certain points of these are they exactly
our 2d projections of the same 3d point
okay so you need to know some form of
Correspondence in the old days
photogrammetry would click on these
points manually but now we have sort of
not completely foolproof but somewhat
automated techniques for this and a very
important development has been in
computer vision the advent of very
powerful features that are very stable
stabili detected in images what I mean
by feature is a particular point in the
image that looks salient and they can
reliably detect even in images that have
are transformed you know the lighting is
different or the camera position is
different so sift is sort of the most
famous of these and it's the first that
really worked incredibly well and well
enough for these internet photo
applications so what you're seeing here
is a set of sift points I put in an
image of the this is part of the trevi
fountain and it returns back thousands
of little features little patches in the
image that look salient okay things like
corners or blobby things things that not
not completely blank surfaces but things
that have structure to them okay so you
put in the image and outcome these sift
features that's just a black box you do
this for all the images in your
collection here's some images of the
trevi fountain we've detected sift
features in them then you compare all
pairs of images and look for matching
sift features okay so you know that sort
of builds a graph on top of the images
these images match they have features
that look similar these images don't
match because that's a plate of pasta
and this is the actual fountain and it's
smart enough to tell that those things
don't match each other those features in
those images don't match okay so you get
back this graph okay here's an example
of sort of matching SIF features between
two images it's an incredibly they ever
need to detect features you should try
sip and match features between two
images so these little squares are
features and the colors colors mean
they're the same feature between the two
images so you can see that they're
pretty well found matching features
despite the fact that
these images are very different in
lighting that image has a bunch of
pixels that are not the fountain because
their heads and it so it did an amazing
job and most Lee's are correct there's
one on the shirt that can't possibly
match any featuring in this image but
that's okay we can tolerate some amount
of noise in this feature matching
process okay flex gives you an idea of
what these matches look like okay so
we're finding the same 3d point in
several images so here's a larger
version of an image graph okay this is a
graph where each image is a node and
images are connected by an edge if they
have visual overlap if they are matching
sift features okay so this happens to be
a graph of images of Rome so there's the
sort of Colosseum part of the graph just
part of the graph is mostly images of
the Colosseum and then over here this is
part of the forum or possibly the arch
near near the Colosseum down here is
this ugly building I think they call the
typewriter in in Rome and then the
Pantheon is over here and sort of its
own cluster because you can't there's no
path of photos that takes you visually
from the Colosseum to the Pantheon it's
just they're too far apart people don't
walk between them and take photos every
meter which is what we need to sort of
connect them in this graph so this sort
of gives you an idea that we're finding
this sort of topological structure in
the image set it's not geometry but it's
a step towards it this just tells what
photos how the photos are connected to
each other in a visual way okay so then
the second key ingredient once you have
this correspondence is this big
optimization problem the optimization
that takes the correspondences and
actually tells you the geometry of the
scene okay so I wanted to take that
image graph that messy thing that
connects images and get actual
geometries so here's a 3d version of the
Trevi Fountain and that's a problem in a
very old problem you computer vision
actually called structure for motion
getting the structure the scene from
these cameras from different viewpoints
and it's sort of a chicken and egg
problem because you know the Greeks knew
how to do triangulation if you know if
you have cameras and you know exactly
where in the world these cameras are and
you have two points that match well then
you can just triangulate them and find
the 3d
that they correspond to you can also do
the reverse if you have points in 3d
that you know there are 3d coordinates
and you have an image of them you can
figure out the pose of the image but
here we don't know either we don't know
where the cameras are and we don't know
where the points are we have to sort of
figure out both at the same time it
seems like a difficult problem but it's
actually doable there's enough
constraints that these correspondences
give you that you can derive the 3d
structure it's not an easy problem
though so this sort of gives you a
picture of what at a very high level is
going on so we have these images with
corresponding points in this toy example
and we don't know where the images were
taken and we don't know where in 3d
these points are but we know that
whatever the answer is it should have
certain concern eat certain constraints
so what we want to do is solve for a set
of 3d points here labeled P and a set of
camera parameters so every camera has
both a rotation so what direction that's
facing and a position where in 3d it is
we want to solve for all these
parameters such that if you project an
image a point into an image it projects
where you expected it to go so that's
the key that's the key constraint here
you know that whatever the configuration
of all these cameras and points are a
point should project close to where it
appeared in an image that's the that's
sort of the law of perspective
projection okay so this turns into an
optimization where you want to minimize
the disagreement between where you saw
the points and where they get
reprojected to it's not that important
to understand the details but what is
important is that this is a huge
optimization that's very nonlinear it
has tons of local minima it's just messy
and gnarly to solve so right here it
sort of looks small here because we only
have three cameras and seven points I
guess one two three four five six seven
yeah
in reality will have many many thousands
of cameras and possibly tens of millions
of points so it's a huge search space to
try and search for the all possible
configurations of points and cameras
that's why this is a hard problem so the
way that sort of up till now we've got
resolved
ms2 do it a little bit at a time so we
take all these thousands of cameras find
a few of them reconstruct a little mini
scene and then sort of add stuff to that
scene greedily and build up this bigger
and bigger version of the reconstruction
of the reconstructed scene so here's a
picture of that for this dubrovnik model
i showed earlier so what's happening
here is you're seeing sort of a
time-lapse not a real-time time-lapse
this really took you know many hours but
what you're seeing is sort of the city
taking shape it's sort of appearing out
of the ether as this reconstruction this
incremental reconstruction processes
happen okay so let me show that one more
time maybe a couple more times so parts
of the city you you know cameras are
appearing points are appearing that's
the flavor of this reconstruction
process we're doing so things sort of
shift around as more cameras are added
because you have more information to
work with and at the end the entire city
is sort of or whatever you can
reconstruct from this set of images has
appeared okay and sometimes little
details get filled in and sometimes the
entire city's changes just depending on
sort of the order in which cameras are
added to this optimization so this is
what I'll call incremental structure
motion this is sort of a very standard
technique it works very well it's it's
something I'm used a lot but has a lot
of problems the primary the primary
problem is that it's very slow you do
this every time you Gribble the should
get every time you grow the
reconstruction you have to do this
optimization which takes a long time
okay
so and if you have all this odors of
Rome on Flickr for instance all three
point six million of them then you don't
want something that scales very poorly
or you're gonna wait you know 100 years
for your algorithm to finish so just for
fun here's what 1 million images of Rome
looks like 50 you made every image a
pixel in size and tiled them into one
ginormous mosaic so somewhere in there
is you know the trevi fountain and then
the pope whatnot all sort of interesting
things in rome are represented here and
interesting things as well I might add
there's runs of photos I think taken by
the same user that sort of have the same
color maybe they were testing I camp
right now so here's sort of we've
recently moved to a different view of
this problem which is a more global view
so this incremental structure motion it
sort of builds things a little bit at a
time but we've recently switched to
something that tries to solve the entire
problem at once using very sophisticated
modern optimization tools I'm not going
to really talk in detail about those
tools well give you sort of a flavor of
what this problem looks like okay so we
have our image graph images connected by
edges and we're going to treat this as
sort of a physical system right so the
images are going to be nodes which are
connected to each other by certain
constraints and you have to sort of
solve for the assembly of all those
images that sort of satisfies those
constraints so you can imagine this is a
set of Tinkertoys as I'll talk about in
a second and so we have this graph with
these certain constraints constraints
between cameras and maybe we have some
geo tags so that we don't quite trust
but we want to use them where they're
helpful okay so I'm gonna call those
unary constraints they're things we know
about an image without having to look at
any other image and so we're going to
label each camera with a pose a 3d
position and orientation such that all
of these constraints are satisfied
satisfied as well as possible so so
imagine I gave you I just dumped a bunch
of Tinkertoys in front of you okay and
each of them has label one two three
four five six seven eight to maybe ten
thousand and then I give you a bunch of
instructions that say Tinkertoy 13 and
tinker Tori 17 need to be in this
relative position with respect to each
other okay at this angle and at this
distance and pointing in this relative
direction okay so you know and then for
some blocks I tell you this block should
be at coordinate you know 100 200 300
and 3d space okay
so I'll give you a bunch of these
instructions and then I tell you well
try and put the tinker toys together in
such a way that these instructions are
satisfied as well as possible and by the
way some of the instructions are lying
to you I may have given you a geotag
that was ten kilometers away from the
true location of that image okay so you
have to deal with instructions that are
not that are trying to deceive you
that's the sort of flavor this problem
the the noise and the geotags that we
deal with in real life okay and also we
have thousands of blocks and we have
millions of these edges that you need to
add okay that's what we're dealing with
but and you know at a very high level
you can imagine as this sort of simple
tinker toy assembly problem just follow
the instructions and you'll and you'll
be a winner so where do these sort of
pairwise constraints come from how did I
tell you that camera 13 and camera at
100 have this relative configuration
well they come from looking at two
images and automatically determining
sort of the relationship the spatial
relationship of these two images so if I
showed you these two images you could
probably guess that if this is where the
first image was taken where the second
image had to have been taken you could
probably say okay well the cameras
probably turned this way a little bit
and maybe was taken a little bit to the
left and forward if given enough time
you could probably do that reasoning
just by looking at the two images so it
turns out computer vision can do that
too if you can give it if you know
matches between the images say sift
correspondences then you can figure out
the relative pose of these two cameras
okay so you can tell that you know if
camera one was taken here and looking in
this direction then camera two must been
taken here and looking in this direction
but you don't know necessarily the
heading of any of the images you don't
know it was looking north
you just know that relative to the first
image this image was taken looking this
direction okay so it's all relative
these things could move as a whole in
any which way as long as they have this
relative configuration yes
so the question is has anyone studied
what features are most photographed and
how that can contribute to other fields
that's sort of what that work I talked
about the very beginning in clustering
these geo tags photoes is doing one
thing I didn't mention is that they
looked at the visual connectivity and
tried to figure out well what photo is
the most representative of this
collection what photo appears the most
times what well it's so that that little
image I showed you the little thumbnail
that was in each of those little boxes
that was the most sort of popular photo
as judged by it there's the mo it has
the it's the photo such that the most
other photos are similar to that photo
if that makes sense well first of all
you probably would what it would
probably find is near duplicates because
that's where you probably have that's
probably the only place you have a lot
of overlap is between it would it's
potentially useful in showing fraud like
if photos doctored so I think there's a
whole area in sort of meta-analysis of
how people take photos that I'm not
really qualified to work on but I would
be very excited to work with somebody on
so that's what I'd say about that yeah
so ok so we have these constraints built
up and you know that sort of builds this
graph of these Tinkertoy blocks where we
have connections between images we have
these pairwise camera transformations on
edges and then we have any kind of
sensor information like GPS or heading
information labeling each node these are
the instructions that I talked about we
you know 3d points fold in there somehow
I won't talk about that but here's
here's another way of visualizing the
graph picture is connected by edges and
some
just have GPS tags okay and then from
that again we turn the crank and we get
out of geometry by the way there's other
ways you can sort of try and get
information about the camera parameters
so if you can detect a vertical
vanishing point in an image that tells
you how tell to the camera was and how
twisted it was so twist meaning was the
camera as straight as most people take
it or was it taken at a weird angle like
45 degrees so you can figure out just by
looking at an image sort of certain
facts about the orientation question
so the question is how important is that
it is it that it's an image of an
architectural scene where you finding
things like lines or you know standard
architectural scene you can't detect
them you won't be able to detect a
vanishing point reliably if it's not if
there's not straight lines in the image
that's where sort of the way this works
is detect edges in the image connect
them into lines find strong lines see if
they a lot of them intersect at the same
place and call that a vanishing point if
the building does not have that property
well I guess what I'd say is our
algorithms are robust enough not to get
fooled into thinking there's a vanishing
point that isn't there it would just say
I have no idea where the vanishing point
is I give up okay you're free to do what
you want with this image but I'm not
going to help you so so it we're pretty
robust in that sense yeah question we
don't so the question is do we use color
information and the answer is no we
don't use color information that sift
algorithm I talked about that works on
this grayscale image so you think color
would be amazingly helpful but really
almost all the information is in the
grayscale fix or sufficient information
is in the grayscale image yeah yeah
color is a very subtle and crazy thing
that is affected by white balance
illumination this you know same
viewpoint might have very different
color histograms question we use all the
sift features the question is do we use
all the six features yes but some of
them naturally fall away because they
don't match anything else so a lot of
stuff features are just always this
person's eye which never appeared
anywhere else in the data set so it just
sort of gets lost during the matching
process yep it automatically it's
automatically able to find the ones that
are good and ones that are
are sort of meaningless for this task
good question the question is what about
shadows shadows don't matter all that
much
sift is pretty robust to illumination
changes so so we're not fooled by
shadows okay so here's sort of a toy
version of the graph here's a slightly
bigger graph that's from the Acropolis
and it shows you sort of the structure
of some of these image sets so what
often happens is you have a very tightly
connected part of the graph because
everyone takes the same photo and then
you haven't part of the graph that's
very weakly connected because only a few
people bother to take a shot that saw
the inside and the outside of the
Acropolis in the same image so it's very
common to see these very non-uniform
graphs where you have lots of clumps
connected by sort of sparse connections
okay so we have all these so we have
this nifty optimization technique that
uses these sort of newfangled discrete
optimization techniques to to very
quickly search the search space of
possible configurations of cameras and
points so that's sort of the key here we
can use very powerful discrete
optimization techniques for this problem
I'm not going to go into it just to sort
of emphasize again the scale of the
problem we're solving we could have
millions of nodes in our graph both
cameras at point nodes and many many and
several million edges so this is a very
big optimization it's faster than the
old incremental approach we couldn't
solve this globally using a single
optimization technique but it's still
quite a big problem to solve so you have
this big Hadoop implementation it's very
fancy so I'll just show you a few
results of this of this technique I
showed you the Dubrovnik weren't already
here's a few others this is a little
closer to home this is the Cornell arts
quad reconstructed from thousands of
photos so sort of a you know beautiful
campus quad fly through of the
reconstruction
here's a larger reconstruction of
various parts of central Rome so we're
going to start by flying over the
Colosseum then through the forum which
has all these cool Roman ruins over to
this viewpoint looking back across the
the forum back to the Colosseum you can
see a bunch of other buildings sort off
to the side so these are all images that
were visually connected in one way or
another these are Flickr images visually
connected and reconstructed using these
out structure promotion elevators okay
as a by-product since we are using geo
tags even though many of them are noisy
were robust so that so the output
reconstruction is automatically geo
registered with the world so that's very
important because that means that any
image we can add to this is also gia
registered with the world and so we can
figure out the exact camera parameters
of that image as well here's another
example these are images where in this
case the points reconstructed are red
the cameras are green so you can see
that the points line up with buildings
and the green points sort of line up
with paths except where people took
shortcuts across the grass here's a
another reconstruction plotted on a map
this the Rome one and you can see that
the Colosseum is where it should be and
various buildings line up with with the
map pretty well so this reconstruction
I've been talking about accuracy at all
but at least it's accurate enough to
line up pretty well with a map and so
we're applying this sort of at scale to
all of the landmarks we can get our
hands on
so we've downloaded at this point
millions of images from Flickr all
across the world more building as much
geometry as we can from these images so
all you know for all parts of the world
we're running this algorithm and getting
back 3d models so there's Chicago here's
Grand Central Station we you know
there's photos taken indoors as well
here's the Tower of London these are
three of many thousands of 3d models
that we've built so far and then you
again these are not computer graphics
three models these are more like you
know computer vision 3d models but
they're useful for the task that I'm
going to describe which is localization
of images it doesn't always work you can
imagine computer vision is better than
it was ten years ago but it still fails
in interesting ways
this is the reconstruction of the Tower
Bridge in London
can anyone see spot the problem yeah I
mean the most obvious thing is this yeah
this is flipped so I think this is yeah
it must have confused the left and right
towers because they look the same and it
put this guy it duplicated this guy over
here well sort of hard to tell from a
close-up which tower you're looking at
so vision got confused as well it there
very white the left-hand side this one
this one well I think the promise has
two of them which you only have one
doing something back here I said there
are things yeah it's I agree there's
probably multiple things wrong with this
image yeah I should do a fly through so
you can see more clearly what's what all
right here's another example which is
not obviously wrong except when you look
at it from the top this is the Notre
Dame Cathedral what happened here is
that well if you've been to Paris this
should be the river ascend and there
aren't buildings in the middle of the
what happened is that there's a steeple
which has an Eightfold symmetry and so
there's a 45 degree copy of part of the
notre-dame Cathedral that got slapped on
to the real one and the whole city of
Paris is rotated 45 degrees and put put
down around it so anytime there's
something that's ambiguous as to which
direction you're looking at it from can
cause problems here's sort of the worst
example I've seen this is st. Paul's
Cathedral where there's both a dome
shape point there's a there's a dome
shape set of points and there's a dome
shape set of cameras but these dome
shape things should be on top of each
other because you take photos from the
dome but they're not because well
actually there's many many things wrong
with this one thing that goes wrong is
there's a tower with a clock with three
sides showing a very similar clock and
so all three of those copies of st.
Paul's good hold it up together and so
everything it's like a weird mobius
version of the universe so the gap so
the question is why don't we throw data
away if we know that it's that if we
know that it's wrong what the problem
here is that the SIF features that we're
matching that the towers are so similar
that it matches two features between
cases of the tower when those points
really aren't the same 3d point they're
just three points that have to look very
similar to each other so there's no way
that it knows that whether it's seeing
something that looks like the same 3d
point or whether it's saying two or
three or four or five copies of the
something looks very similar
so the question is is could we use from
unique features could we disambiguate
similar features I think so and that's
something we're working on I think you
that your intuition is exactly right you
should be able to tell that this is not
the same thing because you see something
you shouldn't be able to see from
different viewpoints yeah so this is a
little bit higher level reasoning than
we currently do but I think that's
probably what we need question whether
these sorry were these it's not adding
images so the question is would adding
more images help I think and I don't
think it would it's really that it can't
tell the difference between what's
really happening is that it needs to be
able to distinguish between very similar
objects and it can't do that and that's
regardless of the number of photos or
you can come up with more unique sift
features that you know base if the
pattern of dirt on one thing is a little
slightly different it would pick that up
GPS tags should help but they're not so
we don't trust GPS tags very much so
it's this weird interplay where we have
to treat we have to trust the vision
data to some extent and trust the GPS to
tag some extent and there's lying to
each other and it's hard to figure out
which one is telling the truth and then
ends up with the wrong answer so it's
it's a difficult it's difficult to know
who to trust in all cases
yeah yep so the comments that could
knowledge of symmetry help this I think
I definitely it's not clear how to
encode that knowledge into the system
but if there is a way I think it would
help it a lot yeah yeah yeah symmetry
groups it would sort of have to know
about that yeah yeah question so why is
GPS data noisy report there's a bunch of
reasons frankly not all of this is GPS
data some of it is manually entered by
Flickr users it's not always clear what
is manually tagged in what's machine
tagged GPS is also for whatever reason
either you're in an urban Canyon or you
just turned on your camera and it hasn't
quite established a lock so maybe it
tagged in with the last place you were
you know there's a number of ways that
can screw up and so it's just it's it's
not quite good enough that you can trust
it for these for a variety of reasons
because of the tall buildings yeah yeah
yep so that's you know in cities indoors
is also problem question yeah
yeah so the question is why not use
Street View or aerial views to help here
and the answer we we would love to yeah
we just haven't it's a it's a quite as a
question so we're both data access and
we're lazy I guess yeah yeah I'll get to
that in a minute yep okay let's see I
know you had your hand up for a while
I'll take one more question and then
continue for a few minutes yeah yeah
it's just it's sort of time intensive
because video you know 30 frames per
second there's probably smarter ways to
handle it but yes video is very
interesting too and I think video I
can't quite tell whether video is gonna
take over pictures that seems like it's
not happen did there's something special
a lot of pictures that people like so
we've been satisfied enough with
pictures there seem to be enough of them
that we haven't started using video okay
I just want to show a demo of sort of
other things you can do with this so
this is something I did a few years ago
now where I want to create sort of a
visualization of these of these datasets
so what you're looking at here is a
overhead view of the Statue of Liberty a
reconstruction of the Statue of Liberty
from Flickr images a few hundred in this
case and so we can sort of fly around
the scene you can see photos taken these
are photos that people took in on the
boat ride ferry ride these are photos
taking on the actual island itself I
think I saw that statue out there may be
a higher floor but it's cool it's
visible from this building but not only
that you can actually fly through the
scene in sort of use other photographs
as your virtual as your virtual camera
okay so as I fly around images will show
as soon as I get close to the to the set
of Flickr images and they're sort of
renderings
I'm stealing them too
render my virtual view so we can click
on a photo will fly down into the image
set and you'll see that now it's showing
the images that are close to my virtual
viewpoint and the reason we know they're
close is because we reconstructed the
geometry all these images are in the
same coordinate system it so I can do
it's like you know I can just treat this
like my virtual camera I can zoom in and
as i zoom into the statue it will load a
high-resolution view so I can see the
image of the right resolution I'll zoom
back out I can also rotate around the
object and we'll show images from the
right viewpoint because you know it
knows where those images were taken and
it knows enough about the geometry so
that I can align them up as it I fly
around and you know there aren't a whole
lot of images at the back of the statue
so I sort of get empty view there but
this shoulder shows you sort of the
power of combining all these image
together in this case for visualization
in showing what a place is like and
these images are very very different
weather and lighting but they all fit
together through sift matching and you
sort of get the range of all experiences
of the statute ok so that's sort of the
first part and the other part so I'm
going to go through very quickly because
so necessarily but there's fewer slides
there which is fortunate so just just
like a couple different projects that
sort of fill in well some of the gaps in
this process we're doing quite literally
so if you look at photos of the Coliseum
that are on Flickr you know there's very
few that are not looking at the Coliseum
in our era view so I'd sort of like to
be able to build a model of everything
including this restaurant or whatever
that no one takes picture of so I'd like
to be able to fill in gaps in these
reconstructions you know the backside of
the statue well a photo exists of that
but not many of them not enough to
connect it up to the front of the statue
uninteresting places maybe there's this
is the only photograph on the Flickr of
this alley in Rome and also building
interiors places where photos tend to be
rarer so you can do some things to fill
in the gaps you can use aerial images as
was suggested or
images here's the street the image which
has pretty good coverage of Rome but
you'd like to be able to do this sort of
at scale very efficiently and very
quickly whenever something changes you
would like to be able to get a new photo
of that so you can update your model
okay so the idea is why not use people
who are already walking around with
cameras to do this sort of task for you
so you don't have to drive there
yourself plus you might not know what
changed you so don't want photos all the
time from everywhere so we build a game
called photo city this was with
collaborators at University of
Washington where we sort of made this
photo capturing process into a game
where if you take a photo of something
new then you get points for that and
literally 3d points that you create new
3d points in our model become virtual
points they're not worth anything except
you know karma or kudos but you know
it's it's worth it to play the game if
you're a certain kind of person so and
we did get their alt I'll talk about
some of the ways we incentivize people
to play part of it was just they're
interested in the game and they're
interested in seeing these models grow
but there's other incentives as well so
the idea is we start with some models
that we created ourselves just from a
few photographs and then at the borders
of those models white flags appear and
those are flights you need to capture if
you capture a flag you get some points
okay so and if you capture enough flags
around a model you capture that model
and you become the landlord of it okay
so there's both models and flags and
these are things that you can capture in
the game okay so the idea is you know
over time more image will be added
people would play the you know more
complete models will be created and you
know eventually this would cover in this
test case the entire Cornell campus so
we did this as a competition there's
another incentive you were competing
against another school University of
Washington versus Cornell seeded a few
lot of they're not traditional enemies I
know but anyway we created a company
about the same size which is good in
terms of number of buildings so we
wanted to see in three weeks of gameplay
how much of the campus could be
reconstructed using photos contributed
by players who are mostly students in
this case so here's the students could
also seed new models like take pictures
of new building and add that to the game
so here's the state of play at
University of Washington after a few
days at the end of three weeks we had I
think nearly a hundred buildings covered
most of the buildings at campus and a
few that were not on campus
similarly with Cornell most of the
buildings were captured at least partial
models existed after the three weeks and
so so Cornell actually won let's see by
virtue of many more points created 3d
points in the world even though they
took fewer photos and had fewer models
reconstructed so it turns out Cornell
students were better at exploiting
features in the game where if you take
close-ups a very detailed texture then
you're going to create a lot of 3d
points because they figured it out they
figured it out so they were more
efficient in photos per point and by
that metric they were deemed winners but
in any case we got a lot of we've got a
lot of models out of this you know many
more photos than were available when we
started if you look at photos on Flickr
so if you look at the actual gameplay
this is sort of number of points over
time blue purple is u-dub red is Cornell
sort of seaside for a while there's some
you know as one took over the other one
took more photos and so on and
eventually Cornell won because they
found later on how to exploit but okay
so anyhow here's a reconstruction of one
of the buildings at Cornell that
resulted from this process a fairly
complete reconstruction again much more
complete than we could have
reconstructed from Flickr photos alone
so here's sort of fly around that this
entire building sort of like iconic
building at Cornell with the clock tower
here's the winner of the game with this
prize we created a winner he contributed
the most number of photos out of any
player I think over 10,000
and we gave them a laser-etched cube
with the model that i just showed you
engraved inside of the inside of the
cube yeah here's a few other models of
nice-looking buildings at Cornell
you know people some people got into it
many players gave up after contributing
a few photos because it's actually hard
to it's not the most obvious game to
play you have to sort of get try it a
few times to see what happens if you
take a photo that doesn't match anything
we don't know where it goes in the model
so we don't know how many points to give
you so it takes some time to sort of get
into it but people who I stuck with it
tended to get addictive and one of the
addictive things was seeing the models
grow over time so people would upload a
thousand photos at night and in the
morning they'd be very excited to see
all the different things that they had
done with those photos we didn't
reprocess orphan photos so they never
got any points unless they uploaded it
again later yeah that would have been a
coup if they had yeah if they had that
would have been great I agree or a blend
of some kind yep maybe in version 2
question yeah
originally we wanted to encourage cell
phone use because the idea was you could
play the game as you're walking around
but people didn't do that or most people
didn't do that most people did go the
digital SLR route because they
discovered that the better the camera
was the more likely they would get more
points so what they what they ended up
doing was there go out and take a
thousand photos and then upload them and
batch the CRO oh the odd that that's
very yeah we didn't really there are a
few times when the police got suspicious
okay what question over here yes so from
if we have no metadata we we can figure
it out so we that's part of the G how
much even the focal length we figure out
even radial distortion parameters we
figure out yep all that's automatic yeah
that's the beauty of computer vision
it's that secret but it's come it's as
long as yeah so I should make one
distinction here which is that we can
compute if we don't have any let's say
you gave me a photo collection without
any geo tags at all I'd be able to still
give you geometry but I couldn't tell
you where in the world that model lives
okay it could be in Alaska or you know
New York City or Mexico I have no idea
it's just relatively speaking I know
where the cameras are but I don't know
where it fits in the world so I do need
some geo tags to tell me where it fits
in the world oh well you need one too no
you need one reliable one to tell you
you need ideally you'd have three
reliable ones because there's also
orientation and other things yeah okay
question and I we have to wrap up fairly
soon ten minutes yeah correct nope
I would love to incorporate those yeah
it seems like there's no place I can go
on the internet and download all of them
in the world but yeah like building
footprints I've seen for New York City
you know if people have ideas if people
can point me to the right resources I
love any GIS type data I'd love to get
my hands on yep so let me just get
through the rest of this talking then
maybe it's time for a few more questions
afterwards all right so I'll ask one
question where was this photo taken yep
it was in Havana I heard heard that I'm
not gonna ask you the other ones because
we're running out of time you'll have to
think about that later although I
already showed you the answer so so what
we're able to do now is by matching to
these models matching new photographs to
the models we can tell exact camera
parameters of new images so here's an
example of an image we match to our role
model and it thought about it gave back
your position and orientation so this
image was taken here looking in this
exact direction so we're getting very
accurate pose estimates by doing this
matching to 3d model process we also get
the horizon line you know just from
knowing the geometry of the image where
it's in where it was taken in the world
and this I think is very useful for
being able to relate this image to GIS
data like if you had a street if you had
a map of road beds in Rome
you could just project it into this
image and you know which pixels in this
image are roads and then for image
understanding tasks that's you know that
would seemingly be very useful it's oh
it solves a number of problems here's
another image here's its field of view
if you knew for instance what the bus
schedule
busses in Rome are you could maybe tell
approximately what time this was photo
was taken so there's a number of
interesting things like I think you can
do once you have sort of exact once you
can recover exact pose of an image and
sort of relating it to all the stuff
that's in Google Earth and all the stuff
that's in New York City's GIS database
would be very interesting here's just a
few other examples this sort of shows
you the accuracy with which we can
localize images so this image is of the
by the way we didn't know any GPS for
this image it just sort of figures it
out by matching to these models I it
knows that it was taken here and not you
can see that if looking through this
fountain and indeed the image is looking
to that fountain so that's sort of the
level precision with which we're
localizing the imagery here's a few
other examples it works indoors as well
since it's just based on computer vision
as long as the models are registered
correctly with the world this works okay
so that's sort of now we are that that's
the system we're building for starting
to calibrate these new images I just
wanted to mention one other thing why
would you want to do this here's an
example of a Geo register data source
that were leveraging here so I love this
this is really cool I'm glad someone did
this but you meant you may have been to
the monument in London this is the
monument to the Great Fire of what was
1666 when a lot of London bring down
Christopher Wren and designed this
monument built it it's pretty cool I
think is originally designed to be a
telescope but it never worked
it was telescope that's always pointed
at the same place but there's turns out
there was too much traffic around like I
guess horses and so on that it was too
shaky to work as a telescope the spiral
staircase you can see how it could be
used as a telescope someone well there's
an artist called Chris meas Andrews who
put a panoramic webcam at the top of
this thing and filmed every 30 seconds
took an image every 30 seconds for
several years a high-resolution image
here's what one day's worth of that
imagery looks like from midnight to
midnight you can see
whether you can see the shadow
eventually of the tower of the monument
going across the city it's very
beautiful
I'm really glad this data exists but
we're just thinking of sort of crazy
things you could do with this and what
we came up with is sort of the world's
most convoluted way of telling time
which is to take a photograph from about
the same location and to correlate it
once we know the position and
orientation actually the orientation is
more importance to correlate it with the
with the clouds visible in that
time-lapse video so we we know about
enough about this image to segment out
the sky pixels so here's the sky pixels
and then you correlate it with we happen
to know the day this photo is taken you
correlate it with the sky in this image
in both cases north is north up is north
and there's a clear peak and so you know
that this photo was taken at that time
because the clouds match so it's a cloud
matcher
or knowing time and so if you match the
photos you can see that they're don't
align exactly because well first of all
the photos aren't exactly in the same
place and also clouds can move a lot in
30 seconds depending on how fast they're
going but it's it and yeah but it's
believable match and it's it's it's
clear enough that you can see that there
indeed the same cloud so you know
there's just to wrap up there's a lot of
interesting challenges here this is sort
of just the first steps and leveraging
this internet data for useful
measurements okay we've done sort of the
first thing which is figure out location
we're starting to do time as well but
it's an ongoing challenge so scalability
is is still a big challenge doing this
for really the entire world and not just
small percentages of it dynamic scenes
how do we model things that are changing
can we get people to help with things
besides just taking pictures can we help
can people help annotate and sort of
understand this image data sort of like
the Galaxy Zoo example changes that
occur in the actual star
the world can we model those like you
know in China before the Olympics lots
of things got built so it'd be
interesting to try and understand that
progression from photos interiors are an
interesting challenge too they're very
they tend to look very different from
exterior and so there's different vision
challenges associated with that we
started to do try and match him and just
over time there's all these beautiful
paintings and etchings from you know all
the way back at least to the I guess
15th century were done prospectively
correct at least by some people so
here's some examples of all betting's
and photographs photographs of different
times these are things that are very
challenging for our method right now
because the features look very different
so we're starting to work on simps using
symmetry as a feature for telling that
two things are the same so I'd like to
thank all the students who I have the
pleasure to work with and collaborators
and with that thank you very much and if
there's more time for questions I'd be
happy to take and more information can
be found there it looks like it's 7:56 I
know we have to be out of here by 8:00
so maybe maybe I'll take questions off
one question one question I think you
had your hand up first so what you'd
have to know is that if the camera if
the so the question is can we use video
to help disambiguate rotational symmetry
I think so yeah I mean you'd have to
know that when you move to something
that looks similar to something you saw
before that you hadn't done the full 360
but had moved not quite you know
to be louder you can work out that
weapon since you work out the range from
the magnification other pieces of data
you can work out a reasonable velocity
approximation of how far the camera goes
move the frame right yeah I agree I
agree with that you can work it out I
think the bigger question is I have no
idea how people take video like what are
the what are these 72 hours worth of
video on YouTube
first of all can I find I'd like to not
do it myself I'd like to be able to find
videos that help that already exist how
do I find the video and do people
actually take video walking around an
object or how well if I'm at the
pantheon how do I take a video maybe
that's an interesting question in itself
how do people tend to take video
differently than they take photographs
so I'm not convinced that the video data
exists that will help with us but it's
sort of an inch video is a very
interesting space okay thank you yep
thank you all for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>