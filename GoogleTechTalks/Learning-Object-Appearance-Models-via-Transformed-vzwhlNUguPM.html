<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning Object Appearance Models via Transformed... | Coder Coacher - Coaching Coders</title><meta content="Learning Object Appearance Models via Transformed... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning Object Appearance Models via Transformed...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vzwhlNUguPM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have Eric sudderth with us Eric just
finished his dissertation at MIT and now
he's at University of Berkeley
California at Berkeley he's working
there with mike jordan and Stuart
Russell and Erik is responsible for a
number of interesting innovations in
machine vision including nonparametric
belief propagation which is an idea that
he's put a little bit on hold and we'll
be talking about today but maybe we'll
have him back to talk about that he's
going to talk today primarily about his
work on hierarchical object appearance
models using dearest light processes
okay thanks Tom thanks for having me
here today hi everyone hear me okay yeah
okay good okay so as Tom said i'm now at
UC berkeley most of what i'm going to be
talking about today is actually work
from the sort of ladder year year and a
half of my thesis at MIT although at the
end we have a few hints about some
things I'm thinking about over the last
six months and so I'm interested in
object categorization so how can we take
images and recognize bicycles and llamas
and and cannons and separate these from
all the other categories we interact
with on a daily basis and so I'm not
interested in recognizing particular
object instances i want to see we
generalize from a small number of
examples of say bicycles to other
bicycles that I've never seen before and
still be able to do recognition
accurately and so I'm going to describe
a set of models that allow me to learn
appearance of objects from few training
examples and I'm going to do this by
hierarchical models hierarchal is going
to be in the sense of sort of kind of a
conceptual hierarchy and getting into
coarser scales that let me transfer
knowledge among object categories and
I'm going to take something from the
nonparametric statistics literature
called the dr site process which is
going to let me avoid making sort of
very restrictive assumptions about what
the different stages in this hierarchy
should look like and learn more from
training data
of course in applications that were
usually not interested in recognizing
isolated objects we have a scene with
had lots of different things in it and
in the literature there's I would say
two main approaches to taking sort of
object recognition systems and mapping
them into scene analysis the first is
what i'll call the sliding window
approach where you've trained some
classifier that map's a little
rectangular win to window into a sort of
yes no object present absent decision
you slide this window over the image
until you find a location at what you
think the object is there and of course
you're going to have sort of lots of
different detections usually there'll be
several kind of partially overlapping in
the regions of objects and then either
some post hoc processing that goes on to
prune those out a second approach is
what I'll call a greedy feature
extraction approach so you look over the
whole image you find your best
hypothesis for what the object is then
you take the pixels or features or
whatever your representation is you pull
out that part of the image and then you
look around for your second best
hypothesis and then you repeat this
process until there's no hypotheses left
which are sort of sufficiently good and
I'm going to sort of take a different
approach in this talk I'm going to try
to build models and think of scenes as a
whole so a scene is a collection of
objects which are in turn a collection
of parts and these parts are finally
going to be related to features so for
me features are going to be things that
I can actually measure from the image in
a low level fashion and the parts and
objects are going to be things I'm going
to learn about to let me interpret those
features and I think there's a number of
reasons why you get a lot of benefit out
of thinking about scenes in this sort of
more integrated way and i'll come back
to some of those issues later in the
talk so there's going to be two halves
to this talk the first is I'm going to
focus not on scenes but on just
individual isolated objects and let me
talk about ways of building
representations that share information
among different option categories in the
second part of the talk i'm going to
return to multiple objects scenes and
describe a set of part based models
first in sort of just a simple 2d
representation and then moving into more
3d representations of the visual world
so I imagine we have a kind of varied
background in the audience so please
don't hesitate to ask me questions if
I've fail
to explain something or there's a piece
of background information I'm assuming
that you don't know okay so park based
models have a really really long history
and the vision literature both in the
computer vision and human vision
communities so some of the oldest ones
go back to nineteen seventy-three on
what we called pictorial structures sort
of mass and spring models for the
appearance of objects a lot of things
more recently the constellation models
are essentially a sort of probabilistic
retelling of this pictorial structures
there's other things like people use
support vector machines criminal methods
things that integrate models with
segmentation dozens of other papers that
I haven't listed up here so there's a
big big literature and I'm going to have
my own notion of parts which we think
has some both conceptual and
computational advantages but in all of
these models all these park-based models
one of the first issues you come across
is well how many parts should i use to
describe my object what's what's the
right level of granularity or detail and
if i wanted to recognize just a
particular object say just this
particular elephant then maybe the
answer is as many parts as possible i
should use as many as i can but really
we're interested in recognizing from
seeing a few elephants elephants i've
never seen before and so it's no longer
clear what the right level of detail is
what are features that are particular to
this specific elephant what are things
that are going to generalize and then of
course when i start thinking about
scenes containing multiple objects then
this question of how many objects are
out there in the world is really the
fundamental issue i care about this is
really in some sense what I want my
machine vision system to answer so you
really it becomes important to think
about this uncertainty in the number of
how many objects are how many parts are
out there so before I get into the
details of how we solve that I'm going
to make a sort of initial transformation
the image that's going to make it easier
to think of our models and so we're
going to take the images and extract a
set of features in a low-level fashion
I'm using sort of what are now
relatively standard methods in the
literature to do this so there's going
to be a set of essentially coroner
features what are called maximally
stable regions you can think of this as
just sort of large regions that are
relatively homogeneous and appearance
there's also edges and if you look at
these sort of three these
the features for these three these
different scenes you can see that kind
of capture complimentary aspects of the
visual world and so the goal here is not
as not to sort of do a lot of
compression where to see we're taking
lots and lots of features and we're
hoping we're not throwing away too much
information by going to this feature
representation but rather it's just to
try to get to get some invariance to
scale lighting pose those sorts of
variations and also make it a little
easier to build up models at higher
levels and we also need a way of
describing the appearance of these
features and so I'm getting going to use
something that's relatively standard
that David Lough is proposed called the
sift descriptor if you haven't seen sift
the idea is pretty simple I'm going to
take a bunch of sort of derivative
filters apply them to my image I'm going
to compute histograms of the response of
those filters so how much orientation
energy is there at different angles and
then I'm going to take these and map
them to this big histogram and then to
do a little extra dimensionality
reduction we're going to take these Ross
if histograms which are hundred twenty
eight dimensional vectors and vector
quantize them so use k-means map them
down to on the order of say about a
thousand visual words okay so this is a
terminology that others and i have used
is is sort of by analogy with with work
that's gone on the text literature by
taking these continuous descriptors
doing this quantization you get this
sort of your each little patch in the
image goes to say one of a thousand
different words in your vocabulary and
so some of the models I'm going to be
used or actually generalizations of
things that have been used before in the
text analysis so at the bottom here this
is just to give you some intuition for
what these visual words look like I took
all the features in all of these images
and I've mapped them to their closest
visual word and hear each color
corresponds to a different visual word
and these are the visual words that tend
on the line up most with computer
screens in a data set of office scenes
as you can see there's some
repeatability you have these purple
features that tend to show up on the
upper left and there's these green ones
that tend to show up on the lower right
but they're certainly not perfect
there's a lot of noise there's a lot of
sort of extra detections and they the
unfortunate thing is that any time you
try to do low-level feature detection
and make decisions it's the feature they
are not you're always going to have a
lot of noise no matter how care
will you tune your thresholds so one of
our goals is to build higher level
models are going to be robust to this
sort of variation okay so I'm going to
start off with a sort of simple
generative model so think of this is a
three-part model for a bicycle and I'm
going to elaborate on it and the way a
generative model is going to work is
that in each image I'm first going to
sample a reference position so this is
just going to model the fact that I
don't know where the bicycle is located
in the frame it depends on where I
pointed the camera when I took the image
so here it happened to be at the bottom
there was at the top then for each
feature there's going to be a two-stage
process I'm going to sample one of the
parts of that object and then given that
part i'm going to sample a feature
location so a position on the object
where that feature is and there's again
there's two parts to a feature there's
an appearance which I'm not explicitly
showing here and also a position where I
found it in the image and so the think
of these red dots is the position of the
features for the second feature I repeat
the same process I randomly choose a
part condition on that part I choose a
feature so I might choose the same part
twice or I might choose a different part
okay and so I repeat this process and i
generate a whole collection features
okay so each part generate is generate
to a randomly size set of features than
there in the actual observed image and
this so there's been several previous
models that have tried to sort of have
defined what they call it apart and
associated each part with one feature in
the image so like the constellation
model there's for each part there's one
feature which may or may not be observed
and in practice those models are
sometimes fragile and and so this the
way that our model associates parts with
collections and features is really
motivated by trying to do a little more
pulling of information to avoid noise
and that low-level feature extraction
process okay so if I fix the reference
position and that generative process i
just described what what i have is just
a finite mixture model so there's a
probability of each part or mixture
component and then condition on that
part i have a probability of feature
position which is for us going to be a
gaussian with the mean shifted by the
reference position so that's what
captures how the object translates
around
and also a distribution over the
appearance of the feature and since
remember we did this vector quantization
to map appearance to visual words so
this is just going to be a multinomial
distribution says you know this word
appears three percent of the time this
word five percent this word half a
percent of the time so on okay so but
now I have this model again the question
is how many parts should I choose what's
the right number all right because I
don't have a model for that mean k is
just a fixed parameter at this point and
if i use to few parts that's going to
make an inaccurate model if i use too
many that's going to get kind of
overfitting i'm going to generalize
poorly to new examples so you sort of
want a way of choosing k that belt that
trades off these goals and so we've
looked at something called the Dare site
process which comes from the
nonparametric evasion statistics
literature and essentially it sort of
tries to sidestep this whole bottle
selection issue entirely it says okay
well I'm not going to pick some finite
number of parts I'm just I say there's
going to be an infinite number of parts
but I'm going to put a prior on these
weights pi those probabilities of these
parts that sort of encourages simpler
models I'm only going to use all those
parts if I really have enough data to
support it and this prior has a pretty
intuitive form it's what's called a
stick breaking prior so you can imagine
I have all these weights for all the
mixture components it's a probability
distribution they have to sum to 1 so
I'm going to start with a unit length
stick a probability mass and then the
first mixture component I'm just going
to take that stick and I'm going to
break off a random proportion of it okay
so pi 1 is going to be some random piece
at the stick I'm going to draw that from
a beta distribution which is a standard
distribution on the unit interval so now
I have some smaller part of the stick
left again I'm going to draw another
beta random variable and break off some
random proportion of that so I'll give
me a second wait pi 2 then now I have
this this yet smaller piece of stick
remaining so now I again draw random
proportion get a third mixture component
and so on and so you can see that over
time I have less and less of the stick
left so on average these mixture
components are decreasing and so if I
sort of pick any sort of thing
significance level say I'm interested in
the mixture components that generate
more than say half a percent of the
probability mass there's going to be
some small randomly sized subset of this
full mixture weights that do it ok ok so
now I come back to the to my model of
objects I have exactly the same thing I
had before except now there's an
infinite set of parts and I'm going to
put this dear sleep process prior on the
the mixture weights ok and now i'm going
to it turns out when you start thinking
about building hierarchical models that
relate multiple objects together is a
nice way of thinking about this not as a
mixture model but as a distribution so
think of these parts are defined by
parameters right there's some mean and
covariance for this Gaussian and then
there's the parameters my multinomial
distribution and these are what encode
the distributions on visual features ok
and now each object is just a
distribution on these infinitely many
potential part parameters so for each
each object there's a collection of
parameters the parameters of each part
and then for each part there's some
weight saying you know this which parts
are more and less important to the
features you actually observe from the
object and so now I can take what I had
before I can write it down yeah the
previous slide
you're saying the word object as
distribution should be object categories
as distributions or sure sure yes yes a
number of times i'm going to use i'll
just say object and i implicitly mean
object category not object instance yeah
yeah so there's going to be a learning
process so we're just going to infer the
weights it turn this order that it
doesn't matter because the model is an
invariant to a essentially the fact that
part is part five it it so easy if you
think of a finite mixture model right I
have like it all I could arbitrarily
permute the fact that this one mixture
component is number three in my code and
one is number 17 in my code doesn't
matter I could always just kind of
permute those around and so what the
stick breaking prior does is is it it's
it's it's it's a sort of mathematical
tool you're right that on average the
first one is going to be bigger and what
the learning process is going to do is
is is it simultaneously going to be
learning what the weights are and also
what the parameters are and so it's
going to put the parameters that are
important in earlier parts of this order
that makes sense
so so you can write the same generative
model down in terms of a graphical model
or directed Bayesian network so i have
this g is this distribution that defines
what my object category looks like so
it's this collection of park parameters
and also the probability pie for each
one then for each image I'm going to
sample a reference position a location
of the object but a particular image so
this if you haven't seen this plate
notation this J just means i make j
copies of these random variables so i
have j images each one I pick one
reference location then for each feature
each of the in features in that image
I'm going to first sample a set of park
parameters so I'm going to when I draw a
sample from GI take the full set of
potential parts and I pick out one make
a copy of its parameters and then
condition on that I'm going to sample a
feature appearance which is going to be
from a multinomial distribution in a
feature position which is from a galaxy
so if you haven't seen a lot graphical
models before this is surely not helpful
to you and this is exactly equivalent to
that generative process I'd ruin picture
several slides ago with the bicycle ok
but but for those who have seen the map
before this is a way this is sort of
more formally what the model looks like
ok so one of the issues with these big
harpal models is how am I going to learn
them from data you know I have
parameters as a worry that it's going to
be hard to specify them and one of the
nice things about Derek sleep prophecies
is that they have a lot of nice analytic
structure that's going to let me develop
fairly simple learning algorithms that
work efficiently and so the sort of
simplest algorithm you can think about
for these things is what's called a
gibbs sampler where I'm going to sort of
sample a set of variables sample from
the posterior distribution of a set of
variables given all the data I've
observed and as it turns out there's a
relatively small set of things I need to
sample I'm going to sample for each
image the reference transformation so
I'm going to be trying to figure out
guess where the object is located and
for each feature I'm going to sample an
assignment which you can make it was a
pointer which says ok for this feature
which part do I come from I come from
part 5 or part 17 or part whatever so
I'm going to sample these the discrete
pointer variables for each feature
given these I can integrate over this
distribution that defines the object
thing so the actual part parameters and
their weights and also this prior
distribution on transformations in
closed form so if you haven't seen this
before this is a really neat trick it's
called Rowell blackwell ization you can
prove nice things about this one what
this does to your learning process and I
think it's this sort of way of sampling
a small set of things and then
analytically just marginalizing away the
rest is really a sort of nice practical
tool for making samplers work and then
finally there a small set of hyper
parameters which fortunately turn out to
be easy to deal with so there's a prior
on the transformations which essentially
just encode the image scale so what
coordinate system have I defined my axes
on it's easy to set there's another you
have to set a prior which says roughly
how large do I expect my parts to be
what percentage of the image as well as
how sparse do I expect these visual
feature distributions to be so on
average should my given part put weight
on ten percent of the vocabulary or five
percent so this is essentially one
scalar that tells you that and then the
loop it's fairly insensitive to those
the one that met the one that matters
the most is actually what's called the
concentration parameter so this was if
you remember that stick breaking process
there's a number which says on average
how big are my pieces of the stick so
you can imagine this affects how many
parts i end up thinking I find and it
turns out that rather than fixing this
what you can do is instead put a prior
on it so we put a fairly vague disperse
prior on this and sample it as part of
the learning process and there's a
literature on ways of doing this and I
found in practices works quite well so
sort of an anecdote I as you can imagine
when I started working with this before
I got the full model working I was sort
of fixing things like the concentration
parameter and I spent a few months
trying to tune it as well as I could
then I got around to implementing the
thing that resample zit automatically
and the first run did better than I
never managed to tune it myself so it
actually I was pleasantly surprised by
how well that worked
so one way to get the intuition from the
model is just now we can look at a
single object category so I have a set
of frontally aligned faces from the
Caltech database and one of the
properties of this stick raking prior
which is maybe not obvious from that one
slide I gave you but if you look in the
literature those nice ways of
understanding it is that it it as you
get more data it expects to see more and
more components so it expects the number
of the complexity of your model to grow
at a slow rate with the amount of data
you observe so if I give it 4 images at
the top row I've shown the set of parts
created by the model so these are like
Gaussian distributions these the
covariance ellipses of gaussians and
this is with 4 16 and 64 training images
so you can see as I get more and more
data it starts off making a sort of
relatively coarse decomposition of the
space and then it makes a sort of finer
and finer grained decomposition creating
additional parts to describe more
structure and so in this upper left
corner that plot is showing the mean and
variance of the number of parts that
creates as a function of the amount of
training data it observes and these
unfortunate these are hard to see in the
projector here these are just
segmentations showing you know which
which features are being mapped to which
part the explanation process but the top
row should gives it I think it's a
better sense for what the decomposition
is doing yes
this is this is an old database right so
this is I mean this is for images that
look kind of like this and this is 60
and and even with 64 images there were
some overlap of the subjects there may
be there might be 20 unique people and
someone that had different different
pictures taken in different environments
it's not it's the Calot this this
particular later said is not considered
to be anything that's interesting for
test purposes I just used it here
because it gives you some intuition for
what the learning process does okay so
but that's a single object category I
remember we're interested in recognizing
lots and lots of different objects so
suppose hypothetically speaking that you
taking the time to collect a whole bunch
of images of llama faces and so you have
a world-class llama face detector and
now I show you an example of this new
animal cow you've never seen one before
it's your first cow does the fact that
you know lots and lots about llamas does
that make it any easier to recognize
other cows do I have to start from
scratch with each new category I hope I
don't have to start from scratch right
because it varies depend on you talked
to but humans can recognize something on
the order of 10 or 20,000 object
categories and so if I really have to
get hundreds of examples for every
object category ever want to learn about
that's that's a tall order and so you
really like it to be the case that if I
learned a lot about some category and I
show you a few examples of something
thats related you you can generalize
well and there's lots of ways of
thinking about doing this generalization
and one that we've explored is by
essentially the comes from the
observation that if you look at objects
although they're globally different in
appearance often they're sort of local
sub regions that look relatively similar
so noses and wheels are sort of
relatively stereotypical but this really
shows up in sort of lots of other cases
in terms of contours and
coroner's and and small textured
patterns and so forth and so the ideas
we're going to try to learn parts of
objects that are shared across
categories and of course here we really
can't afford to manually have to label
all the parts and how they correspond
that's just that's just not feasible so
we really need a way of automatically
discovering how many parts should there
be what pattern should they be shared
across the categories and the nice part
about the deerslayer process is that
it's fairly easy to put an extra level
on the hierarchy and get the sharing and
so the way the model works is we're
going to have a sort of global set of
parts so you think this is there's a
global bag of parts and then when I I'm
going when I make an object category I
go in this global bag and I take out
some subset of those and those are the
parts that can pose that object and then
the second object category you're going
to pull out some different subset so to
take an example it might be the case
that when I go to compose bicycles and
cannons that there's a wheel part that
shared between them and there's some
other parts that aren't shared now given
now these parts at this level for the
objects the rest of generative model is
exactly like I had before for each image
we sample a location for the object and
then we generate a collection of
features so it's just this extra level
at the top which says that if I have
five categories they're not just five
independent sets of features I actually
expect five independent sets of parts
excuse me I expect there to be some
sharing of those parts across categories
and so if you if you sort of take away
all of the continuous position and
portion of this model so get rid of the
reference transformation and you have
not features that only have a visual
word which is the same as you would have
if you just took a text document and
scrambled the order and just had a bag
of a bag of words you get something
called the hierarchal daresay process
which uit mike jordan and collaborators
developed a few years ago for the
analysis of text it so we've added sort
of added the pieces into this that you
need in order to really model what model
the sort of spatial structure you have
in visual scenes
okay so to test this way I wanted to
look at a set of object categories where
there was some hope that sharing parts
among them would provide benefit so we
put together a data set with 16
categories there's a sort of seven
animal faces five animal profiles and
four sort of things with wheels vehicle
is whatever you want to call them and so
now each training image is labeled with
which of these 16 categories it's a
member of the model we're not going to
tell the model about this sort of course
or level grouping that I've written down
here as far as the first learning
process knows it's just a set of 16
categories and so one of the questions
is whether it's going to automatically
uncover some of this structure that
there's some categories that are more
similar and when when sharing and
defining parts so
and give you a little intuition for what
the parts that it learns looks like
number each part has two things it has a
distribution over feature positions
which is a Gaussian so here I've plotted
this Gaussian think of this as being
relative to the center of the object
okay so this is where the features of
this part show up and there's also this
position distribution which is a little
hard it's this distribution on vector
quantized sift descriptors that lives in
this high dimensional space it's a
little hard to visualize directly so
what I've done is I've taken all the
features and all these images and I've
mapped each feature on to the part
that's most likely to have generated it
and so you can see that in this case
essentially this part seems to be
generating features that show up around
the wheels of these vehicle categories
here's another example of features that
essentially seem to show up around the
mouth regions of the different animal
categories another thing we get is that
the learning process recognizes that
well around the edges of the image you
tend to have these kind of this
background texture that's not very
predictable it's not very correlated
with the object category so you get
these kind of very long gated parts that
are sort of widely shared among the
categories do you think of this is 11
sort of I didn't actually expect this
when I when I first implemented it but
we're getting some additional leverage
of the of the sharing by saying that
well when I want to estimate what
background clutter looks like I can pull
information across the categories rather
than having to treat each category
separately so one way of visualizing
what we've learned is so there's that
was three parts with thirty training
images / gat aghori I think we learn on
about 150 or 200 parts maybe 150 and so
one way of sort of putting it all
together is to say that well so each
object is defined by a distribution over
these parts so I can look at pairwise
distances between those distributions so
we use just a scimitar eyes KL
divergence and then i use
multi-dimensional scaling to map those
distances into the two-dimensional plane
so in this plane each point corresponds
to an object category
points that are nearby share a lot of
parts so they're sort of close in this
metric points that are far in Euclidean
distance here have a large distance in
terms of their part probabilities so
they don't have much overlap in the
parts days and you can see some fairly
natural groupings so you have all of the
faces over here on the left side and
even within the faces you have the more
elongated ones and the the cat faces
separated you have the profiles here and
the vehicles with wheels up here another
way of looking at that same data is to
take do a sort of bottom-up
agglomerative clustering so you're
paralyzed merge and you get what's
called a dendrogram which again sort of
tells you about their split and you see
that there's sort of four main clusters
in the sharing so again we have the sort
of bodies of some of the animals we have
things with wheels we have some face two
sets of faces and then the the in this
case the wheelchair and the llama body
really didn't share much at all with any
of the other categories okay
so to get a more quantitative analysis
we looked at two tasks so in the first
detection task I give you a test image
and you have to tell me is it object
category or is it sort of just
background so I have this sort of data
set of natural scenes background images
and yet to the side is which one of
these two it is and so here are ROC
curves for that detection task so the
first thing to notice is so blue the two
blue lines correspond to models which
share parts the two green lines
correspond the models which don't share
part so i built an independent part
based model of each category so you can
see i got a pretty significant boost in
performance by sharing parts among
categories and another interesting thing
is that so so the the solid line is this
is the full model I've described the
blue dashed line is a simple what's
called a bag of features model where I
throw away the position positions of the
features and I just model their
appearance okay and in this this is
something that several people have have
looked at performing our different
variants of these bag of features models
and so you can see that you get a
significant boost in performance by
considering a spatial structure not just
the appearance but only if I share parts
among categories so if you look at the
the two green lines the green dashed
line is actually higher than the green
solid line so it appears that there's
some overfitting going on that if I only
have six training images per category
and I try to model feature positions of
each category independently I actually
do worse than if I threw those away but
not if I share
right so here's here's area under ROC
curves as a function of the number of
training images so if you look at the
two green curves they cross at somewhere
between 12 and 15 images per category
but even out to 30 images per category
you can see the the best performance of
the unshared models is still quite a bit
worse than the best performance where I
share parts along categories what's
going on I think I think there's still I
think I think you're just even so 30
images is not a large enough data set
that you don't have idiosyncrasies right
so you may have in your 30 training
images of Cougar faces you may have not
gotten one where he was opening his
mouth in a particular way right he may
be they all had closed mouths but maybe
you had a cat image or one of the other
categories that had his mouth open in
that way or maybe maybe maybe there was
something with the shadows and things I
think I think that just kind of it gives
you a little bit of extra it regular
rises the learning process but in a nice
way in a way that that doesn't it's not
me having to set very strong priors and
hand-tuned them it's just kind of
sharing information across data across
things and so and and so I mean this is
this is really this is the really the
thing that makes this is why why sort of
beige and harmful models are useful it
the thing I would say the single biggest
thing that gets you is that there's a
lot of applications and statistics where
you have a bunch of a bunch of groups of
data and you expect them to be related
so similar but not exactly the same and
the simplest way to deal with that is to
make them all independent or stick them
all in one group say they're exactly the
same and with hierarchical models you
don't you can do something that's an
in-between you can say they're going to
be similar so they're related but not
the same and the Harkle daresay process
is a particular nice
way of doing this because you don't have
to make strong assumptions about the
form that similarity should take and
another benefit I think is that so you
get simpler more compact models by
sharing so here I'm showing the number
of parts that create as amount of
training images per category and the the
green line up there is for independent
models of the 16 categories and this
blue line is with shared parts so not
only am i doing better performance I
have you know 100 150 400 so about 130
parts instead of 450 and when it comes
down to test time everything that you do
with these models is the cost is linear
in the number of parts so looking at a
test image to sign what category it is
runs about three times faster the shared
model and you can see if you look at the
slopes of these lines if you went out to
a bigger data set I expect that you
would probably get larger gains but so
the thing that you might worry is that
by sharing parts among object categories
I reduce my ability to distinguish
between them and so to test that we
looked at a ace where you you have have
to make a decision is it the object
category or one of the other 15
categories that that were trained on
jointly and it turns out in this case
that yes at least with this sort of
simple form of sharing you get a small
loss and recognition performance so here
the green line is independent model of
each category in the blue line is where
i share parts and you can see that
there's a small gap between them there
if you look at as function of our
training images there's there's a
there's a gap that stays there out to 30
images it's it's the loss here is
smaller than the game you get on
detection but there is a loss and as you
might expect it's this is these curves
averaged across categories if you look
more detailed it's what's happening is
what you would expect so if I share
parts I do a better job at
distinguishing
sugars from bicycles but I do a worse
job at distinguishing Cougars from cats
from from very similar categories and
and so this is I think that there's
probably a lot of other refinements you
could do in terms of being richer about
the way you share maybe use a more
discriminative training criterion that
might that if you're really interested
in this this categorization task that
might might lessen this cost okay so are
there questions before I move on to the
second yeah
you mean what's the what so the question
what's the final performance relative to
a discriminative model I'm not sure on
that that the 16 category data set later
later on I'm going to show some results
on analyzing street scenes and so we did
look at we looked at a basic sort of
boosting boosting car detector compared
how that worked to our performance of
finding cars and they're basically the
same which we're happy with as a first
shot at this because there's a lot more
sort of engineering that's gone into
those boosting detectors then this has
thus far so my sense is that that you're
not going to you know at the moment
you're not I think the real benefit from
this is israel by it as you'll see later
it's it's easy for us to sort of think
about partial supervision very
heterogeneous amounts of labeling at
different parts of the learning process
and that's much harder to deal with with
discriminative learning so that that's
that's the real hope with this is that
if you give it lots of data where you
don't have a lot of labels but you have
a lot of unlabeled data that maybe we'll
be able to get get some leverage under
that and in fact that's that's what this
is right here so so this is these are
images from the label me data set which
Antonio Tarawa Brian Russell and Bill
Freeman have put together at MIT it's a
nice data set if you haven't seen it so
there's a web interface where you can go
in label images give it whatever name
you want to and you can it's all free
for download i think there's tens of
thousands of images hundreds of
thousands of labeled objects instances
at this point and so if you look through
the data set this is I mean this and any
other data set like this I know this is
kind of what it looks like is there's a
relatively small number of images where
people have really gone in and labeled
them in detail so they kind of label all
the objects that are present there's a
lot more images where they've done some
partial labeling so maybe they went
after the cars and here they did the
road and there they did the tree but
here they just did the cars and so
there's more like this and then there's
essentially a limitless supply
um on level images more than well maybe
maybe you maybe Google could handle all
them but more more than I could ever
handle to deal with computationally so
and so this is really part of the reason
this hierarchal model is that we we
think that by thinking of scenes as sort
of being integrated models with multiple
objects we're going to be able to do a
lot better job of leveraging this kind
of partially labeled data right I don't
this this image doesn't just tell me
this should tell me more than what the
car looks like in this narrow window
right because i have other cars and
there's foliage near them and then that
should tell me give me other examples of
learning what foliage is above about
that so we like we like to build models
that sort of do that automatically and
so a distinction I'm going to make in
this ladder set of models is is that of
separating object categories from visual
categories and so object categories I'm
going to just define it's something that
we provide labels up so I'm not going to
be trying to sort of do everything fully
unsupervised for every object category
that I want say a computer screen or a
bookshelf I'm going to provide some
labeled examples of it but a lot of
these object categories come in very
different appearances so like we in our
office in data set will consider later
there's bookshelves that hang on the
wall and other bookshelves that sit on
the floor and we're not going to sort of
make these sorts of distinctions
manually this is going to be discovered
in unsupervised fashion so when we look
at multiple object scenes and the
question that comes up is well how many
cars are there and where are those cars
located in the scene and the way we're
going to do this is by combining our
earlier dear sleep process models with
spatial transformations so we can think
of these clusters or mixture components
in the deerslayer process are going to
describe what the object looks like in a
sort of canonical objects entered
coordinate frame what does it look like
by it was right in the middle of the
frame when I took my picture and then
there's going to I'm going to compose a
scene by taking multiple objects and
then moving them around to random
locations okay and so this is this
notion of what will cost
shull transformation just says I'm going
to take this canonical object and move
it into a different location okay so
this is an old idea and vision there's
been a lot of things that have done this
for the now analyzing video sequences
but we're going to do it in a sort of
different way to try to do
categorization of scenes so to get some
intuition from the model well I get a
toy world so think of these as 2d images
and these are locations of features and
if you look at the different images
there's kind of two sorts of objects
that show up in this world there are
these diagonal bars that are somewhere
in the upper right part of the image and
then they're these blobs that show up
sometimes in the lower left corner okay
and now the way that what what we call
the transform daresay process works is
that I'm going to have a global set of
object cloggers so that is are these
green ellipses and there's gonna be two
parts there's going to be a shape of the
object so that's the green ellipse and
then the red dash contour is my prior
over transformation so where does this
object tend to show up okay in different
images then for each image I'm going to
take a random number of copies of that
object and put them down at different
locations so here I might have put one
copy of the bar here I put to hear I
didn't put any at all ok and then given
that i'm going to generate a bunch of
features that i actually observe
and so this this notion of
transformation if you think about the
the it's sort of obvious from the vision
perspective maybe not so obvious from
the statistics perspective but it's very
important and the reason is that if I if
I take that data I gave you before and I
give it to the learning process for our
transform Deerslayer process then this
is this is what you get you get these
two clusters it says oh there's these
blobs though these bars and they show up
in these two locations and that's that's
what you'd want but I give the same data
to the standard hierarchal dearest a
process which treats position as being
defined and sort of an absolute
coordinate frame right so the fact that
i'm at pixel 265 that's somehow
meaningful okay and you see what the
hard cold ocean pops it has to do is it
ends up creating an independent copy of
the object for every location that it's
been observed so it's kind of
discretizing the space of
transformations this is bad for a couple
reasons one is that it doesn't know that
you could observe so for example if you
look a little bit to the right there
there's no it never happened to see a
diagonal bar there they can't predict
that that's likely to happen so I can't
generalize new locations even worse is
that when it's learning the appearance
of each of these objects that treats
them all independently so there's no
pooling of information whereas this
model is going to sort of if it
discovers multiple cards going to say
aha I can combine them together to get
better appearance models and that's
going to be important for real data okay
so now we come back to the scene we have
sort of candidate answers to these two
questions we're going to use dear site
processes to give us uncertainty and how
many cars or objects are in the scene
and transformations to model the fact
that I don't know where it happens to be
located
so that previous slide I showed how to
sort of overly simplified model geometry
it which is sort of like each object is
a single blog of course objects actually
have more internal structure and so
basically we can combine that model I
had before with the part based model
from the first part of the talk so
there's going to be a set of object
categories each of which is a collection
of parts so there's this blue category
that has one part the green that has two
and the Red that has four and this toy
example for each image then I take the
random number of copies of these objects
ship then around the different locations
and then given that I actually generate
the features I observe in the scene okay
so you can generalize the gib samplers
that I told you about in the first part
of the talk to this case and if you give
it a data set of street scenes so the
same ones I showed you earlier where
there were the four categories buildings
cars roads and trees these are the
clusters that learns so you can see that
it's got this one sort of fairly compact
cluster describing the internal
structure of cars there's a few
different clusters for buildings because
you know buildings come in lots of
different sizes and shapes depending on
where they're located so there's a sort
of larger cluster here sort of smaller
one for smaller patches of building you
can again see for the road here this
cluster is trying to model the fact that
you know roads tend to be that sort of
fill up most of the bottom of the the
image when they observe and then when it
goes to analyze the test images takes
these things and moves them around and
tries to using the partition up the
features in a way that makes sense so
here's a sort of snapshot of what it
might do on a test image so in the
middle row I've taken each feature and
I've colored it according to the object
category with the highest posterior
probability so red is red as car
Purple's building blue is road Green is
treat or foliage the bottom row here's a
snapshot of the gibbs sampler what it's
doing so you can see in this image it's
put down one car cluster to explain the
one car
sort of fully observe there and whereas
here it's put down several car clusters
to capture the fact that there are
several cars sir so this is the way in
which it's automatically accounting for
the fact that I don't know how many cars
are out there this on the the right this
is probably one of the worst images in
the test set I'm not going to give it
credit for knowing that cars are live in
parking garages and so there's a couple
things going on for why it's messed up
here one is that this sort of light dark
pattern actually looks a lot like a car
to the sift to these gradient-based edge
features that we have and then another
is that at the moment this model doesn't
have any sort of doesn't capture
contextual relationships between objects
so it doesn't know that cars don't float
in the air above other cars and so
that's that's an obvious thing that
you'd like to leverage in the scene
model and i'll come back to that a
little later you can do the same thing
with so this is just to give you a sort
of a baseline this is if I just try to
look solely at the appearance of
features don't do this classification
with position and the sort of
performance I get and if you look at roc
curves so in all these curves the black
dotted line is appearance only the the
solid line is is the full model so you
get a fairly substantial gain and
segmentation performance by considering
and then of course another another nice
benefit is that by this this transfer
dears across this model can actually
count how many objects and things are
present in the scene whereas if I just
use a bag of features model that's
impossible I can try to segment features
but I don't have any notion of how many
objects are contributing to those
features so how much time do I have left
my out
till three okay I'll wrap up in the next
next minute or two so one limitation of
the model I just described is that it's
sort of composed the scenes in the 2d
fashion it's kind of moves things around
in the plane and puts them together like
a jigsaw puzzle and we really like to
sort of get closer to the 3d structure
of seams and it turns out actually that
it makes a lot of sense to consider 3d
struction together with recognition
because so for example if I knew the 3d
depths of points in the scene then a lot
of the sort of segmentation tasks like
separating this bookshelf from this
table which are very hard in the image
they become trivial on the other hand if
I if I know what objects are present in
the scene that gives me a really strong
q4 regularizing depth estimation
recovering 3d structure so we think it
makes sense to really think about these
problems together and so it's a first
shot of doing this we looked at a data
set of office scenes in which the three
categories label those computer screens
bookshelves and desks we to calibrate
our model we use stereo training images
so very simple idea for those who
haven't worked with stereo have two
cameras if I take a picture at two
points displaced objects that are near
me move a lot points off in the distance
don't move at all so you can get
estimates of depth by finding point
correspondences so depth is inversely
proportional to this horizontal
displacement or disparity it turns out
that you can't really get good depth
estimates greedily by looking at each
feature so if I just look at each
feature and look along the epipolar line
which is the line the corresponding
image where its match has to lie based
on the camera geometry and I just
greedily try to find the one whose
appearance looks the most similar this
is the kind of depth estimate you get so
here green points are close to you in
red or far away so you can see that
there's a lot of noise right here is
doing it pretty if here's an overhead
view of the same scene so you can see
like the features on the bookshelf
they're mostly in the right place but
there's a lot of outliers okay so rather
than doing this greedy depth estimation
what we're going to do is we're going to
take a set of feature mattress can
with them to these def densities saying
some depths are more or less likely and
then and then we're going to run our
learning process so try to figure out
what objects are there and use objects
to resolve these fuzzy depth densities
into more precise estimates so the way
the model looks is now I have objects
are collection of parts that live in 3d
for each image I'm going to take a
random number of these parts and move
them around to different locations then
I'm going to generate a collection of
features that live out in 3d and finally
I'm going to observe the perspective
projection of these features when I take
a picture with the camera and so I'm
going to get scaling sort of scale and
very recognition not by resizing the
image which is a trick people commonly
use but by the actual sort of moving
objects in 3d and perspective projection
which is of course how things work when
you actually take a picture so here's an
example of the if I first look at a
constrained model where I only let
myself create one part per object for /
visual category this is what I get so
and so remember there's a prior over
transformations I've placed each object
down to this mean location so this says
if I look at an office scene where the
cameras looking this way there's
typically the computer screens and
deaths tend to be near the front and
then there's bookshelves and background
clutter farther away and here you see
with bookshelves actually it creates to
visual categories so the bookshelves
that hang on the wall and ones that sit
on the floor so it automatically splits
the observed data into those two groups
here's a multiple part model so if you
look at you have a fairly intuitive
model for what a computer screen looks
like so you have these nice collections
of corners and then there's also some
other features that show up in the
middle of the screen desk has these sort
of whores on the elongated parts the
bookshelves are a little noisier I think
that's because there weren't there are
maybe only 20 or 30 labeled examples of
bookshelves and the particular data set
we used okay
and now here's an example test image
where now I to interpret this scene I'm
moving objects around in 3d and placing
them and this is giving the cleaner
depth estimates so these are you can
quantify these these are much more
accurate than those the ones I get by
just looking at individual scenes I also
get segmentations of features into the
different objects that are present as
well as this sort of rough 3d view
here's another example again you know it
does does reasonably well you can see
it's it's hallucinating to monitor in
this sort of reflections and in the
window here but overall overall the
interpretation is is not bad so those
were stereo test images where I had two
images at test time a sort of harder
problem is if I just give you one image
at test time can you still do depth
reconstruction and so with the current
system the stereo test works pretty
reliably monocular test works sometimes
it's less reliable so you can see here's
an example where you get you get more
false alarms so here's an example where
it hallucinates a monitor and it sort of
has been off in the distance in these in
these cords here which wouldn't happen
if you have the depth cues and I think
here's in the case where I think that
context is going to play a really
powerful role because if you look at
this image and you ask well how do I
know what the depth of this desk is in
this scene just by looking at it because
you can't see the edges of the desk you
don't know how big it is well but you
know that the monitor is sitting right
on top of it right so I there's a lot of
sort of contextual relationships that
actually get messy when you think about
things happening the image that are
pretty simple in 3d and so we think it's
going to be that's something we'd really
like to go to and we in work that
actually preceded the things I've talked
about today we looked at a parametric
model so you knew how many objects were
in the scene so like a scene was one
keyword one computer screen one mouse in
an office and there we were able to
develop contextual cues that said well
the keyboard is usually below the screen
and the mouse is below into the right if
you really care about finding something
like a mouse that's small doesn't
generate many features you do a lot
better
this contextual model because it's easy
to find the computer screen that tells
you where to look but at the moment sort
of generalizing this sort of model to
this dears I process context where you
have no less about less global structure
is an open problem and more recently
been doing some work at Berkeley on
wavelet statistics sort of lower level
models that try to get away from this
feature extraction I think I'll skip
this in the interest of time but if
anyone like to talk to me about it later
I'd be more than happy to so so I've
hope I've given you a hint of some of
the things that you can do with these
horrible daresay processes I think that
they let you think about very flexible
ways of transferring knowledge Monica
categories robust ways of learning from
small partial able datasets let you
integrate sort of different things that
a recognition and reconstruction and the
thing I'm most excited about is that
it's been it's sort of hard to sort of
take high dimensional complex data like
images and just feed it sort of as a
black box into a nonparametric method
like a dare slow process but if you
really sort of think about your
representations and how you structure
the model I think you can really scale
these things to a lot more interesting
problems thanks
just curious um what is that picture on
the bottom left oh that was one of those
few slides I scientist so these are our
wavelet or stable pyramid coefficients
so these are joint statistics of pairs
of coefficients and in these sort of
band paths representations of images and
so this is this is like two coefficients
that are at a neighboring orientation so
saying that they're positively
correlated and this is this is sort of
what happens is you go at different
scales and positions and so on and so
the fact that these two rows look
relatively similar says that our model
is doing something interesting because
data sample from our model looks like
the statistics of images and and so to
show the show the the last slide out of
order so what sort of an initial test
case of this that we've done is
denoising so you take this as an input
image where you've added some fairly
high high level Gaussian noise to it and
you want to try to uncover the D noise
image and so this is an example output
of the method on that image which is
which is on par with current say that
they are methods and so we've looked at
this in this sort of image processing
applications and we think some of the
same ideas may be useful also for more
recognition higher level tasks
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>