<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Probabilistic Dimensional Reduction with Gaussian Process... | Coder Coacher - Coaching Coders</title><meta content="Probabilistic Dimensional Reduction with Gaussian Process... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Probabilistic Dimensional Reduction with Gaussian Process...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uCJNGpkxFbo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the talk is I'm going to start off
basically with some motivation for what
I'm going to talk about which I probably
don't need to spend too much time it's
where do you want but really the talk is
about dealing with high dimensional data
and the particular way I'd like to deal
with high dimensional data is with low
dimensional embeddings so I'm going to
talk briefly at the beginning about
existing methodologies and then I'm
going to focus on my work which is this
model called the GPL vm gaussian process
latent variable model what I like to do
this is where we'll get a bit technical
is do the mathematical foundations of
the model and then there'll be examples
now I used to have a few different types
of examples but one of the area's this
work has become quite popular in its
vision and graphics so I've stripped all
the examples back to human emotion
animation examples which means that
they're kind of fun looking as well so
those examples will be on that and I've
got an example that will run through the
extensions as well I apologize in
advance if I go too fast at some points
because I'm going to be putting in
extensions in fairly quickly I don't
think you necessarily want to know all
the details but it's just a way of
trying to show you this sort of things
this model to do and that'll be all
round off this sim complete ok so just
on another point all source code and
slides are available online these slides
aren't up yet they've just been updated
and in fact these toolboxes will
probably change but the updated slides
would go up and this is so that you can
recreate any of the examples I show in
the talk if you are interested in MATLAB
yeah black lab rather than that
everything else is created ok so high
dimensional data so high dimensional
data is something in machine learning
we're often interested in and in fact
machine learning we're sort of somewhat
obsessed with handwritten digits so here
is the only handwritten digit that will
appear in the talk is the digit 6 it's
64 rose by 57 columns now the space
obviously contains a lot more than just
this one digit so if I take a random
sample from this space I see something
like that now that's nothing like the
digit we've just seen and indeed if we
were to continue some
calling every nanosecond from now until
the end of the universe the probability
of seen the original six is something
like 10 to the minus 9 so you just won't
see it i'm making a rough guess when the
end of the universe will be there's a
second sample it wasn't the six which is
that okay so here's a alternative idea
rather than modeling directly in that
space why don't we do something a little
bit more intelligent and what I'm going
to suggest is a model of handwritten
sixes by taking a prototype 6 so here's
a prototype sticks and rotating it so
I'm rotating it right and I'm rotating
it there and indeed what I can do is
just show you with a little MATLAB demo
now what's going on here is I've taken
the 6 and I've rotated it 360 times and
I've created a data set from these 366 s
and then projecting that data set on to
the second and third principle
components of the data and what you can
see is it creates a nice circle with a
little bit of noise particular this
point just around there which is coming
from the bilinear interpolation or
nearest neighbor interpolation that
matlab's doing to rotate the digit so we
could be quite happy that if I take a
prototype 6 okay and I've rotated it all
the way around but we've got a sort of
one dimensional manifold living in this
high dimensional space if we believe in
that model of 60s that it's one
prototypes it and it's being rotated now
of course in practice real digits can't
be rotated all that way so we might have
something a little bit more like this
where we've got prototype 6 which is
being rotated for some portion and then
over the other side it's a prototype
nine so that's just a cheap way of
trying to introduce the idea that
perhaps the data is typically lives as
prototypes in a high dimensional space
on low dimensional embeddings now in
practice of course rotation of a simpler
of a digit like that that's just in case
of Demeter's what rotation of that digit
like that is just far too simple for a
model of a digit in practice of course
digits undergo thinning translation and
rotation
various other transformations but what
I'm going to argue is that the date of
the structure and by that I mean sort of
most data we're interested in if we're
interesting the dataset we typically
believe it has some sort of structure we
can extract for data with structure we
expect fewer distortions and dimensions
for high-dimensional data we therefore
expect the data to live on a sort of
lower dimensional manifold and that's
the motivation for the work that follows
so the conclusion is to deal with high
dimensional data by looking for a lower
dimensional nonlinear embedding now I'm
not obviously the first person to
suggest this so here i just want to
mention briefly spectral approaches are
obviously important for google spectral
approaches to this of the problem
classical multidimensional scaling is a
very important one that's my favorite
reference on it my idea 79 then you're
using eigenvectors of a similarity
matrix and you're projecting onto those
eigen the dominant eigen vectors I so
map is a sort of multi-dimensional
scaling this is a sort of approximation
to geodesic system geodesic distance
it's used in multi-dimensional scaling
and there's a particular property imma
tomato but it can be viewed as a type of
multi-dimensional scaling colonel PCA is
interesting it says they're provides low
dimensional representation but actually
typically with the typical kernels that
are use it actually explodes the
dimensionality of the data so if it's
number of eigen vectors is the
dimensionality in if you use in RBF
kernel you have n eigen vectors so it's
not really a low-dimensional embedding
for typical kernels I'd like to know a
kernel where it is low dimensional
bedding that would be interesting it
also comes with a mapping so it's sort
of the same methodology MBS but it comes
with a mapping for free so that provides
a low-dimensional bedding I will a
high-dimensional a very greedy
mapping locally a linear embedding is
another interesting model that looks to
locally preserve relationships in the
low dimensional space well it depends
how you look you could say that Colonel
PCA is just a specific case of classical
model mention scaling yes but for the
Colonel's the particular kernels for lle
and a particular kernels or arm that's
my laptop for well nice a map and
elderly and also semi definite bedding
which is another really interesting
method i haven't mentioned they're very
difficult you don't get that mapping out
easily you can't just map a new point in
so in some sense I view them I've you
what's new with Carl PCA as getting the
mapping you don't with lle get a mapping
you could try and look but it's not
clear it's not an explicit mapping from
the high dimensional space of low
dimensional space so in my head and this
might just put my perspective on it in
my head Colonel PCA is the specific case
where you get a mapping out because
classical multi-dimensional scaling
existed a lot further before you could
view that for the case of positive
definite similarity measures is just to
Colonel PCA as well I think it's a bit
unfair all the substitutions you doing
that for a while so yes I would agree I
mean I'm just taking the perspective
that Colonel PCA is the case where you
have a mapping so the question is there
well if you've got a mapping you have a
way of getting a low-dimensional betting
I think that would be an interesting on
ok so there's also an iterative methods
so these are within the same family but
the sort of versions multi-dimensional
scaling where you don't solve it with a
spectral solution it reads optimization
of stress functions crossville stress is
a well-known one salmon mappings and
salmon wrappings aren't really a mapping
because that's their just type of
limited authority of MDS but the
terminology is misleading because
there's no mapping involved there and
euro scale is an interesting sort of
early machine learning community example
of visualization which took or augmented
iterative MDS techniques with a neural
network so you've got the mapping in
there as well as the MDS and that's sort
of a nips 97 okay i'm not interested in
lower i'm not going to go into the
details why I'm not interested them but
some of them will come out as we go
through the tour I'm interested in
probabilistic approaches and none of
those are probabilistic incense that i
can write down a likelihood of my data
even the model so probabilistic approach
is if we go for probabilistic approaches
then we're sort of in the realm of
probabilistic pca a probabilistic dca is
a linear method so it's not sort of as
interesting as the things we saw in the
previous slide we can also go for
density networks now these are model in
sort of did the proposed by Mac i-95
where you use important something to go
through a multi-layer perceptron I think
it's useful if I ok so latent variable
modeling I like graphical models but
really simple graphs so why is an
annotation going to be the data set and
X is going to be the latent variable now
in latent variable modeling what we're
assuming is we've observed why and we're
given and we the latent variable that is
why is dependent on is X so what we have
is some relationship between y and X so
i'm going to write that p of y given x
and then maybe some parameters W so this
is a mapping from X to Y so Y is
typically low dimensional we call it Q
dimensions and X is typically earth so
excessively low dimensional two
dimensions and why is typically high
dimensional d damage now in standard
latent variable models you place a prior
distribution over your latent variable P
of X you combine it with this P of Y
given X comma w and your marginalize to
obtain p of y given dozen then you'll
optimize over the parameters of your
mapping so this and this work here
probably stick pca what tipping and
Bishop showed
is the specific the Bible was proposed
independently by these guys but what
tipping a bishop specifically showed is
that the maximum-likelihood solution for
this model if this discern if they're
mapping from here to here is linear and
the noise on the out the output is a
spherical covariance this is PCA so they
call that probabilistic view CA now
that's nice but if you want a nonlinear
method you have to turn to something
like density network now the problem
occurs now is you end up with a prior
distribution in the latent space and a
non linear mapping to an observed data
set now that means you start perhaps
with a Gaussian prior over the X as you
do for probabilistic pca but when you
take this non linear mapping to why you
basically lose the Gaussian OT and you
can no longer marginalize X that's
intractable so what mikhay proposed is
use important sampling in the X space
you take samples from P of X and you map
them through using a multi-layer
perceptron you use and you then look at
the likelihood of those samples using
the important sampling technology to get
your posterior over X now that's nice
and it works but important sampling only
works and low dimensional spaces so
typically you could go up to some two
dimensional spaces with eggs and to get
a nice posterior distribution you have a
sample very density the generative
topographic mapping which is by chris
bishop and marcus spence and chris
williams it was very similar to this but
they used a real regularly spaced out
grid-based sample in the latent space
and an RDF network to map to why and
what they could show with that is that
was related a probabilistic
interpretation of the self-organizing
map but I see it as a sort of child of
density networks so the difficulty for
probabilistic approaches is to propagate
this probability distribution through a
nonlinear map because you can't
marginalize once you've done that and
that's what we're going to do today by
some tricks so what I'm presenting the
new model is a probabilistic nonlinear
pizza cpca has a probabilistic
interpretation that's what I tipping am
Bishop but it's very difficult to
nonlinear right so I'm going to present
a new probabilistic interpretation of
PCA
very briefly and this interpretation can
be made nonlinear and the result is
therefore a nonlinear probabilistic PCA
so I start with a bit of notation so
I've already said q is the
dimensionality of the latent space d is
the dataspace an n is number of data put
why I'm assuming is centered data it's
in the form of a design matrix so it
sort of has n rows and the columns the
latent variables again n rows q columns
and then I'm going to use a mapping
matrix the maps between them okay so the
important thing for the purposes of the
math the depths will go into the
important thing about that is that if
you see Y transpose Y that's a
covariance matrix if you see y y
transpose that's an inner product matrix
so lots of Colonel people get excited in
a product matrixes so that's important
and that's important for different
reasons so linear latent variable models
this is what we were talking about here
now what I'm going to say is represent
the data y with a low dimensional set of
latent variables X to assume or linear
relationship of this form where the
noise is spherical Gaussian so why is
just x times some matrix plus Gaussian
noise probabilistic pca I'm going to
define a linear Gaussian relationship
between these variables and data and
then take the standard latent variable
approach to find a Gaussian prior over
the latent space so there's my linear
Gaussian relationship expressed as a
probability distribution and here is my
prior over the latent space now what I
would then do is integrate out the
latent variables and I get this marginal
likelihood here which is independent
across the date of data point so it's
zero mean Gaussian with this particular
covariance structure w w transpose plus
sigma squared i now what's interesting
about that is if you look for the
maximum likelihood solution and this is
what tipping and Bishop show you can
show and the key points is basically in
this line this is just a lot of
likelihood wit now if you Q are the
first q prints
eigen vectors of n 2 minus 1 times y
transpose Y remember that's the
covariance matrix and the corresponding
eigen values are lambda Q then W this
mapping matrix spans the principal I
ghen vectors of this matrix so it's
equal to u q times L else just a
diagonal it's the eigenvalues minus this
spherical term and V is just an
arbitrary rotation and it's typically in
PCA taken to be just the unit identity
matrix which means that you end up with
right angle Peter principal components
so that's quite nice because what it's
saying is that we've got a probabilistic
model for which the solution for which
is spanning the principal subspace of
the data ok so here's the new model so
define the linear Gaussian relationship
that's exactly the same but another
latent variable approach so we're going
to be bayesian about things we're
interested in marginalizing out x + w in
effect what I'm going to argue is well
as from a bayesian perspective is
equally wrong to marginalize our X as it
is to marginalize out to not marginalize
W as it is tomorrow not marred lies eggs
so you define a Gaussian prior over the
parameters W instead of over X and then
you get a marginal likelihood of this
form which is now independent gaussians
across the data dimensions with this XX
transpose for Sigma squared I covariance
matrix so it's the jewel basically
because I've integrated across an inner
product I'm integrating on the opposite
side now the nice thing about that
people familiar with clara pca will sort
of be pre-empting this is in this case
the dual probabilistic pca maximum
likelihood solution is very strongly
related to the first one because as you
can see if i flip between the two you've
got w's becoming exes why is becoming y
transposes and they're very strongly
related so you just end up with this
solution that xu maximizing with respect
to X now so the trick here is you're
taking a latent variable model which in
the probabilistic perspective you
normally integrate our parameters and
maximize vertical out so I integrate out
lately
ables and maximize with respect to
parameters this is a new approach where
you're saying I'm going to integrate out
the mapping and now i'm going to
maximize with respect to latent
variables so the solution for the latent
variables is this set of eigenvectors
which is these eigenvectors of the inner
product matrix and again the eigenvalues
are the same now people familiar with
Colonel PCA will know there's just a
duality again but the solution for
publicity PCA which is of this form is
strongly related to the solution for
dual probabilistic PCA and indeed if
you're performing colonel pca you
substitute this in a product matrix here
with the kernel matrix and you solve
other positions of the embeddings you
can't do this one in kernel pca because
you're mapping your feature space could
be up to infinite so well if your
feature space is not infinite you can
still do this one and the problem of the
eigenvalue problem is the size of the
feature space if this eigenvalue problem
is the size of a number of data points
so statisticians are well aware of this
and they operate with this one if the
number of data is less than the number
of features so these two things are
equivalent so factor analysis in some
senses this is a way of just introducing
the model you can view factor analysis
of what I'm about to talk about the
Gaussian process latent variable model
you can do it as a Gaussian process
latent variable or if you do similar
things to what we're going to do this
eigenvalue problem doesn't sort it
doesn't hold so the key component why is
this solution this fixed point solution
easy to find where it's factor analysis
isn't easy to find it's important that
this guy here Sigma squared is diagonal
otherwise when you go through this
maximization there's a Sherman Morrison
would be formula and the Sigma square
can come out of one portion but the
diagonal term is inside and in a matrix
inner product and can't come out on its
own so that's why this is my elegant
solution for publicity CA and the reason
such a nice elegant solution for factor
analysis but you could modify factor
analysis in
where I'm going to talk about because in
a moment we're going to throw away
elegant eigenvalue problems and do nasty
things so at this point um I need to
mention a little bit about gaussian
processes so gaussian processes are
probability distributions over functions
these functions are obviously infinite
dimensional so it's difficult to specify
a distribution over something that's
infinite dimensional so the way you work
with them if you specify a prior
different distribution over
instantiations of the function to finite
gentleman directional objects so you
take n data points from the function and
you specify a distribution over that now
the problem is then is what do you say
when someone gives you a new data point
at n plus one data point but you can
show by induction that the Gaussian
process is consistent that it doesn't
matter when someone gives you a new data
points for many distributions if someone
gave you a new variable is effectively a
latent variable that you're
marginalizing over think of a discrete
example where there's an extra binary
variable the cause of distribution
suddenly changes when you add a new
variable gaussians the clerk class
doesn't change so it doesn't matter how
many data points there are out there in
the world that I haven't seen if I
haven't seen them when they marginalized
my predictions about the data points I
do see stay consistent so that's
important but once you've got that out
of the way you can just use them like
you use regular gaussians so instead of
a mean and covariance function the GPS
defined by mean function and so instead
of a mean and covariance matrix the GP
is defined by a meaning of variance
function and the main function is often
taken to be 0 this for simplicity you
can use mean functions but they're
typically taken to be 0 covariance
functions must be positive definite and
that sort of means that the class of
valid covariance functions is the same
as the class of Mercer colonel so
there's this strong parallel between
gaussian processes and kernel methods
the key difference i think i would say
between gaussian processes and kernel
methods is in kernel methods you're
often only interested in the pasta area
what we would think of in probably I'll
seeing processes as the posterior mean
function in Gaussian processes you're
often interested in the variance of the
posterior as well but propagating that
variance is often quite nasty apart from
the standard regression case and so for
that reason the algorithms are often
a bit more involved and get uglier so
zero mean Gaussian process will denote
it like this where K is the covariance
function or kernel and the linear kernel
has this matrix in a product form so not
there in a product matrix form plus a
diagonal term that's what keeps it
positive definite rather than positive
semi-definite so priors / nonlinear
functions are possible and you can
sample from them so this is a sample
from that Gaussian with this kernel with
a small diagonal term added to make it
positive definite here's an RBF kernel
sample from a Gaussian with an RBF
kernel as its covariance Colonel being
computed on this input span here so
these are all different samples each
individual different samples here's
different length scale different
variants overall scale of the colonel
increasing its side this is what we call
the MLP colonel this isn't why the this
isn't the hyperbolic tangent this is the
arc sine based kernel which has a
particular interpretation as a neural
network with infinite hidden nodes this
is like a point symmetric function which
is based on the MLP colonel so you can
do lots of different functions this is
just a bias is taking a constant and
adding a little bit of diagonal so when
this is the culmination of a few of
those plus some noise so you can get all
these different function classes out of
the Gaussian process so I'm going to
skip through that bit that's just
out in process regression so with the
dual probabilistic pca what I'd like you
to see here is take a look at this guy
here given that definition I just said
that a Gaussian process is just the
Gaussian function with this particular
covariance inspection of the marginal
likelihood shows that this is just a
product of Gaussian processes across d
different dimensions independent
Gaussian processes so we recognize that
kernel is the linear kernel whether case
we're talking about and we call models
of this class Gaussian process latent
variable models okay so if we take the
RBF kernel we can't we can substitute
this in but it's no longer possible to
optimize with respect to X
eigenvalue problem because that
eigenvalue problem i showed you earlier
reliant on that factorize ability of the
kernel matrix plus a constant diagonal
as the question there as we talked about
even if you turn that into a
non-constant diagonal it's no longer an
eigenvalue problem so instead we're
going to find gradients with respect to
X alpha L and Sigma squared and optimize
using conjugate gradients so notice not
just with respect to X but all the
parameters of the colonel however many
parameters we've got in our kernel will
be optimizing with respect to those as
well so there's no setting of carol
krabit ok so i want to show you example
of now I cut some slides so I hope
there's not a discontinuity because I
cut some slides at this point which
means i have to sort of spend a bit of
time explaining what's going on in the
example so this is a siggraph paper that
use this model modeling human motion
data so these white dots moving around
here this paper i should say is by keith
prowse steve martin aaron hurts millen
Zoran Popovich it was a couple of years
ago at siggraph now what you see here is
the latent space I'll talk about the
great scales in a moment what you see
here is the data space now you see these
white points are points that are being
input to the model and projected back
into the latest wait so if the man moves
you see the projection back into the
latent space now markers have
disappeared and we'll play it again in a
second so I can explain what happens
with the markets disappearing so markers
disappear once the pink man appears so
what's going on with the mark of
disappearing is that this is a
visualization of this data with white
indicating the precision of the
posterior mapping so in white areas it's
confident about what goes on in this
data space in black areas there's a
higher variance because we've got a
posterior distribution over functions
there so what he does is he takes
markers off the arm and then off the
upper body and then he's filling in
those remaining markers using this
latent variable model so the mapping
from X to Y he's got the x position
given this small
upset of the market he's asking okay now
give you the Y position and i think the
nice thing about this example is it as
far as the models concerned it's just a
missing data problem but as far as we're
concerned this is a very very high
dimensional data set with which you're
very familiar with and you can tell
whether this is matt for work or not and
he's only now providing i think six
markers which are equivalent to three
dimensions each so 18 dimensions or
something of the true 100 dimensional
data set and it's doing a really nice
job of reconstructing the data and it
can do that because it's a probabilistic
model and it's just treating the data as
missing data which is something that's
very difficult for non probabilistic
models to do as easily once you've got
this model it's just that it's just a
matter of turning the handle the
standard publicity handle for missing
data so the other example I'd like to
show you was an ictv paper there's
actually nice of what they do later work
which is even nice of them is Raquel or
tues and David fleet a Pascal flora and
Aaron hertzmann so this is tracking
using the model so they've got an image
and they're imposing what you see on the
image is imposing the three-dimensional
track of the body as you see a turn to
see she skates towards the field of view
the front field of view but the model
can't deal with that because that's out
of the plane so it's basically taking an
image using a prior model of walking
taken from a completely different
sequence in fact the mocap sequence and
using it to 3d reconstruct from the
image this is a golf swing exam now the
interesting thing about these examples
is that this model is trained on i think
ur approximately 30 frames of data so
it's 30 frames of a different walker or
30 frames of a different golfer and in
fact this drive swing i think is coming
from the same model as the the different
model sorry a different sort of type of
Swing then the original data was taken
from so that's a reasonably impressive
and in fact these aren't the sort of
things i would have tried to do with the
model because I wouldn't afford it would
do a great job there's obviously lots of
other important things about when you're
doing that about how you do the
optimizations and so forth and I refer
you to their papers
to have a look at that so I was sort of
impressed with these examples but one
thing concerns me and that thing was
that in motion-capture examples I'm not
sure if anyone's familiar with motion
capture data but you actually have
constraints on the locations of these
points given a skeleton structure so
it's not just simply a matter of points
in a 3d cloud so to my mind well the
question in my mind was what was
actually controlling the naturalness of
the motion was it the fact that there
are constraints was the model just doing
a sort of a half-assed job of modeling
where the points were but then the
constraints are taking over and doing
the rest so what I interested me was
well let's I mean this isn't the way one
should do graphics but you should just
throw away all those constraints and
take a three dimensional point cloud so
that's what you'll see in this data set
now and this is just a unpublished demo
data that you can download so this is
like a guy I found online who's running
and what you see in the point now are
actual 3d points rather than angles
being projected into 3d space so what I
want to try and do is model that with
the GPL vm and then show that the latent
space that the GPL vm comes up with
gives me reasonable things certainly
close to the data I mean I don't expect
it to be giving me something reasonable
away from the data so that's exactly
what this model is and this also allows
us to move into some sort of extensions
and mentioned briefly some things about
dimensional reduction so what I have
here is the latent space now you might
we'll talk about why there are gaps in
blatant space because the first thing
you might notice is you expect this to
be a continuous sequence and indeed and
parts you can see continuous sequences
but there are jumps and we'll see where
those jumps aren't alone they're just x1
and x2 in this thing here so they're
just a two to make sure we're taking Q
equals to be too and that's the only
thing we're giving them all actually
give it an initialization as well which
in this case is pca and then
we optimize X which is we told it to be
a two-dimensional thing and this is
where it puts them they're not
meaningful in terms of I can rotate this
solution and it's the same likelihood or
I can mirror it in both ways and it's
always the same likelihood the only
thing that's actually meaningful because
of the RBF kernel that's being used in
this case is the endpoint distances so
one way of viewing this is that you've
got a data set with an RBF kernel and
you've got some observed data points and
you're trying to match the covariance
coming from the RBF kernel to the
observed in a product matrix the sort of
empirical and product and a polyp matrix
from the data the way you're trying to
do that is by moving these points around
in the latest space and then this is at
convergence where those points end up so
what i can do is because I've got a
mapping I considered and that MATLAB
isn't really designed for graphic so you
get his point hanging around I can move
around and show you okay so here he's
clearly doing a pace of the rock and in
fact here is the start off position of
the run here is another piece of the Run
which is aligned with that one but at a
different angle and the same down here
so in both cases he's running but he's
running at a different angle so it's
basically taking the data and aligned
the positions of the run but some of
them are different angles so he's put
them alongside each other but you might
not like some aspects of this because
most dimensional reduction methods in
fact all the ones before I started
listing probabilistic ones they when
they do dimensional reduction look for
something specific they say a good
visualization is one where if two points
are close in the observed data space
they are close in the observed latent
space now clearly these two points are
very close in the observed data space
and they are very distant in the
observed lately space so what's going on
well the GPL vm isn't looking to do that
the GPR vm is looking for a smooth
mapping in the opposite direction so it
says if two points are close in layton
space they must be close
in data space but that does not mean if
two points are close in data space but
they must be close in layton lace in
fact it means if you point so far in
data space where files are a loose term
they will be far in layton space so
that's what means that the mappings
always smooth so why does it go wrong
well the topology here is wrong this run
is so the cyclic run with changing angle
so what it can't do because of the
conserved length scale of the RV f
function is it can't decide to model
this as a ring and capture the angle and
since it's not constrained it gets
higher likelihood solutions if it keeps
these things together but it's not
constrained to keep these things
together since it's not constrained to
keep these things together it chooses to
split them apart yes if you do in fact
as an example online where do this
experiment with three dimensional latent
spaces and this problem doesn't occur
draws it as a cylinder in a
three-dimensional space with the angle
of run going down the cylinder and the
pace of the run going around the axis
there but in some sense this is a
purposeful example to show the model in
a failure mode and it's also I think
it's interesting it's enlightening about
I mean it's interesting that we do spend
most of our time working with them
actual reduction that doesn't exhibit
this characteristic because if um if you
think of a ball of string I've got a one
dimensional space and I wind it around
around around like a ball of string now
embedded in the three-dimensional space
if that string is infinitely narrow some
of those points in the three-dimensional
space will actually map to the same
position to different positions in the
one-dimensional layton space because of
a loop so you can't actually specify how
the ball is wound with a mapping from
data space to layton space the only way
you can specify how the ball is wound is
with a mapping from layton space to data
space now the problem then is if you've
got the wrong topology you won't
necessarily be able to capture all the
constraints and output so why do we
always end up looking for mappings or
preserving distances in the data space
in the latest wait well because
typically those solutions are concave
the solution of where if you want to do
the solution in the other way so think
of the ball
ring with a crossing loop I've got a
point on my latent space here x which is
mapping out to the three dimensional
space to this point in my hand if I'm go
all the way around there it Maps back to
the same point again so is that point
around the loop once or is it allowed at
the other side there's lots and lots of
local minima and you you can't decide
where it is as a result and that's a
problem for models like this try and do
this there are massive multimodality in
the latent space but in a lot of cases
what we can do is try and deal with that
multi-modality by putting constraints on
the model that we're interested in to
try and smooth out this layton space and
really that's what's gonna I'm going to
talk about next I'm not sure I'm due
time 50 minutes of short on timing ok so
um this is actually here drawn in the
sequence as it goes round yes it does
something roughly cyclic ending up here
and going round and round but it's very
doing these breaks so that's really what
I've just talked about so I'm not going
to talk about that again what I'm going
to say is what the modification is to
fix this up so because we're actually
optimizing this weird thing of in x-rays
what we can do is we can sort of be
inspired by low and tipping with their
no scale algorithm I mentioned at the
beginning and say well each point in
layton space could be a smooth mapping
from data space and by making it a
smooth parametrized mapping constraining
it in effect we can then optimize over
the parameters of that mapping instead
of optimizing over X directly so keen
Canyon gyro candela and I had paper on
this icml last year and we called it
back constraint so we can use any smooth
function neural network RBF network or
kernel-based mappings logistic without
considering the variants and tried all
of these and they all work so really
reasonably nicely so normally the GPL vm
proceeds by maximizing this likelihood
with respect to X using gradients of
likelihood with respect to X now by
putting these back constraints in we
looked optimized with respect to beware
via parameters and we can compute be the
gradient of the likelihood with respect
to B via the chain rule
and optimize the parameters that mapping
instead so back to the example so this
is the same model but with an RBF kernel
based back constraint so let's move
reverse mapping so now what we see is
that you can see the cyclic don't know
why I can't let me try and meet this
smaller keep the man big okay so as I go
around this cycle here you now see he
goes through the run still it's
capturing the some extent the change in
angle it isn't going to be capturing it
as well because if it were able to
capture it as well like this then the
last solution would have gone there
because the initialization is actually
quite close to a cycle the dca
initialization is quite close to a cycle
so it's not like that that's a local
minima to be away from this cycle it's
actually the global minima I've tried
this has a lower likelihood as a
solution than the other one so there he
is running away quite nicely and an
interesting thing about that is that you
can see on this plot here why one reason
why I doesn't want to do that sort of
thing so this is like in a group distr
distortion on old long-playing records
as it gets towards the center it's got
less length to represent the same cycle
of run so it doesn't want to do that so
it tries to flatten everything out into
a sort of square type shape so here it
actually manages to keep the cycle
around the same length by having it like
a cylinder a projection of a cylinder
it's like an isometric projection of a
cylinder what we've seen but of course
then it has to cross at some point so
these these these three points
approximately where it's crossing here
aren't the same in the real data and
it's now sacrificing quality of modeling
of those three points to keep the back
constraints keep the smooth mapping and
now we've got a bio jective what we hope
it's by ejecting there's no guarantee
but you've encouraged it to create a by
jective mapping between the two spaces
why is no guarantee it's dire jective
because we're only imposingly
constraints at these points here and if
that mapping isn't properly regularize
between those points these two mappings
might not match up very well
okay so that's in case the demo didn't
work Oh mommy I'm sorry i'ma fat I
should have showed you that oh I should
show that at the end yeah in fact I
should have showed you that because I
wanted to I wanted to help yeah forgot
about that so another thing you can do
with this model because we've got here Y
given X something people are interested
in if you've got dynamic constraints on
what goes on in this model though you
expect those to dynamically close lately
but to temporarily close latent
variables to be close together and so
calm his first name but Wang and fleet
and hurts one suggested using Gaussian
process dynamics now in this case you
predict where you're going to go next
with the Gaussian process now this is
not Markov dynamics I'm not going to go
into why it's not Markoff dynamics just
now but we can talk about it afterwards
if anything's interested so you make a
prediction where you are going to be at
the next time frame the reason it's hot
market dynamics is a little bit involved
so I'll explain that to anyone
afterwards if they're interested so it's
not a regressive process so in that case
what we can do wrong there we go these
have another look at the visualization
so now because this dynamics the way I
think of this dynamics is like a wind
field it's like within this X face it's
telling you where you're going next so
in this case it's a cyclone or an
anticyclone anyway it's blowing it
around the middle round in circles so
what you can do now is show you can show
the same sort of effect that you get the
cycle and you can also run the dynamics
forward so here we're just taking
predictions from the dynamics and the
point-to-point basis and you actually
get a nice run until he gets to the
center where he just Explorer and
smaller steps so that's good in the area
outside the data it's good actually in
this case but this isn't
example I would wanted to show you
because he's pretty normal in inside
where the data is it's hard to find
something where you can immediately say
oh there's something wrong although in
the other example I'll show you we can
see that he does go wrong so that's
that's quite a nice model and what
they've used this for is tracking and
they've actually used cases where you're
tracking a human who disappears for an
entire pecs and you still pick them up
when they reappear so you get full
occlusion for an entire pace and the
dynamics takes over and then you
immediately you actually get a good
prediction of where they were while they
were missing as long as they don't start
running while they were missing and then
you pick them up on the other side and
that's and cvpr paper by artisan fleet
and hertzmann which took their icv paper
and added the dynamics ok so that's
about in a groove distortion I'm not
going to talk about that I'm just going
to talk about this one final thing so
another thing we might be interested in
is building hierarchies of these models
so this sort of low-dimensional
embedding is an interesting way to
handle high dimensional data but in
probabilistic graphical models they
would sort of maybe say well the way to
handle high dimensional data is to
express conditional independence ease in
the data so say oh there's no node is
conditionally independent given this
wallet and learn a sparse graph
structure to describe the data is
completely different approach to
handling high dimensional data so what
I'm interested in here is stacking
Gaussian processes to consider more
complicated hierarchies so it's like a
fool this is a simple one link in the
graph we're just going to put all these
things together and make a creator stack
graph so if this is an example of two
subjects subject one and subject to
walking together and high-fiving and
what you're seeing here is this top
level though controlling both subjects
and then each of the individual subjects
being controlled independently because
give them you're giving your position
here you know where the subject is you
don't need to know where the other
subject is this subject becomes
independent if you observe them so
you're expressing that sort of
conditional independence the other wrong
I have with a little demo on it is this
example
here where you actually make these
conditional independence ease on the
body so sorry that green doesn't come
out very well but these are latent
variables and these the observed angles
of different portions of the body and
this is very recent work and I am kind
of interested in it because I think it's
two things I think this is interesting
for so this is an example of a run and a
walk in the same model why is this
interesting is interesting because first
of all I have low level control of the
different components of the structure
I've gotta run on a wart which are quite
similar here but I could have a wave as
part of the model and then I can have
you walking with waving because I don't
want to have to learn a run with a walk
with a wave and a walk with the
scratching their head and all these
other things I'd rather be able to
separate how I'm learning these things I
haven't really done that here because
this is just an early prototype example
but what we've got is the ability to do
a walk now i press this thing here i run
through the cycle of the war and you can
see the green points are all associated
with the war it doesn't know those
labels I've just colored them that way
for visualization and then similarly
I've got the run it runs a little bit
slower because I'm sort of the limits of
what matlab seems to be able to do on my
laptop animation wise mapping each of
these is a Gaussian process mapping and
I can just sort of work on the abdomen
directly so you can see that's what the
abdomen does across the cycle or I can
work with the legs directly from the war
or I can just take one arm and wave it
back and forth so it's a decomposition
of them all now you'll notice on all
these examples I've used very few data
points that is the domain where this
model just destroys anything some of
these examples we've seen very
high-dimensional data sets hundred
dimensions library you guys used to more
of them have butts a hundred dimensions
of data with 55 data points and it's
been in fact that all those a man
walking examples of be met and it's
never done anything weird and outside
the more normally you would think that's
linear model thread territory but these
are very nonlinear models to do this and
to do it well now you can also do large
datasets I haven't talked about that
today
I think they are still interesting model
for large data sets but there's a lot of
issues because of n cubed complexity in
gaussian processes we've got that down
two ND squared where D is a hand-tuned
sparsity parameter not d because that's
the dimension of the SI k where k is the
hand-tuned sparsity parameter which
could be seen like as the number of
support vectors at NECN but you have to
sell it by hand and I think that's about
everything i wanted to show it's just
get back to the talk and that's just in
case that demo didn't work so in summary
the GPL vm is a probabilistic nonlinear
generalization of PCA it works
effectively as a probabilistic modeling
high dimensional spaces even when there
are less data points than there are high
dimensions these back constraints force
these local distance reservations that
are present in most dimensionality
reduction techniques and dynamics can be
introduced and there's a hierarchy and
that I'm finally doesn't exist anymore
so and that's it so we have you already
have one of requests which means i have
to run through the demos a little bit
you're not going to see them off again
Yeah right over there onto I like yes
very good and so let me get this right
and then I think yeah so we'll see this
is the unconstrained case lime spice I'm
supposed to be showing this because I'm
supposed to show you funky behavior to
kill you so let's see I need to get him
with the right angle I think the thing
to watch is his feet okay well there you
go there's hey mr. funky so see there's
no constraints telling him he can't be
there all smooth moves smooth
distortions away and the data is not
telling him anything there so actually
what the Barbie echeverria's is doing is
dropping back towards for me so it's
controlled by the data or wherever it is
but as you move away from the data
typically drops back towards me the MLP
covariance I showed you earlier doesn't
do that and you will get
we at distortions with the MLP
covariance but yeah I think one time I
mean the problem with developing these
demos is you just end up playing with
them okay was there a question there are
the observed data that your value to
people are the three dimensional
location of the actual point yeah each
um each point you see on the screen is
got 3d associated with it so each blog
you see on the screen has an XYZ lo que
location and I think there's a 34 of
them so it's a hundred and three
dimensions to what everybody's and the
55 data point subsamples mocap is
typically at 120 frames and we've
downsampled to 30 frames a second to do
this because you don't need apart from
the high fiving thing but one of the
guys who really hits the other guy
that's the only time I've seen where 120
frame a second sample rate is actually
necessary for the model to see what's
going on because he moves a lot quicker
across that and then they're slap causes
a rebound so I'm not even sure 120
frames was fast enough there but in most
of walking and running cases 30 frames
per second example for this film again
here we have the Ironmen oh yes in the
structure I can't do and that would be
really nice but yeah so for the human
motion case you can think of obvious
types of structures you might want to
impose but you're right most data you
don't have that sort of information and
learning any structure like this would
be a nightmare in fact in this case even
learning the X variables you have to be
a bit careful because you're doing a map
solution to X variables across multiple
levels I have sort of reasons why I
think that's all right and you don't
over fit which are primarily dependent
on the fact you've got a temple prior on
the top and as long as your node models
aren't overfitting and you've got this
temple fire on the top and one thing I
didn't
actually if I constrain the Gaussian
processes to be low noise in intervening
levels it seems to do a really nice job
if you allow the dowsing posters in the
intervening levels to have noise then it
some sort of semester you see we don't
have at all of them when you don't with
angles you do I guess you probably it's
right to in this case but with angles
you don't and what you can do there is
and what the sort of modification that
was suggested in the style based ik
paper for that reason it's quite simple
is to actually associate each kernel
with an additional parameter which is a
scale on that kernel and the variant
jointly so an overall scale so the
signal-to-noise ratio doesn't change now
you could also there's nothing wrong
with saying doing something more
advanced in theory there's nothing wrong
with doing something more advanced and
taking taking the likelihood we saw
which was independent across dimensions
and I would I mean my claim is it still
a Gaussian process latent variable model
if here Y given X is the Gaussian
process independent or not and even in
the independent case you get to this
factor analysis type situation where you
can specify a different kernel or
different scales in the kernel a problem
then arises if you start tinkering with
the colonel too much if you do an
overall scale on the colonel that's fine
and that's what these guys did if you
start allowing different noise signal to
noise ratios where that's the signal and
that's the noise then you need to
compute the posterior colonel separately
for every dimension so one thing I
didn't say is the construction of this
model as it is means the posterior
variance is identical for all dimensions
which is slightly odd thing to have but
it means i can visualize it in that
latent space and it reduces the
computation if the Sirica variance is
different for every kernel and this is
the classic people who are doing
gaussian processes have this problem all
the time if you're going to propagate
the variance you've got to store it
you've got to look after that Colonel
and if you allow it to be different in
each time
you've got a multiplied by d time
storage multiplied by d x complexity and
so on so forth in most cases the models
so strong it doesn't really need it but
I'm sure there were cases imitates we've
looked at even the angles but I'm sure
there are cases where the characteristic
so we have some classic benchmark data
set I really like to use it for text
actually but I don't know if it's useful
protects it's a already complicated
because you start to look at multinomial
output but text to speech I'd love to do
so this is the evil swiss roll and this
shows that without good initialization
it fails miserably that's like someone
sitting on the swiss roll from the top
is the local minima problem these are
some digits but that's a particular case
of linear back constraints but it's
other this is an oil flow data set
you're skimming through these to give an
idea this is a full visualization of a
thousand data points that took all night
with the full covariance function but it
does a really good visualization this is
speech data of people making our old
noises and it's being used at a system
in Washington but the two-dimensional
map is to replace a mouse for people who
have difficulty depends so you go out
when you move around that's not the
system they've got implemented at the
moment but it's something they're
looking at and that's the map of the
different valves this is a robot
localization example which was at I jci
I this year where you've got a robot
that goes round and loop read Wi-Fi
signals and try to decide where it must
be in the two-dimensional space is
actually a real map in that plate and
issue digits I said there were no other
digits there are it is just a missing
data with handwritten usps digits so you
so it does yeah I mean really the thing
is it hasn't
it's interesting it's taken off in a big
way in vision and graphics MIT the talk
was really crowded and there were the
machine learning people the vision and
the graphics people there because that's
where they know where things are but I
think that I mean the model is powerful
anyway it dangerous so what's in my head
about it is I mean like you guys know
about this thing of increasing data and
power performance keeps going up and up
with increasing data and speech people
talk about that as well so you've got
evil Gaussian mixture models find a
model high dimensional space and they
don't do it very well but the more data
you give them better they do it and the
more you know the people who get the
best performing speech recognizers have
the most data and if you get the best
performing translation systems however
the most data so I'm hoping that with
models like this you can try and push
back that trend because the reason that
trend that exists to my belief is that a
lot of those data models are modeling
high dimensional data correctly and in
reality humans of course don't need that
much speech data nowhere near as much as
the best performing speech recognizers
to learn to speak so if you can start
using different types of models well
there's a child children children
remember so maybe what one hope or you
know maybe a desperate hope is that yes
there are computational issues with this
model even with the sparse approximation
but is it the case that this is swinging
against current patterns of machine
learning is it the case that you can
take smaller data sets and what is the
performance improvement curve likely to
increase the data set is the improvement
in the quality of the model mean that
you get ahead of the large data set to
eat the same computational power because
you can do something cool with this with
55 data points I mean those human motion
modeling no one really ever managed to
dig out some next two models of those at
all that's why the models are being
successful there but am I the other
thing I'm kind of very interested in it
is ideas for how you can take an
algorithm like this which is quite I
think it's difficult to paralyze and how
you can distribute it across multiple
issues because it's not a great one for
value
we're very end thank you so um text
people have visualized it I don't know I
mean I'm not haven't thought that much
about tears to be top confident about
sort of assertions I make with smooth
manifolds and text as you might be with
other things you definitely need I mean
you need start needing some pretty nasty
stuff to do multinomial outputs on this
model so I'm thinking but language
models are certainly models suffer from
poverty of data and require large
amounts of data yet it's not something
we actually suffer with and there is
people look at things like clustering
concepts and so I was it was so to me
the similarities between that and
reduced dimensionality spaces I don't
think I wouldn't claim that this model
is close to being able to compete with
the top approaches that because I think
it needs a lot of engineering to get the
representation like this example is only
noise models on the digits on the
right-hand side you can do the screen
but of course discrete text has other
interesting issues like massive sparsity
which isn't the case for this but you
see you can certainly do discreet but
you're not going to be doing few large
later said well dimensional reduction in
dates in text data I think he's an
interesting area they had this paper
again I get from the perspective Google
quite Mickey Mouse the stochastic
neighbor embedding where they were
trying to show that you could read text
concepts together in multiple places
they were trying to reflect the gpo
beyond has the right characteristic of
this Bank task everyone talked about
banks or on rivers and banks are
financial institutions so they mapped
multiple places latent space well that's
exactly the same thing as a two-point
crossing the ball of string the
later when you unwind the string they're
in different places but in the
observation space they're in the same
place and what tells you in different
places is things like context and so and
so forth and this model has all that
capability it does that but whether it
does what you can do with the model it
doesn't have that capability not at the
moment and probably nowhere near but in
yeah all right well two dimensional
point if you gave it two points it would
just give you like an interpolation
between it would entail it's difficult
in fact you can't really do two points
because the whole thing's rely on
distances so although it's a two
dimensional space you're thinking of
fitting lines that's not the right quite
way to think that because what's in the
RBF kernel with two points you basically
have to distance you can vary two
parameters in the colonel so you would
have to fix them at some distance apart
because otherwise it can't resolve
because there's no other point but it
would just dumb give you an
interpolation between the two points
basically
oh yeah you can put that pretty much we
can do in fact so Mitch when I first did
this paper I realized that this was
going to be very slow and realized that
nips don't like it if you show data sets
of like 60 days points traditionally
knit people one very large data points
so what I did was I had a sparse
ification algorithm which was designed
to be a Gaussian process like thing of
the HBM and it does that very well but
it's an active site selection algorithm
and it just selects an active set and
decides to work on that now active
sexually it's very good in
classification in regression it's
typically not as good and this thing is
basically a massive regression systems
so white basic what my system did was
take a subset of the data set that was
laid out across the latent space
subsampling it from 3,000 points to 100
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>