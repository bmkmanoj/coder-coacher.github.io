<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>An Overview of High Performance Computing and Challenges for the Future | Coder Coacher - Coaching Coders</title><meta content="An Overview of High Performance Computing and Challenges for the Future - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>An Overview of High Performance Computing and Challenges for the Future</b></h2><h5 class="post__date">2008-01-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zTIKUxO9kf4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">introduce dr. Jack donger his a
professor distinguished professor at the
University of Tennessee his
distinguished researcher at Oak Ridge
National Lab his Turing Fellow at
University of Manchester and many other
things his fellow of I Triple E ACM yes
and many other things he's been involved
in many things through his career his
main areas of expertise are linear
algebra and high-performance computing
tools used for that and everything else
and he's also director of ICL which is
innovative computing laboratory at the
university of tennessee which is the
most fun place to work at after google
and i really mean it and like he'll talk
today to us about future directions in
high-performance computing and discuss a
little bit about multi-course and
challenges that brought to this field
well thank you very much
it's a pleasure to be here Jelena was
was a student at the University of
Tennessee and spent how many years there
four years their degree so I'd like to
talk about some things going on in
high-performance computing that's sort
of the space where I live it has a lot
of overlap in the space where you live
but will you'll be able to see some very
distinct differences I'd like to keep
this informal so if you have a question
you should ask it during the talk it'll
make it more fun for me and for you in
the next 40 45 minutes okay so let's see
I'm at the University of Tennessee
that's mainly where I sit and work I
also work occasionally at Oak Ridge
National Lab it's about 30 miles from
the Knoxville campus where where the
university is and I spend my summers or
at least a month of my summers in rainy
Manchester where I enjoy the weather
perhaps so here's here's a few things
I'd like to do I'd like to talk about
high performance computing and we're
going to do that through the eyes of
some data that we've collected and that
data is called the top500 I'll explain
what that is in a moment and as I look
at things from the area of my research
they're really for sort of things that
stand out for
for concepts that really affect the
software that I deal with my software is
mainly in the area of mathematical
software so I'm looking at solving
systems of equations and eigen value
problems that's the kind of thing I
would say I really do and we develop
algorithms and software for
high-performance computing in that space
and when I look at the challenges there
are really four major challenges today
for that software the first one is the
effective use of many core many core is
going to change everything is gonna be a
very disruptive technology and it's
going to cause all of our software to be
rewritten to effectively use that that
technology the second thing is there's
some intriguing aspects of exploiting
multiple levels of precision so we
normally deal with 64-bit floating-point
arithmetic that's our normal way of
doing business but when we look at the
architectures and look at the machines
that are being used today we see that we
can mix if we can exploit lower
precision we can get much higher
performance so the goal here is to use
some kind of mixed calculation using
things in lower precision 32-bit
arithmetic maybe even 16-bit
floating-point arithmetic and then do
something to enhance the performance to
really get 64-bit arithmetic in the end
so get the performance of the of the of
the lower precision but somehow do some
mathematical tricks to extract the
higher precision in the end getting a
much lower execution time the third
thing which is sort of critical it's
critical and fundamental will be the
ability for our software to adapt to the
architectures that it will run on and do
that automatically so you can think of
this as a static thing maybe you do it
at compile time it tries to figure out
the architecture and do something so be
efficient in that space so one of the
things we did a few years ago was to
implement a matrix multiply so when you
compile but say matrix multiply and
built it on your system in addition to
compiling the code it actually went out
and carried out a series of experiments
a series of experiments in terms of
in those three loops in a way that
effectively used the architecture
effectively used a cache that was there
did some blocking and unrolling some
jamming and some software pipelining to
fully exploit the the architecture and
the software would generate a set of
experiments maybe a thousand versions of
matrix multiply with different different
settings to try to capture the best
performance and it would run those and
at the end of that experiment has picked
the best one and then use that in the
implementation so in the make process it
would it would do that empirical set of
experiments and then generate the right
code or we can think of another way of
Auto tuning is by looking at the data
probing the data and determining which
method might be the best for whatever
problem we're trying to solve on the
data dynamically as we get the data so
those are critical things for for the
future and the other thing is fault
tolerance you know we're talking about
systems that today have and a quarter a
quarter million core in them so machines
have a quarter million cores and those
systems undergoing some kind of failure
during the course of the application
running so in the course of a four hour
run something may fail the probability
is pretty high that it will how do how
can we accommodate that failure without
doing something like a checkpoint and
restart to effectively carry on
throughout the course of our application
run so there's a lot of interesting
ideas there in terms of algorithmic
fault tolerance having the algorithm
adjust to to the failure and carry on in
the presence of that our programming
model doesn't allow it today so we live
in a world where we program in C and
implement our algorithms using message
passing through an MPI framework the
message passing interface framework and
the MPI has no mechanism for failure if
something if something dies the whole
application falls over and has to be
restarted so those things are critical
to to allow us it will be critical to
allow the applications to run in in the
environments that we see in the future
okay so first off top 500 so I think
this is a list of the 500 fastest
computers this list is generated every
six months and it's done by four people
so a colleague of mine in Germany Hans
Moyer horse Simon who works up the bay
here at the Lawrence Berkeley Lab and
Eric Strohmeyer also at the LBNL and I
get together and put together a list of
the 500 fastest machines it's done twice
a year it's done in November and in June
and for better or worse the way we we
judge this is through a benchmark and I
don't want to argue here that this is a
good idea or not but the benchmark
that's used is something called the
Linpack benchmark Linpack is a package
of mathematical software which was
developed about 20 years ago and the the
thing that's retained here is the
problem that we solve the problem that
we solve is this thing here it's a
system of equations ax equals b so we're
asking the machine to solve that problem
and we're gonna measure the performance
when it solves that problem okay and the
ground rules say something like this you
can use any method you want to solve
that problem you have to get the right
answer right in some sense of the
accuracy that that's there and you have
to do the calculation in a certain way
we're sort of vague about that and I
could be much more specific and you have
to use my matrix the matrix that I give
you to solve the problem and you can
make the matrix as large as you want
however so the idea is if you have a big
machine you fill it up with a matrix and
you solve the problem you measure the
time and then you report back a rate of
execution and the rate of execution is
then what we use to rank your machine so
think of it like this as I vary the size
of the problem that we're solving that
problem on your machine the rate of
execution will rise up to some point and
we're looking for that asymptotic rate
so the rate of execution is operations
per second right that's what we're
looking at and the operations that we're
talking about here are floating-point
operations and in multiplication those
are the operations we're going to count
and we're talking about
64-bit floating-point operations so
64-bit floating-point operations to
solve that problem if you found that
problem there are two thirds and cubed
operations and this the size of the
matrix okay so this list is updated
twice a year it's done at a meeting
that's held in November a supercomputing
meeting that's held in November this
year it will be held in Austin Texas and
then also a meeting that's held in
Dresden Germany in June if you're
interested there's a website which
contains all the information you want so
in terms of high-performance computing
this is us this is sort of a chart which
looks at the history of high performance
computing against that data so we've
been doing this since 1993 and the way
to read this chart so we have years and
we have rates of execution the way to
read this chart is this this this curve
here plots the machine at position
number one so that's the fastest
computer so today the fastest computer
is at 478 teraflops 478 trillion
floating-point operations per second
64-bit and and multiplies of what we're
counting this curve here is plotting the
guy at the bottom of the list so that's
the machine at position number 500 so it
just made it on the list right so that's
at about six teraflops of computing
power and this green line here is
plotting the sum of the 500 machines so
that's an artificial number right but
it's already gives you a snapshot or a
picture of where computing is going so
we see a few things you know things are
rising all the time that's a good sign
this this machine here the fastest
machine is called the IBM Blue Gene
that's a machine across the bay at
Livermore and I'll say more about that
machine in a moment before this machine
was number one this is this was the
number one machine something called the
NEC earth simulator so that was a big
deal so it was a Japanese machine and it
became the fastest machine so that
caused a bit of a stir in terms of
high-performance computing in the States
because high-performance computing or
supercomputing is thought to be some
kind of strategic technique
and another country had a faster machine
than we than we have that was a big deal
you know there's a lot of interesting
things about this chart the slope of the
line so what's the slope of the line
what should the slope of the line be now
we know something about Moore's Law
Moore's law says we're going to double
every 18 months
so Moore's law talks about gates we
loosely translate that into computing
power and we're applying it in this
context here so the slope is actually
improving faster than Moore's law it's
is proving doubling every 14 months so
why is that it's because of parallel
processing so we're using more and more
processors every time in our machines
and we get a little edge because of that
use of parallel product moore's law
talks about a chip and we're using
multiple chips and we're using more and
more chips in our machine so many
interesting things so the guy at the
bottom of the list here this year at 6
teraflops that's that was equal to all
the computers in the top 500 list not
too long ago right so in 1996 all the
machines on the top 500 list you know it
takes about six to eight years going
from number one to falling off the list
right which is sort of an interesting
thing the list has a half-life about
half of the machines fall off the list
every six months which is a switch is a
striking striking number so a lot of
change going on here in this list you
know the other thing that I like to look
at is my laptop so I have a IBM ThinkPad
here it's got a dual-core and if I I
could run the benchmark on my machine
and if I run it on the machine actually
using one core one core the laptop goes
about three gigaflops so I'm stunned at
that number you know a few years ago if
you told me I was gonna have a machine
that would run at three gigaflops and I
would use it to read my email and do
something on an airplane that would be a
joke right but today I have that machine
and you know a few years ago that would
have been on the top 500 list again a
pretty stunning stunning thing going on
so again 1995 that would have made the
with its single core processor this is
the top 10 machine so we're looking at
the top of the list the way to read this
is machine number one is manufactured by
IBM it's called a blue gene/l it's a
dual core machine and each core is at
0.7 gigahertz so the first thing is
point 7 that's pretty low low power low
power and its speed was 478 teraflops
478 teraflops in the benchmark run so
that's an actually achieved rate of
execution it's at Livermore across the
bay
it's run by do e and it was put in place
or modified in 2007 so they added some
cabinets to the machine I'll show you a
better image of the architecture in a
moment and it has custom architecture so
this was this is a machine where you
cannot go out and buy the processor and
you cannot go out and buy the
interconnect if you could buy both of
those things I call that commodity and
loosely speaking that's a cluster if you
can buy the processor off the street but
the interconnect is proprietary I call
that a hybrid system so as we look down
this list here we see we see a number of
those in each of those camps and then
this is the number of core so this has
two hundred and twelve thousand core in
it right so it's a big honkin machine
sitting there the green things are new
machines or updates to those machines
things that happened in this last list
so in November that we have a lot of
changes and if you scan down this list
here a few things sort of stand out if
you take a look at the number of core
lots of core in these machine right so
lots of computing power nothing under
10,000 right in the top 10 take a look
at the countries so we see a few
interesting things here it's not unusual
to have other countries but this is the
first time that India has appeared in
the top 10 so there's a company in India
of this company Tata which is a very
large company in India
they have like about three percent of
the gross domestic product in Indiana is
held by that company tremendous and very
diversified and they have a computer
research lab and they bought a machine
from HP to do high performance computing
they're also won another surprise was
this machine in Sweden so Sweden has an
organization a government organization
similar to the NSA in the States for
doing something and they bought a high
performance machine recently again from
HP so those machines are clusters
they're commodity parts commodity
processors and a commodity interconnect
I think it's InfiniBand in that and
those machines so a lot of a lot of
power a lot of countries in the list
Germany has a very large blue gene P the
next generation of blue gene again very
low powered 0.85 gigahertz this is that
machine at Livermore across the bay it's
interesting machine in a number of ways
IBM came up with this blue gene
architecture so this is one of the chips
it's an ASIC that blue this comes from
an ASIC that iBM has originally it was
to be an embedded processor who's going
to be in refrigerators or something and
they modified it they modified it so
that it has a dual core there's two two
processors there and on the chip itself
as some of the interconnect so there's a
tree and a torus based interconnect for
this machine and sew both interconnects
are in the architecture and the part of
that is embedded inside the prot inside
the chip itself so there's two two core
on a chip and that gives about 5.8
gigaflops peak performance for that chip
and they have a card which has two of
those chips on it so they've got four
core on a card they take that card and
they populate a node board with that
card so that we put 16 of those cards on
the board and that gives 64 64
processors on a card they take that card
and they put it into a rack they put 32
of those node cards new node boards into
a rack giving rise to two thousand
processors
so that's the basic unit for this
machine as 2000 processors so if you
wanted to go out and buy this machine
they would sell you that I'd be able to
sell you that's the minimum they would
sell you and Livermore they decided to
put a bunch of those racks in a room so
they filled up a room with a 104 racks
are sitting there yes
where are the discs okay so there are
some discs on this machine but not
associated with the individual
processors so they're sitting off on the
side there's some controllers which can
talk into the machine itself and you can
you can stage things into this into this
machine but that's an interesting thing
there are no discs on the machine on the
processors themselves so checkpointing
if you were thinking about doing a
checkpoint that becomes a bit of a
struggle now to move to stage things off
and the bring things back into this
architecture so we have 2012 core the
peak performance is a little over half
of a petaflop 596 teraflops and they
achieve 498 teraflops so that's pretty
impressive in terms of the percent of
peak right that's a pretty high rate 84
percent of peak peak performance there
so that's the machine at number one
position you know the other the other
thing is the cycle time for this machine
is point eight gigahertz so low low
power is the important aspect you know
two point six megawatts is what this
machine consumes in terms of its
operation if we look at the top 500 list
today and bin the machines and look at
the number of core and those machines
there's a sweet spot so about a thousand
to 2,000 processors inside of the
machine is what we see of course we have
machines with very large number then we
just saw and then a few machines with
very small number right so everything is
parallel everything is parallel on the
top 500 list there's no single processor
machine capable of that six teraflops
entry point today and if you take a look
at the number of core of the lists you
know as the lists evolved from 1993
there was some number of processor
and today you see this this this rise
here about 1.6 million core in in all of
the top in terms of the top 500 systems
today and of course that will continue
to increase you know the other thing is
you might expect we're looking at the
500 machines here and looking at the
performance so the machines at the front
of the list have very high rates of
execution and then there's this long
long tail that we see right again the
entry point is at 5.9 teraflops 5.9
teraflops you can go out and buy a
machine which has about 1,500 processors
and 1500 in it and put it together with
Giggy and that that would qualify for
the top 500 list running the benchmark
on that so it's possible for modest cost
to enter the top 500 list and you know
half of these machines will go away in
in the next list in June so this part of
the list will be dropped off and new
machines will commence the list and the
bottom of the bottom the entry point
gets gets raised by about we can we can
predict it pretty accurately at this
point where we're going into the future
in terms of you know the processors in
these machines if we take a look and see
who what kind of processors are being
used in the 500 machines
intel has 74% of the processors and the
largest majority of that is the Pentium
64-bit 64-bit architecture there's some
Itanium processors and then older 32-bit
Pentium architecture IBM PowerPC 12
percent and then AMD about 16 percent
and then everybody else is just a just a
very small amount off the edge this is
looking at it's a pretty confusing graph
but if we just look at we're looking at
the 500 machines again and looking at
the interconnect that they use
interconnects so I find this quite
striking 270 machines use Giggy so Giggy
is a very poor interconnect right it has
very very high latency and very low
relative
bandwidth so but it's very low priced so
people are low balling the price of
their machines they're going out and
buying a a switch which really can't
keep up with the processors in some
sense for high high data movement
through the Machine Mirror net this
proprietary much faster and they have
about 18 machines on the list InfiniBand
again faster than Giggy about 121 and
that's that's a sizable share eighty-two
percent of the 82 percent of the market
is contained there so the surprising
thing is Giggy none of the machines at
the top of the list use Giggy right if
you take a look at the 50 top 50
machines on the list they're all using
InfiniBand or some proprietary
interconnect as they really have to have
much greater movement of data bandwidth
and latency are really critical for the
applications that are run on these
machines and this sort of looks at just
that point this is looking at the
efficiency of running the benchmark so
you have a benchmark that you're going
to run on the machine what's the
percentage of peak that you get if we
take a look at the machines that use
Giggy it's less than 50% of peak
performance that you see from Giggy and
some of the other interconnects deliver
much better much better ratio of that
so again low-balling the machine you can
get away with a very poor interconnect
but you're not going to be able to
recover recover the performance from
that countries if we take a look at
which countries have which machines not
surprising us has 60 percent of the
performance of the 500 machines Germany
has a 7 per 7.7% UK Japan 4.2 percent
that's a bit surprising that in Japan
it's so low I would say and they're
trying to revive that or have an
initiative for using high performance
computing as a as a key thing for the
next for the next five years France
India has just a few machines but they
have one very powerful machine which
gives them this 2.8 percent and then
Sweden and everybody else has a much
smaller much smaller share of that
of that pie you know power really is an
important aspect for running these
machines you know Google knows that very
well so recently about a year ago this
article a year and a half ago this
article appeared where you set up a
facility out in Oregon and the
interesting thing is that facility was
set up near the Columbia River in an old
aluminum smelting plant so a ready
source of cooling and power coming into
the plant
I would guess cheap power as well for
running one of your data centers so
that's that certainly is an important
issue for high-performance computing and
you know it's being felt more and more
with respect to the machines that are
used in the scientific side as well this
is looking at flops per watt for the top
20 machines top 20 machines you know
higher is better you want more flops per
power that you put into the Machine and
there's some very interesting effects
here the machines that have the best
ratio there are those machines from IBM
the blue-jean machines those are the
machines with the very low power they're
very low cycle time but lots of
processors being used yes
well so in some sense everybody's doing
the following thing they do multiple
floating-point operations per cycle
multiple floating-point operations per
cycle and in this case here they're
doing two flops per cycle per core
that's pretty standard for all of them
so the question is really why are they
getting such a higher fraction of it per
power and I guess I can't answer that
directly other than their wattage seems
to be better for for the for the
performance that they're delivering
proof of that edge some of the other
ones are not far behind but clearly this
one here is has it has an edge over over
them there's a there's a companion list
through the top 500 called the green 500
so these guys are looking at just that
metric watts per per flop and the top
the top 10 machines on the green 500 or
all IBM machines are all the IBM Blue
Gene machines either the previous
generation the L or the or the or the P
series machine and you can see what what
their consumption is and they're they're
ranking on the on the top 500 list
itself so that that becomes an important
aspect for high performance computing so
this is a chart which looks at the data
that we saw and then tries to
extrapolate tries to look at where
things are going in the fuse is a
dangerous thing right we're in a log a
log scale here extrapolating but
basically it says the fastest machine
will be at a petaflop roughly in 2008 or
2009 so we'll have a machine at a
petaflop
at that period you know the question
might be where's my laptop going in this
in this chart here and you know this
would predict I'd have a laptop at a at
a teraflop
in in 2015 now we think that's a joke
but you know there's something in the
background Lang saying you know I'll
have that I'll have that I'm not sure
what I'll do with that power just like
I'm not sure how to do with the power in
my machine today maybe bill will have
something to say about that I'm not sure
what the banner you look
like I'm not sure how many cores it'll
have but but you know I've got to
believe that teraflop will be there and
again what I'll do with that power I'm
not really sure so sorry yes
markets
that's right so I yes yeah that's right
so so there's certain certainly I
wouldn't dispute that you know if I'm
gonna just do web browsing I don't need
that power if I'm just gonna be writing
a paper or looking at the newspaper I
don't I don't need that that kind of
power and I shouldn't have to pay for it
but if the market is driving the
commodity processors in a certain
direction the cost of those things would
be dropped significantly as they are now
with my with my Pentium processor so I
can I can have a I can have something
which has tremendous power I have to say
I don't use all the power in line
machine but if I go back and try to use
a machine that's two generations old and
for me that's about a year and a half
right I get a machine every every few
months if I try to use one of those old
machines I feel terribly frustrated
because I feel it's so slow right so
there's something there that I'm
enjoying not getting the full benefit
clearly but I'm certainly enjoying
something on that machine so if the cost
is low enough if the if the if it's
being driven by the commodity maybe
it'll be there I agree though that we
don't certainly don't need it for all of
our applications that we have so you
know it takes six to eight years before
things drop off the list it takes 8 to
10 years before a machine that's on the
list is something that we hold in our
hands and use in some sense again that's
a that's a it's a remarkable thing I
have a hard time with that I know about
Exponential's and when I see this I have
a hard time really getting my hands
around what I'm gonna be doing and how
how will that machine be effectively
used in that in that time period so this
is this is the guy that they're building
at Los Alamos so Los Alamos National Lab
in New Mexico is putting together this
machine and they claimed by May the
Machine will be put together this
machine will be at a petaflop they'll
achieve a petaflop of execution for the
benchmark this architecture it's a it's
a highly parallel highly parallel system
it'll have something like seven thousand
Opteron processors chips
so that's dual-core so 14,000
roughly 14,000 core of AMD processors
each of those cores will have a
companion chip from a companionship that
that's the Cell processor right the cell
processors has chip made by IBM Toshiba
and Sony it's a it's a chip that's
multi-core it has nine core in it nine
core sort of Nod number it's asymmetric
it has one PowerPC core and then eight
vector cores so that's the chip that's
used in the PlayStation the PlayStation
3 has that chip in it very interesting
chip have very high rates of execution
possible driven by the need for graphics
right so the graphics and the
PlayStation demand floating-point
computations the scientific community
realized that maybe we can use that chip
so as eight vector processors that are
doing the floating-point arithmetic very
specialized think of it as an attached
processor in some sense so the intention
here is for each core of the AMD to have
one of those cell processors but one of
those cell chips those nine core things
attached to it and that nine core thing
is is a chip which each one of those
chips is a hundred gigaflops of
theoretical peak performance 100 Giga
flops and now we're going to put in
place machine which has 13,000 of those
one gigaflop chips so that's 1.3
petaflop s' if we just count the IBM
cell processors sitting there inside of
the box then there's these AMD cores
also which will be used the programming
for this machine will really be a tour
de force it's a sort of a you know
multi-level programming here you got a
program for MPI to move information
between the AMD processors and then you
need some another mechanism for
communicating with the cell processors
and then you have to implement your
algorithm in the Cell processor
exploiting those vector those vector
units in that in that device so
implementing that is going to be a real
a simple matter of software as we would
say right so it'll be really a tour de
force and
the expectation is in May that machine
will run at one petaflop they'll run the
benchmark at a petaflop so i think that
they're accelerating things that so you
know the story as well as I do there's
this incredible situation happening
moore's law is driving things in a
certain certain way we're limited in
terms of - things have been happening
right with the chips
we've been doubling the number of gates
and the manufacturers have been
enhancing the cycle time of those chips
every time a new Rev comes out the
problem with enhancing the cycle time is
you need to put more power in and we
reached a point where they can't
continue that path of increasing the
power so our chips now consume about a
hundred watts hundred watts of power
from a Pentium processor for instance
and I think about a hundred watts
sitting in the size of your your
fingernail right and I gotta get the
heat out of that and you know doing
increasing the cycle time means there's
more more power required for that so the
so right now the cycle time looks like
we're stuck we're going to be stuck at
this point with our processors the
number of gates that we have will
continue to increase on our chip and the
question is what are we going to do with
all those extra gates so manufacturers
were quick to figure out that we're
going to give you more core on that on
that chip so today we have two four
eight and we know where that's going
into the future so we're gonna be driven
to a high numbers of core on our chips
you know things are really governed by
this kind of equation here so our if we
think about our chips the power required
is proportional to the voltage squared
times the frequency the frequency and
the voltage are connected so there's a
relationship between them so that's as
the power goes up by the frequency cubed
so now you see the problem right if I
enhance the frequency I get this cubic
relationship so if I double the
frequency I've got 8 times the power
required to do something here which is
which is where we're out of control with
this so if we take a look at our
machines today let's say we had a
superscalar architecture
1cor had some frequency and some
performance associated with it that
would give some power power requirements
if I think about having a new a new core
a new processor which had a higher
frequency higher frequency 50% faster
machine that says I need about 3.3 times
the power to to fuel that thing right so
so the wattage is going up for that 50%
improvement in performance gives us over
a factor of three requirement in terms
of power if I think about a multi-core
system with two cores and think about
the frequency going down for that two
core system that says that the power
requirements now for those two core is
at point eight and if I could fully
utilize those two core even with the
reduction in the cycle time I get an
enhancement over my one core of about
one point five so I'm getting better
performance if I could utilize those
core with less power being consumed so
that's sort of driving things in terms
of multi-course today of course the
caches that I have to use both core in
the old days going from here to here
I did nothing my application would run
50% faster by by the frequency being
enhanced but today in order to get that
enhancement I need to use both of these
guys here and that's going to get more
complicated of course as we go to eight
and the 16 and the higher number of
things and we're being driven in that in
that direction and what's what we see
happening is that manufacturers will use
those core and devote some of those core
for special purposes on the chip for may
perhaps graphics so we'll have a
multi-core chip where some of those
cores will be used strictly for graphics
others will be used for floating-point
perhaps some for integer calculations
and we'll have a mixture of core on our
chips that are sort of tuned to the
specifics of the needs of the community
that they're being put into and we see
very high rates of floating-point might
be necessary in the scientific
scientific area here at Google you
probably wouldn't need all
that floating-point computing power but
with all of these chips you know we have
a crisis in terms of getting data into
the processor today our chips the data
is coming in on the sides on the
perimeter of it as we put more and more
core in there the perimeter doesn't
really get enhanced by much so
architects are coming up with ways of
having stacked memories so
three-dimensional memory where the data
would be able to transfer in and out of
the chips in in multiple directions it
would be the aim to get the bandwidth up
of information flowing into the into the
chip itself and you know intel is
experimenting with a number of things so
this is a research prototype that intel
has it's a 80 core processor 80 core
chip the performance is a peak
performance of a teraflop within that
chip and it consumes about 62 watts so
that's a that's something that's B
that's in in their lab today and they're
using to experiment with it so that's
not something in the future it's not a
product either but but they're certainly
driven in some direction there so
there's major changes that we have in
our software that we have to adapt to
multi-core and I just want to give you
an example of one of the things that
we're doing with this we're interested
in numerical library so we're interested
in solving systems of linear equations
and eigenvalue problems that's sort of
where the space where I fit and we have
a number of software packages that have
been developed over the last 15 15 years
there's a package called la pack for
solving systems of equations and that
was done for shared memory machines
basically and then there's a package
called scalar pack which was developed a
few years later and that's primarily for
distributed memory machines right so we
have a set of processors which each have
their own local memory and use message
passing to coordinate the execution of
those machines and scalar pack fits into
that that environment and that software
that we have runs pretty well on
architectures that came before and now
with multi-core we're going to have to
rewrite that
so this the software stuff even goes
back further in the 70s we had a package
called Linpack and Linpack was a package
which was in some sense targeted for
vector architectures so we organized
calculations along the lines of vectors
and vector operations were the things
that were singled out and if we could
exploit that in terms of the
instructions we'd be able to get very
high rates of execution so that was in
the 70s and the 80s shared-memory came
about and the shared memory was it was
important to effectively utilize that
shared memory and data locality was
important so blocking of our operations
in terms of operations that can
effectively get reuse of data was
critical and that's what la pact was
good at and in the 90s distributed
memory was the mechanism used to do our
computations so we invoked a message
passing paradigm in our software and
effectively use those machines but with
multi-core we have we have another
challenge so again it's a very
disruptive technology all of our
software is being rewritten and all of
the software in the scientific community
has to be redone and all of the software
everywhere has to be redone is the way I
look at that so guys like Adobe and
Microsoft and you will have to rewrite
your software to effectively use
multi-core architectures you don't have
to if you don't want to get at the
advantage of those extra processors but
if you if you wanted to you you would
have to rewrite your software so in my
space one of the heavily used packages
is something called MATLAB comes from
the math works you know they're
terrified right now of multi-core
because everything has to be rewritten
from the MATLAB standpoint and we're
doing the same thing here and it
requires a basic fundamental change and
how the algorithm is expressed and it's
going to be expressed and I'll save more
in a moment through a directed acyclic
graph and we'll use a scheduling
mechanism to allow the tasks to be
executed in a way that makes sense
so think of it like this in the old days
this is the Lu factorization so I want
to solve a system of linear
equations using Gaussian elimination and
the way this works is there's an
algorithm the algorithm has a loop and
the loop is going to go through n minus
1 iterations at the end of that time I
have an Lu factorization which I use
them to solve a system of equations most
of the work is consumed in generating
this Lu factorization and the loop that
we have carries out these operations so
this is sort of the think of it as the
kernel for doing this thing the first
thing I do is I operate on a panel I
take a panel and do some operations on
it that's a very sequential operation I
do some interchanges to maintain
stability of the algorithm I do row
swapping if you want and then after that
I engage in two sets of floating-point
operations I do a triangular solve and a
matrix multiply those things can be
parallelized and the matrix multiply is
where I extract the performance matrix
multiplies a wonderful thing right I
have this N squared operations and I'm
going to do n cubed sorry you have N
squared data and n cubed operations so
that surface-to-volume is exactly what I
want in terms of exploiting or
extracting performance because I can get
very high rates of execution when I do
that so here's where I extract my
performance I do some things serially
here the question is how well is that
going to run in parallel so if I think
about implementing this it gets
implemented in the following way I do
something serially think of that as this
set of loops here or this part of the
part of the loop and then I do a bunch
of things in parallel and then I go into
a serial phase and then a parallel phase
the serial pace so I end up with this
kind of structure a very bulk
synchronous kind of operation going on
here where I alternate between a serial
part and then a parallel park and if I
think about running this on a machine
let's say with four four core I end up
with a timing diagram we think about
time evolving this way and I think about
exploiting those four core in this
cartoon think about each one of these
rows here as being one of the cores I
end up with one thing being utilized
a bunch of things being utilized one
thing in a bunch of things and the
problem here is all that white space
right the white spaces where the
processors are idle because they don't
have any work to do that there's not
enough parallel tasks one guy has to do
something it's a bulk synchronous thing
we're at that moment just the serial
part of the calculation goes on
however the computation if I look at
this this computation this is a directed
acyclic graph the arcs look our
dependencies between the tasks that are
there and if I think about executing
that in a slightly different order than
what I've done in the past I might be
able to effectively exploit more things
to be done in parallel so if I think
about something which is event-driven
trying to exploit the multi tasks that
are there and reorganizing the algorithm
along those lines I could end up with an
execution which follows the critical
path of that dag so that's really what I
would like to do and the execution then
looks something like this where I don't
do all the things I could have done in
parallel
i execute part of the graph which allows
me to do more of the stuff that I was
doing sequentially before because their
dependencies are satisfied and and they
can now be executed and that frees up
more tasks so I end up with a graph or a
dag which gets executed in this order
right rather than the other block
synchronous fashion and that leads to a
compression we're very very little white
spaces left within the algorithm itself
so at the beginning I have one thing to
do and then that's the only thing
unfortunately at the beginning but after
that I generate a lot of tasks to do and
after that until the end of the
computation where we have a shutdown of
things we generally can fill up fill up
our time doing useful work so that's the
way we're reorganize reorganizing our
algorithms along those lines you know I
can think about doing this on a 8/8 core
system this is a dual socket for core
Intel board and I think about doing that
same algorithm there and I end up with
an execution flow which looks something
like this in that machine and then with
my dad based scheduling it looks
something like this and
we we can project or look at scenarios
where we have a larger number of core
and we see the same kind of execution
unfolding for our computations so that's
a very very realistic thing these ideas
are being used also in the in the IBM
Playstation so in the in the Sony
Playstation there we have nine core
we're going to use eight of those vector
cores to do our computation again we
want to keep things executing as much as
possible and that same idea that same
idea there can be used and exploited in
this case giving rise to very high rates
of execution in this environment as well
there's also possibilities here to
exploit single and double precision and
that's another part of a story which I
won't have time to talk about here but
it it allows that to to occur in a very
natural way in fact I think I'm going to
skip ahead here and go to my conclusion
a reminder here that's not nice
okay so let me just make a couple of
comments here you know for the last
decade or so whenever we look at high
performance computing we often focus on
the hardware aspects of high performance
computing and rarely do we look at the
the software side that is we're driven
or we're looking at the the very high
rates of execution that that are
possible through effectively exploiting
that hardware unfortunately as our
machines have become more complicated we
haven't evolved our programming
mechanisms to effectively use the
architectures that we have today
so in some sense we're at a stage where
things are very unbalanced I think of it
as a an ecosystem for high-performance
computing
we have in this ecosystem we have many
things we have things related to the
hardware we have operating systems we
have compilers we have our mathematical
software we have our libraries and all
of those things sit in this ecosystem
programming languages and in some sense
the hardware is far outpaced of the rest
of that ecosystem generating systems and
architectures which have tremendous
rates of execution but our software base
has not been able to keep up with that
with that pace and we have this crisis
in some sense today effectively
utilizing our architectures that we have
there we have machines now with 200,000
core in them and our software base is
not prepared to fully exploit that we
have machines which have multiple levels
in their in their architecture and again
our software and our programming
mechanisms are not prepared to
effectively take on that that challenge
and that will lead to you know major
problems with effectively using that
that are those architectures for our
scientific or scientific problems so I
would say that one thing we need to do
is to invest more in terms of
programming languages and mechanisms to
help support the architectures
we have available to us today so that's
the end of the story I guess there's a
few things I'd like to do is to thank
some of my colleagues here who have
helped on the project I receive funding
from a number of sources NSF and do-er
their traditional sources of funding for
this but also the math works the guys
should do MATLAB and the guys at
Microsoft have helped Microsoft and
MATLAB are very interested in developing
software that fits into the multi-core
multi-core framework if you're
interested in any of this one one way to
go one way to find out about it is to go
to my webpage and the easiest way to
find my webpage is to is to go to google
and click I'm feeling lucky and that'll
send you there so thanks very much and
I'd be happy to answer any questions
that you might you might have
I guess could you say something about
stability and with these computers
becoming so much faster that seems that
even with double precision you're going
to be running out of precision in
seconds I've got some interesting into a
little arithmetic so right so so you're
saying that because we're doing pedal
flops we'll the accumulation of roundoff
error in our in our numerical
calculations is such that we will end up
with very little useful results in our
in our data yeah so that's that's that's
certainly a concern and we have very
good for some of our algorithms a very
good analysis which shows that that the
accumulation of roundoff error in
certain cases can be catastrophic that's
usually controlled by something called
the condition number of the problem so
think of the condition number is this
thing which magnifies the roundoff error
the badly condition problem will take
roundoff error and magnify it to the
point where we have very little very
little accuracy left and in terms of
matrix computations that's an
ill-conditioned matrix classically a
Hilbert matrix is a very ill conditioned
matrix and if you try to solve any kind
of problem with it
you'll end up with just rubbish because
of the accumulation around off here if
you can say something about the
condition number of your problem and you
have an analysis which shows that the
condition number says something about
what you can achieve for a given
algorithm you can do the sky's the limit
in some sense in terms of what you have
for linear systems the accumulation of
roundoff error for a well-conditioned
problem grows as a function of n the
size of the problem so it's not getting
out of control in that sense so you can
solve very large problems without losing
it but that's a very interesting thing
in terms of what can you do to recover
the precision so is there a mechanism
for iterating after you have an answer
given that approximation to get
something better and it's those a lot of
work that goes on with that as well
actually for one more question because
there's
so you mentioned a little bit about
failure when anodes drop out of the
system and can you recover there's
another aspect of failure which is the
memory cell doesn't return what you put
in it the floating point computer the
floating point operation returns wrong
bits how are you going to deal with that
problem right so so so when I I asked
for something I generally expect I'm
going to get the right answer and what
if it doesn't give me the right answer
how do I accommodate that in my in my
problem so you know one thing I would
advocate is check your answer check your
answer so there's ways of checking it in
the context of the thing I deal with I
can compute a residual a computer
residual the residual will basically say
how close did I how close is the thing I
got to to the original problem I was
looking to solve and that check will
show some some Fault in that way so will
actually show up if I'm able to
calculate that residual if I can't have
a figure of Merit if I if there's no way
of calculating that figure of Merit in
my computation then you know the output
I should be able to have an idea of what
the output should be right so if the if
the answer is that it's going to rain
today and I end up with you know
earthquakes coming out as my answer then
then maybe maybe I've got a problem with
with my data somewhere or my algorithm
that's another source of problem and if
you have no way of really understanding
intuitively what your solution should be
for a given problem
then all bets are off so you need some
way you need some way of checking it
really and and you know most machines
are pretty stable but occasionally we do
find systems that have problems you know
there's a classic problem with Intel
chips that produce the wrong answer on a
divide early on in the penting days and
that was a that was a crisis okay let's
think of speaker will be around and
he'll stay for lunch so if you want to
join us Steve thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>