<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Essence of Caching - Ehcache | Coder Coacher - Coaching Coders</title><meta content="The Essence of Caching - Ehcache - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Essence of Caching - Ehcache</b></h2><h5 class="post__date">2011-02-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TszcAWgCXD0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone I believe
there's a few people remotely as well so
hello when Bo when I met him a few
minutes ago said we do a lot of caching
at Google obviously but when Bo wanted
me to come and talk here at Google
because we're really different type of
caching it at Terra Cotta we do
transactional caching so read/write
read/write caching
xa transactions local transactions not
read-only caching we do to read own
ocation of course but but we do much
more so the talk itself is going to is
going to cover the theory of caching
which is my favorite part
that's that's a good portion of the talk
and then I'm going to do a new and
notable so I think education gets used
in a few few places here at Google
internally and on beginning we actually
just get a show of hands everybody
familiar with the education yeah okay so
what I'm going to do in terms of the the
project because it's an open source
project and also a terracotta product
I'm just going to limit that to a few
slides on just new and notable features
that have happened in the last 18 months
so whatever you knew about eh cache in
five slides it'll just bring you up to
date in terms of what we've added anyway
without further ado I'll dump I'll jump
into the essence of caching so firstly I
just want to look at demand and and how
it like just a frame location problem
and look at how we how we get to caching
and in some audiences a lot of this is
news this may not be in use in fact I'd
be surprised if this is news to this
audience that's a that's a real shot of
puppet master just showing just showing
spikes in load so the first thing about
load you don't build an app with an
average load in mind
I mean hides a lot of variations so
there's variation there's variation
second to second minute to minute as
shown by that spiky load graph
if you were to one if you were to plot
the mean the mean is that green line so
you know just talking about the mean and
capacity planning for the mean doesn't
do it the second thing is that in the in
Belgium they actually talk about the the
elephant curves that picture comes from
a book the petit Prince which a lot of
people read and the top picture is what
it looks like if a snake was to swallow
an elephant just get the outline of the
elephant so it's called the elephant
curve and in Europe they typically use
this to describe the daily pattern of
load and once again I mean where would
the mean be the mean it'd be kind of you
know just you know some way up the body
so you've got those you know just like
with right waves you know in terms of
your peak load mean means or mean
doesn't mean much you've got you've got
that daily load and then you've got
spikes within that so you've got to
you've got a factor for much much more
than mean now this one might interest
you this this I got out of a recent a
recent paper that's that's Google web
search lied for one of your data centers
and you can see it's a pretty fair
approximation of the elephant curve just
another aspect of it my background is in
is in hotel and is in online hotel so I
was the chief architect at what if calm
which is where I created the education
much my production experience comes from
and we would have we would have peak
days on Mondays so Monday could be three
or four times the normal peak load and
then the other aspect of it is March so
if you look at the if you look if you
take an annual return for what if and
you actually look at sales through the
year you'll see there's a curve that
repeats more or less the same shapes at
the same times every year so what would
happen we get through one we get through
like a peak load on a Monday in March
and then we'd kind of spend the next six
months thinking about how we were going
to get through
the same peak that was going to come a
year later and add what if it was
growing at it was growing at about 40
percent per year so we had to actually
plan that much more capacity now the
other thing and this once again to this
audience this shouldn't this shouldn't
be news in fact Google Google has made a
has done a great job at being being fast
that what if at what if we we had about
15 competitors and what we focused on
was being available which was partly
about planning for the capacity and
actually being fast and the just to take
you back to a fundamental piece of
research on human-computer interfaces
back in the paper by Miller and card in
68 they looked at they looked at
attention span and computer response and
point one of a second is the limit for
having user feel the system is reacting
instantaneously and one second is the
limit before users flow of thought gets
interrupted so incredibly important so
just to frame the problem you've got to
cope with those huge variations in
demand you also have to do it you have
to that your whole system really needs
to respond in less than a second ideally
so that's quite a challenge
at the same time the dream is that your
app is horizontally scalable and that
it's been designed architectural II so
that all you need to do is slide in
another rack and I believe that that is
the case with a lot of the Google
architectures but it's far from the case
in most of enterprise computing the
other problem the other problem that you
see or observation which is setting the
scene here is what I call the hammer and
the nail so it's a truism that I kept
coming up against that if you if you
have a if you have a performance problem
and you've got a you've got a meeting
room you've got people sitting around
the meeting room then you'll have a DBA
there and the DBA will say oh you just
need to give me Oracle RAC you know this
is in an enterprise computing context or
I just need a bigger server and the Ops
guys will say yeah you know I just need
to improve the network I need to do this
I need to do that the programmers will
say you know you just need to actually
take take a few of the feature feature
workload office it actually just give us
some time to actually performance
attuned everybody actually is holding a
hammer and everybody sees a nail and and
performance to get performance you
really need to be holistic and so you've
got to try and try and break through
that hammer and nail problem the the
other one a favorite one of mine in
looking at in looking at performance as
Amdahl's law where to get the speed-up
of the system is is that formula there
and what it boils down to in common
sense is if you if you break up your
response time into into segments each of
the different pieces then if you speed
up one of those pieces you can calculate
using this formula what the system speed
up will be and one of the problems with
the hammer and the nail interacting with
Amdahl's law is if if say that i'm a
java guy if the java code is if the java
code is sped up by two but it was only
2% of the time
so rendering a rendering a page then
then you're not going to get much of a
speed-up you get a 1% system speed-up so
I've done I've looked at this through
lots of enterprise systems applying
Amdahl's law and figuring out where the
bottlenecks are and it's obviously very
specific to each system and what I'd
encourage you to do is do the empirical
analysis and then apply em Dal's law to
calculate a priori what your system
speed-up will be from speeding up one of
the component parts however to
generalize these are the these are the
big four that that I keep seeing the
first one is the first one is static
content now you know this is what
content distribution networks exist at
what if we we had a 28 second page load
time from London with servers in
Adelaide Australia we rolled our own
content distribution network and we
brought that down to 2.8 seconds now you
know obviously here in the valley
everybody's discovered content
distribution networks but are there an
enterprise computing not everybody has
but that if you if you apply em dollz
law end-to-end right to the right to the
human end-user usually the biggest win
starting out is to actually stick in a
CDN beyond that you've got you've got
render time render time renter time of
off pages or creation of creation of
JSON or XML whatever it is it doesn't
really matter that tends to be quite
significant as well so in eh case we
actually have a module called eh cache
web which allows which allows web
caching and has got some interesting
properties like if multiple threads come
in to to grab to grab a view and the
view doesn't yet exist only one thread
the first thread will actually go
through and actually crater to the other
threads are held up so you don't get a
stampede effect once you have an expiry
of
view or the view hasn't been created
first the the next one is what I call
collection caching I don't know if
there's a a good term and the literature
for this but you've done work to
assemble a graph of objects and those
graph of objects are close to your
business logic and they're ready to be
used the the cases tend to be very
domain-specific but there's obviously
there's often very significant work many
database accesses sometimes web service
access is involved in getting that so if
you can actually catch that data you get
a big win the next one is anything
involving a network access to another
service so it doesn't matter whether
it's soap it's soap or it's it's rest or
my own experience with with travel
companies there's all sorts of exotic
protocols you've got with these types of
services in our in our industry one of
the ones that we had to do was from
Sydney we actually had to go to France
to actually um to Amma diets to actually
do flight searchers so you've got the I
think it's a couple hundred milliseconds
round-trip best case that you've
actually got to add there so it becomes
very very expensive in terms of time and
then the fourth one and and I'm
deliberately putting it last because
it's the one that the only one that
anyone ever wants to think about is the
database obviously the database or the
data store is expensive but there's much
else these other things in terms of the
the benefit you get in terms of that if
applying am dials law the size of the
segment that you accelerate by caching
the further out towards the user that
you cash the larger the speed up you
tend to get the slowest speed up benefit
cashing the store what you get from
sometimes and sometimes the focus is on
the database not because of the speed up
you get the speed up is still quite
significant I mean looking at speed ups
speeding arm step one so caching web
responses you get about a thousand to
one speed up on the on the data by
you get you get about a hundred to one
that's the kind of speed up you get but
but on the on the database often you've
got an expensive piece of infrastructure
often the architecture is not
horizontally scalable often the
architecture cannot be scaled and what
you're doing with caching more than
worrying about speed-up you're actually
trying to avoid the cost of an
architectural rewrite and so you're
trying to offload the database that's
one of the main reasons why people cash
against the database now when I've used
the term database I don't just mean
database because we do in process
caching we're talking about low load
micro load numbers low single-digit
microseconds for access like education
stand alone one of the perfect tests I
do is got a hundred 100 threads doing a
mixed workload of about 75 reads 75
reads and then a balance of I've
iterates and removes and the average on
that is 7 microseconds the core got
reworked in 1 6 2 to remove almost all
synchronization the only synchronization
I think is on is on there's one
synchronization step on put but it uses
it uses it starting off in in 160 that
uses concurrent hash map and we had to
have a separate way of doing and we what
we do is we use probabilistic eviction
so we don't keep track of mutations in
terms of LRU instead what we do is we
actually do a random sample of 15
elements we actually choose the best and
there's something like 50 or 60
different eviction algorithms that you
can craft because we have our own
metadata the three that we provide at
least not least recently used because we
were probabilistic sites less recently
used less frequently used and and and
most recently used
so just to look at just to look at
caching the properties of caching
abstractly as a solution to performance
it's got some interesting and unique
properties sure you get performance the
other thing you get is offload and it's
offload against against all expensive
resources now at Google at Google on any
large comm caching caching is built into
the mindset everybody realizes that you
need to cache at multiple levels to
actually get the performance is an
interesting mindset in enterprise
developers majority of enterprise
developers that I've come across
including people at my own company
thought that caching was dirty it was a
dirty trick and and really if you
engineered things properly you wouldn't
need caching I don't think that's the
case you get offload so at the same time
as you get performance with caching you
also get offloaded the expensive
resource and one of the things that
seems to that I've come across it seems
to be a truism is almost all new systems
start off being engineered
monolithically with a single database
and what happens as they grow if they're
successful they have to get re
architected Andry architected and we
architected I read somewhere that at
Google here you guys actually
reengineering most of your services
about once every two and a half years so
you may yourselves even be going through
the same thing what happens in
enterprise computing contexts is that
there's usually a vast amount of
management resistance to the art to the
costs involved and actually completely
react architecting a solution and so one
of the one of the big use cases for 4eh
cache is to actually try and delay the
re-architecting the system and that's
where offload comes in and doing caching
you can offload pretty much any resource
the the other thing is scale out so with
with the distributed cache
you can use the distributor case to
achieve scale out now this is a this is
a slide from ARS technica on speed what
I want to do now is just just give you a
flavor of the different the different
speeds of access and how we leverage
those in and this is something I guess
that makes a case and a distributor case
fundamentally different to something
like a no sequel solution or even
something like a memcache which is an
over the network case so if you're
looking at your if this is kind of the
classic view IBM have got a new I've got
a new mainframe chip which apparently
has set a record for speed I think it's
now running over 5 gigahertz and I've
actually got l1 l2 l3 and l4 cache but
this is more typical the other thing the
other thing is is from a ram point of
view there's often not one set of access
speeds for ram because in your numeral
architectures like your AMD's you've got
you've got daughter cards and you've got
cross bar architectures where you get
different speeds depending on whether
there's locality of to the memory on the
same on the same cpu which by the way
you can now leverage in java there's a
switch there's an X X X X colon use Numa
switch for actually leveraging that
there's the there's the speeds so what's
just a just a couple of interesting
observations about that is is that is it
obviously gets slower down what's not
shown in that slide is is the network
the network is faster than hard disk
generally so it also invites another
type of architecture where you actually
have memory over the network
now how do we apply this to to Takei
Shing now this is a education specific
but these similar approaches are true
for most of the cases out there so we
have we have an on heaps tour which is
which is standard Java heat about two
three months ago we actually introduced
our off heaps tour so the the rattling
cutlery is a little bit to get used to
isn't it sorry guys I'm getting a little
distracted by that so we have an off
heat store and just like just like over
here just like over here the size the
size that you can store goes up as the
speed goes down so so what we do is if
we actually leveraged our design that
way we say okay we'll have on heat on
heaps the fastest right we just store as
just objects as basically it's just a
memory access that we're using
references resolving references now how
big can that heat go now the question is
there now that actually depends on your
app it depends on how many objects there
is what garbage collector you're using
how much time you spend garbage
collecting most people give up and they
say too hard and they run their
enterprise apps with one or two gig
heaps in fact we serve out our customers
two thirds of our customers are actually
using 32-bit architectures which means
they're limiting themselves somewhere
between two and four gig depending on
their architecture and OS and so they're
not running into garbage collection
problems the largest I've come across is
somebody running 80 gigabyte education
with an 80 gigabyte heap and they suffer
60 second full GC pauses now that's a
batch application obviously that
wouldn't work for online getting back to
our one second you know maximum response
time for the whole system the I have
seen I mean that my rule of thumb is and
what we use the what if was six to eight
gigs and we were able to actually tune
that to have no nor then
no more than 100 Mille second full GC
pauses I've seen people get it up to
about 12 to 15 gig but beyond that that
really is the Achilles heel of Java and
and you can't go any more so we've got
this big memory thing big memory is is
in process but but off heap it uses
direct byte buffers which were actually
introduced in Java and way back in 1/4
we figured out a way to actually make
them work really really fast one of the
things that we do is we actually
allocate the entire buffer as a slab of
memory at startup and it takes takes
Java actually about one second for each
four gigabytes to actually do that
allocation so what we do is we actually
we actually have log statements showing
your um showing your direct byte buffers
being allocated we we we've performance
tested that up to 300 or 50 gigabytes
and at it's scales linearly so that's
big memory and then we've got local disk
now the critical thing about local disk
is that local disk is restartable so
what you have in the cache what do you
have in the case when you shut down your
app and bring it back your case is all
there it's in disk it loads up from disk
which you can do quite quickly you can
do that much more quickly that can do a
web service call or it can do a database
lookup now the other thing that the
other thing that we have below that
level is is network and that's where the
Terra Cotta server comes in remember
what I said about the speed of the
network of a fast network being being
faster than disk access so it brings in
the possibility of being able to
actually access a distributed cache
where the server pieces are in memory
that actually outperforms data stored
even locally on disk now so that that's
kind of what our design looks like now
in terms of in terms of when to use
caching and how you know that caching is
working the critical statistic to look
at his cash efficiency sir case
efficiency is defined as for a given
workload what were the case shifts over
the total hits so if if a piece of data
was was written once and only read once
then you have a case efficiency of 0 if
a piece of data was written once and not
read at all okay so a log you'd also
have a case efficiency of 0 if you've
got a piece of reference data like
California state of California then
that's likely to have almost a 100% case
efficiency rate so and then in between
in between you've got you've got most of
the data in your app your transactional
data and you'll have to calculate what
that what your case efficiency is and
depending on what that calculation is
it'll tell you whether it's a worthy
candidate to cash or what you have to do
to go about improving cash efficiency
now higher case efficiency implies high
performance because a case the case is
much faster than anything else and also
implies high resource offload now let's
say let's say that you've got location
efficiency because you've got
transactional data which is really big
and it's bigger than what you can fit in
the case obviously if you've got fairly
low hit rates on any given piece of data
but you can fit the whole lot in your
cache it doesn't matter you can still
have a high cache efficiency ratio but
generally that's not the case and if you
go back to diagram you've got your
different speeds of access as well let's
say you just want to use the education
stand alone which is my understanding of
what you guys use here at Google and a
few systems right you've only got you've
got you know you've got limited by heap
and let's say your whole data set is a
couple of terabytes and you've got one
gig of cache so you know you've got a
point five percent chance if all data
was created equally but is all data
created equally if all data was created
equally probability of having a hit
would look like that
Hits would look like they'd be kind of
evenly distributed but actually in most
ecommerce systems you get this you get
this this type of distribution which is
more is most formally called a Pareto
distribution it's also referred to as a
power-law distribution focusing on the
extreme right end this is what Chris
Anderson referred to as the long tail
we're not actually interested in the
long tail what we're interested in is
the left-hand side which is the fat head
now what you can do and what I've done
many times is if you actually get a
piece of data that you want to cache and
remember we don't care about what it is
right could be anything could be could
be the old the key could be URLs it
could be it could be primary keys in a
database it could be it could be a web
service lookup doesn't matter what it is
right you can just think about it
abstractly what you do is you look at
your production logs
oh you instrument your production logs
to get this data out you produce this
curve and then from this curve what you
can do is you can say okay if as I move
along the x-axis
you then calculate the area of the curve
that you've covered and that area of the
curve the area of the curve will give
you your cache efficiency ratio so then
what you'll see from the shape of the
curve is that you get something like an
80/20 or 90/10 so if you do about 10 to
20% of the population
oftentimes that's going to give you 80
or 90% and that's why that's why caching
are only part of the data set works so
well so often now that's well and good
that's well and good except these days
with the education terracotta we can
actually go one better we can drop in
the terracotta server array and we can
go up to a couple of terabytes in
storage so you can have a couple of
terabytes in your cache now that's not
quite big data as understood by people
like you but that's actually more than
enough data for most enterprise
applications
and so then then the way this curve
looks is a little bit different and to
go then then you end up with one of
those curves for each of the tiers in
the cache and so you can use this
approach to actually calculate what your
performance will be from the cache
because the cache is fastest offer off
on heap then big memory then then disk
and and the Terra Cotta server about
about similar now if it was that easy
and read-only caching is that easy then
then we could all go home now that
unfortunately this is where we leave
read-only caching behind and we'll start
looking at a tree write caching
transactional type caching so how do you
deal with the problem that presents
itself as soon as your cash with
coherency with the system of record now
it depends on the business domain and
sometimes it doesn't matter is the
answer sometimes and most times you can
sit down with a business analyst and you
can find out that staleness
is kind of built into everything we do
in life anyway it's like a time table a
time table could be style look at the
Google index what's the average darkness
of the Google index probably a couple of
weeks except it's not evenly style data
that changes more frequently is actually
updated more frequently so this this
this is normally handled with the TTL a
time to live but there are actually
better patterns for doing a coherency
with a system of record one is the is
the eternal you set your cache elements
to be eternal that never expire but you
capture when they've become style and
you invalidate them so that's eternal
plus invalidation there's there's a
couple of ways of doing it
I mean each case if you've got a
distributed cache and you find and one
of the notes finds out the data has been
updated you do cache don't remove we
have a rest we have a rest
API that you can plug in so you can
access it from multiple languages so
often what happens is you've got data
setting a system of record and you've
got tons of applications that are
working with that data and so you need
to expose some sort of easy Web Services
API to actually do the invalidate so
we've got that too with it with a REST
API the other one the other one is the
classic hibernate one in hibernate you
set your karma you set your case to be
read right or you set it to be
transactional and what will happen when
one of the nodes writes new data it'll
actually invalidate invalidate the
vacation and finally in the edge case
we've got case writer so that's for the
right through pattern so they're better
they're better patterns for dealing with
coherency with the system of record
right so up to this point we've just
been talking about standalone single
notes now as soon as you get into
multiple into multiple notes you get a
new set of problems now why would you
want to run an application cluster well
the answer is that for anything that
matters you always want to run a cluster
because you always have a plus one
you always have whatever the number of
servers required to handle the load is
plus one for redundancy or you and
there's different or you might want plus
two or you might want mirroring there's
always more than one server so you're
always running a cluster and what are
the caching problems that that creates
the first one is the end times problem
and I'm going to run through these
problems so if we look at if we look at
standalone in process caching that looks
like that you've got the edge case
you're educating in your app doing
caching so requests and from here on out
I'm just going to talk about the
databases the thing that we catch but
you realize that I mean much more than
that vocation we have for requests that
come in so within the time-to-live of an
element right each node is doing a
request and then it expires and you'll
go back to the system record right so so
what happens so n requests are made so
you can generalize this and you can say
that when you have a
cluster that's not distributed you will
make n requests to the underlying
resource within the time-to-live of each
K shaman and that can add up really fast
and this creates a huge problem because
you've got your standard enterprise app
that's been built monolithically and you
start scaling up you add standalone
caching starts to work really well
you've solved your problem
then you get busier so what do you do
you add more app servers guess what
happens you get this end times problem
your cache efficiency cache efficiency
stays much the same but your offload
doesn't because the number of requests
starts screaming into the artists
streaming into the underlying resource
so what's the solution to that the
solution to that is to create ocation
cluster now one thing that you H case is
done for about three or three or four
years is replicated caching now it's not
just the education lots of the open
source projects actually take this
approach so what you do is you have the
entire data set in each node and when a
change is made that change is applied to
each of the other nodes now in each case
we do that with our my J groups and JMS
with our my being by far the most widely
applied now there's a couple of problems
that that that present themselves with
this approach one problem I call the
bootstrap problem so what happens when
you restart restart one of the nodes I
was reading in a paper that that here at
Google you actually have to restart
servers between 1.5 and 16 times per
month and in any long-running
application you're going to be
restarting service now you could have
disc persistence turned on in which case
in which case just partly solve your
bootstrap problem because when you
restart the thing will come back up
if it's a if it's a crash then you
destroy the distal wields app itself for
protection so you'll start up empty so
you've got a bootstrap that's somehow
now in eh case could provide a bootstrap
protocol which will copy data from the
other nodes there's another problem as
well though which is the case coherency
problem so whenever there's a change in
one of these nodes there's really a race
it just kind of propagates out to the
other nodes like dropping our a pebble
in a pond and they could all be doing
that you could be having concurrent
updates to the same element across the
cluster and they're going to be racing
each other around the cluster what
you're most likely to end up with is is
it's different versions in different
parts you cluster which is your classic
case coherency problem and things like
memcache which which don't use locking
I've also got a cache coherency problem
although it's slightly different they
don't suffer a bootstrap problem because
the data is not stored in the note it's
always remote you can still have races
and you have last one wins with with
medication so is there is there an
answer so the answer the answer that the
the caching industry has come up with
and terracotta is no exception is to
create a distributed coherent cache and
the implementations vary the
implementation that we have is is a
terracotta server array we have a
coherent protocol between the two the
the general the general approach that
I'll give you a general approach but but
in reality we are actually very
configurable there's lots of different
settings and in fact that's true
more or less of everyone that there's
and actually what I'll do is I'll talk
about the next problem before I get into
that but we have a coherence II protocol
that solves the problem and if you think
about it just just to park just to give
you something to hang on to for right
now what we do is we implement the
happens before of the Java memory model
so and we do that with locking in the
strict case so that so that to read from
the case you need to require a read lock
that you can hold on to greedily if
anybody wants to do a write they acquire
write lock a write lock is not granted
until all the read locks are reclaimed
and no more read locks are issued while
the write lock is outstanding then then
the write is done and the writers
released there's timeouts and things
like that to deal with partitions that
is that is a eh-eh to trigger the edge
cases strict coherency mode I'll get
into some other modes that we have in a
minute and the reason we have those
other modes is because simply saying
that you're going to implement a locking
distributed locking protocol like that
is actually not sufficient it's now well
understood that that the cap theorem is
true so that you can't have all three
desirable properties of a distributed
system which is consistency availability
and tolerance to partition now there's
lots of ways of carving up the caviar
cap theorem and a professor at professor
at Harvard had actually looked at this
and actually came up with a a very
interesting observation and that is
there's really two different scenarios
for the cap theorem the first one the
first one is that sure during a
partition you've got to make a decision
between what you're going to favor is it
going to be availability if you can only
is its availability consistency you've
got to do a trade-off between that
however you know what if you don't have
a partition what is happening the rest
of the time is there a trade-off it's a
different trade-off the trade-off is
actually between latency and can see
agency so in other words a strict
locking protocol like that is not going
to work when you have a partition so you
have to choose whether you're actually
going to become unavailable or whether
whether you'll allow rights and become
inconsistent or allow reads because
rights could have happened somewhere
else on the cluster otherwise otherwise
the trade-offs latency and consistency
the locking adds an overhead and that
overhead slows down the operation of the
case so so given the cap theorem what we
actually end up with and in fact what
what all real-world
distributed Keysha implementations end
up with is is a much finer grained view
of the world so we offer we offer
coherent equals true which is which is
that full lock mode we also offer a bulb
mode load so what is quite common in a
lot of enterprise contexts is that an
application can be taken offline but
still still operating for maintenance
there might be a maintenance window and
what what is quite a common use case is
wanting to jam a whole heap of extra
data into the case during that
maintenance window so what we what we
have is this some is his bulk load mode
where you can actually set set the case
into incoherent mode and just slam
everything in as quickly as possible on
the basis that there are there is no
business rights going on to the system
the other thing that we the other thing
that we do is we support XA transactions
which is even slower than our normal
locked mode and the reason is is that
simply simply having that type of
happens before guarantee is is often not
sufficient when you're changing multiple
resources we also introducing in in the
next version
comes out GAE in in a month what we're
calling non-strict coherent mode which
is which is not strictly there is no
strict happens before guarantee but most
of the time you'll get your your arm
cache nodes will be updated with changes
within four or five milliseconds and so
if you say the cap theorem is true then
then you as an application designer
really have to make the trade-off now
what we do is we we let you decide on a
case-by-case basis and also with things
like the bog load mode we let you
actually have a setting and then put it
into an incoherent mode for a short time
and then change it back and like there's
our there's a wait for coherence so you
can actually wait until the thing
becomes coherent again before you
continue so so we start off at the
beginning with the performance problems
kasing is a great solution however when
when you end up with full reality the
problems get quite difficult what you
really need to make it work and be
useful as a distributive cache but you
have to solve these problems and
fundamentally you come up against the
cap problems at the end and as we move
through time we are going to do more and
new interesting approaches one thing
that we've just done which is in the
next releases we've done local
transactions so it's not XA so it uses a
resource manager if you just want to be
able to apply changes to multiple cases
with an acacia manager within our local
transaction then you can do that and
that runs about five times faster than
our XA mode now I've got nine minutes
left I'm just going to very flicked
quickly flick through what education
looks like from our getting started
point of view yeah yeah the master file
is re-educated XML config file
stand-alone typically looks like that
210 and is distributed it's the red text
there he's chucking a terracotta
terracotta config up of terracotta
contact lines
the kit that you download we comes with
both maven and ant integration so
there's your commands in maven to start
stop you guys are still an ant shopping
have you moved on to other build tools
rembo what what built ill do you use
here you didn't you never went down the
maven path to g the from an ant point of
view we've got in the kit we've got a
macro def it shows you how to get going
with ant or any other build tool now new
and notable I've got a couple slides on
you and notable and then I'll take
questions so so eh cache is about seven
years seven OMA so this must be its
eighth year it started off as a
standalone case for solving those
problems that were that seems simple to
me at the beginning until I kind of
fully understood everything so ocation I
joined forces with Terra Cotta about 18
months ago what's happened since then is
quite interesting the engineering team
size is about 40 now our 87 percent of
revenue comes from eh cash so about 87
percent of engineering resources go into
eh cash so it's been a lot going on
we've done we've done six releases and
we're actually our seventh will be next
month what is changing is it went from
that the early slides that I showed you
of eh case standalone and replicated
noncoherent into the full Enterprise
distributed coherent case the Terra
Cotta server rate Terra Cotta was a
technology for doing for doing kind of
foot for clustering really and it had
multiple applications that is all still
true of Terra Cotta however a huge
amount of works gone on to actually
re-engineering the core to make it be a
very very good across the network case
store and it pretty much gets gets
reject every time we do a release just
to give you a little bit a little bit of
a taste of what's been going on so I
said that on heap about six gig was
about your limit now in a distri
we can take it up to about two terabytes
with it with in terms of your in process
case size we can take that up to about
one terabyte so if you have a if you
actually have enough memory and you can
buy machines from HP and Dell an Oracle
with two terabytes of memory in them so
you can actually have a Java process
with one terabyte you can ever get a one
terabyte of cash which you can do a lot
with it's interesting when you start
talking to joint coms there's a big
social network and they have their
entire address book of people in any age
case they've got 40 gigabytes in
education they put up with the GC pauses
I'm not sure how they how they cope with
that but that's an example of like where
something like big memory I'd eat that
up performance has been improved by
multiples and we're getting more and
more into the fine-grained trade-offs as
well as doing really hard stuff like
making two phase commit work properly
the other thing that's interesting we
see ourselves as being distinct and
separate from from no sequel because no
sequel is always across the network and
it and it's always a durable store it
gets a little bit blurred we have people
using us for no sequel use cases so I'm
not going to say we're a no sequel
solution but terracotta the terracotta
case can be set to be persistent and
we're also skinless we have obviously we
are key value get put and in the new
version that comes out in a month we've
actually added on object-oriented Search
API and the the performance
characteristics of the Search API Big O
you go login divided by the number of
shards so it's indexed but it has the
property of it as you as the case grows
and you add more and more servers that
your performance remains the same so
it's been a pretty interesting print a
pretty interesting ride I can tell you
just to whiz through a few of the
features in detail
there's the fine grained cap trade-offs
another one that's interesting that we
added was some caz methods so even when
you have
even when you set your cache cluster to
be incoherent you can actually still get
you can still get coherent methods put
if absent and replace using caz style
operations it's not always going to work
it'll return false if it fails to update
from a management point of view we've
actually are created an education
monitor which is a GUI monitor which
makes it much easier to visualize those
types of graphs I was telling you about
when doing when looking at case
efficiency and tuning there's something
like 15s about 15 or 20 statistics that
you can look at with case efficiency
being the main one there's also the
Terra Cotta dev console hibernate pH
case always been used with hibernate we
completely reworked that adopted the
hibernate 33 plus SPI those guys did a
pretty good job the maximum speed that a
case could run out when used by
hibernate with actually limited by
hibernate not by the case because
hibernate actually use synchronized
everywhere through three plus they
actually got rid of that and actually
move thread safety to the case where it
should have been all along so we've got
a whole new implementation that does
that we also support for the first time
we actually support all of the hibernate
features it's kind of amazing given the
number of people have used the education
with hibernate over the years but we are
never supported a subset of hibernate
strategies now now we support all of
them including but most importantly
transactional JTA as I said we did now
when you're using us transactionally
it's that's a read uncommitted that
should say read committed semantics its
read committed semantics is other
semantics that you get we do write
behind when when your data store a data
base is struggling to keep up with
writes we do bulk loading that both
special bulk loading that might I talked
about where there's absolutely no
coherency or no no effort at coherency
OHA it's about 10 times faster
we've got search I've spent a lot of
time over the last few months designing
this it's it kind of Nod to the
hibernate criteria API which I always
liked but it
definitely its own thing and it's
definitely very unlike a query language
for a database because cases are
different and we have metadata you can
search by key obviously you can search
by value you can also extract extract
fields and core methods in the values
and actually index those so it's a full
search we do text searching all sorts of
things it's quite a powerful language
you can create you can create really any
level of complexity in the query in
terms of what we return whether returned
keys a key set or we return the values
or we or there's a whole stack of
built-in aggregators so you can just
return the aggregator so so really what
you can do is search is you can probably
replace potentially thousands or tens of
thousands of cash operations with a
single query depending on what your your
business logic is I'm a really great
example there's a worldwide logistics
company and they generate 400 gigabytes
of data of consignment notes every two
weeks now they need that people need to
be able to search for the consignment
note they don't know the ID often who
knows the ID right so it turns out that
98% of searches are for the last two
weeks so if you could actually put and
then there a database that's dying under
the search light so if you can actually
take take 400 gigabytes of data
stick that in a terracotta server array
it's all indexed and then it's only the
2% that you miss that where you go over
to the over to the database so it's a
really good use case for search the
other one is big memory by the way in
case you are wondering that command line
is that is the command line that you
need to add when you want to allocate
off heap memory in the JVM and that that
switch works for all of the different
JVM implementations which is actually
nice given that it's a miners X X it's
actually at the moment the same and
we've tested all of them
now for the slides that get up uploaded
there's there's I've actually got
applied areas that actually show you
that work through web caching database
caching and what I call general purpose
caching which is using the case API
directly it just shows you how to work
through those and do those I'm not going
to cover those now instead I'm just
going to whiz right to the end and take
questions
and there's the further information you
can follow me on on Twitter or you could
follow e-education on Twitter for
announcements there's the Doc's like a
lot of open source people I consider the
documentation and the Javadoc and the
code or one thing so I actually write
all of the education Doc's myself and
that they they are representative of my
understanding of how it works
so hopefully most people consider the
documentation pretty good they're doing
a lot of work over the next couple of
weeks to go through and update all the
docs full of new features coming into 5
so 2 4
okay so rainbow there any questions
coming in bigger pun the sort of also a
fun question yeah I guess so
you know if your interest against the
park it's a question that gentleman
there okay so the question is is it open
source so eh case is is the Apache 2
license Terra Cotta server single node
is the Terra Cotta public license which
is which is an open source license so
what you download from SourceForge is
open source if you want to scale out you
have to buy a commercial version of
Terra Cotta
and there's some features like big
memory that are not included in the open
source but transactions and probably
about 90 95 % of features are in the
open source
so it's under the covers under the
covers in the Terra Cotta server it's
actually powered by Lucene index so it's
more about the convenience ah they got
to search the cache because often being
often if all you have is a key but you
need to be able to search the cache then
you really can't what are you going to
do you're going to go and build a
separate Lucene index on the side it's
it kind of makes sense there's so many
use cases where people need more than
just the key and it's loose seen at the
moment using Lucene as the
implementation is not without its
difficulties in terms of integrating it
in the Terra Cotta server so it made the
underlying implementation may change the
thing that will remain stable and will
get added to is is the API but it's some
from a performance point of view it
works it works much the same at the
moment it works much the same speed as
we've seen except that the key space is
distributed over the different the Terra
Cotta server array there's the cases
partitioned where the the partition size
is the is the total cache divided by the
number of servers so if you've got ten
servers ten active servers you've got
ten partitions so search is done using
scatter gather so we fire out the
requests the search is done for some
things like the the the aggregators then
we return partial results and they're
put together
once it's gathered back in but for other
things other things we just sum the
results together and pass them back so
it's I'm pretty excited by it actually I
mean I think a log n divided by a number
of partitions is pretty pretty nice
performance and has got that special
characteristic of true linear
scalability and that's what really
excites me standalone what we're doing
for standalone it's exactly the same API
the standalone standalone we're just
doing brute force table scans so people
typically don't like using the edge
cases des store because it's something
else to worry about
and so turns out that the case itself is
so fast that up to a moderately sized
case and we'll have to figure out
exactly where that limit is like ten
thousand or a hundred thousand elements
you can actually search just using brute
force but but anything large at all it's
only practical with the the server array
we're looking into the possibility of
actually creating indexed for standalone
as well but I'm just not sure whether
people would actually like that idea
there's a lot to go wrong you know I
mean like you get a corruption you got
to rebuild your index you're doing a lot
of work really in in an app in what for
most people they expect as their
application server node not a not a
database node or not a soul I know</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>