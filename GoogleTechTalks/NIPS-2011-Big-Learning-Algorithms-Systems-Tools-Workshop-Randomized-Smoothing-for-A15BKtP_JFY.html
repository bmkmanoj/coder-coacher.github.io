<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Randomized Smoothing for... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Randomized Smoothing for... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Randomized Smoothing for...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/A15BKtP_JFY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay what's that oh yeah maybe uh
sometimes my title changes its parallel
yeah is the mic working we good good
enough okay yeah thanks a lot for giving
me this chance to talk so this is a
joint work with my advisors or something
like that Peter and Martin I don't think
either one of them is officially my
advisor so let me start by just setting
us up so I want to solve the Catholic
optimization problems I mean I can I can
essentially write everything I do in
about 15 characters so I want to
minimize some function f of w subject to
w belonging to a constraint set and for
this talk i'm going to assume after some
expected function so either an empirical
expectation on some data set you get a
set of X's and you want to minimize the
average loss over your data or you're
just getting samples from some
distribution you don't know you want to
minimize that expectation so examples
we're all probably familiar with
logistic regression where the data is XY
pairs why if the label x is some feature
vector svms again data is XY pairs X as
the features why is the label so these
are the types of problems that I'm going
to care about so real quick review
you've probably all seen this or heard
it ten times but so stochastic gradient
descent what we do is it's an iterative
it's an iterative algorithm to solve our
minimization problem and how it works is
that at iteration T we we received some
gradient GT which is random but an
expectation is the gradient of art of
our loss evaluated at the current
parameters then we take some step in the
negative gradient direction with some
step size alpha T and that's going to
decrease over time so simple example if
we have this empirical data set where
we've got an average over our excise we
can just choose I uniformly at random
and that's our gradient so what does
this look like so here's the contours
for a loss function you know we start
here we sort of take random steps but
eventually
they shrink small enough and we kind of
converge to the minimum of the function
so what happens when we have kind of
like parallel computation so this is
something that I think everyone sort of
knows we should do which is to try to
get lower variance estimates of the
gradient right so if we start off this
is the true grain of our function right
here so points right down towards the
middle and we can imagine getting lots
of stochastic gradient samples and
averaging them so you know if we have a
fairly low variance estimate it looks
pretty good they point us pretty much in
the right direction so this is when M is
and the number of samples here is
something high and as you get fewer and
fewer samples we get more and more noise
and the gradient becomes less and less
indicative of where we should go right
and eventually we sort of I mean they're
almost random although an expectation
they point the correct direction so it
turns out there is a theoretical
justification for doing this although
it's pretty recent so the normal sort of
stochastic gradient rate that we've all
seen at least for not strongly convex
functions because after T iterations of
stochastic gradient our sort of
optimization error is one on square root
T give or take and then in the past few
years there's been a lot of work or
there's been some work on what happens
when we can sort of get reduced variance
estimates of the gradients so if we use
em gradient samples in our function f is
suitably smooth then we get an error
which is sort of scaling is one on T
plus one on square root TM so we've
essentially got this linear improvement
in the number of samples we get but the
problem here is that there's this kind
of caveat which is that we have suitably
smooth functions but there's lots of
non-smooth problems we care about right
so I think all of us like to solve svms
you know they're kind of nice things to
solve sometimes we might like to solve
robust regression problems because we
don't want to pay some huge error for
being too far off you know we want we
don't want to go as the square we want
to grow linearly in sort of our errors
of these two you might argue well I can
smoothies out you know I can use a Huber
law so I could use logistic regression
and their smooth but you know we also
care about structured products
where we have some some loss say say
we're like trying to solve find parse
trees ok so these we have some feature
mapping by of our data X and then some
structured label Y and we want to find
weights that so that sort of the
maximizing why within this feature
function is correct and I kind of I defy
you to find an efficiently computable
smooth version of this for most problems
I mean like from it say you're trying to
learn matching you know this set is
exponentially large it's going to be
really tough to get a CRF over the space
of all matchings in a graph hey but and
we really care about solving these kinds
of problems so to give some intuition
for why it's hard to solve non-smooth
problems or why sort of getting multiple
stochastic gradient isn't really helpful
I think this picture can help so if we
get a really low variance estimate of
the gradient of the absolute value up
here it's a pretty good indicator of
where we should go you know we should
well here the minimum is way far away so
we want to go fast that way we get a
really good low variance estimate of the
gradient right next to the minimum it's
going to tell us that you know it's
going to have the same exact gradient is
here but that's totally wrong it gives
no indication of global structure so if
you have lots of parallel computation I
think sort of the natural thing to do is
instead of asking you know instead of
asking over and over for the gradient at
this point let's ask for gradients or
sub gradients around our current point
so we start here and then we just sort
of randomly perturb our grade are the
point where we ask for the gradient over
and over and then we sort of get most of
the most of the sort of observed greens
to get point this way but enough point
back this way that it sort of
effectively smoothes out our loss
function and we get a much better sort
of picture of what the function looks
like you know now if I ask you the
gradient here it's going to tell me
don't go that fast when you go down so
we can do this in parallel if you've got
lots and lots of processors so what's
the algorithm well the normal so I'm
just going to try to tie this back into
normal stochastic sub gradient descent
so normally we have our weight vector W
and we sample some facts from our
distribution over exes at random and we
take the gradient and we go in that
gradient direction now our approach is
going to be to add random noise to W so
we just add some random perturbation we
take the gradient there and we decrease
the magnitude of them excuse me we take
the gradient there take the gradient in
that direction and then we decrease the
magnitude of the noise over time so what
this looks like it's essentially it sort
of smooth out our gradient estimates and
make some point sort of in a more
correct direction right so we went from
here to here and then as we get closer
to the optimum we decrease the amount of
noise we're adding in and just because I
would feel really bad about having to
talk without actually giving the
algorithm I'm going to write it here
it's a bit complicated if you've seen
accelerated gradient methods then it
should look mostly familiar if you
haven't then I'm sorry I won't spend
that much time getting intuition for it
but essentially what it is is we have a
sort of query point where we're looking
for gradients and then this exploration
point which is really aggressively
trying to minimize our loss function so
what we do is we we basically we
randomly perturb the point where we want
the gradients we get this average sub
gradient and then we have this kind of
approximation to our function it's some
sort of linear approximation and we
regularize it to make sure we don't take
steps in totally awful directions and
then when we after we solve for this
aggressive exploration point we sort of
interpolate between it and our previous
points and keep going so the nice thing
is that we have actually pretty strong
well reasonably strong theoretical
results for this so again this is our
objective we want to minimize this
expected function so in the non strongly
convex case where you just have I don't
know say robot an l1 regression problem
with no regularization or things like
that our convergence rate is one on T
plus one on square root T M where m is
the number of gradient samples we get so
if we compare back earlier this is
essentially exactly the same speed up
you get in the smooth case but now we're
getting it for non-smooth losses in a
strongly convex case
slight extension of the previous method
gives basically optimal results here so
if you're solving say and out to
regularize SVM where you're
regularization multipliers lambda now
you can vergence rated something
exponentially decreasing plus one on
lambda times t times the number of
samples you're getting of the gradient
base of this theoretically we're getting
basically the optimal speed up it's
impossible to do any better than this
for any convex stochastic convex problem
so just a few quick remarks on
distributing so you can basically having
if we have a if we have our computers in
a network or even just on one processor
communication can sometimes we did some
problems right you have communication
Layton sees and if communication is
expensive what you can do is basically
each of your processors you have them
compute a sort of a mini batch gradient
of size m and then you after they've
sort of computed these larger batches of
gradients then you average them then you
send them to your sort of master
processor whatever and it aggravates
everything and the idea here is that
sort of you take em large enough to
swamp the communication costs and a
little algebra shows that basically as
long as the communication cost isn't too
big essentially we get a linear speed-up
in the number of processors yeah so
that's basically the content of the
theory here so if you have any questions
about that you can feel free to
interrupt me but let's go into some
experimental results so first I mean
this is just to see like is our is my
theory accurate was I was I said it was
I basically doing the right thing and so
I'm going to define this T of epsilon m
to be sort of the the minimal iteration
until I have some epsilon accuracy in my
optimization problem and what our theory
predicted was roughly that until m got
large enough that these two terms were
about the same i should be getting sort
of linear improvements in speed and this
plot shows basically that we're correct
so the blue line is the actual number of
it
patient's it takes to get to some
epsilon accuracy as a function of the
number of sub gradient samples were
getting for this robust regression
problem and you can see that roughly
they sort of the prediction and the
truth matchup another thing in our
method so there's there's sort of two
parameters that we have to specify when
we run our method we have to specify the
amount of random perturbation like how
much how much noise are we going to be
adding in and we also have to specify a
step size right so step size selection
is generally fairly important in
stochastic gradient algorithm so we want
to know okay is our algorithm robust to
this like if I pick too much noise to
put him and I put in am I totally
screwed and the answer is really you're
actually pretty robust so this is a this
is a plot so ada is my step size here
what i choose in my step size algorithm
and then you is the amount of random
noise i'm adding in so one on you is
sort of some kind of measure of our
estimates of properties of the problem
and so this is a plot of our
optimization error after two thousand
iterations of solving this synthetic SPM
problem what you see is so this is a
range of ten to the minus 1 to 10 cube
so that's four orders of magnitude in
the step size and in the amount of
smoothing we're adding and basically for
almost this entire plot or optimization
error after two thousand steps is less
than 10 to the minus two and even when
we're like way far off in this back
corner we're still you know within point
one of the optimal solution so it's
really robust you could totally mess up
the amount of perturbation you're adding
in and the method is going to work just
fine which is kind of satisfying I think
you know unmess up of both methods are
pretty nice okay
in some problems there's actually not
you can actually get performance
benefits without even distributing so
for example if you're if we're doing
projected gradient and we have to
project to some complicated set then
sometimes I mean it it may make sense
just to aggregate gradients a lot before
projecting so this is an example of a
metric learning problem where we're
trying to learn some positive definite
matrix so that points and we're getting
points and we want to sort of have
related points be close together
according to the metric we learn and we
have to project the positive
semi-definite cohen which is much more
expensive than computing sub gradients
and this objective and so this plot just
shows the actual time in seconds to
learn a metric as a function of the
number of gradient samples and you can
see that you know up to getting 128 or
so samples we basically have huge
improvements in training time even
without doing any multi-threading or
anything okay but those are all
synthetic experiments so now I'm going
to go through a couple real ones on a
real like I think this is a 12 or 12
core machine with like 12 gigs of RAM
something like that and so this is these
are experiments on an SVM problem using
the Reuters RCB one data set which is
this you're trying to classify news
articles from the Reuters news service
as to whether they're about economics or
government or something like that and so
this plot shows the number the amount of
time it takes until we get to an optimum
optimality gaps on our problem until we
have optimized to a certain accuracy
right and the red line is when we use a
batch size of 20 on each of our
processors right so we had that picture
each processor now is computing 20
gradients and then sending them in and
the blue line is with the batch size of
10 and we see that I mean essentially
you know the time to actually solve this
problem goes down and down and down
until we get to about six or eight
threads and when you actually require
more a more exact solution it you still
get more improvements by adding more and
more threads and this kind of matches
our theory as well that if we increase
the batch size we actually get some
benefit because the communication
latency doesn't hurt us quite as much
and just another view of this data said
this is a plot of the optimality gap as
a function of time in seconds for using
different numbers of threads and this
black line at the top is Pegasus which
is shy shell of shorts and yorma singers
sort of svm algorithm which i guess is
roughly state of the art for single
processor solutions and you can see that
even with one even if we're only using
one processor we actually do better than
Pegasus which was kind of surprising to
me but uh it's sort of a nice result
that this even helps if all you have is
one single processor to solve these
problems and I also have some results on
learning a parsing model so this is a
these are tests learning a hypergraph
parser for using with the penn treebank
data set so the goal is you know you're
given so here are points X the the data
are sentences and why our parse trees or
hypergraph parses and this is just the
and so this this problem is really nice
for a sort of our our setting because
actually computing the objective at all
takes a while right so each of these
hypergraphs has something like I can't
remember off the top of my head but tens
of thousands to hundreds of thousands of
edges like each individual hyper graph
is on the order of a megabyte so it
takes a long time to I mean a relatively
long time to even compute one objective
so doing this random sampling is really
helpful because we don't have to worry
about communication and things quite as
much and so on the Left plot I'm showing
the essentially the gap and the
optimality of our algorithm as a
function of the number of iterations for
different numbers of threads one two
three four five six seven eight or one
two four six eight and we can see that
the more threads you get the better the
better off you're doing and this plug is
the actual amount of time in seconds it
takes to do this and you can see they
don't totally match up sort of at the
beginning all of the multi-threaded
versions are sort of right on top of
each other and then eventually they
separate and I think this is roughly
because I mean when you you can only
load part of this data
I into the memory of your computer it's
about 40 or 50 gigs if you actually load
the whole thing Thanks and so there's
some memory latency is that I haven't
totally figured out here but that's
still a substantial benefit using more
and more threads and this is sort of the
Pegasus type algorithm with a single
thread yeah so that's a that's about it
for my talk I should acknowledge my
collaborators so Martin Wainwright and
Peter Bartlett my co-authors and then
sloth Petrov and Sasha rush it Google
helped a lot with the NLP experiments
and your arm singer and Mike Jordan gave
some feedback to earlier versions so
thanks a lot for your attention
I have a is just a multiplier of the
learning rate I didn't want I didn't
really write it here this decision yeah
there's there's one right in it it is
one specific might agree that you use
its best time they were saw you two yeah
it probably increases it 100 te I didn't
put up at the pharmacy
right
I
it certainly applies to that setting
because you can you marry a corporator
regularizer risk if you can drag the
incorporate a regularizer into this
update it just appears here and put your
image makes the slide with universe
this is
oh yeah also visit so I'm excited Russia
Mexico is any kind of fun I mean any
kind as long as it's a density with
respect to a very measure they're going
to basically tactics I mean there's some
weird thing mentality and
price
very generous
the old days the desert is sort of the
central limit theorem which showed that
the averaging trick and even mentions a
chimeric worldwide you know started
doing allergy
just
many practice people started showing
she can do better enough
what always
with the new advancements that you've
recorded theory
it would be one will be able to put
these out the difference
this domain sebastopol
and if they do then we come up with
just a minute yeah that's it was I need
to be hard in mixing these guys about to
get a new method because all my problems
are not so there's always green exactly
yeah I don't know
actually
who is working
really something
so you are way to becoming your general
actually my mystic city that
results
that's after them asking you just
scooting with a gas composition
sort of a NASCAR that's different I on
that smooth optimization not a function
for something like that I insert smooth
process
Mr Speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>