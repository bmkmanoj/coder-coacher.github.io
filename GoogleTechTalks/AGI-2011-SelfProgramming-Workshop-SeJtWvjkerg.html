<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: Self-Programming Workshop | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: Self-Programming Workshop - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AGI 2011: Self-Programming Workshop</b></h2><h5 class="post__date">2011-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SeJtWvjkerg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">for those of you who just wandered in
here randomly and weren't here yesterday
I'm I'm been Gert so from from Nova
Monta and one of the organizers of this
conference series what one that I saw
that there was a workshop organized on
self programming at first I thought to
prepare a presentation on how my OpenCog
AGI system eventually will do advanced
self programming and then rewrite its
own source code and so forth which is
not something that we're doing in the
system right now
as those of you who were here yesterday
saw were involved with making the system
do do considerably simpler kinds of
learning just learning to cope with a
simple virtual environment and learn and
reason about about what it's doing in
the virtual world are making making a
robot solve simple problems in in a
robot lab on on its own rather than more
advanced topics like modifying its own
source code we're not nearly there yet
but as I began to prepare that paper I
started thinking more about what does
self programming really mean I mean how
do you distinguish self programming from
learning because after all and anytime
that an AI system learns new information
in a sense it's doing some kind of self
programming because what it what is your
what what is the difference between the
knowledge that you've acquired about the
world that guides your activity and your
programming is that really a crisp
distinction or is it a fuzzy distinction
between your knowledge and your
programming so I I wound up spending
some time trying to clarify that to
myself just what's the difference
between learning and then self
programming and ended up just submitting
a short paper on that theme trying to
clarify what the self programming
actually means and I may not have the
same understanding about that as
as everybody else but I'm gonna discuss
for the next few minutes is just what
one view of what is self programming
which views self programming as a kind
of learning basically just learning
about the features of a system that are
most critical for its intelligence so
the view I'm going to present is self
programming innately doesn't exist as
distinct from learning you can learn
about various things some things you
learned about maybe kind of in essential
to your intelligence like a phone number
or how to play tic-tac-toe or something
some things you learned about like say
how to think about mathematics or how to
feel about your relation to other people
these things you may learn about are
more critical to your intelligence and
your approach to the world and I've used
self programming is learning about
intelligence critical system features
then in the in the paper associated with
this talk I went through some effort to
kind of mathematically formalise what
that means what is an intelligence
critical system feature so after briefly
presenting that then I'll address the
question of is there is there something
fundamentally different about learning
about intelligence critical system
features versus learning about anything
else and that really depends on on your
AGI architecture I think in in OpenCog
there isn't really a profound difference
there look at least in in theory in the
architecture learning is just learning
and we we believe that as the system
evolves and follows its developmental
trajectory it will be able to learn
about itself when it's intelligent its
critical features using the same methods
that it uses to learn about simpler
things no of course what we'll see how
well that works when we when we actually
get there there could be other AG
architectures in which learning about
intelligence critical system features is
done by significantly different methods
than learning about simpler kinds of
of content and maybe we'll hear about
some of that in the other talks so just
to start off introducing you to my train
of thought I considered two different
cases so you can consider the case of
two different AI systems that do the
same thing but with a different
underlying software architecture system
a in this thought experiment system a
contains it's kind of an agent operating
system you have an infrastructure layer
that just runs AI agents and they can
interact and adapt with each other now
system B is a more specifically
hardwired AI system which contains say
10 cognitive processes and one doing
reasoning one doing learning one doing
concept creation one doing language but
then suppose in your agent operating
system system a you implement 10 agents
that are identical to B's hardwired
cognitive processes and what you have
here is in case B these 10 agents are
part of the infrastructure in case in
system a those same 10 agents are
cognitive content and their agents and
their program is content within the
system so if the same 10 cognitive
agents are infrastructure for B or their
cognitive content for a if those are
adapted by learning is that learning or
self programming right then in the most
glib level you'd say well in case a this
is just learning because the system is
just modifying cognitive content that
it's running in case B you'd say that
modifying those agents is self
programming because it's modifying its
infrastructure but really it's quite an
arbitrary distinction right it's it's
the same thing you're just doing using
two different architectures so if you
really think about it the difference
between learning in self programming in
the in this case it would just come down
to a trivial distinction which isn't
really what you want and when I think
about that in terms of our OpenCog
system it becomes quite concrete because
you can you
a cognitive process which we call a mind
agent acting on our atom space store of
knowledge in two ways I mean you could
you could program in C++ you can hard
code something like a logical reasoning
system or a procedure learning system
inside the system's C++ code or you can
implement the same thing in the language
called combo the previous version of
Moshe wrote and I mean that it could be
the same process in the case where it's
a combo program that agent is
implemented as cognitive content so the
system can reason and learn about it
using its current methods in the case
where it's written in C++ it's part of
the system's infrastructure right now
the system has no way to modify that but
in future we could enable the system to
learn and reason about its C++ code and
then it could so that's that's the in
this case the difference between say
learning to modify the the combo schema
or self programming to modify the
underlying C++ code again that seems a
kind of trivial implementation level
distinction and you would like the
difference between plain old learning
and self programming to be more than
just a distinction about the the
particulars of the software
implementation so the two cases were
OpenCog modifies it's PLN inference
process implemented in C++
which it now can't do we have an
instrument to modify its ooh plus plus
code or case B where it modifies its
inference process implemented as a
schema in combo really like logically
and in terms of AI those are the same
thing we don't want to call one of them
learning the other one's self
programming so how do you work around
that well I decided to take a different
approach think about self programming as
a fuzzy quantity so if you can measure
the intelligence of your system in some
way and if you can somehow divide the
state of your system into fairly
discrete features
then you can think about the degree to
which the feature ex is self programmed
as sort of how much does the system
learn about that feature X x how
critical is the feature X to the
system's intelligence and in this way
you can just view self programming as
learning about intelligence critical
features so as a human being like if if
I learned about how to throw this thing
up in the air and catch it without
dropping it that's not that critical to
my underlying intelligence
whereas if if I learned about say how to
how to better apply Bayes rule to
aspects of my personal life right then
that that would be more critical to my
intelligence and so that would qualify
as self programming to a greater extent
so if you accept this then there's a
question of how do you define those two
qualities and I talked about that a bit
in the paper
I mean intelligence criticality you can
basically think about the intelligence
criticality of a system feature X is how
useful is that feature for predicting
the intelligence of the system and then
you can you can formulate that
mathematically presumably how well I can
throw this up-and-down is not going to
be that useful as a feature for
predicting my intelligence in the
environments I usually operate in
although in some environments it could
be if the only way to get food was to
throw something up and down and then hit
the food maybe that is critical to my
intelligence but then in everyday human
environments it isn't on the other hand
being able to use Bayesian inference to
reason about social situations may be a
better predictor of my intelligence and
of the intelligence of similar systems
then you can formally define the amount
of learning about something similarly
just how much does adapting your ability
to do that thing correlate with you with
your intelligence and how much does
learning about that correlate with
improvements in your intelligence and
again for some things you could learn
about that correlation exists
for other things you could learn about
how much you've learned about that thing
probably doesn't correlate with
intelligence of improvements very much
so I won't try to go over the math here
which is fairly simple but I went
through some effort to formally define
those things so in practice if you want
to measure the degree of
self-programming in a system you could
choose a randomly selected act of
learning the system does choose it with
probability proportional to how much was
learned and say well okay how
intelligence critical was that thing so
that if most of the learning is about
things that occurred to the systems
intelligence that would come out high if
most of the system's learning is about
things that are not critical the
system's intelligence the system isn't
doing much self programming so this is a
sort of viewing self programming as a
case of learning which is just learning
about things that tend to have sensitive
impact on the system's intelligence and
then the question that leads up to is do
you need special learning mechanisms to
learn about intelligence critical things
or can you really just do it using
generic learning and then in an open COG
context I come back to to the Piazza and
stages which I'm so fond of and you look
at the hierarchy of more more abstract
types of learning from infantile
learning which is just making sense of
simple goals and in your sensory motor
reality through concrete learning which
involves a sense of self and then
reasoning about the specific things in
your environment formal and abstract
learning then when you get to the
reflexive level which involves a deeper
learning about yourself and you in your
relation to your environment then you're
getting to the level where self
programming becomes more more profound
and the system once it reaches this
level which OpenCog in any other
existing a GI system is certainly very
far from but once when you do get to
that level you have a system where it's
able to do a lot of learning about the
more intelligence critical aspects of
its of its function and
in terms of an uncertain inference
system you're talking about having a
logical inference system that can learn
learn new inference rules learn new
quantitative truth value evaluation
formulas learn new inference control
strategies that are pertinent to
different topics say it as inference
control strategy for inferring about
social situations an inference control
strategy for doing inference about
mathematical theorems as separate from
say scientific content once you had a
system that can learn new inference
rules and inference control strategies
from experience and from abstracting
from experience then you have a system
that's doing reasoning about things that
are really critical to its intelligence
and we have a high degree of self
programming according to the the fuzzy
conception of self programming that I
presented which is basically given given
by that
that's simple formula so this this is
one fairly rigorous way to think about
self programming which seems to make
sense in the abstract and to tie in with
that with particular things that one
could it with a AGI systems like
reasoning and OpenCog um I don't think
it's the only way to think about self
programming but it seemed to clarify the
concept to me and I'm actually I'm
curious to see how the other speakers
have have construed the concept which
we'll see in the subsequent talks all
right well that was that was 19 minutes
including fiddling with the laptop so
that's not bad
good morning my name is Brandon Rohr and
I'm from Sandia National Laboratories in
Albuquerque New Mexico and I'm going to
be talking about self programming in the
context of an implemented architecture
that does both feature creation and
general reinforcement learning let's see
if I can there we go
so most robots today have one thing in
common which is that they work well in a
very structured environment that is with
a known model or in environments and
tasks that are easy to create models of
on the fly my goal is to enable robots
to work in environments that are very
difficult to model in complex pursuing
arbitrary goals specifically more
loosely speaking to do everything it is
that I can do including cleaning up or
fighting my battles or building a robot
or giving a talk interacting with other
humans and the term that I put on this
is natural world interaction this is a
specific subset of the goals within the
broader AGI community you notice I
didn't say anything about biological
plausibility my bias is that the most
likely way to do this is by mimicking
how the brain does it and how people do
it but if I can figure out another way
to do it I'm perfectly satisfied with
that and the current state of my work in
this area is captured in a brain
emulating cognition and control
architecture Bekka and you can see it in
block diagram form here there is an
unsupervised feature creator in tandem
with the reinforcement learner and you
can see the whole thing is set up to be
reinforcement learning paradigm
observations are taken in actions go out
and there's also a reward signal Beka is
set up to make very few assumptions
about the tasks that it performs or the
hardware that it's implemented on one of
the few constraints on this interface is
all the inputs and outputs are vectors
values real values between 0 &amp;amp; 1 each
one represents the presence or absence
of an attribute or of an action so there
you can think of them as binary but
they're they can be scalar valued I just
want to point you right now to the
website at the bottom of the screen
Sandia gov / Rohr all of this
information papers code videos it's all
there feel free to pull it up any time
so what I'll do is I'll walk through the
pieces of Bekka and then give some
examples about tasks that I've applied
it to so far the unsupervised feature
creator and reinforcement learner have
both been created in my lab the reason
is I wasn't able to find anything that
does what I needed them to do I expect
that someone probably someone in this
room will create better solutions for
both of these at some point at which
point I'll happily replace these and
I'll describe what it is I need them to
do here in just a second the feature
creator takes in observations and
separates the input vector into groups
so into subspaces and it does that based
on inputs correlation with each other so
inputs that tend to be correlated or
broken down into subgroups of inputs and
then each of those groups within each of
those groups commonly observed features
or commonly observed patterns are
defined as features then at each time
step as the inputs are processed the
inputs vote on which feature is active
at that particular time there's a
winner-take-all process and within a
given group one feature comes out as the
winner those are all combined and passed
on to the reinforcement learner these
feature activities are also real values
between zero and one there they look
exactly like inputs so there they're fed
back and concatenated with the input
vector and they become inputs to this
grouping and feature creation process
again this allows for the creation of
hierarchical features of increasingly
complex
over time it's entirely unsupervised
driven only by frequency of observation
so the characteristics of this that are
desirable are that it's driven by
experience it's all incremental and
online it's suitable for use in a
behaving robot which is a constraint on
all of this work
it continues lifelong learning there's
no separate training phase and it can
handle high dimensional input spaces
well because it breaks them down into
subspaces before it begins processing
them so it doesn't get bitten very hard
by the curse of dimensionality it's
biologically motivated which I won't
bother to argue right now but it's fun
to talk about over beer it doesn't
assume very much about the task or
hardware and I want to emphasize that it
works with multiple and mixed sensor
modalities Beca doesn't know anything
about where the sense what type of
sensors produce these inputs whether
they're pixels and and it represents
whiteness or whether it's FFT processed
audio or a bump sensor or radiation
detector or the presence of a certain
concept as determined by some
pre-processing step and the
reinforcement learner also has some
unique features some unique attributes
so the feature activities from the
feature creator are passed in at each
time step the first thing that happens
is those are associated with the reward
at that time step features that are
consistently present when a reward is
received are noted also there's an
attention step where one feature is
selected to attend based on its salience
and that attended feature at each time
step is combined with recently attended
features with decayed versions of them
and so you get a short recent history of
what's been attended the current
attended feature and recent working
memory are combined in a model of the
environment what that allows you to do
is note which if you have observed a bee
and C and then D followed that
transition is an example of one an entry
in the model you can think of it like a
tabular form of first-order Markov
transition model where you can extract
the transition probabilities based on
the statistics of your entries using
that model you can look at your current
working memory and make predictions
about what's likely to happen next
conditional on taking various actions
and then based on that the action
selection block looks at likely outcomes
the axons actions associated with them
and the rewards associated with them and
chooses either a greedy action or an
exploratory action as a result and then
sends that out to the world so the
features the attributes of this
reinforcement learner that are desirable
are that it is also incremental and
online it learns which features are
rewarded and which are not
also it continues to learn over its
lifetime there's no separate training
phase it's driven its behavior is driven
entirely by the task at hand but it
assumes nothing about that tasks to
start with so the status of Bekaa
development is right now all of the code
is written in MATLAB it's all
downloadable I think the most recent
version is a couple weeks old there's
new versions go up usually every couple
months
it is researched code honestly that
caveat but I do try to make sure that it
works so you can download and pull it up
in MATLAB and run some of the examples
that I'm about to show you it's
separated into agent code that
determines the behavior of the feature
extractor and the reinforcement learner
and tasks code there are separate
folders each containing a different task
that you can select between the ultimate
plan is that individual users can
download this and then create their own
tasks modeled on those tasks using the
same interface we're in the process of
porting to Python this is motivated by a
desire to interface with the robot
operating system raus it's becoming the
closest thing that the robotics research
community has to a standard and because
becca is in
to be very broadly applicable embodied
approach we want to be able to get it on
as many robots as simply as possible and
if you've ever worked with hardware you
know that 98% of your pain is just
getting the software to talk to this
stuff so something like Ross is really
appealing so in the remaining time I'm
going to go quickly over some of the
simple online tasks sorry simulated
tasks that Becca has behaved on and also
mentioned that our robotics work will be
presented tomorrow night at the demo
session there are two students sitting
in the front row here Shawn Hendrikx and
paint broker they've been working in my
lab this summer and have been working
hard on integrating it with some robots
we brought those robots and we'll be
showing them tomorrow night doing some
simple behaviors but with with this
cognitive architecture under the hood so
this is an example of one degree of
freedom task there is a white image with
a black bar in the middle there is a
frame a field of view that mecha has
this square and it can move it up or
down depending on the position of that
field of view there's a Associated
reward and you can see here then it gets
rewarded for centering this window on
this black bar again it doesn't know
anything about this the structure of the
task
it just gets vectors of inputs and
actions and rewards at each time step
and so what you can see here is over the
course of time there's quite a bit of
exploration that goes on as the groups
are being formed the features are being
created the reward map is being
populated the model is being created and
then in this case after about 5,000 time
steps you can see that learning then
begins to pay off and the position tends
to cluster much more tightly in this
region here where the maximum reward is
received and you can see the average
reward jumping at that time one of the
benefits of this approach as opposed to
a lot of model free reinforcement
learning approaches is that the model is
the same
even if I were to suddenly change and
reward this for moving to one of the
extremes it would keep all of its
modeling information and it would only
have to relearn the reward map that's
different than q-learning for instance
here's an example of this shows in this
task the features that are created and
you can see that they look like a black
bar at various positions within the
field of view and there's 12 of them and
if you look in the representation of the
reward map
this shows the reward that's associated
typically with each of these features
and you can see these where the bar is
essentially in the middle are rewarded
most highly this plot here looks a whole
lot like this blot here showing that it
learned the reward map pretty accurately
and that the features that it's using
are useful in accomplishing this task
here's here are some examples of some
transitions from the model so if it sees
has this input and moves its frame
downward the black bar appears to move
upward this the model is rendered so
that we can interpret it in terms of the
task internally Becca doesn't know what
it is it doesn't know this is an image
it doesn't know what it means but when
rendered for us this is this is what is
storing based on its experience here's a
two degree of freedom version of the
same task now I can move vertically and
horizontally and you can see here that
after an initial period where it learns
the groupings creates the features
populates the reward map populates the
model it begins to bring that position
in close to the center and stay there
where it's maximally rewarded here are
the groups and features are a little bit
more complex down here on the bottom you
can see the groups that it created if I
remember right there's a little more
than 50 of these and these are you can
see the the pixels that tend to be
correlated with each other also tend to
be located next to each other within one
of these groups you can see the various
features that were extracted so these
are commonly occurring patterns within
this first group of pixels here it's
interesting to note these are groups of
pixels and these features if you look
over here carefully at this feature and
our sorry at this group and this group
for instance these are actually
combinations of groups that tend to be
correlated so it's an example of
hierarchical feature creation similar to
how in the human visual system in v1 we
may create short line segments than
moving to v2 and to v4 into the media
temporal area those visual features
become more complex and richer this is
designed to do the same thing and here
are example of some excerpts from the
model created in this task as well
here's a very simple one degree of
freedom task B let's see if I can make
it play so the dial can step right or
step left I promise no
I even tested it the demo demon could
see every time so uh I'm gonna skip that
because it's not not even the best one
see if we can get any of there we go
so here's an example of a two degree of
freedom task each of these robots can
move the elbow and move the shoulder and
the goal is to get the hand on that
little target so you can see initially
this is like a newborn has no idea what
motions produce what changes in its
sensory input it wanders until it
stumbles onto the target and then
restarts and does it over again in this
case it's been learning for a little
while and it moves haltingly but more or
less directly to the target in this case
after additional learning it moves
nearly optimally to the target here's an
example with a seven degree of freedom
robot based on based on a robot we had
in the laboratory power cube and we
started working on just gripping a
little block so it's just expand
exploring that one degree of freedom and
what changes it observed in a sensory
information when it when it gripped that
block and we expanded that to a planar
two degree of freedom actually three
degree of freedom gripping task and and
it was able to learn to move too and
grip the target pretty readily and then
it was extended to a full seven degree
of freedom task initially codec
unavailable that's cold okay so
initially it wandered in an exploratory
way it was not able to complete the task
at all and then after starting it close
to the goal so it could explore and
stumble on to the block when it became
successful at that it was moved back and
moved back until finally also
unavailable you could start it far from
the goal and approach the goal in
starting at a random position far from
the goal and it would be able to achieve
the goal even and occasionally it would
miss but it had a good enough model then
it would correct and get it so as I
mentioned these are all simulation
example
the real goal are really excited about
is hardware and you'll be able to see
the initial implementations of that
tomorrow evening it looks like there's
just a couple of minutes left I'd love
to take a question or two yes yeah right
now right now it is it's a big matrix
there's an incremental estimate of
correlation that gets updated every time
step it's it's still by far not the
slowest part of the code but it could be
an issue but it's the type of thing that
would be well addressed by the high
performance computational tools GPUs
things like that big matrix operations
yes can you say that again how is it
compared to so this feature extraction
method because it's constrained to be
online and incremental it's very dumb it
looks in the subspace of that group
and it looks at the essentially the unit
vector that represents that feature and
if a new feature comes in that's
sufficiently far away it adopts that as
a new feature so it says a version of
imprinting is commonly used in vision
processing but its goal is not to
represent the inputs in some optimal way
but to spread those features in some
uniform way across the input space
that's used there's lots of regions of
that group space that actually never you
never gets to so it doesn't have to
create features to cover it I'm not
familiar with the neural gas algorithm
so I can't comment on that sorry yes I
would be interested in learning about it
okay as you can see my topic is about
how to tour this self-programming
by a reasoning system so first I will
gravely clarify what I mean by self
programming what I believe is the
following is kind of like a
straightforward Wow
something weird happen and stunning the
first fly is supposed to be definition
about what I call a basic action so
anyway the idea is for any computer
system you already you can analyze is
activity in terms of a small set of
small as the unit of what the system can
do we can we can call that a basic
action and then using those things we
can form something we call a program a
program is just those actions are
organized together yes or no outer or
something and then then the next one is
about what is programming programming is
is the process of putting actions into
into programs then what is self
programming well self programming is to
do this process not a by a human
programmer but by a computer itself
right to me that's kind of like a common
sense understanding about what self
programming means you know it sounds
very simple as to something we need to
clarify one important issue to me is we
all know that computer system can
typically analyze to be analyzed or
described at different levels of
description as different levels of
virtual machine so that means we can
analyze the actions and program also a
different level so otherwise familiar
with you know those levels
so a different level of what we call
action and what we call program are
actually all more or less different from
each other and sometimes they use
different terminology for them even
though they kind of share the common
property is a mission before it's kind
of like action is the basic unit okay
program is the bigger unit which we use
to handle practical problem so the
basically means cell programming cards
will be happen at any of those levels
also that means when we say cell
programming it didn't necessarily mean
to modify its own source code even
though that's a possibility
yeah so five sample our source code
typically happened when we say it you
really mean the coded high-level
language level if you're self
programming happen above that it's a
knowledge representation level and then
it doesn't really change the source code
so I'm Philippe already to address that
issue also according to this
understanding many existing AI or ATI
technology communities different
approach toward self programming you can
see that this list each of them actually
are based on very different assumptions
I use different measures but yes since
they can also see is different attempt
okay to start with a small set of fixed
basic actions and then find a way to
organize them together so in this sense
we can call them a different approach
toward self programming and also address
the issues of their mission what's the
difference between some programming and
running according to man's Danny
well this who of course have some
overlap of course a related to each
other well just like everything in AGI I
relayed her to each other but still are
the two terminology to me have different
different focus okay like in my list the
last three are typically considered
running the first real typically thought
okay of course they are related
so that's that's the about my
understanding about the problem then the
solution proposed so how to do that or
use reasoning this is clearly a big
topic I don't have the time to go into
details so the paper is online and also
there are other related paper so
basically the idea is also something
Pangos or mentioned yesterday I've been
working on a project called NARS
the would make it different from many
other ATI system as I try to use
reasoning to almost everything which of
course will include how to handle
appreciations so the basic idea is first
to treat operation is something that can
be handled by logic so to me Ora nurse
operation is a statement but is
executable okay so it's very similar
well actually the idea come from logic
programming like Prolog so you can have
a procedure interpretation of a
statement okay so you can actually make
something happen or make a statement
rule like you can walk from one place to
another I mean for a robot or some
similar system and then you can define
different types of relationship okay
here the most important one were pool
while the tool is holding a heart
installation which represent general
relation between operations for example
we can say a walk is a special form of
of move okay move it's more general
concept than walk even though both of
them describe operations there are
another one in ours is called
implication which is kind of like if
then replies so that one can be used to
describe the condition or an effect of
operation like for example if you move
from axle two while after that
we'll be at why but the condition for
that to happen is you need to be a taxi
at the beginning Robbie speaking well oh
this should look familiar to many of you
because many other technology used
similar teams but what make math a
little bit different from many of them
is allow all kind of uncertainty so of
the relationship or all the statement in
the system are a tool to a degree okay
and furthermore this uncertainty is
measured by a kind of two dimensional
whose value I use two real numbers
between zero and one or to indicate the
statement uncertainty the first one and
both of them measures are kind of like
evidential support it's Caroline
according to the system's knowledge to
what degree this statement is true the
first number is called frequency it
measures how much positive evidence you
have amah evidence first of course I
assume the amount of evidence can be
defined and measure so it's kind of like
probability but it's not the limit of
the ratio okay it's the current
proportion according to what you already
know at the moment and the second value
is what I call confidence but that's
very different from the confidence
interval in statistics even though it
has some similar intuition behind them
so the second measurement is used the
total amount of your current evidence to
compare the total amount of evidence you
are going to get in the near future
defined somehow so it's kind of like how
much you already know about this thing
so
so that's kind of like the basic
components in knowledge representation
then how about reasoning well in a
system there are whole bunch of the
inference rules which include the
deduction induction abduction derivation
Papa a long list each of them has a
truth value function attached so it can
calculate to what degree the conclusion
is true according to a law degree the
premise true and also depends on the
type of inference deduction is kind of
straightforward because both are the
inheritance and the implication I
defined before a clearly transitive so
now a become if it's not in battery
logic it becomes transitive to a degree
especially if I'll have a two
dimensional truth value how to calculate
that induction abduction and analogy a
more special in my system okay
it's kind of like based on evidence for
example if the system see event a
followed by event B by induction it will
form a hypothesis right usually a May
will be followed by B the next time it
will see a I will use deduction to make
expectation say P will happen after that
if that indeed is the case then that
will be positive evidence for the
previous hypothesis and the revision
rule will make frequency higher and
confidence higher if it's negative
evidence or if that B didn't happen
doesn't happen after that then the
frequency will be decreased but the
confidence will still be increased
because you have more evidence after
that so that's the basic idea for all
the details are see the paper so so you
can see that with those the inference
you can begin to learn or the system can
begin to learn the condition and effects
of each operation you don't have to
teach everything to the system or defy
everything to the system at the very
beginning okay
and also for the more learning is also
carried out by inference okay so there
are some inference rules you see a
mention in first item at the end so they
not only derive new conclusions
they also come form new content
operations okay so to comfort to compose
compound operations means C form some
structure for example if the for basic
structure in program language sequential
parallel conditional recursive which is
also responsible for loops right so you
can see that all of them can actually be
derived in this kind of logic okay so if
you know operation a and have them read
out and then followed by B then you'll
see a B as a unit will have some
condition and consequence so in the
future you can take that unit as a
Parisian by yourself okay so in this way
you can recursively build a bigger and
bigger and bigger unit that to me is
cell programming so so you can see that
reasoning can be used for many different
functions as at least here clearly I
don't have the time to explain or
justify this claim but he finalized the
interest that I can explain later
afterwards how each of the functional
mission here can be carried out by not
use reasoning so compared to the other
technologies of South programming what's
the property of this new approach why I
think this is a interesting way to do
things well the number one were the most
important assumption of nerves is the
assumption that the system always work
this is sufficient
yeah you never have enough knowledge for
the task what you already except in very
trivial or special cases and also it
never have enough resource to trial
possibility concretely in have finite
processing power I have to work in real
time will make real time response and
need to open all kind of new import
tasks a new knowledge which may be the
task may be beyond the system's current
capacity and the knowledge may conflict
with the system's current knowledge okay
so that's the basic assumption which
everything else kind of follow from this
assumption so clearly as I explained the
system is highly adaptive learn from his
experience is contact sensitive in a
sense that for a given task you already
in it and didn't follow a predetermined
algorithm with respect to that task
instead I will just use whatever
knowledge and resource available at the
moment are trying to find the best
solution and here perception and action
are both taken as operations and also
operation can happen or that perception
is also is organized as a hierarchy so
it can be it can happen a different
level of granularity and scope
operations goes and events are all
handled is a special type of statements
so it can't become the object of a
reasoning system and so you cannot only
reason on propositions you can also do
inference on flags and vocals and and
actions and events as special type of
statements another thing that makes it
mega gnar's different from say the tool
a GI system covered yesterday ok as you
see here I don't have different type of
memory even though I agree that your
different type of knowledge
like declarative episodic a procedure or
whatever but in ours the approach is try
to use the same format for all of them
and use the same memory metronym to
store and process of them on the other
hand in semantics we still operate you
you know there is clearly a difference
between different type of knowledge and
there some special treatment hero there
but overall there is only one memory and
also you see operations are analyzed as
a hierarchy not always I don't think
it's a good idea to always to planning
or self programming it's the basic
operation level even though at the end
of course everything will be reduced to
the basic actions or operations whatever
you call it okay but planning or
perception both need to happen at
multiple levels of description and also
the system should be able to learn a
causal relationship between operation
and related events by learning an
inference
another thing that makes our special is
multiple type of inference it's not
deduction only or abduction only or
induction least many of them unified in
the same framework and also that's also
a big claim I don't have the time to
justify but you can see the basic idea
is used to or to use a extended sense of
reasoning it's not the the normal time
of reasoning I already extended the
concept of reason in nerves in several
different ways
but it's still reasoning in a sense that
the process does nothing but produce new
knowledge from given knowledge according
to to make independent rules or patterns
in that sense is is reasoning but the
logic is very different from say for
solar particles or normal Authority
cloudy or probabilistic logic or
fidelity similar to all of them here or
there but overall still were different
because of the basic assumption and also
you can see that some for example in the
study of robots a reactive and a
planning based are about a typically
agreed it's a very different approach
okay but from a north point of view I
can be seen as a Canova's extreme
situation a continuous approach okay so
that well again if anyone's interested
we can talk about that in detail I will
show you that the both reactive I'm
planning can be handled by now so
special case
and also you see that some of the
operations will be executed by the
system yourself but some other
operations actually can be executed by
some outside device so we can actually
build something what I call a hands
passports model so what do you have
built in are some basic operations kind
of like our hands okay but it can be
used with the tools which other things
like not part of the system but you can
hook it to the system for a given task
then the system begin to earn the
meaning of the operation which means the
precondition a consequence but that
thing is not really part of the system
except in a certain short period of time
so overall the the goal well this
respect to this discussion of doors is
to build something like a
general-purpose mind it can be put into
different special-purpose body to
function for example
oh I hope rental I everything I
mentioned is already implemented in the
sense of the inference rules I haven't
really looked it up with for example our
robot even though I did some very simple
simulation ok but that's the plan for
this for the next stage ok so if anyone
is interested we can talk about the
detail so anyway if this approach
actually works and even surely I hope we
can have a general perverse reasoning
system which can be plugged in into a
different system and control the special
perverse sensory motor mechanism and
some other special purposes
software and hardware ok that's all
I don't think I have any time for
placement sorry but we can do that it
during the break or in in the second
half of the workshop thank you
good morning my name is Wojtek Tsukuba
the AG now is a privately funded
enterprise the title is the extraction
program space for the aging our
cognitive architecture it is a title of
my paper submitted to the main
conference I will focus more on on the
topic of this workshop meaning
self-affirming of which heuristic self
just only powered off we are using a
robotic embodied approach to be AGI
using nail robot from Aldebaran robotics
and since the robot is what has a
limited power we use an external host to
communicate sensor and actuators and all
the processing is done on a host and the
robot is only like a fire front to to
communicate this and so it activators
information so effectively it is a basic
setting for reinforcement learning and
the objective of cert programming is to
develop the program to control robot in
possibly intelligent way and to give you
some insight on what the robot has built
in various visual camera 640 x 480 to 4
mics in sensory well also joint
positions and other and as activators
were new joint positions and movements
of the joints are two speakers but also
a actuator called by us virtual phobia
it is artificial actuator we we have we
use such a spot in the visual area to
move around the visual area just to have
a better resolution in that area I have
no time to discuss it in in detail but
it's it's not a built in actuator and of
course the robot doesn't have any
predefined information on the nature of
the sensory versus only distinction
between for instance visual and auditory
sensory but nothing else is known and
the basic data flow model is like that
we have atomic sensory nodes and atomic
actuator notes and the those nodes that
are inside the atomic those are non
removable and the other everything is
dynamically changing and all these nodes
are examples or cases of a general
notion of a concept no the concept is to
mean what concept in cognitive sciences
means and the concept is program code
built into the this node and also IO
structured meaning how many inputs
output it has very salsa type whether
it's atomic or evolved there are other
fields property resources etc and of
course was a list of actions and rewards
values during but they come updated
during learning and so in general this
may be envisioned in first approximation
as a generalized neuron with a program
as transfer function so we have like a
two-step self-programming one step is
generating this code for the nodes and
the other is constructing and we call a
network structure however this approach
this explanation is imprecise as you can
see there is a case of a double link
which is doesn't make sense in this
configuration I will come to this later
to explain so in our approach what we
mean by self programming is managing the
concept Network what we mean by this is
constructing new concepts especially the
embedded code which is programs as I
said it is adding and removing concept
from the concert network it is adding or
removing individual links because we
don't need to remove the whole concepts
we can only update the structure of
links in this structure and
there's also evaluation of the concept
network and for the reinforcement
learning setting by state we mean a
active concept so it's the output value
of a concept which is which is a vector
and the state is independent on the
value so any any active concept is a
state I will come to it soon and action
is the next program to be executed on
that output value so action is selected
independently of the current state of
this active area source of this
statistic is probability distribution or
selection is independent and the
executed program is embedded in the next
concept the link points to so when
action is directed then this next
program is in the next concept so should
for instance any other concept linked to
the same concept the program executive
would be the same without the same as
for instance one may fail and the other
may well but but it's it's it's the code
is in the next concept and in case of
many inputs of course the execution is
postponed until all inputs are available
and now what we mean by concept
evaluation it is possibly more probably
most important part of my presentation
the concept note is like a long-term
program start on a mass storage device
like on a hard disk and there's also a
runtime note and a runtime note is an
executable of a concept so it's it's
like a in a Ram program executed in RAM
and a lifetime of such a program is
usually very short like like a hundred
milliseconds and it's worth noting that
concept are evaluated by launching their
runtime code and there may be as many as
thousands of runtimes of the same
concept like versus a single sensory
atomic note four four four concept note
for four forward sensory and it may have
thousands of copies concurrently
executed at the same time because is a
visual image currently a processed and
so if you have to run types of the same
concept then they are inputs that are
specific for this runtime and not so
they differ for different concepts and
here is explanation of this double link
so this is justified by having two
independent runtimes so for instance if
we have a concept of a pixel and the
concept of a line and we want firm line
from a pixel then it is required that
these two pixels are different then if
they are different they may be used to
to be executed in that way and I will
skip this and so one way more to
envision our system is like four layers
there is a virtual machine layer it is a
computer simulated on an in software it
is a computer like early 80's micro
processors we say about 850 or more
instructions not more but our fixed bar
but custom is able but in general they
are fixed and on top of these are built
small programs consisting of a few lines
and so we may think of this like
elements and chemistry built on top of
these elements and this this programs
built of items of elements are the we
are building blocks of the whole
structures also they may change there is
a large diversity of possible programs
and they may change in many ways not all
may be combinations may be used and they
are sorted out if they are useless and
rules of execution like we come to it
soon first I have to say that the we use
a single data format for or
communication it is an array of integers
and this is what a visual pixel would
look like the size of this pixel and
row-column and why you vie for the color
pixel data is also so it is like like
what is data vector of visual pixel
looking like we use currently 16-bit
integers in this implementation and the
this is what the processor is looking
like source of our inputs vector inputs
and output and this is like a black box
and internally various various
accumulator and in this register 0 flag
- flag and the local static memory the
static memory is shared among all
concepts so should one runtime update
the static memory this is available to
the our runtime it's like multi-threaded
programs in NC with static local
memories and this is what programs look
like after if we do that sample code it
is very much like micro processor it
calls and we use a number of variables
to control execution and many of them
may be attributed features of may be
attributed to artificial economics this
is like expiration time priority or
resources so expiry time is that there
is a deadline and whatever happens the
runtime is discarded so it's is the for
instance how to solve this top problem
or the problem of infinite loop it is
soft by by exploration time there is
also priority this is ordering the audio
of execution they are placed in 30 queue
and from poetic you are taken first
those most highest in the priority and
there's also resources how much of the
processor time may be exhaust so it's
not we are time like exploration time
but this is what
the runtime is executed it is it is
until its resources are exhausted
exhaust and it's just a quick look at
this a runtime passes through stages of
its execution it's it's fast in pending
state then it comes to goes to do to
queue waiting for execution when it's
executed after it finishes then it may
end in either terminated or exited state
which are successful states I'll discard
as the disk art is for instance if
running out of resources infinity loop
or executing an illegal instruction like
like out of memory so so in this case
runtime is discarded so there's a
lifetime and once it is completed then
these results are used to update concept
so runtimes update data of concepts and
concept decide which which one time to
execute next so there's also such a
natural environment parallel we use the
concepts are like species and wrap times
are like individuals of species and
links our dependencies like who is
feeding whom but we have also non Paris
in this comparison concepts do not
evolve once created they stay until may
be discarded or live forever and species
may exist even if there is no single
individual that's that's quite obvious
if there is a concept of letter e and
there's no e in visual area then the
concept still exists and the same code
base also have a difference the same
code may be used in different places so
it's not like one genotype is the same
species we may use different in
different places we may use the same
code like adding two numbers may can
have a different semantic meaning in
different places of of this whole
structure I should also tell a little
bit about temporal patterns because we
have special-purpose instructions for
controlling temporal patterns
wait instruction that suspends execution
for a given time of in milliseconds like
if you are controlling actuator then we
have to to wait until this instruction
is executed and we have also a delay
instruction that measures the difference
between two between the current runtime
and the one of the inputs the equation
times of this so for instance this is an
example of how this could be used there
are possibly a concept that measures
differences of three pixels in in visual
scene so for instance if we have a tiny
object we can we can have like just
measure its behavior you don't have idea
what it is but if it's moving like like
that you know it's a mosquito if it's
moving like like that it's a fly or if
it's moving like that it's it's an
airplane for instance so so we don't
have to to see the structure of this
visual pixel because it's a single pixel
but from movement we we have a a a match
for instance that to say that it is a
mosquito so number of such measures may
make my cause another concept to say
escape and run because it's its danger
and here's another example of how the
processing of vision is performed first
an image comes from the Vrbata - to the
host and this first compared with the
country's third image and on the basis
of difference between individual pixels
a vector is created if it's built at
threshold in difference and from this
vector what is created is sensory atomic
visual runtime source of runtime with
this value of output in runtime and it
is also assigned a priority priority
that is currently said to be a
difference in the luminance just just as
an appropriate priority but maybe then
overwritten by other mechanism and then
it's placed in priority queue so so
but how start the processing of vision
we have also may keep it as a mekinese
for linking two concepts into one and
here I would like to talk about learning
a little bit because it's not in my
paper I could hurry to common convention
we use exploration to mean adding new
action to - list of actions and
exploitation - executing any action not
only greedy or because we don't use the
on policy of policy action selection
there is only probably the distribution
and what is more important if we are in
a given state it may cause many from the
given state may may launch many actions
this is the main launch many actions -
voice over evidence of actions always so
it's it's not that I can say what is the
next stage and there's a concept of
intrinsic motivation by many authors
discussing that like Schmitt Harbor
about or
oh they're the intrinsic motivation
comes is both from cognitive sciences to
mean operation of an agent for its own
sake not not fulfill dot to fulfill some
drives but for its own sake and so many
others just control the intrinsic reward
which is a related term and by
controlling intrinsic rewards they
control intrinsic motivation but they're
also other approaches you know this is
our case - most rewarding actions would
be most common anyway but we want also
to create in the independent mechanism
to control the to push robot into a
expiratory direction and we use
probability of adding new action to be
proportional to this equation where
there's a constant and in the Dominator
denominator if this is sum of all
rewards cumulative rewards of all
actions so should there be no actions at
all not rewarding then probability would
be high or creating new actions and if
it sits there are many actions with high
reward though probability of creating
new actions would be quite limited
and there's a general question of how in
AGI how to reward was the ultimate
fitness function maybe reproductive
success or survivor or eternal salvation
I don't know but no matter what we say
about them it's it's it's too distant to
be even in chess it's very very
difficult to learn you know in first
foot learning if we have only win lose
or draw as a reward and nothing in
between so the the challenging part is
its immediate reward and we use the
following approach here we have
rewarding concepts and rewarding
concepts we were the concepts is one
that for instance does truncating only
so color image into black and white it
may be still justified by the system
because descendants and reward comes
from descendants so it may be useful and
not and non but rewarding constant but
rewarding concepts are done in the
following way the input is like a state
space and this state space is you might
think of this as temporal spatial part
of not only spatial pattern it's and
there's we do a binary partitioning and
this binary partitioning is done in this
way there's a program there's a
conditional jump and there's either exit
or instruction the exit means negative
example and written means positive
example and from this we calculate a
probability and we watch okay so let's
end yeah okay okay so we calculate from
this revolt and and based on this revolt
also the robot is evolved so by giving
these more resources to continue that
action I have no time to compare with
other approaches but to make a comment
on what Ben said I would say that for us
to name something self program
I need two features first I want to be
it chewing Clank machine on top or self
modifiable code which is unlikely and
this chewing machine code must be
executed on a machine that is the
programs will have created in some way a
random or not not hand coded so so so
that's a bit different approach then
that's that's all I believe there is no
time for more questions now well alright
I'd like to explain my researcher is bit
first
in a nutshell when you write a program
what you do is you're writing a record
of your thoughts and address the other
question what is before your thoughts so
what is the mechanism that makes your
thoughts be there in the first place
I studied refactoring in my approach
where computational experiments so I
carried out a number of computational
experiments on refactoring to trying to
answer that question and in the process
I found several new things one of them
is that I noticed that partially ordered
sets are a very good knowledge base
it's they are universal they are very
simple and they certainly support the
experiments I'm talking about then I
found a new type of inference that I
named emergent inference and I
discovered only recently just about a
week ago that Helmholtz in 1850 a
very mentally proposed what he called
unknown no I'm sorry and conscience
inference because we have no awareness
of doing it but it happens in our brains
so the same thing as help us unconscious
inference in view of that I propose that
emergent inferences and explanation for
emergence and self-organization in
complex systems and I also conjectured
that emergent inference is unexplained
aeration for intelligence here is my
experiment I start with a body of
knowledge on the upper branch you see
that I provide that knowledge to a human
analyst and the human analyst creates
some structures for me which I call the
natural structure for example it could
be a problem statement it could be an
object-oriented designer and the natural
structures would be the object-oriented
design on the lower branch you see that
the knowledge is first converted to a
partially ordered set format pass
through emergent inference and that
gives me some structure that I call the
predicta structures because they are
predicted by the theory and then I
compare the two and they compare very
well I did several experiments so far I
wish I have done more but I did a few
here is the first experiment ever that I
perform which is where I actually
observed a they emergent inference on
the left there is a very simple problem
in C very simple which I selected for
for trying to find out the answer to my
question all the Greeks are are given
input and on the left are the variables
and on the right is a canonical matrix
which is the matrix the canonical mate
is corresponding to that program
and you see that I have listed the
variables along the diagonal of the
matrix and the statements of the program
which initialize a variable statement in
sha is the variable are listed as ace
stands for argument as arguments on the
lower triangle of the matrix for example
the statement which I could see a little
better here and equals C plus D means
that C and D are predecessors of n in
the partial order and there is an a
there in in the column of C and another
way in the column of V in the row of D
then there are two red lines that you
can see on the screen that indicate flow
of information now in any equation any
mathematical equation and in any
computer program if you will like this
one there is always a flow of
information for example a statement n
equal a plus D C plus D there is a flow
for a variable C and from variable D to
arrival in information flows there in
that flow is indicated by the two red
lines that I'm indicate indicated there
please keep them in mind for a second
they will correspond to connections in
your brain neural connections between
neurons
so I started refactoring that program I
I have considerable experience in
refactoring
so I refactor very fast but I didn't do
that I actually started if I can be very
very slow one very little step at a time
I would choose couple of statements in
the program that we're adjacent and I
will reverse the order of those
statements if I'd like to do so
following what my brain was saying so my
brain was telling me somehow well these
two statements are interesting so
reverse them I did that at the same time
well you see that if I reverse two
statements in the program I also reverse
two rows and two columns in the matrix
and that in turn causes the ACE the
arguments to shift they move around as I
refactor I can see in the matrix D a
moving to different positions and I did
not exercise for a long time in one day
I realized that what I was actually
doing was causing a the age to come
closer and closer to the diagonal of the
matrix and the flux lines to become
shorter and shorter so I thought well if
my brain is doing that why don't I just
write a little program we just perm use
the matrix in such a way that it makes
the flux line shorter and I did that and
it worked wonderfully by doing that I
had automated refactoring because now I
had automatic refactoring if I run the
little program on the matrix and then
the refactoring transformations what
we're before just a second ago where the
transformations in use by my brain are
now in used by the program so the
program is telling the matrix and the
matrix is telling the program and the
program gets refracted without my
interventions at all automated something
else and when I tell you in a second
I went one step further I thought well
if I can have a little program that perm
use my matrix and that means the
refactoring of the program then there
should be something like that in my
brain too there should be something like
these canonical matrix and something
like that little program in my matrix in
my in my brain which refractors that
similar to that matrix which is in my
brain whatever that is and that is what
makes my thoughts to appear in I create
a brain model if I have time I tell you
about it and it worked
but let me tell you first what is the
result there is the result of the
problem you see how close they are to
the diagonal you see how sure the flux
lines are they became real short and you
see something else you see one more
feature you see that the matrix is
partitioned now each one of those blocks
in the partition are objects they have
behavior they each block has a number of
statements right this was a program in
the first place so it has statements
that do something it is beautiful
statements so those are objects they
have Delta so that that's an object
behavior and that those are objects
that's an object so apparently have also
automated the creation of objects
you see other things if you look here
you see inference is before your eyes
there are features here that are not in
the program on your left for example you
see two different classes of objects
each one with three objects each that's
not in in the program on the left
you also see as Ben would call it
intelligence critical value because you
see information the classes the
refactoring the organization the
association between entities here that
you are seeing here that were not in the
original program and all that was done
automatically without my intervention
laughs so this is not easy
it is it's easy once you have you have
done it this is a rigorous mathematical
solution obtained from first principles
not from phenomena not a
phenomenological gaze or an engineering
compromise it is universal you can do
that with any system because you can
take any system you want converted to
partial order set and organize it into
into classes of objects the way would it
by applying emergent inference it is a
side effect of on a related process more
than that in coming a side effect is not
intended I said I say something morning
in a second
it requires no specific are specific
domain knowledge you can put any submits
objectives to any learning process of on
any domain and it will happen and best
of all it is ready for use in computers
here my claims I claim that any system
has a natural hierarchical structure
that can be found by emergent inference
and my two conjectures already mentioned
are that immersion inference explains
emergence and self-organization in
complete dynamical systems and that
emergent inference in the brain gives
rise to intelligence
now to the more practical things some
people ask me or worry or concern about
well it's probably very difficult to
convert the system to partially ordered
sets imagine that I tries very easy what
do you do if you have a system and you
want to perform a mathematical analysis
of that system the first thing you do
you need a theory you develop a model of
the system and you write some equations
by that theory let's say Z equals some
function of x and y is one of those
equations conversion of that equation to
partially ordered sets is immediate the
set is X Y Z and X % z and y proceed Z Y
because you can calculate Z without
calculating x and y that's the whole the
whole conversion to partially ordered
sets it's a lot easier than solving the
questions and you know many people are
in fact solving the questions to
understand the systems in a computer
problem it's even easier to convert the
computer program because it has
statements and all the precedence
relations are just there just read them
all you need is a parser conversion at
for a computer program the degree of
difficulty is at the level of compiling
the program that's the degree of
difficulty yes I wrote the parser for
see it works some C let's say and
finally I developed a brain model which
I call the CFA's brain model systems for
a correction correcting a45 is for
shrink CNF if you already know shrink
means that the neurons
once they establish their
interconnections those connections
shrink they start making shorter because
they try to save energy in the
transmission of information the shorter
the connection the less energy waste in
heat or in chemical energy for the
transmission of that information so the
reason for that the motivation for s is
the shortening of the connections
between neurons but the result of that
is the following the connections
correspond to the Flex lines that I
asked you to keep in mind for a second
by shortening the connections they are
shortening the flux lines in the
canonical mate in my canonical mater is
the one in my brain and they are
becoming clusters of neurons which
correspond to the objects that we just
observed in the matrix there is
confirmation of that one of the one of
the confirmations is that film roscas
as I said he was told his physiology
history division and he observed the
unconscious inference the other is that
recently in 2006 they observed what they
call neural clicks which are the
clusters highly interconnected clusters
of neurons in the brain in the cortex
which are weakly interconnected among
themselves and now they are discovering
already fire kiss of those clicks so
that's how the brain is in case you were
wondering okay nice imagine the
inference is great but how does the
brain do it well this is how now two
more practical things
this represents a traditional process of
programming by a human programmer the
brain is represented on the upper row
and the structures it creates on the
lower on the lower row so there is a
stream of experience that's called
knowledge it goes to the brain
you talk to the analyst you don't talk
to it if you are a user you talk to the
analyst not to the problem and the
analyst thinks of what the
subject-matter engineers are telling him
and creates some structures which are
going to be the problem the final
product okay I thought that was the time
remaining okay in then at some point the
analyst is gone this correct analyst
he's gone and only the structures remain
well I'm afraid I claim that these
structures are basically zombies they
cannot update they cannot learn except
to the extent that the human programmer
has told them to but that's what the
program is and now you can input it
still works it does the sum function you
can still send the stream of experience
to the problem which now is called data
somehow is change the name and you have
the program I skip that one
and here is a comparison between
traditional AI and a GI not a comparison
just a thought about that you have a
program that drives cars kinetic
position sensors and the car controls
into drive your car you
Anastacia control program you connect
the stage the theater stage controls to
the stage control program it will
control the stage but if you connect the
red arrows it will not work and there is
nothing you can do about it
you cannot tell it to the program go
ahead and integrate yourself there is no
integration and no refactoring can I
take one more minute I'm sorry I try to
do okay that's how the brain does it
does how emergent the inference does it
which one looks more like the brain
you'll be the judge in conclusions I
conclude that self programming requires
AGI full AGI full feather the J and that
AGI cannot be achieved with by writing
programs a j-shaped is a process that
requires emergent inference thank you
assume this is de rigueur AI where for
google okay all right I just want to
remind everybody about production system
programming this is a fairly standard
programming model in C if I make that
disappear no I guess not
okay a spiral a standard programming
model in artificial intelligence
programming most commonly used at
Carnegie Mellon but basically have this
just big pile of if-then rules and
without running them sequentially you
just have some hardware software that
whenever and if rule happens to be true
the then rule fires and changes the
state of your memory and the if if rules
can
take off from both memory and input and
obviously this being a very parallel
what kind of process can can use
appropriate accelerating hardware the
basic idea of the the approach here is
simply that you can get a big pile of
if-then rules simply by watching
somebody do something you you if you can
parse out what a situation is in which
somebody does something and then parse
out the action your memories
appropriately parsed follow from in into
the the style of a production system
program the the key addition is that you
don't ever have a situation that you
remember that matches exactly the
situation that you're in and so you need
some way to modify the match and what
I'm doing is is I call this analogical
quadrature and it's roughly the same
sort of thing that Hofstetter Mitchell's
copycat does you you force a match with
a situation and then you carry that
forcing over to the action in a in a
parallel sort of way and the base guy
the talk is essentially about what
architectural features in software and
hardware would be necessary to support
this kind of programming of the system
so himself by its simply watching and
imitating so the the the key element I
think is actually that that it's
representation needs to be an
abstraction hierarchy you have to have
the situation represented as a whole
bunch of different kinds of concepts at
different ontological levels and then
you have also things like the the
parallel processing that it assists with
the production system execution and the
the next to the abilities of the
representation
to take part in the analogical
quadrature type operations and and
finally the given the abstraction
hierarchy you need a way for the
pressures that are evolved in the
matches to move up and down the
hierarchy in such a way as to affect
other levels as well okay the key item
in a for the abstraction hierarchy
essentially simply that when you are
matching a situation or an action there
there are three levels in the in the
hierarchy there they're actually ranges
you have many levels but at the very
higher levels you get an exact match
because you're you're looking at a an
abstraction of the of the situation so
if I'm a assured Lu type robot and I'm
trying to imitate someone picking up
blocks at the very highest level the
description of the action they simply
pick up blocks and and that matches
exactly at the very lowest level the and
the hand/eye motor control isn't is not
exactly the same at all and so you you
can't really match that and you have to
do that by some kind of control or trial
and error or planning or what-have-you
and the middle levels the ones in
between are the ones that are actually
susceptible to the analogical quadrature
operation one of the main reasons that
you do want the abstraction hierarchy
however is because since you're
representing the situation in the action
that you're looking at at at lots of
different ontological levels of
abstraction you can actually be
imitating more than one thing at the
same time and that gives you a lot more
coverage in terms of the kind of
memories that you have to work from then
if you only had one particular kind of
action that that you are imitating okay
this has all been done before by
evolution and in the architecture here
is evolutionarily inspired
and I don't need to say much more about
that the if for example you're you're
trying to parse a sentence and picking
out individual words you use your your
parallel matching hardware and each one
of the words that you're trying to match
there is this is again essentially just
a memory of a word and so that feeds
into your your need for parallel
hardware and your use of memories as as
items in your rule base in the copycat
program you if you have studied it
there's a there's a whole lot of work
about slippage --is in taking a concept
forcing a match with some other concept
that doesn't work but because you're
forcing the match you say okay something
on in in concept a has to look like this
thing I didn't think it was in concept B
and when you do that you can force up a
change that they called a slippage in
one of the fields say for example a
record that describes something and then
you carry that change across from the
from the situation to the action giving
rise to the analogical quadrature one of
the cool things about that is that you
can decide which parts of concept is
salient for a given match by having a
theory of causality if if I pick up a
block and it and it doesn't fit her in
another block I'm more likely to believe
that was because of its size of shape
and because of its color for example and
I can derive a theory of causality by
the same kind of memory driven
programming and use that to give rise to
an exponent explanation based
generalization parse of the of the
situation that allows me to pick out the
salient kinds of attributes of the
things that I'm matching
okay the key to this of course is
parsing the action sequences that you
see in order to imitate them in such a
way that and that they make sense and I
claim that parsing action sequences and
parsing sentences is a very similar kind
of operation and in fact there's there's
plenty of evidence that the same parts
of your brain are involved in parsing
sentences and generating them and
parsing sequences of hand operations and
actions and generating those and so
they're probably very evolutionarily
linked and similar okay the other aspect
of representations besides their being
plastic is that if you can have them be
metric if you can use n vectors numeric
vectors for your representations and
you've mapped the spaces right
analogical quadrature turns out to just
be vector algebra it's very simple to do
and there's also also a lot of reasons
that you want to use representations
like this if you can basically because
it's just a backlog of scientific
statistical and machine learning
software that that already operates on
things like this and and people
understand them very well and so that
the more that you can use metric type
representations for your systems the the
better the analogical quadrature and the
imitative program it will work here's a
case where Google Books doesn't do this
right and this is a breakdown in level
communication if you look at this is
this is from the second oz book by Al
Frank bomb and it gets the title wrong
that the actual title here is the
marvelous powder of life and you see
what happens is is clip the picture out
and also taken out part of the capital T
and then it's best guess of what the
rest of the capital T is as an AI and
that forces it to decide that the the H
is a K and the only word it
it starts with I care that might
actually be their hike and so what it's
doing obviously is missing any kind of
semantic feedback from a high level that
would make sense out of what it was
doing anybody that actually looked at
the page would never guess Ike marvelous
powder they would just look at it said
of course it's the marvelous powder well
that's the kind of thing that you get
from having the appropriate two-way
communication between levels of your
abstraction hierarchy and and and and it
keeps you from making mistakes like that
all right so the bottom line is
essentially writing programs are is is
very easy all you have to do is just
watch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>