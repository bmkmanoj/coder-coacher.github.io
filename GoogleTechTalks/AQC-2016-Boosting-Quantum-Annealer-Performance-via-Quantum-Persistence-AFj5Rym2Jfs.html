<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC 2016 - Boosting Quantum Annealer Performance via Quantum Persistence | Coder Coacher - Coaching Coders</title><meta content="AQC 2016 - Boosting Quantum Annealer Performance via Quantum Persistence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AQC 2016 - Boosting Quantum Annealer Performance via Quantum Persistence</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AFj5Rym2Jfs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is Gilly Rosenberg I'm a
researcher at one qubit it's a quantum
software company out of Vancouver in
Canada I'll be talking to you today
about how to boost quantum annealer
performance using quantum persistence
this research was conducted together
with Hamid karimi a colleague of mine at
when qubit so in this research we
introduce a new method for reducing
variables in cuba's using quantum
annealing using the method improves all
the success metrics of the quantum
annealer compared with using the quantum
annealer alone even while using less
than eels the method can be used for
both camara graphs as well as embedded
graphs so we see it as a general-purpose
method for using the quantum annealer we
call our method iqp VAR which stands for
i creative quantum persistence variable
reduction so fixing variables to their
values in the optimum results in smaller
effective problems which are easier to
solve that's something we like in the
Isaac based approach fixing a variable
adds a constant term to the Hamiltonian
to the energy and shoots the biases of
the neighboring spit it's possible to
fix variables classically and there's a
large body of wickedness in particular
there's exact in yer istic methods an
example of a an exact method is fixing
variables based on weak and strong
persistence ease an example of a
heuristic method is fixing variables via
thermal statistical persistency in this
method they utilize simulated annealing
to fix variables whose values remain
constant as the temperature is decreased
a motivation for the study was a figure
like this which we call the quantum
barcode basically the observation was
that in the quantum annealer sample
often variables have the same value in
all or most of the solutions to
illustrate this point this figure shows
the quantum annealer sample with 500
anneals for a chip that has one thousand
one hundred cubits
blue represents values of +1 and red
represents values of minus 1 so along
the y-axis you have the 500 solutions
and along the x-axis you have the 1100
cubits and when can easily see common
vertical features now let's mark the
variables that have the same value in
all of the solutions you see that almost
half of the variables maintain their
value in the sample this motivates the
idea that perhaps those same variables
have those values in the optimum this is
the basic idea behind our method it's
easy to understand let's consider again
the original sample I showed you and see
how we improved our method in two ways
first we improve the properties of our
sample we do this by trimming down the
sample such as we only consider the
lowest energy states for example we can
look at only the twenty percent of the
lowest energy solutions we refer to this
threshold the twenty percent for example
as the elite threshold another way of
improving our method is to fix qubits
that maintain their value in less than
one hundred percent of the solutions for
example let's say ninety-nine percent of
the solutions in the trim sample we
refer to this threshold as the fixing
threshold let's look at some results in
these tables we show the dependence of
the number of fixed variables on the
left and the fixing success rate on the
right on both of those thresholds the
fixing threshold and the elite threshold
I'll remind you that the elite threshold
controls the trimming of the sample down
to the lowest energy states and the
fixing threshold controls what
proportion of the solutions a variable
must be set to certain value in in order
to be fixed on the left we see that as
we lower either of the thresholds from
one as we relax the criteria the number
of fixed variables increases on the
right we define the fixing success rate
as the percentage of problems for
all variables were fixed to a value in
the optimum when I say optimum it should
be understood that I mean the best
solution we could find with the
heuristic with a long time out and I
will use this convention for the rest of
my talk these tables show that
decreasing either of the thresholds
allows us to fix more variables but we
risk fixing variables incorrectly let's
focus on particular values of the
threshold in this case fixing threshold
point 98 and delete threshold point too
so we're looking now at detailed results
for those fix for that fixing threshold
and Natalie threshold in the top panel
we show the difference in energy between
the best quantum annealer solution and
the optimum zero in this graph would
mean that the quantum annealer managed
to solve that particular problem
instance as the and there's a hundred of
them here there's no zeros in this graph
this means that the quantum annealer was
not able to solve any of these 100
problems to optimality with 2500
daniel's in the middle panel we show the
difference in energy between the optimum
before and after fixing variables a zero
in this graph means that the method
fixed all of the variables that had
fixed correctly in the two cases for
which it did not where you can see the
value was to instead of 0 the residual
was low it was only 22 corresponds to
the first excited state for these
problems finally in the lower panel we
show the number of fixed variables for
each of these problem instances the
average was 732 as we saw in the table
in the previous figure let's explore the
physical intuition behind why this
method works consider the case of strong
biases in that case the ground state is
fully determined by the local fields so
we can ignore the couplings completely
zinchenko atau showed that for weak
biases the state of all qubits can be
determined locally by solving a problem
that on the spins in a cluster that
includes a spin that you want to find
the value of
the cluster size for that local
calculation is determined by a
correlation length that correlation
length scales inversely with the
standard deviation of the biases so as a
standard deviation of the biases
increases the correlation length of
shorter in this case the state of the
central spoon becomes uncorrelated with
the values of the spoons on the boundary
let's look at the figure on the right as
in cheng coed i'll also measured the
response of the central spoon averaged
over different boundary conditions
that's the be over there and averaged
over disorder which is the H here
different Hamiltonians the response
depends only on the standard deviation
of the biases and on the lens scale the
correlation length as seen in this
figure it decays exponentially with some
landscape as a result each cluster will
align with its effective local field
with some high probability some of these
clusters will experience larger
effective fields those clusters will be
more robust under perturbations a good
low energy sampler can observe and
detect these clusters this could explain
the existence of spins which maintain
their state in the low energy samples
obtained from the quantum annealer as we
saw in the quantum barcode based on the
physical intuition if we increase the
standard deviation of the biases we'd
expect the correlation length to become
shorter and then the method should fix
more quibids we tested the hypothesis we
ran an experiment in which we fixed the
couplers and chose the biases from an
increasing range and indeed we observe
that the number of fixed variables
increases as you can see in this figure
each curve corresponds to a different
elite threshold you can see that the
lowest elite threshold fixed the most
variables which corresponds to what we
saw builder in the tables so relaxing
the thresholds allows you to fix more
variables to enhance this method further
we apply it repeatedly we notice that
the graph structure of the effective
problem
after applying the method is
significantly simpler than the graph
structure of the original problem in
particular the largest connected
component size is smaller the tree with
the tree width is molar and so on
classical pre-processing can benefit
from the simplification of the graph
structure for this reason after each
step of the method we apply classical
pre-processing to fix more variables in
particular the classical pre-processing
we used employed roof duality and weak
strong persistency to fix variables in
addition to this we can fix additional
cubits by using correlations something
that I will describe shortly let's look
at some results for this I creative
method on camara structured problems we
generated 100 problem instances for each
problem set each of which corresponds to
a row in this table the definition is
here un gives you J and H the couplers
and biases that are chosen for minus n2m
discreetly so you two corresponds to
choosing 4 minus 2 minus 1 0 1 and 2 and
so on so you 100 or high precision
problems first we try to fix variables
just with the classical pre-processing
and you can see how many variables it
fixed in column 0 it was very few
variables one percent or less in the
rest of the table we show the number of
variables fixed in each of four steps of
using our method for each step we show
the number of variables fixed by the
method which is denoted by M and the
number of variables fixed by the
classical pre-processing which is
denoted by C it can be seen that the
classical pre-processing is able to fix
many variables actually but only if it's
applied after our method and not before
the last column shows the total number
of variables that was fixed in these
four steps and you can see that almost
all of the variables were fixed on
average eighty-seven percent here
so we've seen how many variables our
method fixes but what about the success
metrics which is what we care about in
the end we compared the success metrics
for the quantum annealer alone on the
left with the success metrics for our
method with the quantum annealer on the
right let me define the three success
metrics we have here the success rate is
the percentage of problem instances for
which the optimum was observed at least
once the residual is the mean energy
difference between the lowest energy
found and the optimum so we want it to
be low the frequency is the mean
frequency of the optimal energy in the
quantum annealer sample for problem
instances in which the optimum was
observed at least once we want this to
be high the more times you see the
optimum the lower they are 99 would be
we also show in the last two columns the
percentage of problems for which the
lowest energy solution found by IQ PVR
was equal to or better than the lowest
energy solution found by the quantum
annealer alone the final column shows
the method fixing success rate which I
defined earlier a method shows
significantly better success metrics in
particular the success rate was much
higher the residual was much lower and
the frequency of observing the optimum
was much higher notably except for the
lowest precision problems you to the
frequency for just the quantum annealer
was negligible even with 50,000 and
neil's whereas with our method it was in
the hundreds
let's look at problems where all the
biases are there in this case there's a
Z 2 symmetry this means that for every
state if we reverse all the spoons in
that state we reach a state with the
same energy for this reason a sample
from the quantum annealer would be
expected to include many states with
their reversed pair using our method as
described before would then fix no
variables since no variable takes the
same value always now it's possible to
break this degeneracy by fixing an
arbitrarily chosen cubed after which our
method can fix variables but we can do
better than this instead we fix a
cluster of qubits which have perfect
correlation in low energy states we do
this by obtaining a sample from the
quantum annealer then finding all the
pairs of perfectly correlated cubits and
fixing the largest cluster of these
correlated qubits let's look at the
results for these zero bias problems
these problems are known to be harder
than the problems we looked at before
which had nonzero biases but our method
is still able to fix many variables in
step zero the largest correlated cluster
was fixed as I described before in the
rest of the steps the method is as
described before for the non zero bias
case we see that the number of fixed
qubits per step increases here whereas
for the non zero bias problems that
decreased the reason behind this is that
as more qubits are fixed they induce non
zero biases on the neighboring cubits
and the resulting problem becomes
increasingly like the nonzero vice
problems and in the non zero bias
problems we can fix many variables so as
it becomes more like the non zero bias
problem we fix more variables let's look
at the success metrics for our method
plus the quantum annealer compared with
just the quantum annealer alone we see
that the success metrics for IQ peeve
are on the right were all improved
versus the quantum annealer alone except
for the highest precision problem set
you 100 notice that the quantum annealer
was only able to solve
problems from the high precision sets
you 5 and above for each of which the
optimum was only observed once in 50,000
and yells in comparison the frequencies
for IQ PVR will not hire in the tens
hundreds of thousands what we've looked
at until now with Khmer structured
problems but general Cuba problems are
not of chimÃ¦ra structure to solve them
on the quantum annealer we need to
identify multiple qubits with one
logical variable we refer to these
qubits as a chain we've devised two
methods for embedded problems in the
physical method we fix variables on the
corresponding physical problem as
described before in addition we have
some criteria for fixing a whole chain
based on our knowledge that all of the
physical qubits in the chain are
identical for example if we fixed most
of the qubits in a chain we can then fix
the corresponding logical qubit in the
logical method we obtain a sample from
the quantum annealer for the physical
problem as before but now we translate
the sample into a sample analogical
variables we do this by repairing the
broken chains for example by majority
voting then we fix logical variables on
the in the logical sample using the same
method as described before so we have
two methods a challenge in studying the
performance of our method on embedded
problems is the large range of problems
available as well as the necessity of
controlling the parameters given our
objective of studying the dependence of
the method on the precision of the
couplers and biases in order to perform
a case study we obtained an adjacency
matrix for a particular problem on which
we generated random problem instances
the problem we studied was an optimal
trading trajectory problem in the ott
problem the objective is to maximize the
returns of discrete financial assets
given forecast returns and risk and
taking into account transaction costs we
chose the largest problem instance that
could it be embedded on the chip
available to us
this problem had eight assets five time
steps on a bit depth of two giving a
total of eighty logical variables and
872 cubits with the longest chain of 14
this table shows the number of logical
variables fixed in each of four steps of
our method and the additional classical
pre-processing step at the end in this
case classical pre-processing was unable
to fix any variables at all initially
which you cannot see in this table that
would have just been a column of zeroes
the top off of the table shows the
results for the logical method and the
bottom half shows the results for the
physical method around half of the
variables were fixed and more could be
fixed by performing more steps of the
method or by tweaking the parameters
further let's look at the success
metrics for these ott problems once
again for all problem sets the results
with IQ PVR have significantly improved
success metrics over using the quantum
annealer alone in particular the
frequencies are hired by two to three
orders of magnitude which would result
in much improved our 99 I'd like to
emphasize that we purposefully did not
spend much time tweaking the parameters
we observed that better parameters could
be chosen for solving any one of the
problem sets i showed you before for
example for you five but our intent was
to choose good general purpose
parameters and to avoid overfitting to
summarize our results clearly show
better success metrics for both Camaro
problems and embedded problems even for
high-precision problems that appear to
be very difficult for the quantum
annealer our intuition is that the
quantum annealer is better at finding
low energy states than it is at finding
optima for difficult problems for which
the expected number of occurrences of
the optimum is low looking only at the
lowest energy solution found by the
quantum annealer does not make full use
of the information in the sample it
appears that the sample contains
additional information about the
structure of the low energy states
information that allows our method to
boost the performance of the quantum
annealer for more details on our method
please see our preprints on the archive
thanks for attending my talk and I look
forward to your questions concussions
Wiggily um I didn't really understand
why you would stop at four iterations or
your method is there some heuristic a
rule that you use some decreasing
productivity gains that you get that you
stop at or is you just keep going until
you until you hit your hard limit of the
number of qubits you're using so I want
to like this if you were to use more
steps you would fix more variables and
if you were to use less steps you would
fix less variables so the answer is
there's no particular reason why you
should use for we had to choose a number
for the study what works best for your
method and for for your priorities will
be different depending on the problem in
the use case for example you might
prefer to fix as many variables as
possible to get a quick solution or you
might prefer to fix less variables and
be sure that you're preserving the
optimum so it's a very valid question
and like the other parameters in the
study the elite threshold and the fixing
threshold and the number of anneals the
number of steps we haven't tweaked these
in any way which I think enhances these
results because we did not over fit the
data in any way so this may be kind of a
naive question but so while I was trying
to understand once you fix the variables
do you then recompute the graph
embedding in such a way that they don't
take up cubits at all or do you just fix
the qubits in the hardware that's a good
point so for the embedded problems it's
true that every time you fix a variable
that variable is then taken out of the
embedding so in principle you could
either find a new embedding from scratch
would be potentially better easier for
the quantum annealer to work with or you
could clean you're embedding by taking
out those chains for the saturation of
the project we haven't done that we have
some ideas for future work that we might
end up doing and we expected to improve
the results and no they don't they don't
in both the Chimera in both the Camaro
problems and the embedded problems if
you fix a variable it has some effect on
this effective problem that you get at
the end in that it shifts the energies
and adjust the biases on the neighboring
qubits but the variable is gone it's
fixed okay let's thank you again
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>