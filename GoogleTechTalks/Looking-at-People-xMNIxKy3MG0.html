<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Looking at People | Coder Coacher - Coaching Coders</title><meta content="Looking at People - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Looking at People</b></h2><h5 class="post__date">2008-09-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xMNIxKy3MG0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's my huge pleasure to introduce David
foresight he was my advisor at Berkeley
but he moved to Illinois so it's a it's
a great treat to have him here and there
has done a lot of really influential
work on analyzing images and words and
at analyzing images and videos of people
he wrote the computer vision text book
that has been translated into Chinese
Japanese and Russian he has been on the
program committees for various
conferences and one best paper words and
cultured major vision confidences and in
addition to this he is also known as the
baby-eating Bishop of Bath and Wells so
long mr. prophet thank you so gay so I'm
delighted to be here I'm gonna talk
there's gonna be a broad impressionistic
talk of a lot of work to do with looking
at people done with a lot of other
people whoops there we go so i should
mention leslie akimoto james o'brien
ryan white ant nila bay or con arakune
dave arrahman on a leaf aha Dean as liac
is the Alex Oricon yo Tron young Tron
way on these are people who've worked
with me and made substantial
contributions to what I'm going to talk
about ally taught me a trick which
unfortunately i didn't use which was he
showed me that these talks turn up on
youtube so the first thing I have to do
say hello to my children and the second
thing he said was you know everybody
gets about the same amount of hits
except for the people who talk about
pornography who get two orders of
magnitude more views and in fact years
ago I used to give a talk called finding
naked people which I said okay that
would be good for my hits but all the
slides were in plastic so I couldn't do
it so why would we care about human
motion as something to look at so the
tremendous applications of understanding
human motion in video some of them are
sort of slightly disturbing three-letter
government agency type applications for
example prosecution the FBI might film
people at truck stops for a year to try
and figure out who is handing what
illegal item to whom and the problem
with having
year of surveillance video is it takes a
year to watch it and there might only be
10 minutes that are really valuable for
prosecution stuck in that video are
fairly obvious intelligence gathering
applications there might be crime
prevention application so right now
Britain is is plastered over with
cameras watching what people do and
generally that means that they might if
you're lucky gun do to somebody else
somewhere else and that already has seen
as a application now there are other
applications this sort of less three
later government agency one is in
human-computer interfaces for ten years
or so now people have been talking about
interfaces that watch people and react
to them in sensible ways you might
people's gestures might drive games
we've seen some of that becoming reality
are you might one you might think about
whether you can interrupt someone or not
depending on their behavior or what
they're doing and all of this I all of
this is still in the slightly iffy but
plausible stage here's a funny one and
I'll show you some more about it
architecture the design of real
buildings as we move along I'll share
some examples that suggest how people
behave in buildings is not terribly well
understood and the design rules have
errors in them and you can find
interesting things out about how people
behave in buildings just by looking at
video another application for
understanding motion involves motion
synthesis and this problem is but we'll
talk about very briefly but it basically
says make me a human motion that looks
like somebody is doing this and the
reason you might want to do that it has
to do with computer games and movies
it's hard to put little people into ps3s
so you need to generate artificial
motions and the mechanisms for doing
that are still fairly primitive you
might be interested in it from the
biomechanical point of view so there's
some great work from Zoran Popovich for
example explaining precisely why bird
feathers have certain properties to do
with the biomechanics of flight what you
do is you simulate flight you observe
flight and then you say well this has to
be
and finally if one does any computer
vision you discover that people are
interesting so years ago I went and
downloaded about half a million news
pictures that i filched off a new site
that wasn't what didn't expect me to be
doing something like this and we were
interested in using that and the
captions of the pictures to learn things
about the world and we discovered that
in half a million news pictures there
are no rhinoceroses there are tremendous
numbers of things that just don't appear
in the news almost everything in the
news is about people and it's about
small classes of people who just appear
again and again people are interesting
so what are the themes of the talk now
some of this talk is sort of a core
vision talk about core vision issues
that we don't fully understand there's
some very very nasty issues in this
question of watching people one of them
is we don't really understand how to do
activity recognition because we don't
know what the answer means there isn't a
clean taxonomy of activities so we've
got lots of papers that say they take
some features stick it in a classifier
and it'll tell you the difference
between walking and running the it's a
big favorite these days because actually
it turns out you can do that rather well
more generally when you want to think
about what are people doing it turns out
that you don't have a particularly good
vocabulary for describing that we don't
have a tremendous number of accurate
terms and we say well classify into one
of these and you'll understand what's
going on and that problem turns up again
and again through difficulties in
transducing signals through difficulties
in knowing what to do with signals it's
the main source of difficulty here the
other thing that goes on and that's part
of this problem is that activity
composes in quite complicated ways and
I'll show some of this big theme current
signal representations are
unsatisfactory we know two of them one
of them says track the person figure
about a figure out what they're doing in
3d and that has problems the other thing
says look at some image patches look at
the motion of those patches and classify
based on that and that too has problems
so let's start with thinking about actor
a big thing that turns up in human
motion is composition so why is that a
good thing if you're a human well if you
know how to do one thing and you know
how to do another thing you can put the
two together and you can maybe learn
something you you can scratch your head
while you're walking with either side
and you don't have to be walking to
scratch your head and that is useful it
gives you planning flexibility and
control flexibility now as I shall show
we can join up human motions in time to
make very good new motions as I
suggested you know you walk and then you
stop and then you walk and we can do
that from the animation side as well as
the human side we can also join up parts
of motion across the body but it doesn't
always work these two joining up
processes which I will demonstrate our
exhaust sources of fearsome complexity
and understanding so here's a problem
produce a human motion that meets some
constraints and looks good if you can
solve this problem they're all sorts of
applications as I talked about in
computer games and there are some
traditional solutions one of them is you
hire an animator and make him do it and
that's still the best way of getting a
good a good answer another way you could
do it which is now quite strong and
seems to be sort of the way to do it is
a nonparametric strategy which says take
a bunch of observations and join them up
why do we care because it will expose to
us some practical properties of human
motion so here's a method for making new
motions from old ones I'm going to go
out I'm going to use a motion capture
system to measure the configuration of
the body for people moving motion
capture systems place sensors on joints
they hang large numbers of cameras and
various other infrastructure in the
ceiling they produce three-dimensional
measurements of those senses and those
management measurements are turned into
a reasonable kinematic representation
now I've done all that I've got a 3d
representation of the body what I'd like
to do is to be able to make motions I've
never seen before how am I going to do
that well I'll make a little graph and
the graph i will make has each 3d
representation as a node and then if I
saw this frame after that frame when i
made the data i'll stick in a directed
edge
and then I get some directed edges now
that's not terribly interesting because
it says you can play the sequences
you've already observed know what I'm
going to do is I'm going to say well if
this frame and that frame look a lot
like each other in terms of acceleration
and velocity then instead of going from
here to here you could go from here to
there and I'll stick in a directed edge
there as well and i can compute whether
that's the case by comparing differences
so I stick in a whole bunch of these new
computer directed edges and then I say
any path in this graph is emotion and
various people argue in various ways
about different algorithms for finding
paths in this graph that meet various
constraints we don't care about those
arguments what we do care about is you
can get really nice motion out of this
business so fairly clearly I didn't
observe this less clearly it's about 14
little pieces of motions sewn together
in different ways why is it less clear
because it actually just looks like
seamless movement okay what that means
is this process of joining the pieces
are at least works as a model can be
used usefully and should be something
that one has in any attempt to interpret
motion now I said motions were
compositional they join up and we join
them up across time it's reasonable to
believe you might be able to join them
up across the body as well so I said for
example I could walk and scratch my head
with one hand or scratch my head with
the other hand now one way I could learn
about walking and scratching heads is I
could take movies of both events but
that would get dull after a while
because of the complexity that is
generated by this composition what I'd
really like to be able to do is to have
a model of walking and stretching with
one hand and then know that you could
cut this arm off and this one move the
arms across and scratch with this hand
instead and a natural thing to do is to
say well why don't we take some of this
motion capture data cut the arms of some
motions and
attach them to others and build new
motions does it work and the annoying
answer is sometimes so here's a little
red guy running and here's a blue guy
doing something else and we cut his arms
off stick them on the red guy and it
looks fine right now that is an again
you should see this as a source of
fearsome complexity in recognition but
great flexibility and animation it's a
dangerous source of flexibility though
here's a little blue guy doing something
with a football uniform here's the red
guy doing something we're going to cut
off the blue guys top and stick it on
the red guys legs and it doesn't work at
all okay why does it not work at all
well in fact they're not many terribly
good theories about that basically what
seems to be going on is you expect to
see correlations between motions across
the body and they're not there and that
motion is mechanically and coherent in
some way right so we've established that
motion is compositional but there's more
to it than that and that is where we
leave animation so what we really really
want to know is what are people doing I
got a movie I want to say what are
people doing in the movie the big
difficulty here is it this is a Rumsfeld
Ian statement that I just can't resist
it is not known what needs to be known
what do you need to know about the video
to be able to answer the question of
what are people doing and in fact there
are several different slightly
counterintuitive answers so the three
big answers about signal representations
one of them is a recent answer which
says just basically spatio-temporal
motion features that's work due to
lapped Evan Perez I'll talk about in a
second and the other two are fairly
natural one is to say okay figure out
where the people are in the movie track
the bits and pieces of their body
workout where those are in 3d and
interpret that using some form of model
and the other thing you might do is say
well we don't we know the the big
mystery of computer vision over the last
20 years has been to understand that 3d
doesn't matter
as much as you think it does um so maybe
it doesn't matter and what we'll do is
we'll figure out where the body is in 2d
and we'll reason about that to come up
with features that tell us what the
person is doing so firstly some
counterintuitive stuff about the right
signal so this is work by Ivan laptop
and Patrick Perez and they took a movie
I can't remember what the movie was
called but it's always known in the
vision community of smoking and drinking
because it consists of a bunch of people
who sit around and smoke and drink okay
and they talk to each other as well
while they're doing it um what we'd like
to be able to do is to tell when someone
is smoking or when they are drinking and
these are nice and interesting
activities because they're the complex
they appear in a bunch of different
forms people actually do them and it
might be the case that you could think
about tracking the body but there's
another thing you can do in a movie like
this typically the actor is framed in a
particular way either there's one guy
drinking like that or they're a bunch of
guys drinking but there's not much
background when you drink when you lift
your hand up like this you generate very
characteristic motion fields so what you
might do is say look what I'm going to
do is take windows in the image simply
compute a spatio-temporal feature out of
those motion fields in those windows
basically figure out the motion for a
little bit in the future and a little
bit in the past put that motion field
together turn it into something like a
sift feature and sling it into a
classifier can you find what people are
doing like that answer yes
embarrassingly well so over here these
are the top 20 returns from frames
collection of frames of the movie where
they said return drinking and you'll see
the ones that are wrong are done in red
you see on the whole are getting it
right and these are the top four from
that list so you can get a closer a
closer look just by looking at motion
features with no other information no
segmentation you can tell whether a
person is smoking or not you can tell
whether they're drinking on okay this
trick has been used more recently to
look at other activities as well I
managed to leave the reference behind so
see here's another little miss
she on the it's not known what needs to
be known line so about four years ago
way Yun and I wanted to do just a little
public observation study he is he was
doing an architecture PhD architecture
as in sort of real buildings and
fountains and stuff like that rather
than little bits of computer and he was
doing an architecture PhD and what he
wanted to do was to understand the
design rules by which you built open
public plazas they're a bunch of rules
and those rules are set up by
observations that were made with film
back in the 70s I believe so what he did
was he stuck a camera about here looking
down on sproul plaza buck at Berkeley
and then it turns out that it's quite
easy to segment the back of people's
heads from backgrounds if you know what
the background is and if you segment the
back of their heads you can usually
match up the backs of heads as well so
you can build a real simple tracker that
just says well here's a person here's
where they are in the next frame here's
where they are in the next frame and
it's reasonably accurate now they're two
things you learn from this one of them
is that people are a lot like ducks
which is the first person at a fountain
takes quite a long time to arrive the
second one comes very shortly after the
first one so he did a plot over here
which says number of people after the
previous number so what we mean by this
is where it says one that's the time it
takes for the first person to arrive and
where it says two that's the time it
takes the second person to arrive
conditioned on the first ones there so
what that means is the first one takes
about six minutes to arrive at the
fountain from when the fountain first
empties and the second one is there less
than a minute later and after that they
keep coming in right ah that was no
that's an established feature of
architecture the second person comes
fairly quickly to sit down what wasn't
known is a little mystery that you can
actually see and it's in the concrete &amp;amp;
Sproul Plaza if you look at these tracks
closely attending to the fact that the
tracks of the heads of the person which
is why they're up sort of that way it's
not that people like to walk into trees
as we're tracking the top of their heads
which is up there you will see there's a
small concentration of tracks about
there
usually what people do with applause
like this is they come in on this side
and they go up on that side sometimes
they come in on this side they go out
and that site occasionally they'll do
that or that if they go down the stairs
on this side they go past this point
okay why I don't know and I couldn't
find anybody else who could explain it
to me either and I think it's one of
those many little mysteries of human
behavior that are mysteries because we
simply don't have the observation data
yet or the techniques but you can
confirm that this is true by going to
the location you can actually see it in
this picture there is a stain and wear
pattern on the concrete over here and
that concrete if you go to the location
and look at it that concrete is
significantly more worn than anywhere
else in the stairs a very simple public
observation using almost no features of
of people have already shown a mystery
that to which I'm of which i'm not aware
of an explanation so let's talk about
more complex features for a bit so
tracking people is hard but you can do
it and there are big advantages for
thinking about aspect and composition as
I will show in a second there are
problems with accuracy and tracking and
we'll see them but there's recent work
by Ferrari and zisman now why is it hard
to track all the bits and pieces of the
body by the way the one person who's
been engaged in kinematic tracking his
name doesn't appear in the slides of
course is so gay who built one of the
very early versions of the tracker that
I will talk about I have some references
on tracking slides but it's a huge
literature and I wrote a review I don't
know a year and a half ago which is on
my webpage and it has 450 references if
you want to know about tracking and
motion synthesis that's sort of the
first place to start why is it hard to
track the body and when I talk about
track the body I'm saying kinematic
tracking I want to keep touch of I want
to keep in track or with the upper arm
the lower arm the upper leg the lower
leg the torso and any other bits and
pieces that I can track first thing is
it's hard to detect people
it's genuinely very difficult to detect
people in pictures it's getting easier
but it's still hard and it's hard
because in different pictures they look
different because they rearrange their
bits and pieces in funny ways they first
you get changes in pose in an aspect
over here you will see that the people
are getting in front of each other and
knowing just how many people are in this
collection I think its Australian Rules
footballer you're allowed to kick one
another when you do it figuring out how
many people are in this tangle is really
not easy counting the legs and dividing
by two doesn't do it and there's a
tremendous variation in what people look
like what they wear the shapes of their
body where all the bits and pieces are
so that makes them hard to detect people
also move very fast and they can move
quite unpredictably so your hand is a
good example of something that can move
fast there's a great pack of muscle on
your shoulder your hand is at the end of
the long lever and that means we can
generate big forces here in big
accelerations there that means that
dynamical models of what the body does
are probably not terribly useful unless
you look at very very high sampling
frequencies and that in fact has been
borne out by experience the other thing
is that many body parts are small and
tend to have poor contrast so the things
you might like to know about like upper
arms are small which makes them hard to
detect and I'm not wearing the right
shirt but if I was wearing a long
sleeved white shirt and I waved my arm
in front of my body all these bits and
pieces would have almost no contrast so
there are real serious technical
difficulties here one can build trackers
and one can build trackers now that work
okay spectacularly well is hoping for a
lot but okay is a possibility and the
trick to building the tracker this is a
line of work as I say that started with
Sergei's work and then went on with work
by Dave arrahman on and and resistant
and a variety of others the trick to
building these trackers is to know that
even in quite short video sequences
people take lateral walking
configurations which are highly
distinctive somewhere in a short video
sequel
I'm going to look like this why I don't
know it's just an empirical observation
that happens you you can detect that and
you can detect that you don't have to
detect it particularly reliably as long
as you get one in the sequence for each
person and if you do get one in the
sequence for each person now you know
what that person's arms and legs and
head and torso look like why because
you've detected the person you know the
legs are over here these pixels are on
the legs you also know a ton of pixels
that do not look like the arms the leg
the head whatever so you can build
little discriminative classifiers that
say these pixels look like the leg and
these don't and then I'm going to take
each frame and I'm going to say well
there might be some leg pixels over here
some head pixels over here some aren't
pixels over here and sub torso pixels
over there once I've done that I know
that I'm going to get a whole bunch of
other stuff wrong so those are not arms
those are not arms those are not heads
but the second distinctive feature of
the body is the bits and pieces join up
in particular ways and I can use a
device from the vision literature known
as of pictorial structure which is
basically just a souped-up
tree-structured Bayes net I called it a
hidden Markov model until people smack
my hands for doing so and I can link
these detects up into the correct
spatial structure so these legs that
head that arm and that torso all belong
together once I have done that I have a
detection procedure that I can just
apply to every frame and that in fact
will give me a track will give me
another track that is quite neat because
if there are two people another property
that is neat if there are two people
that I detected and their clothes are
different I know the difference between
them so this tracking procedure works
rather well so she is running as you
will notice from bright light to shadow
and we still know roughly where her bits
and pieces are and we've got a
reasonable sense of what the body is
doing and here we have know what I'm
saying as a report of the covariance of
the track so instead of drawing where
the arms and legs are I'm drawing
if you like covariance ellipses for
those arms and legs and there's somebody
doing something what is that person
doing its figure skating yeah can you
tell the gender of the person me and
David Roman on a convinced that you can
but nobody else seems to agree with us
um it's female and in fact yeah it's a
figure skater it's a female figure
skater and as this sequence runs what we
have here is a fairly decent
representation of where the bits and
pieces of her body are it's not perfect
but we know where the bits and pieces
are and we can probably use them and
this will go on forever that's a long
sequence of a skater I'll show at the
end I'll run it up while we discuss
questions in the light okay so what
could you do with that well one thing
you could do would be you could try and
reason about where the body was in 3d
based on where it is in 2d and then I've
got this 3d stuff I got a bunch of 3d
labeled data I can I can classify how
would I reason in 3d well there's a
trick which is not as widely known as it
should be in the vision community which
says actually if you see a body and 2d
up to some discrete ambiguities you know
where it is in 3d and the reasoning is
roughly like this almost always you see
people in orthographic views rather than
perspective views sometimes you see them
in perspective views it's usually a
famous person holding up a hand to keep
the pepper at serve from taking a
picture right but usually you see the
body in roughly orthographic views for
most people under most circumstances you
know how long the body segments are
right there's a factor about one and a
half between tall people and short
people here and that's of no
significance at all so if I know how
long something is and I see it in an
orthographic view the reason it is
shorter than it should be is because
it's been for shortened and that tells
me the cosine of the angle between the
viewing direction and the thing that I'm
looking at now there's a two-fold
ambiguity over here the status of which
is not terribly well understood to be
honest but I can go using this line of
reasoning and this has been established
for it and I 89 years now from something
which says the
ankle is here the knee is here there's
another ankle there's another knee hips
wrist elbow etc to a 3d representation
and small changes in scale give me
slightly different 3d representations
but I've got a reasonable configuration
of the body I won't speak to the slide
but I leave it here there's a great deal
of confusion about how ambiguous this
lifting process is which is still
unresolved and is probably fertile
ground for science ok so now what I want
to do is name activities so what am I
going to name them with well the first
thing I could do is I could choose
actions with names so I could study
gymnastics or I could study a Tai Chi
your ballet and there's a name for a
particular activity and we're okay but
we're really interested in everyday
motion so the problem I going to run
into is there's never really enough data
of every day motion to learn good models
so what could we do one thing we could
do and in fact sorry I put in an extra
slide here and lost my rhythm the there
just isn't enough data to learn models
from video if you try and learn
generative dynamical models you get
clobbered and you get clobbered because
there are too many dynamical parameters
they're hard to learn and you don't have
enough data or there are two the model
is insufficiently expressive and if you
try and learn discriminative models
you'll also find out you have not enough
training data to get good performance
because people change aspect and
clothing in the line okay so what could
we do well one thing we could do would
be to label motion-capture data numerous
people have done this mainly using a
collection of motion capture data
released to the research community by
Electronic Arts about six years ago
Georgia Tech is producing a bunch of
this data as well ah there are some very
desirable features of a labeling that
we've already seen one is it should be
composable so if I take a sequence I
should be able to label a frame waving
and I should be able to label another
frame running and also waving so I'm
going to have tags that
compose what I'm going to do also is say
I'll guess the best vocabulary I can and
I want to be able to real able very
quickly and it turns out you can do this
how do you do this well what I'm going
to do is I'm going to choose a bunch of
different vocabulary items walk run jump
stand carry wave turn left turn right
labels like that I'm going to take my
motion capture data and then I will
label some examples and train a little
classifier and then I'll see does it
label everything else right and I'll do
that for walk and if it doesn't it turns
out to be very easy to display all the
labeled motions and you say well that
one isn't walking move it to their that
one is walking move it to their retrain
and you keep doing that until you do
walk well and then completely
independently you're going to tag the
data for running and then completely
independently you tag it for jumping and
completely independently you tag it for
standing now there could be problems
because you cannot in fact stand while
you're running and we will just ignore
those dependencies completely as being a
sort of second order thing so now we go
and label we've got 13 bits what can we
do with it well the first thing is we
can tell little animated creatures what
to do because we know what they're doing
so this little guy can be told to run
then jump and catch the ball we tell him
that and lo and behold he does it that's
convenient and it's got applications but
we're not going to concentrate on that
instead I'm going to take the fact that
i can do that problem and use it to
label data so i have a bunch of
annotated motion-capture data i have
some video i went and tracked that video
i can take that 2d track and lift it up
to 3 D so I know where the body is in 3d
at the same time I did a bunch of
annotation I built a an annotated 3d
motion library I have a motion
synthesizer that will build me good
human motions that follow a sequence of
labels and I demonstrated its output but
I didn't describe how it work now what i
could do is say look
here are a bunch of 3d lifts build me a
motion sequence that lies very close to
these lifts in 3d and then tell me what
the labels are and we will do that does
this work sort of okay I think you
should be reminded this is for your old
work now so things are a little better
you should be reminded of my colleague
Robert wollensky's remark that it's
bloody at the cutting edge meaning we've
got a rough idea of what these guys is
doing and we've got a rough idea where
they are but the labels aren't by any
manner means perfect but we're still not
doing as well as we could on this
question of composition so what I'd
really like to be able to do is have
much more complex three dimensional
motion models right now my motion models
are string things together in time and
what I'd like to be able to do is to
string them together in time and across
the body so I know what an arm looks
like when somebody is walking and I know
what a leg looks like when somebody is
walking and I know what an arm looks
like when somebody's waving and I know
what a leg looks like when somebody is
waiting so previously i said from the
point of view of an animator are cutting
things up across the body and
reassembling them is not necessarily a
good idea when i cut the blue guy's arms
off and stuck them on the red guys legs
sometimes it works sometimes it didn't
but for interpreting a signal I should
be okay here because people don't move
like that I'm going to see the signal
it's really generated by a person the
fact that my model can sometimes do
something weird is less of a problem so
what I'm going to do is this I'm going
to take that all that motion capture
data it's labeled so I've got arm walk
models I can build arm walk models I can
build leg walk models I can build arm
stand models I can build leg stand
models then what i can do is take those
little short time scale models and join
them up to each other so if you're an
arm and you're walking you can do your
arm walk and then you can join to
another arm walk or you could join to an
arm wave and then i can string them
together across the body as well so i
can say
well I can take the product space of
this finite state model essentially take
an arm walk and a leg wave and join them
up and a leg walk and an arm wave which
is likely to be much more useful and
join them up and when I do that I come
up with a sort of Holy Grail which is a
very big finite state model of what
emotion could be with very few
parameters to learn and the parameters I
need to learn are what do the little
local models work like this is a motive
this is modeled after a trick that you
see in speech rather alike you learn
phonemes and then somebody else writes
down how phonemes form into words so
then I'm going to see um so I say I've
got many parameters in this many states
but few parameters I am going to see
material from attract body I'm going to
lift bits and pieces of each quarter of
the body into 3d and then I'm going to
join them up to be a consistent body in
three dimensions now why would I go
through all this trouble well the answer
is if I build a model of this form I can
query for motions I have never seen
examples of my model is capable of
generating a sequence where the leg
walks on the arm walks then the leg
walks and the arm reaches even if I
haven't seen that and furthermore by a
droid manipulation of finite state
models I can take a video sequence and I
can say what is the posterior
probability based on you know all the
learnt parameters and the like that in
the sequence there is a sequence that
looks like that anything generated by a
finite-state certain one of these
combinatorial things and what i could do
then is search for things I've never
seen before so what did we do we took a
bunch of people jumping around in our
laboratory we gave them instructions
Crouch run jump Jack run Carrie etc run
backwards wave run jump reach walk stand
wave walk we've got sequences of
reasonably complex motions on fairly
easy
backgrounds and we then said what we
want to do is search for these things so
these little bars are different colors
depending on the individual so they're
wearing different clothing and this they
have different body shapes and the like
and then what I'm going to do is I'm
going to take my sequins and I had about
75 video sequences so these are the 75
and when I do a query like walk stand I
want a sequence where somebody's walking
standing than waving than walking what
I'm going to do is ranked all of those
75 sequences by this query and then put
a little mark over here every time
there's a relevant one so in this case
the first two are relevant and then
there are a bunch that are not relevant
and then there's one that's relevant and
then they're a bunch that are not
relevant so they're in a picture like
this if it was working perfectly that
all the color would be on the one side
so the two things one is it works okay
is there can I compare numbers with
other numbers no because there isn't
anything else that does this to the
clothing that people are wearing doesn't
seem to be a big issue in the reason it
doesn't work we'll see what what why
some things go wrong fairly shortly okay
there is a second thing that's quite
neat about this 3d reasoning and it's
important which is aspect changes in
aspect don't on the whole cause
performance of this kind of query to
fall off so what am I going to do to
establish that I'm going to take a bunch
of sequences sequences like wave reached
jump jack jog jump etc I'm going to do
queries on those and then i'm going to
ask what's going to be the mean average
precision for those queries as a
function of the angle that the camera
makes with the body i'm going to view
the person from different viewing
directions i'm going to ask what happens
to retrieve and the answer is it's not
perfect at any of the angles some of
them are better than others but we don't
see a catastrophic failure as the aspect
changes why is that because we can get
into 3d and we can reason in 3d and when
we're there aspect isn't that big
there is one major problem here which is
these trackers although they work or by
no manner of means perfect and you can
see this guy in the background over here
has grown an extra leg in a fairly
uncomfortable spot for an extra leg and
you expect that to happen regularly
across frames and it's a major nuisance
and one word given this pace of the
development of the state of the art of
trackers it's a nuisance one's going to
have to live with for some time right
and it generates all sorts of noise and
it probably explains why even on
relatively simple queries the retrieval
results are at best okay so what could
one do well the alternative thing I
could do would be to take that whole
discussion of appearance seriously and
use that as a cue to what people are
doing and we know some things about this
one of them we know is where you are can
be a very powerful guide to what you're
doing but there's certain things you do
only in certain places right there are
other things where if you're there it's
pretty clear what you're doing even
though it's not absolutely certain the
second thing we know and this is sort of
very well-established is the
configuration and motion of your bodies
in flat images do seem to be quite
distinctive so the sort of there's lots
of history in these issues and what you
might do is say well what I'm going to
do is take a video um I have I'll put a
little box around a person having put
the little box around the person I'll
build a feature that essentially looks
like where do I think the bits of person
are which is not absolutely accurate
where do I think the motion is take
these features and bucket them in the
usual way of sift features and
histograms look at this property over
several frames into the future and
several frames into the past with
reduced precision as you look far into
the future and reduced precision as you
look far into the past rack up a big
appearance feature like that and then
stick that into a classifier and say
we're
if I the motion based on what you look
like okay again these appearance
features are quite well tested the one
I'm describing is one we did but there
are a fair number of appearance features
out there all sort of along the same
lines there are a bunch of data sets one
is due to enri it's called xmass there's
one at vitamin there's one at Maryland
we did one as well you typically have a
bunch of actors a bunch of actions a
bunch of sequences most of them have one
view but the xmass has five different
viewing directions and that's that's
worth keeping track up so if you do this
now I've got sort of discriminative
results for everything on everything
over here and it's a table full of
numbers we only really need to pay
attention to a subset of these numbers
one is if you do a reasonable job of
signal and the like one nearest
neighbors with metric learning will do
extremely good classification on these
data sets there are a variety of
different discriminated tasks you could
leave one person out you could leave one
sequencer you could leave one action out
etc but generally what you want to be
looking at here is all of these numbers
are pretty high you could take those
features and you really can say what
people are doing based on those features
and two if you do that you can in fact
on markup video automatically so
mysteriously we possess a whole bunch of
badminton video and you can sit down
with this badminton video you marked a
little blue guy and the little red guy
and then what I'm going to do is I'm
going to classify what he's doing and
for the blue guy that'll give me a
little dot over here he could be running
walking hopping jumping unknown motion
forehand backhand smash unknown shot or
non shot and then I'm going to in grey
mark up ground truth and blue will be
marks of what he's actually doing the
main thing here is the blue sits on top
of the grey most of the time so I've got
a fairly good idea of what this fellow's
doing and I got a fairly good idea of
what the other person in the sequence is
doing as well right in this Rose will be
restricted vocabulary
now the problem with this representation
of activity as a pure appearance thing
is of course people look different when
you see them from different directions
so this x-mass data set is intended to
emphasize this fact you get a whole
bunch of different cameras and somebody
walking or somebody doing something in
camera zero might look like the top row
but in camera four they look very
different indeed camera four is overhead
camera or camera zeros somewhat natural
so to hammer that home if I what I'm
going to do is I'm going to go and I'm
going to learn a model from camera 0 and
then I'm going to test it in camera 1
and I guys I'm going to ask how often do
I get the answer right and the answer is
not very often basically right so
somewhere between forty percent of the
time when I'm doing well and eight
percent of the time when I'm doing very
badly and deep so all those nice 90s and
hundreds and stuff have just put in the
dust it's hard to directly learn a model
from one view and then apply it in a
second view now what might we do about
this problem idea what I'd really like
to do is to use features that are robust
to changes in aspect and in fact i'm
going to encode aspect explicitly in my
discriminative procedures so how would i
construct those features well the nice
thing about objects in general or about
the visual world in general seems to be
that comparisons behave rather better
under change of aspect than exact
features so it might be a bit difficult
to say what this exactly was but the
fact that this is like that seems to
hold as the aspect changes I'm sorry
this is one of these annoying animations
that I can never quite purged from my
slides um these two look like each other
in this view and they also look like
each other in that view these two look
like each other in this view and they
also look like each other in that view
and that suggests a feature
representation that is comparative
instead of saying this is what this
person looks like when they're doing
that
you'd have a whole bunch of examples and
you'd say they look like this and that
behavior might that might behave rather
well under aspect you can think about
this as taking the space of appearance
examples and splitting it so over here
what I'm doing is I take a bunch of
known examples I'm putting a random
split in that bunch of known examples
I'm going to take my unknown objects and
I'm going to describe them by how they
fall in this app so my animation seems
they've gotten mangle whoops come on
here we go so that red tomato is an
unknown object a new thing for which I
want to compute features here is a
separation of my space of objects into
two parts roughly the semantics are
round and not round although round is
perhaps a strong way to talk about I'm
going to take that tomato and say by the
green split of the space of objects the
tomato in fact is on the not round side
not sure why but oh it's on this side on
the round side by the green split the
mug is on this side it's more like these
things than it is like those things that
comparison i will use as a feature i
will then tag training examples with an
aspect variable which I will have to
estimate and I will estimate a
classifier and an aspect variable what
happens remember ignore the first two
columns because it's a little piece of
Internet sign fighting about different
mechanisms of feature construction which
is not relevant to us over here all
those eight and six teens and 38
suddenly turned into a tease so what we
can do is learn a model of what the
person is doing from camera 0 and
classify from camera for rather
accurately ok now question what about
composition in these
appearance features now we're almost at
the end of the talk which should qu to
the fact that this blank space here has
semantics it's not that I didn't know
what well it actually is I didn't know
what to put there we simply don't know
the answer to this question okay so
conclusions the fact that we don't know
what labels what the taxonomy is the
structure of the labels of motions the
fact that we don't know that is a major
nuisance it's a nuisance generated by
this composition motions are really
complex your body can do a lot of stuff
and it's usually related to what you're
doing if it were not for this issue of
taxonomy and composition appearance
methods would win the hands down because
they actually perform as we saw in those
numbers rather better and I was able to
turn aspect into a nuisance rather than
a deal-breaker so what do we need to say
about activity should we name it should
we be reasoning about goals should be
reasoning about intentions and what are
the objects nearby what do they
contribute to what we understand and how
do we use these seem to me like the very
big open questions finally actually
given this has been publicized I should
stand in front of these words because I
looked for a synonym for fool's paradise
right now in computer vision the object
recognition technology is actually quite
well developed there lots of classifiers
that'll tell you this is a motor car and
this is a person right they're not that
good at dealing with things that are
unknown but quite like things they've
already dealt with so if you know about
cars and you know about motorcycles and
you know about buses if it's a car or a
motorcycle or a bus you're okay but if
it's a truck you're dead meat and you
shouldn't be right and i think that
problem is the same problem there's this
absent taxonomy composition problem and
object recognition community is about to
run into it in a big way Phil's paradise
was the best phrase I could find it
sounds a bit pejorative there's there is
of course a bonus question so if you
look at this little animation video you
notice that the characteristics of
motion tell you a great deal about the
individual ok so this exaggerated sit
it's an animation and the like but this
guy doesn't move move even remotely like
the other guy does and you know that
that is true for people most of you
should be aware of the fact that you can
tell who is coming to your office by
listening to the footsteps on the floor
outside you know you know who it is why
because people everybody has their own
central nervous system and it leaves
profound marks on the motion as you move
your body you should be able to
recognize people from the way they move
we don't know how to do that and that's
the end there's my holiday behrami nuns
little ice skate at doing her stuff
she'll do it all the way through I think
I've run a few minutes long but I'm sure
we've got time for questions
exploration these people in this square
right you know people have been working
on modeling type of exploration by other
mammals like practice yes mark I was
wondering if there is any relation oh so
I think I want to turn it around the
other way which is simply being able to
watch and track things whether they're
people or other things in video can be
tremendously helpful in revealing
there's some nice work going on in UC
San Diego where people have been
tracking mice well why would you track
mice not because you care so much about
you know what mice do every day but your
canonical medical experiment is to feed
something to mice and then see where
they die or they run around a lot or
whatever so how do you do that well
either you make some put graduate
students sit in front of the cage and
count how many times the mouse scratches
itself we just film the thing and watch
how it moves around and what it does and
it turns out just keeping track of
roughly where the mouse ears is
incredibly informative there was a bunch
of work at Caltech on phenotypes in
drosophila so these are these little
fruit flies which basically they do
different mating dances in different
ways depending on what genes they have
and some some sort of biological thing
how do you know where one thing you can
do is you put pairs of drosophila into a
lot of translucent boxes you film them
from above and then you cluster the
spatial tracks that you see and that's
very revealing about the behavior I
think it's important to notice that
apart from the stuff you know with cell
phones and little worn devices that's
been going on recently at MIT there is
not much mass observation of what people
do in public based on my experience of
surveillance video mainly they walk from
point A to point B and that's another
source of complexities of its own
because what you really want to be able
to spot is the unusual stuff and the
problem of the unusual stuff is you
haven't got bring much of it um but yeah
there are there are lots of applications
um this work in I think chimpanzees
maybe I heard about it yesterday so with
this morning so I'm not all that
switched on rue it but the similar
things you know where you are tells you
what you think i have a question when
you when you talk about composition it
seems like there are some things that
you can compose for example you know
what what one leg is doing you run to
pose with what another arm is doing but
some things I'm not quite so I'm talking
about to each other I believe that to be
true so question is can you figure out
automatically what you can compose and
what you can and it it it comes up an
object recognition as well every time
I've tried it hasn't worked I think is
the best I can say about it so yeah
there's a visit there's a big nasty
issue that I showed you that example of
good motions and bad motions the bit
that i suppressed in the interest of
time I had to do with the fact that you
know the obvious thing to do is to build
a classifier and say tell me a good
motion from a bad match stick it at the
end and just throw away the bad ones the
downside of that is that classified
doesn't work and they've been three
quite good attempts to build classifiers
there was some stuff by renan I think it
was a toll in CMU a couple of years ago
that was sort of okay it's a little bit
better than chance and then Leslie
akimoto built one that was a little bit
better than that one and the way it
worked was it looked purely at foot
skate I said when you generate a motion
like this does the foot skate on the
ground and and we know that it should
but we also know that what's going wrong
you know in sequences like the one I
showed you has nothing to do with
footscape so the problem is you can't
just engage in a statistical fishing
expedition are because there are too
many variables to be able to be sure
that you actually know what correlations
are important based on a reasonably
sized data set remember these these
frames are several hundred variables
each so who knows whether the left arm
is correlated to the right leg but
having a theory that would be
we I'm not aware of one that's that's
viable at present you might describe a
theory in terms of goals of intentions
of planning but you know it's it's
nothing
yes
so let me read the question back because
there may be people who can't hear it
the question is basically there are
large quantities of actions that have a
kind of parametric structure you can
reach for different places in space or
you can hit things harder or softer or
whatever and how does that play into
representations of action and intention
well I wish I knew the odds the best
answer I know is there been some
small-scale domain studies of specific
parametric actions there's a little bit
of work on animating these parametric
actions which you know it's something
you can do fairly well if the ballistic
in structure so something like reaching
is essentially a ballistic movement then
it doesn't work so well if there are
collisions that occur during the action
or contacts or things of that form so if
you think about striking something if
you think about hitting a ping-pong ball
with a bat because the collision is a
dominant part of the action and they're
very fast accelerations right now we
don't have really good parametric models
that allow you to do that or that from
the recognition side I'd say the game is
pretty much open I know there's some
work about parametric classes of gesture
you know the fish was this size the fish
was that size but as a I'm not aware of
a broad theory I would regard that as an
opportunity rather than</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>