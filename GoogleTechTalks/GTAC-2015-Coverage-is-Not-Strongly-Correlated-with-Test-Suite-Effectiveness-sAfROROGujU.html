<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Coverage is Not Strongly Correlated with Test Suite Effectiveness | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Coverage is Not Strongly Correlated with Test Suite Effectiveness - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Coverage is Not Strongly Correlated with Test Suite Effectiveness</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sAfROROGujU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have Laura from the University of
Waterloo who's going to talk about
coverage is not strongly correlated with
testsuite effectiveness all right good
afternoon everyone
I'm going to start my talk today with an
assertion high coverage does not imply
high quality in the next 10 or so
minutes I'm gonna try to convince you
that this is true but first let's take a
step back why don't you use coverage at
all well we want to get the bugs out of
our programs and we write tests to do
this but if we're not finding bugs it's
not immediately clear whether the
program is just very good and there
aren't many bugs to find or whether the
test suite is very bad and it's just not
doing a good job of finding those bugs
so what we'd really like is some way to
estimate the fault detection ability of
a test suite and coverage is often
recommended for that purpose but is it
really a good way of measuring tests to
equality well a number of studies have
been done to try to answer this question
the results have been mixed the studies
indicated with the green check mark
found that there is a relationship there
is a correlation between coverage and
effectiveness one study indicated by the
red X found no relationship and the
others marked by black Tilda's found
mixed results so the relationship
depended on the coverage type the
program type or some other variable but
there were a few other problems with
these studies first of all only the two
most recent studies used real software
prior to that they'd all been using
small toy programs 100 lines of code
that sort of thing and so it's really
not clear that any of these findings is
generalized to real software second some
of the studies used unrealistic suites
and I'm grouping two different ideas
here some of the studies used test suite
generation tools to make the test suites
that they evaluated and some of them
didn't consider
actually no it was mainly just the yeah
the generated test weights that were the
main problem here although some were
also a hundred percent coverage they
were known as adequate test weights and
those adequate ones are not common in
practice you don't usually see sweets
with a hundred percent coverage so again
it's not clear that results on adequate
test Suites or on generated sweets will
generalize to more realistic test
weights and finally some of the studies
do not control for the number of tests
in sweet and this is important because
the way you increase your coverage is by
writing more tests and when you write
more tests you catch more bugs but it's
really not clear if you're catching more
bugs because the coverage increased or
whether it's just because there are more
tests so we wanted to do a new study
that took all these things into account
we started by picking five realistic
programs on the order of a hundred
thousand lines of code and the one I'm
going to talk about primarily today is
the closure compiler which many of you
are probably familiar with these still
aren't the largest programs you'll see
an industry but closure was around seven
hundred thousand lines of code I think
at the time of the study so they're
respectively sized and like I said we
want to develop a written test weights
however we wanted more than one data
point for program which meant we need
some way of creating test Suites that
wasn't a generation tool so we did this
by randomly selecting test cases without
replacement from the suite that came
with the program so first we made Suites
of size three by taking three test cases
then Suites of size ten thirty hundred
and so on
we made 1,000 Suites of each size that
we could get statistical significance
over the results next we measured the
coverage of each of these Suites that we
created using the tool code cover
we measured statement coverage decision
also known as branch coverage and
modified condition coverage MCC is not
very common in practice so you may not
have heard of it
but it's a stricter coverage criterion
it's pretty difficult to satisfy so we
wanted to see if there were any
differences between something like MCC
and something like statement coverage
the final step was to measure the
effectiveness of the sweets and this is
a little bit less straightforward so we
did it with mutation testing now if
you're not familiar with mutation
testing or if a vet's mom is still
watching in mutation testing you make
small syntactic changes to the program
under test and then you run your test
suite on each of these new programs
these mutants to see if it can detect
the fault that you injected if the test
suite can detect a large proportion of
these mutants it's considered to be a
good suite now this might seem a little
bit artificial but it has been shown to
be representative of the Suites ability
to detect real faults so to briefly
recapitulate we selected five large real
programs large-ish real programs made
test suites through random selection and
measured the coverage and effectiveness
of those Suites so let's go ahead and
look at the data for closure here we're
looking at all sizes we're completely
ignoring the effect of suite size we
have coverage on the x-axis from zero to
a hundred percent an effectiveness so
percent of mutants detected on the
y-axis again from zero to 100% the color
of the points indicates the coverage
type so the green that's furthest to the
left is MCC in the middle of the pink
color is decision and blue is statement
so when we ignore size we actually see
some nice diagonal lines here there's
there's a good relationship right but if
we look at only the size three Suites
suddenly we just have a big cloud of
points the coverage isn't telling us
very much if we move up to size 10 well
the average coverage increases the
average effectiveness increases as you
would expect because there are more
tests but again we have kind of a cloud
of points here no nice relationship is
visible and this continues as we move on
through the size 30 Suites the hundred
suites and so forth the point clouds
become more dense as we move on because
there's less variation in the larger
Suites but at no point do we ever see
that nice diagonal line this brings me
for the first result from our paper
which is that coverage is not strongly
correlated with effectiveness one suite
size is
controlled to really really clarify this
distinction I'm going to show you
another graph here the red line on this
graph is the total revenue generated by
arcades in the United States the blue
line is the number of computer science
doctorates awarded in the United States
these two variables happen to be
correlated with a very high correlation
coefficient 0.98 but it's almost
certainly the case that there is not a
causal relationship they probably both
depend on some other variable like the
size of the population so well it's very
tempting to make causal statements it's
very tempting to say our test suite has
70% coverage so it will catch a lot of
bugs this is equivalent to saying
arcades made 2 billion dollars last year
so a lot of PhD students will graduate
now if we go back to this graph there's
another interesting finding in this
paper these 3 different colors of points
look pretty much the same and we can
quantify that by measuring the
correlation between each pair of
coverage types and we see that it's
always above 0.9 even using a
conservative nonparametric measure of
Association so what this tells us is
that the stronger coverage type provides
little extra information about non
adequate test suites that Suites with
less than 100% coverage and we need that
little caveat because it's possible that
a suite with a hundred percent modified
condition coverage is strictly better
than suite with a hundred percent
statement coverage but in the typical
scenario where you have a test case if
you want to learn a little bit more
about it or sorry test suite you want to
learn a little bit more about it so you
measure its coverage it doesn't really
matter which type you measure you may as
well just measure statement so I've been
a little bit negative about coverage
what should you do instead well if you
were really paying attention earlier in
the talk when I talked about the method
you might have noticed that I said the
mutation score of the test suite is
highly correlated with the Suites
ability to detect faults we demonstrated
this last year in the paper titled here
so this means the mutation score is
exactly the metric we want but and
there's always a but
mutation testing is a little bit
impractical you can generate tens of
thousands of mutants for a typical
program and you need to run at least
part of your test suite on each one of
those mutants to see if it can detect
that the mutant is faulty that you
injected that phone so working on this
is one of my current research interests
as a matter of fact my supervisor just
submitted at Google faculty grant on
this topic so hopefully if all goes well
next year I'll be back with a really
great answer for you and a really great
new tool something that can get you the
information of mutation testing with the
speed of coverage but in the meantime
I'll just leave you with this assertion
that I started to talk with high
coverage does not guarantee high quality
I hope that I have convinced you or that
I believe given you something to think
about so the next time one of your
co-workers comes into your office and
says our test suite is great it is 80%
coverage you can think yourself yeah and
a lot of PhD students are gonna graduate
the official versions of the two papers
I discussed today are pay wold but I
have put them on my website here 'lord
org so you can read them for free if
you're interested in learning more about
those two studies thank you very much
are there any questions thanks Laura and
thanks for my mom to the first question
would be does low coverage guarantee low
quality that's interesting I would
probably say yes because if you're not
running the code how can you possibly
find any bugs in it I would say that
coverage is necessary but not sufficient
what the mutation testing paper is kind
of telling us is that the strength of
your Oracle's is what's really important
about your tests so you can execute
every line of code in your program
without actually checking that any of it
is correct right so how representative
real test Suites do you feel you're
randomly generated sweets are is it not
more common for some sweet some areas to
have good coverage in some areas to have
poor coverage
so well we didn't exactly use
randomly-generated because we didn't
rate them we picked the test cases but
if you just mean is that representative
it's someone representative and someone
not so if you look at testing textbooks
they'll say okay developers this is how
you are supposed to test used to find
the area with low coverage you go right
to test for that or they give you some
very deterministic method of producing
tests and in practice you test a little
more randomly than that right it's oh I
wrote on a feature already test with
this I found a bug here already test for
that in practice yes you would see a lot
of tests for some spots and less for
others but unfortunately there's a limit
to how realistic you can get when you
want to have a thousand sweets for each
program you know you can't get people to
sit down and write a thousand sweets so
could you give some examples of mutants
used in your experiment oh that's a good
question I should have talked about
mutation testing a little bit more some
things you could do are change a plus
sign to a minus like an operator swap
you can switch the arguments to a method
if they're the same type you can just
flip them to a different order
you can change constants to a different
value you can change strings to
different value lots of different things
that you can do actually have you found
any metrics that are correlated with
high quality so the mutation testing
score does seem to be the only downside
being that it's expensive to compute
I've seen some very new work on a new
metric that's trying to sort of
approximate the mutation score I can't
talk too much about it because it was in
a paper I was reviewing and the reviews
are done in confidence but there is some
interesting new work in that space
what about test-driven development
approaches which can effectively produce
100% coverage well I might take issue
with effectively producing 100% coverage
but I mean what about them you can cover
all the code without checking anything
that's really important or you can cover
all the code and write really great
tests that test for every pop
well air-conditioned so they can be good
or bad just like writing tests after the
fact none of these are actually new
questions are there anymore Diego the
statement that higher coverage does not
correlate with effectiveness if the size
is controlled seem like a stretch the
data isn't not spread too much long
either axis isn't that just noise or
inconclusive data no the fact that we
had a thousand Suites of each size lets
us get statistical significance so these
findings were all significant the 99.9
percent level so only 0.1 percent chance
that this is noise and that seems to be
the end from what I can see
go Thank You Laura</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>