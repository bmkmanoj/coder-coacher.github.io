<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2016: &quot;Can you hear me?&quot; - Surviving Audio Quality Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2016: &quot;Can you hear me?&quot; - Surviving Audio Quality Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2016: &quot;Can you hear me?&quot; - Surviving Audio Quality Testing</b></h2><h5 class="post__date">2016-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-8ITJzN4KdU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay next up we have Alexander and Dan
it will be talking about audio quality
something we could really use for
conference audio planning so perfect
perfect environment for it so take it
away Alexandra damn okay thank you can
you hear me which is actually the title
of our talk got you switch slides please
oh oh oh girl it okay so can you hear me
this is the title of our today's talk we
want to show you how you can survive
audio quality testing audio quality
makes or break real time products of
audio quality is bad people will use a
different solution they will move away
eventually from your product so as we
add Citrix have several products using
real-time communication we developed a
web service a common solution that
tackles common problems our solution is
now adapted by different teams and we
are going to present you the solution
today I'm Alex I'm a student from
germany and besides studies I've always
been involved in an audio I've been
involved in scale scale testing
automation and also most recently an
audio quality automation and I'm Dan and
I've worked at citrix as in many roles
all around audio so as a as a test
engineer as a manager and as a test
architect and we're here representing a
large team that has worked to deliver
something and that team is now we're
just lucky enough to be the ones who are
presenting well that seems at home
working on the innovation that will show
next year
so we're going to start at a very high
level and then we're going to funnel
down into something specific which alex
will present and so i'm going to share
the scale of audio at citrix some of the
testing challenges that we set out to
solve a few years ago and then i'm going
to set up one specific end end test and
show you the components involved in that
and finally then we'll focus in on
component that alex work this last year
to build called the audio quality
assessment service so you're probably
familiar with in fact have used one of
the products on this page so go to my pc
and go to assist at the bottom don't
have as much to do with audio but the
top four is what we're talking about
today and open voice is our telephone
only conferencing system and then as
well go to meeting go to webinar and go
to training which are online
conferencing systems typically involving
a screen share a webcam share and then
for today's talk the audio so each of
these products have a client piece maybe
a mac win pc mobile web and we are we're
basically taking screen webcam and some
other information in some audio and
we're sending it up to a conference
bridge centrally located that's mixing
and then sending back out with some
thought to what gets sent out for
instance you don't want to hear your own
voice back at you or perhaps one of you
is muted and you want to make sure that
person is not hurt in the conference and
all of that audio comes back and goes
out at a rate of 1.4 billion audio
minutes per month so that's the scale
that we're working to test and we've
been working hard to improve a couple
other metrics while the audio numbers
keep going up we've also reduced the
number of customer escalations so these
are the cases where customers call in
with an issue that can't be solved by
first line and they go up to our audio
ops so that number has been going down
even while the minutes go up and then
we're really proud of this are our
footprint our star the number of servers
it takes to deliver audio has actually
been reduced by a factor of 10 and it
all along the audio minutes are going up
so we've increased the scale and the
quality and the efficiency ok so let's
get into some specifics here
what are the testing challenges that
we're trying to solve the first thing
that we've learned is that no audio in
an online meeting is a deal-breaker so
we work very hard to deliver screen
sharing webcam and some other features
without any interruption but if you have
a minute or two of interruption on your
screen share you can typically live with
it because you've seen what was there
before or what may be coming but if you
miss a second or two of audio it's
pretty critical and you you all know
this from being on phone calls or
conference calls and you can imagine
being on a call with your your boss
who's happy to tell you that he's going
to give you a dollar raise well you
missed a pretty important word there you
want to know how much it is and so
obviously the network and other factors
are always a part of it but we want to
work as hard as we can to deliver the
best audio we can given the given the
situation so let's look at a couple
equations we're trying to solve the
first is that we have a limited number
of audio experts for testing and so
we're very concentrated on the audio at
citrix we have over a dozen PhDs just
simply focused on delivering audio but
guess what each of those PhDs have only
two ears so we needed to find a way to
clone them into to get their their work
in to automate in automation so that
everyone can use it and secondly there's
many teams that are consuming the audio
platform into the product and so the
first goal let's make testing easy for
non-experts secondly we know that
testing audio manually is time-consuming
and it can be subjective as well and so
we wanted to find a way to a number one
make it objective and then also
something that is repeatable and what we
find is that if you make it easy enough
than any team will consume and add this
to their basic acceptance test but
otherwise you have teams on the
peripheral who whose whose work is
somewhat related to audio delivery but
not specifically and they're less likely
to introduce a new basic acceptance test
unless it's made very easy for them so
these are the two testing challenges
that we set out to solve and all with
the idea that better audio testing would
lead to better audio quality and then
happier customers and as for all of us
our goal is just let's get out of the
way and let our customers talk to each
other we want them to have the same
delight that this tin can
conference has on the screen right now
and so that's what we're setting out to
do so let's talk about a pyramid we
looked at a pyramid earlier today this
one is specific to audio testing and
it's a way to kind of frame the
conversation this might apply to other
types of testing you're doing but in a
way with audio specifically we want to
start let's start at the bottom of the
pyramid so let's think about
functionality anyone who's delivering a
cell phone or a conference call or any
kind of audio sound related software if
you ask them are you testing your audio
you'll most likely get an answer of yes
but the question is which one of these
tests are they doing when they talk
about testing audio and so a very common
feature in an app is to have a mute
button so let's use that as one of our
examples here at the bottom of the
pyramid the audio functionality would
say yep i'm in a call look at my screen
you can see that i'm in a call i press
mute and guess what it turns red or i
press unmute and it turns green so we
have an automated test around that but i
would still consider that audio
functionality and if we wanted to extend
this to the analogy of a plumber if I'm
if I'm a plumber building out a new
house I'm going to go in and i'm going
to set up all the pipes and and connect
them tight but somebody outside the
windows going to yell to me are you
ready are you ready for them for the
water to turn on yet and at that point
all I've done is kind of check the
functionality and make sure everything's
in place but I haven't actually turned
on the tap and when we move up when we
turn on the tap we move up in the
Pyramid of steps so this is audio
presence or in the case of the plumbing
analogy it's saying yeah water is
flowing out the pipes now we know we
have water so audio presence is a very
valuable way to test it doesn't concern
itself with the quality but it just
simply says do we have audio or not and
again the muting example in this case my
automation presses the mute button and
now I'm actually verifying that while i
muted you're not hearing my voice so
very important for those of you who work
from home and mute from time to time
I'll leave the rest to you at the top of
the pyramid now so if we turn on the
faucet and we've got water coming out
right maybe we pour it into a glass but
we haven't yet answered the question do
I want to drink this water is this is
good enough for me to drink and so it is
with audio do we want to is this audio
quality good enough
that in one sense I'm not even thinking
about the audio is just it's just there
in other senses we can even enhance what
might be poor conversations and make
them better and so it's this top of the
pyramid that we've been focused on it's
the it requires the most expertise and
yet what we wanted to do was take take
as much as we can make it common
components and then deliver it to those
who are working in the functional level
who already have automated tests and
give them something that wouldn't
disrupt their existing test much they
can add in the audio quality and boom
verified on the way out the door and we
know we're good so again just kind of
restating our design goals here what we
see in in green are the product teams
the many teams who have their automation
working on their functional tests and
what we see in the orange our desire is
to separate out that which required
expertise and we can build into common
components and deliver so it's one
universal test solution that all teams
can use ok so let's get a little more
specific here i'm going to set up for
you what we call an end-to-end tests and
we have a lot of different kinds of
tests that we do it at different levels
component levels and so forth but this
is kind of our our end end let's get a
view of it make sure that number one our
quality is good but also that maybe we
haven't introduced something that would
that would cause a regression so here's
the four key components you would need
first of all a standardized input file
you know like any kind of test and
you're doing you need a control set
we're using actual voice files we found
that using different voices because it
processes and sends differently we want
to make sure that every kind of voice
would sound good in the product and then
we've got to figure out a way to get
that into the product in an automated
fashion so just as you know my voice is
going into this microphone goes into a
system let's call it a black box and
then comes out the other end and and
then it sits out in the airwaves again
your ears can hear it what we're trying
to do is kind of automate or virtualize
or mock those those interface points
right where the air meets the digital
world and so we had to arrange a mock
microphone that would inject audio as if
it were a microphone selection in the
gotomeeting application and there's
third-party tools to do this some open
source and we started with those and
then eventually we
we adjusted our own audio engine to
include the inputs ourselves and from
there now we've got a known input file
its going into the product and of course
then you send it through the typical you
know the conference bridge that I
described and and bring it out the other
side so we've got a client on the other
side that we started with automation and
the audio arrives there and now again we
have a mock speaker which captures that
and pulls it out into something like a
WAV file or a raw file so now we're
sitting with two files one's a known
good ones an unknown and now this is
where we need a way to analyze and
figure out what to do with that audio is
this good or not have we improved or at
least not regressed on the on the
quality and so the audio quality
analysis services is what we came up
with and what we've been working to
build it includes some several key
components here and I'll show you on the
next slide so alex is going to talk in
depth about this one of them is the
Polka license which is a third-party
objective measurement tool that we chose
we've also doing some frequency analysis
and he'll give you a lot more detail on
that but again we're here with the
orange we've abstracted the common
layers and made it available to anyone
and then whatever client type we're
working with whatever platform we're
working on and whatever automation our
teams have we wanted to give them
something that they could use something
simple maybe a rest interface that they
can upload their audio files to will
give them back a score and then they can
decide whether that's a pass/fail so
here the area in red is what Alex is
going to be focusing on as he comes up
to share next ok thanks 10 for providing
the background and let me take over on
the motivations I will recap and go more
into technical details so first
motivation we want to make audio quality
testing as easy to use with no expertise
needed and second we want to have
platform support we have different tools
for automation we see and we have
different endpoints Windows Mac mobile
endpoints android windows mobile but
also web-based endpoints and automation
we want to support oil fet
third motivation we want to visualize
results over time we want to track
results and see how how the audio
quality evolves in different builds and
forth license restrictions some
algorithms videos are bound to certain
machines and making making this audio
quality analysis as a service check with
this problem so we yeah it's quite a bit
catchy AQA as a service that's internal
service we expose to our to our
infrastructure and our tests and we
provide tests for quality the top of the
pyramid and we heard earlier and
presence which is the medium level this
all as accessible through simple
interfaces to the testers so here it's
getting a bit more tech technical it's
the design and implementation of the
service on the left hand side on the top
we see there's a client-side and
server-side the service has a back-end
it connects to a database and object
storage and let me dress go through a
simple interaction between client and
our service first we upload audio files
or audio raw data with parameters like
sample rate second we describe and we
create a job we call it a job that is
specified by files and by a method of
evaluation then third step is starting
the job and in the end we just fetch
results and see what the evaluation gave
us implementation wise we had some
requirements to the service and it had
to have a native component because
signal processing algorithms are running
natively so we chose nodejs and made a
native c++ module for it
and this native C++ module integrates
the Polka algorithm I will talk later on
about then the Sox utility which
provides a lot of features for audio
analysis and different codecs like opus
and mp3 to decompress files to a sample
based format the service on the front
side exposes its interface through rest
and gr pc based on google proto buffers
to support multiple clients and we
defined a client api that we implemented
in Java Python and C++ ok so I want to
talk about the first category of our
testing methods and this is quality here
we chose polka we evaluated different
algorithms and polkas a third-party
algorithm be integrated it it basically
gives us a mean opinion score MOS score
the mos core is a value a score between
125 and five is the best and
historically MOS score or what it
basically means you you ask people how
they would how they would rate their
perception of audio and polka does this
automatically it it just evaluates the
part of the audio file that the human is
really the human really percepts as good
or bad speech quality so along with the
mean opinion score polka gives us other
metrics like speech presence or levels
and polka is called a full reference
algorithm so we need a reference signal
and we need a degraded recorded signal
and yeah that's what then showed us
earlier on the slide we we have a known
reference file and a recorded file and
in the end we can computer score between
five and won the second category as
presents testing here we currently have
exposed three algorithms first at some
frequency analysis speech presence and
amplitude analysis for frequency
analysis we can identify different
active speakers so imagine a meeting
each participant would play out a
different frequency and in the end you
can verify that that certain speak us
have been playing out frequencies in the
meeting so we get regions classified two
frequencies as result then a second
category is speech presence here we get
regions classified as active speech and
the third method as amplitude analysis
here we can verify that for example that
certain speakers are not too loud or not
so silent okay I will I want to give you
a live demo of the service
ok
you should see it in a second
okay so what what i'm doing here is i'm
using the Python client API that that
defines or I've three three tests and
each test uploads a set of files and
execute the Polka drop so first we we
create the drop we add the files
reference biodegraded file we add text
for later on identification and tracking
off the results and we start the job in
the end we get the state like the result
and specifically the mos score and we
verify that the score is above a certain
threshold of 4.0 the first test is using
like a non degraded file the second
testers is using a recorded file that
has been recorded in a condition with
five percent packet loss and the third
test in a condition with ten percent
packet loss okay
well it's running it takes them some
seconds I would yeah the first one is
passed but a switch here and I will just
show you the front end here we have a
dashboard we can see different kinds of
tasks and how much have been processed
and how many currently are running along
with information about the machine then
in a separate view we have jobs we can
see we can see all the jobs that are
have been processed you can filter for
checks and dates and what we also have
as a pair a description of API and locks
ok so the tests have all been run
through so first off we have the ten
percent packet loss and here we see it
it gives us a quite bad score of 1.6 I
will just play out the degraded file so
the first one is the reference file the
second one is the degraded file i will
play out and we went to pick him up
would you please read the date and time
stamp indicated on the lower right hand
corner ok we see there's we hear there's
some distortion so the five percent
packet loss simple i know you went to
pick him up would you please read the
date and time stamp indicated on the
lower right hand corner yeah this gives
us a score of 2.4 and then the best file
which has been recorded in a condition
with no packet loss simple i know you
went to pick him up would you please
read the date and time stamp indicated
on the lower right hand corner yeah we
can see the scores 4.6 which is a quite
good poker score
okay back to the presentation
ok
goods
yeah so we have some minutes left for
questions we are happy to answer very
good round of applause crowd today I
think I need to switch this laptop to
display the questions but I have the
first question here and I'm not lying
this is a true question do you have any
way of measuring audio latency in
addition to quality well in a sense the
the latency here we take that the
latency in a test like this if they were
even if the file arrives or the audio
arrives 33 seconds later but in the
exact same quality it was still score
good on a test like this because where
it's just time shifted but it's still
good quality so in terms of latency then
we would need to set up a different kind
of test and where we could kind of time
when the audio was begun to send and
then time when it arrives and arrive at
a latency score so not something within
this system but yes okay and how do you
replicate different bandwidth scenarios
EG good use of EG by the way slow
internet connection yet maybe i can
answer that EF and our test
infrastructure it's a degrade on that we
can control with with api and automation
and there we will just limit bandwidth
and packet loss yeah ah yes now that we
can see the questions I'll take another
one from the online is this is the
acceptable threshold for audio quality
dependent on the language or dialect
being used no in this case we're
comparing waveforms so it doesn't matter
what language or dialect is being used
which is very helpful any questions live
questions thank you
hi I'm loose man from blah blah car I
just wanted to know if there's any way
of testing certain filters or
Corrections that might be happening on
the client side or wherever it may
happen such as noise cancellation right
this would this is a challenge for
objective testing because in many cases
it's it's comparing a source and an
output file whereas in the case you're
talking about the clients actually
improving the audio through something
like noise cancellation so in effect it
makes the input file different from the
output file in a better way but the
algorithm does not always detect that so
for that kind of testing we have to go
back to those 24 years I was talking
about know how consistent are the
measurements if you run a test many
times we always get the same outcome and
it kind of depends if you run the whole
test many times like recording the file
many times because yeah network
conditions can can change but yeah the
algorithm the Polka algorithm for
example is completely deterministic so
you would get the same outcome but if
you have changing network conditions you
would get slightly different outcomes
and just raise your hand if you have a
another in room question how would you
do you think it should be effective in a
podcast scenario where maybe delivering
via web you know some sort of applet or
mobile based audio quality testing
that's a good question so it's one
directional audio so we no longer have
kind of the time sensitivity we can
buffer and make things sound better on
the other end so a little bit less of a
challenge for the audio quality but I
don't see any reason why we couldn't as
well put it in get it out and then
compare the two scores it seems right
I think we have time for one more so if
you run an algorithm with ten percent
packet loss ten times how much score
variability wouldn't this indicate true
value of your polka test was a couple
parts of that answer maybe I'll take one
and Alex can talk about the second write
the packet loss itself again whether
it's deterministic whether it changes
over time that is going to affect what
comes out the other side and so we we
can work with degraders that do both
which degrade it in the exact same way
10 times or we can we cannot maybe just
a comment on the true value of the Polka
tests i think the true value of this end
end test that I that i'm showing
describing that we're presenting here
today is is essentially making sure that
audio quality stays the same over time
no matter what you do what changes you
make and so it's not that this test
itself is kind of the hey here's the
number and here's how good gotomeeting
is an audio it's a 4.7 it's it's a
matter of hey we scored 4.6 yesterday
and today it's a 4.2 why what went wrong
what can we double check there anything
to add all right well thanks Alexander
and Dan thank you
although I guess as a final follow-up
question I wonder if this means i can't
use you're breaking up as an excuse
anymore thanks to their testing i use
that all the time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>