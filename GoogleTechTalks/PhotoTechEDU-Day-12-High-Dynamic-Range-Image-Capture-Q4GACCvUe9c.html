<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 12: High Dynamic Range Image Capture | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 12: High Dynamic Range Image Capture - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 12: High Dynamic Range Image Capture</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q4GACCvUe9c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thanks for coming too photo tech
day 12 we said we'd do 12 or more
lectures today's our first outside
lecture and without further ado I give
you Greg Ward from any here software
Thanks Oh applause I apologize for
looking like a drowned rat I made the
mistake of taking my motorcycle this
morning so it looks like I can't hold my
orange juice my rain pants leaked so I'm
going to talk to you about some software
I've developed since working at
shutterfly so started working on this
about 2000 and gradually have plugged
away at it it's an image browser that
allows you to take and manage high
dynamic range foot photographs so it's
the it's going to be mostly a demo if I
can get the slides to move so my
motivation at the time was the existing
browsers were either file browsers or
they were cataloging tools and it wasn't
anyone that would do both well generally
generally the file browsers were snappy
and the cataloging tools were painful
and I wanted something that was snappy
and would also catalog my my images and
none of the existing browsers had any
support for HDR I think that's changed
somewhat and then I didn't like the way
the browsers were set out so my goals
were to be able to browse high dynamic
range images and maintain catalog
information and also track image files
as opposed to storing them in my own
data structure so so I wanted my own
database of where the files were then I
want to be able to find them wherever
the output so things that that I have
achieved so far as I do have fairly fast
interactive response and you can access
the thumbnails to images even if the
images are not currently accessible and
then you can build you know do the usual
things with building photo albums and
web pages in and drag and drop and so on
so things I haven't quite gotten yet to
yet a plug-in interface for photo
printing I mean that was
the initial motivations we wanted to
have a decent client tool but there was
no money to develop one and I haven't
gotten to linings or Windows versions
currently I only have a mac version but
I'm talking this afternoon with the
person who's supposedly working on the
pc port and then I'd like to support
more image formats so the basic browser
layout is shown here and the user
interface looks looks immature because
i'm a pretty immature GUI programmer but
the concepts are clear enough you have a
tab tabs for selecting different ways of
searching the data and then you have
just a well with your thumb nails in it
and each time you you know double click
on one of these guys you it starts a
database search and my catalog has
20,000 odd images in it and the database
search takes a fraction of you know a
second like tenth of a second and then
you have the image viewer window which
allows you to do the usual things of
zooming in but it has additional
facilities for controlling ton mapping
on high dynamic range image which will
play with and you can also do some some
database operations within the viewer
window there's an additional info window
that allows you to access other database
fields either in this month either
either allowing you to modify them in
some cases are read-only fields the
basic architecture of the system is you
have the user interface or you have the
program that talks or creates a
thumbnail cash various catalogs and of
course the preference is filed and then
you have just your images wherever they
are on the file system and you can have
them on multiple file systems I have an
external drive i have dvds i have my
laptop drive and I'm always like kicking
images off my laptop drive when I'm
running out of space it's a chronic
problem with disk space on laptops
so the architecture of the browser is
divided into the system specific GUI
layer and the system independent library
and underline the system independent
libraries this memory cache manager and
this is this is what's critical to
maintaining speed because images of
course are large and you don't want to
be reading them in all the time nor do
you want to keep them in memory all the
time so you have to have some
intelligent way to deal with memory and
so I essentially wrote my own it's it's
not a virtual memory system but it it
helps the virtual vm system do a better
job and then there's the database
manager which is tied closely to
thumbnail manager and there's this whole
to the imaging library that handles
floating point as well as other image
types and that's a plug-in interface so
with that brief intro I'm going to talk
a bit about high dynamic range
photography how do you how do you
capture these images what can you do
with them and so on most mid-price
digital cameras well even the low end
some these days have an exposure
bracketing mode and the header
information of course contains the
exposure information which you can use
to to combine the images to get a single
high dynamic range image from the
multiple exposures and you need to do
this properly you need to compute or X
or estimate the camera response function
you're not going to get it exactly but
you can get some reasonable estimate to
undo the undo the tone curve in the
camera so you can combine images
properly and and I don't use this tech
news the Dybbuk and Malik technique that
they were the ones who popularized it
and there are actually earlier
techniques but I use the one by mid to
nog and I or which is a polynomial fit
and that there are quite a few
techniques and they're all pretty
competitive the real trick is image
registration because most of my pictures
are handheld and then you then I've
added options to reduce lens flare and
ghosting
so I'll do this in a minute so again the
concept is that you take multiple
exposures and Yuri combine them into a
single high dynamic range image and what
I'm showing here in this graph is the
estimated camera response function which
is this curve it resembles a gamma curve
but it doesn't exactly fit at gamma
curve because actually as many of you
know camera makers like to give their
images a little more punch little more
saturation and higher contrast and so
this curve doesn't exactly follow a
gamma curve but if you apply again you
know a gamma curve to it you stick put
this up on a standard display you'll get
an image with boosted color and contrast
so in order to linearize the the color
space you really have to figure out what
this curve is for your particular camera
and then the other curve shown here
which is a hat function specifically i
called the Stetson hat function because
that's what it looks like just Maps just
figures out a weight to use for each
pixel so here are pixel values from 0 to
255 down here this tells you what wait
to give each of those pixel values
according to how accurate you expect
them to be so obviously the ones near
black are going to be pretty noise you
don't really want those and then this
weird shape is based on the differential
between quantized values and how
accurate they are based on the
quantization steps and then you you of
course have to drop it off at the high
end again because saturated pixels are
less reliable so this shows an example
response function this is a log-log plot
where we have the image value down here
and the scene value over here and if you
take a gamma to 2 curve that's actually
a straight line on a log-log plot that
would be this line so if the cameras
really did match a standard srgb color
space then your points should all I
along this line in fact they don't as
you see here we have this boosted
contrast going on and here's the
polynomial fit in red that that the
Minton augen higher technique came
so how do we do HDR caption photosphere
like I said you can take bracket
exposures such as these and then
photosphere will align the images if
necessary and there's options for doing
ghosts for removal and lens flare
removal which I'll show you so the image
registration is important because even
if the cameras on a tripod unless you
have a tether there's going to be a
little bit of shake a little bit of
movement just from touching the camera
so I came up with this technique after
trying a number and failing of using the
median threshold bitmap which is shown
here so here we have two exposures there
at very different levels obviously so if
you create an edge map out of each of
these even if you're you're careful you
end up with a very different set of
edges so matching these two up and
trying to align them you could do it
visually but you can't really do it
easily with an algorithm so using the
median threshold bitmap that the median
value in a scene as long as you're
looking at the same roughly the same
scene doesn't change much and so by
using the meeting as the threshold you
can generate bitmaps that are very
similar even for quite different
exposures and then we use a pyramid
scheme to arrive at the app at the
actual alignment so you start with a
very low resolution version of this
bitmap you line those just by going you
know a pixel or two left right and
rotating and then you work on way your
way on up the period pyramid and this is
quite fast that alignment results here
we have the original unaligned exposures
which are shown in this blow-up detail
you can see they're off by a few pixels
anyway and then here's the hylian
persian and and the lineman algorithm
gets within half a pixel but I don't
subsample yes
yeah it's rigid rotation there's no
warping involved radio distortion
correction person that's not necessary I
mean generally the motions we're talking
about are a few pixels so in terms of
distortion and and other things going on
it's not that important if you frighten
larger disposals
this is handheld a lot of these are
adding up but but then there are going
to be within a fixed
yeah and line it out and we'll work up
to about that but it depends on the
resolution of your camera to as to you
know have how far ago but but the
failures that I've had with his
algorithm haven't had to do with too
much motion they've had to do with
uneven motion like waves coming in and
clouds going another way so it tries to
do globally and the things are moving in
different directions globally it will
get confused and other scenes like
weight and grass you know it's just too
much stuff going on and it can't figure
it out maybe
parallax is a whole other kind of
problem but but I mean in the time that
it takes it to to capture one of these
sequences I mean I brought this this
little Olympus camera but not my Nikon
will go through shoot through nine
exposures in under two seconds so you'd
have to be on a fast-moving train even
get much paralyzed laughs oh yeah okay
I'm going to talk about that in so goes
removal is another problem of course you
can't keep the scene static every time
so people might be moving trees might be
blowing around and what what happens is
you get this sort of an effect where
your multiple exposures when you merge
them together with that weighting
function you get different positions for
for the things were moving so I
developed a simple algorithm was based
on a suggestion someone made that if you
just pick particular exposures you know
for those regions it actually works out
so here we can see you know this
exposure or this exposure even would
have been fine but it's just the
combining of the all of them that causes
this sort of ghosting effect and we do
want to combine it for other regions
regions that aren't moving but we'd like
to somehow identify these regions with
motion and treat them differently so
that's easy enough to do you can just
compute the variance between exposures
pretty easily and that and identify
areas that have high high variance I
change between exposures and then you
need to segment those regions because as
you can imagine you need to pick out
regions and then choose an exposure to
go with that region you don't want the
regions to be too large you don't want
them to cover the whole image because
then you're not going to get one
exposure that fits the whole region so
you want them to be compact but you also
want them to be disjoint so this is just
a coloring of the different segments
that it came up with and some of these
areas segments that it came up with
aren't really in motion but they were
over threshold for the variance because
of fringing in the
and so on so this is the the combined
result and you can see we sort of like
lost a bit of this guy's foot it got
faded out here and parts of the people
are are still a little bit ghosty but
it's way better than it was so any
questions about that forces like your
hair varies calculations just really
good at least get
some
devotion and you just get one person
walking through give variants over the
entire to second sequence you would have
big spear but airline air much smaller
space oh I can't see what you're saying
staffers upset with some additional
now that would be interesting to try and
I didn't think of that yeah this isn't
the same solution so showing the other
slide and where the just exposure see ya
characters would place this year see so
looking at the final image the one that
the one what we chose is where this
guy's right next to the sign for that
big purple segment so he was right next
to the sign and like the lightest
exposure so I think we chose from this
one for that region you base it on you
know what didn't max out the exposure in
that region say word about the solution
of the slide that increased president
it's decided before them not only fun
yeah those those things look too dark oh
well part of it says town mappings
what's your computing under mortgage any
protein composite evangeline such as you
brought me madam well they are in the
dharmic article see the backlit theme so
good from the middle exposure in Venice
equal to Islam
I don't remember the long ago dive into
this slide but anyways the best solution
to get later as much better than version
yeah you could have been an early not in
your twice that sometimes happens the
person will end up in there twice as
they move from light to dark earth
reverse that could end up in their place
so another problem with especially the
less expensive cameras is they were
never meant to capture it high dynamic
range and so the lens design is such
that there's a fair amount of a flare
that they just don't they don't control
and so get scattering both on the optics
is but i'll put primarily on the actual
aperture there's scattering on the edge
of the aperture good cameras have very
good lenses have very carefully designed
aperture veins so i took this camera or
one just like it and took a pin hole in
a piece of aluminum foil and
photographed it to see what the flare
actually was so this should be a perfect
circle if it were if we're a perfect
lens but as you can see there's quite a
bit of energy that that is exposing the
sensor around that that position so from
this image you could actually derive
what the point spread function is
incorrect for it I mean you could just
also go into Photoshop and make a circle
of the right size and say I'm done but
you don't get this kind of image to work
with what you typically get is this kind
of an image where someone is just taking
a photograph and you could say well
can't you calibrate the lens and figure
out what the point spread function is
and just correct for that well
unfortunately the point spread function
changes with humidity changes with
aperture changes with with all you know
everything that that touches or changes
will n zoom anything changes the point
spread function
it does a little bit well we kind of
ignore that in our solution because
we're going we don't have enough
information to characterize the point
should function as it changes over the
over the image but what we can do with
an image like this and is look for dark
areas dark regions around bright regions
and estimate the point spread function
using using a rms kind of you know
immune squared kind of approach and
doing that we took again it was this
camera and those previous two images the
the flare image and this image we're
taking it with the exact same settings
like shortly after each other so
hopefully nothing changed on the lens
and then we measured the point spread
function from the pinhole and that's
shown on this red dashed line and then
this is the point spread function that
we estimated based on our algorithm
applied to the image of the apartment
and they're not a perfect match but we
wouldn't really expect them be because
the the point spread function estimated
from the apartment image was taken of
was averaged over the whole image
whereas the point spread function for
the pin hole was measured at the center
and that's going to be the best case so
we expect a little more spread when
you're averaged over the whole image and
then we can undo that and here's an
example where we we've undone the point
spread function in this image of a
Rosetta stained glass and and this is
the corrected version and this is
basically the flare lens flare that was
removed from this corrected version
my SLR camera doesn't have this problem
I almost never need lens start for me
because the lenses are much better yeah
that's the most of the major question
comes how do you do foo let's go I had
no ability I think you're talking about
sundogs where you have like a bright
flare source and it's causing reflect
into reflections that shows multiple
color kind of now as you get a second
different cameras will do different
things
nevertheless the worst
no I haven't worked at anything more
complicated than this I mean yeah like
dick says different lenses will do
different peculiar things and it's kind
of hard to come up with an algorithm
that will just generally recognized what
doesn't belong in the image and remove
it just as always there's always that
easily distracted estimated time to
fight
this
essentially spray get back to
very people request oh um he was asking
if it would make more sense to apply it
deke involve one Tina the point spread
function just to put the energy back
where it came from I think that would
make sense if it were a much energy but
it's not much energy it's the only a
tiny fraction of a percent that gets
spread but because it started so bright
it becomes a parent and that's why it
only shows up really on the HDR it's
only really objectionable on HDR sit see
if we showed this as a linear tone
mapping you basically be looking at
little white spots surrounded by black
but we've tone mapped it too and that
you know crushes the contrast range so
you start to see these things that you
wouldn't see otherwise but you'd also
see them on high dynamic range high
dynamic range display so okay so now let
me do a little demo this works so
currently photosphere is just being
distributed as unadvertised freeware we
also have a command line version that
runs under linux as well as OS 10 and as
i said my friend is working on a
multi-platform port so i'm going to have
a little issue with my screen
huh so it looks like PowerPoint was
trying to be clever and throwing it into
mirror mode which kind of work never
done this before it's interesting new
version of PowerPoint new fancy features
so let me start with a quick demo and
try taking a series of bracket exposures
just to show you how easy it is assuming
it works ok so this is when I bought
that this camera was like four hundred
dollars but now you can get these these
particular ones on ebay for under 200 I
like it better than the newer cameras
because the new cameras just have higher
noise tinier pixels with worst lenses so
besides this I have an SLR as I
mentioned which I'm quite happy with so
I first choose the mode and I have to
make sure that it's in a bracketed mode
which had a way to display this but
basic basically I'm just choosing 5
exposure brackets separated by 1's top
and then I also have to make sure the
white balance is fixed because if you
use an automatic white balance at least
on this camera it will change the white
balance from one exposure to the next
which is wreaks havoc on the whole
algorithm and I've heard that the newer
cameras do even worse than this they
also change their tone mass curve per
exposure which is a disaster shooting
roller dude i'm shooting jpg and and you
can shoot raw on some cameras but
photosphere doesn't handle raw and it
doesn't actually benefit the results
much in mean the main advantage of raw
is that you're in a better known color
space and you can just say yes wondered
if it was possible to
people have done that in fact I saw a
poster or two he a poster to emit what
if the thing is if it's handled is not
normal a line so you have to you'd have
to be really clever about how you did
the automatic alignment and how you did
the D mosaicing afterwards because you
know once you've entangled those two
it's a bit of work so I'm just going to
take a couple of HDR shots so people
hold still other thing that I have to
fix is the the aperture not the fastest
camera in the world wonderful little
camera but you can't get the smart media
cards anymore stop making them alright
so
I used to get compact flash and smart
media confused and then I realized that
it's easy to easy to remember because
the compact flash are the big bulky ones
that have some intelligent built
intelligence built in in the smart media
or the compact ones with no intelligence
yeah the last question so if you did
something like this technique but same
exposure just multiple rapid-fire
exposures you take advantage of the
jitter sort of good super-resolution
deal with the Bayern a sort of eliminate
Bayern windup of a bogey on camera
possibly but um you need things to be
really really still for that for you
actually want a big solution for that
right because then you're great with
your friends know you want to know where
things are but you have to
sorry I have a bunch of Oz when you're
removing ghosts I'm not going to do any
of the options right now I'm just going
to have it combine the images to show
you what it looks like without any fancy
stuff what's the use you save response
oh I'm
some exposure sequences are better than
others for deriving the camera response
you're better off with like big smooth
areas and fairly wide dynamic range
scene and so the wave photosphere works
is it will give you the option the first
time you use the camera of saving the
response and then you can reuse the
saved response rather than recomputing
it each time which is more less reliable
and the new version of photo Photoshop
the cs3 beta does something similar but
it's a little more intelligent about it
ok so you can see people moved a little
bit I wasn't holding camera quite still
so I can try next just doing the image
alignment without ghost removal and see
what that looks like nobody's moving
that much so I don't expect ghost
removal to do much but I'll do the image
alignment anyway so now it has a second
pre pass where it's figuring out the
alignment between images as you can see
it's pretty quick like I never turned my
mic on testing
so that was before this is with
alignment so you guys are good at home
still it's all that all that practice
working on your computer's staring for
hours ok so I don't think that goes for
movable is going to do anything for this
so I'll just leave it like that and let
me do the next sequence so I took two
and and this scene is probably not going
to do much was as far as flair and
removal either sell and bother with that
you said oneself or yes what's the
maximum
I think three f-stops you can even go to
3f stop so the canon 30d i think we'll
take three expo only take three
exposures so there you're better off
setting three f-stops apart if you have
if you have a scene with a lot of
dynamic range like you're in a forest or
something alright so there's the second
image so let me pick a point between
them looks like I didn't get that much
overlap so I'll choose feature over here
maybe so this is my panorama algorithm
is is kind of clunky the interface to it
you have to basically help it find a
feature you sort of say features around
here and then it does a little wiggling
to actually align the two or match the
two up and then you can make it in 10 HD
or panorama
so it's computing the translation
rotation it just all it does for the
feature is it finds a match point and
then it grows a seam from that so it
doesn't basically constrains X&amp;amp;Y what
doesn't do rotation it actually it it
actually just assumes they're the right
orientation and it creates the seam by
blending the low frequencies and
splicing the high frequencies so if you
look at it up close you can see a little
bit of funny wiggling going on as is
trying to get things to line up and it
totally messed up lenses face there and
dicks shoulder but for natural scenes it
works really well your other scenes not
so well yeah
yeah well I wanted to get something
working quickly so I'm not going to save
this
I have examples which can of natural
scenes where I've done panoramas
so this is from a recent trip up to
Crescent City to visit my brother so I
think this is three images and so
there's a splice running along here
somewhere I've even spliced waterfalls
and then work pretty well this you can
see some schmutz up there but for the
most part it's hard to find the scenes
and it's because the seam with the seam
winds its way through so the high
frequencies are spliced and the low
frequencies are blended in that and your
eye doesn't pick pick that the cmap as
well as easily so another example this
one is like just two images splice
together but again it's it's better on
natural scenes than an artificial ones
because artificial ones if you start
wiggling straight lines it really jumps
at you jumps out at you now they were
taken with the nikon so that they're
either they might have been seven or
nine exposures going in and the other
thing i wanted to mention is that most
of my HDR images i store and this is jpg
HDR format because otherwise they take
up a whole hunk of space like this this
image as soon as it opens up is 13,000
bye-bye almost three thousand pixels so
in an HDR format that would take on the
order of thinking ninety megabytes or
more but with jpg HDR this is four and a
half megabytes what is judy's yard
jpg HDR is a format that I developed for
bright side technologies that if you
take the image and just drop it on to a
standard viewer that doesn't know about
jpg HDR such as most viewers these days
you will get just a load low dynamic
range tonemapped version of the image so
this is what you would see so this is
basically Reinhard's tone mapping
operator applied to that image but then
there's additional information in the
JPEG markers that allows you to
reconstruct the high dynamic range
version the additional information takes
about twenty to twenty-five percent more
file space so it's a chunk of data but
it's not huge no is stored in the JPEG
March marker 11 so the JPEG has I think
15 user 15 application markers that are
used for various things and so it's a
compressed ratio image so it's a JPEG
compressed log luminance ratio image and
the other thing that I didn't mention
about that HDR or about photosphere is
that the images actually have physical
values in them so if you go in it'll
tell you the skies is fifteen hundred
candles per meter squared and the ground
averages 200 candelas per meter squared
and so a you can use this for image
analysis photometry and that sort of
thing the sun yeah and you can calibrate
it i mean this i seriously doubt i
captured the full dynamic range of the
Sun because it's well I'm close three
times ten to the fifth I think the Sun
when it's full up in the sky is about
ten to the six candles per meter square
so I might have actually captured the
top end of this you can sometimes tell
by looking at the histogram now see it's
clipped so I didn't quite get it
there's also a false color mode that you
can use you can play with anyway so
there's a lot of stuffs you can do for
from a photometric application
standpoint and as far as I know
photospheres the only application that
makes any attempt at photometry so any
other questions I'm going to talk about
jpeg HDR in a minute so yes there are
issues with sensor noise not necessarily
related to low light photography because
the cameras have the ability to subtract
that most of them but there are issues
with see some CMOS sensors and some
noisy sensors if they if they have like
little bits of noise or leakage at a
higher level sometimes when you combine
them you'll have this really dark
exposure that has a not quite black
pixel that really should be black
because of leakage is not black and that
will show up as a bright spot if if the
images are perfectly aligned in all the
exposures will show has a bright spot
about the various algorithms is what
bright lights in dark regions shocked
more experienced I haven't noticed that
I haven't noticed that this other
problem with with sensor leakages 1 i'd
like to solve though but it is one that
would really benefit from looking at the
raw file because otherwise it's
difficult to identify where this where
those problems are
yes the images aren't perfectly aligned
yes but if the images are perfectly
aligned and there's one pixel that isn't
corrected for and that's what I mean on
my Nikon i have a few pixels like that
that aren't corrected for this this
camera which is much less expensive
camera has a function to remap the
pixels the Nikon you have to send it
back to the factory and wait six weeks
fit Thank You Nikon ok so going back to
the slides are we for time so as i
mentioned i have this HDR stitching
thing and I here i have a quick
comparison to competing methods like the
splice everyone's favorite photoshop
doesn't in the cs2 anyway it doesn't do
much more than a splice it does this
crap linear blend and they have an
advanced version that has really poor
implementation as far as i can tell her
tailed sins algorithm burton adel since
algorithm that looks like this well
actually now it doesn't look like this
it looks a little worse than this this
is my implementation of burton Adel's
and problem with it is that you can
still see the splice because even though
your splicing the correct frequencies
you have changes between the images that
show up yes library
the legalities
I didn't go all the way down to DC no I
didn't go all the way down to DC because
are you talking about the the variation
variance you see from here to there well
the visibility the scene is actually due
to motion in the scene so you have a
diff you know you have the five
frequencies but they're in a slightly
offsets place because the clouds are
moving and and the waves are moving well
this is zoomed it we're seeing the
pixels here I think okay the only fun
through today I don't know
I went down to a pixel level at the top
and so the high frequencies are let the
bit method because the clouds move you
actually see the the break in the high
frequencies
well I don't know what to say maybe
maybe there's something missing in my
implementation but it's better than
photoshop's well the blend what happens
in Photoshop is it blends and so you
lose the high frequencies at the blend
and in photosphere it's not perfect
either I mean we have this problem with
the clouds or the fog rolling in over
the hills and it's in such a different
place doesn't know what to do here up
here we'll be doing some blends we'll
just be doing blending and then down
here where we have right high
frequencies will do a splice high
frequency splice as well and the splice
moves around so it's not as visible if
you look really close you can spot it
it's not a graph cut it actually it's
similar to a graph cut but only in the
high frequencies it also does some
polling so if the image is aren't
perfectly line it will sort of like pull
them together
yeah because then that's what happens it
makes the edges look nasty so I need
some hysteresis in there but I just
haven't gotten around to implementing
them so that in the future I sincerely
hope that they have high dynamic range
cameras so we don't have to do all this
nonsense and it's quite easy to do using
CMOS sensor technology or even just
reprogramming existing sensors they have
they have some some HD r CMOS sensors
but they're generally pretty low
resolution but big part of it is just an
education process getting the Japanese
manufacturers to change their ways as
Sir dick wolf agrees is really difficult
so we came up with this method for
changing the way a ccd scans out and get
two exposures in one shutter opening so
the old program they have this
electronic shutter that allows you to to
take a rather crude short exposure and
this is what it uses in video mode but
you can you can leverage this this chip
design by taking a short exposure right
after your longest exposure so you you
you clear the sensor you open the
shutter you have you know nine-tenths of
your exposure time go by you slide those
pixels under the electronic shutter wait
the 110th remaining close the mechanical
shutter then you scan out the longer
exposure or shifts can out the shorter
exposure and you have two exposures that
are really close together it's kind of
like it you like a slow flash because
you have the short exposure immediately
fall in a long one so I compared results
I didn't actually implement this because
unfortunately no I don't have access to
any cameras with open firmware but I at
least tried a comparison of five
exposures versus to exposure to see the
differences and they're actually not
that big difference in ISIL was
separating I was really stretching it I
was separating them by four f-stops
which is kind of pushing
limit of where you have a good central
region but the results are acceptable
they have slightly higher noise as you
would expect because you don't get as
much exposure averaging but it works we
just couldn't convince the manual we got
very this close to convince you to
manufacturer to implement this but they
ended up refusing because they said well
you have to prove to us that it works
and we're like well we need to your help
to do that it's like well we're not
going to help you can't prove it so I
have licensed parts of photosphere to
bright side technologies and also to
Adobe mostly just the HDR image builder
stuff but bright sides using all of them
so if anyone's interested so one of the
hindrances to HDR adoption is of course
file size oh let me remember something
here kick out my camera before it goes
dead
um will start to show him excuse me so
so we came up with this idea for this
was work I did with Marianne Simmons for
bright side technologies she's now at
walt disney for as I explained before
including information to take an
ordinary jpg or ask for the JPEG that
contains a tone map image and turn it
back into the HDR Israel so there's some
rationale between the behind this kind
of lossy high dynamic range lossing
cuttings are all about perception and
HDR is also all about perception there's
not much point in recording what that I
can't see especially if you don't have
the space to do it and we want we want
our loss eh dr to support imagery to the
extent the eye is capable of perceiving
it and it's really requirement for
consumer digital cameras as well as web
sharing i think i mean i love being able
to send people hgr JPEGs as the email
attachments should never could do before
and what if it were backward compatible
so the jpeg HDR encoding is pretty
simple we ton map the HDR input into a
output referred 24-bit image and then we
write this out as a standard jpeg
segment and then we have restorative
information basically this ratio image
they're naive applications you see the
ton map version the HDR applications can
extract the original simple so the tone
map operate tum mapping operator is not
specified you can actually put a variety
of different tone mapping operators
according to taste and they can have
local properties there or be a global
tone curve our standard implementation
just provides the Rhinehart photographic
global photographic operator but you can
plug in your own just by changing the
library calls so you get this 24-bit in
you divide it into the original HDR
image to get your sub band ratio image
and they both go through the JPEG
compressor to come up with the the
stored result so this is what the ratio
image looks likes for the standard
memorial church image so you don't
recompute band based on the press
we actually do you know it's a good
point view there's a number of things
you have to do for error control like
that so the decoding process we naive
application would just use a
conventional decompress and get back the
tongue mapped image but if you have your
HDR application you get also get back
the sub-band multiply them back together
and get recovered HDR and again there
are caveats to how that's done so the
compress 8.jpg HDR is about 127 bits per
pixel depending on settings which is is
you know one third to one twentieth the
size of the standard HDR even with even
with the compression so simple the ratio
image so it's probably misuse of the
term but it it was the idea was derived
from ntsc where it has a sub band that's
like has the extra information to get
tab color and it was a misnomer really
I'm sorry sir racial what what the ratio
of the original image to the tongue map
version so we implemented this as an
extension Tomlin's public jpeg library
this has changed bright sides now
licensing it for commercial as well as
non-commercial use royalty-free and it's
implemented in photosphere of course i
think i skipped a slide no i did just
flickered okay so what about displaying
HDR imagery well there's actually a
laundry list of technologies that are
still in development that might be
capable of displaying or are capable but
may or may not ever make it out of the
lab both for projection and ends in
conventional desktop but the the one
that has made it out of the lab is the
one from bright side and I have this
viewer over there which I'll show you
later but it's sort of like an early
prototype of the idea of having
to modulation to modulating layers in
this case they're just transparencies
that were generated on film recorder but
one of the ok so just quick anatomy just
have these two really bright lights
shining through these they're basically
mandarin orange cans but they have the
nice property of causing a uniform field
because you fold over the Gaussian of
the spotlight and you get a uniform
field onto the diffuser and then you
have the transparencies and these are
they are v1 optics were used in the
original NASA the our system is pretty
cool but hard to get these days and then
we just create to reap repair to
transparencies now even getting this
even if you use a single transparency
you could get close to three orders of
magnitude if the film recorder permitted
it but none of the film recorders are
that good at least now the ones i have
access to so what I did instead is I
have these two layers of black and white
layer in a color layer but if you had
them at the same resolution you'd have
all sorts of registration and parallax
problems so we use different resolutions
and the different layers to avoid that
and then the bleep optics have this nice
wide perspective and then there's has to
be a correction for chromatic aberration
so here's the scaling layer it's just a
black and white look very low resolution
layer and then we have the detail layer
which undoes the loss of resolution by
having enhanced contrast so when you
compile it to you get back the original
so how does this work you go to scaling
layer there's this blurred version of
the original lead it's actually a
blurred version of the square root of
the original then you have the detail
layer which compensates for that to get
back the the desired output
there was which I'm going to talk about
here what happens when you exceed the
limits of your detail what happens is
you can't quite Purdue reproduce it what
you get instead is this sort of bloom
around the the bright spot but luckily
for us the I also has problems with lens
flare and you get a scattering of light
in your eye that almost perfectly masks
that bit of inconsistency so you really
can't see it well use a Gaussian kernels
of the Rings is pretty minimal and it's
such a wide kernel that it really
doesn't show so bright side has a cup
that had a couple of prototype displays
there early version uses DLP to create
the backlight actually their earliest
version tried to match resolutions front
to back and that's where I said well
that doesn't work does it let's do this
way so this is their early version which
they showed at emerging technologies at
siggraph like three years ago something
like that and it looks pretty cool
actually it has an LCD panel here and
then a dlp projector in the back but
it's it's this long so it's not very
practical so they came up with an
led-based version that that generates
the load the low resolution backlight on
the hex grid and then has an LCD in
front of that and keep thinking of
skipping slides because it's jumping
around of so what kind of color gamut do
we get out of this
so this is a standard srgb gamut
represented in a perceptual space so
we've used UV color coordinates and
we've tried to map the luminance to to a
TV I using a TBI function to get a rough
idea of what the the true volume is but
you have to ignore side-to-side relative
to the top to bottom because you can't
really relate luminance in color very
easily but what you see with the HDR
volume is that it covers roughly the
same color area but because it's bands
such a greater luminance range you you
can maintain these saturated colors too
much higher level so so it also gives
you additional apparent color so this is
what the what they could were calling
the production unit looked like and they
they produced a couple dozen of these
but they were never really were more
like beta units still look nice though
and the other advantage of using LEDs is
that you can replace the white LEDs with
RGB LEDs and get it with much wider
gamut so this is something that's also
in the works so here's here's what you
get here's the srgb gamut and if you use
red green and blue LEDs instead of the
white phosphoryl ities you get you can
reach out to a much wider range of
colors we're sort of waiting for the
stability of the LEDs to improve though
so that's it you have to take any
questions and I have a demo over here
which I'll show in just a minute
tune in my head huh yeah look at the
pudding though it was his eyes oh yeah
good yeah if I thought about using
multiple cameras and using the single
shutter release to capture different
exposures at the same moment you would
have parallax problems with that and of
course the expense of multiple cameras
so I guess I'd say I thought about it
but didn't really think seriously
tennety work on spatially varying some
nestel rendering I haven't myself I mean
I i have but but only using really crude
algorithms that look terrible I haven't
actually implemented any nice ones but
yeah I think it's a good approach and I
i know i've used the bilateral filter i
should should correct myself i've done a
poor implementation of that and even a
poor implementation of that looked
pretty good but i haven't used it in
photosphere it takes too long simply
takes too long you don't want to wait
that long for done mapping all musicians
and so taking images with different axis
of polarization to what ends questions
ah so like identifying specular
reflections using two polarizers I have
I have played with that for the purposes
of a reflectance measurement but I've
never thought about using it for for
dynamic range reduction that's it that's
an interesting suggestion you true the
the actual reduction of dynamic range
isn't that great because unless you were
at whatever it is Brewster's angle you
would you would still get a fair
percentage of the light coming through
to the
submission of licenses chapter yeah if
yeah that's a really good point is is if
you use polarization that way you could
actually estimate the haze and subtract
the haze and that would be a really nice
thing to do because a lot of images that
you take it outdoors your brain sort of
does this haze subtraction for you and
then you take the photo you say okay
this looks terrible what happened it's
because that haze so by using different
polarization I mean you can choose the
polarization that reduces the haze but
death but you never can find one that
eliminates it so that's a good
suggestion take two polarizations and
you have two points on the line and you
can actually may be eliminated so yeah
yeah but maybe he use the two camera
approaching okay started or two sensors
interesting idea review sweater and I
have their life's problems because a lot
of
to be cameras working
yeah and that actually has been looked
into for video as a way to capture
capture high dynamic range video is to
either use a prism with different
neutral density filters or take
advantage of the existing beam splitters
that are in the three ccd cameras and
just change the filters so you have
instead different exposures dinner the
exposure mask neutral density task on
the sensor yeah in fact a screen iron
and his group have done exactly that
shit they have a couple of papers if you
go to the Columbia Columbia's research
is if you can just google him and they
have a few papers on what they call it
sorted pixels whereas in addition to
changing color on the bear pattern they
changed neutral you know they change the
amount of light that gets through and
then they have more intelligent ways to
reconstruct the image based on that
thank you yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>