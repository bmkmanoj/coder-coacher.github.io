<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Multi-Feature Part-Based Object Detection System | Coder Coacher - Coaching Coders</title><meta content="A Multi-Feature Part-Based Object Detection System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Multi-Feature Part-Based Object Detection System</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EbaXS9qrh6U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today's speakers burnt izola OSS
scientists at honda and cambridge
massachusetts and his talk will be
titled part based multi feature
pedestrian detection system alright um
thanks Patrick for the invitation and
it's pleasure to be here and on the
timing is not really great since friday
afternoon is long weekend coming up foot
and i'm also laid so sorry about that
anyway so i'm going to explain to you a
system that i actually developed at MIT
with a student we have a collaboration
with the center of biological and
computational learning at MIT and it's
about it pedestrian detection but before
that I want to show you what I'm
actually what we are at Honda walking on
and then probably most of you guys have
seen this robot it's a see more and
we're developing in that group in in
Cambridge we're working on the a ipod
and mostly division part of that of that
robot it seemed that couple of specs you
and they change to be the maximum speed
the maximum speed i think is a it's much
faster now it actually can run so it
should be over five kilometres now but
what what we actually work with is a we
have two cameras there and two color
cameras which we use for for doing a
vision division part show you a couple
of things that already exist on on that
robot get an idea of what it can do
so here it actually recognize the
gesture a pointing gesture and ASE is
going to it's going to look at the point
where person pointed to n is going to
move there so the idea behind the behind
his robot is you know right now we have
just a couple of demo applications but
later on we actually want to have a kind
of service robot which can assist people
and can do some useful tasks right now
it's mostly you seen shows you know it
wasn't a museum of science and in Boston
they're touring around there and giving
shows but hopefully we eventually get
something really useful you know done on
that so that people might have actually
robot at home to help to help them
okay I think it get the idea I see
there's obstacle avoidance I'm just kind
of interesting so it is actually using a
the stereo camera pair to estimate the
distance between objects in front of the
robot can see actually it stops when it
when it sees a person crossing when it
comes to clothes
and it actually can avoid obstacles so
it walks around the person here alright
and here is as simple actually as a
simple application that needs this video
I've been recorded in the main research
lab in bako and in Tokyo I had some
receptionist application I think you
know it's like it's becoming a person
and then asking where the person wants
to go so it has some speech recognition
on it too
alright so let me show you what actually
we are working on so if a robot if you
want to have a robot at home it has to
do has to be able to do a couple of
things from the from the view of
computer vision applications it has to
be able to recognize objects and at
least the most important ones like you
know you want to recognize the desk you
want to find the door be able to leave
the room and I also you want to
obviously find any people if you want to
communicate with people so you want to
determinate identity and you want to
recognize the gestures and the
expressions so that's basically what we
are working on Ed's in general object
detection but specifically we focus on
on on things that are related to people
so we want to actually do personal
identification and also expression
recognition and and now I'm going into a
kind of approach it became really
popular the last couple of years and you
find it often called part based approach
I usually call it my papers components
but Shimon Ullman call it fragments
other people call it patches and this is
a general approach for object detection
and recognition in media images and I
want to show you what most of these
systems have in common so now assume
first a global approach which is the
standard approach where you have this
pixel image and then it just each pixel
in your image is a feature so basically
a feature is connected to a position XY
position in the image now if you want to
see what I actually if you have this new
image here and you take the same feature
at the same XY position you see it's not
connected to the same part of the object
right so it shifted because the person's
face rotated slightly so what that that
actually means if you do classification
is that you have large in class
variations if there are rotations off of
the object and that will make the
problem really hard so if you just take
these each pixel as a feature and
I would classify that you want to make
sure that actually there are no
variations in the image so that these
pixels are actually in correspondence
which usually is not the case so the
idea behind the part plays approaches
you first localized parts of an object
and then take the features within these
parts relative to the to the party
detected so this X star and y star are
actually corner its relative to the to
the to the corner of the of the part you
located so now you try to locate the
same part of this image and then you
look at the same feature here and indeed
it corresponds to the same part of the
object so what you try to do is actually
solve the correspondence problem so the
features are in correspondence that
gives you small in class variations and
that should lead to better
classification results that's the basic
idea behind that the couple of other
things if you use parts instead of
trying to do a global recognition system
like you can better handle occlusions
partial occlusions it is more
flexibility in the geometry and so on
but this is this is the what the basic
principle that isn't that all the
systems have in common basically have a
party Tector which can move around a
little bit and then you take the
features inside these parts and classify
those alright
and also what you'll find is that that
these ladies systems are at actually
have two levels and the first one is
what I just mentioned is the detection
of the parts and at lots of different
ways of doing that and I'll show you a
couple of them later but basically let's
say you want to localize the eye so you
have an ID Tector that moves over the
image and then finds the I location you
do the same for the nose and the mouth
and in the second step you actually want
to see there's a face so in this case
it's a phase detector so you use the
outputs of your party detectors and
somehow combine them and determine if
you have a face there or not so you
always find this to at least two levels
in and this in the systems they couple
of biological systems that actually have
they divide parts in the sub parts so
you have a whole hierarchy there so it's
like you can have more inches two levels
but at least at least two you're fine
okay so here's a hit to principal
approaches actually how to detect the
parts and I'm not sure how familiar you
guys are with the these systems but one
way is as an interest point the a based
approach where you first actually run an
interest point operate on your image
which can be just you know a corner
detector and then you extract the
patches around these interest points
then you can perform all kinds of
normalization on these parts to get
invariants against rotation and I sure
that here for two patches so you extract
these two patches and during training
actually looked at the object and you
also extract these patches and now you
match them with a test image and you
just find the matches and based on that
you determine if if you've seen this
object or not if you've seen it before
or not the other approach is you have a
sliding window so you actually don't
compute interest points but you but you
slide classification window across the
image and you analyze the content
uh of this of this classification window
and again inside the classification
window you run your party Tetris and you
try to localize parts and base based on
the outputs of the party detectors you
determine if if it's the object you're
interested in or not so as you can see
in this and this and this step here you
remove most of the image in the first
first part just extract like a hundred
points and how to patches and that's it
that's pretty fast while here actually
you look at every pixel and you also
shift the object window pixel by pixel
so that's that's really slow but it's
more thorough so here's a comparison of
these two approaches the sliding window
approach works very well for low object
resolutions so you see the best phase
detector which is poly systems based on
the view electrons detector they operate
at phase at resolutions of 19 by 19 for
face images at this resolution actually
you can't find many corner points
anymore so all this interest point based
systems will fail because it's just at
this resolution you just don't get
reliable detection of interest points so
that's that's what interest point based
systems they don't really work that well
at low resolutions but they are really
fast because you throw away 99 percent
of the image in the first step and you
only analyze the patches around the
interest points here they're really slow
millions of options windows have to be
processed per image still they're real
time systems out there but you know
especially if you have to deal with
hundreds of classes I don't think this
is a really feasible approach in real
time right now the rotation invariants
is problematic here basically you have
to for each rotation you want to if you
have implant image rotation you
basically have to rotate the image and
repeat the process over and over again
so that makes really slow here their
systems where you actually can have
invariants rotation
but just performing this invariant
invariance operations on the patches you
extract it which are usually just a
couple of hundred so rotation variants
scale invariance it's relatively simple
with this interest point based systems
it's really hard to do the sliding
window approach as I said you choose a
generic interest point operator most
people these days use the corner corner
point operator the only thing you have
to worry about is that it's kind of you
can repeat the performance you can it
repeats the detection so basically if
you have the same object when you change
the view point slightly you want to have
the interest point operator respond to
the same point on your points on the
object so the repeatability is pretty
it's pretty important but their studies
out there show basically can't go wrong
with the fuse a Harris corner point
detector here you can choose any kind of
detector to localize your parts match
fillers I used work with SVM's and so on
so you're pretty free in choosing a poly
tech tree large number of parts possible
he or not really because you're limited
to the number of interest points you
detect you'll find is mostly in
detections as is their best face
detection best pedestrian detection
systems operate like that recognition of
large number of objects say if you read
one paper it's probably David loves
paper where you can wait recognize like
I think run a thousand objects using
this interest point based system there
what kinds of parts you want to choose
here obviously you're limited to the
parts around the interest points here
you can choose your free you choose any
part of the object so you can actually
use machine learning techniques to
determine which parts are important
which parts are characteristic for an
object so it gives you more freedom
there so anyway each of these approaches
has advantages and disadvantages and
really depends on your application what
which one you want to you want to choose
I didn't talk about the geometry right
now I just mention how to actually
detect these parts but you also actually
want to model a geometry basically their
space
relationship between the the parts and
again there a couple of different
possibilities of doing that and one way
is actually just completely ignored
geometry basically just look at the
appearance if you find the parts if you
find all the parts it's good you don't
really care where they are and in which
you know which configuration they appear
so you'll find the especially Tommy
Potro he did that and is a biologically
inspired model it claims it's really
hard actually in the brain that we have
this geometrical model so it says there
is there is no geometry there so it's
pretty much all appearance based and
then you can increase kind of go up in
the complexity of how you want to model
geometry say well which is model the
geometry of the location of each part
relative to a center point of the object
ok so say you this part you would expect
and this in this region such as you can
model by a Gaussian or whatever but you
don't model actually the relationship
between this position and the position
of that power right so it's called a
star graph model it's relatively simple
simple and if you have em parts and
detections per part you basically just
have to evaluate n times n
configurations here now the most complex
model is a fully connected model where
your relationships between all the
positions of the parts you'll rarely
find that actually because if you have
any Texans and then parts you have to
evaluate n to the power of M combination
spatial combinations there yes
is all the time will survive say
geometric model seems attractive at that
point yeah because it covers all the
correlations between different views and
the futures that exist in them just
yesterday was Wallace inclusion and like
share sorry but look at that I mean the
sort of thing is if you run your party
detectors you get let's say you look for
the eyes you know at a relatively low
resolution for the eyes of in the face
you might get a couple of thousand
responses of your party texture where
the eyes might be located in the image
so in this case you would have a
thousand detections for this part now
you have let's say you have 100 parts
you have a thousand to the power of 100
possibility possible configurations
which you would have to evaluate which
one is the most likely one right and and
you can't do that so you want to have a
geometric model but you want to have the
you might keep the complexity low so
usually you model the parts the position
of the parts is independent so you just
say well if you model the position of a
part is relative to the center but not
relative to the other parts so it's a
pretty simple at you all right I mean
you could have a sophisticated knowledge
as you just run into problems into
computational problems they're flexible
champions we have
by spring exactly that'll be that right
that will be tell you that and and and I
would say if you pick a model pick this
one here all right i mean that's that's
my personal preference i would say you
know rather go for a large number of
parts and a relatively simple model then
choosing a small number of parts and a
relatively complex model but again you
know it's like people people would argue
about that so let me show you what
systems are out there it's a little bit
simplified actually I have to tell you I
still stole the idea from Corona so here
as simple as a similar graph there it
shows you the training samples you need
for training your system and the number
of object classes that you actually can
recognize so you see that I've been
working on face detection actually since
2000 and you see that the they're really
good systems out there now for phase
detection I think the problem can be
considered be solved and they're good
systems for pedestrian detection to what
they need actually they need thousands
actually hundreds of thousands of
training examples so you need a ton of
tunnel training examples and what you
can do is you can choose to distinguish
between two classes right so it's a face
or it's not a face it's a pedestrian
it's not a pedestrian so it's two
classes and a lot of training samples
there so you go to the david lowe sift
system which is a one-shot learning
approach it take one image of an object
so you have one training sample per
object but you can actually recognize
thousands of objects with that system
and also works in real time this one
does not generalize and i'll show you
the generalization performance later so
then probably most of you are familiar
with the Caltech 101 and 256 database so
i think you have about you know i'm not
sure like 40 or 50 around that sample
training samples per object class and I
think you're obviously in the 101
database of 101 classes to 56 year of
256 object classes their systems out
there actually you can you can read
papers about on all these on all these
systems here so if you look at the
generalization and a generalization I
mean generalization across a class of
objects here if I say you have one
object I mean you really recognize only
this specific object is so it's not a
class of objects so if you show it a cup
it just recognize this cup with a
specific pattern on it but it doesn't
recognize any other cop so there's no
generalization they're basically and the
generalization increases as you go down
here this systems generalized very well
so pretty much work on any phase in any
pedestrian um so what I would say about
the applicability this stuff works in
real world applications so if your goal
is to recognize specific object or lots
of specific objects yeah just go with
that system this one I'm not really
convinced about this you know this
they're mostly academic papers there
when they claim they work on 101 classes
where one class is like chairs and I
know that nobody solved the problem of
recognizing chairs right so so the
performance of this you have to you have
to take with a grain of salt I mean I
would say these systems are not you
can't really use and I don't know of any
real-world application whether it can be
applied to this stuff works very well
you know face detection pedestrian
detection just plug it in if you need it
and it's fine so the question is how you
make actually these things your work too
and I think I think you probably need
more data I mean people people claim you
know the goal is to train a very few
training samples like just 40 images of
chairs and be able to recognize chairs
I'd say that's not possible so you need
you need a note somehow you need to
collect more data you probably have to
develop learning algorithms that are
more capable dealing with large number
of
samples and at training time is an issue
and obviously so here's what I say and
you know it's like you hear other people
say different things despite the trend
you see these days in the papers you
know working on on large number of
object classes training on a few samples
I think it's not possible actually
training system on a from a few number
of images to be able to reliably
recognize a complex object class and
complex you know I don't even mean as
complex as the class of chairs which is
super difficult because chairs vary in
appearance and it's like I would say
phases already a relatively complex but
most people would consider them as
simple as four objects I think it's not
possible here at rest so I think you can
from scratch i mean the system that
doesn't know anything about computer
vision it's just a machine learning
system you showed 40 images of chairs
and say 500 background images and from
that it's able to recognize chairs in
the future i don't think it works so as
i said what we need is maybe in a
machine learning techniques actually for
large training sets which operate in
high-dimensional space and usually in
computer vision you have high
dimensional space because you work in
pixel space or you know so that's I
think what we need or otherwise you know
if you want to be able to train on a
small number of of training images you
need a system that has some information
already that has some capabilities
that's it you know people often say well
humans are able to recognize an object
you show it only twice or three times
and they can recognize it it's true but
we have a lot of experience we have a
huge computer vision machinery that that
segmentation we can recognize you know
shape from shading and and that stuff
that we already have we don't have to
learn that anymore so if you want to be
able to train on small the training sets
you know you need a system that already
knows a lot and I don't you know I don't
see any any of those systems around
these days
but you know who knows maybe we have one
in 20 years down the road so anyway let
me let me come to to our system here
it's actually pretty simple and you know
I want to keep it relatively simple
because I want to keep the option open
to implement that in real time and
here's the basic idea so you have an
object you put a grid on it and
basically that's my geometrical model so
i confine each car be within a grid
element that's that's basically geometry
and then what you do is you have you you
have some parts which assigned two grids
which is localize them in that image by
using by doing normalized correlation
and pick the maximum value of the
detector inside that that grid element
and let's say if you have 500 parts you
get 500 output values and you use them
in a combination classifier which tells
you if in this object whether the object
is there or not but that's basically it
so it's really simple i mean the the
time consuming part here is is the
normalized correlation which you know
that there are lots of different ways of
doing that efficiently in this can you
special hardware you yeah so that's
basically a technic well known from
template matching so the interesting
part actually is how do you determine
the part so how do you do the training
how you build the classifier and and
what we do is we first compute what we
call future maps so basically any any
kind of you know it's like when you
think would be useful feature you could
just use the gray values as they are we
use gradients and we used the responses
of Gabor filters so you compute these
future maps for your training images you
apply this grid to each of these feature
maps and and you randomly extract a ton
of parts you know you can extract a
different sizes or whatever it's like in
the hundreds of thousands of parts you
right from your training images what we
do then a step to kind of reduce the
number of parts we have is we cluster
pods which belong to the same grid
element it's just a data reduction step
and then these cluster centers we found
we correlate with the training images
again each part is just correlated with
in its corresponding grid element so
from that we actually get the maximum
correlation values for each part which
is actually the feature we derive from a
part so once we determine the part we
apply it to each of our training images
and it gives us one output value which
is the maximum normalized correlation
within its corresponding grid element so
so we still have too many parts there so
what we do is we use a we use general
boost actually to iteratively select
parts that are useful to discriminate
between the object and the background so
that's a two class problem here okay so
that's basically how it is so you have a
huge pool of parts and then you
iteratively select the ones that you
think that the classifier channel boost
classifier things are useful for for
solving the problem the classification
problem so it's pretty in our pretty
standard approach and I think lots of
people do it now more less with
different variations but it's surprising
how well it does actually if you compare
it on to other system more complex
systems on standard databases there so
what would you do in the test case now
that you found your parts again you
apply the grid to the test image you
correlate each of your selected parts
with the test image inside its
corresponding grid element and you get
this maximum correlation you just put it
into your boosting classified will tell
you will give you a value Chelsea if the
object is present or not so that's all
there is to it now here's a couple of
interesting things and these are
experiments on a pedestrian database
that tells you give you an idea actually
how many parts we need so this is the
channel boost performance on the
training set we iteratively select here
wats goes up to 500 parts and this is a
training error I think it's just wet
start with 5000 we had five thousand
images there and then we looked at the
performance and you see it's pretty much
with 500 parts you don't make any
mistakes you anymore so now how does it
translate on that on the test set you
see it's like there's not really much
overfitting going on there you improve
performance all the way to up to 400 and
from there on it stays pretty much the
same the training error goes down a
little bit but the test performance
stays the same but it doesn't get
actually worse so it's not over fitting
really what does that tell you actually
it tells you that especially in this
problem whether we actually we operate i
think that the resolution of these
pedestrian images was something like 50
by 100 pixels so it's relatively low
resolution that the individual party
detectors are not super reliable so
you're not going to get away with just
picking five parts and trying to detect
pedestrians there so what you instead
need is it you need a ton of parts there
and so here's a conclusion with fuse
with few parts in a relatively small
part size our part size was like 5 by 5
pixels that's because the objects
themselves were relatively low
resolution you'll not get a good
performance so most of these systems who
claim that can detect objects using just
five parts and it's like maybe they can
if you know the object is super high
resolution and they can find some detail
which is always the same in its object
but if there are lots of variations in
the object like pedestrians they change
their clothes you know it's like sighs
and you know you're not going to get
away with a small number of parts all
right so features I told you we computed
different feature maps and the first
thing of course everybody tries which is
using the gray values here so our parts
you would just be part of it the gray
value pattern next thing it tries the
magnitude of the gradients it's
basically kind of edges and which is
really what is became really popular
these days
is double wavelet responses just because
it seems like biological systems kind of
rely on these kind of pictures here ok
so that's where we try it you know grey
gradient and gabor and expected parts
from each of these maps and look how the
performance is this is an hour C curve
it's tells you basically how would the
classifier is further up it's better
it's a false positive rate correct
recognition rain and you see that the
individual future is all about bunched
up here they're all about the same as
there's no no big difference but if you
combine them if you throw all these
parts together you get better and you
know unfortunately you know people
usually want to know which feature to
use and I say well you know it depends
very much and usually you're better off
using a lot of features right and there
is no real recipe how to pick the right
features except for doing experiments
and trying it out that's all I can tell
you it's kind of black magic there's not
one single set of feature which I would
say is is the right one from computer
vision so you get a significant and
actually increase in performance by just
combining those those features so what
what was kind of interesting and in our
experiments is that usually people if
you if you want to say you want to
detect phases you will say well look for
should train your pocket if you
extracted parts the parts you want to
you know using a classifier so just look
at faces and try to find parts with your
current phases and ignore the background
class so when when you when you train
your part when you select the parts you
would just look at the positive class so
to speak right and that's what people
usually do so when you do the part
selection they just usually take the
positive class in a detection case and
ignore the background class I turned out
actually so what if you have a part of
the in our case of pedestrian and you
correlated with the pedestrian images
and with the background images and you
build a histogram you will see something
like that so the correlation in average
for the pedestrians is
then with the background that's exactly
what you want so it has some
discriminative value of this part so you
you are able to discriminate pedestrians
from background based on this part so
that's what is that's how a histogram
typically looks for four if you
extracted from the positive class now
what we figured out actually we also
expect the parts from the background
class and the background class now okay
is just random background right i mean
it's like just you know street scenes
without any pedestrians in it and then
you find occasionally you find parts
that have a distribution in the
correlation histogram that looks like
that so they correlate higher with
background then actually with
pedestrians so that means these parts
are absent in pedestrians usually but
you you find them frequently in
background and they also have a value
that obviously you can use them to
classify distinguish between the two
classes and that's kind of what's kind
of surprising i think that people didn't
do that before so it helps if you add
the negative class to your training data
when you when you actually try to find
useful components or parts actually you
know if you look at these parts selected
from background it's five by five parts
so that they don't can't really tell you
is that expected from a pedestrian or
not so that they don't tell you anything
I just looking at them but we figure out
if you add this background images to
that to the hard learning stage your
classifier performance increased by 5%
it's actually quite a bit if you're
dealing with a detection problems so
five percent is a lot so I i would
suggest you know if you if you want to
if you take an approach like that just
always use the background the negative
class too and here is graph actually
that shows you how positive and negative
parts where they come from and what I
mean is the positive part is a part that
correlates more with a pedestrian class
and with the background class and a
negative far would be a part of
correlates more with a background class
and with a pedestrian images and you see
actually that the
see that you actually get a negative
object path so you actually get parts
that were extracted from pedestrian
images but still correlate more with
background class and with it with the
pedestrian class which is kind of
surprising on the other hand you get
positive background parts so you get
parts that were extracted from from the
background but they have higher
correlation with pedestrian images them
with background images and that kind of
shows you how the relationship between
these four different kinds of parts are
as you increase the number of parts in
it in your training training set it's
kind of kind of interesting to see that
a little bit surprising so anyway it is
I mean that we tried it on on this
pedestrian detection data set as
published in 2006 in a pommy paper by
mundo and guerrilla and they tried a ton
of different classifiers their support
vector machines you know and a
nearest-neighbor k-nearest neighbor and
so on and and they also had a couple of
different feature extractors there and
turns out that the our system with the
hyperlink components and three different
types of features performs better than
any of their systems there so you get
really good results and we trained on
face detection who trained on detection
of objects in an office environment and
pretty much in all these tests we got
the results that are as good as the best
systems out there or even better alright
so let me let me summarize the
presentation I showed you two different
types of pop based approaches one is
with the interest points the other one
is with a sliding window I mean I
implemented the sliding window approach
but you know for other applications you
might want to use the interest point
based approach which can deal with the
large number of objects is much faster
but it's not as accurate as a sliding
window approach especially the
resolution of the object is low
I would say you know choose a large
number of parts small of medium size for
highest accuracy and small medium-sized
relates to the size of the object in
general you know it's like maybe maybe a
quarter of the size of the object keep
the model geometrical model simple
otherwise you're not able to use a lot
of lot of parts there our system is
flexible in the type of feature you can
pretty much generate any arbitrary
feature map you want and extract parts
from that you can choose a flexible part
size and the number of parts is flexible
too we hope we're going to put that
software on on the MIT web page soon so
you guys can actually download that and
do your own experiments if you want we
got a state-of-the-art whistle results
on several detection and recognition
tasks there the last the last one I
showed you was an extreme pedestrian
detection it's conceptually really
simple it's just you know basically
template matching with the large number
of small templates so you you could if
you really want to you can do that in
real time if that's your goal so the
potential is there and that's pretty
much it so if you have any questions
we actually what did what you do is you
use a multiresolution approach right I
mean busy I mean to achieve scale
invariance what you do is well well I'm
in a different ways of multiresolution
approaches so there's like you could
possibly train different classifiers and
different resolution so you basically
cascade them so you use the low
resolution classifier first to eliminate
large parts of the background and then
you go as you decrease the number of
image parts you look at you get it you's
better classifiers yeah that's that's
definitely and I implement a system like
that for face detection and it helps if
you look at beulah drones it's slightly
different here's a cascade too but he
has a feature of cascade so he just
starts with very few features he looks
at which he things are important for
phase detection removes background parts
and as he goes up the cascade he adds
more and more features and classifying
it slower so that's a slightly different
approach so that's not really resolution
base its feature based but yes that's
definitely what you would have to do if
you want to have real-time system you
have to do something like that that's
right
this energy and your rotation and I
never claimed there so if you want to so
what what well so here is that different
types of rotation as it is in plain
image but in plain rotation and and
there you look would look at David Lough
system which uses sift operators why
basically first you find the interest
points you compute a main orientation
around this interest points and then you
based on that you normalize the patch
that's not possible in that system
obviously right because we've a
continuous power detector so we can
normalize at every possible location so
what people do here there is you will
see a rotation invariant phase detector
which runs on a sliding window you
sliding window approach which is also
invariant to rotation depth actually and
they implemented that so its rotation
image plane and in depth and it's real
time and they just train different
classifiers that's what they do so you
would have four four possible you know
first like you some are discretize you
know the 360 degrees in a couple of
steps and you train different
classifiers on that alternatively you
can just well actually what you do
actually is you rotate the image I think
you use the same classified just rotate
the image for rotation that you really
have to train these different
classifiers and it's just you can't just
rotated that the image and then and run
the same classifier but yeah it's not
it's not you know it's not easy to do
but then there are people in biology
claim that our system is not rotation
invariant either yes maybe you don't
really hear you really store different
views right so you basically have
different classifiers it's up to you if
you believe that or not
changes and
when I say future combination what we do
is we calculate these future maps so you
run they do convolution right so you
choose whatever whatever operator you
want or compute the magnitude of the
gradient you generate a new image
instead of the grain is you use that to
find parts in there and then you and at
runtime you would again first compute
these feature maps and then you
correlate the parts with the
corresponding feature maps so you don't
really throw anything together item
insect so you don't mix up you know
different features with different you
know parts and states all the same there
was another question a couple possible
yes the stash box pretty well
call system support
state-of-the-art pedestrian detector
performance
preprocess based on Eric Williams for
problems simple geometric a statement
oh yeah you know i'm not i think that
that paper was published in kind of
context context recognitions away true i
mean i'm at tons of you know i walked a
tunnel chrysler for a long time it's
like 10 years ago and they they
estimated the ground plane you know
using stereo and then they just look for
pedestrians on that on the plane
basically and said well obviously that's
better than looking for pedestrians in
the trees right i mean it's it i would
call that a trick to improve the
performance so so that geometric context
includes all your classes oh yeah i was
working with a stand for a long time you
know I'm in the same group basically you
know my student was working there I
would say these systems you know he used
a biologically inspired system but it
just used the first level of it which is
very similar actually to what we do so
just use slightly different features but
basically it's the same thing you know I
just that we don't have this claim that
is biologically possible so i just use
any kind of feature i want to run but it
is pretty much the same thing and what
building that i was curious
geometric context and the horse but yeah
he uses that to actually and you know I
would say this context recognition thing
you know sometimes I would say the
context is much more difficult actually
to determine than than the object itself
you know I mean you will find
applications where it's really hot
actually what seems easy for us to say
you know we're indoors or we're looking
at the scene where you know that's in
the back and and and and the cars there
you know it's like sometimes it requires
a lot a lot of processing actually do
some useful context recognition right
and then a simple object recognition
task actually the object recognition
tasks you want to solve is actually
easier to do than the context
recognition task I agree you know I mean
if you have you interested in a certain
application like pedestrians and street
scenes you know then yeah why not use
any information you have available to
make it better right to eliminate false
positives it's not my goal right now
right so so I I'm not looking at these
techniques but i would say it is
definitely promising approach right i
mean you know I prefer datasets what you
have actually and that's why I took this
one where you have extracted patterns so
what it don't give you an whole image
and say say full image and say well just
see if you can find the person in there
because that lots of tricks actually how
you can make your system work better
right if you can say they have a sliding
window approach you count the number of
detections around the same position and
based on that you make a decision so
it's really how to compare the systems
just saying okay is this classifier a
better that actually a lot of it depends
on how the person implemented the
sliding window approach is scaling and
are different image scales and so on and
I worked a lot on face detection and
that always made it difficult to say
which system is better or not and then
many people use bootstrapping so that
they they use tons of background images
to bootstrap their data so they use
actually millions and millions of
background images or parents to increase
the performance of their classifier but
they don't provide that in the training
data so you don't really know what they
trained on
so this set gives you exactly the
training set and the test set extracted
so there's no sliding there you know you
know exactly this is your pattern so any
machine learning guy can just simply
apply doesn't have to know anything
about computer vision and just run his
algorithm on it and see how well it does
all right thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>