<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mining Your Logs - Gaining Insight Through Visualization | Coder Coacher - Coaching Coders</title><meta content="Mining Your Logs - Gaining Insight Through Visualization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mining Your Logs - Gaining Insight Through Visualization</b></h2><h5 class="post__date">2011-06-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9LnpFCrX2uc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what I want to talk about today I'm
going to talk a little bit about log
analysis and visualization so I'm Rafi I
I've worked in the log management and
log analysis space for about the last 11
years I grew up in Switzerland I worked
at IBM Research out in Switzerland for
for a little bit did my master's thesis
there and then with a quick stint at PwC
consulting I that's where I learned how
to dress I moved out to the US and it
worked for ArcSight which is now HP they
just acquired them about half a year ago
then worked for Splunk for awhile which
some of you might know it's a city up
company up in the city startup that were
actually not startup anymore I guess
it's doing log management also and about
two years ago I left Splunk and then
started my own company called log Lee
and I will talk about that in a little
bit a little more on the way I sort of
discovered a passion for visualizing
security data because I was always sort
of on the security side of log
management I at some point figured out
well there are ways to actually generate
visual representations of all of that
data so I figured out how to visualize
data I really got into that and tried to
work more on how do you do that how can
I help people visualize their own data
that they have and make their analysis
process more more efficient I wrote a
book on the topic and started a
community site called secular org and so
I've done a lot of work in that
community and you will see a bunch of
that coming out here so the agenda looks
as follows I want to talk about very
briefly about what what's log analysis
most of you are gonna be like yeah it's
pretty obvious I want to talk about the
history very briefly how did it even
come about what are some of the
commercial milestones that we hit in
terms of technologies also I'm going to
talk a little bit about log
architectures what sort of the
difference is out there between log
management and symbols so and then I
want to launch into what's working and
what's not and you will realize that the
second part is going to be much bigger
than what's not working I think we as an
industry have failed at this point to
really deliver working systems but I
will elaborate on that then I'm going to
look at what do we need in the future
and as part of that I will show that
visualization can actually help us a lot
with analyzing and mining our data and I
will show you a bunch of examples
on security visualization and on how
that can actually help sifting through a
lot of log files all right
so log analysis this is actually a fun
picture I found when I was up in Seattle
I guess some of you guys out there and
the VC site and we went to Amazon they
had these these logs outside and I had
to take a picture it's kind of cool so
you have probably seen log files before
they they come in all kinds of formats
some of them are a little more
interpretable than others usually
they're fairly crazy there's all kinds
of stuff in there and if you haven't
worked with these logs before it's
really hard to interpret what's really
going on in there most of the time so if
you look at the brief history sort of
logging has been around for quite a
while and I'm not sure if it really just
started with eric allman developing
syslog but but back in the 1980s he
coded syslog and mainly for for sent
mail at that point he just used it for
sent mail itself and then there was a
whole bunch of development around that
and it evolved and at some point in 96
it was the first company Intelli tactics
that actually entered the market as sort
of as siem security information
management tool that started taking
feeds mostly from intrusion detection
systems to help kind of the reduce the
number of false positives by correlating
the data with other data sources so that
was really the first sim out there in 97
T Billy came out with risk management
that was actually something that was
developed in Zurich and the team I was
working on back there and that was sort
of the first attempt to try to take
security data and start correlating it
vulnerability information that you have
and that gave you a lot of benefits from
that standpoint and then in the next I
guess 14 years there was a lot going on
in terms of different companies came on
the market there was like net forensics
and gardening and all these early sim
companies that entered the market and in
2000 marked that arc site specifically
there because I think that was probably
the most successful sim they were not
the first to enter the market but they
were able to learn about the mistakes
from the
others and when when when ArcSight
entered them they had a lot of benefits
and they came up with a different kind
of approach to dealing with things and
they just got sold for 1.65 billion
dollars to HP which is fairly successful
outcome for them and the last point on
there I have is in 2009 I I marked
luckily there because we're probably the
first company that does real logging as
a service where we're a cloud-based
hosted service where you sent the log
files to us and we take care of them for
you so you don't have to install
anything anymore now if we look at it
not necessarily from a timeline or from
a kind of company perspective there it's
interesting to kind of follow how did
these products or these solutions even
open source tools how did they develop
why why was were they even there
and he started with network management
so feebly the tech the tool the event
correlator was really a network
management tool it helped collecting all
those SNMP traps and he had HP openview
and all that kind of stuff so the first
problem it really came from the network
management side there's a lot of
information out that he wanted to gather
but then the whole sim world came about
security information management where
the biggest problem in security at that
point was he had all these intrusion
detection systems and they threw up all
these alerts and a lot of them were
false positives and unfortunately they
didn't manage to reduce the number of
false positives by themselves so the
Sims came in said well we need to look
at a more holistic picture we need to
correlate all that data with other data
sources to reduce that amount of data
which I'm not sure we have really
succeeded in doing because now that you
get all the other data feeds you start
looking at all that data as well and so
you just pile up data that you look at
so that's what's happened so security
monitoring came into place more data
sources were used and and collected in
one central place and then some of the
Sims started pushing the whole
combination of stock and knock so they
wanted to combine the Security
Operations Center with the network
operation center because that's
historically it was a lot of network
management that came in and there's a
lot of interesting information you can
use for your security use cases also and
they wanted to combine that but I don't
think that has really worked that well
there are some installations where these
two consistencies are put to
in one operating center but generally
it's still separated there's still the
network management part that makes sure
that actually you have bandwidth and you
can actually communicate and then you
have two security people that make sure
that there's no dirt on the lines
basically then what happened is it's
interesting as well so this was all sort
of infrastructure related people started
collecting logs for infrastructure
purposes and maybe for web analytics but
that was a little to decide but what
then happened is that people realized oh
we have all these log files and
applications so they started pumping in
all these application logs into these
Sims and it just blew up because there
was too much data the schemas didn't
work because it was all relational and a
whole lot of problems came came about
and I will elaborate on that a little
bit here is a graphic I came up with
which I call the maturity scale of log
management and what I mean by that is
the following if if you look at
companies or people that do log
management they basically they start up
cell door on the left hand side and in
the in the beginning what you do is you
collect logs you centralize them so that
you can actually access them in one
place instead of logging into every
machine I mean especially in your scale
you don't want to log into every machine
to look at the logs you want to have it
central so a lot of people just
centralize things and then use grep for
something to look through the data which
is absolutely non-scalable it's slow
it's it's cumbersome to do so then they
start using some log management tool and
they do some forensics they might keep
the searches that they do in a text file
or something so that they have them and
can pull them back out again and so they
don't have to write the same command
line all the time or they write little
scripts to do things and then they start
sharing these scripts with other people
so they can run them and then you get on
to the next thing which is sort of
reporting and that's that's already
quite a big leap you may to actually be
able to report because now you have to
extract fields or data from the log
records that are completely unstructured
and then people set up alerts right you
have things that you do every day and
you're like well that doesn't make sense
if I do that every day I just run a
script that doesn't and summarize my
information and then generates alerts
then you start collecting more logs
because people realize oh there's
actually real benefit to what I have
what if I had these logs so you can keep
moving over to the right inside you
start correlating information and then
really far to the right-hand side you
start with some visual analytics maybe
some pattern discovery or some anomaly
detection I'll talk about that in a
second also but if you look at the
distribution of companies out there and
what people are doing I would say 90%
are to the left of probably about here
they do a little bit of correlation but
not very good and not much most people
are just collecting the logs and
centralize them and maybe search them
but that's that's pretty much where
people are today and that has a very
interesting impact because if you think
about it if you if you're developing a
product on this side or a solution or a
tool or something an algorithm you don't
really have the sites to test it because
you don't have the data collect it that
you need to do this and you don't have
to people that understand this side so
we need to start moving things more and
more to the right-hand side to actually
enable all this stuff if you look at the
tools of how is it done on the left-hand
side is a lot to just do it on yourself
there your own you use some scripting
and maybe a database you throw the stuff
in then if you move a little bit over
there's there's log management tools
that can take care of up to the
reporting part you have the collection
there's open source things like lock
stash or a gray log and then you have
the commercial tools out there then if
you move more to the right-hand side
especially when it comes to correlation
you have things like complex event
processing or security information
management again there's commercial and
open source tools some people's use map
videos for some of the analysis case use
cases there and then the advanced
analytics side on the right hand side
there's really not much there especially
not log specific there might be some
like a coconuts or something that you
can use for visual analytics of generic
theta and people are trying to like push
in the log data into those tools which
works so-so
if you look at the tools that are out
there open-source there's a whole bunch
just this list is definitely not
complete there is a bunch of tools there
is a great log and log stash features
which are probably the most advanced at
this point in terms of log management
tools log stash is actually written by
our head of operations it's an
open-source tool it's been out there for
quite a while there are little tools
like 10g your swash that you can use to
look for different patterns in your log
files if they just monitor the logs and
you have different queues you can set up
and then send alerts or take some action
I marked snare here it's probably one of
the better solution to collect logs from
Windows which is always a huge problem
how do you get those windows logs out
because it's sort of proprietary they're
not really open standards you can use
WMI to get to the data but that's often
a headache for security and there's all
kinds of issues there an interesting
tool on here is SCC the simple event
correlator it's really old at this point
I don't even know when it was code it
was probably around 10 years old it's an
interesting correlation engine there's
some downsides to it but if you are have
a smaller set up somewhere and you want
to play with correlation it's an
interesting engine to use yeah there's a
whole bunch of other tools and in the
commercial world this list is nowhere
near complete but that's I think sort of
the major players that you've probably
seen item I'm not going to talk about
that much more unless anyone is
interested so let's talk about some of
the log architectures and how these
tools do some of the internal work if
you look at log management tools and I
make a very clear distinction between
log management and then security
information management so log management
itself is basically trying to collect
data from all kinds of data sources and
probably about 90% of the data sources
are some kind of a SUSE log based
transport but you have all these other
tools out there that use some kind of a
proprietary protocol like checkpoint for
example uses something called OPSEC
which is a whole binary protocol you
need to write a client there's there's
all kinds of overhead there there's
entire cisco has SDE for their intrusion
detection system it used to be our dev
back in the day you have net flow data
out there
which is I'm putting NetFlow on here and
not traffic flows in general because I
think 90% of people that collect flows
use them in net flow and not as flow or
anything like that and then you have
databases
that's another data source that for
example ISS real secure your security
people the IDS the intrusion detection
system they actually log in to a
database which is very interesting so
you have to create a database for the
new records if you want to keep up with
the data that's in there which is an
interesting approach or interesting
challenge also so then when you collect
all this data your index it generally
that's what the log management tools do
mostly then they add some context
generally to it so you have like asset
database where you can say these are my
web servers these are my financial
service this is the criticality of the
machines maybe you can add some flavor
to user names or something like that but
it's usually just IP address space you
can add certain more information there
and then in terms of scaling this you
can come either close to the log
management appliances or software
installations themselves or you throw
the data into some kind of a SAN or
something like that where you actually
keep the data that itself now what some
tools started doing is that the problem
is if you collect all these raw data you
can really do not much else than just
index it and then do a full-text search
on it so what a lot of these tools start
doing is they introduced this kind of
collector or agent or something that
parses the data and then enables you to
do actual field based searches you can
say I want to have the user name is
Raffy and not just look for Raffy
anywhere in the log files and that's
sort of what a bunch of these log
management tools did and what they do
for basically the main data source and
it's it's a huge pain to build all these
these parses if you think about all the
data sources that are out there there's
so many and building a parser for all of
them is just incredibly hard it takes a
lot of time and these data sources keep
changing you don't have documentation
for them so you can't even generate all
the logs how you test this in an
intrusion detection system case how do
you generate all the signatures it's
almost
possible to trigger all of them so
there's a lot of challenges associated
and a lot of people sort of ignore this
problem a little bit as well if you have
certain data source we give you an SDK
and you can build those parcels yourself
which is not really a solution in my
eyes we'll talk about that in a sec you
know a little more also so these
connectors and agents a lot of people
when I talk to them they're like oh is
your solution age-based or is it like in
any
RFP you get that question is that agent
base is agent less there's always like
oh it's agent base that's bad because
you have to install something on your
machine but what people don't understand
is there's actually benefits to having a
piece of code that does some processing
and often what what they really mean
well is - I have to install something on
the end system to collect the log files
that that's what they really want to
find out there's always some sort of a
connector or agent involved or should be
so what these guys do these connectors
or agents some of the features that you
can batch data so if you could get
real-time feeds in general you can still
batch some of them up and maybe every
second you send a new batch of data and
that helps if compression helps if
bandwidth consumption helps if
encryption of the data you can sign the
data also as you can show that you have
integrity so if you like hash chains or
something on the log files you can do
secure timestamps even if you want
inside of these connectors and often
there you'll also use for failover so
that if if you have a couple of servers
where you collect the data they can one
if one goes down or overloaded they can
switch over to another one often what
they also do is they parse the data
obviously in most cases but you can also
aggregate so they can say well if I see
a port scan for example happening in our
network and the firewall just goes crazy
there's I have all these connections you
can aggregate and say well I see this
one a hundred times and here's the
here's the data for it sometimes what
you can also do is enrich the data with
context information so you can configure
these these agents to have more
knowledge so that they can go out and
say well I'm creating your configuration
management database and get the role of
the machine and add that to the log
files for example
there's all kinds of other enrichment
you can do inside of the agent already
said he offload that from the central
server or manager where you send all the
data and then obviously they can help
you support different protocols things
like ops a core as the you have to have
a client that actually talks to your
resources to do that then sometimes if
you install an agent on your end system
that can have benefits you can for
example monitor your file system for
changes and then report on all of those
you need to have something on the Box to
do this otherwise it's really hard to
create a machine for things that have
changed on the machine itself now if you
leave the log management side so we had
the log management tools themselves
mostly raw data sometimes they have a
connector that parses the data and all
kinds the data then you have the sims
which have a very similar sort of
architecture where you get the data and
in raw form you have a connector that
parses it but at this point all the data
needs to be parsed and understood so you
need to have a connector for each and
every data source that you have and if
you don't part or if the connector
doesn't understand certain types of logs
or certain log entries generally it will
just discard them and put them in some
error lock that no one is ever looking
at I had a case a year and a half ago I
was on site at a large car manufacturer
and I was helping them they were really
mature in terms of using the tool they
really understood it I was really
excited to go there to work with them on
advanced use cases and when I started
looking at things I'm like guys you're
dropping about 40 percent of all of your
Cisco router messages because they're
not parsed correctly and if you think
about that it can have incredible
impacts on the whole down the line if
you don't see them ever you miss all
kinds of stuff
potentially attacks that are just going
by and you have never seen it what then
happens generally there's a relational
database schema where the data is being
dumped into its column just pros and
cons and then the sames usually takes
take all kinds of external information
into account so they can connect live to
a CMDB where you have the configuration
of a machine or an asset database or
they can connect to identity management
so that you know that these users are
all the same so from one system my
comments are marred if
the other one on Rafi they can control
all that and I can get additional
information on the identities of the
users so I mentioned this already so
there's generally a fixed schema to the
relational database there's a fixed
number of types and fields now what
happens if you get a new data source a
good example was I think we got we
looked at email systems at some point
and there was no field for emails but we
had a user name field were like well
emails are very close to username so you
just start overloading these fields
right so certain connectors now I just
parse the email into the user field well
if you have certain rules that look at
user names and do certain correlation on
that you might suddenly get correlation
rules that trigger on all kinds of stuff
that you're not intending to do so it's
just a hack that you start overloading
so how do you do that in a relational
database usually these clusters that
really like if you have to scale this
stuff it gets really expensive and it
it's also really hard to set up the
database itself for for making
historical queries fast but they usually
do is they try to optimize for insert
speed and then the queries are extremely
extremely slow and that's a bunch of
tools out there that are just horrible
at careering debate now there are also
benefits to doing this and that's really
what the premise was when these things
were built one of the things is that you
can actually do a real-time correlation
now based on parsed data that you can
extract so I can say I look at a certain
user name or a certain source IP address
and say well if I see that hitting my
server 15 times then I'm gonna alert
right away I can do statistics on things
so kind of that correlation theme
the other interesting thing is now you
have a unified language you can define
to access all of the data so you know
that my source addresses are always
called a certain way they I can address
them right away so it's it's easier to
write correlation rules for example now
what happened probably long kind of
pioneered this where they said well why
why do we either have full-text search
and no parse or no access to fields
themselves or you have the are DBMS and
with all the problems that you have
associated with that which slowness and
all that kind of stuff why don't we just
index all the data and then you can do a
full-text search so you search for
denied it basically pulls out you use
the index to get all the results and
gives them back to you well if you want
to have feel extractions down or parsing
down what you do is you get all these
results and now you apply your parsers
on the results and then you look and
which ones it actually matches the field
you're interested in and you throw the
rest away and that's really what Splunk
started doing and now a lot of the tools
out there are starting to do it the same
way which makes a lot of sense but it
still doesn't enable your real time
correlation so you still have to kind of
either fork your data one part goes into
the index and the other part you do the
real-time correlation by it by adding
all the partials in there applying that
and then doing your Europe correlation
rules so that's kind of what's happening
now that people start going with these
sort of hybrid approaches but by having
learned all this over time the nice
thing is also that now there are some
companies in the doings where they're
starting to use different approaches
they're starting to use more more
scalable things with all that the whole
big data movement now you go to column
based databases you use all these new
technologies and if there's there's I'm
fairly sure there's going to be a new
wave of tools coming out of the market
that actually support these new
technologies that we just kind of
discovered in the last couple of years
now another thing that's interesting to
know and something that these sims are
doing is they're using categorization or
tagging however you want to look at that
to basically talk to the data so if you
want to look for failed logins for
example you have to across any kind of
data source you have you would have to
write this crazy query say well if
Windows reports a security 538 event or
in UNIX you have the sshd authentication
failure oh and in some cases is actually
called sshd failed password so you have
to know all these different ways of how
these tools talk about the failed login
and maybe if you add a new tool you have
to go and again and do that and it's
just in every correlation well you would
do that again so you always have to say
that so what what people started doing
is you basically find sort of it I call
this a taxonomy it's not really a
straight mathematical taxonomy or you
can call it categorization schema where
you basically map all of your events
into this sort of n dimensional space
where you have what we used or a lot of
people are using now is that you have
sort of an object that you talk about
for example a file or a holy sonnet
system or a service or something then
you have a behavior or an action what's
being done to this it's there's a login
it's created it's deleted and then you
have a status was it successful or did
it fail and and then suddenly you can
write queries that are much nice so you
can say well show me all the objects
that have an authentication and the
action is a login and it was actually a
success but then you can also say well
show me all the the the file objects or
all the events that have to do with file
objects that actually failed so I get
all the failures for files that's very
interesting and then you can obviously
in the queries you can combine searching
on these taxonomy entries and regular
fields so you can say show me all the
logins from user RM RT and I will get
everything that way this approach scales
a little better the problem is someone
has to build these mapping tables and
for example at our site we had a team of
at this point probably three or four
people that basically maintain this
table add new events to it as soon as a
new data source is supported you add the
new entry there you map it all if
there's changes due to different tools
it's incredibly hard because you have to
get all these updates all the time you
have to do it in a timely manner and the
problem is if you don't keep these
things up to date and keep keep them
really in production up to date then you
start missing information or events
again if you look for failed logins and
there's a new event that talks about
that on Windows if you don't update your
mappings you're gonna lose that okay now
the last thing here is what I'm quickly
want to mention is what what we see
happening now is that everything a lot
of these services are moving to the
cloud as this or milada these
applications are enterprise features are
moving to the cloud as a service and it
makes a lot of it
comical sense for especially smaller
companies bigger bigger installations
there there's always a factor well candy
services actually support you so if you
were starting to send me a terabyte of
data today it's probably not something I
want but there's a lot of smaller
players that have a few machines and
they don't really want to set up their
own thing so it makes a lot of
economical sense for them the
interesting thing is for us for example
is that we're really elastic so that if
we get more more people to just add more
machines and it just scales really well
versus a regular log management tool at
some point that hit some kind of limits
because the way they're architected the
other thing that we're really big on is
providing an open platform with all
kinds of API so you can search the data
you can manage all of your data and all
your objects in the system so that's
another trend I'm seeing with all kinds
of enterprise application services that
are really open so you can interact with
them in and automate things so here's
here's what we do in a kind of a schema
it's fairly fairly simple you basically
we take the data in and either in syslog
or its HTTP posts a lot of times in for
example Google App Engine right you
don't have access to sockets so I can't
really open a SUSE log connection to
someone but what I can do is I can do a
back post so we accept web posts where
you sent consent the log files there's
all kinds of overhead and inefficiency
is associated with that but at least you
can get the logs out and into us so we
get the data we write a copy to an
archive so that the customer can
actually get to it untouched and the
rest goes into our index and they're
using solar cloud to index all of the
data in full text and then store it and
then we have api's to access the data we
have a very interface that's very
interesting because it's basically a
shell inside of your web browser so you
actually interact with a be typed and
it's there there was a google mashup I
think it's called goosh to Google shell
or something so it's kind of mimicking
that kind of approach and then yeah I
think that's a
now I get asked by people often what
tool are we going to use should we build
it ourselves we used luck management
tools and in this slide I've basically
built because a couple of weeks ago this
guy came to me he's like well we have
this log management tool and we're doing
this and this and this and we want it
we're gonna use it for this use case of
looking at I forget what it was it was
one very specific data source in a very
specific use case of looking at that
data and he's like can we make our log
management tool do this right why would
you even do that that doesn't make any
sense your cost is gonna be so high
buying a license for that because you
pay by the volume just build your own
use it use a database for it and parse
the data it's you have one data source
that you need you can build a parser for
that one reg X probably it's fairly
fairly structured log just store it in
the database and then build a little web
interface to it to address exactly your
use case build your mashup use some
visualization API to generate an image
from that and that's really all he
needed so oftentimes you have to think
about how many different use cases do I
have how many different data sources do
I have and and then you can kind of map
yourself into here and you guys have
access to the slot you can look at it
details yourself if you want so don't
fit a square peg in a round hole so
what's working and what's not and I
think I already mentioned a bunch of
this before I think law collection is
working today we can get the data into
central location we can alert on
previously known patterns we can't
really alert on the more intelligent
things yet I believe and we can solve
specific known use cases for known data
sources but as soon as you have sort of
the the more dynamic data sources it's
it's getting kind of iffy whether we can
actually still do it so what's not
working is the way big a list I have and
this is sort of the beef I have of the
whole log management industry and I
think people are not necessarily taking
this very serious one of huge huge
problem is that there's so many lock
formats out there they're not documented
they're not standardized the standard
we're working on is is still it's not
released we've been working on this
four years at this point we have all
kinds of entities involved and if you
look at just if you take two very big
operating system vendors and you let
them talk about how to standardize log
files you get two really interesting
point of views there there are no
guidelines out there on how to log and I
think this is a huge problem we have
it's sort of analogous to the security
problem right like we're trying to make
things more secure well you have to go
through the source of to the program
it'll actually teach them secure coding
so you have to educate them it's exactly
the same thing we've logged in you need
to teach people early on look here are
some guidelines and here's how you do it
it's not super complicated but come up
with these guidelines they're gonna help
you so much not I'm fairly sure you guys
actually here do that but you said look
here's how sort of a logging framework
works
I think parsing is completely broken and
we talked about that you have regular
expressions you have to write and they
are always wrong and they are not they
don't scale because now you have a
different log format coming in think we
need to find something else there and
then we get into a little more hidden
problems that a lot of people don't talk
about don't even know about
I think normalization is broken and what
I mean by that is that some log files
report of an IP address another log
reports a fully qualified domain name or
hostname how do you correlate that well
there's a couple of ways to do this you
can say well when I collect the data I
map everything into either an IP or a
hostname right let's say you map it into
a hostname so you do a DNS lookup at
that point well guess what it's really
slow a DNS lookup takes a lot of time
for the result to come back and if you
have real-time streams coming in that's
that can be a problem now if you did it
later the problem is now the DNS might
have changed you don't know what the
original DNS was and make sure you use
the right DNS server to actually resolve
because maybe depending on which DNS
server you use you got to get a
different result so there is interesting
problems there and a lot of the sims are
actually doing it at the correlation
time they're not doing it in the
beginning but they start doing it later
then you have to hold username problem
mapping different users and identity
to the same entity and that's not just
username specific there's all kinds of
entities that you address in different
with different names of different ways
but how do you know they're the same
this whole categorization taxonomy
things is or tagging I guess is is
interesting I think it's a good approach
to sort of abstract certain things but
it's it's always out of date how do you
update that if you have a new signature
like a new IDs signature coming out you
need to update that thing immediately
you can't wait for the vendor to get it
done and usually they don't do it right
away so you need to have subscription
models to do that and push things down
and the next problem I see is a dark
side we were big on having this like we
call it threat level formula where every
event you calculate how important is it
if it's a ten you have to look at it if
it's a one forget about it there's no
formula that works we tried everything
and we look at contacts you look at the
roles of two machines of the history of
the machines we looked at the importance
of the event itself and it comes in
incredibly complicated and it's never
right I don't haven't seen a way that
works it and then finally anomaly
detection I think is voodoo you've read
a lot of research on anomaly detection
of log files and oh my goodness we could
talk at length about that I haven't seen
anything that really works
in dynamic environments and dynamic use
cases
I think one of the other core problems
is also that we don't understand the
data if you're a security analyst you're
sitting in front of a screen and you get
all these data sources feeding in you
might understand your security products
but what happens if you suddenly get a
feed from an application you have no
idea what that application exactly does
you have to understand fairly
intrinsically hot an application works
to actually make a determination of
what's going on unless you have really
good developers and guidelines or it
actually the logs are very descriptive
that say hey if this happens we have a
security problem so you have to think
about these things when you come up with
your guidelines and I don't think there
are really no tools out there and
support this notion I think this is
absolutely crucial today so what do we
need in the future well it's definitely
more getting more and more more more
data and current architectures of these
log management tools they've just don't
cut it so we need to use new approaches
Big Data stuff to actually deal with
this problem of large data we're moving
more and more in the application layer
people are really interested in
monitoring business layer logic it's not
just about the infrastructure anymore
I even say forget the infrastructure
look at the applications right you can
detect most security problems up there
I this is a little exaggerating here but
that's very where the money is so how
are you going to do all the parsing are
we going to have guidelines how do you
do all that and then we need to have
something that helps analysts or people
looking at the logs understand things
and my take there is that a lot of times
it's not really that I need a specific
security analyst looking at my
application data to understand what's
going on but I need to give the
application developer access to their
logs and tools so they can very quickly
look at them and if they find something
then they can maybe escalate it but we
need to make that a priority okay
so let's launch into the more fun part
here let's talk briefly about data
visualization I think we have some
people in here that know about that
topic much more than I do but when I
brought my book on it was called applied
security visualization one of my goals
was that I sort of see this dichotomy of
having security people they
understand networks and security and
policies and all kinds of technical
details really well that that's where
I'm coming from and then not that I
understand it very well but that's where
I grew up but then you have the
visualization people and these people
know all kinds of things about
perception and know how to make graphs
that actually work things you can look
at and interpret and make them
interactive and all that kind of stuff
and I feel there's this huge gap between
these two two groups and we need to
bring them together and my approach was
like well how can i educate security
people to give them the minimal
knowledge of these visualization things
and things they can apply right away so
I went to a whole bunch of university
courses and books and just try to
extract what I need to know about
visualization I'm not interested in how
the eye works and optics and all that
stuff that's usually what you find these
courses I was interested in like how do
you make good displays how to use color
in a good way how do you like if you
have color palettes or schemas for
example or how do you some of the
principles that ever Tufte talks about
if reducing non-data ink things that are
family if you look at them and like that
makes a lot of sense but we need to
teach technical people how to do that
it's not complicated but that's the
stuff that people should know so when I
look at visualization now usually
there's sort of four important things
that I think we can do with it it's one
it's exploration and discovery a lot of
people come to me and say well but why
don't you just write algorithms a tool a
script that does this I'm like well but
you don't know what you're looking for
if you first have to explore your
dataset figure out what you want then
you can code your thing but first you
have to get an idea what's really going
on then you can use it for answering
certain questions like certain things
you just your answer much quicker if you
have an image that you look at it and
you're like oh yeah that make sense look
we have all this kind of traffic going
on instead of writing a tool that
there's some analysis and comes up or
something because we can use our brain
to interpret these things and often
probably one of the biggest use case is
communication if I want to tell you what
happened I can give you an image to look
here this machine talked to all these
others and here we had these things
going on and the hairs cluster of
something it's much easier to
communicate with in
and how it can help you making decisions
so if we look at the security
visualization field so I'm focusing a
lot on security here because that's what
I know most about I think we're
absolutely nowhere with security
visualization we have had a few years
now that people started talking about it
but we're nowhere visualization is
generally an afterthought right you
start collecting the data and you're
like oh I have all these use cases and
I'm gonna write some correlation rules
and isn't that a report but
visualization in the end is always
afterthought people start building these
tools and then they're sort of plopped
on top of an existing solution there
they're never that main goal I talked
about that economy dichotomy and then
the tools we have out there they lack
basic capabilities there they're really
rudimentary and what they can do I think
in terms of what we need a security
analysts so here here's some quick
concepts on things I think extremely
important if you're writing or if you if
you're building a security visualization
or a visualization tool in general one
thing is a concept that ben shneiderman
sort of came up with which is basically
have if you look at data then usually
one display or one look at the date at
the data it doesn't really solve your
problem right away so if I asked you or
if you asked me to visualize some of
your NetFlow data I can't just give you
one display and you're like oh yeah
actually that solves it for me but
generally what you want to do is you
want to look at an overview to get an
idea of what's happening and then in
this graph here you see there's two
spikes and they're in the top the two
top bars and then there's some gap so
that might be what I'm looking for so I
can then zoom into that potentially
using a completely different display
form I might use linked graphs for the
for the next analysis to see what's
really going on in that kind of data and
then from there I might see oh there's a
certain cluster here what really
happened here and am I generally I have
to go back to the original data I might
have to pull up my peak apps or my net
flows to actually understand what's
really going on in the data so these
three things people need to keep this in
mind that it's really a process to go
through
some of the things that are interesting
when I try to look at security data what
I like to have is simultaneous views on
the data we've different for example I'm
looking at the source IP address
distribution destination IPs and then
I'm looking at some kind of maybe in the
categorization schema what do I have in
terms of what objects are impacted what
what are the status is what are the
actions that are being done on this data
so I can very quickly kind of pivot
around hopefully this is interactive so
that I can select certain IP addresses I
think it's the next one or so here like
you use a dynamic coloring scheme I can
say well I want to use I want to look at
the the top source addresses and as the
color I want to use the severity of the
events or I want to use some other field
that I have in my data so I can very
quickly sort of pivot around and
understand what's going on in my data so
here you see different examples of
dynamic coloring and then something that
I find try to be used for is linked
views so we have your different views on
the data and if I select one of those
bars or one of those sectors and in the
in the pie chart I actually see the
entire displays updating and I see my
selection across all of them so I can
very quickly pivot around to figure out
what happened in my dataset and
hopefully I can then filter things out
that oh that's interesting
let me get rid of the rest or let me get
rid of exactly this so that I can
interact with the data and the tools
that actually and the screenshots here
are from business intelligence tools
like this is advisor solutions there's
Cognos out there it is tableau there's
all these tools that support this but
they're not really built for our
security use cases in our security data
sources so that's always an interesting
problem and then one of my probably the
biggest the most important principle
that I find is really what Edward Tufte
said is reduce your non data ink so the
the left hand graph is the same as the
right hand but it uses all these
different things that clutter to display
the background the grid the bounding box
the three-dimensional pair
you can reduce that all but you also see
I added some things the the labels and
the x-axis are not there so what are
these different pyramids showing there
well you you want to have labels so you
don't always want to just remove things
but you want to have the information in
there that you need and then maybe maybe
the second most important thing is use
the right way of displaying your data
three-d park charts might be really cool
looking but this one is so bad if you
can read the labels the red you can
barely see the red slice there is just
kind of the smaller the villain has the
least height and it says 108,000 and
then the next one is 126,000 if you look
at the sector's width
the red one is bigger than the gray one
but it's 108 verse 126,000 so what is
that and how big is the yellow one in
that case well if you if you sort of
tilt to the side and look from the side
as like a bar chart that it makes sense
but if you tilt it and look at it as a
pie chart it doesn't make any sense
anymore so use the right chart for your
for your problems a lot of people have
have very strong aversions to two pie
charts and this is what I found from
deal but I'm not sure if you can read it
all we can
yeah so in general I also believe that
pie charts are often not the solution
you want to use if you just have two
numbers you want I maybe compare them
percentage is something maybe yes but if
you have more than the slices all look
the same that you can't really tell
which ones is bigger you have to put
labels in there why not use a bar chart
that shows it much much quicker and
nicer and then something that a lot of
especially junior analysts are to blame
is they jump to conclusions very very
quickly and you always have to sort of
question your entire data collection or
the entire data processing process and
here this is a network graph that I
created very early on when I was playing
with visualization on security data this
is actually from a honeypot I was
collecting data and I got this graph and
I was super excited I'm like we have
attacks going on there's malware in our
honeypot until I started looking at and
state a little more and trying to
understand what's going on so I'm not
sure if you can really read the labels
in Kenya sort of is that what do you
think is going on here
I'm already giving you the careful signs
though it's probably a little more
obvious but but a lot of people say well
there's there's something scanning for
different services or sears the circles
here are destination IP addresses so
machines and then the squares are ports
that are accessed on those machines a
lot of people jump to the conclusion oh
yeah wow we have all these different
activity there's there's these this
malware trying to scan us on these ports
and trying to figure out whether we have
something running well the problem here
was when I investigated this a little
more is that there's something I called
a source destination confusion where
these destination ports a lot of them
are actually source ports and what
happened at parsing state there was a
mistake where someone assumed that the
first column is always the source
address and the second one is the
destination as same for the ports but it
happens though that if you look at a
server client-server communication
that's true for the first the third the
fifth and so on the communication
between client and server but from
server to client it shows you those
things inverted so if you look at like a
TCP dump for example that that's exactly
what happens to you so you have these
every second one it's the wrong way
around so if you look at things question
the process before you jump to
conclusions at a we have an attack going
on and I see that way too often so here
are some examples that I collected and
generated over time this is a couple
pictures i have and i visited the
norwegian cert there also the secret
service up in norway and i taught a
class up there and visualizer security
data and what they have is they have
this security operating center where
they monitor all of the critical
infrastructure in norway so the power
grids the water and everything term in
terms of IP traffic so they can see very
quickly if there's spikes and certain
traffic in certain like like in the
water sector for example or if it's on
gas and oil or so they can see attacks
across the critical infrastructure now
usually when I go into these security
operating centers you hand in your phone
and your your your camera and if you
have a laptop with a camera that you
can't bring that in so they're very
strict I mean I asked the matter I could
take a picture in their sock they're
like yeah absolutely
a little baffled and there's they're
like what they have is this red button
on the desk and they press that and what
happens is it anonymizes all the screens
so it's all random data it still shows
the real displays but the data is
completely random so on the top left you
have different NetFlow sensors out there
it's a completely random number of
sensors that they show it's completely
random information but they found that
so many people interest in what they're
doing that they want to support this and
they want to promote what they do and
want to be able that people can show
pictures of it or screenshots when I
walked around in in their office I found
another display another dashboard and
that one is actually really interesting
because the right hand side on here I
thought that was the most important
information that they were broadcasting
in their office if you can if you
understand German or Norwegian cafe con
it's a coffeepot so
they actually have the two webcams in
front of their coffee pots to measure
how much white versus black does it see
and they can tell you how much coffee is
left in the coffee pot so that sometimes
important for security as well to keep
your analysts awake then so here are
some actual examples of security data or
security relevant data this is the graph
I didn't create but this was Chris
Horsley down in Australia and what he
did is he takes kind of a similar setup
like the Norwegian cert where they look
at a whole bunch of NetFlow sort of
sensors out there and they're trying to
figure out what what's just generally
going on and what he does here is he's
using a tree map where he's showing the
amount of data by sensor and by port
number so the sensors are in color and
not sure if you can really see it down
here under the TV there it's actually
really good so you see like blue up on
the top left is all one sensor then you
have two red then the the little
rectangles in here are different port
numbers so if you have large rectangles
in there it means there's a lot of
traffic happening there now the ones
that are sort of highlighted here is
port 445 across all of these different
in different net flow sensors so there's
something obviously going on now the
interesting thing here I thought what
Chris did is he's using the the
brightness of this the direct angles to
show the variance over time and so
basically he looks at how much traffic
did I get in the last five minutes
versus to the last hour and he if he got
a lot then it's much brighter so that
you can see changes that are temporally
very closed are much brighter so there's
a lot of activity going on somewhere in
here a little bit here so you see
different things popping out and if he
sees a certain port across all of these
net flow sensors suddenly being really
bright he knows there's an attack
happening across all of the different
sensors and not just locally to one one
of them so I thought it was an
interesting way of using tree maps and
colors here's another example that I was
able to bring home from Norway that it
was just a fun example that one of the
guys ran a firewall at home
what you see here is time and then the
port number then he visualized that
basically whenever there's activity you
draw a dot if you have more activity
just make it brighter or use a different
color so I can color a color scheme for
that and you see there's some scanning
going on over here on different port
numbers and you have some horizontal
lines but there were these two guys that
knew what this guy was doing so they
inject the traffic into the firewall and
out came their two faces I thought that
was pretty cool
there's other ways of analyzing or
looking at firewall data and I'm gonna
cut these exams a little short in the
interest of time here what I'm using
it's just a tree map and I'm looking at
the source address as the outermost box
so everything on the left-hand side here
is this whole what is that about a third
so a little more is all one source IP
address connecting to multiple different
destinations that are the next one here
and then on different ports so the port
number is inside and on the left-hand
side what you see and if it's red its
blocked and if it's green it was passed
so it is the other left hand side there
was this one machine going to a lot of
different machines in my network on
usually it is just one port here was
echo request I'm putting that into the
port number category here so there's a
whole lot of pings that came in they
were blocked but then what happened on
the top left some things came in were
blocked but so many things were passed
to the green parts up there and right
after there was a connection attempt of
port 135 it's a very common behavior for
some of the warms out there back when I
generate this example the date would
ping machines if they find one they
start connecting on the windows share to
try to see whether they can access it
and exploit it
so fortunately firewalls seem to be okay
configured but my question here would be
well why do some machines allow pings to
come in so that's very quickly I can see
that the other thing that I probably
want to investigate is this here that
that really sticks out what's this 427
traffic that has been blocked turned out
it was a misconfigured Mac that was
trying to talk
Bonjour the Bonjour protocol and say hey
I'm here talk to me talk to me
fortunately the firewall block
and didn't let it out to the world so an
interesting way of looking a lot of data
on kind of prioritizing understanding
exploring what your landscape of your
firewall data is here's a way we're used
sparklines for port number source IP and
destination lab you should see kind of
trends over time and you see again and
the port numbers for example there's a
whole bunch that have just like one
little tick here and I'm really curious
what that is he might be so the source
destination confusion it might be
something else that happens every now
and then maybe there's someone standing
me very very slowly I might see other
patterns in here there's a bun machine
that it's very periodic for that 212
machine up there it's a very periodic
behavior why is that is that at the NS
or a backup servers some kind of a
scheduled thing going on so it's very
interesting to kind of explore
time-based things here then one of the
big problems with intrusion detection
systems is you have a lot of false
positives and you have to tune your
signatures so how do you do that what I
like to do is I like to basically what
I'm doing here is I look at the source
address as the outer kind of part of the
hierarchy then you move into the
destination address the signature and
then I color it by the priority that
I've given these different signatures so
I can look for a large dark or red
rectangles in here to kind of focus my
work on and I see very quickly the
left-hand side here seems to be the
stuff that generates a lot of events so
I can start focusing on that and often
you will see that for example ICMP based
signatures are triggering a lot of these
signatures and then you can go in and
say oh well I'm going to define
exceptions because my network monitoring
station is pinging all the machines and
this is in the network and that generate
a lot of data so you can see these
trends and things very very quickly and
prioritize your work the same with
vulnerability scan data where here I
basically by I think it's the by Machine
I'm showing the different
vulnerabilities by port also and then I
associated a certain priority with those
vulnerabilities they're all red ones are
the important ones so I see very quickly
I have one machine at the top left it's
super vulnerable to all kinds of stuff
the next one down as well
and then I have one over here that seems
to have a very large or very important
vulnerability that I should probably fix
and then there's a whole bunch of little
stuff on the bottom right but this in
the end were probably about two hundred
thousand if different entries in a you
know sort of lock file that I had about
vulnerabilities on machines and you can
very quickly see these are the things to
focus on instead of going through and
figuring out what's happening so what do
we need for visualization in the future
I think I mentioned some of these things
before we need some kind of a solution
for the entity extraction or the parsing
like I still feel this whole regular
expression based approach is completely
failed we need something different
we need dynamic and interactive display
so what I showed you here was mostly
static that doesn't really cut it if you
want to investigate something these are
good screenshots of like a certain state
of my investigation that I was happy
with but if you want to do something you
really need to Center active and dynamic
displays and then something I call
computer-aided intelligence I don't know
that was this morning it was kind of a I
don't know if that's a good way to call
it but what I mean by that is I feel
like we don't need to chase these
anomaly detection algorithms that are
really hard to make right well I think
what we should chase is is using us or
the analysts that the experts more and
giving them the tools that they need to
look at the data because teaching a tool
that to tomorrow at 2 o'clock there is
going to be a maintenance window where a
lot of stuff is can go wrong versus
having the analyst that probably knows
it because he saw some email or
something that's really hard to bridge
that gap right there i encode all that
knowledge that the person can have so we
should make things to support analysts
or people that look at the data more and
make it super highly interactive so that
people can pivot around and really use
the data to look at it and then
hopefully capture also the data from
from these experts when they when they
work with the system so they can say
well if I have a graph that looks like
this this it's a port scan maybe that's
something very simple to detect but if
you have a junior person coming in they
can help them a lot to understand that
so
capture the knowledge and make it
collaborative that people most multiple
people can work on this so if you want
some more information on security
visualization there is a secretary
storage sort of a portal I maintain very
loosely it's really for people to submit
things they do and visualization for
security purposes it's sort of you just
submit your post of your question or
whatever a little library of graphs in
there your gallery of graphs you can
look at people who have done work in
this area there's a live CD that's
unfortunately kind of out to date at
this point but Kristin how we're talking
a few months back the CD was actually
built by a friend and actually one of
your co-workers he works in Zurich
it's called David's the data analysis
and visualization Linux and it's just a
live CD that has all kinds of
visualization tools installed they're
readily running so that you don't have
to go and compile them yourself and then
you don't have the libraries and all
that stuff but it's a little out of date
data at this point but maybe we can
revamp that and we're looking for people
to help us there's a Twitter feed
there's a mailing list that's absolutely
quiet no one ever posts anything on it
but feel free to use it there's a bunch
of subscribers on there and with that
I'm done and if any questions I think on
the point an hour but if there are any
questions please
or comments yeah
so the question is how does public
powders pricing work and our cook
pricing work in a cloud model what we do
is based volume based we wanna make it
we want to keep it simple if you look at
the pricing of the different log
management and same tools they look ok
they use all kinds of different metrics
like how many data sources do you have
and how many different ones and how many
hosts and then for vulnerability scans
it's a little cheaper because of a lot
of them or you said it's like just keep
it simple you want to get as close to
utility based computing as you can or we
have a tiered model with log lis
whereas we could do the utility based
just what do whatever you do you what
you get is what you pay for but but
that's I think the simplest thing to do
and then sometimes in higher tiers you
can add more features to kind of price
that way also but volume seems to be the
thing to do and the other question yeah
so what we do is basically you send the
logs to us and then we give you the
tools to look at the data again so it's
just a fully indexed data at that point
and we give you tools to graph it and
explore the data so we don't go and
analyze it and tell you all this
happened on the data and to turn around
if you're asking like what's the time to
index it's like 10 or 15 seconds and it
shows that way yes yes yeah and that's
that's actually a big problem that we
had to solve if you use something like
solar and indexing engine how do you do
the near real-time indexing yeah okay
yeah I have I use machine learning to do
anomaly detection I haven't I read a lot
of research like the the raid conference
recent advances in intrusion detection
has a lot of different approaches that
people tried for all kinds of stuff
neural networks and we should learn
whatever you name it what I've read and
seen is just hasn't worked it's just
it's hard it's not that easy because the
environments change data source has
changed and even if you try to keep
these things fairly stable you have to
generally you have to train these
systems and and that's where it falls
down
no one has like how do you train a
system in a live environment you don't
know you don't have labeled data you
don't know what's bad and there it's
like every time someone comes to me says
oh I have this thing you have to train
the system I like forget it it's not
going to happen but maybe maybe you can
prove me wrong maybe you have done some
work on that but I'd be interested in
hearing that if anyone has any
approaches to it
I used to
yeah yes sure
hmmm that yeah that's interesting
probably won't scale because your
environments are very dynamic in general
right like even if you keep the data
sources static you have new machines
being introduced to your network if new
users coming online
if new applications coming online so
your patterns keep changing even if you
keep them static I mean yeah you can
probably do you can go through all the
data and say oh this is this distance
that but what you will find also is that
a lot
you have big clusters of things and
those are easy to classify but you have
this all down the long tail it's gonna
kill you it's gonna be so much it like I
can guarantee you every system you're
gonna look at you will have things where
people like we've no idea what this is
there's one log record here one log
record there you have no idea where that
happens it takes a lot of time to
investigate what is this why is it
generate the backup solution triggering
my IDs every night that's not that hard
because it's periodic but you see the
dis log entry here no idea this
application just does it right but yeah
if you can make that scale so the
labeling of your data
if you introduced a complex API to let
people label their data I I think yes
well I'm not sure about the complex part
make it simple but I had thought on this
first a second ago the question is can
you the question is how do you let them
what are the features you let in label I
if I if you have your data set and I
have mine and you label some of your
data based on IP address you say oh this
is a web server doing this then I can't
use that because it's your IPS so you
have to abstract it to the right level I
thing and that will be very interesting
if you can come up with sort of a
taxonomy for that and then let them yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>