<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Twitter's Finagle for the Asynchronous Programmer | Coder Coacher - Coaching Coders</title><meta content="Twitter's Finagle for the Asynchronous Programmer - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Twitter's Finagle for the Asynchronous Programmer</b></h2><h5 class="post__date">2013-03-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/30v5WVFvlno" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Matt Howe I'm the CTO for
loyal three and today I'm going to talk
to you about one of my favorite
open-source platforms
Twitter's finagle well in Twitter's own
words finagles and network stack with
JVM that lets you build RPC applications
in a protocol agnostic way oh why do we
need another one of these things you
might ask if I take you back a little
bit long ago when we were running web
applications or maybe when company first
starts writing them they're very simple
you've got a web application talks to a
database pretty easy but as time goes on
what you find is that you add complexity
to the system it's not enough just to
talk to the database now you introduce
the services layer you might have an API
server as an abstraction so you've got
service a and service B but it doesn't
stop there it goes on alright you decide
that oh you know service B really needs
to call these other services maybe we
need a cueing solution maybe we need to
call this other external service and
your environment grows maybe you decide
that that point now is the time to have
a mobile system so you add the mobile
interface and your environment grows
again what happens is not before not too
long boom you've actually got a pretty
fully fleshed environment a distributed
processing system looks a lot like
tumblr looks a lot like Facebook Twitter
a lot of the systems that you see so in
addition that not only you have lots of
services but most of these services have
their own protocols you got a little bit
of HTTP here a little bit of thrift over
there some JDBC Cassandra Redis etc and
typically each one of these protocols
comes with its own libraries jars etc to
go manage it with all this stuff Hey
look at Gorog
the reality is as you heard from the
previous talk there's actually quite a
bit of stuff that can go wrong I'm not
gonna bother to go through this list we
the previous talk I think you did a
great job of going through but there's a
bunch of stuff that happens and that
can't happen so what was what is a
solution to this
well the way I think of it is I think of
finagle as an expressive network library
to develop these applications so not
only does it handle the networking layer
but it also handles some of the core
libraries and services underneath that
well what does that mean finagle
actually has quite a few pieces to it
and what I'll do is I'll break up this
talk into two parts one is as an
asynchronous futures framework and the
second is as a common infrastructure
that you can build your client and serve
our applications on so let me first turn
to the the asynchronous platform
finagles nice it reduces the world and
says if you're going to be doing a
synchronous RPC development there's only
three basic constructs you need to worry
about a future a service and a filter
the first one a future we've heard a
number of talks on it now it's a
computation that nut has not yet
completed it's more or less a promise of
something to be done later most of the
time it's either non-blocking or its
implemented through either non-blocking
code or a thread pool in the case of
finagle finagle almost entirely uses
non-blocking code so if you've heard of
frameworks like event machine for Ruby
or twisted for Python or nodejs it's the
same idea rather than creating a large
thread pool to handle these asynchronous
calls what it does is use a small number
of connections or a small number of
threads to base and manage this so it's
using non-blocking code almost
throughout the entire thing it gives you
the option if you want to use a thread
pool you can but by default it's going
to be non-blocking
I think the thing that the previous
speaker alluded to is that one of the
challenges with futures is currently
there's a lot of choices you know you've
got the Twitter futures the concurrent
the actors ACOs version Scalzi there's a
lot of versions of futures what I'm
going to be focusing on as you might
expect is just one of them even though
there's a bunch of dialects
all right the dialect that I'll be
talking about is the Twitter future and
what we find is since so much of our
stack is built on this there's not a lot
of interoperability for us we just kind
of picked this one I went with it I find
sometimes the easiest way to grok the
way it does is just to give you a bunch
of examples so I'm gonna go through and
talk about different things you can do
with them so here's a really basic
example Val future abuser is take this
user service and look up an email so
it's just a basic thing of look up this
user by email and give me a promise of
the future when it'll be done now let's
see what we can do with that the first
is I can say well forget this
asynchronous stuff I just want to treat
it like regular blocking code so I can
call get on it and that will just return
me the user object so I can treat it as
if it was synchronous this is really
good for testing where you don't
necessarily want to have to wait you
just want to say go do it now but in the
real world we oftentimes we don't want
to block we want to do things in
parallel so here's a case where I might
have a cache service and I want to look
up three key values key one key to key
three and I want to do all those
simultaneously so when I make these
three invocations none of these blocks
so I immediately drop down to this
location so that's kind of nice but I
really want to be able to at some point
maybe render a web page so I want to
know when all three of these are done
and I don't want to have to wait and do
again on each one of these sequentially
what I'll do is I can combine them
together take future one future two
future three put them into a sequence
and then just say join them together and
just say tell me when this is done so
now when finish get is called when this
finishes it means that all the futures
that are
posed in there we'll have finished it's
a nice handy shortcut in other cases I
might want to transform the future I've
got this user that we talked about we
looked him up from the email and so now
we've got this reference but now I want
to create a session off of it so I can
use just a standard collection operation
of map and transform that user now into
a future of a session it gives me a
really easy way of composing and
chaining these things together really my
goal in kind of machine-gunning through
these is to give you a sense that once
you've got it into this format it's not
very difficult to work with it all
there's a lot you can do with in terms
of transformations in terms of chaining
it's a very simple way to think about
things it doesn't add a lot of overhead
to your code keeps it very simple there
are cases where you might have do this
then do this then do this then do this
and as we've seen other talks you can
use a for comprehension to basically
turn those that chain into a singular
operation where you can use a for
comprehension to just go through and say
here first do this pass it to this and
then you know render the result here's
one of my favorite things to do if you
guys have ever done performance testing
what you'll notice is that usually the
response rates look pretty good and
usually when you get to the 90th 95th
99th percentile your curve goes from
flat to straight up right there's always
that 1% out there that 1/2 percent that
responds really badly right well an easy
way a way to get rid of that is instead
of making a single request and having
that long tail out at the end turns out
if you make T requests for the same
service let's say I'm looking up
something from my my cache or I'm
looking up something remotely if I can
afford to make if the backend can handle
two calls if I make two simultaneous
calls and just take the first one that
comes back turns out that long tail at
the end at the 90th percentile 95th and
99th goes way way down your curve goes
from something looks like this to
something looks a lot lot smoother and
so the effect of that is the response
time to your users is much much better
how much code does that take with
finagle
well let's say I have these two things I
want to look up in parallel future one
in future two I can now just say here's
a sequence of two things I want you to
look at and the number could be any any
number look a five look up six and at a
time whatever and what I want is tell me
the first I want to select the first one
that comes back as my result what it
returns to you this object I didn't want
to do the the signature because it's
kind of a long signature is it gives you
the result of the first response and
then all the remaining response is just
a case you want to cancel or do
something with them so in this case I
just said give me all the responses give
me the first one and here's the value
you know again the beauty of finagle
them in my mind is it made it very very
simple to do this without a lot of
complexity right this is something that
you could easily get your developers to
go do well let's keep adding complexity
to this sometimes I want to get back to
my users and let's say I've got that
long tail still and that long tail is
somewhere out there at 15-second
response times I don't want that I don't
want to want to respond to my user in 15
seconds if it takes that long I want to
go do something else so we want is to
say after maybe five seconds stop it and
let me do something come back to me so I
can just take that future same thing we
have before the lookup and I could just
say within within this time out within
five seconds you have to finish or it's
an exception right so now I have not
only the ability to control that long
tail but I can say well my service level
is a five second response I'm going to
give you something at the end of five
seconds and again not much code to do it
well sometimes problems happen all right
so let's say there's something wrong and
maybe the service is down with the
future I can just say handle this
exception and it looks met like many
others sort of exception handling block
and I can hand the exception here and
then return the same type and go on my
merry way
so as far as the service is concerned I
can deal with something return a future
strength in this block and life will
just keep going
right so it gives me the ability to
maybe I want to retry through something
else maybe I want to try a different key
you know whatever the option is I have
the ability to go do it so that's really
futures in kind of a a quick nutshell
and that's the first leg the second leg
is service so finagle treats everything
as a service so if you're talking to a
remote thrift application if you're
talking to a REST API if you're talking
to whatever it is a talks thrift
protobuf you know Redis memcache
whatever it's the thing that handle ups
wrong direction
a service is the thing that handles the
our pcs that takes the requests and
gives back replies it will always
conform to the signature now if you
actually look at the code itself there's
a little bit more to it I've simplified
it to make it easier to to see here but
ultimately service it has a request type
and response type and depending what the
protocol is that you're talking for an
HTTP you might end up with an HTTP
request HTTP response if you're talking
Redis you might end up with command and
reply as the instances of objects
depending on the protocol the request
and response types will be the same but
the interface is exactly the same it so
is service request response that hasn't
apply so let's make a web server out of
finagle so here is the here's the toy
version of creating a web server so here
is my service my service is basically
something that will respond to anything
and just say ok and gives you a blank
page back right it's you know it's the
toy version that you might see unlike an
event machine or nodejs and all I'm
doing is saying here give me a new
service and I'm creating a new future
and giving you back a response with a
version of 1:1 and a status of okay now
let's create the actual web server so I
can use this affluent interface and say
I want a server it's going to talk HTTP
I wanted to talk on port 5
I'm gonna call it webserver and it's
bound to this service so in this case
this is a web server importation but
this might be a thrift implementation
protobuf etc this doesn't change it's
the same so once I run that I've got my
asynchronous non-blocking server up and
running what's the flip side if I want
to do a client well client turns out has
another builder to help you create it
and it looks awfully similar so client
then you specify the protocol in this
case we want HTTP how long do we want to
be able to take to connect what hosts do
i connect to
how many concurrent connections will I
allow before back pressure starts
kicking in boom create this client and
now to utilize this finagle client I can
create a new default HTTP request just a
simple thing I've here is a get request
for the home page and I fire that into
the the client and it gives me back a
future response now the thing another
thing that I love about this is because
there's a very simple interface for
services this can be mocked up
tremendously easily so when we're
testing the application you know if we
want to do a actual live connection or
if we want to do a fake connection that
just implements this it's very very easy
to just swap one out because the client
doesn't really care you know when you're
invoking this you just want something
that implements this interface
so just for giggles I'm going to change
up that web client and now we're gonna
make it a Redis client so now we've
basically taken the previous code it
swapped it so now it'll make a simple
Redis request for my key and you notice
that nothing's really change that much
has really changed in fact here's the
only change that we've made as you can
see HTTP requests HTTP response has
changed into command to reply the codec
has now changed to Redis and instead of
creating a new HTTP request we're now
creating a a Redis command so gets
delete whatever the Reds commands are
those are the things we're sending in
and instead of getting an HTTP response
we get a reply back it's really nice to
have your team just only need to learn
things one way and once you've learned
things the one way regards to the
protocol that you're working with you
have the same conceptual model it always
works the same way and this is just as
asynchronous as the previous bit was if
we replaced it with thrift again it's
still non-blocking and asynchronous so
the last leg of oh what finagle is built
with is a filter and filter can
basically be thought of as something
that transforms a service common filter
might be something like authentication
so instead of taking an HTTP request and
returning out an HTTP response maybe
instead you transform that HTTP request
to an authenticated HTTP request right
just like service it also is a future so
that'll be non-blocking it's a great way
to be able to decompose services into
phases for example let's say you have
something like you want to do rate
limiting maybe you want to do rate
limiting for your internal servers maybe
you want to do it for your external
servers you can implement the rate limit
as a filter and then whatever sort of
RPC
you have client or server side you can
just say oh great
stick this rate filter on and boom you
don't have to rewrite the rate filter as
the memcache version of it and the
thrift version of it and the Cassandra
version of it etc
okay so that's part one of basically
finagle as the asynchronous platform it
makes it very very simple to write a
futures based non-blocking code the
second part and for me this is almost
more exciting than the the first part
it's a common framework to build things
on there's a bunch of things that I
don't want to have to write over and
over again every time I create a new RPC
call
so there's connection pooling load
balancing failure detection failover
sugar EDA tracing service discovery
statistics ssl bindings etc what
typically I would have to do without a
finagle is for every single protocol
they will usually have their specific
way of handling these things right there
is how do you do service discovery with
memcache well it works like this how do
you do service discovery when it comes
to finding your thrift services kind of
works like that so in each one of these
environments you have to have a
different way of doing it and it means
you have people that are more fluent
with hey I'm just really good with
configuring memcache or I'm really good
with configuring this that or the other
it's just nicer to have it done one way
and the code can be shared so these are
client based features it turns out that
much of the pain with RPC applications
is not server side it's really client
side what do you want the client to do
if things go wrong do you want it to
retried you wanted to reconnect how long
should it wait things like that that
said on the server side finagle still
has a set of features and offers things
like back pressure server registration
via zookeeper distributed tracing the
native open ssl bindings you know and
all these things just like the futures
code was really easy to access finagle
is made it really easy to access these
features on the back end what I'll do is
I'll go through a few examples of those
the first is let's go through just
simple round-robin Inc so now I have red
is configured and it's the same code as
before but now I have a comma separated
list of hosts this is just a very simple
case of try one of these guys and
wherever you get the answer great
but let's say I want something a little
more sophisticated you know maybe I
don't know exactly what my hosts are my
hosts might be configured in zookeeper
or some external service so finagle has
this concept of a cluster and so here's
now the same Redis configuration I'm not
going to bother with the configuring of
the zookeeper cluster there's kind of
there's a bunch of work involved in that
but all you're telling finagle is
ultimately just go find your information
from this cluster you know and the
implementation the cluster will figure
will figure it out so if you want to
write different strategies in terms of
how you should load balance how you
should get connections should it be
based on the most number of connections
response times network topology etc
there's a common way to do that
regardless of protocol what I did here
is I just sort of highlighted that just
changing it from a simple round-robin to
this you can see is only just a few
lines of code
my granted this is a bunch of setup to
get the zookeeper up and going so I kind
of ignored that part but to set up the
cluster itself is not too much let's
take a look at services like Zipkin so
Zipkin is something that as another
Twitter open source project to do
distributed tracing now this one is a
pain to set up the actual Zipkin service
but to integrate it in with with finagle
is actually pretty simple and so finagle
one of the options you have is a tracer
Factory and by default you get the null
tracer factory which does nothing but in
this case I just want to trace to Zipkin
so in it goes and if you get Zipkin
setup you get this really nice-looking
you know kind of cool tree view of all
the different services you've called how
long they've taken you know the actual
operation time so I could see well here
you know there's something going on here
and hopefully this is supposed to take
this long you know but as your
environment becomes more distributing
you have more services it's just hard
to figure out where the heck did the
problem go so this is kind of becoming a
theme it's really just add the add the
magic line and the service is available
so for ostrich I'm not going to bother
again to show you the configuration of
ostrich itself just the way it plugs in
to finagle so if you want to do the
gauges metrics collection all the things
that the the tumbler presenter mentioned
you get it out of the box if you just
say here stick it in ostrich and again
cross protocols which is really really
nice here's a - I don't believe that
this is not ostriches normal way to
render the data but this is just like a
rendering that Twitter uses of the data
comes out of ostrich yeah they're just a
key plot of data and it's nice you don't
have to do much to get this
so in summary the way I think about this
is whenever we're writing application
code there's a bunch of pieces that
we're writing we're writing business
logic the operational code the drivers
and we're writing it you know along many
different services when the team's
writing the business code logic I love
it
you know that isn't that is time that is
moving the business forward it's great
when we're using third-party drivers I
love that too because it's code that we
didn't have to write we picked up open
source from some other party when we
have to write operational code
separately for each one of these
services this is when I go nuts because
we're doing error detection logging
service discovery failover etc and we're
repeating the same things over and over
and over again what I prefer is just all
of these things to be compacted together
we have the team learn one style we have
that sort of unit that that's similarity
so people can just learn this and not
have to worry about the rest of the
details they can focus and spend their
time up here and then we can have a
smaller team work on this so I'd like to
blaze through way faster and I thought
questions we had a collection of Sajak's
weapon services I'm sorry we had a
collision Sajak's web services like
jersey oh okay we could just bolt this
right on top of that right there's no so
what like a lot of legacy weird legacy
stuff if you have a if you have a jack
server so typically that will run within
a servlet container and it has its own
little stack for doing things so one of
the chat one of the challenges with
finagle is you really get the most
benefit when you just make your stack
finagle and you basically say everything
from everything from business logic on
down is just finagle so if you were to
use Jack's then you would have to
replace some of those Jack's things with
finagle I mean you can get some light
benefit so if you want to use finagles
thread pooling you know you'd be able to
use the future component of that I don't
know if that that's as valuable as being
able to use the holes and angle stack
if you wanted to let's say use the
distributed logging and the metrics you
could use ostrich but finagle wouldn't
help you very much there because it
wouldn't be pre integrated finagle works
best if you could just take the thing
out of the box say okay great here's the
stack we'll use it as is yes so notice
the service signature of the apply
method that basically the request in the
response were specific to the protocol
type yes
wonder if you could comment on that so
every protocol has its own request
response type whether you're talking
about a synchronous HTTP or if you're
talking about comment each one of those
will have its own distinctive ones which
is so for example I included the the
Redis example so Redis instead of HTTP
requests HTTP response has command and
reply and that's part of the codec so
the codec is really what determines like
how to interpret the protocol does that
answer your question yes so the part
that you're gonna reuse between a like a
Redis interface to your business logic
or an HTTP interface that's yet another
layer down I guess well so here are some
examples of things you might reuse let's
say you had let's say you had service
discovery right you could just write
service discovery once plug it in and it
doesn't matter if it's Redis or HTTP
let's say for example you have a let's
say you want to be able to throttle
usage of some sort so in that case you
could create a filter and you would
write the filter using a generic type so
that regardless you know you would just
specify that this is for command and
reply but the filter itself doesn't care
what the underlying protocol is it just
knows that every time a request comes in
I'm counting you might do that with for
example authorization so let's say that
there's an authorization scheme and once
the person's entered your web
application you have some sort of
context where you manage the the fact
that they're authorized and so now
you're firing off these request remote
services regardless of whether or not
they're thrift or protobuf Mike you
could take advantage of the fact that
you already know about the authorization
so there's a bunch of things you can do
that are common among things so another
might be that you have a retry system
where if it fails you try again but you
don't try again at the same interval you
might have a back off strategy
you know where it's half a second than
two seconds and so you could do that
with memcache you know Redis protobuf
etc you only have to write it once so
the example we're in these servers you
put it in like a series of server
addresses or maybe you discover a
variety that's from zookeeper you set by
default that does round-robin I mean can
you configure to say hash on key amongst
that collection of servers or something
like that
sure um let me get to that okay so here
is the example where this is the default
round robin example and the reason it's
used it's the most common one so you
could think of omus this as syntactic
sugar it's not really but I mean you can
think of it that way if you want to do
something like what you're suggesting
there's a bunch of cluster
implementations already built out of the
box you would just pick the cluster
implementation that most closely
reflects what you want to do if it's
based on you know a number of concurrent
connections or whatever so the cluster
would be the way to do that I guess like
the hash on key seems I could require
understanding the protocol level to say
somewhat oh I see you're saying I see
were saying so like if you want to do
say a consistent hashing scheme with
that think about that can I get back to
you sure I guess I get the impression is
this more for just the general
reliability and not so much I haven't
actually written a custom cluster
implementation my understanding though
is you actually get access to the
request object so I'm willing to my gut
says like I've called 7525 that you
would still use the cholesterin
implementation and they would have
access to the actual request so you
could peek inside it to make that
decision but don't don't hold me that
can you can you hide the marshaling and
marshaling of response behind finagle to
or you have to do it on your own outside
finagle can you do the marshaling an
unmarked swoop let's say you're getting
JSON back and I want to convert to port
or you know like a object gonna hide
that behind flagel so I don't need to
know if it's coming JSON or whatever and
I just get an object
okay so there's a couple ways to do that
the first is you could write your own
codec right if it's a very specific
thing so you take the HTTP codec and
kind of make your own specific
derivative of it a second is you could
create a filter that takes a generic
HTTP request and converts it into your
you know your JSON based one I was
actually playing around creating a
library that was finagle based to access
all the AWS services and then what I
found was easy is just also do it at the
at the application layer is just take
the straight HTTP and do a simple
transform on it so there's three ways
you can go the codec the filter and at
the application level I think codecs are
for me they're a little more challenging
so I would probably go the filter or the
application level but it's whatever your
couple with given sip 14 and that Scala
actors future I got a special feature
play frameworks future have already gone
when will Twitter util future follow
that's a good question so I know Marius
put out the the 210 a version of finagle
that compiles with 210 but it doesn't
actually extend from the 210 tree I
don't know when they're gonna do that
from my point of view I'm not as worried
about that because for us it was easier
to go with an entire stack when we have
interoperability I worry about the
problems like the Twitter or the tumblr
person mentioned that for the time being
we don't have cases where we combine the
two we just have the one and so I guess
you could write a transformation but I
the answer is I don't know I don't know
when it would be nice if for example
from play you could use vinegar
components and bars you could just
complete the request with the female
future right absolutely I don't know
just quick question of introducing team
members to futures I really like your
turret but I've found that awk when you
first introduce them into futures be
really frustrated that none of their
debugging tools work anymore or not the
way they're used to I was wondering how
you get people through that and past
well the chance of we Vosges found with
with using them is normally used to
having the full stack trace and you can
see all the things that are going but
all of a sudden once you start using
code asynchronously you find your stack
traces are very very short and smooth
it's difficult to figure out where
you've come I'd like to say we have a
better solution but really the debugger
becomes a much better friend of yours
especially if the service is localized
and then past the debugger logging
becomes a really good friend and so I'd
love to say that we have a more
sophisticated answer but you know that's
premature nuke refining a support of
fertilization of futures or you know
changing their sequence if they live
long for example if they live long like
in the queue so there are a lot of key
feature of futures submitted but some of
them are not processed for long time I
mean we could prioritize them to process
right away oh I see prioritize a future
once it's already in the list at this
point there's only really limited
functionality for that so one is you can
say give me the first future that
responds and then it's up to you to do
whatever you want with the rest futures
and if you're worried about the future
being very long-running then can specify
a timer or time-out on the future so you
can as you construct it you basically
say this needs to respond in five
minutes in thirty seconds or whatever
time period you want or else timeout
I don't know if there's anything more
more detailed than that so I think the
question was on integrating dapper
right so I'd love for someone to go do
that and that's I know that Zipkin comes
pre-integrated which has a lot of the
concepts from dapper but I haven't done
something I've dug into myself and
looked through the code of last question
is retry part of the stack or is you
have to do that you know user code it's
in both places depending on what retry
means for you so the simplest way to
implement retry on the client is there's
in fact a line here that you could add
that says if you just add retries and
you just pick a number so that's at the
basic sort of connectivity level if the
connection fails it will do the retry
but if you need something that's a
little more application-aware then
you're gonna have to write that just
some more related oh what if you want to
have support for queuing behavior I say
some something similar to a message
queue would that be part of the support
oh that's totally a user responsibility
for queueing behavior so whenever you
throw things into a future it will
basically get queued up to do but I
wouldn't treat it like a real queue it's
not this small because I heard something
from the previous talk where you feel
bring you and I say you server-side on
the client side will be able to queue up
certain scenes to up to an hour
I don't know if that kind of behavior we
can get out of this oh that's actually I
think a separate tool use mentioning so
if you look at a zip key in the
distributed tracing tool it uses a open
source project called scribe and wet
scribe does is it's just a way to
collect data and if the end point that
scribe is ready to is down it'll it'll
just buffer things and when the target
is up it will so that's that's pretty
much another project outside of finagle
Thanks
all right well thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>