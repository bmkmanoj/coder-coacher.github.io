<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Evolution of Reddit.com's Architecture | Coder Coacher - Coaching Coders</title><meta content="The Evolution of Reddit.com's Architecture - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Evolution of Reddit.com's Architecture</b></h2><h5 class="post__date">2018-02-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nUcO7n4hek4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">great so I'll get started if you've used
reddit before you've probably seen this
a few times I'm I'm hoping that you'll
see it less and less but you've
definitely seen it and this talk will
hopefully hopefully help you understand
why if you've not used reddit then how
about a quick explanation right it's the
front page of the internet it's a
community hub and it's a place for
people to talk about everything that
they are interested in but more
importantly for this topic reddit is a
really big website there we're currently
the fourth largest in the US according
to Alexa and serve 320 million users
every month with doing all sorts of
stuff like posting a million times a day
and casting 75 million votes it kind of
adds up so let's dig into what the site
looks like this is kind of a very
high-level overview of the architecture
of Reddit and it's focused only on the
parts of the site that are involved with
the main core experience of the site so
I'm leaving out some really interesting
stuff like all of our data analysis and
the ads stack all that kind of stuff but
this is the kind of core of the reddit
experience the other thing to know about
this diagram is that it's very much a
work in progress I made a diagram like
this a year ago and it looks nothing
like this this also tells you a whole
lot about our engineering organization
as much as it tells you about the tech
that we use so that's that's actually
really interesting here in the middle
there that giant blob is our - that is
the original monolithic application that
is read it and has been reddit since
around 2008 that's a big Python blog and
we'll talk about that in a bit more
detail the the front-end engineers at
reddit got kind of tired with the pretty
outdated stuff that we have in our - so
they've been building out these modern
front-end applications those are all in
node and they share code between the
server and the clients they all act as
an API client them
selves so they'll talk to api's that are
provided by our API gateway or r2 itself
and act just like your mobile phone or
whatever other API clients out there
we're also starting to split up r2 into
various back-end services and these are
all highlighted here the the core thing
here is that they are kind of the focus
of individual teams so you can imagine
that there's an API team there's a
listing team
there's a thing team I'll explain a
little bit more what those mean in a
later so they are written in Python
which has helped us splitting stuff out
of the existing Python monolith and they
are all built on a common library that
allows us to not reinvent the wheel
every time that we do this it also comes
with some monitoring and tracing that
kind of stuff built in well on the back
end we use thrift which gives us nice
strong schemas and we allow also for
HTTP on the front end for like the API
gateway so that we can still talk to
them from the outside finally we have
the CDN in the front that's fastly and
if you saw the talk earlier they do some
pretty cool stuff one of the things we
use it for is being able to do a lot of
decision logic outside at the edge and
figure out which stack we're gonna send
that request to based on the domain
that's coming in the path on the site
any of the cookies that the user have
including including perhaps experiment
bucketing so that's how we can have all
of these multiple stacks that we're
starting to split out and still have one
reddit com so since our two is that big
blob and is really complicated and old
let's dig in a little bit more into its
details the the giant monolith here is a
very complicated beast that has its own
weird diagram we run the same code on
every one of the servers here there it's
monolithic each server might run
different parts of that code but the
same stuff is deployed everywhere the
load balancers in the front we use H a
proxy
the point of that is to take in the
request that the user has and split it
up into the various pools of application
servers we do that to isolate different
kinds of request paths so that say a
comments page is going slow today
because of something going on it doesn't
affect the front page for other people
that's been very useful for us for
gating these kinds of weird issues that
happen we also do a lot of expensive
operations when a user does things like
vote or submit a link etcetera and we
defer that to an asynchronous job queue
via rabbitmq and so we put the message
in the queue and processors handle it
later usually pretty quickly
these memcache and Postgres section we
we have a core data model which I'll
talk a bit about called thing and that
data model is what you would consider
most of the guts of reddit accounts
links subreddits comments so all of that
stuff is stored in this data model
called thing which is based in Postgres
with memcache in front of it and finally
we use Cassandra very heavily it has
been in the stack for about seven years
now and has been used for a lot of the
new features ever since it came on board
and has been very nice for its ability
to stay up with one node going down that
kind of thing cool so that was a bit
about the structure of the site itself
let's talk about how some of the parts
of the site work starting with listings
so a listing is kind of the foundation
of reddit it's a list an ordered list of
links you could naively think of this as
just selecting links out of the database
with the sort these you'll see as the
front page you'll see it in subreddits
etc the way that we do it is not
actually by running this select out of
the database instead initially what
would happen is the Select would happen
and then it would be cached as a list of
ID's in memcache
that way you can that's that list of
ID's very easily and then you can just
look up the links by primary key and
that's very easy as well so that was a
nice system and worked great those those
things those listings needed to be
invalidated whenever you change
something in them which happens when you
submit something but most frequently it
happens when you vote on something and
so the vote queues were are something
that really update those listings very
frequently we also have to do some other
stuff in those vote processors such as
anti cheat processing so it turns out
that running that select query even
occasionally when you invalidate it is
still kind of expensive so when you're
doing something like voting you know you
have all the information you need to be
able to go and update that cached
listing you don't really need to rerun
the query so what we do instead is we
store not just the ID but the ID paired
with all of the sort information related
to that thing and then when we do
something like process a vote we fetch
down the current cached listing we
modify it in this example you'll see
that we vote up on link 125 which moves
it up in that list and changes the the
score that it has in that list and then
we'll write it back that is kind of a an
interesting read mutate right operation
which has the potential for race
conditions so we lock around that and
you'll notice that once we're doing that
we're never actually running these
queries anymore it's not really a cache
anymore it's actually its own like
first-class thing that we're storing
it's persisted index really ad
normalized index so at that point they
started being stored originally in other
things but nowadays in Cassandra so I'm
gonna talk a little bit about something
that went wrong I mentioned that
sometimes the the queues usually process
pretty quickly well not always back in
around middle of 2012 we started seeing
that the boat
she would start getting really backed up
in the middle of the day particularly
you know peak traffic when everybody's
around this would delay the processing
of those boats which is visible to users
on the site because a submission would
not get its core properly it would be
sitting on the front page but you know
it's scores going up very slowly and it
really should be much higher than it is
and then it would finally get it many
hours later when that queue processed
well one easy thing is why don't we just
add more skill more processors right
that actually made it worse so we had to
dig in we didn't really have great
observability at the time we we couldn't
figure out what was going on we could
see that the whole processing time for a
vote was longer than before about beyond
that who knows so we started adding a
bunch of timers and once we narrowed it
down we found out that it was there was
LOX I mentioned that we're causing the
problems the very popular sub reddits on
the site are getting a lot of votes it
makes sense they're popular right so
when you have a bunch of these votes
happening at the same time you're trying
to update the same listing from a bunch
of votes at the same time and they're
all just waiting on the lock so adding
more just added more people waiting on
the lock and didn't actually help at all
so what we did to fix this was we
partitioned the vote queues so this is
really just dead simple we took the
subreddit ID of the link being voted on
and used that and just did like modulo
10 and put it into one of 10 different
queues this just looked like you know
you're voting on a link that's in
subreddit 111 or 111 you go in to vote
q1
you know vote 777 go in to vote q7 and
what that did is we had the same total
number of processors in the end but they
were all divided up in two different
partitions and there were far fewer
vying for the same lock at any given
time this worked really well
smooth sailin forever not really
so late 2012 just a few months of
respite we started seeing the boat queue
slowing down
the lock contention time was okay in the
average and the processing times looked
okay in the average but then we looked
at the p90 nines
the 99th percentile of those timers and
we were seeing that there were some
votes that were going really poorly and
that was interesting so we had to dig in
and just start putting print statements
in to see what is going on when you're
taking over this amount of time it's
pretty dumb but it worked and what we
found was that there was a domain
listing so we have listings on the site
that are for all of the links that are
submitted to a given domain sorry
dangers of a touchscreen and the domain
listing was the point of contention now
so all of these things were partitioned
and not vying for the same lock for
subreddits but they were then in the
same thing vying for a domain listing
this was not great and it was causing a
lot of issues so long story short much
later we comprehensively fix this by
splitting up those queries all together
instead of just processing one vote in
an entire single job processor we now do
a little bit of upfront stuff and then
make a bunch of messages that deal with
different parts of the job and those all
work on the right partitions for
themselves so they're not vying across
partitions interesting stuff here is
that you really need to have timers in
your code
nice granular timers but they also just
give you a cross-section you get a lot
of info from your P 99s and tracing or
some way of getting info out of those P
99 cases is really important as well for
figuring out what's going on in the
weird cases there's also kind of obvious
but locks are really bad news for
throughput if you have to use them then
you should probably be partitioning on
the right thing and so going forward
we've got some new data models that
we're trying out for
during those cash queries in a luckless
way it's it's pretty interesting and
it's been promising so far but we
haven't we haven't committed fully to it
yet and more importantly we're starting
to split out listings altogether
so we've got this listing service and
the goal of it and we have a whole team
working on it is to make relevant
listings for users and the relevant
listings can come from all sorts of
sources which includes the data analysis
pipeline machine learning and these
normal old listings like the rest of the
site so that's that's kind of the future
here where we extract it out into its
own service and r2 doesn't even need to
know about how how it's coming anymore
cool so you've got listings of things
but what about the things themselves the
the as I said earlier thing is in
Postgres
and cash this is the oldest data model
and reddit in r2 and it's a pretty
interesting data model getting some
smiles the thing to know about it is
that it's designed to take away certain
pain points but also make it so that you
couldn't accidentally do something like
expensive joins it's vaguely schema-less
and it's very key value there's one
thing type / noun on the site so like an
account thing a link thing a subreddit
thing and each one of those is
represented by a pair of tables in
Postgres the theme table looks like this
it's a little bit abbreviated but the
idea is that there's one row for each
thing object that exists and there's a
set of fixed columns there which covered
everything that the original day is read
it needed to do the just basic selects
query is to run the site so that's all
the stuff that you would sort and filter
on to make a listing back in the day the
data table however has many rows per
thing object and they each have a key
and a value and this makes up kind of a
bag of properties for that thing this
has been pretty neat for reddit in terms
of the ability to make changes making
new additions to the site without having
to go and alter a table in production
it's very very cool in that way it's
also been a lot of performance issues so
it'sit's interesting thing in Postgres
is done as a these set of tables live in
a single database cluster each primary
in that database cluster handles writes
and then we have a number of read-only
replicas which we replicate to
asynchronously our two connects directly
to the databases and we'll try to prefer
to use the replicas for read operations
so that they scale out better at the
time it would also do this thing where
it looks it determines that if if a
query failed it would guess that the
server is down and try to not use it
again in the future thing also works
with memcache this helps us reduce the
load on those reader
we read replicas the whole thing object
is serialized and it's popped into
memcache our two reads from memcache
first and only hits Postgres on a Miss
and we write directly to memcache from
r2 when making the change rather than
just deleting from memcache and allowing
it to be repopulated on the next read so
a width this broke 2011 we were waking
up a lot with these errors we would wake
up suddenly with an alert saying that
the replication to one of the
secondaries had crashed and this meant
that that database was getting more and
more out-of-date as time went on which
so obviously we need to fix that the
immediate thing to do is you just take
that thing out of replication and out of
usage on the site and you start
rebuilding it and go back to bed but
then we started seeing the next day when
we woke up that some of those cached
listings were referring to items that
didn't exist in Postgres which is a
little terrifying so you'll see this
cache listing here says one two three
for but then the thing table only has
one two and four not a great thing to
see this cause the pages that needed
that to crash because they were looking
for the data and it wasn't there and
they just died so we built out a lot of
tooling at the time to be able to clean
up those listings find any bad data that
shouldn't be there and remove it from
them the this was obviously really
painful
so we were looking into a lot of things
going on there there weren't a whole lot
of us either it was about five people at
the time the issue we found always
started with a primary saturating its
disks so it was running out of AI ops
and something was going slow so what do
we do about that
she's got beefier Hardware it's pretty
good right problem solved
not really so a few months later
everything had been nice and quiet for a
while but we were doing a pretty routine
maintenance and accidentally bumped off
line in the primary and suddenly we see
the replication lag alert firing and
looking at the logs from the application
at the time a light bulb went off so I
mentioned that r2 would try and remove a
dead database from its connection pools
the code for that looks something like
this very pseudocode e we have in
configuration a list of databases and we
consider the first database in the list
to be the primary so then when we're
going to decide which database to use
for a query we take the list of
databases we filter it down to the ones
that are alive we take the first one off
the list as our primary and the
secondary czar the rest then we choose
based on the query type which server to
use and go for it
there's a bug there so what happens when
it thinks the primary is down it would
instead take the primary out of the list
and now we're using a secondary as that
first item in the list and we try
writing to a secondary well we've
have proper permissions setup so that
worked and you could write to the second
area oops
so you wrote to the secondary it created
the thing and we write it to the cached
listing all is good then we take that
secondary out and rebuild it and that
data's gone not good so yeah you use a
use permissions they're really useful
they're not just there to annoy people
that are very helpful if you do
denormalize like the cache listings it's
really important to have tooling for
healing and going forward some changes
we're making new services are using our
service discovery system which is pretty
standard across all our stack to find
databases so they don't have to
implement all this logic in themselves
and that that helps with reducing
complexity and making it a battle-tested
component and also finally we're
starting to move that whole thing model
out into its own service the initial
reason for this was we had new other
services coming online and they wanted
to know about the core data that's in
reddit so we had this service that
starts out by just being able to read
that data and now it's starting to take
over the rights as well and take that
all out of our to a huge upside to this
is that all of that code in our - had a
lot of legacy and a lot of weird twisted
ways that it had been used and so by
pulling it out and doing this exercise
it's now going to be separate and clean
and something that we can completely
rethink as necessary cool so another
major thing I said that reddit is a
place for people to talk about stuff
right well they do that in comment trees
so an important thing to know about
comments on reddit is that they're
threaded this means that you nest
replies so you can see the structure of
a conversation they can also be linked
to deep within that structure
this makes it a bit more complicated to
render these trees it's pretty expensive
to go and say okay there's ten thousand
comments in this thread I need to look
up all ten thousand comments find out
what their parents are etcetera so we
store hey another denormalized listing
the parent relationships of that whole
tree in one place so that we can figure
out ahead of time okay this is the
subset of comments we're going to show
right now and then only look up those
comments this also is kind of expensive
to do so we defer it to offline job
processing one advantage in the comment
tree stuff is that we can batch up
messages and mutate those trees in one
big batch that allows for more efficient
operations on them
an important thing to note is that the
tree structure is sensitive to ordering
so if I insert a comment and its parent
doesn't exist that's kind of weird in a
tree structure right so that that needs
to be washed out for because things
happen and the system had some stuff to
try and heal itself in that situation
where it will recompute the tree or try
to fix up the tree the processing for
that also has the issue that sometimes
you end up with a mega threat on the
site some news event is happening the
Super Bowl is happening whatever people
like to comment you have 50,000 comments
in one thread and that thread is now
going pretty slowly that's affecting the
rest of the site so we developed a thing
that allows us to manually mark a thread
and say this thread gets dedicated
processing it just goes off onto its own
queue called the fast lane well that
caused issues early 2016
there was a major news event happening
pretty sad stuff and a lot of people
were talking about it the thread was
making the the processing of comments
pretty slow on the site
and so we fast lane did and then
everything died basically what happened
immediately then was the fast lane queue
started filling up with messages very
very very very very quickly and rapidly
filled up all of the memory on the
message broker this meant we couldn't
add any new messages anymore which is
not great in the end the only thing we
could do was restart the message broker
and lose all those messages to get back
to steady-state so this meant that all
of the other actions on the site that
are deferred to queues were also messed
up what it turned out to be the cause
was that stuff for dealing with a
missing parent so when we fast lane the
thread the earlier comments in the
thread that had happened before the fast
laning we're still in the other queue
and they were still not yet processed
and when we fast lane we suddenly
skipped the key with a bunch of new
messages so then the site recognized
that this thread was inconsistent and
every page view was putting a new
message on to the queue saying hey
please recompute me I'm broken no not
good
so yeah as I mentioned we restarted
rabbit everything kind of went back to
normal afterwards and we now use queue
code quotas which are very very nice
resource limits allow you to prevent one
thing from hogging all of your resources
so it's really important to use things
like quotas if we had turn turned on
quotas at the time then that fast lane
queue would have started dropping
messages and it would have meant that
that thread might have been more
inconsistent or comments wouldn't show
up there for a little while but the rest
of the site would have kept working cool
all right so this is a little more meta
we have a bunch of servers to power all
of this stuff and we need to scale them
up and down
so this is kind of what traffic to read
it looks like over the course of a week
it's definitely very seasonal so you can
see that it's about half at night at
what it is in the day there's a couple
weird humps there for different time
zones but it's it's pretty consistent
overall and so what we want to do with
the autoscaler is save money off-peak
and also deal with situations where we
need to scale up because something crazy
is going on it does a pretty good job of
that what it does is it watches the
utilization metrics reported by our load
balancers and it automatically increases
or decreases the number of servers that
were requesting from AWS we just offload
the actual logic of terminating or
launching servers to AWS auto-scaling
groups that works out pretty well the
way that the autoscaler knows what's
going on out there is that each host has
a demon on it that registers its
existence into a zookeeper cluster this
is kind of a rudimentary health check
and an up note is that we were also
using this system for memcache it is not
really Auto scaled in that we were ever
going up or down but it was there so
that if a server died it would be
replaced automatically so in mid-2016
something went pretty wrong with the
autoscaler that taught us a lot we were
in the midst of making our final
migration from ec2 classic into V PC had
a huge number of benefits for us in
better networking and security that kind
of stuff and the final component to move
was our zookeeper cluster this was the
zookeeper cluster being used for the
autoscaler
so it was pretty important the plan for
this migration was to launch the new
cluster in the BBC stop the autoscaler
services so that they don't mess with
anything during this migration repoint
the autoscaler agents on each of the
servers
in the fleet to a point at the new
zookeeper cluster that's in VPC then
we'll repoint the autoscaler services
themselves at the new cluster restart
the autoscaler and then act like nothing
ever happened what actually happened was
a little different unfortunately
we launched the new zookeeper cluster
it's working great
we stopped the autoscaler services cool
we start repointing things we got about
a third of the way through that when
suddenly about a third of our servers
get terminated and it took us a moment
to realize why and then we all face
palmed rather heavily so what happened
was puppet was still running on the
autoscaler server and when it did its
half-hourly run it decided to restart
the autoscaler daemons and they were
still pointing at the old cluster and
they they saw all of the servers that
had migrated to the new cluster as being
unhealthy and terminated them very
helpful thanks autoscaler so what could
we do about that well just wait a minute
the autoscaler will bring a bunch of
whole new servers up that was relatively
easy except that the cache servers were
also being done in the same system and
they have state and the new ones that
come up don't have that state so what
this meant was that suddenly all of the
production traffic that was happening
was hammering our Postgres replicas
which couldn't handle that because they
were not used to that many caches being
gone at the same time
it's pretty reasonable so what we
learned from that was the things they do
destructive actions like terminating
servers should have a lot of sanity
checks in them hey am i eating a large
percentage of the servers I probably
shouldn't do that we needed to improve
our process about the migration itself
in just having a peer reviewed checklist
where we made sure that we had extra
layers of defense there would have been
really helpful and the other important
thing is that staple services are pretty
different from stateless services and
you should treat them differently so
using the same autoscaler technology for
that was probably not the best idea the
next-gen autoscaler that we're building
has learned a lot from this stuff it has
a bunch of tooling in it to
automatically recognize that it's you
know affecting a larger number of
servers than it really should and
stopped itself and it does it uses our
service discovery system to determine
health instead of just an agent on the
boxes so we actually get a little more
fine-grain detail like is the actual
service ok not just the host but yeah
it'll it'll refuse to take action on
large numbers of servers at once cool
so in summary observability is key here
people make mistakes use multiple layers
of safeguards like if we'd if we'd
turned off the autoscaler and you know
put an exit print at the exit command at
the top of the script nothing would have
happened if prep it brought it back up
you know little little extra things
where no one failure can cause you
issues and it's really important to make
sure that your system is simple and easy
to understand cool thank you so this is
just the beginning for us we're building
a ton of stuff and hiring and also on
Thursday the entire infrared there are
gonna be doing an AMA in our sis admit
if anybody
like to ask some questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>