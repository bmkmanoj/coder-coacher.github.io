<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Speedup Your Java Apps with Hardware Counters | Coder Coacher - Coaching Coders</title><meta content="Speedup Your Java Apps with Hardware Counters - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Speedup Your Java Apps with Hardware Counters</b></h2><h5 class="post__date">2017-11-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QLgWN-B2szE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody thanks for coming my
name is Ed Koch Shankar I am from Oracle
Java performance team and my everyday
job is to make general faster I am
working on always starting from class
libraries and featuring JIT compiler
everywhere so right now I'd like to
share experience win with a sound
technique which sometimes may be useful
for performance analysis to finding
performance issues to do every single ad
so let's start as an order to employee I
have to show you this slide in relay we
are presenters from Oracle loves this
slide because of it means I could live
for you television and it doesn't matter
it's safely okay so I'd like to start
free from one some example and some
technical details will be explained
later after that example so imagine we
have an application and we realize that
our application is slow a couple bit
what does it mean that the application
performance our application is slow in
reality there is only one matrix for
that is customer satisfied or not and
this metric takes a boolean value it's
very difficult to works with boolean
values that's why we expand this is
doing some measurement and cetera anyway
we examine we have a salmon application
it's slow and we find we found a hot
mess at inside our application for
example this one do you know what
dozens me that do it's a matrix
multiplication straightforward from
algebra please raise your hand who wrote
such kind of code in university in your
college a lot of them okay and right now
please raise your hand who wrote such
kind of code after after education in
your work nobody that's that's that's
fine because of in reality there is a
very serious issue when you show
performance examples on the real code
you show example you show how to solve
the problem it start proliferating
through Network different people it
became urban legend people start repeat
that code but the situation change have
a change shift or Machine change that
solution still don't work but it
continuing continue propagating through
the code of other guys that's why I love
such kind of symmetrical examples what
the problem is that code you know this
method is the hardest method in our
application what's the problem here the
key problem that is cubic complexity the
complexity of that code is three loops
and it's it's not a very very effective
algorithm what usually we do if we be
stuck with some well-known algorithm
with well-known things and we have to
make it faster what usually you do for
that we are going to Google first
okay well known algorithm which preys
faster you sleep like three and 2.81
it's not a big difference but let's
check it in reality let's implement the
fastest trust now green 4matic
multiplication and compare okay I did it
I measure it is different values for my
matrix and here is a time how many
seconds
I need for one multiplication I'm sorry
guys I came to show a chart you see
everybody loves charts but for these
numbers are so different it won't be
visible on a single charge so like for
matrix 8000 by a thousand elements I
need almost an hour to do multiplication
in in the first way but when I did the
stress no didn't I need less than 5
minutes okay
Goddard is our optimization done what do
you think it's classical way we found a
better algorithm and it works and here
is a chart to show speed up which we got
at our speed up with certain times for
notch matrix let's let's try the other
way not going to bingo but trying to use
some some cells okay what I do I collect
some basic have the counters for these
matrix multiplications this is exactly
for these size of matrix and what could
I see here I thought this details should
be explained later I see that I spent
these huge numbers different events for
the whole multiplication and if
normalize it per one iteration I mean
iteration of the most inner inner loop I
need that
most in the loop it's required eight
instruction to be executed because of I
need a lot one element the second email
in multiply up way low finally what is
he here I see that inside one iteration
of enormous cycle in innermost loop I do
for loading data from memory in that
case they load it from from caches and
two of them are cache misses and we know
that cache is a provides fast operation
for us but cache miss may be another
good for performance
ok I know that the problem may be in
cache misses if half of my memory
operations - missed it's a problem I
will start another to collect another
statistic and pinpoint exactly place
where the problem happened the problem
happens definitely here and I didn't
show I don't show this on slide but
particularly both cache misses are
happened from a loading data from B
array do we have some some guys here who
deal with GM internals or something else
could you guess why we have to cache
misses here
okay it is short queens
I give a hint I want to say that we have
for memory loss one of my relatives from
for array a and three memory loss for a
bee and two of them a cache misses I
will explain later but after
presentation but maybe you have time to
think about so ant our cache miss here
why we get here cache misses because of
the most in the loop with the loop on a
k' index and we change that K index so
access story be is not cache friendly
cache cache loves when you access for
subsequent memory locations it's obvious
it's everywhere it and this this cache
friendliness it's how to independent
like Java because each CPU right now has
a cache and every cache is a friendly
for for subsequent access for sequential
access it doesn't require for us to know
the details of hardware we just have to
understand what's happening here what we
could do here how we could fight against
these cache misses yeah exactly
if we change interchange these two loops
J and K in that case we'll got
sequential access for array B and
nothing wrong will be happened with
visiray a because of eater parades
winner a and K in excess and here is the
code here's the difference a little bit
difference which may have influenced
late like here we make a soup of two
vectors into one local variable s and we
hope and usually those did compiler
locate this variable on a register and
only that we we are doing only one write
to memory
final result here we can't do that in
that case we should load a data from
result and store
result on each iteration will check out
influence okay here is results of that
optimization here is old code old
sequence of new ijk as was written
originally and he has a modified AKG
Renan interchange and here is a time he
was multiplied here is trash stress and
stress an algorithm here straight forth
multiplying again and we could see that
the new algorithm is faster than a
Strassen algorithm in spite of the fact
that Strassen algorithm obviously has
better theoretical complexity because of
Vigo doesn't show for us the constant
which hidden by this because and here we
improve that constant and the other
benefits which we get here the Strassen
algorithm doesn't work for small
matrixes it's a partition algorithm and
at some level it should use some kind of
convention and multiplication Street for
multiplication because of it became an
effective and my experience show for Ag
care loop ordering the 128 is the
minimum really when a Strassen algorithm
a gift performance benefits below it
doesn't work
no no effect from that and let's do
later because of stress now very even
doesn't depends on the our inner loops
but it meets some basic multiplication
aside it means that we could make a
structure matrix multiplication over
updated sequence and here's our other
numbers in a second so the final here is
a chart the matrix here for 100 to 18
huge Trustin doesn't work it
doesn't give any performance benefits
but here here is original Strathearn
here's improved our lip sequence and
here is modified stress and finally we
got four watch matrixes we could get in
soon up to 45 times performance boost at
significantly better any question about
that example guys I will continue and it
doesn't relate to the topic of our
presentation but it's nice example from
other things to talk about performance
performance improvements is a non compos
dosing if you have function I function
be doing in such composition you could
from functional point of view you get a
combined result if you have a
performance improvement a performance of
human being you combine that you could
not you can get any kind of result you
can get the performance benefits some a
plus plus B mark so from a and B even it
could be worse and etc right like here
here's a combination of Strathearn and
loop interchange approximately the same
but here to small performance
improvements given results very huge
performance performance which much like
an editable check what will happen later
here is my counters about instructions
and cache misses what I see is green the
green color shows where I could see an
improvement and I see that we have spent
much less cycles for our matrix
multiplication much less instruction
much less data much less cache misses
and more stores to date as as predicted
but anyway even we have storing the data
21 cache we have less store aneesa's so
it's good it's a fine optimization it
doesn't creates problems for us
and there are English not issues a
suspicious thing to which I could see
from that number is this I realize that
if I normalize all these numbers by
amount of iteration that internal
iteration requires only the single
assembly instruction so I need no data
from one away from secondary multiply
them up to the previous value store how
could I do this
into this one instruction the cingulate
we got a bit realization because of we
were fighting against cache misses and
we did sequential access to elements of
memory not even metrics that's the
cleanser accessed elements of memory and
we know that such kind of musicians
friendly vocations but also sequential
access to memory is friendly for
vectorization and if in first case hot
spot can do victories Asian code because
of it should load different elements
from different places but here we load a
bunch mount of elements array by one
instruction multiply it with SSE
operation on x86 okay that's all about
this example and here I'll show general
introduction about the sinks of course
as this technique we show it's a not a
silver ball and I want to show the
example where it may be useful aware not
and again I would like to ask you if you
have a slow application what is your
first first step
okay first step we should get an answer
for the three questions in a direct
order first of all we should understand
what's our problem what creates a
problem for us the second step you
shouldn't wear it where is happens
and only after that we should think
about how to fix it so we should usually
we should start from when entering from
profiling the second step and Kunik it's
a general direction and another thing
which also useful to know from
transmitted disease top-down approach
for job for palliative applications
there is no GLA also usually three
levels for general we have four levels
doesn't matter this is the difference
between these levels and on higher
layers the cost of performance issue
much higher so he shouldn't he should be
dig into my critique attack shells and
think about instruction cache misses and
cetera if you just load in huge amount
of data from some hardware drive
produced in 1995 because of you even
won't see that performance issue so we
should start from the top we did some
presentation about that mean my
colleagues and we recreated this mind
map how to dig into some particular
areas in some case but it's impossible
to fit it onto slides that I'll show you
a link where you could find and read it
ok what we are doing will restart the
system monitoring we start top-down
approach we check what happened in the
system
we understand that we've got a small
amount of system time if we have a lot
of system time it's a topic of another
presentation we check interruption on
other problems we check in to tout we
check we idle time and finally got it
it's only place when we should start to
think about our hardware counters
is a situation when we reached a high
CPU utilization before that is a problem
or nozzle analyst is a you just need
don't need that tools and that technique
and if the high CPU utilization is a
place where it could be useful and the
key question what does 100 CPU
utilization means what no no no no okay
I say it's a classic consequence of that
but what it means if you have one hand
on me or 100% CPU utilization yes
you just have enough something to do you
don't know is it something useful is it
effective no you just have enough task
to do and cook traditional profiling
help in that situation usually no
because of profilers show where the
problem exists but not why of course if
you kill profile you find where you
could you can looked into the problem
way and you can guess about problems
very frequently it works
but sometimes not and that is why I
suggest to use normal order why way how
so what the reason of this 160 percent
utilization the reason of that the
difference we have just a lot of
construction a lot of things to do and
lower our course completely maybe we
have a memory intensive application and
we are sitting in our CPU is just
waiting loading data from memory pulling
data to memory etc but in that case you
you also will be see 100% CPU
utilization there maybe have a some
tricky algorithm which which has a lot
of branches a lot of conditions inside
and you constantly stock with
by applying flashes after branch
misprediction you will see on the top
level you will see 100% utilization and
you you want no details about what
happens at etcetera etcetera a lot of
things inside CPU when executing
instruction here sheepy was told outside
is shown as 100% CP a lot that is why so
definitely we need hardware counters to
understand why our CPU is told stolen or
not okay just uh the card counters is
quite a simple sink its internal in
hardware profiler inside CPU every CPU I
don't know about ability area
unfortunately but every CPU nowadays has
hardly counters the first appearance of
had my counters introduction CPUs it was
a pain tomb the first Pentium they had
to ven's probably I don't remember
exactly but it was so successful that it
started developing evaluated later and
you should care about a lot of counters
so those things this counter just
collect events for us and events is that
area which we should know which events
we want to collect and which went
issuance should take care for our
applications how to understand which
events we need like we could read Intel
documentation it's intentionally
unreadable because of its very awful
stuff I can't explain a lot of events
here I just show you that without
reading documentation or reading
documentation for particular tools when
these hundred events at least explain it
maybe you should do that anyway it's
required unfortunately so which problems
are existence at around a lot of
different events you should understand
how how microstructure is wrong how CPU
down inside because of its
for that era as a total indifference
like even inside to see one line of CPUs
against may be changed because of
hardware vendors are free to do whatever
they want here okay how do workers that
usually uh it's intentionally designed
that counters works in two modes
counting mode just we execute our
application count count events and got
the results of kind of application all
selected or in during the selected
period of time it's a counting port but
what's the most interesting things that
all counters could be executed in a
sampling mode and if you reach Sumter's
halt we raise interrupt and in that case
the tool which collect counters for you
could find the place where as these
little flows this interruption happens
and point you the places way and
happened that is why we could use
profile there are profilers not only so
many West application spending time but
show you which issues I exist in
different place and there's some some
issues working with with counters just
just leave them like like we have a lot
of advances really hundred against for
modern CPUs but the amount of hardware
counters quite small for for for latest
not latest a little with modern CPUs are
approximately four hundred counters
through fixed which can be changed and
four counters which collect different
events it creates a problem for our
experiments because of either we should
execute our application several times to
collect different counters and we need
repeatability
all the reason our situation samples
unfortunately not all of them but some
tools could do multiplexing for you I
thought I could set up for true I need
for execution for thirty seconds and I
won't collect these thirty counters and
the two will change counters
small period sometime during the same
run but in that case you should should
should guarantee that the behavior of
your application is the same for a long
period of time you call it steady state
sometimes it's hard to buy and to work
with instruction and counters but it's a
key it's some national sinks from
hardware and you could realize that if
for examples to show you that you have a
cache miss on multiplication instruction
it's not it should be some load before
that multiplication because of sometimes
cabinet could point exactly
so typical usages of hardware counters
by design it was a to usages first of
all as Hardware validation and
performance analysis because of hardware
vendors loves do performance analysis
application and doing their past but in
reality in several years after that a
lot of other different use disappears in
the world there are a lot of them like
one timed unique jerky did it
who knows about Georgia rocket no no no
no no Mission Control is profiler
I mean it's run time tuning J rocket
corrected have no counter statistic not
the profile which is doing but hotspot
with sorry its setting different things
about code but it collects counters
about application behavior and react to
Mai's code into new conditions
unfortunately it brings some benefits
but not so much benefits of a
conditional profiling during transport
compilations that is why it's not used
in hospital for me for me it was very
interesting to read a lot of articles
about security and defence areas like I
saw a lot of station when guys collect
some
hardware counters like branch
misprediction or cache misses about the
application which doing the encryption
and they significantly read use good for
space for attack that is why you can't
get access to hardware counters directly
you need get secure user privileges in
in any operating system because of its a
problem but i realize that other guys
did some security defenses you have no
counter if you kept a long time and
complication like a so it usually the
plates in the same way you collect how
to the counters a lot of different
hardware counters and making like
snapshot about application behavior and
if these behaviors change it may may
signaling about injection about tools on
this page i saw 3 to switch i'm using in
my performance analysis first of all of
course it's Oracle Solaris to do
performance no wiser don't afraid the
word Solaris it perfectly works on Linux
and it's free it's a perfect torrent
profiler which could profile you in a
traditional way in a time and also
performance analyzer could collect you
different kind of hardener counters and
show you places of code whereas counters
located and the second thing the celebs
to the perform oxidizer perfectly works
with Java like some some different tools
don't don't work with general badly
workers job right powerful two things
there are two which I use in my everyday
life it's called perf Linux guys should
know it's it's some some student be
embedded into a Linux kernel and help
you quickly collect different cosmic
counters they
use the bad name because of by perfect
can't Google anything that is why is
there help page and the documentation
called pair underscore events try to go
by this and of course I have to use our
geometry Bhishma hardness
it's a hardness which we created for
writing micromesh much for Java which we
use in our ordinary life it it doesn't
create benchmarks we just solve routine
routine problems for writing micro
benchmarks and if has some intersection
with jmh image could just ask Perth to
collect different halma counters but it
could normalize their preparations that
sometimes may be useful Oh unit could
you could show you assembly the most
expensive instruction of course there
are other tools like first of all its
tools from vendors its MD code Excel it
seemed to be Kuhn
I heard some small all profile probably
it died already a lot of tools exist I I
don't use it so I I can't say anything
about that I used with him ten years ago
but I'm not using it right now
I heard rumors that it's significantly
changed they did something very great to
show you details about your application
behavior but it's not real you should
pay for info for that not only for CPU
but for system and here is example and
no there is no common understanding
about how to code these zillions for
these tools because of documentation we
have one names in performance we have
the other names Oracle studio
performance knowledge covers the
different names so but it's a key if you
have two or three tools for your daily
life you used that but the key ones here
it's amount of cycles and instruction
okay I have the amount of time and a
little bit later I'll show a couple
another example about what
we have to do with hardware counters we
collected a lot of events what we have
to do how how tallest and what doing
that I'm just wanted to say that our
goal for performance improvements it's a
reduced amount of cycles of course the
customer may be satisfied you should
bring a bottle of liquid into him and
speed up frequency of your CPU but it's
not our way it's not soft web team
ization that our goal is reduced amount
of cycles which you spend for execution
and usually if we collect statistic we
should about application there is such
equation like Oh general amount of
cycles is a path length just amount of
stock instructions which you need for
some operation multiplied by CPI is the
average number of cycles spent for one
instruction some guys use other metrics
IPC but it almost the same but in
different order and what we have to do
our goal to reduce amount of cycles so
we have to reduce these result of that
multiplication our optimization should
reduce path lengths or should reduce CPI
or maybe it's better to reduce both of
them but anyway sometimes I'm reducing
supply but increasing path length after
optimization but it's okay for the
multiplication still still low and what
does it mean if we have path length is a
metric of your algorithmic efficiency
it's just as simple as the number of
instructions which you have to be queued
to do desired task it may be first of
all from karma counters or it's a
assembly instruction but it's correlated
with the amount of Java instruction ask
our instruction erode and CPI is a
number of metrics which show us
solutions and here's a typical values
for modern architecture if I see that
CPI is 4 it's a problem
kappa stalls and we should solve that if
I see that Sofia is 0.4 I won't care
about any kind of stalls I would look
them the path Landsat check if I could
do something with pass lens to reduce
the amount of traction for my task its
goal activity so if if I already got a
low CP I adjust universe reduce
mountains instruction but if I've got a
higher CPI I should find the reason of
stone
it could be memories table rhinestones
and etc there are different areas where
this instance appears but the most most
frequently appeared problem is a memory
bound issues there are different issues
here and all of them have descriptions
and particular counters how correct the
second problems will be a core bound and
we have reverse the finest memory but we
doing some expensive capitation inside
our core front end bounds is at least
the least the least problems in real
applications usually we can do anything
here we just talk with that problem if
you stock with such issues like
instruction cache misses but it's not
our job like in Java 9 we are D we
implemented segmented code cache which
significantly reduced instruction cache
misses for okay that's all what I want
to tell about hardware counters because
of just shows us the guys there is such
things you could do Contessa I just want
to show a couple of examples if I hope
you have time for that
ok I I'll take the ROG standard
well-known for Java performance guys
base work it doesn't matter which its
particular I take I took it and I
collect hardware counter just just
manitto could you see the problem it's
impossible to much numbers
I drop out everything which is bad
we wish irrelevant and try to make such
table from here what do we have we have
CPI 1.85 its probe leads bed but what
doing later how to understand where is
the problem here what what I see of my
first step I have a lot of cash load
here cash load here first level cast
last level cache here maybe case and
memory the logic data from memory could
be a problem
let's check potential issues I know that
the cost of load data from my l1 is 4
cycles the load data from it on that
particular machine not in general but I
did experiments on particular machine I
know that load data from l2 is caused 12
cycles and load data from l3 cost 36
cycles
this is Lord superior Lord so I need
this amount of cycles for loading data
from these amount of data to load a data
from murcia I didn't get a true
statistic here the first execution
it doesn't including list of default
counters I should but I could guess that
if I don't hit into l1 but but if I have
a 1 means but and 3 hits this very those
difference a hit and tell it's a pre
mystic assumption because of some of
these reads could pass through a tube I
don't have misses here but it's ok it's
low bounders what I got result for this
is all beta wards only from criminals
remember I need more cycles than I ever
spent of my problem execution how is
that possible
which kind of parallelism no I have to
return us here nobody's noticed that
things we have the ideal CPI cycle
spoons construction is 0.2 but really
the cycle is a clock tick inside CPU
nothing in hardware can be done less
than in one clock it's it's a physical
limitation of hardware how we can read
the sum less than one cycles per one
instruction execution so it because our
CPU has superscalar architecture and
execute up a lot of instruction on fly
and it's an average male so here is the
same station these loads from caches as
they have a lot of them but they enter
leaved they intersect that's why in
reality we don't know how much time it's
required he could make an assumption
that it's memory intensive application
that maybe it's problem but without
further analysis to try and locate exact
places we can understand where the issue
is the superscalar architecture ok the
problem is here I show that example
because of if you have a memory
intensive application you should look
exactly entity will be it's the first
thing which you have to solve with
memory intensive application who knows
what you'll be easel ok
it's a till be it's a small small but
very very fast cash inside CPU which
help us when we need a translation from
virtual address to physical address and
we
exact translation for each memory access
for each memory a lot and each memory
star that is why it should be very very
fast cash and the that is why because of
its have a limitation is fast cash as
small cash and if our value doesn't feel
he hit into that cash it cost us a lot
of performance and happily we have
different events at least an info
hardware which help us to understand
where what the problems it's this is
called again to each show us where which
missus cause the work which we work the
most expensive things and even there is
another event which count for us the
amount of just works we just could get a
statistics how much time to spend on
this page walks I collected it and I
realize that what duration is certain
percent of time for that particular
benchmark on that particular machine in
that particular execution I spent a
certain percent of time doing nothing
execution of my extraction stopped and
waiting until CPU asked operating system
I show me the address of page mapping
how to map virtual address to physical
and as after that I will do the lot of
these byte we could cause this for even
particularly separate byte how to solve
will be problems there are usually two
ways as I told every counter spokes not
only in statistical sampling mode in
encounter mode when we collect all date
and check what happening but you you can
design with and of course you could you
can find the place where it happened you
could change that
data load in general that suggestion to
reduce working set it does mean of
course it's ideal if your application
could less could consume less memory but
just splitting can time
consume not less memory in a wrong
execution but less memory particular
point of time will help you it's the
first way and
second way don't just do nothing can
turn on lodge pages because there are a
lot of articles how to enable latch
pages for Jared how to work with that is
that options I did it for my benchmark I
did mass and I didn't change the code I
even doesn't don't I don't have the code
for that benchmark I just changed the
comment line option and I got 20%
performance boost and a little bit
showing the difference who wish a better
result I collect a statistic for for a
fixed period of time that as well as the
amount of cycle is the same CPI is
better as instruction we could execute
us better even logic date is better but
significantly less she will be misses
and time spent until B and this says the
same values but normalized per
transaction like we have the same amount
instruction here but in that case
different amount of cases and to show
what the difference in that case of how
it cost
okay do you have any question about this
yeah because of its default rates
possible huge pages are default on
Solaris
but on Linux Windows you need to do some
super user settings for your operating
system that is why it can be done by
default the other options are at least
for Linux is options which called
transparent hitch pages in case of
transparent huge pages you even don't
need tabs is command line option because
of in that case it is transparent the
huge pages provided by betting system
for you but there is some community here
transparent huge pages may be
inefficient in some cases I could
explain later wave if you observe skinny
when it comes to transplant huge pages
that they are known to cause some GZ
issues right
transplant hit pages they are known to
cause some GC
she's when collecting cries no it was
the compaction memory and stuff right of
course operating system has GC four
pages and it's a problem not Java not
for Java GC but but for putting system
when we collect small pages into large
pages sometimes when we have large pages
they are not payable then also to hard
disk and if you want to do paging of our
memory we need split large page small so
in some scenarios it create performance
issues sometimes no you have to check
it's elastic and just I don't know we
have only five minutes
do you prefer so the examples of
continuous questions it's last example
okay well as the example who knows what
for sharing keys I realize that was last
couple years there were a lot of
discussion about for sharing that is why
I took that that example what is for
sharing if you have two variables
located somewhere in memory one near
each other with the high probabilities
they will be located inside one cache
line and I have to say that from a
memory point of user is no such thing as
a byte and even four bytes it's a
virtual entity because of bytes for you
is provided by cache when you split but
from memory parent of user is one single
and divided identity is called cache
line and tall access to memory is
performed by cache lines the cutting
these lines provides it's question near
the CPU not from memory and if these two
elements two variables allocated in the
same cache lines and you have two
threads which are doing modification of
these variables from different course
you are in trouble because of cash for
this core and cash for this course they
have implement consistence of that cache
line but come
is atomic entity in that place and it
starts to run from one page to another
cache waiting until data arrives I think
it gets a performance issue a lot of
suggestion how to fix it etcetera that
evil we did for Java 8d sanitation
sanitation contended implemented by Jap
142 it's not public unfortunately it's
for internal use as there is an option
which enable you used as an say because
of really needs such kind of notation to
do proper protection or for sharing for
very sensitive place and concurrent
utilities in Java 2 concurrent package
and etc this annotation checks the
existing Hardware just just move these
these fields inside one clause with the
some gap because of of course there is a
way to do it wisp atom can source some
variables between but it's not grantees
and say you are not eliminated
compilation and just here is a code we
have two fields here we are doing
increment from one Center and we are
doing increment for others right and
here is the results and our results they
depends on where our two sheds executed
so in case of Windows they could execute
it on different course or on the same
core which usually has a hyper threading
on two hyper threading the same cone or
even on different sockets and in that
cases you have different different
numbers it's amount of non a second per
one increment in average so Charlotte
case it's case where we have Sharan
current budget and when we don't have a
false sharing issue for example in case
of different circuits is cost very much
and the table I just show which we which
level which resources shared between
this rest like of course on different
cost we should communicate through l3
cache in case of hyper stress hyper
threading we can
and in case of a different course we
don't have any kind of sheriff - it's a
problem all here is have no counters
could you find a problem yeah yeah guys
not neither do I neither do i I just
want to say that what the full shine
here in case of hyper thread our data in
l1 cache where the full shank as the
problem is that I had called if execute
two threads on a same call under hyper
threading there is no such thing as true
fortunes it is a forceful shank in
different score we have full tool for
sharing choir orchestra and running for
monk asanas but here we got completely
different issue and probably somewhere
in futures this issue will be solved but
as the issue is called memory ordering
cause there is some some piece of card
which called disambiguate track writing
from different hyper stress and some
hardware and try to understand is a
conflict or not if this data tells the
CPU that there is no address conflicts
for writing country in from memory this
application could be executed in
parallel if there is a conflict it stop
wait until some writing so flight first
and only after that start read and here
is the problem for that particular case
we rewrite the different locations and
different addresses but inside the same
cache lines from different hyper stress
and into this greater fact out he can do
anything and but they help us as any
particular event which looked into that
issue we could collect that event with
that name and we could see that on the
same core hyper strategies we have a lot
of machine clearance memory ordering
bias they don't happens in any other
cases and it's how to call catch the
situation for hyper-threading and for
for honestly different course we have to
fight for of all requests for ownership
in the situation when one course and
while one cache of one course and
request to cache father called give me
this cache lines I am going to write
here I want to write here and just we
could realize that these kind of events
there's a difference kind of events
drastically different in case of zero in
Paget and functional case ok I'll skip
this stuff and just summary about high
performance
you shouldn't realize that sometimes you
don't need even know that you can't
fight with that situation you you could
achieve a high performance on other
levels more top layers but if you have
to like people who is writing very
concurrent very fast performance
application and etc you should know all
this here you should know the full stack
but of course it's making problems of
overflow for you and just some kind of
useful reading thank you guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>