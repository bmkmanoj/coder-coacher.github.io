<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Making the Web Rock: The Web Audio API | Coder Coacher - Coaching Coders</title><meta content="Making the Web Rock: The Web Audio API - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Making the Web Rock: The Web Audio API</b></h2><h5 class="post__date">2013-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wZrNI-86zYI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome good afternoon I'm Chris
Wilson I'm a developer advocate on the
Google Chrome team and one of my
personal passions and part of my job of
course is is Web Audio and the Web Audio
API and some related stuff that I'm
going to talk about today so I wanted to
get into this and I actually how many of
you might have seen the video of the
presentation I gave last IO on Web Audio
I see you like two hands three hands
okay well you don't count Peter sorry
great so if you haven't seen that
there's a ton of technical detail in
there a bunch of like this is exactly
how you do some of the things that I'm
going to talk about I'm actually not
gonna go for the most part into quite as
much depth although I am gonna dive into
the code in a couple of places that one
was really broad it took me an hour and
I actually nailed it with like 13
seconds left and I don't want to move
that fast because I want to cover more
in this to this deck but there's a Web
Audio talk out there you can watch it if
you want more detail on a bunch of this
it's actually actually decent which is
my bar for saying my talks are decent is
pretty high so but I wanted to get my
clicker to work I wanted to start off by
talking about why we need another Audio
API we already have this great html5
audio tag it's not even that old like
it's only been out for a few years and
and it kind of works really well like
it's far better than object tags and
it's way way better than using BG sound
for those of you who remember that but
audio kind of wraps everything about the
audio tag I should say wraps everything
about audio up into one step it it wraps
up loading decoding and getting ready to
play you can actually tell it not to
automatically play into this one element
and this one element is kind of the
controller of all the audio behind that
and sometimes this is totally what you
want to do there are two things that
this is really good for one of them is
just playing an audio file whenever you
want to whenever it's it's ready I mean
and the other thing in if you don't want
any precise control and the other
thing is it's really good because it
handles streaming for you so you don't
have to go implement a streaming
adaptive streaming system to get content
as it comes in if you want to do those
things just use the audio element so
none of this is audio element is bad or
anything like that they actually
complement each other pretty well which
I'll talk about a little bit later now
the web audio element sorry the Web
Audio API on the other hand provides
three main things the first one is
really precise timing of a lot of
overlapping sounds so one of the
struggles with the audio element was
developers started trying to build games
using the audio element and it turns out
this is not a really good way to build
audio for a lot of games this may be
really loud so we'll turn it down and I
think the first big game customer using
the Web Audio API was Angry Birds
knew it was gonna be loud sooner or
later and you know for anybody who is
familiar with Angry Birds which of
course is basically everyone at this
point well I must not have played on
this machine before you know it has a
lot of elements aound elements going on
at the same time like you get a lot of
rocks bouncing and stuff like that and
they this is kind of challenging to do
with the audio element because you need
one audio element for each one of these
each one of these aural elements
happening and many systems actually
start topping out they start being very
resource constrained when you have even
just five or six audio elements at the
same time that are trying to play they
start competing for resources so one of
the things that the Web Audio API is
very good at is doing very precise
timing lots of sounds together I'll talk
about scale in a couple of places the
second thing is Web Audio provides a
pipeline for audio you can do routing
you can have sends and returns and
things like that if you want to build
them your system that way you also get
built-in effects and filters that are
managed for you so if you want to do
delay effects if you
want to do filtering to do low-pass
filters or things like that those are
built-in you don't have to go right fft
code or you know find an fft library and
hand apply that to the stream of audio
data in fact the great thing about Web
Audio is all of the processing of that
actually happens on a separate thread
you basically set it all up and then it
it manages the the process of processing
all that audio for you so you don't have
to worry about it on your main UI thread
and if you have some bit of JavaScript
that takes a hundred milliseconds to run
it actually doesn't glitch your audio
because your audio is happening on a
separate thread and then finally the Web
Audio API provides hooks to analyze and
kind of visualize audio data on the fly
I talked about how we have filters built
in we also have an analyzer built in and
this is a very very old demo this is one
of the first Web Audio API demos I think
we did which hopefully it will mmm for
some reason not there we go
and this is basically just kind of a
graphic EQ spectrum analyzer so you can
see what's going on in the the spectrum
space building this if you actually go
this is an example from html5 rocks and
if you go and look at this this the
source code for this it's actually an
amazingly small amount of code there is
more code in the loop that draws all of
these lines than there is in the web
audio part of it which is kind of
amazing for what it's doing so I took
this since there aren't very many of you
who are were around for my IO talk last
year I took this and I immediately
started playing with it and I wonder
just how far I could get this to go so I
built a vocoder and I'm gonna guess not
very many people understand what a
vocoder is it's the robotic voice effect
you frequently hear the hole sticks domo
arigato thing that's a vocoder heavily
used in a lot of electronic music and a
lot of music from the
but it's actually this process of taking
an input file or an input stream and
chopping it up into frequency bands and
then kind of tracking each frequency
band and applying those spectrum
characteristics over time to a different
signal usually a analog synth there's
something rich in spectrum analysis and
I ended up with a demo that looked about
like this so it takes this file of just
me talking our fathers brought forth on
this continent and it maps the frequency
characteristics over time and applies
them onto this really nasty sounding
signal trust me I know it sounds awful
but when you put them together you get a
robotic voice effect right and you can
see the frequency bands bouncing here
because I animated it just so you can
tell what was going on now the really
interesting thing about this to me is I
realized after a while this is actually
very complex this is setting up hundreds
of nodes and connecting them all
together there's a lot of filters this
is a 28 band vocoder which is a pretty
big vocoder in terms of like comparing
to an analog vocoder system and the only
bit of my code that runs while this
process is going is the stuff that that
shows you the visual displays like my
code is not sitting there moving
internal sliders on unsound or anything
in fact we after I did this demo for i/o
last year we added live input support
and two to Chrome's Web Audio
implementation and it took me about 20
minutes to get it running in the vocoder
and now there's this little thing that
says live input and you can click it and
I would love to click it and the only
problem is it feeds back really really
nastily if you're not using headphones
so I'm not actually going to click the
button but all of this stuff is online
already you can go play with it yourself
and test it out
oh forgot a part the other cool thing
about this is because you've just set
all of this up and it's running this Web
Audio code in the background you can
change things about the web audio setup
as it's going so if I tell it to start
playing this I can very easily do things
like change the synthesizer voice in the
background and change the pitch or you
know change the noise levels or things
like that and all that I'm doing is
changing properties in the web audio
setup it's set up this huge graph of of
nodes and plugged them together so
that's great
but you're thinking okay I'm not
actually going to build a vocoder the
uses of a vocoder are pretty limited to
be honest there's some great music from
the 80s that uses it but other than that
not so much but there are some really
important uses of this kind of audio
even simplistic simplistic scenarios of
this API the first and foremost one of
course is gaming like I'm actually kind
of a weird person to be really
interested in audio because on any given
day I can guarantee you that the audio
is turned off on my iPad and my tablet
and my laptop just because I'm in
meetings all the time and I don't want
you know stuff to start playing or
anything but at the same time having
audio feedback to your applications just
as part of the user experience is
actually really powerful it's the same
thing as lighting up a button when
someone hovers over it so they know it's
clickable or lighting it up so that they
know they've actually clicked it you can
also give that kind of feedback through
audio and it's pretty easy to do using
this API the part of course that I'm
really excited by is music applications
and audio processing moving all of this
music industry production stuff into the
web platform and we can now start doing
that because of this API
so if all you want to do is build kind
of feedback like beeps kind of thing
noises that go on when somebody jump
when your character jumps in a game or
something like that it's really pretty
basic you use XML HTTP requests to load
up a song file use Web Audio to decode
this into a flat buffer and we have
decoding API built in and then you
create a source node you pointed it at
that buffer and say start playing and
then that's pretty much it it just
starts playing and it will take care of
cleaning up after itself as well like
any of the transient memory that it's
using if you release that it'll play all
the way to the end and then those just
get garbage collected when they're
available in fact IO last year I was
hard at work you know as two weeks
before i/o I think you know I was hard
at work finishing up my talk and one of
my colleagues who Colt McAnlis who is
working on this game because he was
doing a gaming talk like how to build
games in html5 he was building this
grits game and see if this actually
loads because I'm on Wi-Fi and this is
an open source game it's an example of
how to build games in html5 and he came
to me and was like 2 weeks before and
said hey we kind of need some sound
effects in here and I've got something
partly running but do you have a sound
manager I can just drop in because I
know I'm supposed to use Web Audio now
and I'm like well let's you know let's
take a look let's sit down and talk
about because he'd never looked at Web
Audio at all and in about 20 minutes
he got the idea that actually he didn't
really need the sound manager that he
had before at all because he didn't have
to sit there and manage well I have to
keep six audio elements open and I have
to switch it back and forth between
which sound files are playing and when I
want to play a file I have to see which
ones available like you don't need to do
that because the Web Audio API is
designed to be this very lightweight API
so he can do things like you know he's
got a background track going you can
hover over things and it makes these
neat little sounds and the game itself
has tons of sound in it all of the
server I noticed this
was down so I'm not going to attempt to
play it so if you if this is what you
want to do just load and play sounds you
basically load the sound and get a
buffer and that's what this first
function is it's just dead standard xml
httprequest call all that it does
differently than some calls is it asks
for an array buffer back so I can get it
binary data and then it calls decode
audio data so as if it's an mp3 file or
an OGG file or any other data type
encoding type we support it'll
automatically decode it get it as a
buffer and then whenever you want to you
can call play sound
once you've decoded it you can just play
this buffer by creating a buffer source
pointing it at that buffer and and
connecting it in starting it once you do
that you can actually you'll notice this
actually throws away the reference to
source node like once you've hit play
sound start is not a blocking call it
just falls on through and you've
released the source node so once it's
done playing all of that that object
goes away the connection to the
destination goes away you don't have to
worry about hanging on to this or
managing it at all and the first time I
saw this I thought this was really
wasteful like you're creating it's kind
of like you're creating a whole audio
tag inserting it into the document and
then forgetting about it and the next
time you want to play the same beep
noise you create another one but that's
not really how the Web Audio API works
the buffer is the big chunk of memory
and you can reuse that as many times as
you want including simultaneously it's
also kind of important to understand
that connect call is setting up
connections within a graph and when I
first started playing with the Web Audio
API I didn't I had a tough time
visualizing this so I actually built
this tool that I also demoed it at i/o
last year and I've tweaked it a little
bit since then
but I have some more ideas and what I
want to do to it I just haven't gotten
to it but it's a great way to kind of
explore the API because what it does is
it lets you create a buffer source node
and you know I can just connect this
just hard to do on my trackpad but
come on and once it's connected if I hit
play you know equivalent to hitting
start you can hear the noise beeping but
I can also do things like create one of
those spectrum analyzers and of course I
have code built into this that's
actually visually visually displaying it
that's not part of the API but it is
pretty straightforward and you can see
how how that comes out in the analyzer
and I can also do things like adding
filtering on it like let's change this
to a drum sound come on I had to make
the connection points really big so that
I can actually hit them when I'm up on
stage and let's loop this and
interesting that doesn't seem to be
going through there so the gain in this
filter type shouldn't matter actually
there we go now you can hear the filter
in there so all of this again all I'm
doing is setting up a node creating
these connections between the different
nodes and then twiddling parameters on
the node the all of the processing of
the audio is happening on the separate
thread I don't need to directly even
understand the math that's going on
there and I also don't need to worry
about the visuals of this this display
interfering with that audio and making
it glitch so this playground is kind of
a good way to explore how graphs can
work I keep meaning to have it output
what the Web Audio code looks like -
although I haven't quite gotten to that
part yet
so part of the reason that we do this
glitching or one of the ways that we do
this this preventing glitching is most
of the time you're not actually directly
affecting the audio what you're doing is
are scheduling an event in the future so
you're doing something like instead of
saying I'm gonna play this sound file
right now you say I want to play the
sound file precisely at this point in
the future and the scheduling in Web
Audio is so precise that you can
actually stack cycles of a waveform next
to each other if you want to and it will
actually play them as a continuous wave
that won't like have skips or anything
in the middle so you can do things like
this where this code is set up so that
it will actually do exactly the same
thing as that play sound code did except
it will do it once every second in the
future so the the last call there start
actually says context now plus I context
right now is the audio context current
time and I'm adding multiple seconds to
it this is a loop from zero to nine so
I'm adding zero to nine seconds to that
and basically I call this it schedules
ten copies of this this sound playing in
the future and then they just kind of
happen because they've been scheduled
for me I don't have to to sit there and
try to call it again this is really
critically important because as most of
you who have ever played with set
timeout know you don't actually have
very high precision with set timeout set
timeout can be really really out of
order really not when it's supposed to
get called back if you have things like
garbage collection happening even in the
best of times you're probably not going
to get below three or four milliseconds
of latency on set timeout calls and they
get throttled really badly when when
your tab is not active when it's not on
the front they actually get throttled
down to a second on most browsers so
this way the Web Audio API can just
handle all of the things being scheduled
in the background
now I actually wrote a longer article
about this I'm not going to go deeply
into the into the details of this
there's an article on html5 rocks about
how to do kind of dynamic scheduling
because the problem with scheduling like
this code is once you've called start
ten times to schedule these things in
the future you can't do anything with
them you can't actually cancel them and
get rid of them or you can't say oh wait
I didn't want it to be a second in the
future I wanted it be one point one
seconds in the future you don't have
that control anymore because you've
already scheduled them in that case you
need to do this kind of partnership
between the scheduling and using set
timeout that's pretty much what this
article is is all about but it means you
can build things like a full-on drum
machine and this is actually also a
pretty old demo this one's been around
for a while but it basically just lets
you set up beats if we want we can
completely reset this start it playing
and just and then you can tell I can do
things like change the pitches of things
live because although it's scheduled
those sounds in the future it's not
scheduled like 20 copies of them into
the future it's only scheduled a couple
you know 10 milliseconds 50 milliseconds
ahead of where it needs to be and I can
change things that are past that so I
can change the tempo or whatever and it
takes effect fairly quickly quickly
enough so you don't really notice but
they're very precisely placed other than
that so you don't notice any glitching
in the rhythm or anything if you try to
do a drum machine rhythm using set time
out you'll actually usually hear that
it's not quite right like it's not
regular because our brains actually hear
that reasonably well it's one of the
reasons why drum machines felt so
mechanical for a while is because they
were extremely precise and a human
drummer even a good human
drummer usually doesn't have exact
precision in where they're placing all
those those hits nobody meant to do okay
mmm now the other interesting thing
about scheduling and Web Audio is it's
not actually just scheduling starts and
stops it's not saying hey I want to
start this sound file now and end it now
that's not the only thing you can do you
can schedule any parameter and you can
schedule fades of any parameter
parameter you have this API called audio
program and almost anything you would
think of as an input value to the system
so you know gain volume on a gain node
or delay time or things like this
they're all actually audio params and in
in addition to the the value being
exposed on that audio purim you actually
also have you can set the value at a
particular time in the future you can
tell it to ramp from that value to
another one you can tell it to do
exponential or linear ramps you can tell
it to do you know successive
approximation all kinds of things and
then the system does this for you so if
you want a really smooth crossfade all
you have to do is set this up as as an
audio program fade and forget about it
it will just magically do its business
for you and it does this at the audio
rate precision like it's actually
smoothly doing this inside processing
blocks even in the audio system so like
this example here basically just fades
in a sound and then fades it back out or
fades in a gain node I should say
whatever you've connected to it it will
fade that in and then fade it back out
so it fades it in starts by setting it
to zero and then at two seconds in the
future it's setting it to one so full
gain and then two seconds further in the
future it's setting it back down to zero
so it fades it in over two seconds and
then fades it out so pretty easy to set
up now I talked about effects earlier
but I didn't actually go into the
details of what kinds of effects are in
there and this is the
place where if you want more detail
about what and how these things work
like what convolution does or wave
shaping go look at my i/o talk it has a
bunch more detail in it filtering in
general is is one of the basic musical
tools which is super helpful we also
have things like positioning and panning
and Doppler effects even so you can do
3d positioning if you're building a 3d
shooter game or whatever it's very easy
to set that up as a 3d sound space and
just kind of share it directly with your
your graphics code now obviously
advanced game audio you know 3d games or
things like that you will need these
things for you may not need them in
Angry Birds but there are there is
actually a lot of demand for many of
these features in fact if you look at
this filtering can be used to do
environmental effects so you walk in and
you're like partly behind a wall from
somebody and they sound muffled you can
do that you can get radio effects or
telephone filtering so it sounds like
you're talking over a telephone or
something like that compression is
usually used to keep from keep from
getting distortion on loud noises and
all kinds of things convolution is
reverb basically so you can get sort of
real space simulation now I did say I
didn't say at the very beginning that
the audio tag and web audio complement
each other this is also true because Web
Audio can process the output of an audio
element and this seems a little weird if
you just want to play back a sound file
but you have to keep in mind that audio
elements also do streaming so an audio
element can handle adaptive streaming
for you but then you can pump it through
a processor to do compression or to do
filtering or analysis or any of these
other kinds of tools I keep wanting
actually to go hack into into Google+
Hangouts because we use them on our team
pretty heavily and and I work in a
different office than most of the team
so I'm on video conferences all the time
and you're always in a set of a bunch of
different people and there's the people
in the conference
who are really really quiet and then
there's a one guy with a really really
hot headset mic that's really really
loud and I was in one the other week
where I had to sit there with my hand on
the volume control for an hour-long
teleconference and like pull it down and
then push it back up because I couldn't
hear one guy and you know the other guy
was actually causing people to knock on
the conference room door and say could
you turn it down and also this applies
to web RTC streams as well so well once
we move hangouts to WebRTC I should be
able to do that now musical applications
but kind of the same set of things like
you need filtering compression that sort
of thing you really start wanting audio
input even more delays you start needing
to be able to synthesize audio so doing
oscillators you know being able to draw
sine waves and sawtooth waves and that
sort of thing set up envelopes which I
kind of already showed you how to do
with with audio params scheduling you
probably want to do offline processing
too for things like convolution effects
that are pretty expensive and CPU time
and this is also a place where I made
this point in my eye on as well but I
think that it kind of got lost if you
look at most complex effects like if
you're trying to replicate what a guitar
chorus pedal does this actually is a lot
more complex than just a single filter
or a single delay node it's actually a
delay that's getting changed by a low
frequency oscillator well you can
actually model that inside the Web Audio
API set up a low frequency oscillator
and then anything that's an audio
program you can route a signal into
which is basically what I did in my
other set of examples other set of demos
which you also really need headphones
for because otherwise it feeds back
really really badly so go take a look at
the deck later with a headset on and and
play with it just like regular you know
iPod headphones actually work reasonably
well for these kinds of things because
we now have input as well and this was
the the huge thing for me over the
summer was when we got input turned on
and started to turn it on it's actually
it's mostly done now but it's still
behind a flag so you have to go in and
enable it in flags but now you can do
audio recording so see how well this
picks up my voice but this is the test
and you can see you know what that looks
like you can even grab it and I have
this little encoder that's just encoding
it as a WAV file this is a test not too
bad and you can also do different
effects you can use this to drive games
some people have actually started
building games that use audio input as a
way to control things in fact I think
the Cirque du Soleil chrome experiment
that was done last year late last year
used audio input in one spot and you can
also do things like instrument tuning
and this is a quick and dirty demo that
I wrote and if you're interested in in
pitch detection or tuning don't look at
this code because it's awful but is this
not the way you should do pitch
detection at all it's really really
super naive I just wanted to see if you
could do it but you actually can and
I'll we'll see how bad my whistling is
here so you can tell it actually picks
things up but it gets a little bit
confused whoops and it's on github
and of course you can use this like you
saw with the recording example to do
analysis of audio signals as well and I
do want to say by the way I'm very
careful to use the term input not
microphone because they're not actually
the same thing and people think of this
as oh this is just for recording things
off the microphone that's built into my
laptop well the microphone that's built
into my laptop despite it being a nice
laptop
is kind of crappy I mean I have much
better microphones at home and I also
have instruments that I can directly
plug into my sound cards so I have a
guitar sitting in my office now because
I can plug it straight into the audio
inputs play around with chorusing and
distortion effects and all kinds of
things and I can build my entire amp
modeler if I want in the Web Audio API
and there are actually some really great
open source projects right now to do
exactly that and to sort of build
plug-in effects pedal modules pedal
board j/s is one of the best ones
now I also looked at this late last year
I guess and said okay well you know I've
built all these other things
it's I've always thought it should be
super easy to build a synthesizer using
Web Audio so I went and did this did
that and so for those how many people in
here are like synth fans you know played
around Wow okay awesome that's a lot so
this is kind of based on the voice
architecture of a mogh prodigy synth but
it is not intended to be a replication
of Amogh prodigy so don't you know don't
defy all issues saying why doesn't this
sound drier you know why did you use a
different LFO signal or something but I
built this this thing and it took me
about a week beginning to end to build
this entire synthesizer and it turns out
it took me 85% in that time was building
the UI 10% of the time was tweaking the
end ranges of each one of the parameters
like figuring out oh wait you know
attack should really go from 0.0001
seconds to you know two seconds rather
than one second to 15 or whatever
because that's not musically useful and
only about 5% of the time remaining was
what it took to write the core code that
does voice architecture you know that
sets up a synthesis and and everything
the rest of it was all hooking it up but
you can also play around with all the
parameters because this is this does
actually fully work now there were a few
parts that I found a bunch of bugs in in
our code actually discovering this but
if you want to do if you want to get
your really low maybe not there we go
so all these parameters are directly
controllable in here
and one of the the really cool things
about this was if you actually go back
and look at the source code to this the
voice architecture part is pretty simple
the UI part was really really hard and
one of the things that I wanted to do
and I'm working on getting back to is
taking the same thing and plugging in
different synthesis architecture so
doing a simple playbacks sample playback
synth and things like that so I decided
to take this guy and create a template
so this is basically the same the same
piece of code like worked this off of
github but ripped out all of the UI and
ripped out basically all the the voice
architecture as well and just stuck in a
simple sine wave with with an envelope
on it and if you look at the source code
for this oops not that one if you look
at the source code for this this is it
for creating the voice and starting to
play it so in this when I get a note in
I create a new voice object and the
voice object says okay I have a note
number translate that into a frequency
and then create an oscillator which
defaults to sine wave set its frequency
create a gain node to be my envelope and
then it just sets up an attack an attack
time and a linear ramp for the attack
time and then a exponential decay to to
the sustain level and then call start
and that's it
like I just wrote a synth voice right
there and it's kind of interesting
because I used to be a I started out in
my teens as a really big synthesizer fan
and I always thought it was Kent on
weird because at the same time I was
really getting into computers I just
thought it was kind of weird
that they called the people who design
voices for synthesizers they call them
synth programmers like that's their job
title basically a synth programmer and I
thought this was a little weird because
they're just sitting there twiddling
knobs right they're not actually doing
any programming but it turns out this
blurs that line pretty heavily because
you can add new features too
synthesizer very easily by going in and
tweaking the Web Audio API code if you
wanted to add you know a couple more
oscillators or some other weird you know
make it a four-stage envelope or a 15
stage and envelope or whatever on the
synthesizer ahead all you have to do is
go add a little bit more code into it
okay so now I want to jump to my my new
favorite topic which is how you control
all of this because for those of you who
haven't who didn't see me set up up here
I actually have a little keyboard right
here that I'm using to play most of this
that that synth code actually does
support mouse clicks on the keys it
supports the computer keyboard actually
gets turned into kind of a virtual music
keyboard it actually does touch support
as well if you're on a touch device I'll
talk about that a little bit later but I
really wanted it to be controlled by a
real device in fact this is how I got
interested in the Web Audio API to begin
with was I I had an iPad and with iOS
4.2 Apple added core MIDI support like
if you have the camera connector kit and
he which has a USB plug on it you can
take this this keyboard plug it straight
into here and use it to control the
software synthesizers and by the way
there's like 400 different software
synthesizers or sequencers or things
like that up on the App Store right now
like because I counted interestingly
enough and I think I have most of them
on here actually but I looked at this
and before that it was like I had these
really great software synthesizers but
they were toys because I wasn't really
gonna go play music by tapping on on
this touchscreen I mean there were some
interesting other applications but there
wasn't this I have a synthesizer that
I'm really gonna play well it turns out
now for the last year or more if you go
into your local music store going to
your local guitar center going to Best
Buy even actually they have a great
music section now they sell keyboards
exactly like this one except they have
an iPad slot on the top and this is
totally the what I would suggest to
anyone who wants to get into
synthesizers today just because you can
swap
the software very easily and it is just
a controller and my goal really was to
say okay
now let's take the iPad out of the
equation why isn't that just any web
platform device you have sitting around
now why isn't that my laptop or or my
iPad or another tablet or whatever so
there's a standard called MIDI and I
meant to bring one of the actual
original cables but I didn't it used to
be this weird round 5-pin din connector
runs that are really slow and very odd
rate 30 1.25 kilobits most of the time
though no one uses that anymore for the
last 10 15 years most MIDI devices
certainly not all of them most of them
are actually what's called USB MIDI it's
a class of USB device so the two MIDI
devices I have sitting up here are both
USB like they both have a USB plug but
they're automatically recognized by my
laptop as a USB MIDI device they don't
have a different device driver they
don't have to be specially recognized or
anything like that now unfortunately a
lot of the time when I say MIDI people
think of this reel in the 90s there was
really cheesy background music on
webpages a lot of that was standard MIDI
files this is totally different
when I say MIDI I'm really talking about
this idea of taking musical controllers
and plugging them in and getting that as
input into the web platform and being
able to send messages back out to
synthesizers or lighting systems or all
these other kinds of things so I
actually am like the co-editor of a
standard for this called the web MIDI
API to add MIDI support to the web
platform there is not an implementation
of this natively in any browser chrome
included there is however I wrote a
probably fill a polyfill for a probable
web standard that is based on an NPAPI
plug-in so basically it look you include
this javascript file into your project
and it looks just like web MIDI is
implemented as long as the user has this
this plug-in installed so you have to
get people to install the plug-in but
other than that it works really well and
feels just like the real thing for the
most part if you were at all interested
in getting MIDI added to the web
platform I would encourage you to talk
to me
for one and go vote on the chromium and
Mozilla bugs that we have open for these
because certainly a lot of people think
this is super rare I think it's not
critically important I actually think
it's a great way to get musical control
into the the platform and in fact I'm
gonna end up whoops I'm gonna end up
with how many of you went to the local
html5 user group last year where Peter
lubbers showed a DJ app ok awesome only
like 2 or 3 so nobody has seen even that
before so I wrote that originally
because I was having this conversation
over lunch at Google i/o last year with
a DJ and he asked me this question of
well you know can you set playback rate
to make things go backwards and some
some other interesting questions that I
didn't really have an answer to and so I
started playing with it and I actually
went straight from i/o on vacation with
my family and I spent about a quarter of
my vacation actually sitting with my
laptop in a chair playing with this and
came up with this app so this app
basically is kind of a very simplistic
version of a virtual DJ deck and I'm
actually I'm actually controlling it
with a real DJ deck like I bought this
from I forget where a bah or something
like that and these are all over the
place like most DJ's these days actually
use this kind of this kind of deck
rather than you know a real turntable
and these are just MIDI controllers like
when you spin this it actually is
sending MIDI messages to the computer so
so I wrote the code to pick that up and
wrote the code to do all the you know
crossfading between things if you look
at the top it's kind of hard to see in
this lighting unfortunately but if you
look at the screen it actually shows you
the waveforms on here
so you can see them animate as it goes
through I can do things like set cue
points make it stutter that sort of
thing I can even if I want to oh go away
well that's hang on
needs to not be full screen to drop
another
I can drop another sound file on
I can even sit here and make one of them
stutter or try to line them up of course
change the playback rate
in fact the original point of this whole
thing was to test and you can in fact
get them to go backwards not something
that most DJ cops actually do but anyhow
there are a bunch of other features
built into this thanks
now the other thing is by the way these
things are still are working even though
I'm not I don't have them on the screen
right now so my keyboard actually is
still live right now and I can still be
playing things if you'd look at my tabs
up here I have Web Audio synthesizers
the third tab over and in fact my drum
machines still there - with all my with
all my sounds it has a scheduling issue
right now because of that set timeout
problem I was talking about so I don't
want to start it playing because it
would sound really awful but I can
combine all of those things together
even though I haven't directly combined
the applications and it will still still
end up sounding pretty good alright so I
want to briefly give a status update of
Web Audio Web Audio is in Safari and
Chrome already obviously it's changing
over time a little bit we're still
working on this the specification I
guess that means it will soon be an
opera as well Firefox is actually very
hard at work
Mozilla's hard at work getting it into
Firefox if you look at the nightly
builds you have to enable it but there
there is some very basic stuff already
working and they're super active on the
working group right now which is awesome
and almost more exciting to me is the
story on mobile where with iOS 6.0 we
have web audio support on iOS now so you
can actually do a bunch of this stuff on
an iPad or an iPhone and there are some
caveats processing power is not always
great for things like convolution nodes
and that sort of thing and we are
actually working very hard at getting it
out to Android as well I just got a new
build from the developer yesterday but I
haven't had
to look at it but the last last build
actually was looking pretty good so
opportunities here really are around
gaming obviously gaming's kind of a
given with audio but think about how you
could use audio as feedback in your
application UX if you're a music person
at all I really you know encourage you
to take a look at what you can do with
music in the Web Audio API and with that
I'll take any questions
Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>