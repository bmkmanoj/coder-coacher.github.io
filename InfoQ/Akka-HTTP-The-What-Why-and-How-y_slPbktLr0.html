<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Akka HTTP — The What, Why and How | Coder Coacher - Coaching Coders</title><meta content="Akka HTTP — The What, Why and How - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Akka HTTP — The What, Why and How</b></h2><h5 class="post__date">2015-02-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/y_slPbktLr0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well thank you Seth I'm I'm going to try
without the mic probably going to be a
bit easier for me it's it's great to be
here and I'm really glad to be here
and of course I'm particularly honored
to be opening up this great event for
you guys with this talk on akka HTTP
it's always nice for me to come back to
Boston first because I've studied in
Massachusetts a long time ago and second
because it was actually in Boston at any
Scala 2012 that I did the very first
public talk on spray a lot of stuff has
happened since then of course we've
moved a lot longer a lot forward and it
was quite a ride actually but this talk
is is still on YouTube and you can also
find it find the link from on the sprite
web site and even if you haven't seen it
yet
if you haven't seen it yet then I would
totally recommend you keep it that way
because the talk isn't very good so we
were actually quite surprised about the
amount of uptake and and an interest in
this HTTP library that we've developed
and we weren't really quite ruined
initially where all this is coming from
like where does all the interest come
from why does everyone really always ask
us about you know presenting and and you
know talking to conferences and so on
and but when you think about it you
realize well there's a good reason why
we all need a proper HTTP stack and the
the reason the ultimate driver for that
is is the same as it's probably for a
lot of the talks that we're going to see
in the next two days the world has
changed significantly as you all know we
now live in a multi everything world
multi threaded multi cord multi machined
multi data center which has the effect
that all our applications are now
distributed they are and that this is
the core driver for a lot of the
developments also in the scholar world
there are they really are distributed
even if they are just distributed across
threads and cores and and it makes sense
sometimes to even at that level think
about your application as being a
distributed application but it's not
just across threads on cores it's across
machines and data centers across
organizations and essentially across the
world right if we build an application
today we certainly don't live on an
island anymore we are going to talk to
the world in one shape or form and that
distribution is the core and since it's
a naka talk let's look at what the akka
toolkit promises when you go to a kayo
you'll see the promise that akka put up
and let's to make easier to build
concurrent powerful concurrent and
distributed applications that's what
akka wants to do right and because
distribution is the core of everything
we do now it's important there are two
two big things concurrency and
distribution and when you think about
distribution then this always implies
integration right if I distribute stuff
I need to integrate the stuff that I've
just distributed and since akka is the
toolkit there is distribution for
example within one machine just across
threads and course and akka gives you a
tool to work with that it's not the only
one you cannot you know actors akka
actors are one possible way to deal with
the challenges of programming
concurrently there are many other tools
but akka gives you one if you are
outside of one machine between your own
subsystems that you controlled and akka
gives you things like akka remoting an
aqua cluster to integrate the different
parts of your whole application and of
course when you talk to other external
systems that you don't control that's
where akka HTTP comes in that's where it
makes sense for akka to take on an HTTP
module because integration with the rest
of the world probably is going to happen
through HTTP it is the most successful
integration protocol up to now it's not
perfect it has all kinds of warts and
and there are many good reasons to
critique HTTP but it's still it's the
standard it's what everyone needs to
talk and so our systems need HTTP in
order to be able to integrate with the
rest of the world all right
so when we look at akka HTTP there are
certain things that make it cool right
why why is it cool the design goals that
have flown into this is they come from
the learnings of the last year of the
last year's and of course we want to
achieve maximum throughput with
acceptable latency right that's that's
what you're looking for you want
performance we want to be able to
properly scale across all our cores
crank up the machine really use it
properly and basically give you an
unlimited number of connections
unlimited in the sense that you're
limited by your memory by your or by
your operating system but not by the
library the HTTP stack that you use one
important thing is that we always had in
spray in an HTTP server and client side
support at the same time because
integration with other systems means you
need to consume as well as provide
services right and it's not enough to
just focus on one side then of course
what you expect from a modern library is
open composable api's you want to be
able to easily integrate it into your
stack into whatever you choose it
shouldn't be hard to work with it should
nicely allow you to factor out things
and so on so that's important and
commonly as is on VOC is it's not a
framework style right you you probably
have heard that before and I'll focus
with this layer is integration as I said
it's integration is not necessarily
building web applications there are
certain differences you can use it to
build web applications if you really
want to or if your application needs are
simple but if you know it's that the
core of your application is the
interaction with the browser then things
like play might be better suited alright
so if that are the goals they have
certain consequences for the
implementation and it means if we want
to scale if we want to really get high
performance we need to be fully
asynchronous and non-blocking
right there's no way around around that
otherwise we won't scale and since we're
an akka it needs to be somehow act or
friendly even though akka HTTP is now
moved a bit away from the pure actor
basis it's actually it's
implemented on top of actors but that
might change and you don't need to worry
about that and the current API is
actually not actor api's anymore of
course it needs to be lightweight
modular testable everything that you
expect and it's fully well is that even
a word i I kind of I'm not sure this
this thing you can decouple it from the
underlying TCP interface we're going to
see what that what that means and as you
know it's based on spray i/o so we built
on top of the experience they're reuse
where whatever was possible from the
code base but also wrote a lot of code
completely from scratch so what what is
actually coming in from spray as
heritage so to speak oh there is a
pretty good immutable case class-based
HTTP model that many people have grown
to like and that didn't need a lot of
adaptation we have a very efficient
parsing and rendering logic for HTTP
which is the reason of one core reason
of why spray has been so fast and then
many people might know spray mainly as
its as its surface layer which is the
the routing DSL on the server side but
it's only one part and that part is
going to be moving or is moving into
akka HTTP with small tweaks but no major
changes and there's the test kit which
is basically the same of course we've
learned over the last years a few things
that weren't as great in spray and they
are centered around the common theme
which is mostly a lack of having the
right just having the right abstraction
we didn't have the right abstraction to
tackle a few of those guy a few of those
weaknesses if you've worked with spray
then you might know that handling
chunked messages requests and responses
can be a bit clunky it's it's not as
easy as it should be dealing with large
messages kind of similar it's sometimes
hard and we're going to see a little bit
later what exactly can be the problem
there then the routing DSL has its own
kind of small traps that you can fall
into if you are kind of new to scala
especially and there's there are things
that we can improve and of course if you
come into scala then you sometimes have
trouble with implicit debugging and
these kind of things and missing
features are always a thing as always a
weakness will always have missing
features one common one is WebSockets
support for spray so there's one example
that I want to go into a bit deeper
because we have the ability to work on
the client as well as on the server-side
proxying request is actually quite easy
because you usually work with the same
kind of data models on the on the one
and on the other side so this is a
scenario that has come up quite a few
times over the last years where someone
said wow cool we have this new back-end
written completely in scala we use spray
for it it's it's all great but small
parts of our application they still need
to run we just keep it running on the
legacy machine on the same line it runs
whatever jungle whatever and for certain
types of requests we need to proxy the
requests over to the legacy back-end and
there's just final funnel the response
back to the client so that's what it
usually looks like you have a client
that requests something we proxy the
request over to the legacy back end the
legacy back-end answers proxies the
response back to the client done you can
build that easy now you can see where
the problem comes in if the response is
really really large because you have a
disparen see the disparity in in in data
rate right the lan is fast and the
client might be new zealand
really really slow and you can see where
the problem is it's it's there right you
have a risk of getting squeezed there
because the legacy packet might just
dump the eight gigabytes across a ten
gigabit connection unto you at but you
actually need to spoon-feed it to the
client right so how do you how do you
solve that what we need is is push back
we need to somehow slow down the legacy
back-end in sending stuff to us now tcp
the underlying transport can do that but
your your your stack in spray your HTTP
stack needs to handle that properly and
this is tougher than necessary with
spray is actually it's possible but it's
brittle it's not really robust and it
doesn't work
in a composable way and that that sucks
that's one of the main problems that we
needed to solve so akka HDPE is
something like spray - oh we address the
weaknesses we do across the board
publishing we add java api is for
everything which is something that we've
tried to avoid all the time and spray
now we can't avoid it anymore
everything in akka has a Java API so
akka HTTP needs needs to have one as
well
we simply have simplified the module
structure a bit but there's one core
improvement and that's the that's that
we moved a lot of the core code on to
reactive streams now the act reactive
streams is is basically the basis of
everything underneath akka HTTP in the
new Akai oh I don't really have the time
to go into reactive streams too much
today here in this session but I'm going
to be offering a session on reactor
streams tomorrow I have a lot of
material that have been presenting
several times across Europe in the last
month so if anyone's interested tomorrow
we can have a deep dive in into into
reactive streams so today just one chart
reactive streams all you need to
understand at a very high level in order
to be following the rest of the talk
reactive streams are basically a new
abstraction for asynchronous and non
blocking pipeline processing if you do
pipeline processing in a synchronous way
then you always can resort to blocking
and that kind of solves the back
pressure problem but if your
asynchronous and you don't block then
handling back pressure in a similar way
that have just shown slowing down the
source of the data that you are
digesting is tougher than then you might
initially think and reactive streams is
one attempt to do that properly it's the
core improvement over prior works like
re grx java is that the back pressure is
built in and handled properly across
everything and tomorrow we're going to
look at that tomorrow if it really
in-depth if you are interested director
streams project itself defines not much
more than just an API that different
implementations can implement and the
core goal is to have all the different
implementations of reactive streams be
interoperable so the Twitter can have
their implementation
talk to the one from Netflix talk to the
one from typesafe and so on so that's
that's great there are many many use
cases for reactive streams and as I said
it's a joint effort from Big Shot guys
from important companies so back to akka
HTTP where do we have streams there are
quite a few places where streams are
important in akka HTTP of course you can
think about the requests that come
across one connection as a stream it's a
stream of requests that come comes out
and on the other way what is going out
is a stream of responses going out
across the same connection if you have a
request then the entity so the content
of the request can be chunks a stream of
chunks and if you model that as a proper
stream then it's the same abstraction
again just one nested level in and the
bytes if it's not chunked if it's just a
raw request and even the bytes of this
request or response can be a pure can be
looked at as a pure stream so these are
the four major points where we have
streams in akka HTTP now this picture is
actually this is important this if you
take away one thing from this talk and
it's this picture this is what akka HTTP
gives you let's let's look at what this
means the yellow stuff here is your
application right on the client or on
the server side on both sides and the
stuff in the middle is what our KH TP
gives you now let's look at the client
side first
you basically have some kind of
processing pipeline where you implement
your business logic that does certain
things to whatever data you are dealing
with and what akka HTTP gives you on the
client side is an element that you can
plug into your stream logic that
transforms requests into responses
because that's essentially what HTTP
client does right it's an it's a
function essentially asynchronous
non-blocking properly modeled function
that you stuff in requests and you'll
get out responses that's it that's a
client and on the server side it's
exactly the opposite what you provide as
a server is a function more or less that
turns requests into responses
that's what you build and if you stay
within the stream abstraction then on
the server side you provide a piece of
pipeline that properly connects onto
those sockets and that transforms
requests into responses and on the
client side you need a piece of pipeline
from the library that transform requests
into responses but this was all
streaming fied it's all streams now the
benefit is for example let's look at
what this means suppose here the client
pipeline has some slow stages something
that takes a long time to process
because you're doing expensive CRC
checking of the content of the response
or something so if this is slow then
this stream abstraction makes is
possible that this this back pressure
that is building up so that's the slow
down signal gets carried across this
part of the TCP connection and TCP does
have flow control so we can use that all
the way here it slows down the server in
producing responses now if the server is
built properly with akka HTTP on this
side then this back pressure will
actually carry over to this side go
across here and slow down the client in
sending requests so the client can
because this stage is slow maybe this
stage is being slowed down right and
that's important because you need to
stay if you if you rip up this the the
pipeline somewhere then you're going to
have data loss or you can have out of
memory errors or whatever so if you stay
within this tight stream pipeline then
the back pressure will properly carry
over all the way to wherever the data
are coming from and that's really nice
and that solves the problem that we sing
in the beginning by just automatically
slowing down whoever is signing the data
so if we do a double-click on on this
thing I've shown it as just one element
but it's of course several elements so
this is the stack of of what it is you
have underneath obviously the the
operating system then you have the niño
JDK currently as our basis layer and
then you have two or three layers on top
there's a curio which is a module that
has been in
for a year or two I think that
implements the translation between the
raw and i/o interfaces in a proper actor
eyes message based TCP UDP interface or
a proper streaming fied TCP interface
right so you get stream interfaces for
TCP on top we have akka ITT Pecor which
does just the translation between TCP
and HTTP right that's it that's all it
does and your application can choose to
sit directly on the core layer and not
use the this part which is the higher
level stuff like the routing DSL and
marshaling compression all these kind of
things
annika stream which is also in the new
occur module kind of sits on the side
because it's an it's an it's a help of
technology that is used across all
stacks it provides the stream
infrastructure that is needed here and
here and here and potentially also at
the application level you can use to you
can choose to build your application
with akka stream if you want to you
don't have to but if so then you'll be
using that as well so now I have a few
charts that just do explain what I just
explained as well so that's ok i/o i KH
t peak or what i just said and the
responsibilities of the akka HTTP layer
so there's one before we dive into the
code finally there are a few basic
concepts that we need to understand in
order to be able to understand the the
code is coming and tomorrow we're going
to dig into that deeper if you're
interested so there are a few things
that are important a source in akka
stream is the open end of a pipeline so
you have a stream pipeline that you know
can produce T's if you ask it to produce
T's and that's a source of T and the
sink is just the opposite now you have a
piece of pipeline that you can stuff T's
in if it requests them from you and a
flow is basically just a piece of
pipeline with two open ends so with a
flow you can take a source plug in a
flow and then you have a new source here
right you can put in another flow and
and you have a new source at this end
and in the end you need to somehow
connect it to a sink and once you have
connected to a sink the whole thing is
done and you can push the button and say
go right that's roughly the logic of how
this works and there's one important
thing generally naka stream sources and
sinks and flows are reusable meaning if
I have a piece of flow for example I can
take one source plug it in start it
connected to the sink started and then
take the same flow instance that is that
I still have plug it into another source
and another thing and then crank it
again so so you can materialize a stream
several times that's the proper the
concept of materialization it's
important to understand that concept
before we can understand the code so
materialization is take the template a
source a flow a sink put it together and
then when you push the button that's
materialization you can push it several
times and you get several
materializations incarnations of the
same stream okay so let's look at the
server API what does it look like right
now so it gives you a ket P gives you a
bind method and the bind method takes an
endpoint and a bunch of settings doesn't
matter and it immediately gives you back
a server binding a server binding
instance so there is no future here and
nothing really happens at that point you
just get a binding right so you have a
binding but what can you do with the
binding a binding gives you a source of
connections because in the end you can
look at it at a server as a TCP server
or HTTP server as something that
produces connections right so you and
they are modeled as a source as well so
it's a stream of incoming connection and
when you materialize this source this
source that's when the binding happens
right up until that point the binding
doesn't happen now you can take the same
binding and actually bind several times
materialize several times if you want to
now if you have a concrete port that
won't work the second time you try to
bind the port port that won't work but
if you bind to port 0 for example the
operating system is going to assign the
port the free port and you can actually
bind ten times
hundred times if you want to for the
same server binding so let's let's just
skip across the rest there's a bit of
sugar here but that's the basic idea now
let's look at what is what do I get out
of this source once I've materialized it
what is an incoming connection an
incoming connection is this guy it has
at the time you see it at the time you
have an instance in your hand the
connection has been accepted so you have
a local address and the remote address
right there and what you then do we're
on the server side you can call handle
with with a handler and a flow so what
you supply as we seen is the piece of
pipeline that translates requests into
responses and if you give that to the
incoming connection then the connection
will take that attach it to the socket
and have it run right that's it and you
can take the same flow instance for
another connection and just have it
materialize there right so that's the
that's why it's important that you can
reuse flows because you can supply a
flow once and then it can be
materialized for every connection again
and again and again and there's a bit of
sugar here that we're going to see
tomorrow how that is implemented is just
one line it just helps you to if you
don't want to stay within the flow and
you already have a true function a
synchronous function that translates
requests into responses you can just use
that or if you have an asynchronous
function that produces a future on a
response you can just use that obviously
streams and futures they interoperate
nicely and there is there's all kinds of
things that make it easy for you so
there's one more interesting bit which
is there's a function that allows you to
use the HTTP layer all by itself and
what it does is you'll give it the flow
the server flow that translates request
into responses and what you get back is
another flow that you can use on the
underlying TCP layer so because I oh the
IO IO layer is you essentially exactly
the same api's not from request to
response but from bite string to bite
string because that's what TCP is right
so and if you
if you use that that basically just
translates or encapsulate everything
that the HDP layer does which is parsing
requests and rendering responses all by
itself and then the nice benefit is you
now have a flow that you can plug into
the TCP layer if you want to but you
could also just materialize it in your
tests just for yourself and and you know
just use your own bite strings and just
run the tittie at the HTTP layer
detached from everything else and that's
really nice it's completely decoupled
and it allows you to just work with this
in a much nicer way than it's been
possible with spray or with other
libraries or frameworks okay this is a
very simple example it just uses the
what would look like to have a small
ping-pong server so you have a binding
and then you just call this sugar method
which does this and you can probably all
understand what this does so that's
that's of course it's bit of a bit of
cheating because we we just put all the
hard stuff in there but this is just one
line so and tomorrow we'll see how that
will how that looks like underneath so
on the client side let's move over to
the client side on the client side you
call outgoing connection to an endpoint
and you get back an outgoing connection
instance again no connection is actually
being established nothing happens at
that point you immediately and
synchronously get back an outgoing
connection and only when you take the
flow that is part of this plug it into
your stream setup and then materialize
that then the connection is established
and that can happen several times you
can take take this this out actually the
name we should improve the name is if
you take this instance here and you
materialize it ten times ten new
connections to the to the host will be
opened right because you can just open
several connections and on the client
side as well there is this the ability
to separate it the HTTP layer from
everything else and it's just the other
way around so you give it a transport on
the TCP layer level and what you get is
the upgraded
flow the up
created piece of pipeline that includes
the request rendering and the response
parsing on the client-side again this is
great for testing okay so the HTTP model
is very similar to what we have in spray
so if you use spray who view has used as
you spray actually so far many of you
okay great then this will all be very
familiar that's model is the same there
was no reason to change a lot of the
things because we already got it kind of
right the first time it's high level
abstractions for most things that you
work with it's immutable little logic
there and it just gives you gives users
the core types and Singleton's and
definitions that you need and open for
extension this is the request in akka
HTTP it looks very much the same as in
as in spray so it's properly typed as
you would expect in scala
so the method is not a string its
importance of proper type and the URI is
a proper type and so there's no string
Li typing here the HTTP headers there's
one difference to spray is that now the
entity here is a request entity we're
going to see that in a second what that
means on the response side we have a
status code which is not an integer it's
a proper type and so on the same kind of
headers and here we have a response
entity right the entity of an HTTP
response and this is just to show that
even the URI is another time properly
typed so we have proper types for the
Authority for the path for the query so
so there's there's a lot of deep model
here that gives you safety when working
with your eyes and and just everything
in HTTP which is which is great so the
entity is interesting because that's
where a big change over spray has
happened so let's let's understand this
so there's a super type HTTP entity and
then we have a response entity which you
can use on responses then we have a
request entity that is used for requests
and what is interesting of what trips
people up is request entity it extends
response entity now what does that mean
does it make sense it does make sense
because what it means is
that everything that comes in in a
request you can also use on the response
side so if I see a request I can take
the entity and put it in a response and
it's all fine but it's not the case the
other way around we're going to see why
so anything I can use that I can use for
any type of message is a message entity
and it's it's just an alias for for this
guy and then HTTP what an important
feature that is used very often is a
multi-part messages where several
smaller content pieces are chunked
together into one big request or
response commonly for example for forms
or for file upload that's the type that
you would use and there's entities again
inside of those parts of a multi-part
message and that's body part entity I
mean Universal entity you can use
anywhere right because it's it's just
standard so let's see there are five
different incarnations of these guys the
first one is a strict entity which is a
universal entity you can use it anywhere
it has a content type and it has a byte
string so if I already have the data in
memory if I already have the string
there's no point in creating a
complicated stream abstraction around it
I want to be able to use it just like
that so many times you just use a strict
entity but it's not the most general one
the most general one is this one it has
a content type it has a content length
because you need up need to know a
priori what is the length of the whole
of the whole guy and then you have a
source of byte string so a stream that
you can instantiate also materialized
where you can pull out the data but you
don't need to if you don't want to read
it then it's going to stay where it is
if you don't open up this data source
then the stack will actually not read
the stuff from the socket and the client
or whoever is on the other side will not
be able to send the data because you're
not reading it right that's where the
backpressure comes in but you can
already look at the rest of the request
or at the response for example the
headers or the the URI even before the
entity has been read of the network
that's a big improvement over what we
haven't sprayed so a chunked entity
models the age
the chunked transfer encoding in HTTP it
has a content type no content length
because chunk messages don't have a
content length header and it has a
source of chunks right I'm not showing
the chunk stream part it's basically
just chunks with the exception that the
last chunk in HTTP can have additional
data like a trailer additional headers
and funky stuff that really is used so
and all this is a message entity meaning
that you can use it in requests and
responses but not in body parts because
in body parts doesn't make sense to have
chunked in in there that's clear now
this guy is strange because this guy is
a response entity you can only use it on
responses not on requests and it has a
content type no content length and a
source of byte string what this is is
the exception that we have inherited
from HTTP 1 0 which allowed an implicit
ending of a response by just terminating
the connection so HTTP HTTP 1 0 allows
the server to send the response and then
at the end just terminate the connection
which isn't a great idea because the
connection might have been you know
closed for whatever reasons not
necessarily for the right reason and you
don't have a way as a client to figure
out was this a mistake an error or is
that the proper ending of the response
so that's discouraged in HTTP 1 1 but
it's still around and tons of clients
and servers rely on that so we need to
model it and obviously you can only use
it for responses because closing the
connection at the end of the request
kind of defeats the purpose you can't
read the response then so that's why why
there is a difference between what is
allowed on the request and what is on
the response side and then in the end we
have something similar just for body
parts ok so a few headers just to show
you the model how it works if you know
spray that's exactly the same we we use
the funky backtick notation in spray and
Anaka HTTP quite a lot because it makes
it easy to understand or to think about
what is the type for this header right
if you if I look at the setcookie header
I need the type of the header then I
know what the type is without having to
apply a certain transformation of what
do I do with the - is it camel cased or
is it an underscore or whatever which
use the same string and it makes it
really easy of course this is a problem
for the Java API right how do we well
there are tons of problems with Java API
and of course we have a row handler
become model everything so there are
tons of headers that we don't know about
that you make up yourself and you can
still use the escape hatch of just
having a string string thing this is
basically all that play does right play
doesn't have a strong deep header model
it gives you string string headers so
the benefits over the spray design we
have a deeper and more properly typed
model for requests responses and the
entities inside them in in spray this
was all kind of compressed onto the same
level you had chunks and and which are
actually a part of a request on the same
message level as the requests themselves
which made it really tough to to
separate the two entities can now have
arbitrary size properly we're not
reading them from the network unless you
request them that means we can deal with
arbitrarily long messages in bounded
memory and that's great
and as I said now a nice benefit is that
you can see the request or response as
soon as the header has been read the
header part the rest of the of the
response can still you know be in flight
somewhere or sit in some buffer and you
don't care you can already decide do I
take it
do I read it to a reject the request
because there's no authorization and I
don't have to read the upload that is
you know the 10 gigabyte upload that the
client is trying to push onto us
so that's it's much cleaner much nicer
to be stream based here so there's more
of course I could be talking about the
server side routing DSL it looks very
similar to what you have in sprays or no
big changes there there's testing I
could talk about and client-side API is
I haven't really shown and the Jason
Jason is always a big question yeah so
I'm not going to talk about that there's
the the roadmap for us is since we are
on top of akka stream and akka stream
depends on reactive streams on the
reactor streams API we're kind of this
is our our upstream in terms of
dependencies and this reactor streams
di is almost completely specified
there's an RC out and we're going to c10
very soon which means that we have a
stable specification for what reactor
streams are like and all the
implementations can properly program
against that and then it'll be even more
stable and we are very close on
finishing up the first one zero release
of the new akka modules which is like a
stream a chi-chi TP core naka HTTP you
can see where we're almost done on the
client and server side the bottom layer
is almost done one thing that where
we're not really we haven't really had
time for right now is the high level
client side so there's no high level
client side right now so if you need if
you're on the client side you need to
drop down to the connection level right
now but that's going to change of course
the most requested feature of spray
WebSocket support there like one hundred
and eighty five different plus ones on
that ticket on github so if you want to
you can put your own on to it as well
but we're going to add it we have been
kind of pushing out WebSocket support
because we knew that we were going to
change large parts of the underlying
infrastructure and if we had implemented
that in the spray world we would have to
re-implement everything now on top of
streams that's what we kind of pushed it
out but now since we have the new
infrastructure underneath it's easy to
just well of course it's just going to
ninety percent done you know always the
case now we haven't really started yet
and but it shouldn't take long maybe a
few weeks we'll see and then of course
in the end the goal is that play we'll
move on to this new shiny stack and move
off nettie which they are successfully
using so far and Nettie is a good
project but Nettie is a different world
right it's it's not Scala it uses its
own thread pools it's configured in a
completely different way it's can it's
logging in a completely different way so
it's its own world and there's a benefit
to just unifying everything on the same
kind of clean stack and that's where
play actually wants to end up so the
goal is to build a new kind of driver
for play that sits on top of our HTTP
put it on the side and let you configure
which driver you want to use the old
proven natty one or the new shiny akka
HTTP one on top you shouldn't feel any
difference except that the akka HTTP one
will be a lot slower right now because
it's
has been it hasn't been any optimization
so far we've been focusing on many
different things but not on performance
so far so if you look at our Chi HDPE
right now it's it's dark slow but the
cool thing is that the architecture is
is very nice and there's no reason why
in the end we shouldn't even be faster
than spray because we are now we have
all kinds of optimization options that
we can pull we haven't pulled yet so
don't look at performance in the end we
will have something that's that's really
really really fast so and then obviously
after we've matured this new driver
underneath play for some time then we
can deprecated the Neddie one and then
move into the new shiny world of having
everything from akka HTTP and then of
course in the end we want to have the
nice new model which is much nicer than
the model that we have currently in play
we want to have that on the surface so
that you can actually use all those
types right as long as we as play has to
keep the api's stable we can't surface
this nice nice stuff so what is
happening if you play moves on to akka
HTTP just like that then we'll have a
nice passing layer underneath build a
really shiny proper model and then we'll
call to string on all those types to you
know give them funnel them through the
play API switch is kind of its it pains
me to just think about it but so we want
to change that in the future of course
so these are the resources we have a few
minutes left thank
questions yes timeframe well I always
dodge that question I might as well just
do it now um I can't really talk about
it too much is it
it's always tough because initially we
thought this is not going to take a long
time right
just streams are well understood let's
just go out three months we're done
right now it's a year after we've had
that talk and things creep up especially
in this new reactive streams world
because there is a lot of stuff that we
have not really understood yet there's a
lot of research really happening in a
certain way how do you properly plug
together those streams that have a back
pressure channel and tomorrow we're
going to see exactly where all the
complication comes from so I'm I'm I'm
not going to give you any date the goal
is of course ASAP everyone wants it play
wants it everyone wants HTTP out and
we're working on it as much as we can
the goal is to have one zero out the
akka HT p10 thing currently at the end
of q1 this year so there's not a lot of
time left two months or something so
we're currently kind of digging through
tickets what are we still doing what are
we what what can't we do it stuff yeah
well the good thing is an akai oh we
completely owned the stack all the way
through so if there are any issues and
you're right that Nettie has certainly
had much more time to mature over time
and really address small tickets and
bugs and here and there if we are aware
of any bugs on the i/o layer we'll try
to address them as quickly as possible
because it's the base layer of
everything currently I don't think we
have any known issues there but it
doesn't mean that we don't have them I'm
sure that that he has more kind of
special casing of Windows on a certain
word version where you have a certain
bug there has been a few things there
have been a few things that have crept
up over the last two years that we have
a kayo now but you're of course you're
right we're not as mature yet just
because we didn't have that time but the
goal is not to leave you alone with that
but to give you give us the chance to
react quickly to the bugs that you find
rather than trying to fix something in
nettie and work with them even though
the cooperation with the nettie folks
has been
has been very nice but it's still one
level away of course right so that's
that's the goal yeah well you need that
we have for example if you implement an
HTTP proxy that's basically what it is
you just fold it up and then you have a
client you have a proxy which is taking
in the requests and sends it across
another connection and does the same
thing on the on the other side and then
you need to if the server is slow you
need to slow down the client because you
want to be a transparent proxy in the
middle right so there needs to be a way
to trance transport back back pressure
across your own stack all the way into
another connection and if you have
several proxies which might well be the
case then it needs to go across machines
right and you can do it if you just
connect TCP and all the stacks are
properly kind of closed within the
within the streams world then this will
work and we're going to see there's a
chart on that tomorrow in tomorrow's
session that deals exactly with that
because it's very cool yeah one more
question okay maybe we're out of time
maybe we can take it outside thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>