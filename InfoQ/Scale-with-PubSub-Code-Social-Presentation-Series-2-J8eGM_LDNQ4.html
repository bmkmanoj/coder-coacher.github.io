<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scale with Pub/Sub: Code Social Presentation Series #2 | Coder Coacher - Coaching Coders</title><meta content="Scale with Pub/Sub: Code Social Presentation Series #2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scale with Pub/Sub: Code Social Presentation Series #2</b></h2><h5 class="post__date">2012-02-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J8eGM_LDNQ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for the introduction so for
tonight I thought we can talk about a
pattern called pub/sub who has heard of
pops up before okay good so we don't
have to just drink beer we can actually
we can actually have this talk okay and
so I work for a company called quanta
find and we'll talk a little bit about
you know what we do just in a quick
slide and so you know the idea is
basically not to go into super detail
you know if you you know think hey you
know this is really cool this can help
me solve some problems we'll talk about
an implementation called Kafka done by
linkedin which I think fulfills some of
the you know the goodness of pub/sub
that we're going to introduce you to and
then the end we can have a Q&amp;amp;A session
for my guess 10 10 minutes or so quickly
about quantifying what we do is
essentially we take unstructured data
and so it could be like tweets it could
be amazon product reviews could be
facebook public pages and then we
correlate that with structured data so
could be financials could be product you
know sales data could be ratings and so
like a simple example would be what we
did for a customer who sells iphones
they were wondering like what we can
help them with their iphones a sales
experience and so the what they did is
essentially people were lining up they
went into the store purchased an iphone
and then filled out a little survey and
so what quantity fine did in this simple
example is essentially we found out if
you serve people coffee that the
satisfaction goes up and saw by on
average like it went up by two points
and so in this simple example of what
that tells you it's basically
quantifying finds drivers that impact
you know something that's meaningful for
us so in in this case is in the future
you know if i have served coffee to
people there's a high likelihood that
they will be more happy and while this
simple example is is you know is
everyone probably could have figured out
without quanta find the point is that
we basically do this on a massive scale
so we have hundred millions of Records
and then we figure out which
combinations are good predictors for
future behavior for your KPIs that
you're interested in and so you know
this is a simple example but you can use
for movie opening box office predictions
you can do this for pretty much anything
that you know you have some historical
data to train over and then you know
something you want to predict against so
I thought first you know let's talk
about what is what is so great about
pops up and so in my example I thought
you know let's say we're a little
developer shop and we have this website
and we start with let's say some website
that you know allows people to log in
they do something on it and then they
log out and this thing really takes off
so the ops guy says hey you know what I
should really really really have a
dashboard because otherwise I'm flying
blind I don't really know what I'm doing
here and so we said hey no problem
there's this great splunk think I'll
just they have an API a pumping my logs
into splunk and you know you can see
what's going on and so typically what
happens in overtime is that essentially
you know the the analytics guy says hey
you know what but if you give me the
clickstream I can do lots of cool things
with it I can build our you know our
advertisement guys I can figure out what
the clickstream is like which feature
people like which don't and sort of
overtime that thing spirals out of
control they say you end up with you
know something like this where you have
you know these producers in our case you
know the clicks from people and you have
these consumers so each of them is
basically someone that is interesting
that piece of data and so you know in
our case it's the the dashboard it's the
Hadoop analytics job it's d someone
wants to take the actions and replay
them for like a low test and then of
course there's the other problem hey if
I want to take one of these guys down
like we know Hadoop has no you know I
still a single point of failure they
just fixed that I want to take it down
then if i make point-to-point
connections in my consumers gone I can't
push the data anywhere so there's a
pretty good chance that your main side
will be down to and so it if your system
grows it will become a pain point and so
I guess the
away from tonight hopefully will be that
you'll be able to identify your word
actually makes sense to use this
publish-subscribe technology to avoid
these things so you can take systems
offline you can add new consumer of the
data and you don't have to basically you
know a be interested in who's consuming
it and be you can you can sort of start
you know keep on developing the
functionality as you want with that
without adding these these endpoints and
coming up with this ugly graph that you
end up with and so what is this
publish-subscribe that we've been
talking about so the idea is pretty
simple so on the left-hand side you see
we have producers instead of actually
talking to a single endpoint I push my
information to something that's called a
topic and so a topic could be for
example you know some people like to
name it hierarchical could be dev or
prod dot you know user click stream and
so everything I push into this topic all
these subscribers on the right-hand side
these consumers will get so you see I
pushing data one in data to and I can as
many producers as I want and all the
consumers will see the exact same data
and so the the nice thing about this one
is it's basically super nice if you want
to get your data distributed to many
many endpoints it promotes loose
coupling so the consumer the producer
does not really need to know who's
consuming the data so i can add both
sides at will and most systems also give
you increased robustness so if you're if
you're the system is up and running that
provides you this topic
publish-subscribe a system I can take
the consumers completely offline the
messages will be just queued up when I
bring the consume back online everything
they miss they will basic add replayed
and then there's a another that is I
guess concept that isn't strictly
published subscribe but is sort of
related in most systems offer both and
so if you have you know a situation
where you actually want to work a
distribute workload so let's say you
know I have you know many producers of a
piece of data so it could be like in my
previous example it could be that you
know I need to process those click
stream
so I have have someone pushing the
clickstream into my queue so it could be
all the web servers and then I have many
consumers that need to do something with
it so it could be that you know i'm i'm
basically running and running something
that tallies up you know my
advertisement you know bill and so if i
push that in a queue you see the
consumers in this case are actually not
getting all the same data again but what
they're getting is each consumer is
getting the data exactly once so i
pushing data one in data to and then it
basically is load balanced over those
consumers so each consumer gets
basically you know a a separate piece of
data and so this is really excellent
especially when you don't have threads
you can start just start multiple
processes they all listen they also like
you listen to the same queue and they
will basically get round robin you know
the information for processing
distributed and it has also the nice
property that you will have nicer or
better robustness because I can take the
consumers offline and the queue will
just queue up everything that has not
been processed or and then when the
consumer comes back online it'll be a
just get everything it missed and so in
the past one of the problems that I've
seen was really that like all the open
source implementations they all had one
or the other problem with it so there
were some that you know yes you could
have q's in topics but in the end they
were all kept in memory and so if that
thing would go down you know everything
was gone and then of course you know you
can only queue up as much stuff as you
can keep in memory which you know even
if 50 gigs of ram if you were a larger
system it's it's kind of like a joke a
little bit and so the in a while while
publish-subscribe also always like was
desirable there was just no
implementation would really make it
usable I guess and so in the in the past
that has changed significantly so
LinkedIn contributed a project called
Kafka and while it has a you know
questionable name especially when you
google for it
you'll end up with lots of other stuff
it actually has some properties that are
really really interesting for you know
for writing larger scale applications so
you know like I mentioned so some of the
contenders in the past like you know
there's activemq and RabbitMQ and they
all had either questionable scalability
speed or there were the wheels were
coming off when you put some load on it
and so or they put it in a database and
so things were getting slower the more
messages that you were getting in and it
basically takes care of all these
problems and so you know I'm more than
happy to discuss later i guess the you
know the intrinsic detail of Kafka but
what what it really provides is it has
really robust publish-subscribe and it
has also something called groups which
means essentially you can have if you're
in the same group and you're listening
on the same topic you actually achieve
something like a queue so you would get
everything everyone in the group will
get a piece of data on the topic
delivered once and so you know same
scaling pattern that we talked before
and then also you know if you have
another non JVM language it's they have
you know not the most I guess blind
bindings but you know quite a few like
you know C++ and Ruby and a PHP and so
you know it's decently enough plus the
protocol is easy so I guess in the
future we'll know more people
implementing clients for it and then if
you know it's it's designed in a way
that it's that it has really high
throughput because essentially what it
does is when a message comes in it
appends it to a file and when it sends
messages out it reads sequentially from
that file so there's no fanciness it's
it's fairly similar like Cassandra were
you know depends to a file and then so
if you look at like the the typical you
know development of disk drives like the
the actual seeks are slow so they only
were proving about five percent a year
well a throughput like almost doubled
every year because you add more platter
you increase the density and so things
were getting faster from the sequential
scan perspective but not from the random
jumping around so databases weren't
really getting all that much faster
because they rely on jumping
one ball well this thing could you know
pick up speed essentially because was
banking on the fact that you basically
do mostly sequential scans and then it's
backed by zookeeper which is a a highly
available product for cluster
coordination and and essentially
everything that is important is stored
and zoo keepers of a broker Goes Down
resistible rebalance and keep on you
know keep on ticking away at it so the
you know the situation that you that you
sort of have to face is if everyone
publishes into a pub sub pub sub system
but the pub sub system goes down then of
course you have a bigger problem than
before we're like one endpoint goes down
like you maybe start timing out but you
can probably take care of that your pub
sub system goes down that's basically
your single point of failure you're
basically host so that they actually
have zookeeper and keep things ticking
in that way is is a very good property
and then they have been running that in
in LinkedIn production for quite some
time so you know you're not the first
guy putting that thing under load and
then see the wheels come off and then
look at the bug reports and work with
the developers and not getting the
actual work that you want it to get done
you know pushed forward any questions so
far oh and so my suggestion is you know
if you think back you know you have one
consumer and there's in the one producer
in in the future there might be a chance
that someone is interested in that data
piece do I would suggest that you just
push push it into a queue well you know
initially might seem silly because you
know this is one thing and the other guy
just needs it why should just make a
call and be done with it I think over
time there's a high likelihood that
someone else will need it and then it's
super easy you can just tell me this is
this topic just go and get it yourself
it's pretty easier and then if you you
know Kafka is not a silver bullet so
there are other systems out there they
get a lot more press sometimes but I
suggest if you pick something else
because you might have different
requirements actually try it you know
just take a worst-case use case that you
have and then just just hammer it or or
try some weird functionality edge case
that you that you have
because if the wheels come off it's
really bad for this particular component
so make sure you don't trust the height
but you actually tried and then I really
like Kafka it's one of you so i guess
the obligatory page of course is you
know quantified has as funding and as
kicking kicking ass but we're having
problem finding people like I guess
everyone else so you know if you're
interested I think we're a pretty nice
you know 12 people small team we like
new technology we pivot fast come talk
to me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>