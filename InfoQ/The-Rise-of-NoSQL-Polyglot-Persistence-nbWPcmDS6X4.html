<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Rise of NoSQL &amp; Polyglot Persistence | Coder Coacher - Coaching Coders</title><meta content="The Rise of NoSQL &amp; Polyglot Persistence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Rise of NoSQL &amp; Polyglot Persistence</b></h2><h5 class="post__date">2012-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nbWPcmDS6X4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everybody for showing up the session
right after really good heavy lunch is
always the hardest so I appreciate you
guys making up all the way up here I see
as well some familiar faces from the
other from the earlier talk i gave the
meta programming one thank you for
coming back makes me happy again this is
the rise of no sequel and polyglot
persistence this is a little bit about
me I work for just me I'm a frequent
speaker and all in a lot of conferences
and they started a bunch of users groups
and organizations over the years and
that's my Twitter account at polym ethic
code again the same thing this
presentation is licensed under the
Creative Commons license so feel free to
use it and don't make any money out of
it and just say that it was my work so
the way i would like to i don't like
usually I don't like you have an agenda
because when you have an agenda in a
talk and kind of give people the idea of
it's more of a distraction because
they're always waiting for like the next
thing so i would like to pretty much to
slit the the tall kind of flowing in and
I mean call it organic matter whatever
you would like to call it but the first
thing they would want I wanted to talk
about is pretty much relational
databases to make sure that everybody is
on the same page and the first section
it's pretty much the golden aid of
relational databases as you see in the
title so what a relational data store is
it's been pretty much the technology
that has been dominant or like just the
obvious choice of storing data over the
years because of many reasons we have a
lot of existing material solutions
Oracle my sequel to name a few and it
was just pretty much a wide adoption of
the technology and a lot of familiarity
if Bible developers or advanced business
users themselves and there is an
abundance
abundance of Julian out there so pretty
much almost became the de facto standard
for storing data over the years to the
point that we just pretty much assume
that we're going to have a relational
database we assume that we're either
going to run on my sequel and on Oracle
if we can afford it and what a
relational model is is pretty much this
data and I'm sure a lot of you are
familiar with this that is stored in
these two-dimensional tables or
relations in the form of pretty much
rows and column attributes as a
well-defined enforced schema that means
that the relationship between the data
itself in those tables is well defined
and enforced and there was a lot of
integrity constraints that I mean force
and integrity constraints like the type
of the data or referential integrity
like the relationship between the data
itself and anybody that did any work
with the relational model knows that
normalization is pretty much of a good
practice and while normalization is is
pretty much these small tables well
define relationship between them to
minimize redundancy and to pretty much
avoid modification anomalies so no
redundant data whatsoever adjuster was
considered to be a bad thing and if you
have a table at the table depart a table
of departments and employees when you
deleted department you would assume or
you would would you would like that all
the employees that are linked to that
particular department of deleted as well
because there is that referential
integrity of that propagation that is
supposed to be that that would expect on
would like to happen so this relational
model is also supported by sequel which
is a somewhat standardized query
language that were familiar with it's
very flexible and it supports a lot of
operations that there are across
multiple relations or across multiple
tables such as joints many allows us to
go very nice aggregations with
statements like group by etc so it's a
very nice
language that we enjoyed using over the
years one important feature that the
relational model has is pretty much to
be able to chase transactions to be
transactional and what we call these
acid transactions and as it stands for
autumn a city that means it's all of
nothing I do a lot of I changed a bunch
of data and it's treated as one one one
unit neither happens Oh everything
pretty much gets rolled back to two
before which ties into the second one is
consistency that means that every time
we do a transaction on this database it
takes it from one valid state to the
next one to another value 1 and the
isolation which is pretty much
concurrency is taking care of for us by
the system whenever worried when we use
on our DBMS of a lot of clients querying
the database or changing it whoever it
is we expect that to just pretty much be
taken care of by the LG VMs system and
your ability I once committed it's
forever I mean this is not an in-memory
database when the machine crashes
everything just goes away this is
something the changes are persisted
their own disk so it's really important
to kind of understand the the reasons or
the design trade-offs that the people
who first created these are DBMS systems
pretty much took and this this this
model was pretty much brought forth
because of many assumptions that they
made the first one is that the end user
would directly interact with the
database so so in meaning that there is
this guide it was this data right there
is this guy sitting in front of a
terminal avoiding sequel statements
that's pretty much how it all started
and he can intraday and he can query it
back and so forth and it makes sense
that the audio EMS should manage
concurrency and integrity of that data
right if we've designing a system like
that and this means also that the access
patterns are unknown sequel is very
flexible I mean we're talking about an
infinite combination of queries
data and everything you're just giving
it to a user and giving them this
language to pretty much use and so
sequel became that way flexible query
language close to English and what we
ended up doing is we store data in these
data structures that have no bias
towards any particular pattern of query
we want this model that would support
any pattern of Korean because we just
don't know about it I mean it's up to
the user to do that so the data and the
other assumption is that the database
runs on a single machine the
distribution was not something that we
considered to begin with I mean we just
didn't have enough data then so the only
way to promise the database devons on a
on a single machine was anyways the only
way to promise acid the only way to
promise pretty much that you have data
that you that your data is that your
operations are going to be transactional
so we had some road bumps throughout the
year this has been working very well for
us it has been working great no
complaints but mr. what will happen is
that we started building more complex
applications on top of these relational
databases a lot of the business logic
moved out of the business databases to
the application layout itself we started
writing started hating triggers we
started not writing a lot of sort
procedures anymore we all that code kind
of moved to the application layer as
these new frameworks and new languages
and platforms evolved out of the
database the second thing is that these
applications themselves that we built
outside of the database itself evolved
beyond the procedural paradigm and
became more o P so all the sudden we
have this object relational model that
is that kind of looks at data and
structures it in a way that is
completely different than the database
itself do we have what is called the
object-relational impedance mismatch it
sucked but a lot of people put a lot of
work into these ORM frameworks we have
hibernate we have all of these things
and everything was pretty much may
our life a lot easier you have an
inheritance something that does not or
like a hierarchy something that does not
translate to the relational model and
you just annotate it using jpa to kind
of figure out like an inheritance
strategy of how that data is supposed to
be persisted and queried back from the
database and it was great I mean it's
not really that big of a deal so next
section is about scalability you can
just go to next so we start we became
data holders as the database grow up you
know grow out of pretty much control
more data all started putting a lot of
more data into them we started worrying
a lot more data during the performance
pretty much decrease exponentially so
started spending a lot of money buying
the next big machine with multicores
buying oracle RAC and making this guy
even richer like he needs more money and
they kind of put the problem off for a
while we just buy this new Oracle back
and everything is good now performance
is back to an acceptable level so we got
to the point that we have the most
expensive machine out there we spend
millions of dollars of this one but
we've still not getting really good
performance so we hired this guy that
walks in to optimize our database he
walks in there and then you create
secondary indexes all over the place
certainly made a lot of queries a lot of
joins a lot faster he said you know what
we're gonna actually go and create some
materialized views and for those of you
for complex joints for those of you who
are not familiar with familiarised views
she's pretty much just the result the
result of a query the data set of a
query that is pretty much pre pre
fetched for you by the database that is
stored cached for you by the database
there are different different different
types of view some of them happen during
the run time and some of them pretty
much would happen periodically so
anyways I went off on a tangent on that
for a little bit but they were like a
nightmare to maintain and they
got stale and the ones that I'm not the
ones do are the ones that I'm not real
to real time pretty much became useless
the ones that are real time didn't
really mean anything because performance
still sucks so he said we need to
denormalize we just couldn't join four
tables with all this data and get good
performance so we did it was horrible
you know we ended up with this
redundancy that we did not want to begin
with and we have to pretty much manage
so the guy said you know what we're
going to introduce caching we introduce
caching the data is too stale we have
even more redundancy on top of that we
need to worry about when do we evict the
cash and when it was just a mess anyways
next one so we have another guy I was
supposed to be smarter hey guys is
pretty much tells us that we had the
limit of the one machine that we should
pretty much scale out of scale
horizontally that means we should have a
cluster of machines instead of the one
machine big problem remember the audio
BMS was not designed that way but we're
going to sit down and then try to think
of other creative solutions he tells us
that we have two options the first one
is a master-slave kind of architecture
that assumes that you're actually
reading a lot more than writing which is
the case in a lot of systems and but the
problem with that is your right to the
master and there is this time that you
would have to wait for your data to
pretty much be replicated for the rest
of the the slaves that pretty much
provide the reading which means that
you're risking that you read in every
you're risking that you're going to get
get data that is not consistence another
one down no consistency so that was kind
of a problem and then he said all right
if you don't want to do that you know
for one of these visa because you read
and write the ratio of the video read
and write is pretty much does not work
for you you're writing as much as you're
reading this chart all this data and
what
chardon is that's pretty much you take
your data set and you pretty much divide
it across the cluster based on some kind
of strategy you could say that everybody
is last names from A to C are going a
machine one from C to Z are going in
machine to on such right and you try to
kind of keep an equal load then if one
of the machines pretty much grows out of
control you kind of be shard it again
try to keep that data distribution even
to an extent what's big problem because
all the sudden we could enjoin across
petitions you just couldn't do those
joins anymore you just because the data
is literally in different databases but
it mean was a significant improvements
of read because the reason why it's at
the same time no referential integrity
no across tables we kind of required the
modification of our client applications
so all the sudden the client
applications needs to be aware of where
the data exactly is in the shard and it
was a problem so we kind of introduced
again a single point of failure one of
the machines died one machine dies and
pretty much you lose like a big subset
of your data right there how is that
consistent next one so what's the point
we vertically scale our relational
database we're no longer consistence
don't longer know acidity anymore we no
longer acid and we lose the query
flexibility so are we doing something
wrong maybe so the next thing I want to
introduce is the cap theorem and what
cap theorem is is this is something that
was introduced by Eric Brewer on
distributed systems pretty much for any
distributed system as nothing to do with
databases so it says that pick out
picked you out of consistency
availability and partition tolerance
pretty much if you are consistent and
available no partition tournaments if
you are available and pretend you have
petitioned tolerance tolerance you're
not consistent and I missed one and so
forth this is kind of similar to fast
cheap goods
there's no fast cheap good service hey
chief good service won't be fast and
fast good service won't be cheap and
fast cheap service won't be good so you
can only have two of them next one so
the relational model and cab the
relational model data store happens to
actually favored consistency and
availability well I mean by that every
time you query the database is ready for
you and gives you some pretty much
answer back and it's one hundred percent
consistent so this is for historical
historical reasons we have certain types
of applications that require that banks
everybody knows the example I deposit
one hundred dollars in my friends bank
account and all that kind of stuff and
you you know I'm not even going an
example but according to camp partition
tolerance is impossible which means that
it's impossible for you to horizontally
pretty much scale because partition
tolerance means you have a distributed
system right so by cap the relational
model is impossible to be distributed
right so yeah we are in a pickle but too
much data in a CA marvel vertical
scaling is too expensive and it's pretty
much not sustainable mean doesn't really
matter dependent it doesn't really
matter how how much money you have
you're going to buy the most expensive
machine you're going to have pretty much
like more date and you're going to have
more data sooner or later so that pretty
much forced us to kind of explore other
alternatives in light of cap itself so
we say our partition tournaments since
we reach the limit of the one machine
want to be pretty much like what not
have scaled a lot of choice that scaling
out all right which means to be
partitioned tournament I have like some
kind of store that has all this instance
this cluster of machines and kind of
manages them in a way and we want to be
available we always want to be available
so nobody's given is pretty much willing
to give this up to give availability up
because that but then the good news is
it becomes even better with distribution
if you have one machine it's hard for
you to keep the machine always available
we usually would have always like a
stand stand by machine and all that kind
of stuff with distribution it's even
cheaper and better because the cluster
as a whole it's inherited inherently
more reliable and more available than
the pretty much one machine as a whole
you can have one died but your whole
system is still there it's still
standing serving customers so anyways
according to cap we simply cannot have
consistency you know that means that i
make an update also and what consistency
is this should be clear maybe i should
have discussed this before is what what
it means to be consistent is that you
make one right and you update a record
everybody who reads the data right after
that right gets the most the same most
recent value i wrote to the database
right there's no in-between there hey
this is impossible so they're not going
to work out work out for us so we said
all right let's look up like an AP
system something that is petitioned
tolerance and the lip and the vacation
tolerant and available all the time and
the first thing that comes to mind is a
dns server right you're going your
register no sequel com alright and we
have let's say 100 dns service servers
right owned by two companies first
company update their record the second
one doesn't mean it takes two days or
three days for all the data to pretty
much pop the gate so you're going then
you try no sequel com you don't really
get an IP back but the next day
eventually you're going to get the
meaning would be in every server and
every dns server out there so it's just
like a matter of time because they kind
of need to communicate that change to
all the notes that are there to serve
you that IP address that record next so
what we're talking about is what is
called eventual consistency which is not
so bad it just means that we're just
going to not give off consistency old
altogether but what we're going to do is
like just settle for like a lesser
degree of consistency data is going to
be event eventually available this is
kind of scary in a way it could work for
you and cannot depending on what kind of
system you're doing all right some
people even argue that this could work
for banks as well and we can discuss
that later by anyways this guy goes and
update his relationship status to single
he's got somebody else in Spain that
goes to Facebook and refreshes that page
and it says that he became single and
available but over somewhere else in the
United States Sarah does not see that
change yet it just says that he's in a
relationship still because she is she
does not have access she's on a
different edge node she does where the
data is not it still stale it's not
updated but his brother in where his
bottom in Japan gets it like three days
later who cares they're all going to get
that information one day I mean over
like a day you're like over an hour like
two hours but it's not really critical
for them to get it immediately fly like
a 100-dollar example so it's cool that
we can that eventually consistency is
totally cool within this application so
this is as opposed to immediate
consistency next so the compromise
pretty much is we would settle for a
weaker consistency model it's weird
consistency model we call base as
opposed to acid which is basically
available soft state you are in this
soft state where some of the data is
updated some of it is still stale and
eventual consistency and that all the
soft state is pretty much going to be
translated to the most current one
correctly so we're talking acid on the
individual node based on the cluster
right next slippery slope of the
faithless I don't know why I picked that
button peg on so you might ask um who
might as well ask questions yeah we
doubted this consistency thing and this
asset that the rdbms
us you know preaches and says that it's
a must-have who might as well start
doubting everything else that this model
brings in right so we're right there
just keema we were talking about a
logical schema that we had is will
define and vision in relational
databases you can't change the scheme up
so I not even have a flexible schema or
not even know schema at all let's just
throw all this scheme away actually
would be good for certain applications
because you never know how your data is
going to be avoided it's going to evolve
10 years from now you start collecting
the neighbor's name a last name and
phone number of certain users all the
sudden it's a requirement that you
should keep it to keep track of their
emails as well what try to do that in a
relational database it would be a
nightmare I tube it's not it's not so
easy or like physical schema itself most
of these databases rely on be three
structures all right so why not use some
other end of line data structure that
would be it would be more consistent
with the natural data structure of the
natural way out of data that we're
trying to model organizes itself as or
like clusters ass will be a better one
and the integrated constraints who cares
you know we have all these databases
that you have this one needs to be a
test text this one needs to be a number
in this relationship foreign key primary
key relationship all that kind of stuff
we can even like throw that away I mean
if you want if it's not as important for
you because we can never achieve one
hundred percent consistency you can
choose to manage that on the application
layer and a query language it's simple
really that you know do we really care
about sequel that much I mean it's nice
to have but what if we don't even care
for it's like something that comes
pretty much secondary why because there
was always the application that goes and
gets the data for us we interact with
the application we don't interact with
the black screen and sit there and then
right for you Marcus equal statements
for reporting all day as we used to do
ages ago not anymore it's nice web
interfaces
do a lot of stuff for us so sequel who
cares we hate it anyways anybody who
uses hibernate never goes back in whites
jdbc code security ah ah dbms comes with
security a lot of security do we really
have a bet you seventy percent this is
just a random number of us create their
jdbc connection zone like root that's
what whoever it is on my sequel log
doesn't really matter much or like it's
like a different is the one user the
entire cluster just hits the database
with the same credentials root password
wherever it is so name it anything that
this relationship matters we're well
into question because we lost the most
important thing is the ability to pretty
much scale we hit a wall the wall of the
one machine next one so no sequel I so
what this no sequel is that you keep
hearing about it is not really something
in particular it is pretty much it's a
wide range of specialized data stores
with the goal of addressing the
challenges of the relational model one
of these challenges happen to be the
inability for it to be distributed or
the inability for it to be scaled to
scale out or scale horizontally and eric
events the guy who coined the terms put
it the whole point of seeking
alternative is that you need to solve a
problem that relational databases are
bad our bad fit for now as simple as
that instead of the assumption that oh
we have to use a relational database and
we have to solve it this way and make
your life harder so let me make it
easier it does not mean until or anti
relational any data store that is
non-relational that's pretty much what
it means anything non-relational is no
sequel and somebody came up with not
only sequel instead of no sequel thing
but that came like later so i don't know
if any but anyways no sequel is
non-relational next one just to compare
and contrast these things like in a
little bit I
be mindful that there are some outliers
no we're talking about the relational
model versus like 10 different models or
you know tens of open source projects
out there that makes different design
decisions that make the choose different
trade-offs so no sequel usually ones
designed to run on a single single
machine most of no sequel to Knight
technologies would be designed to run on
a cluster of the machines no sequel is a
CA other sequel CA I don't know sequel
technologies would have a PC acp
sometimes did sometimes like even
different degrees of consistencies and
different degrees of C is out there as
as long as you're willing to pretty much
pay for it and dynamo DB is an example
of that and mongodb is an example of
that as well sequel scales vertically
you know I'm doing no sequel usually
scales horizontally if you give a no
sequel a bigger machine you tell me I
mean take advantage of the processing
power but it's designed to scale
horizontally unlike even commodity
commodity machines sequel sequel relies
on sequel or relational databases rely
on sequel no sequel usually have this
custom API is pretty much do not there
is no DSL on top of these things I mean
usually you're just going to have to
make a series of method calls sequel is
acid we talked about acid as well no
sequel is base sequel supports full
indexing of the database I mean usually
the primary keys are indexed but
anything that you were query by or put
like on a web statement all right you
can create a second index for and get
pretty much better performance no sequel
most mostly on keys only mostly some
databases very another thing and sequel
as a rigid schema that you cannot change
so it's hard to change and a lot of no
sequel is pretty much does not have any
ski ski model does not enforce it sequel
would allow you to have flexible
flexible queries in no sequels you
actually have to have predefined queries
and kind of design your data database on
more than the way you would query data
versus
versus modeling your data to support any
time any combination of queries or like
any query out there well actually I have
that over there so see koalas in
relational is concerned about what what
the data consists off so no redundancy
and they've tables and the relationship
between each other and all I kind of
stuff is well defined in the schema but
in non-relational they've conservative
how the data is pretty much queried
right so if you're if you have if you're
querying for example and an employee
first name last name and his department
wherever it is it's very common for a no
sequel database to have that pretty much
on one table you create like a one index
table that has all that information so
you have like pretty much easy access to
it right of course there are outliers of
course and it's thing I found online but
you know the guy hates relationships
anyways the zoo I so what the zoo is
Imperials are going to go over like a
taxonomy or like different kinds of
relational databases kind of talk
feature some of them and talk about them
a little bit here in just a little bit
so and give you kind of an idea when you
use them versus when not to and the
challenges without associated with them
and the first one is pretty much the key
valued at key or key value data stores
this is very simple fogler like it's
just a big hash map or a big associative
array that's it right there distributed
across Fitzhugh large large cluster
extreme very simple very fast reads and
writes you know you just go give me the
record return this record and now set
unfortunately no secondary indexes you
only index is pretty much the key so you
can only worry data by that particular
key that's it if you decide to kind of
filter by one of the values the
different values in your table you're
talking about full table scans I don't
think that's a good idea at all even if
you use MapReduce or anything like that
because as your data grows the
performance of your queries is going to
decrease um when to use this guy use it
when your data is not highly related
there's not a lot of relationship
between your data and pretty much use it
when all you need is
basic current basic feeling much reading
and and and and right there are
certainly challenges with complex
queries if you have a complex query
you're going to have to literally go and
duplicate data entry index tables and
query those tables instead of the
original instead of the original data
set check out the Amazon dynamic paper
it's a very good read you know what
that's the link for it is up there but
if you google for it you're not going to
have a problem finding it featured
father project of course DynamoDB forum
amazon react and ritas next thing i want
to talk about is column based or calling
family data stores what these things are
they're pretty much the shake table you
know when data in the same column is
stored together so this is a data
structure that is completely different
from the way tables are structured in a
relational database now relational
database you have the concept of a row
there as in rows right now right here
it's like you have instead of an actual
world as monolithic you have like a
linked list of columns or linked list of
column families this makes it pretty
much like easier to just go and fitch
like an entire column of like the
database or an entire pretty much like
column family it's a great for sparse
tables in a relational database if you
remember your Oracle or wherever it is
you have to do ver and very care 24 that
reserves 24 whatever that thing is bites
were there were 24 characters for your
queries so every one of those tables has
like pre reserved space that you pretty
much end up wasting per row even if half
of your values are pretty much no it
doesn't matter what if they're not all
not the space is reserved this is great
because if you haven't no it doesn't get
stored in disk so you save a lot of
space in these guys but so storage is
not wasted they're very very fast column
operations including aggregations use it
when you have big data they have
excellent leverage of the MapReduce and
technology is like feeling
hadoop HH base once on hadoop as well
and if they perform really well on large
large clusters the challenges is that
you better know your access patterns
really well beforehand and key design is
not trivial to give you an example with
that don't expect cereal for example
keys for some table because what that
means is all your data is going to end
up in node 1 and node 1 is going to have
to handle all the load of the queries so
you want to be mindful that your data is
distributed across the entire cluster if
you want to take full advantage of the
MapReduce technologies if you happen to
query by zip code for example employee
and zip code make sure that you stop all
your queries would start or like we'd
have you know and after the second byte
number 13 the zip code right there so
you can just go scandals scan the keys
and look for like a particular pattern
within the key itself so the way
relationships are established is by
actually creating a composite key itself
in the in the database there are a lot
of best practices out there and the
HBase documentation is great for that
check out the Google's Google's big
table paper as well and it's it's a good
one it's available online featured
projects HBase and Cassandra am i doing
timewise ok thanks so the last one I
want to talk about not the last one
should I lied the only one want to talk
about is document-based stores what is
document-based stores so pretty much
nested structures of hashes and their
values I mean it's just a of just like
documents well structured way xmls do
you have like this huge JSON as an
example because a lot of these guys
would actually store the actual Jason
Jason would have like an object and it
would have values but it can have nested
structures as well so if your data looks
like that a lot of our data looks like
that you have like a user and the user
have this and that have you know a
profile and such um use it there they're
very flexible they have a very flexible
schema
doesn't really matter I mean you can
have a user with name and first name you
can have another user with like a whole
different you know with these profile
information in there it doesn't really
care what schema is right there and it
has no limit in depth you can have a
nested nested nested nested nest in this
structural you know forever they don't
really care it's a very it's very
flexible well index data this is
something that a lot of no sequel
databases actually miss it's the ability
to do queries this one where this one
equals that and get like acceptable and
have that pretty much that capability to
begin when you get acceptable
performance and yeah and then it relies
on these Big Data technologies to
perform that well it works very well for
LP no impedance mismatch well that's
kind of obvious in a way but D
formalized should be denormalized
they're more dia normalizing is pretty
much the best practice so you want to be
as flat as possible in dynamo the same
way that you would take your data and
serialize it into XML I like serializing
into JSON use it when you don't have
much when you don't know much about the
schema you expect your data to pretty
much change over the over the years over
the whoever it is you know your
requirements is not really defined yet
and use it when the schema is very
likely to change if your log in the IP
address today and your log in the actual
operation or the service that was
accessed and your boss comes in the next
day and he tells you that you want to
log in the credentials that will use
though he wants to log in the time or
something else you should be no problem
in dynamo and actually this is true for
a lot of no sequel technologies the
challenges complex joint queries are
very hard when you have two documents
you try to join both of them it's very
hard and self-referencing documents and
circular dependencies are almost an
impossibility I mean you couldn't have a
document that kind of references to
itself a user reference into itself
would be very hard to pretty much like
Marlene dynamodb projects features
featured MongoDB CouchDB
they also reference um I think CouchDB
has like this vest very nice rest
interface that you could pretty much
leverage as well for a lot of people so
last thing I want to talk about is the
graph data stores graph is a graph it's
perfect for highly interconnected data
allows for explicit relationship find
great relationships reversals very
flexible works very well with opie no
impedance mismatch if you remember your
graphs from your data structures it's
the exactly exact same thing you could
literally asked the question who's the
frame if you have the social graph for
example you want to know the friend of a
friend you could totally totally pretty
much just to get your answer like that
because it's a graph it comes with Chan
I mean use it when your data looks like
a graph when you require it requires
like graph questions like graph
traversals the salesman problems is an
example and use it when you are smart
enough not to try to do this on another
data store you want to be able to answer
those questions in relational in a
relational database good luck if you
model it you get good performance good
luck the challenge is it does not scale
horizontally just like a relational
database you couldn't take a graph and
distributed over a lot of machines it
has it's actually acid this is one of
the no sequel databases that is pretty
much acid it's easy to do because it's
on the one machine and the featured
project for this one is neo4j since you
can only scan on the one machine you
want to be mindful I'll delay this this
thing until later this comment until
later we don't want to forget about the
relational databases we have to mention
these these guys so this is the Jurassic
Park of the zoo where the dinosaurs live
use it when your data is highly
relational use it when there is no need
to break data into small pieces where
there is a need to break that into small
pieces and assemble it in different way
reporting you want to have you know this
from this table this from that table
that's from that table name report and
then group in a certain way perfect
nobody beats relational
use it when consistence is a must
because you're not going to immediate
consistency you're not going to get it
anywhere else and use it when the access
patterns of unknown you just don't know
how they said these people are pretty
much going to go and query your data and
doesn't scale horizontally we talked
about that featured project skip yes no
oh how do you choose right if it doesn't
fit you must acquit wondering when I got
that quote from but um what you want to
take into consideration when choosing
the data store that fits your need best
is the actual data itself does it have a
natural structure does it like cluster
and does it relate to each other in a
way that I mean what data how does it
relate to each other I mean if it's a
graph maybe you should look for a data
store that is more or like friendlier
towards like graph questions for example
or like things like that I don't know if
this might not have made any sense but
you know what I mean right I hope so and
how the data is connected to each other
it's extremely important as well how is
it distributed and how much of it you
have more importantly so when I look at
your data how much how distributed how
is it connected to each other and does
it have a natural structure as your data
and you try to answer these questions
you want to actually talk about the
access patterns what is the Reed and
white ratio becomes extremely important
are you reading more than your writing
and how much because certain data
structures mean few of them the complete
I mean hash tables have min trees for
example three data structures of like
the perfect for search whoever it is so
you would if you have a search problem
would want to use like for example a
tree things of that of that of that but
if you're writing or like you want like
constant look up on certain on something
you might want to put like a hash table
over there or I mean it's more so it's
important you know whether you're going
to favor a solution that performs
reads really well but rights are ok
whatever you need both or not I want to
get there and also whether how
distribute whatever your data is pretty
much uniform or random that one should
be moved up to the data it's really
important to know how distribute the
distribution of your data is that
uniform or is it pretty much like random
as well and you want to actually look at
cap and figure out you know which one of
those letters you you're willing to
pretty much give give up for the other
ones so other consideration is the
maturity of a solution a lot of these no
sequel databases I'm not mature you know
the stability of the code the disability
of the project itself is something that
has a community around it is it
something that is well maintained and
stuff the durability as well are you a
more interested in something it gets
persisted to disk or you just want
something that is in memory although
some people argue that caches are not
really no sequel to begin with but
that's something you might want to look
at you might want to look at costs as
well how much is it going to cost you to
lean mean nobody beats our DBMS in the
availability of tools and also the
familiarity I mean this is a big
learning curve for a lot of us coming
from years years of no sequently expect
things to be a certain way next so for
fairness relational data stores did not
fail us they didn't are they actually
performed really well for what they were
designed for we kind of found ourselves
if you have a Phillips screwdriver they
try to use like a flat head to kind of
unscrew it you know you kind of going to
do it but you're not going to do as well
as the actual Phillip if you use like
the actual Phillips screwdriver to trend
that was designed for or sometimes it's
just pretty much impossible to unscrew a
flat head with a Phillips so it's us
using them for things that they were not
intended to be used for so take any data
store and you'll get take any data store
I need a strong you get as much in
in as much as much trouble as of dbms if
we started with graph databases we said
everybody writes all their application
or graph databases you're going to end
up with the same thing because there's
those silver bullets there are always
trade-offs so yeah you can skip that now
polyglot persistence that we talked
about in length how we got here you know
on our journey with relational databases
talk a little bit about the no sequel
and you know and got to know some of him
we're going to move to something to this
like new movement called polyglot
persistence and before I go polyglot is
disturbed i was first years by near fort
he is speaking in this conference for
programming languages he is a very big
believer in ones that one of that we
should use different programming
languages to solve different pretty much
solutions within the enterprise he wants
an application where half of it is will
be enough of it is java and half of it
is this not just for fun to play around
but some problems are easier to solve
with with some languages so the same way
for like data stores we're dealing with
these enterprise applications that are
very complex and they combine complex
problems the assumption that we should
always use one datastore is just absurd
you can try to fit it all in one model
and and but don't expect new problems
take all these problems and try to put
them in a relational database you're
going to have problems take them all and
put them in a column base you're going
to have problems so they just don't all
fit in the one model so this brings that
to the polyglot persistence which is
pretty much to leverage multiple data
stores based on the way data is used by
the application they have multiple of
them have your application have you used
multiple of them at the same time you
strategically of course this is
associated with a learning curve and a
big risk and it's a long to body it's a
long term investment because it becomes
more productive and in the in the long
run and you should do it really
carefully and not go out of control it's
like the
guy that gets introduced to design
patterns and stop seeing them all over
the place I was that guy when back in
college but what this pretty much gives
you is give you the ability to leverage
the strengths of a multiple pretty much
data store to provide the best and the
simplest solution for your problem so an
example these are just pretty much
examples i would use MongoDB for product
for product catalog that's more likely
to pretty much change and it has all
this hierarchy of like you know home and
in the end of home there is bathroom and
the bathroom only of towels wherever it
is and you know some guy introduces an
electronic towel so I should have no
problem pretty much saying that this
towel takes like batteries and things
like that the verb MongoDB is very
efficient in have slow beads that's all
rights by very fast very fast beads and
so you have that ratio out there so we
totally go with something like Mongo
radius for shopping cart data is
temporary and I need to pretty much they
should go and give me the shopping cart
for this user ID and you get it right
there used for example dynamic DB for
like am for like social profile info
it's a key data store neo4j for the
social graph and I'm going I'm going to
go over there since this thing cannot
scale vertically it's completely okay
for you to have the data of your social
graph are only the idea of your users so
you have a graph of the idea of the
users and you kind of kind of save in
space their profile information is
stored somewhere else the profile
information is stored in pretty much
DynamoDB so you go and you have this
user profile you take the ID and your
query neo4j of all his friends you get
all hit the ideas of his friends you
turn around and you pretty much get
their profile information from the NM OD
be so you have the one who actually hold
that key and with it you can go and ask
different data stores in a way that is
more efficient HBase for inbox and
public feed messages we depend on how
much time we can discuss why is that I
mean I think Facebook use it for that
particular purpose don't quote me I
might be completely wrong my sequel for
payment and account info $100 problem
and Cassandra for audit and activity
logs most
important thing of the slide I am not
making any recommendation here and by
the way don't just go and do this this
can be a management nightmare you could
totally fail if you I mean you want to
be very strategic if it's a real problem
you know maybe use to maybe use one
maybe three I mean depends but you need
to be aware that all your developers and
all of the people who are working on
this thing needs a lot of time to get
that knowledge and that it's going to
take a lot of time for you to manage
these data stores individually next no
sequel in the cloud no sequel become a
commodity talking about fully managed
data stores no maintenance whatsoever
DynamoDB is the perfect example I love
that thing you don't have to worry about
cluster management I came from a company
who actually had to manage our HBase
cluster ourselves there was a nightmare
it was really hard and we're trying to
do that on ec2 and wherever it is move
away from that to use dynamic and
everything is pretty much managed for us
and we could scale it out scaling down
it's a pretty much like a matter of a
few clicks elastic scaling that's what
you get with a lot of these companies
right there that do hosted
Mongo and hosted no sequel and very
cheap storage it cost zero on dynamic to
store a value for for for bandwidth for
how much data to query but you could
have billions of records over over over
there and they will not be they're not
going to charge you anything and ones i
think on SSD or something featured
amazon AWS you know he roku pretty much
have a set of add-ons neo4j a thing is
one of them you know name any no sequel
database out there and you know cloud
foundry these are maybe he will conclude
found available both paths amazon AWS
kind of tethered some of them offerings
of pants offering some of them is just
pretty much infrastructure for you to
use as promised i'm going to answer my
abstract has a bunch of questions those
questions were designed to draw you guys
in and i told you i was going to have
the answers for them there you go what
does the rise of all of you is no sequel
databases me to my enterprise i'm
guessing a lot let's know this
presentation what is no sequel to begin
with any non relational data store does
it mean no sequel no could this be just
another fad I don't think so personally
it's a matter of opinion you're entitled
is that a good idea to abandon already
BMS for all these new exile exotic
whatever it is for my specific
enterprise it's up to you i would say no
guts no glory now i'll give you an
advice though you don't want to go and
take the most critical piece of your
applications start experimenting with
these databases because most likely you
are going to discover as you're learning
that you pretty much made most likely
made the wrong choice to store your data
over there you might want to get these
like small projects that just like
pretty much like float around kind of
experiment with it get you a check nick
down and everything modeling is not
simple make sure you know that really
well then transition in a way that is
very graceful don't just go and say 85
all the DBA is and i'll do that now
although it's very tempted so how
scalable is scalable however much you
need it to be
if you're doing audits and you're
interested in storing you know five year
worth of data you probably would be fine
with just pretty much the one server and
just use on our dbms it's cheap and
you're not going to get in trouble
because you only want five years of
financial do of audit data if you want
the forever and you think that you have
millions of customers and that thing
just grows you're going to have to make
the call right there and make the trade
off for the right database assuming that
I'm sold although I choose the one that
fits my needs best I'll tell you if you
hire me I don't know your needs all
depends to you you know you need more
than anybody else this presentation is
just designed to kind of introduce you
and tell you give you like in a way you
know just a starting point for you to
kind of know where you stand and what
the available technology is out there is
there a middle ground somewhere polyglot
persistence is your middle ground what
is this boy girl polyglot persistence I
hear about it's the middle ground so
when you sit down and you realize that
we deal with complex problems they're
all different it's just fair to say that
I am going to use this for this and I'm
going to use that for that instead of
going all going only like one direction
yeah I think that does it thank you all
for sitting through the talk I really
appreciate you pretty much being here
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>