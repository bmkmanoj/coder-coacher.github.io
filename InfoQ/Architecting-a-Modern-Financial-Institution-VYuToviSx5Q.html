<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Architecting a Modern Financial Institution | Coder Coacher - Coaching Coders</title><meta content="Architecting a Modern Financial Institution - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Architecting a Modern Financial Institution</b></h2><h5 class="post__date">2018-01-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VYuToviSx5Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so let's go so my name is Edie
this is how fail and as Chris said we've
been building new banks since day one
since the initial lines of code together
our initial product is a credit card so
we started out as a credit card issuer
in Brazil and we've expanded into being
a full digital bank from there so just
so you can get a sense for scale we are
at about 2.6 million customers and that
is a lot when you consider the
complexity of the domain model there's a
reason why core banking systems still
run on mainframes in most places it's a
it's a large domain I'm gonna try to
illustrate that for you a little bit to
the extent I can but yeah we have
hundreds of millions of purchases
purchases coming from people visiting
just about every country on earth but
we're not a bank
we are selling financial selling
financial services but it feels like a
tech company we we are a 10th company we
deployed 20 times per day we have 120
micro services and we recently just
crossed 100 engineers on the team so
what we're gonna talk through is is
really credit to the whole team and all
the engineers the great engineers back
in San Paolo and now Berlin we started
our second office hoffa and I are going
to take credit for it but it is really
the work of everybody briefly on our
stack so we rewrote core banking and
credit card processing from scratch and
so we used modern technology we loved
closure most of our production services
use closure it's a lisp hosted on the
JVM and it's an opinionated functional
language it doesn't mean you can't write
object-oriented patterns but you know it
when you're doing it and so we really
like that closure keeps us lean and
keeps us focused on pure functions and
helps us to scale we also use the atomic
this is the database that we use for
most transactional workloads the analogy
that I would use is kind of like get for
your data so you never lose anything you
a fact may be true and then later not
true but you never would do an update in
place and lose that that history we'll
talk more about some interesting
properties of that we're heavy users of
Kafka
in fact Moe our default kind of service
service integration patterns are via
Kafka
we really appreciate similar to the
atomic that concept of an immutable log
and it helps us with logical and
temporal decoupling between services and
it's censoring the financial services
industry is legendary for big batch jobs
and we're starting to understand why but
one of the things that we do is we just
treat best jobs like a stream of
messages over Kafka and we can we can
almost model everything in banking as as
stream processing and lastly and this is
kind of obvious but we're cloud native
we we architected everything on AWS from
the beginning about four years ago and
infrastructure is code we do Bluegreen
deploys not only for new versions of
services but also for new versions of
the entire architecture so we'll go spin
an entire new version of new bank and
then kill the old version once we've
rerouted DNS so that's something that
that's helped us to avoid mutating
things in production and really helped
us scale and I guess just a last thing
on the intro we've seen a number of
benefits from taking a functional
programming philosophy quite seriously
from the beginning of the company and I
guess it would highlight three things on
hiring counter intuitively we see this
as self selection positive self
selection people that have the courage
to go out and try a technical exercise
in a language they've never used before
certainly not in anger in production we
find that that scares a lot of people
but the people that it doesn't are
generally good people to hire so that's
helped us we've also seen now with over
100 services and millions of customers
the complexity of the domain is under
control we have things that are
difficult to deal with but we always are
able to untangle those things and that's
basically the architecture of a bunch of
small pure functions and consistency
closure is a language that encourages
small number of idiomatic language
features composing into big things and
so what we find is it's easy for people
to move between teams and every service
feels very very similar so it's a fairly
consistent architecture and this is our
headquarters in some problem so our
initial architecture for
a card and I'm gonna lay out like a
periodic table here that's gonna make no
sense but the whole the whole point is
that it's it's complex and this is the
MVP so in order to get a card passing in
a machine right we had to build Carter
origination logistics physical shipping
we had to build billing payments charge
back a whole back office tooling etc the
next thing we built was how to go
acquire customers right kyc credit
scoring limit management and this is an
area where we heavily use machine
learning this was the first place we put
machine learning into production after
that we started to take ourselves
seriously as the system of Records so we
rely on ourselves to know how much money
you owe us and that's where the general
ledger system comes in double entry
which we're going to touch on in more
detail more more recently we decided to
bring card authorization transaction
authorization completely in-house so we
built a MasterCard authorizer from
scratch we also built an ETL to make a
mirrored analytical environment to
optimize for four different things we'll
talk about both of those we recently
launched rewards and very recently we
just launched a bank account so this
this is not the traditional path but we
kind of came from credit card and went
to core banking and so the key features
here are things like earning money on
your savings but also being able to use
that same account like a checking
account for instant peer-to-peer
transfers and stuff and right now by
American standards it's it's it's a good
investment we're earning we're paying 7%
per year risk-free in Brazil so it's a
good good place to put your money
so banking adds a few other components
here including real time transfers which
we'll talk about from an architecture
perspective and also there's this point
at the bottom infrastructure
infrastructure kind of glues everything
together and we'll talk a bit about how
that works in your bank so so that's a
lot of stuff we can't cover all these
modules but we're going to cover the
highlighted ones in some detail and
hopefully we go deep enough to give
folks a real sense of of how things work
starting with purchase authorization
okay good morning guys so I thought
we start with maybe the most
representative flow of our first product
and in many ways our core product the
credit card so let's talk about what
happens when you make a purchase when
you do that you actually set in motion a
whole set of participants of the
payments chain so you're interacting
with a merchant and but the POS terminal
the point-of-sale terminal where you
swipe your card is usually managed by
another company a credit card wire the
acquirer is the company that maintains
the relationship with the merchants
connects merchants to the broader
payments chain the acquire then will
forward that transaction authorization
message to the credit card network or
brand we are using MasterCard other
networks are household names Visa
American Express etcetera and then the
brand will finally send that through
sexual relation requests to us to an
issuer on the other hand and that's our
role as a payment situation we are an
issuer this means that we maintain the
relationship with the cardholder the
actual final customer is on a bank
customer and as part of that an
important part for talking about the
actual transition is that we take credit
risk for that customer if the customer
fails to make do on his payments we
earned a hook for that amount so that's
why we have the final say on whether a
transaction was authorized or declined
now technically how does that work on a
data center that we control the credit
card network will place several devices
connecting to the worldwide network and
one of our servers will run a service we
are calling an authorizer which will
connect to that to the brand devices the
specific device that we connect you and
calling it here the MasterCard interface
device it is an edge device that accepts
TCP connections and once we connect we
start receiving authorization requests a
couple of interesting points to note
here are that we are a network client
not a network server this means that's
what chief concurrency to be able to
handle the transaction volumes that we
we must deal with
need to multiplex that single network
connection and another observation is
that when we first went live when I
first connected to the real live
payments Network we started receiving
authorization requests immediately which
is a bit odd since we hadn't issued any
credit cards at that point for the
authorizer what was happen is that we
were receiving attacks attempted attacks
people either brute force in credit card
numbers or people using data from those
large credit card breaches in the past
to try to attacked us and the payments
ecosystem is constantly under attack
something you need to know about before
you undertake these kinds of projects so
the protocol the protocol we used to
connect to the brand interface device is
the same protocol that flows throughout
the entire payment's chain it's based on
an international standard ISO 85-83 but
this is not a standard like HTTP or SMTP
where you can just read the spec and
build a compliant limitation you still
need a lot of specific information from
MasterCard from the brand and we had to
consult a lot of documentation and from
from that burn to be able to build a
compliant implementation but sometimes
that wasn't enough at one point in the
project we needed to pass an input to a
signature verification algorithm and the
actual set of input fields was not
specified anywhere
so whenever engineers had to build a
tool to brute-force or possible feed
common combinations to understand what
you had to build it can also see on this
slide we are teaching a hardware
security module that's a device that
stores a primary key on secure hardware
and every other key that's used for for
the encryption parts of the protocol are
derived from that primary key on the HSM
and one of the ways that we use the HSM
is that when you do a chip transaction
that microprocessor on the chip we
generate a cryptogram which is a small
amount of data that flows through the
network to our services and we pass it
to the HSM for for validation so in the
beginning Edward was talking about
closure how much we loved closure and
through we really love it and even for
the authorizer project most of our code
is enclosure but we thought that for
parsing that binary protocol we could
take advantage of Scala library called s
codec which is a binary partial
Combinator style library and we could
achieve a lot of things by building upon
s codec it's easier to build a parser
it's composable the type system helps to
ensure that all the parts of fitting
together we get error detection and a
cool thing is that it also makes it
easier to evolve that parser over time
when we we were doing one of our early
tests for instance we thought some of
the fields were going to be encoded as s
key s key strings but it turns out when
we're doing the live tests we figured
out we were receiving obsidian coded
Sphinx you know that old mainframe
protocol and to change that we had to
just change a couple of references in
this parcel tree and we were able to
complete that test live while it was in
the running in the test window so this
kind of library for for parsing magneri
protocols is something that was very
helpful for us and now stepping back a
little bit and talking broadly about the
authorizer project there were two main
requirements that drove many of our
architectural decisions one is
availability requirements we of course
on our cloud services we take care to
make that they are available we we have
observability measures we have measures
in place to bring back service when it
fails but when we are talking about a
project like an authorizer
where our customer can be trying to make
a purchase in a gas station midnight
with only our card in his pocket it's
very critical that every single
transaction goes true so availability
was critical second requirement that is
important to us was that we are building
on physical infrastructure as Erin
mentioned the beginning most of our
experience has been on cloud services
most most of our services we are
building upon
the Amazon public cloud we take a lot of
advantage from automation that the
public cloud affords us and then we have
to face the prospect of beauty something
on physical servers bare meadow we of
course had to boot all of that
automation ourselves we couldn't just do
it manually for just once it doesn't
doesn't work like that and when you put
those two requirements together we
decided to try to build a minimal
structure infrastructure within that
those those physical data centers with
composed of a small set of highly
available services they are redundant
within the data center and we have
multiple live data centers accepting
transaction requests and in in addition
to being minimal the other important
part is that it's isolated we are able
to authorize transactions that are
coming in to the payments Network only
with this small set of services running
on our physical data centers we don't
need any cloud communication in order to
authorize a transaction that is
important because we were worried that
if one of the links links goes down or
if there's a problem with the Amazon
this could affect availability and and
the the other thing to note here is that
we are using thrift and finagle for
inter service communication and so I was
making this point about how isolated the
inner loop for transaction authorization
was but some communication is required
the ultimate truth about data for all of
our customers lies on our cloud services
they maintain their data bases that are
the ultimate owners of that information
and so there's they need to know when
the transaction was authorized at the
same time there's information that we
only learn about on the cloud site for
instance if a customer makes a payment
and that entails a change in his credit
limit it's available image we need to
send that information back to the
authorizer
and we are using Kafka for that Kafka as
was mentioned before what is their
default inter-service communication
technology and the cool thing
using Kefka for this project is that we
are not using only for communication one
interesting property that kefka has that
other message brokers usually don't is
that messages are durable a message does
not disappear immediately after it's
consumed and we are using that to build
a log snapshot style data platform on
the authorizers project and we're doing
that because we did not want to have
database running on our physical
infrastructure as part of those those
requirements of being minimal and being
and being highly available so we're
starting everything in memory the data
that you need to authorize a credit-card
transaction is not very extensive so
it's it can be safely stored in memory
in the authorizer services but to be
able to bootstrap that new memory stage
and to ensure that every single
authorizer replica is consistent we
piggyback upon a kefka's log so the way
this works is there's an Amazon service
it's running it publishes a message that
message is consumed by our authorizer in
our physical infrastructure everything's
pretty simple so far at the same time a
second service is also consuming those
messages this snapshot or service we
consume those messages accumulate any
memory stage and periodically it
generates a snapshot to disk and to a
stream and when a new authorizer
instance starts let's say we're
deploying a new version that authorizer
instance fetches that snapshot from disk
loads it into memory and now it's able
to accept new new transactional ization
requests
it also fetches from that snapshot the
offset on the catalog at which that
snapshot was generated so it can start
to consume new messages this ensuring we
don't lose any any data this here are
just some some metrics that we keep
track of and you can see how improved
our standing rates that's the the rates
that's the percent of transactions that
we cannot authorize and the brand has to
authorize the network
as authorized on our behalf we want to
minimize that and after we deployed our
own authorizer because it we were able
to go down from around 1% of standings
to below 1 basis point and perhaps with
this light the thing that are most went
to convey to you was a point about the
process with this project we decided
that we wanted to go live as soon as
possible to Conn to pass all of the
master car tests as soon as possible so
we could learn from real data using
controlled experiments using controlled
set of test cards learn what actually
was coming in through the wire in the
real life because we knew that
documentation couldn't be relied upon
for a complete implementation and that
was very helpful for us after those test
cards we rolled out larger and larger
sets of cards until we were able to to
move all of our customer base to the
authorizer the wonderful job of teaching
everyone accounting this morning which
I'm sure you're excited about try to do
this efficiently so one problem that we
face is that in the example that Hoffa
gave on authorization in order to say
yes or no to a given purchase you don't
need a lot of data but the data you need
depends on a lot of other data so we
need to know if you have an available
limit that is a cumulative function of
everything we know about you for your
entire history purchases payments credit
limit changes all that stuff has to come
together to give us the answer which is
what's your open to buy right what's
your available limit so one thing that
we do to manage this complexity is we
actually treat a lot of the core
financial logic in the stateful
cumulative functions in a service we
call double entry so the model is pretty
simple it's an entry has an amount and
then a pair of book accounts or ledger
accounts the credit account and the
debit account and then the balance at
any given time is a cumulative function
over all of the debits and credits for
that book account a cumulative sum
and a customer's balance sheet by
extension is just the collective picture
of all their book accounts and so what
we've done that's different than what
most people do is we modeled the
double-entry accounting system as an
operational system this isn't an
analytical system that happens
periodically with finance this is in
real-time per customer and we're
relating business events like a new
purchase or new payment to a series of
entries that map to our domain model so
this is our mapping between a business
event and debits and credits right and
collectively we call that a movement
this is an example movement where you
have two entries you have you know the
unsettled for a new transaction that
comes in that hasn't settled yet and the
current limit so this system has been
very powerful for us and it's the best
way we know to show you the customer
what your balance sheet is we have a
limit bar in our app and you need to
know what's your what's your limit
what's your current bill etc and so we
use double entry for this but there are
some challenges for one ordering matters
so as an example if you're late and you
make a payment for more than you oh you
don't have a negative late balance you
have a prepaid balance right if those
events happen in a different order
things would be different and to make
this even worse you have late-arriving
events so you can make a payment we
receive today that should be credited on
Friday right so what we need to do is
time travel back and replay those events
and try to figure out what's the right
balance that doesn't violate our
invariance that we have and that's the
process we call fixing invariants so
this is an example of a generative test
or a property based test that we use and
given that the interleaving of events
creates a combinatorial explosion like
it's you can't write traditional unit
tests that are gonna capture the entire
space for something like this so we we
create a randomized initial State with
randomized adjustments and purchases etc
and then we create a loss event and
generate thousands of examples of this
and every time we verify that are
invariants the the property is still
hold and and this has allowed us to
catch very real bugs and production the
last problem here is something that
Hof is going to talk about next which is
right throughput so this is where we
start to hit the limits of what we can
write through a single database and
double entry is typically our highest
pressure database because most business
events actually do generate debits and
credits for new bank so you have rights
from basically any system in the entire
architecture also causing rights on on
the double entry system so the next the
next item is infrastructure we're going
to talk about how we dealt with that
problem among among others Thanks ok so
we've been in this talk but show you
guys this curve this is our customer
growth curve and we are very happy with
that it's good to have customers ever
mark number of customers but as
engineers we need to look at that with
skepticism and and with scale comes
scalability problems and of course we
are no exception the first couple of
bottlenecks that started to affect her
as initially were the data base to put
problem that Edward was was talking
about we are using the atomic and it's a
great database but when we reach very
high write throughput levels we reach
certain limits we had to actually throw
the message consumption that led to
those rights in order to maintain
service stability initially another
bottleneck that that reared up its head
in our infrastructure was the batch jobs
some batch jobs that used to take just a
few minutes we're now taking many many
hours even more than one day and that as
you can imagine can have a pretty
serious impact for our customers and so
the first thing to do when you you're at
scale you try to optimize and we try to
optimize a few of our core flows so they
are faster but that has a limit at some
point you need to find a way to
partition your workload so we can safely
handle that workload in parallel and in
isolated manner and and we had to do
that and when interesting property of
our domain that was helpful that
that the interactions between customers
are minimal and that's different from
other domains for instance look at
social networks in our case most of the
int of the data and the business logic
that we run pertains to customers at a
time so we could is a partitioning of
our customer base as a proxy for
partitioning the workload and the first
option that we considered when we were
thinking about how to partition our
workloads to scale is to partition at
the database level so here you have
back-end service and it used to write to
a single database it reached certain
limits
we would then partition that database
we'd have database charts that that
service on every right and on every
query would need to route that writer
query to the correct database and that
can work a lot of companies do that but
we saw some problems with this approach
one is that it would take an enormous
effort to go through every service there
was facing scalability problems and they
every single query update every single
transaction and the quality of the code
base after we changed all services to
route to multiple databases might
deteriorate a bit and but that's not the
main problem the main problem in our
hand in our eyes is that this when you
partition the database you're only
addressing scalability problems with the
database and there can be other
scalability problems you can have
long-running batch jobs you can have the
operational overhead of scaling our
Kefka clusters there's a lot of problems
that this option would not address so we
considered a different model the
scalability units model when you're
doing scalability units your shards are
not database charts your charts are
actually copies of your infrastructure
so let's look at New Mexico structure we
have our 120 plus services living on a
cloud our databases
Kefka clusters zookeeper clusters
networking so when we go the scalability
units model we build clones of that
entire infrastructure and we assign
different partitions of our customer
base to different clones those are our
shards and that's what we ended up doing
it did really help we can address
multiple scalability bottlenecks at the
same time but of course some route he
has to happen when an external event
happens we need to be able to route
those external events to the right chart
so on the transaction authorization use
case that we were just talking about a
customer makes a purchase this is run on
our physical structure our cloud has to
know about that purchase and has to be
routed to the right chart so we build a
section of infrastructure recorded
global and the global section is not
charted and the services that live on
the global section they are there mostly
to to maintain mappings from externally
known identifiers to customer G's and
their charts so when a purchase comes in
we look up certain identify information
find it chart routed they're basically
the same kind of thing happens when we
know about deposits payments are there
other events there's a slightly
different use case for interactive
interactive views when our customers are
interacting with the app to talk to our
services we are able to route that kind
of flow only at the very beginning when
the customer logs in the way this
happens you're looking at here a
customer logging the there's a global
service that will validate that
customer's credentials even sure it's a
that person is who she says she is then
it will respond if it's correct it will
respond to the app with hyperlinks with
hypermedia
links where that app will be able to
fetch more information so here that
will fetch information about the account
from a different service this next
interaction is already directly
connected to the shard where that
customer lives and new new links and
further communication between that app
and their services then all phone
through a series of hyperlinks and
therefore we can ensure that that app
talks to the correct chart at all times
and we don't need to build an enormous
routing layer that would intercept every
single request from the outside we are
able to do that only at the beginning at
logging which is help of course for for
managing our load and some just going
through some lessons learned from this
process the first last so it's may be
obvious it is working for us and we are
much less concerned about the impact of
the growth of our customer base on our
production services at production
quality but there's some caveats one is
that this if you go the scalability in
its route versus shard each database
route it's much harder to roll out
incrementally you basically have to be
able to route know how to route every
single external incoming event to the
right shard before you're able to send
even a single customer to a second chart
this means that it's a long project that
only bears fruit at the end and it's
important to pay attention to how your
customer base is growing while that
project is is underway and another point
regarding for structure I want to make
here is that even if you have a solution
in place to deal with scale problems
happen if services might try to interact
with third party providers that are
flying services might run out of memory
a bug might be introduced and services
might start showing exceptions and we
apply a couple of highly interesting
fault tolerance patterns to address
those those problems first going to talk
about dead letters it's a very simple
pattern but it has produced enormous
gains for for
operations message is produced it is
consumed and at which point an exception
happens something bad happened what we
do then our libraries that handle the
message infrastructure will actually
forward that original message along with
some extra metadata about the earth to a
dead letter topic another service
consumes from the dead layer of topic
and merely stores it in a data database
somewhere then offline an engineer will
triage those errors those dead letters
on that fire that service of coded
mortician for obvious reasons and if
that engineer is confident that the bug
was fixed and that the problem huge I
can just republish that message back to
the original topic and that helps us
because if your financial situation data
loss is the worst thing that can happen
so we can recover from possible data
loss eventual data loss in using that
letters another pattern I'm going to
talk about circuit breakers I'm sure
most of you are already familiar it's a
very common pattern
I'm sure doing microservices and but the
interesting interesting thing is how a
circuit breakers interact with messaging
and so here we are consuming messages an
outbound call fails if that happens
enough times a circuit breaker would
trip what we do then is we pause the
consumer we stop consuming messages that
allows what could be a barrage of
exceptions of events happening to become
simply an lag that accumulates on a
Kefka topic and when the problem is
fixed it just start consuming it again
one other thing that didn't scale so
well is using the atomic and our
operational transactional infrastructure
for aggregations even simple questions
like how many customers do we have I
think our CEO asked me this many times
and it became increasingly difficult to
answer and that's it's a simple question
what's going on and the answer is even
especially with sharding and fragmenting
the the operational system so that they
scale better you're making analysis
harder and you're making an aggregate
sand analytical work harder and so what
we did is
the traditional I guess approach of
making an ETL which stands for extract
transform and load so before we get into
the ETL I just want to give a quick
primer on how diatomic works two slides
it basically works mapping business
events to get commit style transactions
in the database so if we think about a
customer joining new bank and then
getting a credit limit making a purchase
getting a credit limit increase blocking
a card right so this is a very common
sequence of events for our business this
is how they look in a dynamic database
right so you have assertions of facts
this is a fact this is a fact and
sometimes like in the case of the limit
changing you have something they used to
be a fact which is no longer effect it's
no longer 3000 it's 5000 that's the
limit and the other thing to notice is
here on the on the right hand side you
have a monotonically increasing
transaction because the transactions in
the atomic are a first-class citizen and
you can use those to great effect when
you're when you're dealing with kind of
log tailing right so for our ETL what we
have is an always-on log tailor that
tails all of the dynamic logs we have
and then pipes them into our data Lake
doing chunking and format conversion and
things like that and interestingly it's
the same thing that we do for certain
Kefka topics whereby the data is
relevant and and it's not in the atomic
right similar log abstraction we consume
from those topics and we load those
things into our daily and from there
what we can do is we can paper over
things like sharding right like an
analyst never wants to know never wants
to have to think about where the data is
all it is that in shard 1 or shard 2 or
shard 3 no so we kind of put that back
together
into something we call a contract so a
contract is basically reconstructing a
single logical table for an entity and
then we build as functions of contracts
and other data sets we build a directed
acyclic graph of data sets machine
learning models and also policies that
translate machine learning model scores
into business decisions an example would
be translating a risk score into a
credit limit for you to do proactive
limit increases
and this is an example from from our
business where this is really valuable
to have in the analytical environment
the green line is an example of revenue
and that comes directly from
double-entry the operational system we
talked about but the the purple line is
from our ERP that's cost right that's
not a real-time system and that's never
going to be in production in a real-time
environment but in the analytical
environment we we can recombine those
things to get the answer of you know are
we making money our business is quite
complex it's not clear if you're making
money on every customer all the time so
having every tool you can to understand
contribution margin per customer per day
actually makes analytics much easier
this is also the environment whereby we
generate reports for regulators to keep
regulators informed and happy and that's
something that makes our business
complex but it doesn't hurt us that much
because we have very good tooling to
make that happen it's also a great place
to run machine learning models without
having all of the constraints of
production and last but not least I
wanted to talk about something really
recent that we launched which is
real-time transfers these are
screenshots of an app so the concept of
the product is you scan someone's QR
code or otherwise get their their bank
account put in how much money you want
to transfer and the money's there in
real time right so it's very simple the
way this works is the transfer request
comes in but as I said you're earning 7%
per year this money is invested right so
what we do instead of the traditional
there's no database transaction here
right because people are in different
shards so any form of kind of in the
database transactional semantics are not
going to work but also we we need to
liquidate your investment we need to
make sure that that money that you have
the money and then it's ready to be
transferred so we do that and and only
then do we initiate a transfer out right
and if this transfer out for whatever
reason doesn't work we'll do a
compensating transaction reinvest the
money and kind of roll it back because
that's a very rare event and we wanted
to optimize for the most scalable way to
run this so we're kind of real-time
gross settlement of transfers here and
at that point we have a global service
that can sue
from every shard to get all of the the
global transfers into a single stateful
service so we can maintain item patents
and at the same time the ledger the
double-entry service is observing those
events and updating the debits and
credits so we have an up-to-date
analytical view in real-time at that
point if this is - another new bank
customer that's internal right we'll
just route that into the right right
shard and complete the and complete the
transaction but an interesting feature
of Brazil is that even if it's to an
external bank account to any other bank
account in the entire country that's
also fine and it's also in real time so
we can basically translate Kafka
messages into a soap approach and back
and forth and get that into a
centralized hub and spoke payments model
for Brazil that is connected to hundreds
of other Brazilian banks that we didn't
have to integrate with one by one
thankfully and so I find this really
interesting kind of as an American
coming from the ACH system I mean this
is pretty much space-age technology and
it's it's kind of born out of a legacy
of high inflation rates in Brazil people
didn't want to wait until the end of the
month to kind of build up counterparty
risk and then in a net settle right even
if that's easier on the systems the
value is placed on real time gross
settlement right now and so this system
does over a trillion hey eyes or about
300 billion dollars transferred around
per day and everybody connected to this
has IBM MQ series hardware and it's all
quite sophisticated you can read more
about it at this link so I think the the
overall kind of big-picture summary is
that the financial domain is large
there's a lot of pieces you need to have
an MVP a Minimum Viable Product when
you're dealing with people's money tends
to not look so minimal it tends to be be
large and there's also a lot of
interactions between the different kind
of bounded context right you put
something in charge back and somehow
that affects whether you're authorizing
a purchase or not and so these things
tend to get highly coupled when placed
inside a mainframe and when you allow
database
connections and stored procedures to to
kind of couple them together so we had
to work very hard to to decouple them
and we generally use Kafka and and
asynchronous messaging as the as the
lifeblood underlying that so I think
yeah we're hiring in San Paulo and and
Berlin and thank you I think we're now
gonna open up for for questions bring
the mic around to the front when you
talk about the authorities of components
when you pull out the messages from the
ik/fk are streams then how would you
return that respond back to the client
that this is the old coder
because Kafka is just one way right so
for our authorizer services they also
can produce messages to Kafka do you
know if this one is HTTP or Kafka when a
new transaction comes in so yeah well so
for us CAF goes both ways we use Kafka
to synchronize state into the
authorizers so we know you're open to
buy in real time so we can respond
immediately if you try to transact but
then we push that state back out into
the cloud and also back out importantly
into all the other authorizer instances
right so that they all get an updated
version of your open-to-buy
otherwise you risk you know speed
attacks and things where if something is
routed to an authorized er you could
spend the same money twice right that's
the critical the double spend problem is
is the critical thing that we have to
avoid as
yeah we use so Kafka has asynchronous
semantics but it basically operates in
real-time for us so we think of it as
almost synchronous we don't think of it
as something that's going to take a long
time it's we think of it in real-time
even though it's technically
asynchronous and it only it only becomes
obviously asynchronous when lags build
up in response to some kind of issue
regular operation most of the time I'll
to use our 0 leg first
I think the architecture probably like
the reason being so many decoupled parts
but I would like to understand like from
using Kafka right so why are you
actually doing any kind of aggregations
in Kafka or because I think the cafe is
interesting choice right because you
have a chance of duplications right and
I mean they we talk about exactly one
semantics but end of the dates it's
really hard right I mean with so many
events message is coming in so how do
you do that maybe a little bit on that
and I mean with so many moving parts so
many decoupled parts how difficult was
it in general - I think what was your
road to that complete architecture and
how difficult was it to just not do it
but convinced everyone around it that
it's gonna work it's a great question so
we don't rely on Kafka exactly once
semantics for our business we actually
make sure that any message that goes on
Kafka is idempotent they have to be I
dumped it in because you could consume
the same message more than once so what
we use is the atomic transactional
semantics to maintain that right so when
you consume a message we have a
correlation ID or something in that
message to know if we've already seen it
before and if that goes into the
database we use transaction functions to
make sure that we don't we don't you
know have something that is is double
double counted right even in the
presence of multiple messages so we use
acid transactions for that we don't use
the stateful
kind of Kafka streams or exactly once
semantics is it a real-time or do you do
like a lambic picture so it is real-time
for everything that involves the
operation we do a bit of a land
architecture for things like machine
learning models whereby we we run a
really expensive model overnight to
pre-compute something and then we merge
that in so that if you were to ask you
know for a credit limit increase we
could respond in real-time even though
we pre computed it in the batch
environment last night so we do a little
bit of merging but but not much and it's
it's an interesting point like how did
we end up with this decoupled model
where are we kind of heroes did we are
we really smart not not really so we we
actually built a massive service this is
still infamous in new bank called
accounts and it had like everything in
there and then we we spent I don't know
six months splitting that so so no we
definitely got the bounded context wrong
and we definitely were tempted because
these things interact right why not put
them in the same database it's very
tempting so we had to split services and
in especially with day Tomic it's not
very fun you end up doing log replays
and and it's a tough it's a tough thing
you know that said it's very clear that
we need to get to massive scale
otherwise we won't be a successful
commercial retail bank right so it's
easy to justify those sorts of
investments when you have a long term
vision for where you need to get to it
you know cutting the corner doesn't
really make sense in our context hi I
would actually have a couple of
questions so the first one would be
choosing somehow an extraordinary stack
would you have some hands
hindsight's about that or what would you
change if you would have an opportunity
to change it also would you change
anything about the architecture yeah I
have one big thing I would like to
change if I could in if I could have
thought of it in the beginning is that
one interesting feature of the atomic
that we haven't mentioned yet is that
transactions in the atomic are
first class concept so you can attach
metadata to transactions and we do that
which does the the version the get
version of the service attach the
credentials of the user that that is
making that that transaction and several
other things one thing that we do not
add and I would have liked to add is a
customer identifier because if every
single transaction had and I and then
the identifier that could point to the
actual customer that owns that data
things like splitting databases for
charging would have been much much
easier I think that's a small detail but
it's something I'd like to get better in
the beginning I think we also made some
mistakes with how we use the atomic so
the atomic is really good for high value
business data it doesn't work that well
for firehose rights or really long
strings or things where you want to be
using different sorts of data stores so
sometimes we over used atomic as kind of
like the default database for everything
and you can kind of get lulled into this
the false sense of security is I would
just start a service with the atomic
when the answer should be s3 or dynamo
or you know something else
I think slightly overused but no no
major regrets I think one thing that I
would do differently if I could do it
over again is think about event sourcing
a little bit earlier because I think we
we have event sourcing downstream from
diatomic but not upstream so we don't
get to see you know every request and
every event that happens before the
database right are events that are
effectively lost so I think that's
something I would have thought harder
about and and maybe treated Kafka as a
persistent thing versus a more of an
ephemeral message queue earlier okay
another one
so I guess debugging hundreds of micro
services sounds like a lot of fun
especially when you have customers
complaining they didn't receive any
money things like that so how are you
attacking that problem
I guess you're using some kind of
distributed tracer or things like that
it's it's surprising how non patient
people can be when you know they don't
they don't see them their money it's
it's not a very forgiving
so we we don't use anything like Zipkin
yet I think distributed tracing is
interesting but what we do is Splunk
right so we use Splunk and we use a
concept of a correlation ID whereby
every service that consumes or makes an
HTTP request or produces appends a
segment a random segment to that c ID so
it behaves like a tree right so for any
given request you can look at a see ID
and then shorten that to figure out the
entire fan-out of events on HTTP and
Kafka that happen in the whole system so
does that make it enjoyable and easy not
really but it's definitely tractable you
can always establish kind of what
happened and why a related comment is
that we use that that feature of app any
metadata to the atomic transactions to
store that correlation energy as well so
we can have tools that that correlate
log aggregation data with the actual
database what happened in the database
at that moment so it's it's much easier
to debug than if you have a system that
can lose data where you don't know the
history of what happened so this has
been very helpful for us ok for
something query time so thank our
speakers one more time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>