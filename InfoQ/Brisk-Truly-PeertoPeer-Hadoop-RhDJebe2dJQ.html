<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Brisk: Truly Peer-to-Peer Hadoop | Coder Coacher - Coaching Coders</title><meta content="Brisk: Truly Peer-to-Peer Hadoop - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Brisk: Truly Peer-to-Peer Hadoop</b></h2><h5 class="post__date">2011-06-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RhDJebe2dJQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the talk today is scoring a new product
which is a taking a look at fresh look
at how hadoop has been done all along
and how we can make it better if any of
you have worked with to do you will run
into issues or have aren't issues have
experienced some of the things we'll
talk about and then we exploit a kit
from there see what Cassandra brings to
the table and what that means for for
the dupe and what that means for us if
you're going to use risk if there are
the Hadoop experts will also do advanced
Q&amp;amp;A at the same time I was in that both
start from ground level and go from
there so so brisk brisca is hive Hadoop
and Cassandra how many of you have heard
of hive or pig perfect two same data
analysis so what Bruce does bring to the
table is it brings high for dupe and
cassandra to one spot we've looked at
our customers we looked at different use
cases within the field and we the gist
of that before most people were using
the new seek no sequel stores in
conjunction with this parallel
processing paradigm that MapReduce
producers including Hadoop and and then
using high on top or peg and trying to
analyze data so we found a perfect
opportunity to to bring them together
mesh them well so it scales and that's
kind of what's the story of risk is I'll
go kind of the behind-the-scenes how we
actually pull it off and what were the
interesting design ditions the second
half but initially although some stage
setup for her David salt so
everyone has heard MapReduce and
MapReduce from the glory days of the
first paper that Kimura off Google to
today has kinetic balls and we have a
huge ecosystem of adoption around
MapReduce if you have large data sets
and you can work on small pieces simple
so having large data sets check but you
can we cannot break it down and so on
not want a good servant but if you can
actually break it down into small chunks
and have it all run in parallel and make
it work you're in the right place you
can use a tube now serve going back to
the key key algorithm in the MapReduce
where you have a bunch input and then
you have a key values and then you're
able to transform those key key values
to an output key values that's what
that's kind of your ideal alright so
that's that's a classic flow conversion
papers continue this is another view at
the same MapReduce I'll garden in for
multicore professor kunle from stanford
as this where you could do parallel
version of MapReduce to run more in
thread mappers around Puddle here's a
parallel execution view of how your task
place partition your data you have a
partitioning function the partitions you
did and then do this nice little salt
group and reduce so that's your
MapReduce classic dupe in action right
how does it play when you actually run
it yeah you have a lots of workers you
have a few mass room and map your data
reads ricky's let's that's your flow
mean that's how you get parallelism
that's why Britney broken down your
problem to seek bunch of sea battle maps
the law and you try to align them such
that you can run them in power as much
in parallel you do the sort so this is
under the view of the same MapReduce
split the data they have workers act on
that do a this master who manages all
these that's kind of we'll get back to
the pay little attention the master I
will get back to that
you usually do a lot of local rights but
never do a local like a remote right
mostly remote reads so that's kind of
your the crux of the strength of this
way of putting your data is is that this
is your input files run the map and
intermediate files or deers put the data
together output files that's your
classic sequence and I'm sure people
have looked at this before sorry hash
but from the original sunjai came out
with paper here's the core pieces how
does Hadoop actually work inside it's a
job tracker as we look at how do you
manage this it's a name node which which
portrays your how do you manage your I
notes and HDFS the crux of the best use
case for a dupe is your right ones and
read many right so its files that are
once created they don't need to end
written you don't need to and amongst
you close them you don't need to change
them again so it's kind of immutable
past so it's kind of a classic use case
for immutable past but and append only
sorts right so so it took very cool
allows for getting outside the plastics
model of being able to change data quite
frequently renaming things quite
frequently so it allows you to do a lot
of a lot of optimizations what do you
want to do you want to move computation
not data that's the other second design
philosophy behind this whole hold space
is you have a data center city so once
you have data don't necessarily have to
move it around once you move its large
data moving it around is not efficient
anymore so you just move the competition
to where it is and that's the whole
philosophy view if you want to take one
line this would be it is to move the
data moving logic towards where data is
if the data is launched and of course
for the small later pieces you want to
move data to the actual
compute double-clicking the HDFS
architecture be the actual setup is you
have a name node which is in charge of
all your metadata all your access to
actual finds you have a bunch of data
nodes where you store the real late it
and and and you have access read access
from clients directly to their when you
do rights you have replication involve
between so when you do a write you
replicate between the nodes classic
distributed file system architecture
right everything is a block so name node
has a bunch of finance and then you have
blocks that you can go and access that's
your classic again from the do paper or
in HDFS data nodes do Friedan rights for
blocks so all your data is actually sold
as real file system blocks and the name
node is that's a single master node
that's your single machine at a space
that's where you put all your data in
that's also your single point of failure
so the name node easy is I mean all that
we heard so far is very parallel
massively distributed one of the single
points of failures in the entire ative
ecosystem is the name Lord because
that's where you store your high notes
that's where you want to you want manish
it's all okay just and if everyone has
written code in Java it's all written in
Java so it has the classic challenges of
scaling up so you're a dupe the larger
the size of your head you could also be
limited by Handsome so managing a single
large in memory system is hard and it's
hard then that's why we are in the
parallel distributed more today that's
where we are seeing the new revolution
of large distributed stores and that's
why a single large node is always a
phone so that's where we focus our
efforts on and try and see most of our
customers who are be running this will
be spending a lot of time trying to duck
tape the name node
I'm trying to create a way to to get it
to stabilize if that fails you probably
lost your entire jobs you're probably
all the jobs that are running grab a
lost at different stages the recovery
part from theirs is hard and so that's I
mean we're in many many sends it's a
single node and it needs to be
dissipated most people agree that the
best way to do distributed computing is
not do it at all and unless you're
really forced to do it and in this case
we are our I mean or customers ran into
that and and so did a lot of duplicates
murs and there would be a lot of
interesting challenge so that's kind of
where distributed name node or how do
you do it is to be named known and
that's kind of the nuances behind why
brisk writer to think about it in a
pictorially given lots of lots of data
nodes doing your data blocks the name
Lord kept growing because you have that
all the metadata and what Cassandra does
in some sense is create one kind of note
there is no name node or it's
distributed across just like a regular
node so that's kind of the before after
off of brisk but you this is your Hadoop
you move into you have data nodes and
name nodes in the in the future we
really see one kind of note so it's
peer-to-peer and so that's kind of your
intro and we'll discuss Cassandra at a
little end as well what is Cassandra and
some of you have heard of it some have
abused it but the crux of cassandra is
how do you get high scale data stores
that can run across lots of lots of
machines tolerate failure and are still
not losing data still not losing
accesses so that's kind of here and
peer-to-peer has historically and has
historically survived peer-to-peer as a
philosophy as a
as a scaling mythology as distributed
computing techniques I mean we've all
seen and lived the Napster revolution
and peer p2p way of doing things and
peer-to-peer is as has survived and it's
distributed really distributed there's
no master there's no central anything
right so Google created big table amazon
the dynamo dynamo is this classic
peer-to-peer operational model where you
lay your data on lots and lots of files
a lots of lots of systems and are able
to end able to use a can for loose
consistency model and get high scale
right at Facebook circuit 2008 is to
combine you get this big table has how
many favored big table sure not much to
explain their big table essentially
gives you a column family like semantics
for addressing your data so you get a
schema of sorts right and Amazon gives
you the classic dynamo model of
operational the dynamo is the classic
operational model of simplicity where
you can add lots of nodes actually the
same kind of techniques are being used
in other models like sequel to be
another show so a big table gives you
that schema and that's kind of what the
new ones are on facebook invention of
putting them together it was open source
incubated and then we have fewer
customers using it live in production
and flex one of times how does it work
really cassandra is a has a ring
structure to it and every node is
actually this is made it to a set
certain range of keys right so so if you
think of this as a big space so and
you've chunked each of them to different
key sizes to keep ranges if you now
insert c and so somewhere in between so
you pick nodes so that's the node based
on the key you find a note to write your
element to
and you can set for each of these rights
how many copies you would like to see
you can set replication factor for every
single right right so if there is
something really really classic well
done for no sequel stores that will be
the replication model in Cassandra I
mean that's one of the good strengths
right every every store has their own
strengths and this one has a very very
nice property around replication now you
could you could set so if you said three
copies the first copy is going here then
you have a synchronous as soon as it
finishes it's right on ave so every node
has a comment log and a kind of clear
in-memory representation once it writes
to a comment log it basically and it
also are synchronously fires off two
more replicas or these two other nodes
which are participating in that
replication now these two can mind you
being a different data center or a
different amazon region they are there a
synchronous and they're going to go
finish those rights and have those
copies ready for you in different
different geographies right so in some
sense do you have as a regular simple
final six node four sides customer have
you can put them say you can actually
there's a snitch which basically tells
it to look up it looks up a small list
of four items and says okay this needs
to be in two different racks make sure
then two different racks so just in case
its power outage so you it automatically
does that for you now let's say one of
these nodes actually down or did not
respond it actually makes a note on the
on the note saying a hint it leaves a
hint they're saying we play this when
that note comes back up it's a it's
tolerant to the failure your it also has
a lot of interesting nuances around how
if this node is out of sync with other
nodes it the the whole said the other so
that's one how do we get the replication
work the second piece interests
this here is as you add nodes typically
I mean people who have used coherence or
dekhe spaces or other large systems as
you add nodes the communication patterns
between them either it's tcp UDP or hi
it's very when one node goes down it has
to communicate with everybody else until
I'm down now right or and or remove the
data or replicate radiator on the
communication pattern in the Cassandra
system is gossip base which is a very
lightweight exponential algorithm when
gossip actually works like real leader
will gossip where you go and tell so
every node gossips with a couple of
nodes neighboring nodes or and those
notes go and gossip with other other
gossip partners so it's kind of like if
you if you every node participated in
sense to two nodes to notes go and 222
is you have a pretty large exponential
algorithm so in some sense it's a it's
it's kind of amortized over the course
of the entire size of the system so in
some sense it's exponential but at the
same time not very expensive so so the
communication between the nodes is a
very is independent is it kind of a it's
kind of like different it's a mechanism
that causes that leads leads to a much
more easier well breathes well or well
growing system but the reason we brought
up the gossip piece was if there is
actually any learning of the notes out
of saying the data can can be repaired
so as you do your reads you can also so
when you pull the data out and you do
the reeds you can actually ask i want a
i want it i want just one of them let's
say we added the data of this this
building this building's geocodes are
not changing anytime soon they're going
to be the same for pretty much history
of your so we're probably going to
it'sit's if you write once you just can
read it from anywhere so if you write a
copy of that node in three nodes you can
suddenly your reads can get 3x the
performance because they can hit any of
these nodes and get
an immediate result so for the data
that's very light very non changing you
can use a rican sense of just one for
data that you want to make sure has
higher reliability or higher correctness
you'd basically ensure that all nodes or
the majority of the nodes are saying the
same answer so you can actually say
quorum so then you get to all of these
three notes returning with the right
answer so make sense so in that sense
there is it is a tunable consistency
model but as we have today and that
allows it to the end user to program it
and learn and be able to not pay the
same cost for every week and which is
the true which is true for today's reads
where you force yourself to pay the cost
of asset for every read here you can
actually say this data the the geocode
data or the address data the maps data
it's not going to change anytime soon
that did it can be just are equal to one
but for the data where you actually do
need more consistency you can enforce a
higher consistency level and that's
that's the onus on to the pushed on to
the end user end developers and that's
kind of the thing that we discussed
earlier which was which came up but
really the crux of that is that it's up
to the programmer to design which parts
of the code can actually get take
advantage of of a very high V
performance already high right so the
right performance of this way to just
discuss the replication the right
performance is really really very low so
Cassandra's real strands shine shines
when it does rights really like orders
of magnitude faster then then it's then
the reeds and then then in the guys
usually singles single-digit early two
digit milliseconds and it's a very it's
a durable right it's not writing just a
memory the right speed of curve is based
more so on on the tree it's the commit
log is append only says so when I do a
right there's a commit log that does
happen only
sequential access to file so in other
words every node has a commit log so
they discuss so as soon as you send it
right a simple a block is is is when you
hang out a record here append only so
it's sequential so this this thing is as
soon as it finishes it allows the mem
table copy to be read right at that
point it's already memory makes it
available for reads also goes for the
other copies replication reader right
this is nothing nothing out of the world
it's classic classic log structured data
so what db2 uses for commit logs right
said it takes it fits on the great
advantages that were created in the
database research it's not something so
data is not lost but the replication
part being a synchronous allows you to
scale on that one platform as well what
does BigTable do for so we looked at the
distribution model that's the dynamo so
we're actually talking about dynamo
dismission model we also know that
BigTable brought something to the table
so that's this thing so it's more of a
dynamic column family model which
basically says you can have like so this
is basically the tables the column
families are more non-uniform column
structure so you have a few columns that
are a few rows with two columns few rows
with no columns and a few rows with a
lot of columns right so red cap how many
of you have seen the effort of the
normal forms for databases
cute right so the first normal form is
it if you think about it in the common
sensical approach a first normal form
you go to the grocery you stir you make
a list of things you want to buy and you
say milk I want you want to buy like so
everything you have like item name and I
want you want to I said you kind of
expect so what if you find one of the
rows with no amount you want to buy
you're worried because you need to find
out that data it needs to be filled up
that's here you expect a structure for
your data but what if it's just you know
it's like you know you know certain
columns certain data it's not amenable
to that structure all the time you're
wasting a lot of here both your space
and query performance focusing on having
all the columns to be similar and that's
kind of the all the rows to be similar
and that's kind of the idea behind this
but beauty about this one is if you are
designing such that you need to get only
you know that certain roles have enough
data you just go after that then so this
is showing the brisk structure and the
sander FS saturday we'll get back to
this is classic marketing slide but
we'll come back to this at some point we
looked at Cassandra brisk basically
brings to the table ability to stir
brisk data and Cassandra data in the
same ring structure right so in some
sense it provides a way to write your
Hadoop jobs as well as run the results
of those data into the same Cassandra
store so you can then run queries off of
it I will look further into how this
works okay the main things to remember
is we had job tracker task tracker every
this bunch of nodes that are tall
structures and the single notes
this is our op Center that's showing the
ring 20 nodes the dip nodes are having
the jib in it and that's the Cassandra
side see what we are seeing here is
basically historically etl you would run
Hadoop jobs on some other you know HDFS
store and take those results put it into
a table that you can then serve the rest
of the your web app from and that table
instead of being on my sequel and not
scaling can now be used as that's where
the no single source came into play and
this one you actually use the same
cluster that you have use it in both for
risk and focus on Hadoop insane so with
that you get to your question how does
this thing work right the Corvette we're
using the party soon to be apache yahoo
distribution which has the security
patches to it apache hive which is the
query mechanism at the top that it's
running on top of this cassandra 08 we
just sort released and thus equivalent
first and drift the column families that
we widely discussed how do we actually
make this whole HDFS sit on Cassandra
Hadoop file system is its own mechanism
what we've done is basically use regular
column families to write I notes and
Xbox win sub-blocks hello little code
here create to complete column families
onto the same key space Cassandra file
system so this is obvious and decrypt
flight anyways it's a very simple
nothing fancy just plated gold good
old-fashioned Cassandra file system
Cassandra column families so nothing new
nothing
out of the world the it's actually
pretty good simple once we started
thinking about it or like well why
didn't we do this before sub-blocks you
can set the caches so I shoky cache size
here you can grow that if you if you
think you can get better better
performance you can set row cash on I
know to get a lot more in memory
performance for your I know it Colin
family but that's that's that's the
beauty behind it it's just pretty good
old-fashioned it's run and maintain
operated and maintained just like a
regular Cassandra column families so all
this all the discussion we had earlier
place off now you know how it works it's
straightforward it's well well
distributed replicated and uses the same
pieces if you look at the actual
distribution how the data is actually
going to the system you have take a file
load it basically has all I know it's
four blocks blocks are distributed
across different nodes as just regular
column families that's that's the
nitty-gritty of it we actually worked
with different performance sizes for
these and came up with different sets
that work for great work where well for
most of benchmarks as well as some of
our customers to figuring out what's the
what's the best use gayest sizes and so
it's a new product in many ways so we
are making sure it's we understand how
the end customer plays with it and get
get a good performance notre on it so
Hadoop is really adding job tracker toss
tracker and all the other intelligent
nuances around around the MapReduce and
what we did is basically take those two
and put it on I mean have all every node
have capabilities run tasks and one node
today to run jobs brisk snitch right so
how do I know what is a Cassandra node
and what is a brisk inert so snitch is
the concept where let's say you issued a
right it said write this to a different
DC to region
I know that East us East good crash so I
want to make a copy in year so that's
right so we have a concept of PC to
snitch for that so where you can
actually during your right and I say
what mechanism to use for what snitch to
use for is one other column you're right
you're right so it actually goes and
make sure that the replica copy nodes
will recite or fail right sir so what we
did is piggyback on the same simple
dynamic snitch simple switch to use for
for risk versus Cassandra so what we did
here is in the same ring we've defined
logical logically these are two
different data centers one is a brisk
data center and so using the same simple
snitch mechanism we say if are you what
do you have tracker yes i'm at hacker
known for the track of node we know it
is a Hadoop node essentially so the same
kissena nodes whoever is easy tracker
there is a job or task tracker right he
belongs to DC she does not have a
tracker and must be a sand auricular
node so we just use the same mechanism
so everything is using the notion of are
you a tracker or not one of the things
that it's going to say is you want to
use your Hadoop to run set the classic
workflow and that's not one of the
questions that I didn't get your name
right one of the questions Danis earlier
was why I Cassandra and what's and why
do I need to have a brisk snitch why do
you have a brisk and a kiss and aside so
typically use cases and there are more
use cases of you're seeing customers use
it is people have a bunch of logs
machine generated data access logs that
you want to process you want to take
that data and run a large Duke job on it
and categorize where for example video
analytics logs right you have you want
you have all these click streams coming
in from all the geographies you want to
run a Hadoop job and says I hit so many
from Europe I want to hit some I hate
somebody from the US and all these
states right so that's your typical use
case so you run this Hadoop job that
runs on all these logs and gives IP
addresses translates them gets you the
answer right now you have this answer
and you know you have like a dozen
different like lots of customers wanting
to serve this look at this data right
away in an web app right that part the
second answers of this Hadoop job you
want to put in a high-scale store all
right so typically people would put it
on my sequel shot it or put it on
Cassandra and run that part on Cassandra
because they know that scales so what
this Crocs of the bris convention if you
will is it brings them two together you
can now run the brisk Hadoop jobs on one
part of your cluster and they can be
busy all the time continuously your logs
you just uploading logs it's running the
Hadoop jobs here and you scrape that
data into the same cluster on the side
so you actually can have a sander
section inside so you don't actually
take the end result of this and put it
into another data set data store but
it's right here run your app from that
in some sense it reduces how many
clusters you as a ops team or a design
team have to maintain and different ways
to manage them so it's just one thing
and then you can you can increase the
size add bootstrap with based on how
you're using it so make sense
essentially the end to end workflow we
looked at the end to end workflow and
then figure out what is an interesting
use case for it so we look to the nation
there is high so high presents a
classics sto like access and to add to
the whole Hadoop ecosystem where you can
run a bit large query upload data run
large query on the
dube stuff right now there is pushed on
predicates which is coming up in beta 2
which is basically allows you to slice
and so I cut down your slices and
partition it upfront now here's a
typical and this is a very small on
select by the way it is usually you can
done do a large this part of the actual
example on our website so we want you to
play with it after you've done but
basically you can load data through a
file around a lot of queries right so
here's where your question on how do i
get joins and cassandra is hive right
it's not the same it's not it's going to
translate that select to a MapReduce job
and come back we're rapidly much faster
than other ways but there is no since
you never had a join this is one of the
first ways you can actually get some
sequel like access to to the Cassandra
ecosystem hi again is a project from
facebook as most of you forever off so
that's that's the gist of what we are
trying to do is to get you cross data
center support and that's I mean
historically this was never available
for ordinary people high scale like
ordinary people as in financial services
guys would do multi data center our big
companies but you can do that to two
nodes right you can you continue to use
your Cassandra Collins family is cosine
the CFS which is the column the sander
file system and that some future etl
will completely become blender the three
all times not there yet is in a real
time but but it's kind of the vision
that we have is there kind of getting
there but it's not there right here's
your job tracker using the opscenter so
when you run a terrace art you can
actually see the progress kind of here
you still get your old job tracker we
don't take that away but this is well to
see progress it's a huge team effort all
hands on deck we basically around the
turn of the spring we had announcement
want to get go mate brisk happen will
make Hadoop happen we integrated the
entire team pretty much all and all ends
of it made sure we're it's QA done well
tested documented but that's the whole
team so none of this has been it wasn't
easy but we made it happen we have a
large user base effector we also have
customers writing clients in non Java
right they also let me cast picasa and
HP cursor both of them are part of
datastax teams so we also have skala
wines C C++ is not uncommon if customers
who've used that and people are not
leaves look to the fifth API very
created the Kree space in the column
family that is the thrift api's it's
it's converted but it's there it can be
used it's all java so in some sense
people are not armed but it is in some
sense that's the focus for us is to make
the API even simpler or so it comes is
here here's a hundred node brisk cluster
let me ran during our tests we kept it
running for a long time enough to pawn
some good easy two hours but what the
APIs so the evolution continues we have
taken that initial facebook Cassandra
I've taken Hadoop five a brisk it's
moving along to get you much more robust
stack that's what that's the I mean once
you have enough nuances you're able to
to use it in a more
on solid fashions our job is to and push
that to push the envelope there see how
the customers kisses play pig is now
part of the new the same stuff so you
get the whole eventual stack which you
need you need to do data analysis get
started it's open source all Apache this
risk is completely when we we are
encouraging lot more work from the
computers from the from people
developers users brisk EMI itself is
available I was the Wi-Fi here filled on
us but it was part of the demo this one
all right to thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>