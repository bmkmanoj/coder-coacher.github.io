<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Panel: SQL over Streams, Ask the Experts | Coder Coacher - Coaching Coders</title><meta content="Panel: SQL over Streams, Ask the Experts - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Panel: SQL over Streams, Ask the Experts</b></h2><h5 class="post__date">2018-01-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ViRyL15RjmY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the sequel over streams panel
there's no real set agenda here this is
just sort of the chance for folks to you
know ask questions of us regarding
streaming sequel I'm probably gonna
start out with an initial question just
to kind of get the ball rolling but I
hope you all have questions to ask and
before that I'm gonna kind of do a my
attempt at introducing everybody here if
I get things wrong feel free to correct
me but this'll at least get us through
the introductions relatively quick so
starting on on that side we've got Jake
reps CEO confluent Patchi Kafka PMC
recently no yeah okay you know Kafka
recently came out with K sequel so this
is sort of an interesting development in
that space next up we've got Julian Hyde
so he's a PMC member of a catchy pal
site Apache calcite which is Apaches
sequel parser and optimizer system was
also a co-founder of sequel streams and
is now on architected Hortonworks
and we've got Michael Armbrust he is a
software engineer data bricks the
original author of spark sequel and also
an apache spark BMC member Stefan yan he
is the CTO data our data artisans patchy
flink pmc member link itself has had
streaming sequel in it for what a year
and a half or something based off of
calcite yes to varying degrees of
sophistication and a nun Tyler Aikido
I'm a software engineer at Google
Apache beam pmc member and beam
similarly has a sequel DSL that came out
in our last release of the 2.20 release
so yeah so I wanted to kick things off
with an initial question and then we'll
kind of see where that evolves and see
what questions we have from from the
audience but just as an initial sort of
starting point the idea of a stream
processing over sequel has been around
for decades right it's never really
caught on well in my argue
and or my in my opinion so my question
is you know what what are you folks why
do you think that is like what do we
thinks been missing what are the
challenges ahead you know what's what's
resulted in this and we've got we've
only got four mics for five people so
you know be friendly and if somebody
wants to talk hand them around and I'll
stand here with my my mic on so go I can
give you an answer yeah I think it's a
super interesting question because this
is a good example of something that
happened in academia that didn't really
happen and now seems to be happening for
real I mean it's too early I said that
so it's like actually academia working
the way it's supposed to work where it
foresees future developments researches
and publishes papers that will go back
and read so you know I have a couple
theories on why this is I you know one
theory is that you need the people who
worked early on on the sequel streaming
we're coming very much from a query
optimization point of view and you know
it's kind of very natural once you think
about these like volcano query optimizer
dataflow things in a database you're
like oh we could stream data through
that um and the problem I think it ran
into as a product was just people didn't
really have ready access to data streams
or not in a way where query processing
was the most immediate problem I think
that was maybe partially I mean Julian
was involved I think prior to calcite
like in a sequel streaming startup and
so it probably knows a much better
answer to this but I think part of this
is like there is just actually a lot
more streams of data you know like a lot
of the IOT use cases that didn't exist I
think we take operational data streams
about the operation of machines much
more seriously you know computers I
think the instrumentation of
applications is now producing data
streams you know I think it's possible
to start to think of a lot of these
databases is generating data streams and
so
you know I think having that and then
having it in a way where it's readily
accessible so like I was previously
LinkedIn we actually started working on
Kafka because we wanted to do stream
processing but we were like well
currently there's no accessible streams
of data the first thing we need is some
kind of like platform for the streams
and then we'll do processing on top of
that and that you know so we kind of
fell down this next layer and so I think
to me those are the two layers like you
kind of need the stream storage before
you need the stream query processing and
then you just the prevalence of streams
of data and the relevance of that to
businesses as businesses kind of get
more digital you're trying to react
faster and use the data more naturally
if you're trying to make reports for
people then obviously the people aren't
only going to look at the reports so
often like they don't need to be more
real-time it's really when you're trying
to make more continuous decisions so at
least to me that those are kind of the
big ones it was one of those things that
I feel like 10 years ago almost close
enough and then just didn't quite quite
get over the hump we'll see if they if
it happens this time my sense is just
from the momentum in the area probably
will so just to kind of echo some of the
things he said I think there's a couple
of different factors that I've seen at
least from people who are who are using
spark streaming so one is availability
of data I think it's exactly what you
said like the ecosystem the fact that
Kafka exists and then all of the systems
that look a lot like Kafka Kinesis and
pub/sub and all of those things now that
that data is available in a streaming
fashion now I want to answer questions
about it so I think that that's maybe
one of the things I think another is
scale there's a wide variety of
questions that people want to ask where
if you were running these as standard
reporting queries and scanning all of
the data from the beginning every time
you wanted to update it that you just
wouldn't be able to do that anymore
and so if I compute these answers
incrementally as new data arrives I can
always have my my report up-to-date I
think that and just that that scale of
data didn't exist before yeah
just for the sake of just just
disagreeing a little bit because I think
there's actually um actually stream
sequel has been around also ten years
ago it's just not called stream sequel
and what was called stream sequel is not
what we think it is today so beg back
then a lot of the academic
work on stream seeker was had a lot of
this time serious kind of as aspects to
it and those were I don't know that this
didn't maybe catch on all that much but
they're like exactly what you were
saying having this like incremental
evaluation of certain queries over
data's in some sense what the database
is internally did is as materialized
views a materialized view maintenance
which incidentally is like at least for
us was the inspiration for streaming
sequel right so there there was
something like streaming sequel actually
was just not called streaming sequel and
it was something that wasn't dumb it was
something that was not not open right it
was something that was like a
particularly piece are built into a
sophisticated in a warehouse or so and
in some sense what what's now happening
is kind of pulling this out as so many
things have been pulled out right like
as the transaction log has been pulled
out as the data processing has been
pulled out
no not this is the next piece of data
processing that is being pulled out the
incremental straight and streaming
sequel evaluation so I see two factors
one is so I was I was here you know the
first time around the sequel stream was
founded in 2003 and yeah we were trying
to convince middleware departments to
use this stuff and they were they were
they were Java folks I mean they they
saw they had to UM be a VM hue series
and JMS and all this kind of stuff and
they saw the stuff flowing over streams
as kind of serialize Java objects or
perhaps RPC parameters or RPC results
they didn't think of it as data and I
I credit J and Martin Clinton for doing
the evangelism to tell the world over
and over and over and over again
this stuff is data it's not Java objects
it's data it's the same stuff as is in
your database so that's one thing the
other is the value of the data so
probably 20 years ago a it's really
worthwhile to think of what is the value
of this record that is one minute one
minute old and what is the value of this
record which is one month old in terms
of cents or fractions of a cent and 20
years ago or even 10 years ago
enterprises were not so real time
you know they were having difficulty
just consuming the data from the
previous quarter because VI technology
wasn't that advanced and now you know
now the enterprising is in the position
to consume this data and act upon this
data that is one minute old or one
millisecond old the only people doing
the real-time stuff 20 years ago were
Wall Street and they were building their
own technology there was no interest to
producing general general purpose
technology so those are the two things
that have shifted I think and we're in
the in the middle of or at the start of
this kind of you know exciting journey
to bring streaming data to the data base
community and all that so all I try to
add two more things I may be repeating
what you said because I was not paying
attention as I was plugging in my laptop
so the slides would come back but what I
think is another facet of sort of
streaming sequel has been has been
around and been used as materialized
views I think is actually a good sort of
example of like those things get used
constantly and really that's a form of
sequel stream processing right it's the
it's the simplest to use most understood
version of stream processing out there
really and there's broad use of it so
from that perspective it actually
already is there it's just when folks
want to actually operate on the output
of streams and not as tables that's one
that's kind of where things have fallen
down and so I think the other aspect is
they just I don't think you know like
all of the the previous systems that
have tried to tackle this before this
current generation I feel like they just
didn't quite get there like they were
like 99% of the way there but it was
just that last last little bit just made
it not it just didn't quite fit together
well enough it wasn't you know quite
useful enough that that's kind of part
another part of what fell apart at least
is that that was the sense that I got I
wasn't actually involved in it but you
know sort of retrospectively I think
another thing that may have changed is
open source you know I think it is
actually hard to make really practical
systems purely in academia I mean people
try but the contact with the actual
problems on what would make it usable is
less and then the difficulty of getting
like really novel infrastructure start
up off the ground with pure proprietary
code it's just difficult right because
people are like well you're a very small
company I don't really want to bet on
this it might not be a
I think open-source really allows these
things to get out there faster and kind
of spread you know you you can kind of
find the early adopters on its own I
think it's actually a pretty powerful
mechanism for getting new ideas out in
the world and I think it would certainly
be much harder for this current
generation of things that we were
building them purely as proprietary
systems I'm just trying to go
door-to-door and sell it all right so
audience question there may be all maybe
all run mics thanks a lot so I'm I'm
sort of getting the sense that we're now
getting there but to me and I hope this
will trigger you it seems a bit too good
to be true the materialist materialized
views I get but what's now the essential
difference between between the new state
and materialized views because I hear
Michael say well but yeah sure you
instead of continuously building up your
view from all of those from all of those
events which is impractical you can just
snapshot your answer and always have
this answer in place but that means you
effectively need to build up already
your query continuously know what you're
doing and what's the difference with
that and a materialized view and and
also it appears to be a rather
monolithic in a sense that there is a
single truth and I can query it and I
get the truth but with our amounts of
data that we're talking about in these
use cases that you're describing
I cannot kind can't kind of fathom that
so I mean I think the thing that's
different I mean it is a materialized
view but it's kind of exactly what
what's different was just saying we've
exploded the database and we've taken
key pieces out so I can take that
incremental view maintainer and I can
feed in you know terabytes of park' data
from s3 and have the entire the entire
feed of everything that's ever happened
and I can read super low latency data
from Kafka and then I can help put that
to Cassandra which is going to store
this gigantic answer and make it
queryable and I think the fact that
we've exploded the database and added
nice connectivity and all the different
sides is is what's kind of different
than what you got when you took like you
know a single you know black box stream
processing engine I you know like I
there were a bunch that were built on
top of post grass that word like single
node and just one one instance so I
think that's what's different yeah I
would totally echo that I think there's
a couple of aspects that are really
important so I think you know there's
lots of things you could view as just a
materialized for you and so when I was
starting on this I had conversations
with a bunch of people in academia
they're like yeah yeah we looked at that
area that's really just but like if you
think about it like Google search is
just a materialized view on top of you
know the web right it's just a think
about the mechanics of implementing
google search using like triggers and
like whatever that's not really gonna
work right like you need a platform
which is targeted at doing something
which has scalability which can have
multiple teams all working together and
collaborating and deploying code and so
on to really be able to build serious
chunks of your business that way and so
when you look at parts of a business
yeah a lot of what they do is
materialize some view on data but you
have to have like a real platform for
that and so I think you know at least
kind of our view of the world is we
really want to make it possible to have
these streams of data kind of span the
company and make that open as a
programming paradigm right so you should
be able to write code in whatever
programming language you like that uses
that in kind of a lower-level way you
should be able to have higher-level
stream processing interfaces that make
some of this easier to me
I think the sequel layers are yet
another interface that like makes a ton
of sense
this kind of thing I think you see a ton
of traction for these server lists
function as a service type things which
I think it actually has some advantages
is another interface for this but I
think the fundamental thing is really
the ability to build around these
streams of events like this is what's
happening in my business this is what I
can compute off of it and I think we're
kind of just early in making that
practical you know whenever you come to
these conferences it's like it's already
done but in reality it's like 10 percent
done and I think as it does it's gonna
be it's gonna be really cool I think
that's actually going to be
substantially different even though
conceptually it's very similar to stuff
you have in database theory or data base
papers or databases I think the actual
practical application of the point and
deployment of that in the world was
gonna be a big deal I think there was
also one part of the question so you
know you now have this this thing that
computes the materialized view you know
be that across X different systems that
end integrated in this nice you know
interfaces are way but yeah there's
there's obviously always still a
decision to make what do you want to
make as a continuous piece of
computation versus what do you want to
make an ad-hoc piece of computation
right for the continuous computation the
streaming computation you maybe you pay
L a bigger resource footprint typically
because it keeps running all the time
but it also gives you ready results
right you don't have to you don't have
to you can yeah you can also do it like
time discretize this is alright but then
again you don't see the you don't see
the results immediately there at the
same time so there's this kind of always
the straight of like do you do you keep
it running for being immediately
up-to-date would you pay the cost of
computation at the point when you ask
right and yeah I'm not sure like the the
database is actually then came up with
all these fancy tools like design
advisors what should actually be a
material ask you to compute based on
this is this is something I'm interested
in
so that should be a materialist view I
don't know I mean maybe and at some
point in time somebody's gonna write
something like that for the you know for
this echo system here's the different
pieces that I'm interested in here's my
resource Patchett and it's it spits out
okay these and these and these things
you should actually compute in streaming
fashion eagerly those things compute
them lazily on you know query time might
be but I think that's gonna be a bit
away we're still
few problems to solve before that so if
the question is you know should I do
streaming you know it comes down to that
its how valuable is this data when it's
one minute old versus one one second old
if it's you know if it's if it's still
as valuable in in two hours as it is now
don't do streaming it's it's definitely
easier to work on data at rest disagree
you play if you're gonna run this query
more than once you should consider using
a streaming system that's that would be
my argument and I think really what it
comes down to is it's really all about
incremental computation I have seen so
many people write batch jobs that run
once a day and they spend so much time
thinking about the logic of figuring out
what's new what's old what was my state
from before what happens if it crashes
in the middle and that that is exactly
what streaming systems can solve so even
if you bring up your streaming job once
a day to run one giant batch you should
still be using a streaming system or at
least a streaming programming interface
I think this is this is where you have
to start to differentiate right modeling
something is streaming like almost
always makes sense unless it's an ad hoc
thing if it you know if it's longer on
more than once I there's like one API
for describing data flow and then
there's like an execution engine under
the covers the name it is more
complicated to it is more complicated to
write a continuously operating ETL
process than a sequel a sequel script
with insert into select ROM so there is
there is a cost to this which we're
trying to reduce the cost of this but
but I think there's a hidden cost of not
having a streaming system that's great
and so devil's advocate here no I
appreciate that I think the onus is on
all of us to move streaming systems to
the point where it's as easy and natural
to do that and have you use the right
tool like our tools aren't good enough
yet but the flip side of the question is
if you are doing streaming why use
sequel and
you know a twitch which was which was
not self-evident for many many many
years people people accepted the idea
they would use sequel for database stuff
but they didn't see the point of using a
declarative language for streaming so
there's you know we're just about
getting to the realization that there's
a point to use a declarative language
with streaming and to throw to throw one
more thing in there back to the original
question of how is this different than
materialized views if you think about
materialized views is essentially taking
a table or a stream and materializing
them as a table the other part of that
that's interesting is what if you
actually want to consume that as a
stream right like materialized views are
kind of like table the table
transformations but there are use cases
like anomaly detection they don't really
fit nicely to a materialized views like
you don't want to have a table that kind
of tells you is there anomaly or not and
I'm gonna go and scan through all my
keys and find where the anomaly is right
like there are cases where you want to
consume your output as a stream and not
just a change load but like you've got
the interesting events only in that
stream that's where where the non
materialized views sort of approach
actually becomes really interesting and
streaming sequel or stream processing in
general yeah thanks
so if you have a sequel database you
have this you know piece of
computational infrastructure running and
it's packed full of data and a whole
bunch of people can just ask it sequel
questions whenever they want right in a
lot of the sequel streaming stuff I've
seen is really just a DSL for building a
data flow and you you sort of throw it
up there you start all the containers
and machines at whatever pump all the
data through it and then it starts
spitting out answers it almost seems
like there's a there's some kind of
waste there where if you were going to
make a closer analogy to the database
you would have says sequel execution
like streaming sequel execution thing
and anyone let's say in a company can
throw sequel questions at it but rather
than getting back a response right away
they get back a stream of response but
then the the execution engine has an
opportunity to take all the different
sequel questions that people are asking
it and do some algebra on them and
optimize write and build intermediate
streams and then take advantage of you
know like overlapping parts of it is
there something I'm missing or is is
that pretty much is that a question for
Julian when is calcite gonna do well
when this concept getting common
sub-expressions I think they would yeah
yeah when my friend hey-zeus gets around
he did a PhD on common sub-expressions
and Horton worked to keeping him so busy
he's not he's not got around to putting
that into calcite i-i-i i like the idea
of sequel as a language but i would i
would love to see a hybrid system that
has you where you use sequel for access
to streaming data and historic data and
it's perhaps perhaps it's backed by a
combination of HDFS and Kafka and data
in memory and you know a variety of
storage systems and perhaps a variety of
engines but it's one interface I I think
one of the most compelling reasons to
use sequel for this stuff is that you
can
look at a stream and the history you
know the the the last ten milliseconds
of a stream but also the last ten years
of the stream in the same query
combining you know historic data and
streaming data in the same queries is a
very powerful paradigm and so it's the
of course it's the you know optimize the
job for life
it's the optimizers job to figure out
where is the best place to get this data
from that they're asking and yes create
recognize common sub-expressions and
perhaps create materialized views on the
fly or you know a materialized view
might just be an in-memory hash table or
it might be just realizing this data is
in I can do a time-based scan of the of
Kafka to find to find this stuff so
there's a it's all this is what got me
started does not summarize a guy it's an
opportunity for the optimizer to be
smart and and and know about data
sources that you don't know about as an
application developer because you're
only trying to write your application
yeah I would strongly suck at that I
think the combination of point in time
lookups against the data that spent
materialized as well as do different you
know the ability to control that
continuous materialization I think that
is the key value proposition and I think
it is not quite there yet because the
stuff isn't done yet because I that's my
take anyway and I think that's exactly
what people want anybody once has a
question come get a mic
hello first I'm really really excited
about this streaming technology and
that's the same time it touches about
SQL because by the way I work for a
company who deploy wind farms on the
ship and on the sea so we have like
1,200 turbines running in meeting a lot
of basically sensor data tags basically
and this is the biggest challenge that
we have because at the end of the day
the business would ask I need to do the
alarms emitting in this wind farm now we
get to that because it's available on a
on a on a on a sequel table that's
basically being backfilled by by a bunch
process so if if this presents these
opportunities I'm thinking should we fix
it here or we go to the source
should we go to OPC foundation and say
hey OPC foundation which is the
organization building all the industrial
communications could you open up and
make it streamable that's just a thought
instead of building because what I'm
thinking here is now we are building a
streaming middleware with a source that
is not stream ready so something like
that I don't it's made even not a
question but a thought that eventually
you're maybe it's like the fall
streaming because your source is
absolutely not not not stream ready yeah
I think it's a great point we thought a
lot about this when we were starting a
kafka a long time ago which is yeah if
everything was already streaming ready
then that would be great
but you kind of are in this intermediate
state I think the you know the there's a
couple things you have to do to kind of
move the world right it's like a little
bit you know if you have to do a little
bit on the supply side and a little bit
on the demand side you know like and
then they kind of bootstrap off of each
other so I think like getting really
good streaming connectors for different
systems I think is incredibly important
that kind of makes data more available
all
these processing technologies I think
increase they make it easier to use
streams of data so it incentivizes you
to you know capture and extract them I
think those two things have some
interplay like we've seen in companies
as they adopt this stuff yeah initially
it's usually for someone application
that you know maybe they're generating
custom event streams out of their
applications but over time they'll often
start to collect a lot of data that's in
existing databases a lot of things
across the business but you have to kind
of start that bootstrapping within the
company before you get some kind of
center of streaming data as there's more
stuff there there's more incentive to
add new things because there's really a
lot of value you can build against there
but getting those first few applications
off the ground is is really important
and then for the larger ecosystem I
think we're in that bootstrapping now at
least from my point of view I think it's
you know it's very likely that we've
gotten critical mass with that so I
think it'll continue to build and that
that kind of makes it where you know a
lot of people who are thinking about
devices or whatever you know
standardizing how that can connect I
think a lot of that will start to come
together over time but you kind of have
this chicken and egg problem where you
have to you have to have the central
platform for streaming in order to get
the streams but you have to have the
streams in order to want the streaming
platform and so so I think we're
literally watching that happen where
there's all these cool different ways of
processing streams of data and web store
them and I think it's kind of coming
about us we speak hopefully so maybe
maybe what you describe will happen
yeah one thing we one thing we we have
tried to do to make that possible is to
the extent you can make the streaming
layer like Kafka really work as a
storage backbone so you can store data
you can kind of upload files or whatever
and treat that almost like a stream of
events that just happens in a very
bursty fashion and still process it in
the same way yeah this goes kind of
indirection what I also wanted to say um
there's I mean even if your if your data
is provided in like bursts of an hour or
so you could still start modeling this
as a nice streaming problem it just have
a piece in your streaming pipeline that
has to happens to have a front quite
high latency right in a similar way if
your data source is actually a file
system in which like rolling files
appear once per hour you can start
actually treating this as a streaming
system it just happens to have the
latency of an hour and its current state
and then if you you know if you happen
to to be able to swap that piece with
something that's more immediately
publishing data like after or so then
you immediately get the benefits right
so you can you can actually start
working on the abstraction before each
individual link is there and you already
get quite a bunch of nice benefits out
of that for example if you you know if
you FTP files one by one you run the
queries one by one you always have the
problem of like broken boundaries or you
know there's some events in here that
might have come a little later they've
been in the previous bursts you kind of
manually have to correct this and so on
so actually mulling this is a stream
kind of gives you the nice behavior
already and then you just have you know
just keep working on the latency of each
link in there in there in the chain can
I add to that I want to give a plug for
Apache knife I who've you know thought
about this problem and the model there
is I mean you know the marketing folks
told me is streaming Apache now if I
isn't really streaming but the deal is
it deals very nicely with the problem of
Internet of Things where the data is
tends to remain on the device and the
device might be sending you know one
message per minute but if something is
wrong then the entire data set is still
there on the device and you can kind of
back to it so it's kind of lumpy
streaming it's it's mostly it's mostly
push but you can whole you can you can
switch to a pole mode of operation to
get extra data from the device if you
want to so it's it's very very
interesting mixed streaming relational
problem and I mean kefka's or awesome if
you're in the in the datacenter and you
know obviously every device is always
connected to you know every machine is
always connected to every other machine
but you know in the Internet of Things
world where connections are less
reliable power is an issue then it makes
sense to move to a slightly different
mode of operation thanks I already kind
of question so this my name is Aman and
I'm from Walmart labs so we use a kafka
heavily J&amp;amp;J knows about it and I think
the question I have is with all this big
data and querying that we are talking
about having guys actually thought about
using GPUs and GP memory databases and
all that good stuff I we looked at that
a little bit I would say our take is
this there's definitely scalability
challenges on the edge of streaming but
I I think the most interesting space is
actually the opposite side so I think
the world has been very focused on big
data there's definitely some valuable
problems which are extremely large a lot
of valuable problems aren't that big but
there's a lot of them within a company
so if you look at the you know what what
is the work done inside my company
there's this apex of problems that's
extremely large often those large
problems are not the most valuable ones
I mean sometimes there's some value
there and so at least for us we felt
like actually what people miss in the
streaming space is the ability to
actually generalize like what databases
do and really empower like all the apps
not just the big ones and actually make
that really usable and powerful for that
kind of thing and
a lot of the use cases I'm most excited
about for Kafka or case sequel or any of
the you know the stream processing they
are there they're actually not the
biggest ones in terms of raw scale
there's definitely these like freakishly
large ones where you're like oh my god
like that's just a lot of computers but
they're the ones that are like really at
the core of the business you know where
you're doing something you're like oh my
god I can't believe that they're doing
that with like Kafka and stream
processing where somebody's building
like a financial exchange or they're
like that got really core stuff and I I
think maybe we as technologists in an
industry we've we've kind of fetishized
big a little bit and I think a lot of
what I think I think that makes sense
because big data was attacking a space
that was very mature and so like
actually there was a very mature
solution to many of the other problems I
think stream processing I don't think
it's really existed I think there's
older messaging systems I think there's
just not like a mature solution in the
space and so my hope is it will actually
be something that is applicable to like
30 percent or 50 percent of what a
company does like all that asynchronous
logic that happens it runs the business
not just these kind of like insanely
large like oh yeah we instrumented you
know every little twitch that the motor
made on every vehicle in every car
everywhere I mean those are amazing
problems of course but but there's also
these incredibly valuable lower volume
things that also need to be modeled and
integrated and tied in and that data is
often the most important stuff but I
didn't really answer your GPU question I
mean basically yeah I've seen a couple
startups focus on it the number of times
I've gone to a customer and they had a
problem that was so big that it would
have been made more sense to focus on
something like much more customized I
think it is kind of on the edge of the
signal processing like more out there
problems like particle physics and
accelerators you know you go to a bank
and like there's a bunch of really
important stuff happening that's making
a lot of money but it's not like they
need like new types of CPUs to process
it may be the machine learning space is
the counter example to that like
training machine learning models
but I don't even know if that's
particularly you know the best the
earliest adopters for stream processing
yeah um just to add another point to
that in some sense yeah I would I would
also say if we look at the at the
problems that you know people report
okay we're trying to do this and we're
running into an issue here
like scale and performance comes up in
rare cases it's it's mostly around you
know can I can I actually can actually
represent these semantics in an easier
way you know can I you know how do I
operate all questions around this you
know continuously running armed systems
come up way more often than somebody
says like Amen if you can reduce my
number of notes here by a factor of five
by plugging in a GPU I would say yeah I
would I would agree with Jamis most
problems have way more tedious questions
so also in this in the stream sequel
space right you can you can start with
my optimizing that the hell out of the
system with yeah thinking about things
like like GPUs and so on where you can
start you know first trying to to think
about being being smarter in the
incrementalization and so on and it it
seems I don't know that that's my take
and in where the space currently is
being smarter in the incrementalization
for example there's still much more to
be won than trying to plug in you know
dedicated hardware so I think J just had
the quote of today in IT sector we
fetishize big i'll take that away
anyways I have a very different question
and I think two guys on stage definitely
know what my question is all about how
about the metadata because you guys you
know correctly saying it's all about the
data but I would really like to look at
the data to a lens of metadata and I
think it's not existing yet in this
space what are your thoughts yeah I mean
I I second the importance of that I
think we're kind of clawing our way up
the stack here like the original Kafka
work was how can I rely
get streams of vitae raise around and
then we had some processing capabilities
on top of that and the problem you have
using this at large organizational scale
is all about like okay what that does
this mean and what's my contract to you
this other team that's continuously
generating data as your software and
project changes like we've done some
work on that stuff a confluent trying to
represent schemas and stuff is still
very much like the you know you're one
of ten on how metadata should evolve and
I I do think it's one of the most
critical things it does tend to be the
problem that comes out with scale usage
so it's like not in the path of your
first three use cases but it's
absolutely in the path of your first 30
and it's like really in the path of your
first 300 or whatever until you can you
can definitely see that if you kind of
tour the Silicon Valley companies that
have done these big central pipelines of
streaming data and have a bunch of
applications hanging off of it I think
they've all gone through what I would
describe as schema hell in some sense
either the hell if not having a schema
and then trying to like live in a world
where all your data can change out from
underneath you every time anybody
releases code anywhere in the
organization or the hell of you know
having to figure out how these things
have all been a principled way yeah so I
mean that's definitely a huge problem
and I think if you go back to like stone
breakers paper about how MapReduce is a
huge step backwards you'd be like you
guys still haven't figured it out but
yeah I think there's there's a lot of
work going on here I think it's not just
about schema but it's also about
provenance and data discovery
what is this column mean how did it get
created and so I think I've seen a lot
of cool stuff happening there but yeah
it's definitely not something we've
solved yet but to some extent that's
actually not even a particular screaming
problem I mean this goes this is like it
data processing in this in the in this
tech in general I mean the
I my first assumption would be that the
amount of added metadata and streaming
isn't actually if you modeled in the
correct way all that much you know if
you if you go to to a high higher view
like that like that the time varying
relations and so on you probably want to
explicitly state a few things like how
to how to express time or handle time
certain amounts of you know like
retention periods going largest and so
on but I guess the complicated stuff
that I think especially you might be
also interested in and like all the
exactly all the provenance what does
this what does this particular column
affect right if somebody told me in the
in the sense of the right to be
forgotten that for that user that column
now has to be dropped matter what all
the downstream effects that I have to
recompute to actually make that happen
that isn't even a particular streaming
problem I think that's just a general
data processing problem that the mean is
so I think it's a problem of integration
it's like when a bunch of things come
together then suddenly it's like how do
these teams work independently and I
changed my thing and you think there's a
break and how what are the dependencies
between so to me it's very much the same
thing happening in the micro services
where I live like what's my contract to
you and like how am I allowed to involve
that contract over time and so I guess
the only reason it's connected to
streaming is to the extent that people
are plugging together around these
streams yeah correct myself give them
that you know like all the micro
services and data of happens it's all
some form of streaming then of course
it's a streaming thing yeah so that the
metadata is is kind of inevitable when
sequels in the picture
you can kind of tell when someone's
building your sequel system because all
of a sudden they need a meta store so
you know spark went through through this
evolutionary path right initially spark
all the metadata was in the scholar code
right it was it was known to the Scala
compiler and as soon as you guys started
building spark sequel you needed a
medical calf gives kind of well you
needed you needed to know the avro
schema but no
you didn't need any richer schema than
that and so calf gets kind of following
the same path of getting of it needs as
soon as you're doing sequel you need a
meta store and it's but it's it turns
into it's a big strength because now
you've got an organizational view of you
because the data is externalized from
the application sorry because the
metadata is externalized from the
application now you've got an
organizational view of who's using what
and you can have integrated access
control you can say J is allowed to read
this stream and this table and this view
and so forth and and and it's easier -
it's easier to do data governance so
it's one of the kind of side benefits of
sequel what is unfortunate is that you
know the various vendors are pursuing
their own major stores right now and I'd
like to see us converge so you know what
you know particularly these guys either
side of me you know it's it's it's a and
and and us - right I mean Hortonworks is
very interested in getting stuff in the
hive Mehta store I I don't want to see
the Mehta store becoming this point of
this opportunity to prevent the lock-in
because as I said previously I'd like
you know I'd like someone that's using K
sequel to be able to see hive tables as
part of their K sequel query so let's
not get let's not go down a path which
involves us being balkanized so if
you're in case sequel you can only see
data which is in Kafka that would be
unfortunate so let's work together
focused and and and and we're all about
building your own meta stores but let's
make it so that out you know my my meta
store can can read from J's meta store
and Michael's meta school so we only got
like four minutes left so probably one
more question
all right yeah
we'll make it work we have we have four
representatives of four different stream
processing engines here
can you guys compare and contrast the
sequel over streams report and each one
of them and don't and don't hesitate to
say why yours is better than the others
because we'd like to hear some power
well I'll say calcite is not a stream
processing engine it's it's a you know
I'm evangelizing the idea of streaming
sequel and there's a reference
implementation inside calcite which is
really not no good for serious use
that's true although I'm no longer with
that organization yeah I can answer I
mean I think it's early for like a
serious bake-off in the streaming sequel
space just in terms of completeness of
the products what we did a compliment
was something called Kay sequel which is
meant to just be a sequel interface for
Kafka it you know puts together the idea
of you know tables which are these kind
of incrementally changing mutable things
and streams and tries to let you work
across those two the you know I guess
our vision for it is just to make it as
complete an experience of working with
these streams of data as we kind of make
it well integrated with Kafka but I
think we're super excited and have
people using like literally every thing
represented on stage here with Kafka as
well as like you know 15 other things
including weird stuff I almost never
hear about so that like you know I do
think this is like big 10th time in the
stream processing world you know the
trying to get the you know the boulder
to the top of the hill and kind of get
it rolling I I also don't think that the
processing layer is
prone to monopolies personally I think
it's very nice to be able to get these
streams around and interface on that but
I think there is actually a ton of value
in a sequel interface I think there's a
ton of value in programmatic interfaces
in all the programming languages you
want to use and really making that a
first-class citizen not something you
know kind of half-baked I think that
there's clearly something to these
function as a service serverless stuff
that people are doing it's like you know
really people have a lot of interest in
that I think that there may be other
more out there interfaces we haven't
thought of I'm waiting for the data log
you know sequel data log our data log
streaming implementation to you know get
some traction so I you know I'm not sure
that I'm not sure it's really a zero-sum
game just because the total number of
people building around stream processing
is still you know I believe a small
fraction of what the total space is
gonna be you know I would hope in a
decade if any or all of us are
successful at what we're trying to do
Brian's being so polite yeah I I think
it's super awesome that there's so much
work going on in streaming I think like
and in fact I spent lots of time
studying all of the systems up here as
we're building different things I mean
the things that I like about SPARC are I
really like the data frame and data set
api's which is one single API for doing
doing ball streaming and batch whether
it's functional object-oriented stuff or
whether it's kind of relational link
style queries or whether it's sequel and
then I think the one thing that's my
favorite about the way the execution
engine works is all three of these AP is
compiled down to one logical plan
representation which goes through
catalyst and tungsten which we've spent
the last five years optimizing the
performance of so those are the kinds of
things that I at least I've heard that
users like about spark
okay I'm good I'm going to follow up a
bit more in the in the michael way what
do I like it what do I like about fling
I think what would I really like about
things the way it's been from build from
the beginning as like a proper streaming
engine it's it's developed an interface
that's also unified veteran veteran
streaming interface has has like
slightly different runtime operator
implementations whether you go like the
batch way or the streaming way which are
you know getting closer together and
it's actually come up with with the with
a very interesting with a very
interesting way of executing this
streaming sequel for example I think
which is different from from for example
case case equivalent and spar sequel it
really it really takes this idea of the
materialized view maintenance and
actually internally this desert there's
a ton of work to really incremental eyes
these things so you don't you can really
get an a very low latency stream of of
updates that that basically tell you how
do you have to modify your result you
have to add the record you have to
remove a record which is which is
extremely powerful you can actually get
you can actually get the the most fine
granular Delta's to your results that
you can possibly get with the lowest
possible latency and yeah I think that
that's actually a pretty cool way
whether whether this is exactly the use
case you have or not you can you know
you can always go more coarse granular
you can always try to choose to not use
it but you can actually do this you can
actually make this a scant a standing
very low latency differential query if
you want which i think is really cool
and then all I'll sort of give a quick
answer for being so so beams somewhat
different than the other folks that have
talked and that you know beam is a
essentially a set of AP eyes and a
portability there and then on you know
so what's interesting there is that you
know we can provide a a sequel
implementation or a non-si well whatever
AP eyes are provided and then give you
this consistent experience across
different platforms and another aspect
that we don't have fully done yet but
the this portability layer once the full
vision is in place well essentially let
you mix and match you know any language
you'll be a sequel or Java or Python
you know JavaScript or whatever with any
of the given runners so you know and
this kind of speaks to what Jay was
saying earlier if you know it's it's not
you you really want to be able to have
things available in the language you
want to use you don't always want to use
Java you don't always want to use sequel
and so having this full multiplex of you
can build any beam SDK and run it on
literally any runner even if it's you
know a haskell sdk that then runs on you
know a Python runner or something random
like that like I think that will be
really powerful but from that
perspective of that being sort of what
what beam is within the ecosystem I
think our approach or at least my
approach has been that we need to figure
out how to make streaming how does make
sequel particularly fitting and with
streaming you know understandable easy
to use and clean that's possible because
we really like how well it works you
know how easy it is to use and easy
understand is really important from that
perspective of where beam is trying to
sit within the ecosystem and then
there's you know once that sort of
compiles down to as Michael saying of
this sort of common execution plan then
you know from there on the the runner
scheme just kind of take it and
and let's not remember that let's
remember that the majority of sequel in
the world is not Genet written by people
that's written by tools and so it's one
of the reasons that sequel has this
amazing momentum is you know tools such
as tableau and Excel and so forth if you
make your system speak standard sequel
then all those existing tools will be
able to talk to it and these tools
actually most of them at this point once
at or relational sequel they want tables
you know something like tableau
displaying a dashboard so let's not
forget that you know even though the
data may be arriving using a stream that
the system actually wants to see
relations it wants to see materialized
views so that's a really important
feature of any of these of any of these
systems to secretly provide up-to-date
materialized views we're probably
technically out of time if your
questions really fast ok my questions
related to micro services and Kafka kind
of related to streaming if I omit the
data from micro services and Kafka
becoming essentially distributed data
log so can I use streaming API how is it
a good use case to generate materialized
views across data sources yeah I think
there's a hidden like phase shift I
think a lot of the stream processing
thought came out of the database world
especially the analytical database world
and the big data world like especially
had you right and a lot of those
problems are these kind of analytics
reporting problems but once you change
the latency you actually move into the
operational domain a lot of the stuff
that runs the business like is directly
in the path of the customer experience
and so I think you know the way that
group of people is different I think
that there's not really a difference
between a micro service and any other
chunk of code but we've been talking
about it that way because at least it
actually makes people think about these
use cases that are not just purely
real-time analytics which i think is
actually a subset of the value that's
capable here so yeah I mean I guess our
view for this is really making Kafka and
these streams of data a kind of a
central hub that things can form around
and you can basically implement a micro
service in a program that subscribes and
transforms these streams and you can
have you a micro service as something
that takes streams of events and outputs
you know streams of output events or
makes changes in the world elsewhere and
you could implement that transformation
you could implement it in you know the
processing layer in Kafka there's
streams API you could implement it and
you know any of these other systems up
here that and you know maybe some are
more convenient or they're suited to
your language but that that can be a way
you can kind of build your core business
around this and you could actually view
a business as something that works off
these streams of events and reacts to
them and does stuff I think I think it's
actually a powerful view just just add
one sends in the stream see for context
of that stream sequel is certainly not
just something for analytics right I
mean an an application like a micro
service that is built on streams can
have a port where it it you know
computes some metrics in a simple
fashion because that's just a very nice
and efficient way to define it and then
these these metrics are consumed by is a
business logic just in the same process
maybe you know compile it down to the
same syntax tree and everything right I
mean it's just a sequel is but in no way
in the stream sequel space something
that's purely limited and a living yeah
I didn't mean to imply that I just mean
very often people jump from stream
processing to real-time analytics and I
think that that that connection it
should be less strong in people's heads
when you look at compelling use cases
some of them are in analytical mislike
they're aggregating stuff but they're
often not what people think of as
analytics they're not built in the same
way analytic things are built they're
also really not consumed by humans
looking at a report yeah yeah I mean
sometimes it is actually the software
that runs your business all right so I
think we're out of time and beyond so
thank you all for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>