<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learn About Wikipedia.org Architecture from the Wikimedia Foundation | Coder Coacher - Coaching Coders</title><meta content="Learn About Wikipedia.org Architecture from the Wikimedia Foundation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learn About Wikipedia.org Architecture from the Wikimedia Foundation</b></h2><h5 class="post__date">2011-04-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6r4qHVawVNQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm a fairly recent hire at Wikimedia
Foundation I've been working for them
for about six seven months now as a full
time employee and maybe a contractor
before that you know for about a year
but I've been volunteering with them for
the past six years or so and at my last
organization
I used the information from the
Wikimedia Foundation to architect the
the site that I was doing there as well
because the documentation that's put out
by what can be a foundation and the ops
engineers and the developers is really
good and it's really useful to building
a very large-scale website that's mostly
read optimized these are the topics I'm
going to cover and I have to apologize
because I kind of crammed three talks
into one and they're all really long
talks so the I may not actually get
through all of this and even if I do it
might be rushed and um or I might go
overtime so a little upfront information
these are the top five web presences in
the world
we're number five in comparison for
users if you notice we're very similar
in the number of users that we have on
our sites but for every other comparison
we're dramatically smaller our revenue
is much much smaller the number of
employees we have is much smaller and
the number of servers that we have is
dramatically smaller and the number of
employees that we have mentioned here is
not operations engineers this is all of
our employees of course these numbers
are mostly a guess in some ways because
not all these companies actually publish
all of this information but it's a
fairly decent estimated guess our
operations themselves were currently
managed by about six operations
engineers that includes volunteers we
have a couple two or three volunteers
that work on our sites which of course
are not full-time they're volunteers so
because of this historically we've been
operating in very ad hoc
fighting kind of mode something's always
breaking the traffic on our sites is
continuously increasing and historically
the traffic was exponential so we always
had something breaking we always had to
do very clever hacks to make the site
work while the traffic was increasing by
so much with so little resources and and
essentially no money and no manpower so
our technical staff of the six of us
were across the entire world we have
three people currently in the United
States one of which is moving in Germany
soon we have someone in the yet we have
someone in Amsterdam and we have someone
in another person in Germany we have
someone in Australia this means that
there's always someone awake but there's
never anyone on a call because we just
simply don't have enough people to have
an on-call list this is problematic
especially because the person that's
awake Nessus isn't necessarily the
person knows how to solve the problem so
the way that things often work is when a
problem occurs we all wake up we all get
called in so there's a couple ways we're
hoping to solve this first of all we're
hiring if anyone's looking for a job and
has qualified skills and the other way
is by trying to get more volunteers to
do operations engineering work this of
course is difficult because it's hard
for us to give route out on our systems
to volunteers but historically we have
so it's not necess
we're not necessarily saying it can't
happen but later I'm gonna talk about
one way that I hope to be able to have
more volunteer contributors without
necessarily giving out route we
communicate generally on IRC for almost
all of our communications this works out
fairly well what the rest of our
community uses including editors
developers both staff and even though
we're very geographically dispersed IRC
works in such a way
someone and they can take us back later
they can go look at the logs of the
public channels they can leave their
client up and running so they can read
the scroll back and such it's a fairly
efficient way of communication all of
our documentation is on a public wiki
for our at least at minimum our
operations work we have documentation
for the software in another site but
unfortunately we can't put all of our
communications out in the public we have
some stuff that's that's sensitive and
just can't be shared like vendor quotes
for instance or vulnerabilities and such
so we have we do have a few private
lists and resource trackers over time we
are trying to open up more of these
things that we have so that we can
document more stuff into public and keep
the private stuff limited to a very
subset of the private things
so the architecture that we're using is
basically a lamp stack for all intents
and purposes that's all it is but since
we've had such historic exponential
growth on our sites we have a number of
clever hacks around this lamp stack to
make it work more efficiently to make it
very scalable so this is our current
architecture and it's missing a few
things like it's missing mobile it's
missing our PDF servers and a few other
things I haven't had time to update the
slides in a while but this is basically
how our architecture works so the
diagrams slightly complicated so I'm
gonna go through each one of the pieces
individually the first one that we're
gonna look at is the core of our
application and application servers its
MediaWiki
PHP with a little enix so this software
of course is MediaWiki this runs all of
our production sites so even though all
of the sites are running this software
that doesn't necessarily mean they work
the same way we have a very large number
of extensions so each different project
has a different subset of extensions
installed and by doing so they can act
in dramatically different ways this is
especially true for third-party sites
that may act totally different the
differently than the way our sites work
the software is developed primarily for
our sites but like I said it's used by a
large number of third-party sites we
have a very large open-source community
around this the software is of course
PHP it's GPL and the software is very
scalable and very localizable so it not
only can be used by by anyone but it can
use it can be used by anyone in most
countries our localization has about we
have support for a roughly 300 languages
or so so since we run this on such a
large site we have to take optimizations
and we have to optimize quite a bit the
main ways that we optimize MediaWiki
first of all is caching if it can be
cached if it is more expensive to
regenerate than it is to cache we cache
it secondly we do a lot of profiling and
by looking at the hotspots in our code
we know what to optimize and of course
if there's a feature that's too
expensive it doesn't get enabled now
when I say that I say it loosely because
if it's too expensive to be run on
English Wikipedia for instance it
doesn't get enabled there but there's a
possibility that we may enable it on a
smaller project for all languages or we
may install it on a specific language
that has a smaller number of readers and
editors so for caching MediaWiki has
caches everywhere
we have caches for metadata like image
metadata we have it for user information
in their sessions we have it for our
parser cache we have it for revision
text we have it for all of our localized
messages most of this data is kept in
memcache which I'm sure a lot of people
are fairly familiar with memcache but
it's it's simply just a key store pretty
much it has very basic operations like
get set and modify for opcode caching
we're using a PC so that we don't have
to reinterpret the PHP every single time
we run it and so actually one thing we
were using memcache for our
parser cache and we still currently are
but when we upgraded from MediaWiki 1.16
to 1.17 recently we've noticed that the
parser cache is having a problem in
memory the objects are too large and so
not very much of the parser cache is
staying in memory so the hit rate is
starting to get fairly low in that cache
so one of our developers Tim Starling
has been working on a solution to take
the parser cache out of memcache and
thus ticket into e cache which is a non
disk caching system and by doing so
we'll be able to have more cash for the
parser cache but also have a larger
amount of data that can be stored what
we noticed is that the on disk cache is
slower of course than in memory but the
amount of time that it takes to
regenerate what's that what the parser
is doing is dramatically higher than
what's on disk so even though we're
losing a little late a little speed like
from latency on going from disk it's
still a much larger gain because we can
keep more in the cache and we have more
that's accessible so from the profiling
point of view MediaWiki has a really
great feature which is that you can run
pro your profiling data live at all
times and there's a number of different
ways you can output the profiling data
you can output it to a file you can
output it to MySQL and what we do is
instead of outputting it to those
methods we do we send our profiling data
out via UDP - well UDP logging daemon so
the logging daemon is continuously
keeping the profiling data from all of
our different application servers and
bringing into one central spot we have a
tool that's publicly accessible where
you can see at all times the profiling
data that's live on our sites this is
really useful for us because when we
deploy new code we can immediately go
and look and see how it's affecting the
sites we can clear out our current
profiling data and see how it's
affecting the the site real time so we
can make fixes to the code this is the
way we profile and we already see
hotspots in our code and ways that we
can optimize the code to make it faster
for our sites
of course this is core feature of
MediaWiki so anyone that is using the
media wiki software can do this as well
so next up in the lamp stack is how we
store content we're using MySQL for our
relational databases and as you notice
it's predict pretty traditional lamp
stack so far but similar to how we're at
we add complexity into the media in a
media we can by using memcache and ecash
and things like that we're also we also
add a little complexity into our
databases to make them more efficient
and scalable as well so our core
database is we optimize by doing things
like separate separating read and write
operations in a master and a master and
slave kind of architecture we also
separate expensive queries from more
frequent and cheap queries by doing
queer groups and we separate big popular
wiki's from less popular popular wiki's
on different database clusters which is
sharding so this allows us to do load
balanced reads between our slaves and
writes to our masters and of course some
reads still come from the masters simply
because they're not lagged and for data
that needs to be very up-to-date this
allows us to keep queries that are still
up to date also each wiki has its own
database that's not necessarily saying
it has its own database server
it's just that has its own database that
way we can move databases between shards
when we need to so next up is the least
scalable part of our infrastructure in
fact it's not scalable at all this is
our image serving our media storage and
for this we're using Solaris with ZFS
and this is the only proprietary part of
our architecture as well I said as I
said it's not scalable we essentially
have one server that has about 8
terabytes of data the number of requests
per second that we can actually do on
this is very low all of the application
servers mount this server over NFS and
our uploads are written there our delete
our delete
delete from there for over NFS and this
is problematic when the image server is
having issues so when the image server
goes down all of the application servers
now can't access the NFS server this
causes problems where we can't reboot
any of the application servers we can't
do any operations that end up going in
there the web server start having issues
because they're trying to read and write
into that so this can lead to cascading
failures across the entire site backups
for this take about two weeks this is
not a good solution so we have some
future plans we're going to be migrating
to an open source distributed file
system right now it's likely OpenStack
Swift we've been evaluating a number of
different solutions over the past three
or four months maybe and this one's
looking good so far some of the real
good advantages to this is unlike NFS
where all of your servers now lock up
when your NFS server goes away if the
Swift server goes away you have graceful
degradation in your services you might
not be able to get to your images but at
the same time the site should still
render the text should still be there
and the entire you won't have cascading
failures across the site because of this
and that's because it's using a REST API
over HTTP we're not doing a block
filesystem
it's just calls to the media also it's
distributed so unlike the system we have
right now where it can't do very many
requests per second if we need more
requests per second we add more servers
we can load balance between the servers
using the same solutions that we
currently use etc so we're hoping to
have this up very soon we we actually
replaced one of our media our thumbnail
servers recently that was running
solaris with linux because we had a bug
in the solaris kernel so we're very much
hoping to move to this soon because
we've been fire fighting this for a
while talking about thumbnails this is
actually one of my favorite hacks in
media wiki it's kind of cool
we noticed that stat operations on files
take too long to run so instead of doing
a stat to see if a file exists we use a
404 Handler and a web server
when someone says I want a thumbnail for
this piece of media it just goes and
assumes that it exists if it doesn't
exist it sends back a fourth
well the thumbnail should always exist
if the media that it's looking for
doesn't exist we set a 404 if it does it
creates a thumbnail and sends it back
this eliminates a stat operation next up
is how we scale our core application the
caching like the way that our sites work
and the use case for our sites is that
as I mentioned earlier we have about 400
million users but only about 200 200
thousand of those are actual editors of
the site so this means that the majority
of it the traffic to our site is read
which is a fairly easy case to optimize
so by using caching layers in
publication servers we can scale very
easily and cheaply so what we're what
we're using for the majority of our
caching is squid it's a reverse HTTP
proxy and we have this split into two
groups we have text and we have media
the text is most of our HTML content
it's when you go to Wikipedia and you
see the main page all of the text on the
page that's coming from squid then we
have the media group which is large
images and thumbnails so all of the
images that you're seeing on the front
page of media wiki are coming or most of
those images are coming from from the
media group so each one of our squid
servers has about four discs they're the
siik io limited so the more spindles
that you have in these servers the
better they have about 8 gigs of memory
apiece some of our new servers have more
than that they're also memory limited so
the more memory the better since we've
started using carp our hit rates for
text are about 95 percent and our hit
rates for media about 98 percent so
squid really delivers almost all of the
traffic for our sites about 7 percent of
the traffic is not
so for carp the way that we do our squid
infrastructure is that each squid server
has two squid processes running we have
a carp squid and we have a caching layer
squid so in this diagram I'm showing two
data centers and in this data center
depending on where the user is they're
either gonna go to Amsterdam or they're
gonna go to Florida so let's assume
they're going to Florida right now so
when the client makes a request for a
resource they get directed into our
squids they hit the car player first if
the car player has the the resource in
its cache since it has a very small in
memory cache it directly returns the
resource if not it takes a hash of the
resource URL turns it into a number and
that number correlates to one of the
back end squid servers the caching layer
so it then sends that resource to the
same squid the same squid server every
single time it gets that that request so
by doing so and not the other squid
servers in the back your your cache is
way more likely to hit similarly if they
come in to Amsterdam which is our cache
in cluster they'll hit the carp layer in
the front which works similarly the way
to the one back here so let's say for
instance the resource isn't in the front
and squid it'll hit the backend squid if
that one does not have the resource
instead of it talking to the front end
squids in Tampa this also acts as a car
player and that car player talks
directly to our back-end servers in
Tampa which also majorly increases the
hit rate so when talking about cats
always have value so when you're going
to a Wikipedia page and you're browsing
around and edit and you go and fix the
typo and you hit save what you expect to
see is your is your edit immediately you
can't wait for it and because of that
it's unacceptable for us to use
expiration times in the pages we have to
do immediate cache invalidation and we
used to do this by hitting each one of
our squid servers in
order and doing a purge on each one but
as your site grows and the more squid
servers you have this becomes impossible
you'll never be able to cash purge all
of your caches that way so a co-worker
of mine Mark Bergsma wrote a patch to
MediaWiki which is one of the only
patches he's done for MediaWiki to do
multicast UDP purges using HTTP and what
this does is when a user edits a page
MediaWiki sends a UDP packet to all of
the squid servers using multicast that
immediately purges all the cash for that
resource of course so squid is really a
front-end caching proxy that's been
beaten into submission over time to be a
reverse caching proxy it's not really
meant for it
varnish on the other hand was written
specifically to be our verse caching
proxy and because of that it's about 2
to 3 times more efficient than squid at
doing reverse caching so we are using
varnish right now for part of our
cluster it's going to eventually replace
all of the squid that we have as well
the what we're currently using for is a
is a caching group called bits and bits
is our JavaScript CSS and other small
static files that you would see in
things like interfaces we have three
servers running this cluster I think we
added a fourth recently it's very
efficient ok we're doing I think 25
thousand requests a second per box I
think that's probably about right next
up so for some of the every service that
we have is clustered for some of the
services we do at the application level
like for SQL for instance and for
memcache but for other services we are
doing our load balancing via the network
layer 3 and layer at layer 4 for this
using LVS we're load balancing for
things like squids and Apaches OBS is
Linux virtual server it's a standard
feature of the Linux kernel we're using
LVS indirect routing mode which means
when a client makes a request for a
resource they hit our LVS server the LVS
server
directs the traffic to one of the real
servers in the backend but instead of
the real servers talking back through
the controllers they talk directly back
to the clients and what this lets us do
is taking a very large number of
incoming requests that are small while
delivering a lot of traffic going
straight back out to the clients this
allows us to not have things like 10
gigabit interfaces
because the majority of the traffic is
outgoing not incoming the unfortunate
thing about using LVS is that it only
works at layer 3 and 4 so you can't do
things like load balancing based and
requests or user information the good
thing though is since we've always had
this limitation we've just simply
architected our application to work with
doing load balancing at layer 3 and 4 so
so far I've mostly talked about how we
do things in one data center and I
mentioned earlier in the squid caching
part about having it a second data
center we want our application to be
fast everywhere and so our primary data
center is in the US this makes our our
sites very fast there but for other
places in the world it's not necessarily
the case of because of bandwidth
limitations or latency and such so the
solution of course is to put data
centers or data close to the users right
now we have two clusters on two
different continents we have one in
Tampa Florida which is our primary all
edits that you do to Wikipedia go to
this data center for instance we have a
secondary cluster in Amsterdam that's
caching only we are currently adding a
new data center in an Ashburn Virginia
which should be up soon hopefully and
once we do that we'll always have two
primary data centers that can failover
to each other and we won't have this
issue of worrying about hurricanes in
Florida which is not a good thing to
think about
so in our Kashmir only data center we
only have we only have a subset of our
infrastructure we have DNS we have lbs
and we have our caching layers which
includes varnish which is not up here so
the question is how do you direct
traffic to one data center or do the
caching data center that is for this
we're doing geographical load balancing
so an observation is that most users are
using a DNS resolver that's
geographically close to them and there
are IP address to geographic location
lookup lists that are fairly accurate
and also when you're geographically
close that generally means your network
topology close - it's not always the
case but it's generally the case so what
you can do is you can map the
geographical location to a specific data
center mark Bergsma my coworker wrote a
back end for power DNS called geo
back-end this back-end takes a list of
countries and and a list of IP addresses
it Maps the IP addresses to a set of
countries which map to numbers those
numbers map to cname entries in your DNS
so for instance when a user from London
makes a request to Wikipedia rgr DNS
server says this is geographically close
to our data center in Amsterdam and so
when they make a request we send them
back a cname that says oh you're going
to text Assam's that wikipedia.org which
is our cluster in Amsterdam
if someone in New York makes the same
request the DNS servers instead return a
cname for text dot that p.m. TPA that
Wikimedia not org I'm sorry
of course this works fairly well with a
coarse coarsely distributed set of data
centers so we have one in Amsterdam we
have one in the United States since
we're doing lookups by country this
works fairly well but since we're gonna
have a data center in Ashburn Virginia
and one in Tampa Florida
this solution won't work very well for
that those clusters because it doesn't
really know which data center to send
things to this is the solution we're
currently working on or this is a
problem that we're currently working on
it hopefully well how
a solution to share with everyone on
this it'll probably end up being some
network trickery so I've talked about
our current architecture and if you
noticed our current architecture is
missing something specifically it's
missing a test and Delta development
network most of the changes that we make
a current alive site we do some testing
beforehand like we'll bring up a couple
servers we'll do some infrastructure
testing we'll push the changes out for
developers we expect them to test their
changes in their local system but when
you're testing things like this you're
not getting to test them in an
environment that's like production so
there's always going to be bugs that
creep in from this so I'm aiming to
solve this historically we haven't had
this kind of environment for a couple of
reasons one is that we just didn't have
enough money to do this the the other
reason is that we don't really have
enough manpower to do this so the first
one thankfully is easier now because due
to virtualization it's fairly easy to
bring up a few systems to make a whole
bunch of virtual machines the second one
is still a problem we still only have
six people we got one new person
recently but still it's not enough to to
read to replicate your entire
environment so the solution that I'm
working on is what I'm calling the site
architecture you can edit and the idea
is to build a virtualized environment
that's going to be using OpenStack Nova
which is an open source ec2 like
solution and this is the current
architecture I'm doing there's two
Hardware pieces
there's compute nodes and there's a
controller the compute nodes can of
course be a number of compute nodes not
just one on the compute nodes there's a
service to make virtual machines or to
manage virtual machines and one to deal
with storage for the virtual machines on
the controller is everything else
so although the controller's done like
this right now the end architecture may
have a number of these services off on
other systems so let me go over a basic
interaction of these services so to the
user there's four services that
our user facing the the one that
controls everything is MediaWiki so I've
written an extension for MediaWiki that
I call OpenStack manager which is to
manage the OpenStack infrastructure and
if you're familiar with ec2 this is like
Amazon Amazon Web Services console if
you're familiar with VMware it's like
virtual Center so the user can an
administrator on this wiki can create a
use a user account for the user when
they do so it creates the account in
LDAP after creating when it creates the
account LDAP it's also giving them
access to the OpenStack infrastructure
by making open sacra dentals it's giving
them access to Garrett because Garrett
is using el dÃ©ficit back-end and it's
giving them access to the instances the
virtual machines and that's because all
of these are using LDAP as a back-end
for authentication and other information
so after the user gets credentials from
an admin on the wiki they can log in
which authenticate them against LDAP
then they can add an SSH key for
themselves which also goes into LDAP
after doing so they can do things like
create an instance then they can create
storage for that instance and attach it
to the instance they can create they can
allocate an IP address and associate
that IP address with the instance they
can create a DNS entry that associates
itself with the IP address and when the
instance starts up oh also when the
instance is created that instance
information gets added to LDAP which is
a back-end DNS entry and it's puppet
configuration since puppet is using LDAP
for its puppet nodes so when the
instance starts up it's going to talk to
puppet and ask to be asked to join the
puppet cluster puppet will talk well
we'll ask LDAP if this node is in LDAP
if it is it's been created by MediaWiki
so it can add it to the puppet cluster
then it adds it to the public cluster
the instance then asks to have the
puppet information from puppet and then
builds itself however the user acid to
Bill itself so for instance if on
creation the user says I want this to be
an Apache server and
wanted to be configured this way by
using puppet variables that's how the
system will bill itself when the system
builds itself at the very end one of the
things that that MediaWiki automatically
puts in for the puppet configuration is
the user's Nate the username of who
created it the person's email address
and their language so when the puppet in
the instance gets to the very end of its
puppet configuration it sends an email
to the user based on a MediaWiki
template that says your instance has
been created but it does so in the
user's localized language after the
instance is created the user can log in
via SSH using their SSH key by SSH into
the Public DNS name so this is how users
will be able to create their own
instances but you also want them to be
able to modify the puppet configuration
too so we're gonna take the puppet
configuration for our live sites and
we're gonna add it to a git repository
we're going to strip out all the private
information of course but this will
allow users to be able to pull their the
puppet configuration for the entire site
and from the git repository make changes
and in submit them for submit ament's
merges for approval we'll be able to go
in the operations engineers and say yes
this is approved no this is not
automatically married or and if it's
approved automatically merge this in so
what I'm hoping to do is to have staff
developers volunteer developers
volunteer Ops people have the ability to
modify our public infinite have them
pushed all the way to the reduction
cluster so the basic use case for this
is going to be so OpenStack has a
concept of projects and each one of
these projects is has restrictions or
rights that you can give users so I can
say that in this project user a has the
ability to create instances they have
the ability to make IP addresses or to
associate IP addresses and such so we're
going to have a default cluster which is
going to be a clone of the production
cluster in this it's mostly going to be
used for testing development
and maybe operations tasks so most users
will not have root here operations
engineers will have root and and it's
going to be essentially kept stable for
new projects that we're working on for
instance we have something we're working
on right now open web analytics to
integration into the wikimedia
foundation sites this is something that
requires new infrastructure so and the
people that are working on this is
mostly software developers and not the
operations team so they need architects
should be able to do this so what we're
hoping to do is make them a project give
them rights to manage that project they
can create all of their own virtual
machines they get the environment
working the way that they want it
working and then once they think it's
ready either they can they can check
they can pull out the git repository for
puppet they can add in their manifests
their files and their templates make a
merger will merge it in will launch the
instances on the tests and dev portion
of the cluster make sure that it's
working in an automated fashion so that
we don't have to make any manual changes
to get the systems up and then once
we're sure that's working in our
infrastructure we can order Hardware get
the hardware in ready attached and then
push push out the puppet configurations
to our production cluster and bring
those systems up so the idea is we'll be
able to have developers come in build
their infrastructure from scratch move
it all the way from the test and dev
environment that they manage themselves
in to the production cluster with the
operations team only having to do merged
so of course the way that we're doing
this we're kind of running our
operations infrastructure as a software
development project because in reality
when you're doing puppet configuration
and such you're doing software
development you're doing your writing
puppet manifest this is really just
software so this is where you guys come
in we love to have help with everything
that we're doing and we're overworked so
having community participation is really
nice and this is also a great way to
have someone come in and be mentored by
someone that has experience in large
sites and to be able to involved in a
project that's very you know large and
nice so I'd like to talk quickly about
how you can engage our community and how
you can become a member so the first
thing about engaging our community is
that you should discuss what you want to
do when you come into our community and
you discuss things ahead of time it kind
of lets us know what your what you want
to work on we can we can assign the
right mentor to you we can even if you
don't want to mentor we can talk to you
about well how your design is is looking
and maybe give you suggestions on how to
change it and things like that
so after you've discussed your project
with us or your idea then you should
start committing code and you can do
that in a couple ways you can either
submit patches to us or you can apply
for commit access we give out commit
access fairly liberally pretty much we
just need to know that you have some PHP
experience and that you're not going to
break things which it's not that big of
a deal
so after you start committing it's
really important to actively participate
and this what I mean by this is so we
have processes like code review if you
submit code you should be actively
participating in the code review portion
of that if you have fix B's on your code
you should be fixing them if someone has
made comments about your code about how
its architected you should answer them
similarly in the mailing lists if if a
topic comes up about what you're doing
you should talk to them about that and
other things like that also you should
be documenting your code and I know this
is this is an obvious thing but it's
more than just documenting the code
itself you should also put documentation
on our wiki about it if it's an
extension for instance you should make
an extension page and
MediaWiki org that says how your
extension works how to install it and
things like that
also if you're making core code changes
you should be documenting those and the
respective pages for the documentation
lastly you should communicate the
changes you're doing if if you're making
a breaking change to something it's
really important to communicate the
changes because there's the possibility
that someone's working on something
they've been working on it for two or
three weeks you make a breaking change
to something and they didn't know about
it you've wasted maybe a week or two of
their work so it works really well when
you communicate with us and I mean
really communication is key for
everything in an open-source project
so like most open-source projects we
have a very similar philosophy and the
first one is engage early which the
first thing I mentioned before was the
first thing you should do is discuss
right this is the same thing when you
engage earlier with our community it
makes it so that we can help you earlier
on you we can work you in our processes
we can tell you how you're doing things
wrong or how you're doing things right
or where our documentation is for things
and such also you should be releasing
early and releasing often the OpenStack
manager extension that I wrote I release
it before it even had the ability to to
do security for basic things like
creating instances so really it was it's
not terribly useful as an extension but
it shows people what I'm doing it shows
people where I'm going how I'm
architecting it how things are gonna be
working and it's similar with release
often the more often I release my
changes the easier it is on code
reviewers they can see what I'm doing as
I'm doing it and they can steer me in
the right direction or it makes it
easier for me to fix bugs especially
bugs are gonna bite me later on I had
one recently where I was taking other a
vision of an article incorrectly which
would have given me a random article of
in the wiki and then edit it which would
destroy the text I wouldn't have no idea
how to fix this later on but someone
pointed out very early on in the process
so by releasing often I saved myself
work later on
lastly you should scratch her
so just because you're part of a
community doesn't necessarily mean that
you have to work on what we want to work
on you can work on whatever you want and
we actually encourage this so we have a
number of high priority things that we
need to get done but just because you're
doing it doesn't mean you have to so you
can work on any extension you want even
if it's not gonna be useful to us so the
way that I actually got involved with
Wikimedia to begin with was I was
working for another organization and we
wanted to use a documentation system of
some variety and I saw Wikipedia at the
time which is kind of a new website and
this seems kind of perfect all these
people can get together write an
encyclopedia together
I think our office can use this document
things so when I started when I install
the software
I noticed well the authentication system
has problems we're gonna have to have
another set of users an occasion support
to meeting with you and although this
has absolutely no useful community
well after I released this it had a lot
of adoption specifically by third-party
organizations by large enterprises and
such that otherwise probably wouldn't
available to use MediaWiki because most
large enterprises expect to have there
an authentication integrated so by doing
this it large into our community and by
large in the community it allows us to
have more developers that are that are
giving back code and with them giving
back code maybe they're gonna get maybe
they're going to put something out that
is very beneficial to us as to what
commedia foundation so I definitely
encourage you work on whatever you want
to work on it helps us so there's some
things that you need to know when you're
specifically coding for condition sites
if you decide to do so the first one is
security security is really important
it's really important people rely on us
as developers to put out secure code if
there's an extension that you've
released in a subversion for instance
that's insecure there's a number of
there's a large number of third-party
sites that may download and install your
extension now all of those sites and all
of their users although have a
vulnerability also if your
tension is insecurity gets installed on
Wikipedia that's 400 million users that
are now vulnerable but there's a
compounding effect not only does it
affect all of those users but
third-party users are much more likely
to install an extension if it's being
used on what can be a foundation sites
because they know we're gonna support it
they know it's gonna be scalable they
know it's gonna be localized and things
like that so not only are you affecting
four hundred million users but you're
also affecting all of the users of all
those other third-party sites these are
the most common vulnerabilities we see
any of these vulnerabilities and really
any vulnerability will cause your code
to get a revert which is you know good
practice so I'm not gonna go into each
one of these into detail but if you're
coding for Wikimedia Foundation sites
these are definitely ones you should be
looking out for so some general notes on
how to avoid these is first of all don't
trust anyone not even your users
especially not your users and you should
sanitize all of your input and things
like this are a lot easier if you use
the tools of the framework that you're
writing in so for instance for a lot of
the vulnerabilities that are up here if
you use some of the built-in media wiki
classes that are recommended these you
won't get these vulnerabilities at all
because they automatically protect
against them and this is part of writing
code that's the mantra Blee secure the
idea is when a code reviewer is looking
at your code it should be very easy to
say yes this code is secure
I know it's secure because they're using
this class and they're using it in this
way it's a normal pattern for us so when
their code reviewer looks at it he
doesn't even have to think about whether
it's secure or not and the best tip that
I can give for this is try to hack your
own code it's fun and it gets you into
the right mindset so when you are
attacking your own code you're thinking
well an attacker can use this to come in
this way and so the next time you're
writing your application you'll already
be in the mindset of well if I write
that way someone's gonna be able to get
in this way next Wikimedia Foundation
sites are very large it's the fifth
largest web site in the world
and because of this we'd get a lot of
requests so code has to be performant
and it has to be scalable and by
performance I mean it needs to be able
to run quickly and it has to be able to
run proficiently by scalable it has to
be able to run quickly when you have a
very large number of requests and you're
on you're on a very large data set and
you're scaled across 100 or so systems
this is an example of a problem we had
where the code we were using was not
performant or scalable and this is
actually varnish but we had a patch to
varnish that does geoip lookups we
during our fundraiser we we serve
fundraising banners and we wanted to
send the fundraising banners in such a
way to where if we were going to do one
that was us centric we sent it only to
the United States or if we want to do an
ad campaign that is or a fundraising
campaign that is specific to Japan then
it will send that those banner and those
banner images only to people in Japan so
the patch itself had a memory leak which
it's a really really small memory leak
but when you're getting a large number
of requests the memory leak has serious
problems so if you notice this flat line
is the normal operation of the server
this is where we install the patch and
this is one week time and one week at
eight up 30 gigs of memory we didn't
notice this in time and so once it hits
WAP the number of the the load of the
systems that it was on went to a went to
a 3.5 K which is really high this is a
site outage specifically when our bits
cluster went down
it caused cascading failures in the rest
of our services so the entire site went
down because of this here's where we
restarted the the servers and reclaimed
all the memory and over time it went
back up we noticed that we reclaim the
memory and here's one time we were
didn't notice again the site went down
for
thirty seconds or so but still it's a
site outage because of something like
this one more week went by then we had a
hackathon in in Washington DC and we
decided one of the things we're gonna
fix for this is going to be the memory
leak and it went back to normal
operation if you notice here the amount
of RAM used is still twice what was
originally used but that's actually
normal we when we added this patch we
doubled our traffic to those clusters or
to that cluster we fixed this later on
by caching the G IP lookup cache or by
adding a cache for geoip lookups so for
scalability and performance basically
you do the same thing that you do for
the hardware infrastructure first of all
you cache things that are expensive
second you profile your code so that you
can see what hotspots are in it and then
you can optimize those pieces of code so
if I can say one thing if you're going
to code for Wikimedia Foundation sites
it's this last one ask for advice
optimizing your code and doing caching
is difficult it's complex and we have a
large number of people in our community
that are experts at this we love to give
advice for this if you're gonna be
running code on our site we're gonna
have to do this at some point in time
anyway so the earlier on you ask for
advice the easier it is for us and not
only that you get to learn you know you
get to learn how to do these kind of
things so also when you're running on
Winnick Wikimedia Foundation sites you
always have to assume that you're
running on a cluster because everything
is clustered this can be problematic
since your code is running in a number
of places at one time you can you you're
running concurrently you can run into
bugs you can have dead locks you can
have issues with conflicts of your code
running in two different places at the
same time which cause really really
strange bugs so some closing notes as
you've noticed we rely heavily on open
source we like using open source we find
the support and open source projects to
be better than Frey eteri projects we
find it to be more reliable we find the
response times from support to be
quicker we get to talk directly to
things like lead developers instead of
talking to Tier one support we also get
to fix the problems ourselves when we
notice
that we can fix and send the patches
upstream which then get maintained by
the developers this is something that we
can't do in proprietary software using
open-source software helps us a lot it
makes it makes us so that we can have
less people and so we're always looking
for efficiency instead of art in our
environment this is both the code by
doing caching and optimization of the
code but also efficiencies for people as
well so we're looking for more efficient
management tools for instance so we
started using puppet about a year and a
half ago that helped us we're now I'm
looking I'm writing this OpenStack
extension and making this architecture
so that we have the ability to do more
things with less ops infrastructure so
we only have three out have 25 or so
developers that are on staff we have a
very large developer community and so by
leveraging them hopefully we can take
less time of operations engineers and
move that to developers who really kind
of want to do that anyway so now that
I'm saying that we're looking for more
contributors we'd love to have more
volunteers we're also hiring right now
for a number of positions we're hiring
three or four operations positions and
we're hiring probably I guess 10 or so
maybe more dev physicians so if anyone's
looking for some work or wants to work
for a really awesome organization by all
means questions
he was asking if we've looked at
alternatives to OpenStack Swift and we
looked at a number we actually have we
hired a contractor to to go through all
of the options for us and he documented
everything in the public on wiki tech
that Wikimedia org so if you go to his
pages about his different options he has
all the pluses and minuses for the
different ones and why they fit our
architecture why they don't and also
about how easy it would be to integrate
in a media wiki or not he asked if why
we're not using cloud servers and
instead of owning our own hardware so
it's good for us to be in control of
what we do if if we're on cloud servers
we are we they're essentially tied to
that organization right so part of our
mission is to make sure that we share
knowledge with everyone having are
having us tied to an enemy is
problematic for us because although
they're it's not likely for that
organization to exert control over our
content there's there's always a
possibility but even then it I mean it's
we don't have that many servers and it's
not that terribly expensive for us to
manage a infrastructure so whether it
would be cost efficient for us to run in
the cloud or not I don't know but but
also we're our downtime is totally new
to us right so we don't have to worry
that our downtime is because of the
hardware resources of a third party
we've also optimized our infrastructure
specifically for certain types of things
so if we're running in the cloud we may
be running on a server that has ten
other things and if we're running our
database servers on there that server is
not necessarily optimized for databases
the our patchi's wouldn't necessarily
that Hardware Whitten necessarily be
optimized for Apache and we're sharing
the hardware with others so the
performance of those other virtual
machines may also cause us problems as
well it's we have a lot more control
over things the way that we're doing it
so he was asking what our what our
experience with garrett is and how it
compares to other code review tools so
to be totally honest i just started
using garrett I've evaluated
for use for this specific project and
really what I wanted was distributed
source control system specifically and
then I choose git because our community
is also looking to get as a possible
alternative subversion so it made sense
to use a tool that was similar to ours I
mean something that we're already
looking at so I don't really know I know
that the code review the code review
tool system that we're using for
MediaWiki is not something that we're
totally happy with and some community
members are also looking at Gerrit as an
alternative as well so I can't really
answer your question terribly well so
he's asking about the it's the Semantic
Web and and how we're using technologies
like this or if we are and so in general
we're not using anything somatic there
is something called somatic MediaWiki
it's an extension to MediaWiki that
allows you to add annotations and
there's a number of other semantic
extensions that kind of revolve around
that ecosystem like somatic forms and
somatic result formats and external data
and a few other extensions like that and
so although we're not currently using
those extensions and we we may never use
these extensions we probably will be
doing stuff with somatic data at some
point in time it's really useful and
specifically in the project that I'm
working on right now the with OpenStack
manager it is using semantic MediaWiki
and it's using it in such a way when
like if you create an instance it pulls
all the information from LDAP and from
and from OpenStack it takes those
different properties with them and adds
them into a MediaWiki template that
MediaWiki template since it's in the
mail system as a wiki page automatically
has all the properties associated with
that and then you can use those
properties and make queries so I can do
something like have that have over 20
gigs of storage and I want to sum it up
and I want you to show me how much
storage they're using total so and this
is just by having wiki pages in there
that are semantically marked up
it's really awesome technology
especially for very specific use cases
it's just that all these technologies
are very resource intensive
so for the semantic properties you have
to have triple stores and you have to
have all of these other things they they
add a lot of complexity and it's
difficult to easily integrate into a
system and especially from the user
interface point of view semantic
technologies are fairly difficult to use
that way as well because you have to
have some way of adding the properties
in and with Wikipedia one of the things
that we already have right now is that
it's kind of hard to edit so because
there's no what-you-see-is-what-you-get
editor and by adding in semantic
annotations into that already complex
markup it makes it way more difficult so
there's a number of reasons that we're
not currently using that we hope to use
it soon he was asking if we use test
units we're using PHP unit and selenium
so those are projects that are currently
being worked on they're still um they're
kind of their infancy stage we don't
have smoke tests for instance a lot of
we don't have very much code coverage
but if that's if it's something you're
interested in doing we'd love to have
help with it he's asking if we have the
ability for third-party applications to
pull data from from us using api's or
something like that
we do have an API we have both a read
and a write API they export in a number
of formats
xml l Jason Hammel etc he's asking where
we get information about the api's if
you actually the API is self documenting
so if you go to either either install
MediaWiki and look at the API class or
just hit Wikipedia from the API it'll
print out HTML documentation or
MediaWiki dot org should also have
documentation about how that use the API
so he's asking about the are you asking
about the content license okay so to
reuse the content all of our content is
Creative Commons by attribution I
believe and so to reuse that content you
just have to what yeah you just have to
go by that license we I think we have
other things uploaded as images and such
that are not Creative Commons but it
depends on which pieces which resources
you're using so the content is that all
the content has licenses associated with
it so just have to follow the license of
the content
he's asking if if the application
MediaWiki is doing all the rewrite
queries itself yeah we have the
databases can you can tell it which
servers are read which ones are right
which ones are the master and which ones
are slave you can make query groups and
such I'm not really familiar with that
part of the code but that's last I look
that's how it works so he was asking how
editing in different languages and and
just I guess dealing with the interface
as well as done so we have projects for
each different language and the
interface for those projects is in that
language for the localization of the
software itself that's done by a
community called translate wiki net
which is not a wicked may need a
foundation project but they they
translate our software they translate
akka Wix I think they do wiki extensions
as well yeah again and in a number of
other applications but they're like
Wikipedia there they they translate via
wiki and it's open to most people to
edit so he's asking if there's any large
high profile next-generation things that
we're trying to tackle yes so one of the
things that were the large one is going
to be a parser rewrite that we're hoping
to have completed at some point in time
that's going to be a reusable parser
that has hopefully a formalized language
and such but we'll see how it how it
turns out the other one is what you see
is what you get because that in
MediaWiki is has been a very large
problem for a very long time
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>