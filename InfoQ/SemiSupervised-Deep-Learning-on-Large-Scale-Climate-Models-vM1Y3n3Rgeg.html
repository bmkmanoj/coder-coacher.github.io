<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Semi-Supervised Deep Learning on Large Scale Climate Models | Coder Coacher - Coaching Coders</title><meta content="Semi-Supervised Deep Learning on Large Scale Climate Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Semi-Supervised Deep Learning on Large Scale Climate Models</b></h2><h5 class="post__date">2017-10-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vM1Y3n3Rgeg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I want to get a better sense for the
background for the scouts are clearly in
the machine learning 2.0 tracks I
presume you're interested in machine
learning but how many folks I have known
about deep learning or are using deep
learning and practice but just by show
of hands
alright cool and I guess how many folks
know about convolutional neural networks
by show of hands
alright great got it all right so I I
guess it was advice to me by by John
Langford that I start slow and walk you
through some of the basics of deep
learning and cognition near let's so
we'll start there so I'm gonna just
briefly talk about the history of deep
learning why it's important commercial
applications at this point in time and
then convolutional neural nets and then
we will get to the meat of the talk
which is about climate science why we
care about this problem and then how we
represent climate patterns using
supervised and semi-supervised
architectures and then I'll get to
scaling D planning on our supercomputer
all right so let's start with what deep
learning so for those of you who are
tracking the field you may know that
this is not the first time that neural
nets have been you know in in why well
have been popular so they started off in
1960s and at that point in time people
were essentially fiddling with mapping
input layer to an output layer by using
one hidden layer
so essentially what what researchers
used to do was to take the input
multiplied by a linear rate and then
apply some non-linearity and hope that
you'll get an output so they they
experimented with this for a while and
and essentially unfortunately early on
it was discovered that such networks
could not even solve something called
the saw problem and exclusive-or problem
and unfortunately that result was widely
publicized and a lot of people were
disappointed that that these neural nets
would not do anything complicated in the
mid-1980s a bunch of researchers kept at
it so they they started looking at
multi-layer networks so not just having
one hidden layer but a cascade of
several hidden layers the idea is still
being the same so you take an input you
apply a linear rate you apply a
non-linearity and then the output of
that first hidden layer would
get processed by the second hidden layer
and so on they also worked out how to do
back propagation so how do you take if
you're making any mistakes at the output
layer how do you move the error or the
correction rather to the weights through
the network and update the weights to
get a more accurate answer so that
seemed to hold some promise now
unfortunately for the the neural net
crowd support vector machines and random
forests were in vogue and that's what
dominated the field for the next two
decades or so till present day so this
we are right now in the midst of I guess
the third wave of neural networks and
one might ask the question of what's
different this time around there are two
important things that have changed
between the nine mid-1980s to to present
day one is that we definitely have big
data and we have a lot of it it's
complex but perhaps more importantly
there are a lot of labeled data sets
that are now available so the computer
vision community which for the first
time demonstrated that deep learning is
extremely powerful had access to
millions of label images and once you
have that then it's been found that
these systems can be really powerful the
other change is big compute so certainly
Moore's Law has been alive and around
for for several decades now so certainly
on your laptop you can have a powerful
GPU or you can have a many core CPU and
get access to teraflop classed computing
so with these two trends a lot of label
data and a lot of compute has power
essentially people started exploring
what if we start adding more and more
layers to the network so here is an
input and right at the end as the
outputs but you'll note that we no
longer have two layers but you in fact
have dozens of layers and essentially I
think what the field empirically found
was that if you can do that if you can
have a very deep network than these
these networks are extremely powerful in
terms of capturing patterns in your data
set and they've applied it now
successfully to a range of applications
I'm going to just walk you through some
commercial I guess some bleeding edge
research so the deep wine startup was
acquired by Google and immediately they
de put it to use to defeat
the human world expert in the game of Go
Sogo is a game that's supposed to be
exponentially harder than chess and
using a formalism called reinforcement
learning
we now have beaten the korean
grandmaster and now the Chinese
Grandmaster self-driving cars are no
longer friction in in California at
least these cars are you know routinely
on the highways so certainly there are a
range of sensors that the cars have so
there might be computer vision sensors
or or depth sensors so processing all of
that data and fusing it is done by deep
learning to some extent but also the
control strategy for the car should I
turn my wheel to the left or to the
right should I press the accelerator or
the brake that's that's determined by
deep learning system as well deep
learning has certainly become quite
transparent so these days if you just
pick up your cell phone and you talk to
it chances are high that there is a deep
learning system either on your cell
phone or somewhere in the cloud and it's
going to be essentially trying to
understand the speech signal and
figuring out what question you have in
mind so so this is you know fully
transparent at this point in time but
really the the the key event that
revolutionized I would say deep learning
and made it successful was computer
vision so in the computer vision
community there's something called the
image net competition and the
competition is as follows we're going to
give you a bunch of images so these are
all natural real world images and you
the computer system will need to come up
with well you need to tell me exactly
what objects are there in that image
so given this image on the top left the
the deep learning system needs to draw a
box around a bird and a box around a
frog and so on and so forth now before
2011 the entire computer vision
community had been at it for for several
decades and every year the error rate in
this particular competition would drop
by maybe a half a percent or one percent
or so so it was really a slow
incremental field till geoff hinton
and and his colleagues in toronto
applied convolutional architectures to
dramatically drop the error rate in 2012
by you
deep learning by about 15% and
subsequently the entire field is almost
entirely shifted over to deep learning
and they have beaten up on this problem
to the extent now that the the error
rate that's reported by these
convolutional architectures is below
that of human performance so essentially
a machine can do a better job in
identifying objects and images than even
humans so I'll just walk you through so
I guess some of the basics of how how
this works in practice so typically what
what happens is that you start off with
an image that's all the way off to the
left here and that might be say 224 by
224 pixel image and there might be say
three channels typically when you're
looking at commercial applications you
have RGB chance and its 8-bit and what
you want the network to do at the end is
to produce its estimate or what it
thinks there is in the object so perhaps
there is a a vector with 100 classes and
it's going to give you a probabilistic
estimate of what class it feels most
confident about so the way you set this
up is you know the very first thing you
do is to identify training data and the
more of it you have the better the
chance that you're gonna converge
something interesting so training data
means that you need a lot of images
preferably hundreds of thousands if not
millions and you'll need labels on what
exactly is in the image to begin with
somehow you need to decide on what
architecture makes sense for this
problem and that's where some of the
magic certainly is but let's say that
you have some heuristics or some other
people in the field have already filled
with similar data and you have a
reasonable starting point so so you
essentially determine what architecture
makes sense for this problem
now once the architecture is set what
you'll do is and typically the
architecture consists of different
layers we're filtering and pooling
happens so I'm just gonna visually I
mean these are standard image processing
terms but essentially what you do is say
you have an input image with three
channels you're going to define some
filters they might be say five by five
and you're going to slide those filters
across the image till it produces some
feature map output
so that's standard convolution and then
there are some nonlinear nonlinearities
such as pooling so you take an image of
affair and then in this case we're going
to apply something called a max pooling
operation so I apply a max pooling
operation to these two by two pixels the
max value is nine and so on and so forth
so generally speaking those are the two
kinds of operations which are applied in
these convolutional architectures
so essentially you you design a cascade
so you have convolutions and then max
pooling convolutions max pooling all the
way till the end when you have a fully
connected layer now you are the key
thing here is that you are not going to
be designing these filters by hand and
this is a big big difference between
what the computer vision community did
for thirty four three decades compared
to what the deep learning crowd tells
you the computer vision community
thought that it could by hand design the
world's best pedestrian detector and the
world's best you know tree detector or
card detector and so on what the deep
learning folks said no we're not going
to do that we are not going to be in the
business of designing filters we're just
going to initialize these weights
randomly and then we will take our
training image we will pass it through
this entire network and we will get an
output now chances of course are high
that well if you start off with the
random weight you'll you'll get junk so
you will make an error write it right at
the end but because you know what the
truth is because you know what the label
is you can compute what the error is and
then you can map back propagate you can
figure out the gradient of the error
with respect to the layer and then you
can back propagate that gradient through
the network and adjust the weights so
that hopefully will do a better job next
time around and then you do this
millions of times as many images as you
have and you hope that you know you can
version on on a reasonable architecture
so near so we start off with random
initialization we do a forward pass
filtering images and making the
prediction we compute the error because
we know the truth and then we do the
backward pass we compute the gradients
and update parameters now what got
people really interested I would say I
think intellectually
was that if you apply this technique at
this procedure to real-world images you
get filters that are really interesting
and they make a lot of sense so right at
the very first condition layer
essentially what the network learns are
low level Gabor filters and
neuroscientists believe that this is
what our visual system does so these are
all oriented edges at different scales a
little bit further the completion in in
the conditional layer essentially what
what the network learns are compositions
of these low-level features so simple
texture patterns move further along in
the architecture the network seems to
pick out object parts so it'll take
these relatively simple texture patterns
and then learn how to compose them to
make parts so you learn the
representation of a clock or the wheel
of a car or the face of a dog and so on
and right at the end the network
essentially learns the notion of an
object so perhaps there is a ship or an
animal or what have you
an internetwork now this was really
interesting because this is not
prescribed in any way unlike what the
computer vision community had done for
three decades there was no notion of a
prescription of this is these are the
filters you need to have in order to
become you know in order to design the
world's best car detector all right so
so hopefully I think that gives you a
sense for just the basics of these
architectures what's involved so I'm
gonna now transition to you know from
commercial applications to climate
science so I'm as Supes mentioned Simon
I'm at Berkeley Lab and you know we're
really not in the business of making
money we care about basic science and
and fundamental questions the the
science question in this case is we know
that the climate is changing
you know politically it may be
convenient or not is a different
question but but the climate is changing
and instead of talking about maybe a a
simplistic quantity like the change in
temperature by a degree or two degrees
or a change in sea level the question
that we want to ask here is how is
extreme weather going to be changing in
the future so what do we mean by extreme
weather
so for this city at least I guess some
of you might still recall hurricane
sandy and and what happened when sandy
made landfall so this was a storm system
that started off in the tropics and it
started propagating towards the
Northeast now unfortunately there was
another storm system to the right of
this figure which prevented this storm
from typically veering off to the top
right which is what it would have done
normally but instead it it made landfall
with catastrophic effects so certainly
people on the coastline individual
homeowners on support' were negatively
impacted certainly also impacted a
number of landmarks I guess in in the
area but you know city like Manhattan
was was at a standstill for for several
days now a city like New York again is
very resilient the economy's really
strong so it can always bounce back but
you know hurricanes do not discriminate
I guess I almost a decade back now
Hurricane Katrina made landfall in in
New Orleans which is perhaps
economically not as strong a city and it
almost I think I would say it took a
decade for the the city to recover now
both of these events sandy and and
Katrina are order a ten billion dollar
of events I mean that's the amount of
damage they caused but really it's not
just about the cost the dollar amount of
the damage it's really the inconvenience
that it causes to humans and fortunately
lives are lost and a number of people
displaced from from their homes so I
think this is the Houston Astrodome and
a number of people who are moved there
and in the aftermath of Hurricane
Katrina so so the question here is you
know we know that these events are
extremely expensive and extremely I mean
there's a negative impact on humanity
what can we say about such events in the
future as the climate change so we're
not going to be talking about course
quantity like degree or two Celsius
increase or sea level rise I mean those
are important things to keep in mind but
the question we can ask is are category
four and category 5 storms going to be
more frequent in the future are they
gonna start making landfall more often
in the future when they make landfall
are they going to be more intense or
less intense so those are
questions we don't ask so how do we go
about answering that question well as we
often do in Earth Sciences you know we
are our lives are relatively short I
mean the the recorded time period for
Humanities is relatively small Earth is
is several billion years old so one of
the things that we can do is to look
back in time and look at paleo climate
records and see if such events have
happened now
those are hard to come by and you can
only dig for for these kinds of records
in very few places in the world but now
because we have powerful computers one
of the things that we can do is to
simulate the Earth's going forward so we
can have climate simulations and we can
plug in a range of effects so we can
talk about what the external force
things are for the climate system so
solar activity and volcanic eruptions we
can talk about anthropogenic influence
so how say the GDP of China and u.s. and
India are going to grow how much co2 the
emissions are going to make and then we
have a lot of physics equations that we
plug in and those take into account the
internal climate system variability so
you plug all of that in and then you run
the climate simulation out for the next
hundred years so essentially what you're
going to do is once you do that you end
up with the last dataset and we will
need to do some pattern detection on
that dataset so I wanted to give you a
favor I understand that most of you may
not have a background in climate science
so I wanted to give you a flavor for
what the day set looks like and the
question you should be asking yourself
as you see this movie is if I have a
hundred years of this movie running in
front of me will I be able to find
patterns in this data set by hand or by
eye so there is a certain field called
water vapour humidity or specific
humidity that we are trying to visualize
this is a model that our production work
house model in the do-e and we run this
at 25 kilometer resolution and
essentially you're seeing the atmosphere
you're seeing how water vapor in the
atmosphere is getting transported some
of you may know that there are
hurricanes that are forming and they you
know some of them fizzle out here but
others develop and grow and you know
make landfall the movie that we played
was just over three months in a specific
year but now
in a movie that runs for 100 years so
you have a lot of data to process now if
you look at any specific patch in this
in this frame from the movie you're not
gonna have 8 bit RGB information you
will actually have a lot of variables of
off interest and these are all going to
be single or double precision floating
point so you might have specific
humidity but then you might also have
Bend and pressure and temperature and so
on and so forth
so essentially this is what the machine
learning or the deep learning system is
going to see and based on that
information it'll need to decide whether
there is an extreme weather event here
or not so the task is going to be I will
give the deep learning system these
fields and then the deep learning system
will need to say you know draw me the
task I'm gonna ask it to do is to say
draw me yellow bounding boxes around
hurricanes and draw me green boxes
around X tropical cyclones and draw me
red boxes around atmospheric rivers
these are all different kinds of
examples of extreme weather events of
interest all right so there is an
analogy here in the task that we have in
climate which is to find these patterns
in computer vision when I showed you the
image net challenge it was roughly the
same problem I had RGB images and I had
to pull out you know pictures of cats
and dogs and so on so there it's a it's
a pattern classification task but there
is also this key step of feature
learning you know I the climate
community has not spent three decades in
hand tuning you know features for
different types of events they have
actually spent three decades in
designing features for one event but but
not others so relieve automate this I
really want to get pattern
classification and feature learning
working at scale now so there is an
analogy but then there are some
differences and as I mentioned there is
the attributes of climate data are
different so we have many channels so we
have dozens of channels they're all
double precision floating point and then
probably the last bullet point is most
interesting which is that the underlying
statistics or patterns are likely
different so if you think about taking a
camera and clicking images in the real
world
perhaps your camera has a thousand by
thousand pixels pixels so you take an
image so that's one point in a million
dimensional space
now I go around taking a lot of pixels
so I'm slowly filling in points in this
high dimension space and essentially
what the deep learning system does is it
tries to figure out how can I separate
clusters in this high dimensional space
non-climate roughly is the same story so
maybe we have thousand by thousand
pixels and now I'm going to be looking
for patterns in this data set
there is really no guarantee a priori
that patterns in this high dimension
space corresponding to a fluid flow
system will be the same as what you find
in the national natural so the
underlying statistics are most likely
different and the question really is can
a deep learning systems be successful at
separating out patterns in this in this
space alright so that leads us to
representational challenges I mean given
this problem set up how do we go about
solving it so so we're gonna use two
approaches first because this is a this
is the first application of deep
learning to a climate science data set I
needed to convince myself that
convolutional architectures would work
at all so we're going to start slow we
can start simple and say let's try
supervised convolution architectures and
then once that works then we'll try
semi-supervised architectures so let's
start with supervised learning
so again supervised learning is the
paradigm wherein you have a lot of data
and you have labels for for for that
dataset so our training input is going
to be nicely cropped nicely centered
patches that have labels so so I
mentioned that the climate science
community has spent a bunch of time say
trying to better understand tropical
cyclones so we have some heuristics that
the community seems to have converged on
which we caught up and that's what we
use for producing labels similarly
people by hand in some cases have
manually annotated and said oh I know
that there is a certain event that
happened on a certain date that hit
California and so on there are very few
examples of these but they do exist now
this list is not exhaustive we actually
have dozens of extreme events that we on
a track and one of the goals here is
going to be to get at the other classes
of events but we got to start here
because we need to be able to operate on
a fluid flow simulation output and see
we can find patterns alright so for
simplicity how about we just focus on
tropical cyclones so I have nice cropped
centered patches of the assimilation
dataset so I give it labels I say this
patch contains a tropical cyclone and
this patch does not contain a tropical
cyclone so this is a classic binary
classification test that's all that the
network needs to do so once the network
has trained itself on say 10,000
positive and 10,000 negative examples
I'm gonna give it some held out data
some hello test data and then I'm gonna
ask this question well you know how well
do you think that this new patch has a
tropical cyclone in it yes or no so
that's going to be the boundary
classification task alright so some
details on the image path size the
variables the channels that that went
into this and then how many examples we
were able to provide now you'll note
that we do not have a hundred thousand
or a million examples so that's always
going to be a constraint to keep in mind
perhaps this constraint applies to some
of the applications commercial
applications that you care about now
what we ended up doing again by by some
experimentation we figured out so given
that we don't have a lot of data what
sort of a relatively shallow
architecture could actually solve this
problem so for tropical cyclones we
figured out that to convolution layers
interspersed with two max pooling layers
and then a fully connected layer right
at the end seems to be sufficient for
solving the problem and it does not over
train or all fit the data that we have
and the similar a similar story holds
true for the other two patterns that we
care about now here are some results so
so this table essentially lists accuracy
so for the test data that was held out
for the binary classification task you
know what percentage of the time was was
the deep learning conventional
architecture able to correctly classify
tropical cyclones so the convolutional
results are all the way to the right and
then we you know for due diligence we
did implement random forests and support
vector machines and
k-nearest neighbors and logistic
regression and generally I would say
that in indeed convolutional nets do get
the best performance but I guess the
fact that was not appreciated by me till
I lie coded up the other methods was
that this is it turns out that this is a
hard problem most of the numbers here
are above 85% to begin with so I think
this is a really important lesson
learned which is you know one should not
use the most powerful hammer there is
right now which is deep learning on any
single data set you could probably start
slow start with something that's
interpretable like logistic regression
or SVM's or K nearest neighbors get a
sense for how hard the problem really is
and then you know move out to deep
learning if you really need that extra
accuracy I'll note that all of these
methods have some free parameters that
you need to tweak so for K K nearest
neighbors
you know perhaps there is the choice of
K that you need to somehow decide upon
for support vector machines you need to
decide on the form of a form of the
kernel function for example random
forest you need to decide on how many
trees you want to have and how much what
the depth of the trees should be and so
on and so forth convolutional
architectures of course there are many
more parameters you need to decide upon
so there is a tool called Spearman that
we've been using to with some success at
dusk so for all of these methods we use
pavement and we figure out what the best
choice of parameter might be and then we
go with a choice and then proceed with
with the training all right so anyway so
I think so far things are promising I
think I convinced myself that supervised
convolution architectures could work on
on the output of a climate data set so
let's take it another take it up to
another level so far semi-supervised
learning what I would really like to do
is instead of having three different
networks that do a great job at finding
three separate patterns I would like to
have one unified architecture a single
network that can find all weather
patterns I would like that network do
not just do binary classification but I
would like it to predict where the
bounding box is so I need to find out
where the center of the pattern is and
then the space
extent of the storm and then this last
point I mean this is sort of the Holy
Grail which is I know going in that I'm
going to be able to provide only labels
for say three kinds of events but I know
that there are many other types of
events for which I don't have labels so
I would like to build in into this
architecture the capability for it to
discover new patterns so this is the the
convolutional architecture that we ended
up with so on the on the left here is is
the encoder so this is the classic
picture the image net picture or the
supervised picture that I showed you
earlier a bunch of convolution and
pooling layers and so on and so forth
till you reach a certain compact
representation of the weights now we'll
just stop here for a moment
ignore the the piece on the right but
just if you look at the the green piece
on the bottom we're going to be
operating on this this compact
representation which is called the
bottleneck layer and we will ask the
bottleneck layer to predict bounding
boxes so that's a regression task and
then also predict labels that's a
classification task so that's what we're
gonna do so from left to the bottleneck
layer to the bottom the the green box
this is a classic supervised problem
nothing new there the peas that's new is
is the part on the right the the decoder
peas so essentially what what the
decoder piece does is it operates on
this bottleneck layer and then it's
going to progressively in some sense I
guess up sample the the features which
are learnt here till it gets an image
which is of the same dimensions as the
input image and the objective that it
has the objective of the decoder piece
is to make sure that it can minimize the
reconstruction error between the input
and the output so this network is
essentially trying to do two things at
the same time one from left to right so
this is called an auto encoder it will
take the input filter it down filter it
up and try to minimize the
reconstruction quality between the input
and the output and the second thing is
going to simultaneously try to do is to
for the given patterns for which it has
labels and bounding boxes it will try to
do the supervised classification and
supervised regression task
all right so we recorded this up and in
general the results look look reasonable
I mean this is pretty much work in
progress on the left or sixteen images
which are the which is just a snapshot
of a certain time instant in the climate
data set so these are all different
fields and on the right the panel of
four by four are the reconstructed
variants as the output of the auto
encoder on the right so in general I
would say that the spatial patterns are
being captured quite well now how about
the the classification and regression
task so in this case ground truth is in
green so all the green boxes here with
the TC label means that there is a we
know that the ground truth is
essentially a tropical cyclone at that
location of a certain bounding box sighs
there's a green bounding box here for an
ATC there is a green bounding box here
for an atmospheric River and the network
again the single network is now trying
to predict these three classes so in
this case you know it's predicted an
extra pikal cyclone here which seems to
line up pretty well it predicts an
atmospheric River here the the bounding
box is not quite size right but at least
the the position is fine and it does
maybe a reasonable job in in these
locations as well so there's this work
to do
but it seems to be getting the the
patterns right now how about pattern
discovery again this is this is sort of
the the hard one so I talked about this
bottleneck layer and again to the extent
that this network is doing the right
thing we should be able to cluster the
the features that the bottleneck layer
has learnt look at the clustering
structure and see how well the clusters
corresponding to known events are
separated so in this case you know blues
might be extra pickle cyclones and red
might be tropical cyclones and greens
might be at mystic rivers and I
apologize for the quality of the display
here but the grey dots are different
other other feature vectors the colored
ones are features that we know
correspond to label data so what I was
hoping and to some extent I think is
true the blue stuff is interspersed
throughout but definitely the green
stuff is it's slightly separated
and and some of these these areas so so
this this chunk here definitely
corresponds to tropical systems that are
evolving and and these this chunk here
corresponds to weaker systems so I I
know that this is not perfect but
eventually I think our hope is that if
you can get a good separation of
clusters in this low dimension space
then the question then becomes you know
if we find a tight cluster which is
separate from these known labels can be
associated new semantic label semantic
type to that cluster so that's that's
our hope for discovering new patterns
all right so so I think hopefully you
have a sense for you know that we made
some progress along by using supervised
convolutional architectures also
semi-supervised architectures now the
problem is is scaling how do we get
these things to run fast so again for
those of you who have some experience
with deep learning you may know that it
takes a long time for these networks to
converge so in our experience I think
when we apply it to scientific data sets
if you want to process say tens of
gigabytes of data it can easily take
several days for the network's to train
and we at this point in time we easily
have tens of terabytes I mean III the
climate data set that I'm showing you is
30 terabytes in size and there's there's
more waiting to be processed so if it's
going to take you weeks and maybe months
to train your network then you're really
not gonna be able to experiment at all
so I mentioned this hyper parameter
tuning problem which is you have to come
up with the right architecture to begin
with and the hypervisor choices might
comprise of how many layers do we have
how many filters do we use the size of
the filter is something called striding
the kind of non-linearity you want to
apply in the pooling or the learning
rate that you want might wanna apply
sometimes people change the learning
rate as the training proceeds
choice of different optimizer so on and
so forth so this is a seven to eight
dimension space that does need to be
explored somehow but if each run is
going to take a week or a month this is
just not happening senia so performance
so single node performance and more
you know scaling is absolutely key going
forward so I guess I just won't share
you know the the stack that exists the
deep learning stack that that's there in
practice right now so as a practitioner
there are a number of frameworks that
you can use and tensorflow and cafe and
tiana are the ones that we use at the
lab but there are high-level frameworks
where it's much easier for you to set up
set up a network easily like Harrison
lasagna
now as you think about scaling actually
much of the magic is underneath the deep
learning framework so there are multi
node libraries that you have to keep in
mind so perhaps you can use MPI or gr PC
as the library Intel has now developed a
new library called ml SL now single node
performance is absolutely key so if you
are using a GPU then ku DNN does a
really good job at getting you that
performance but on a CPU you will
probably need to turn to M care and
hardware of course so we use Knights
landing there are other flavors of
multi-core CPUs coming up shortly and
GPUs are of course a very competitive
option at this point in time so for all
the scaling results that I'm going to
talk about next the stack for us is
going to be cafe and in particular Intel
cafe we used ml SL for as as the multi
node library we used mkl for single node
performance and then knights landing is
the is the chip that we used so a few
words I guess on the target hardware
platform so I I work at the nernst
supercomputing Center and the machine
that we ended up using for these tests
was the the quarry
Cray XE 40 supercomputer and this
machine has 9600 Knights landing nodes
so in theory it's capable of 30 para
flop class performance hundreds of
terabytes of memory IO is a challenge
for deep learning so we ended up staging
a lot of our data not on a lustre or a
gpfs filesystem because those typically
don't keep up but we ended up using a
big
SSD pool called called the data Bob
which is 1.5 petabytes its size and it
can support io rate of one-and-a-half
terabytes second
so a little more specifics on the input
data set that we ended up using for the
for this study so the input images are
768 by 768 in size they have 16 channels
overall the data set as is in in
aggregate is 15 terabytes and it's
comprised of 400,000 images and the
architecture that we ended up using has
9 convolution layers and 5 decomposition
layers and the parameters all of the
filters that the weights are about 300
megabytes in size all right so on the
single node side of things I mentioned
the xeon phi the knights landing ship we
used Intel Cafe with the mkl 2017
library we did have to optimize various
layers the deconvolution layer in
particular was not yet optimized for K&amp;amp;L
so that's something that we ended up
doing and this is the performance that
we get for our architecture so a couple
of I guess two things are being plotted
here in red is the amount of time that's
been taken up by different layers so
there are a bunch of deconvolution
layers in the backward pass and
convolution layers in the forward pass
and so on so red indicates how much time
is being spent in various layers and in
black the the vertical plots or what the
performance level is in terms of the
teraflop rate that we are able to
achieve so in general we are able to get
anywhere from 1 to 4 teraflop on a
single KL node after we did the octave
8/8 the optimization which is not not
bad in there's there's only so much room
that you have on on a K&amp;amp;L chip and I'll
note that some GPUs of course can do
more on a single node basis but one to
four times lops is is not bad at all for
4 K and L so the real piece here is is
in the multi node strategy so now that
we know you can get one to four
teraflops on a single node how far can
you really push it out across this
$9,600 supercomputer that we have now a
few options one is to use data
parallelism where and you take this
400,000 images and then you're going to
just break it up into pieces and each
node is going to be processing
a small fraction of the image a small
fraction of the dataset so that's the
typical data parallelism strategy that's
used and that's what we're gonna be
using for this for this exercise now
some some uses some researchers also use
model parallelism which is we're going
to take a big network we will actually
break up the network into pieces and
perhaps have the same data set on
different nodes and we will figure out a
way to get the the networks to
synchronize with each other all right so
a bunch of optimizations here I'm going
to walk you through some visuals for for
these optimizations but it was really
important to get to develop a hybrid
strategy for for synchronizing
parameters we had to do some topology
aware placement and in putting the
parameter servers and then we had to
dedicate some nodes purely to do to play
the role of a parameter server I'll note
that just like we use cafe for the
single node stuff and mkl 20:17 in this
case we use the new MLS cell library
which gets much better performance
compared to baseline MPI in terms of
bandwidth utilization on our on our
network all right so just on visuals so
I guess there are two options two
extremes perhaps four for converging on
parameters as as different nodes see
data sets so some conventions here so a
worker node will see a small fraction of
the data set it will try to update its
weight because it knows what the error
is it's going to compute gradients is
going to do the backdrop and they'll
compute the updated weights so one
option is that all the workers work in
lockstep so as everyone sees its group
of ten images they update the weights
and then they do an all reduce everyone
gets the latest and the latest estimate
for what the entire networks weights are
so that's the synchronous model of
execution the other extreme is to do it
asynchronously so everyone has ten
images say they go ahead and they
process their ten images as soon as they
they are done with the with the backdrop
then they're gonna just broadcast
there updates to an asynchronous
parameter server and then will be the
job of that parameter server to
reconcile differences and send the
updates out to the other nodes so some
trade-offs here again these are two
extremes for synchronous scaling where
everyone is moving in lockstep compared
to a serial implementations if you were
to do the same task on a serial on a
single node you will likely have the
same number of iterations for
convergence the the negatives the
downsides here are that if any one of
your nodes fails for whatever reason or
lags behind that will essentially slow
everyone down now on the asynchronous
side you can move faster again people
are not constrained to block and and
communicate so you can certainly move
faster there is more robustness to node
failure so if one of the nodes goes down
it's not the end of the world but
chances are high that you will need many
many more iterations to converge
compared to the serial implementation
alright so clearly we don't have to
choose extremes so you don't have to I
mean this is a spectrum you don't have
to be fully synchronous or fully
asynchronous so you can be hybrid and
essentially that's the strategy that we
ended up going with so we break up our
our supercomputer into groups which are
going to be executing synchronously one
of the nodes in that group will be
responsible for communicating its
updates which reflect the the latest
updates for the compute group 1 2 and
asynchronous parameter server and you
know we'll see where that takes us so we
can change by by changing the number of
synchronous compute groups we can
essentially move along the spectrum the
other thing that we ended up doing apart
from this hybrid strategy is to dedicate
parameter servers for every layer of the
network so for example here in this
schematic there are 4 layers in the
network so we dedicate one parameter
server to handle the traffic coming in
from all of the different groups another
node for the second layer so on and so
forth there are some more advanced
topology where placement so exactly
you want to make sure that if you are
within a synchronous group then you
don't really span racks on on a
supercomputer certainly so we just make
sure that when the run time comes up
that we take the topology of the network
into account and try to minimize extra
hops all right so here are some some
results so there are two configurations
here one is a strong scaling
configuration wherein we fix the overall
data set size and all we're going to be
doing is to throw more and more nodes at
the problem so you would hope that
things run faster and faster at least at
the time to completion in terms of
processing the data set is faster so
sure enough the the synchronous approach
stops scaling after some point so as we
go from say 256 to 512 nodes the
synchronous approach stops scaling after
that and and sort of flattens out tilde
tilde 20 1024 node mark but the hybrid
strategy where and we played with having
two groups so two synchronous groups
that would communicate with each other
through an async through a parameter
server or four groups that seems to
scale better now a slide so this is most
likely the the configuration that you
will likely have in your in your in your
work you'll have a certain dataset size
and you want to maybe throw more more
nodes at the problem if you are at a I
guess in a situation where you can come
up with a weak scaling configuration
where in each node will always have the
same amount of data to process but as I
add more nodes I can add more data so
that's the the weak scaling
configuration things seem to hold up
fine for for our implementation so
synchronous is is in blue and the hybrid
strategy seems to be scaling better than
when this then the single strategy Sony
so things seem to be holding up
reasonably well John I guess had a
pointed question for me earlier on well
I mean does this converge at all so you
might be processing the same amount of
data but is your network converging so
for for climate we do see the loss
dropping and it seems to be leveling off
so the hybrid strategy seems to be to be
building of now there is a slightly
different problem where we applied
exactly the same architecture so the
topology of a placement the hybrid
tuning strategy so on and so forth
I could not create the same plot for for
climate but there is a different signs
from in in high energy physics and
essentially I think it does a nice job
of visually representing what the
difference might be between a
synchronous run and a hybrid run with
with regards to conversions so so the
the dotted blue lines are the worst case
scenario for for synchronous execution
so so that's this case again loss is
plotted on the x axis and and number of
epochs or iterations is on the is on the
x axis losses on the y axis solid blue
lines is the best case for synchronous
so again when you run on 9000 nodes not
every run is going to be identical
so they're going to be stragglers and
and they're gonna be node failures and
so on so forth
so the solid blue line is is is best
synchronous run the dotted blue line is
the worst synchronous run and then we
have hybrid in various flavors so with
two groups with four groups with eight
groups and I guess if you look at the
orange line for example and let's say
that we care about this particular loss
level we want to get the loss to a
certain certain level of acceptable
performance the orange line definitely
gets to that loss level faster than the
blue line and you know there might be a
gap of one point six six axes in the
number of iterations that it took for
the synchronous to eventually catch up
now the async which would be you know
all nine thousand nodes doing their own
thing that will likely not not converge
certainly in this time frame so nice so
there is a benefit to do the hybrid
model all right so just to recap overall
performance so on a single KNL Lord we
can get several teraflops for the 9600
node run which ran on six hundred
thousand-plus course we were able to
achieve fifty fifteen pair flop per
second peak and then on a sustained
basis we were able to get 13 pair of
lobster so
so just to conclude I think I'm slowly
getting out of time
so just to recap I think you know
there's there's a lot of fighting
genuine excitement in the field of AI
and deep learning a lot of breakthroughs
in computer vision and speech and
control systems in this presentation I
think I wanted to communicate that we
can get the same success in scientific
data as well so so deep learning has now
been established to be a viable tool for
finding patterns in in climate data set
and this can certainly help us in able
to characterize better what climate
change will do in the future there was
representation challenges in in getting
this to work so I think we proved that
supervisor architectures can do a good
job in terms of detecting known patterns
and then semi-supervised architectures
can potentially discover new patterns in
in diffusion in terms of computation
challenges I think we're certainly
getting reasonable performance on a
single K&amp;amp;L node but then also I believe
this is the largest multi node scaling
experiment that's been done with deep
learning so far I want to thank a number
of researchers from from Intel from
Stanford from Montreal and and of course
bugzilla all right thanks very much I'm
happy to take questions
Thanks provide I had one question so so
when do we start saying the application
of your models towards predicting actual
you know extreme weather events and I'm
sure you also waiting for that so that
you can actually test in your models in
the real world yes I think you know the
word prediction is unfortunately
overloaded in the climate context so in
this case we are not really doing
prediction we've already run the
simulation out and all we want to do is
to find patterns in that simulation so
in some sense the dataset is fixed that
haven't been said there are people who
are trying to look at deep learning so
variants of convolutional and lsdm
architectures for predicting the entire
state of the system for the next few
time steps so that work is also in
progress and you know I think the
results are promising I mean there are
limits to the predictability of the
climate system and that's the property
of the the system itself so it's a
nonlinear chaotic system so we know that
we cannot predict the the state of the
system beyond two weeks but to what
extent these convolutional STM
architectures can do a good job better
than the climate models that remains to
be seen
curiously if you saw you know in your
kind of training in your encoder and
auto encoder and decoder any insights
about the fundamental dimensionality of
the data set right so you have this I'm
sorry what was the question about any
insights on the fundamental
dimensionality of the data set right so
like what's the minimum number of nodes
that it can be brought down to yeah so
you know in many ways we do not want to
artificially constrain the data set size
right so the 768 by 768 image patch
that's that's the that's the native
spatial resolution of the data set
one could course on it I guess but we
found that we were able to get
autoencoders to work at that resolution
reasonably well and in terms of channels
again we we sort of stuck to the 16
channels that we had it is I know that
you know if you if one wanted to train
say 1k by 1k patches or higher chances
are that you'll have a hard time to get
autoencoders to work if time for one
more question
so I'm curious the original input is
actually an output of simulation so is
just a visual representation why did you
choose a visual representation or are
there other representations you could
have used so the input is the simulation
data so the simulation produces grids
and the grids have 768 by 768 points and
then again each each point has 16 chance
on it so you know Allah is doing in the
movies is just creating a visual
representation by map by mapping one
field to an RGB image but we actually do
not create RGB images we work on the raw
double precision floating point eight
asses so let's thank prophet once again
go and probably would be hanging out
here during the break for more questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>