<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>RDDs, DataFrames and Datasets in Apache Spark - NE Scala 2016 | Coder Coacher - Coaching Coders</title><meta content="RDDs, DataFrames and Datasets in Apache Spark - NE Scala 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>RDDs, DataFrames and Datasets in Apache Spark - NE Scala 2016</b></h2><h5 class="post__date">2016-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pZQsDloGB4w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this talk is on rdd's data frames and
data sets in apache spark it's not going
to be as code heavy as rune ours
I don't think I used the word monad once
but hopefully it'll be enjoyable anyway
the general idea here is that our DDS
are what most people are used to using
up until recently when everybody was
saying no no no you want you want to use
data frames and then now people are
saying well you know maybe there's this
new thing called data sets that we'd
like you to migrate to instead and what
we're going to do is compare and
contrast the three of them a little bit
to give you an idea of what data sets
are going to start looking like moving
forward so the endgame here is a little
bit of background on our DDS a little
bit of background on data frames for
those of you with spark knowledge that's
gonna be a little boring right but for
those of you without spark knowledge
you'll get an idea what the api's look
like and then we're finally going to see
data sets and that's sort of the way
forward
so that's or the intent behind this I
will tell you upfront I am NOT a spark
committer I do a lot of spark training
but I'm not a spark committer so if you
ask me under the covers how is that
implemented I'm very likely just to say
you DSL because that's exactly what I'm
going to have to do if if I try to
figure out you know is this using this
particular capability under the covers
all the code is out there there's a
significant amount of it for those who
are not familiar with spark the spark
the smart code base is largely written
in Scala so it's a and all of the spark
emitters particularly the one the guy
behind data sets is actually absolutely
a fan of Scala when I told him I run the
Philly Scala meetup in he looked like he
was about to start applauding so you
know I think he's had to fight to get
people to use Scala so he's very happy
about that all right so the three things
we're going to be discussing here are
our rdd's which is just short for
resilient distributed data set data
frames and data sets which kind of has
an unfortunate name I think what we'll
get to that so what's an RDD
basically an RDD is the is the building
block of SPARC it's what you see under
the covers it's what it was the original
API that spark exposed and pretty much
all the higher-level API is decomposed
down to our DD
under the covers whether they expose
them to you or not there's still plenty
of people out there who who code to the
RDD API they have a couple of
characteristics they are compile-time
type safe they're lazy okay so that in
that sense they kind of work like
streams so when you do things like map
and filter over an RDD you get back
another RDD but you're not actually
processing data yet right the data
doesn't get processed until you call
something that says hey I want something
out of you at which point the whole
thing gets triggered so they're lazy in
that sense they're kind of based on the
Scala collections API if you look at
them if you're familiar with the scholar
collections API you'll look at them and
say oh I recognize half these functions
because they make sense they were
deliberately built around the model of
the of the Scala collections API to the
point where it can actually be somewhat
confusing if you look in the RDD API
this is still the case there are two
collect functions in classes we've done
sometimes people get confused by this
the first collect function looks exactly
like the collect function in the Scala
collect API the one that we commonly use
to sort of combine map and filter right
the second one is the RDD collect action
which drags data back into your
application from the nodes and so people
look at this and they say why are there
two collects I don't understand and the
reason is it's based on the collections
API so here's an example of some code
it's kind of made up but not that much
made up that that file with the demo I'm
going to be giving actually will be
looking at a file very similar to this
the idea is that there's a text file out
that we're going to read it in then
we're going to parse it okay so it's got
some tokens in it so we're to use flat
maps so that I can discard note that I'm
not complaining but I'm going discard
the ones that don't have four fields in
them right so I only want to process the
ones that have four fields in them and
then the bottom bottom I'm going to
extract a key in a value right filter
out those where the key doesn't match
again that in this particular case is
the is the code for the English
Wikipedia and again the demo I'm going
to talk about this in more detail we're
going to do a reduce by key in other
words I'm going to combine and add up
all of the values then I'm going to take
the top 100
and loop over them and print down okay
pretty straightforward if you look at it
but you kind of have to look a little
bit to figure out what's going on what
are we trying to accomplish here what
we're trying to accomplish is to count
up the number of hits on the project
right and in a distributed fashion okay
so one of the problems with the RDD
library that people complain about is
this is a little bit opaque it's a
little bit difficult to see what you're
trying to accomplish because you're
looking primarily at the how right
they're typesafe which is the positive
thing
they're compile-time typesafe rdds have
types they can be an RDD of an array of
string they can be an RDD of a case
class they're compile-time typesafe this
is a real win but the downside is as I
said they express the how more than the
what which means it can be hard to look
at something and say all of this
gibberish is computing an average right
it's a little difficult to read it
understand what's going on they cannot
be optimized by SPARC they are entirely
lambda driven SPARC can't look inside
the lambdas they're opaque so it can't
do any optimization on your behalf I'll
show an example of what I mean by that
they are slow on non jvm languages now
in here we probably don't care right but
an awful lot of people are using spark
with Python for instance and we're
talking you know significant slowdowns
if you try to use rdd's from Python for
reasons I'm not going to bore you guys
with in here since we don't care we get
to use the JVM right so it's it's too
easy to build an inefficient set of
transformations so here's an example of
an inefficient transformation right
we're doing the reduce before we do the
filter so we're reducing the whole data
set counting the whole data set and then
filtering some parts of it out okay that
is not the right way to build this that
is very inefficient you want to do the
filter first so you're doing less work
as it turns out for reasons having to do
with the waste spark works filter is
more efficient than reduce by key anyway
there's no network traffic involved in a
filter there is network traffic involved
in a key based reduction like that so
you want to do the filter before you do
something more expensive like a reduce
here's somebody has coded this I have
coded this intentionally badly right but
spark can't help
here it does exactly and only what you
tell it so that's a problem because it's
very easy to write something that
doesn't perform very well so this is
what led to the data frames API the data
frames API was intended to provide a
higher level API something that is a
little bit closer to expressing what
you're trying to accomplish rather than
how you're trying to do it in a way that
can also be optimized better by SPARC
right so it gives you a query language
if you look at the data frames API it's
really a DSL in both Python and in Scala
it's a DSL and it builds a query plan
under the covers interestingly enough
that same query plan can be built with a
sequel statement they work hand-in-hand
so that you can say I'd like to build my
query using this this DSL this typesafe
dsl type seifish i'd like to build it
with sequel and then that result is the
same thing all right I'll show you what
I mean by that in a minute so this is
the exact same piece of code exact same
work that we're trying to accomplish in
the data frame API so again what we're
doing is we're we're loading up our RDD
and converting it into a data frame
okay we're giving the data frame column
names the the characteristic of data
frames in SPARC is that they have
schemas and a schema in SPARC basically
means it is something that could we can
view as a column and columns have names
n types and that's basically it whether
it's really a columnar store is kind of
irrelevant you can for instance read a
data frame from a JSON file well it's
certainly not a column to store but if
you can pretend that it has columns with
names and types then it can be
represented in a data frame one way or
another so that's what we're doing here
we're converting our RTD into what
conceptually looks like a table with a
project column and a number quests
column then we're doing a group by a sum
and aggregation of a sum a limit on that
and then we're pulling show basically
pulls 100 of the elements back so show
is the action show kicks off the
execution pulls a hundred records back
and displays it in the table that looks
like was ripped right out of the my
sequel repple okay
I can show you an example of that as
well and again you can do the same thing
in sequel this does the exact same thing
and I will contend that this is easier
to read than the RDD version right I
mean you look at this if you have any
idea what sequel does this makes sense
this is not that much harder to read so
the argument behind using one argument
behind using data frames is that you're
closer to what you're trying to do
rather than how you're trying to do it
kind of the same way as if you're
talking to NRT BMS engine unless you're
a DBA with loads of query hinting you
know foo what you typically do is you
give the sequel statement and you're not
worried about how the our DBMS goes in
and figures out whether to do a table
scan or which indexes to use all you
want is the result and you express what
you want and you let the Rd be message
under the covers figure out the most
efficient way to do that right and
that's the general idea behind the data
frames API let's part figure out how to
do it for you it turns out they are in
fact optimized I stole this diagram from
from data bricks but the general idea
here is that whether you start out with
the sequel ast a parsed ast or a data
frame what ends up happening is that
you're building an unresolved query
under the covers right you're at the end
of the day when you're ready to execute
an action you've built up this query
plan well these guys have enough of a
database background that they optimized
their query plans the way in our DBMS
does what will happen is that they use
this catalog down here which basically
is just a list of the available
user-defined functions okay so I put
that in there so we can ignore that now
everybody asks what the catalog is it
takes that logical unresolved logical
plan and it figures out the actual
logical plan that it wants to use imma
show you a diagram about what I mean by
that but it does some optimizations to
produce a logical plan and then some
further optimizations to produce an
optimized logical plan at which point it
starts to construct physical plans how
do I want to execute this how do I want
to express this abstract notion in terms
of the rdd's that everything ends up
being built on under the covers okay
so having done that then it can run it
through a cost model and choose the most
efficient physical plan which then
dictates how to build
the RDD chain to implement this query so
what we get out of using data frames is
this sort of optimization that goes on
with the covers and in classes that we
give I always sort of describe it like
this you know I'm obviously old enough
to have plenty of gray hair and I
remember doing seeing back in the 80s
when they told you things like well you
know this is not fast if you use a pound
a SM keyword and throw some dissembler
in there which nobody really liked to do
but you could right but then there came
this inflection point where the
optimizers of the compilers got so good
that nobody thought that that was a good
idea anymore right why do I want to
waste my time making my code on portable
writing assembler when the optimizer is
so good that I'd have to work really
hard at it and to me that's sort of what
the catalyst optimizer is it's going to
keep getting better and so therefore if
I don't have to spend time tuning my
rdd's I can focus on the business
problem right so that's that's the
general idea so here are some of the
this is an example optimization we use
this one a lot so we're doing a join
here but there's something wrong with
this joint anybody see what's wrong with
this join
um well it's not it's not what you're
joining it's the fact that you're doing
the join before you're doing the filter
if you look carefully at the filter the
filter is against the event I want to do
the filtering first because that reduces
the size of the join and a join in a
distributed computing environment like
spark can can mean network traffic right
so the less data that I have to join the
more efficient my joint is going to be
so if this were rdd's that would be bad
right that would be inefficient so what
so what happens under the covers is that
there's the logical plan in other words
this is essentially your query plan this
is what you typed I could execute this
under the covers if I wanted but instead
spark uses some heuristics and says hey
you know what that's inefficient that
joint is not efficient the filter should
come first
it automatically reorders things and
pushes the filter down below the joint
we read these bottom-up okay so the data
that we're reading is at the bottom in
this in this diagram right so it decides
that this this needs to be moved down
automatically another thing that it will
do that I'm not showing here is if you
put multiple filters in a row it based
on certain heuristics it can coalesce
them into one filter operation and then
it may also do something like this okay
it might notice that the events day of
data source is intelligent in other
words in this particular case maybe it's
a Postgres table at which point I can
shove the filter all the way down into
the database
it just becomes a where clause right so
I never even manifest the data in memory
the database takes care of doing the
filtering forming other examples of that
kind of intelligent filtering are park'
files right a park a file is a very
efficient column based storage makes
columnar access extremely extremely
efficient
so what SPARC will do with a park a file
is if you have a select if you have a
park a file that has a 300 columns but
it determines that at the end of the day
you're only using three of those columns
it will push effectively push the
selected down into the park' and say I
don't need to manifest all 300 columns
just to throw away 297 of them I'm only
going to pull those three columns out of
the data source so it has some built-in
intelligence that it can
decisions like this and my contention is
that this is not the kind of work you
really want to be doing yourself right
so if you work at the RTD level this is
kind of a pain it's a pain to get right
it's a lot of work why not let the
optimizer do this for you and then you
stay closer to the problem you're trying
to solve and you don't to worry about
the implementation detail the bits
become sort of irrelevant they are
faster another diagram I stole from data
bricks so you'll note that that Python
as I said is much much slower than Scala
the reason is that the lambdas are
really written in Python in rdd's and
there see Python it's not JSON because
you can't use Python packages like num
cut numpy and scikit-learn if you do
that so they that spark actually is to
shove that stuff over to a running
Python VM which does the work and then
shoves it back so every lambda they use
an RD deal and in Python does that ok
and it's really slow but but you'll note
that data frames are even faster in
general Scala - Scala from rdd's and
that's the optimization going on in
general it's able to make more
intelligent decisions and speed things
up as well as level out the performance
across languages this is part of the
pitch right this is the pitch to use
data frames but here's the problem
we've lost type safety ok so it this is
really horrible for a lot of people
you'll note that the type if I do a
collect on the data frame which says to
spark hit I've done all the work I want
to do in this thing run the job and pull
the data back that's left I want to I
want to look at it in my application
note that the type comes back as an
array of something called row and rows
are not type discriminated each column
has its own type but that's not exposed
to you as part of the API so to you it's
in any I mean you can't see it as
anything else other than an any so
you've lost compile time site type
safety you have asserted that look my
project column is a string my page title
column is a string my num request column
is a long but you don't see any of that
in the code alright and you can see an
example of that further down here when
you do the collect if you need to map
this back to real types to play with it
now you're doing stuff like as instance
of which none of us likes to see right
that's pretty horrible
and extremely error-prone like what if
you get it wrong right well you've run
this long job that takes 30 minutes to
to chew through a terabyte of data and
then hits the collect and blows up with
a cast class cast exception right so who
wants that so this is part of the reason
that that they've moved more toward this
thing called data sets now that they
have the infrastructure in place to do
the optimizations isn't there our way
maybe that we could drop something in
place that gets back some of the type
safety without losing all of those
optimizations because even now even
before data sets what you can do is do a
lot of work in data frames and then drop
down to our DDS to do the rest of your
work you can always convert a data frame
back down to an RDD to go back to type
safety but you immediately lose the
optimization that's done by the catalyst
optimizer you're back on your own right
so isn't there some sort of middle
ground and that's sort of where the
impetus my understanding the impetus of
data sets came from right because we
want to get back our compile time type
safety but we'd also like not to lose
all the optimizations that this
optimizer this catalyst optimizer can
provide us under the covers so this is
what data sets are attempting to at
least start providing so what are they
first of all they're an extension to the
data frames API which means that they
operate on data through the sequel
context okay when you when you create
one of these things you create it
through the sequel context the same way
you create a data frame all right the
second thing that they do is that they
are conceptually similar to rdd's at
some level in other words you get back
you get back your lambdas and types okay
with data sets you are now operating on
domain objects that are Scala or if you
prefer Java types that are manifest in
the compiler and you get some compile
time protection back now there's this
thing that they've been introducing into
SPARC called tungsten one of the things
that they're trying to do is make memory
management in SPARC much more efficient
so instead of storing stuff for instance
on the JVM heap which immediately runs
into all kinds of kinds of garbage
collection problems they're starting to
move memory management of SPARC data off
heap into this into this unsafe area and
they're doing it with this thing called
tungsten tungsten provides
only off heat memory but it has this
compact columnar based storage that
allows them to operate directly on this
stuff very efficiently in memory so
Tungsten's fast in memory encoding is
used in data sets so instead of rdd's
where everything is stored as java
objects this thing actually uses
tungsten now how does it do that
it has to know something about your data
in order to do that and it turns out
that what it does is it uses these these
encoders we'll get to that in a second
okay but it basically uses these this
special kind of cogeneration to take
your domain objects your case classes or
whatever and generate code on the fly
literally under the covers bytecode that
can that can access these things and map
them from domain objects into this
tungsten store and back again by
analyzing what it is that you're
actually working with a generating code
on the fly for this right and it also
provides easy interoperability with the
data frame API which is not something
rdd's provide so what all these bullets
really mean these bullets really mean
that that you get back some of what you
had with rdd's but you're operating in
the data frame world okay you're not
dropping entirely back down into pure
RDD level this stuff all boils down to
our deities under the covers but that's
all being managed for you you're
operating at a higher level you're still
able to think of things in terms of your
data frames your columns and and their
types but you can switch back into
something that has stronger compile time
types now this is an experimental API at
the moment it showed up in spark 1:6
it's becoming their development focused
for the next the next several SPARC
versions they're starting to do things
like take spark streaming and move it
toward a data set or data frame model as
well spark streaming currently is their
streaming model and it produces rdd's
over time so it takes a stream of
continuous data and chunks it up into
rdd's as it comes in they're moving that
up a level to give it access to data
frames and data sets as well so this is
becoming the forward focus for SPARC
okay so like like an RDD a data set well
how to type so this is an example of
creating a data set from a JSON file
presumably this JSON file has at least
two fields name and age one of which is
a string one of which is a long okay
smart can actually do some analysis of
that file and determine you know do some
sampling and figure out what the common
fields are in the records and then
generate a schema on-the-fly that's
called schema inference and it happens
at the data frame level already you can
do that but in this case what what I've
done is I've said read this JSON file
into a data frame and then I'd like to
view that data frame as a person object
and when I do that when I call that dot
as I get back a dataset so it's
basically mapped to the data frame
columns in this case name and age into
my case class so one obvious question is
how does it do that it it does that by
name alright so the names kind of to
match at this point I have a data set
that is based off of the data frame and
I can start doing some fun stuff with it
and they've also changed the notion of a
data frame a data frame is now just a
data set of type row in other words it's
a data set but it has a type that's not
all that useful right so they've
basically taken the whole idea and
extend it all the way across the board
everything is now a data set this is an
example of rdd's versus data sets you'll
note that they look very very similar
the first lines are slightly different
in the first case with the RDD we use
the spark context to read a text file in
the second case we use the sequel
context to read a text file which will
bring us back a data frame and then we
convert that data frame to a data set so
we're saying this is a sequel file of
type string because I haven't broken it
apart yet right the second line looks
exactly the same except that one is
rdd's and one is data sets and so the
second line is actually going to use the
more efficient encoding it's actually
going to generate an encoder for our
particular type to be able to map this
to and from that internal highly
efficient tungsten based column store
whereas the RTD one
won't it's going to be dealing purely
with JVM objects and that final lines
are almost the same but the the second
line is so much shorter and I've called
those out specifically here so you can
see that they're both doing a group by
that looks identical but in the RDD case
in order for me to count them up I have
to do that by a my own map whereas in
the second case in the dataset case I
can use a count aggregator that looks an
awful lot like the sequel count star
aggregator or the count aggregator in
the similar data frames API yes
I'm sorry
you'll get an error you'll get a runtime
error yeah you'll get a runtime error in
fact I would encourage you I'm going to
show I'm going to give you the demo that
I'm going to do I'm doing a notebook
demo but the source will be available I
would encourage you to pull the source
for that and do exactly that in like the
spark shell and see what happens you see
it not it's not perfect right you but I
mean think about it this way what you're
saying is you're still going to the
question was this gives you the same
problem you still get a runtime error
instead of a compiler time error but
you're always going to run into a little
bit of that when you're reading an
external file right because you can't
compile that external data file all
right so it's giving you it's not
perfect but it's giving you more than
you get with an RDD
on even with the data even with it well
with the data frame what it's going to
do is infer the column name from the it
will make the column names up right so
yes you're absolutely right you have a
problem here but with data frames let's
contrast this so in this case what you
have is if you read the file in and it
doesn't have a name field and in fact
has a first name field right you're
going to get a runtime error so contrast
that with data frames you'd read the
data frame in you get a first name
column in the data frame but what if
your code then says I want the name
column it's the same problem right
you're still going to get a runtime
error so the problem is just moved into
a different place so it's not really
dramatically different okay back here
the only other point I want to make here
is that the dataset code is more
visually compact I think it's a little
easier to read it again it's closer to
the what I'm trying to do rather than to
how I'm trying to accomplish it right I
might be heretical and not even comment
this well they probably would anyway but
whereas if I don't comment the RDD code
it's very unlikely that my own self
three months later I'm well understand
at a glance what it is I was trying to
accomplish right it will also likely
execute faster than its that it's RDD
counterpart okay because of the encoders
and because of the tungsten forum
here are some of the advantages that
data sets are providing and there again
this is this is ongoing work so they
tend to use less memory because SPARC
builds these encoders does this on the
fly code gen it has a greater
understanding of your domain objects
than it would within the RTD world where
they're just more or less opaque and on
the heap right so because it understands
that it can figure out how to store them
more efficiently so you know back again
we it generates these encoders to to
translate your in memory objects back
and forth to this compact in memory
encoding and it can actually work right
out of the encoding it doesn't have to
serialize to that format and then
deserialize it back out into a Java
object operate on it it can actually say
okay you want these things of type
person but I'm going to store them in
this other format this other compact
format whenever you need them back as a
type person I've got this efficient
encoder that I in generated to pull this
stuff out but I'm going to leave them in
that compact format and I'm going to
generate these encoders so that I can
actually pull the data right out of that
compact format and operating on it
directly in that format over the chain
of operations I'm going to minimize the
amount of work I need to do in in the
JVM here and I'm going to maximize the
amount of data I operate directly in
this compact off heap stored format all
right so this way memory is conserved
because the format the tungsten format
is more compact than the on heap format
turns out even more compact than if you
serialized it and stored it on heap and
the speed is improved by the custom code
generation right you're not paying the
penalty of serializing and deserializing
you're operating right out of this
compact format so this is an example of
this base efficiency at least in some
cases that they're observing so if you
implement one thing in rdd's and then
turn around to implement it data data
sets you can see that for this
particular data set that they did it's a
significant difference okay I believe
that's data size in gigabytes same exact
kind of job less than about a quarter of
the size consumed in memory so this is a
this is really useful for those of us
who struggle with running out of memory
when we're running these big jobs right
again serialization you think about its
FARC has to serialize data and has to
build a lot right it has to send data
across the network whenever you get to a
situation where hey I'm grouping things
by this key right well that key may be
the value so that key might be
distributed across your cluster so by
definition you're looking at shuffling
data across the network to get that
stuff aggregated in one place and in
order to do that that data has to be
serialized right there are other cases
where spark has to dump data out to disk
it turns out it dumps it out to disk
when it does those Network shuffles it
also dumps it out or can dump it out to
disk in certain forms of persistence
caching anytime you have to do this it
has to serialize it and you're going to
pay that serialization and
deserialization penalty so the faster
that you can make this the better off
you are
right so so what typically is done in
SPARC 1/5 and before is they tell people
look you can use the standard Java
serialization library for this but we
recommend in your production jobs that
you use this thing called cryo right
because cryo is much faster and much
more compact right so this is faster
even than that right the serialized data
in this tungsten format is often 2 times
smaller which reduces both your network
and disk use right this is really good
if you're moving a lot of data across
the network another stolen diagram but
this gives you an example of speed of
serialization and deserialization so
it's relatively slow with Java you can
see why in 1/5 and Pryor they're saying
use cryo because it's significantly
faster but you go to the datasets
encoders and I mean we're talking huge
difference in speed for serialization
and deserialization so again this is the
pitch for data sets right you definitely
want to be using this as much as you can
moving forward because you can get this
benefit as well but there are some
limitations ok one limitation is they're
experimental they are marked
experimental if you look in the API Docs
it says experimental everybody asked
what's experimental me and what
experimental means is look we think this
API is right but we may
decide that we're wrong and we may
change it this happened with the data
frames API between I think 1/3 and 1/4
they changed everything they changed a
few things right so your code breaks or
at least you have deprecation warnings
so you have to you have to be aware of
that going in that this is still
experimental it's also not done the
api's are complete as I found out when I
was hacking my demo up again last night
there are some things that are missing
that are available elsewhere for
instance with a data frame you can do
you can do an order by to sort the data
with an RTD you can do a sort by to sort
the data with those are missing and the
data set API right now as are some of
the aggregators so these general-purpose
aggregation functions and
general-purpose aggregation capabilities
that you can build on but the common
ones that you would hope would be
they're like Oh dot sum they're still
missing so this this is still a work in
progress and that's why they're calling
this a preview so you kind of have to
use it with a little bit of
understanding that you may have to
change your jobs down the road when
spark 2 comes along and then I find this
term really hard to Google ok within the
training team we have been kind of
complaining that data sets is too
confusing you can't Google it and what
do they mean do they mean data were
reading how is this different from a
resilient distributed data set but this
is the name so I just Google for Apache
spark data sets and well you know resign
myself to all the extra typing so I do
have a code example and I have to try to
mirror the display to do that I will
point out that that I do have a more
resources piece of this slide and the
source to this slide deck will be out
there so I'll tweet you know where you
can look at this slide deck and live
forum sometime later today and then
those who want access to the the kind of
crufty revealed j/s or had access to
that as well but in the meantime I'm
going to mirror my display really
quickly here alright so where I'm going
to go next is to a data bricks notebook
environment this is one of the perks of
doing some training through them as I
access to this thing there are a number
of notebook environments out there some
of you probably use the Scala notebook
there are SPARC based notebooks as well
Jupiter and Zeppelin are probably two of
the most popular SPARC based ones the
data breaks people that's their that's
their product so in addition to paying
people to maintain to maintain SPARC
they have these they have this notebook
product the resolution is really small
here okay make sure everybody can read
this okay is that readable to everybody
all right so this first line I'm
attached to a cluster that I spun up a
while ago this first line is basically
to make sure that my file is out there
data Brooks has this usually when you do
SPARC you're reading from some sort of a
distributed store like s3 or HDFS what
they've written is their own sort of
local file system they call dbfs and it
really is supposed to look like HDFS but
it's really backed by s3 and they have
these special escapes that allow me to
do things like this to see is my file in
fact still out there and let's hope that
I've got till 10 right all right so we
can see that the file is actually out
there what this file is is a is a file
full of edits from from Wikipedia I
pulled it down last night you can see in
the file timestamp that is from 2200
last night I believe that UTC right
about the time that that the debate was
going on so that we should see some
interesting stuff at Wikipedia during
that time so the first step is how big
is this alright sets about 91 megabytes
right it's not huge this is not what I
would call big data but if we did big
data we'd be standing here forever okay
so we're going to create an RDD now the
interesting thing about this is that
because it's reading a gzip file there's
only going to be one partition because
you can't really split up a gzip file on
packet it's not built that way you can't
say you take half the gzip file you take
the other half
pack those two pieces and we'll put them
back together so we only get one
partition out of this which means that
we don't we don't have very much
parallelism so I'm going to beef up the
parallelism a little bit and then I'm
going to see let's make sure see how
many records are in there now this level
we're at the RDD level right I'm doing
pure RDD old style spark hopefully this
thing will return
we're getting there okay so that's seven
and a half million rows of edit data in
there so what do they look like they're
basically files full of four fields
right and the first field indicates the
Wikimedia product that's being that
where the Edit has occurred so this is
consolidated across not just Wikipedia
but all Wikipedia is all languages as
well as things like Wiktionary and
wikiquote and all of the wikimedia
products so the first the first field
will distinguish what it is that we're
looking at the second field is the page
that's being edited the third field is
the number of I'm sorry not edited this
is accesses the third field is the
number of accesses during this hour
and the fourth field is how many bytes
were actually sent over the network okay
and this is actually the list of what
these things mean so these are the
descriptors if you want to go back I've
got an HTML and a source runnable
version of this that are going to be
available to get hub repository so you
can actually read this as HTML and then
pull the source into the spark shell if
you have the data file so so that we're
not one of the things that spark does if
you run a spark job and you read the
file top to bottom and you get you you
go all the way down to the bottom and
you say okay pull back the data I want
and it's finished and it's pulled all
this data back to your application
you're the nodes the executors they call
them the process is out running on your
behalf on the cluster all promptly
forget about all the data okay so then
you do if then you do another action
against this RDD you want to pull back
the count it has to reopen the file in
this case we unzip it reload it all
process it fire all the lambdas again to
pull that second thing back this is why
we have caching caching there are a
number of forms of caching but the
default is to try to cram the whole
thing into memory and so that's what
we're going to do here but caching is
lazy so when I say caching RDD it
basically says okay whenever you run
that next action I'll save that as much
of the data as I can
hence my need to run the count now at
this point I'm going to make use of
something here to pull up the UI this is
just the standard
spark UI but Dave Rick's notebooks make
us make it easy for us to get there so
you can see that cached in memory now we
have this thing called page counts RDD
to this is what I just cashed and it's
storing about a gigabyte of data on heap
across the entire cluster right so
across the cluster when you add up all
the data being cached it adds up to
about a gig save that thought because
that's going to be more important as we
get further down so this is a variation
of slide 5 where what I'm trying to do
in this case is calculate requests per
page so there's my flat map right this
is my way of both parsing the data and
tossing out the bad records you know if
it's good data return to some of its bad
data return a none and then flat map
done packet right and then I'm going to
filter it I only want to be looking at
English Wikipedia so I only want the en
project and then once I've got that that
I don't need a project anymore so I'm
going to map it down to a 2-tuple
which is a special kind of RTD as it
turns out a pair RTD which allows me to
do key value operations at which point
I'm gonna do a reduce by key to sum up
all of the number quests per page then I
want to do a sort by one I'm in
ascending order because I want the top
100 of them the take is the action
that's going to fire the action pull
this stuff back and then this final for
each is actually just running on a
scholar collection and this will take
probably on the order of 30 seconds or
so and I think I'm running out of time
I'm sorry 9 seconds so there you can see
this is the list of stuff going on but
I've got some weird stuff in here like
special search and special blank turns
out there are a whole bunch of special
pages pages where people can talk about
pages pages where people can have
back-channel conversations user pages
and I don't want those so I'm going to
run another version of this that removes
that stuff this should take another 9
seconds or so
all right and now we can see the topics
this one cracks me up in particular okay
so apparently this is what happens when
Mitt Romney comes out and says Donald
Trump is a jerk everybody wants to know
about rip now Mitt Romney's dog on the
roof in the carrier on the roof of his
car all over again right so you this is
this is an interesting data set just
because I get it laughs out of it
all right so there again this is just
emphasizing that I could get this wrong
and spark wouldn't be able to help so
I'm not going to bother going through
this this is just a parsing class here's
how I do the same thing with data frames
okay so notice I have three filters here
if I were to show you the query plan you
would notice that they're consolidated
and you can do that easily I can take
one of these things and do for instance
top 100 D F dot explain and it will show
me the final query plan and I can say
explain true and it will show me each
stage of optimization which is kind of
fun I'm not going to do that here
because I'm kind of at a time but again
not typesafe right if I do a take on
this you can see that what I get back
eventually is a row a row of things and
if I want to do if I want to get back to
something meaningful to the compiler
I've got to use as instance of which I
don't want to do right because I could
get that wrong so here's the final bit
that I want to go over and that's data
sets so now I'm going to read this there
are two ways to read this in here's one
I can read the data set directly into a
data frame and then do it as string on
it and then do the parsing at the data
set level or I can just take existing D
F data frame and do the eyes on that so
here I'm returning a triplet right and
there's my data again now it's in a
strongly typed string string tuple a
string string long tuple three right and
then I can I can do this as well I think
I've much rather have it in a case class
than a tuple three for access and
readability do I still have my
seven-point whatever million rows I do
if I do a take on it you can see that I
get these edit objects back and it came
back fairly quickly and then here's my
filter here's here's one operation I'm
sorry for the wrapping if I get rid of
the wrapping you won't be able to read
it but you can see that I put my filter
in there this is um this is a standard
filter lambda toss out anything that
isn't the en project and contains these
weird characters that I'm not interested
in do a group by on the page title what
the group by returns is this special
thing called a grouped data set that has
aggregations on it this is exactly
analogous to something called grouped
data in the data frame API and
eventually I imagine that those objects
will those classes will have very
similar API is going forward map it so
that I extract only the DSN tree out of
this because that's all I want and now
because there's no order by currently no
sorting I map it back to the data frame
so I can do an order by and then map it
back to the data set not something I'd
really like to do but I can do it and it
does work and then pull the the 100 out
and this should give me the same results
and it does and finally and I'll do this
really fast because I am officially out
of time I'm going to catch a couple of
these and just show you what they look
like in memory so I'm going to take the
data set that I was operating on them to
cache that and then I'm going to create
a an RDD that doesn't contain arrays of
strings but instead contains a raise of
the case class because that's going to
result in a different kind of memory
storage right so we're going to do that
and catch that and now I want to I want
to cache my data set that also has those
edit objects in it and as soon as these
three things are done I will we'll take
a look at how how they're stored in
memory I can look at the storage tab in
that UI and I can determine you know
what is the memory efficiency of these
guys this is assuming they get done so
here we have three of them in memory I
think the fourth one is underweight now
if there's the last one so let me
refresh this and you can see that the
RDD the the unpardonable takes up about
a gig as soon as we add the case class
that's the top one page counts parsed
RDD so now this is an RDD that doesn't
contain just strings it contains these
case classes now the in memory size has
blown up to about one just under 1.4 gig
but look at the data sets the data set
the first one here this is the direct
string one the second from the bottom
that's only 311 Meg in memory and even
when we add the the data set containing
the edit objects it's still only 323 why
because it's not actually cashing the
Edit objects it's using these fast
encoders to translate them into this
compact tungsten format and that's
what's we store so this is a fairly
dramatic representation of how much
memory you can save just by using data
sets and that's it we're done so any
I've probably have time for one question
we got one question in the back and then
we're going to got that bowl yeah so
that's a good question I really don't
know the answer the question was so
what's going to happen with the machine
learning libraries and what predicates
that question is the fact that the
original ml live was based purely on
rdd's and then they said oh no no no no
let's do ml and base that on this
pipeline model with their data frames
and so the question is will they be
folding datasets into the machine
learning library off the top of my head
I don't know the answer to that but I
can get the answer luckily because I'm
training with these guys I can get back
channel info to the development team so
I'll ask that and I'll try to tweak the
answer to that as soon as I find it all
right okay one more that's it and I'm
holding somebody else up so so how does
it work with different data formats like
different file types right so it's the
same as with the width it works the same
way as with data frames so with data
frames what happens is that there are
these adapters that allow you to read
certain file types the classic example
that I like to give is built-in is
support for text files
Parkay files and and JSON files and as
well as you know our DBMS is so anything
that has a self-describing schema like a
park a file or an our DBMS is trivial to
read into a data frame because you can
you can figure out the schema because
the file actually has a schema for
something like JSON they built an
adapter and included it in spark and by
default it samples your JSON file and it
says you know what are the fields in
there because if you think about JSON
you can always determine the field names
right and there are a limited number of
types that can mostly be inferred from
how the literals are expressed and so
what it will do is infer a schema out of
that and what if you have other ones
that you need to be able to do and the
answer typically there is either look
out on the net there's this place called
spark packages org that has a bunch of
adapters people wrote so data bricks
wrote one for CSV files for instance
that will do that or you have to start
with an RDD read it in parse the RDD
into pieces construct your own schema by
a number of different means case classes
being the obvious one and then move on
yeah I think we need to take that
offline the question is how the add
function works and I think we could be
here for 20 minutes talking about that
so why don't we take that one offline
all right so I think if I read the
schedule properly we have another break
at this point and Brendan McAdams is up
next and thanks for your attention I
appreciate it
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>