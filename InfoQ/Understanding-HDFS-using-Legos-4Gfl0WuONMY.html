<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding HDFS using Legos | Coder Coacher - Coaching Coders</title><meta content="Understanding HDFS using Legos - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Understanding HDFS using Legos</b></h2><h5 class="post__date">2015-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4Gfl0WuONMY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is Jessie Anderson and I'm an
instructor and I teach big data concepts
as well
Hadoop and spark in this video I'd like
to teach you about HDFS we're actually
going to do this in a very innovative
and very hands-on way we're actually
going to be using Legos let's start off
with let's talk a little bit about what
HDFS is it is a purpose-built file
system it is purpose built to handle the
needs of big data where that brings in a
few different changes so Hadoop or HDFS
stands her Hadoop the strated file
system where a distributed file system
functions a little bit different than
other file systems it also has some
caveats as a result of that one of those
caveats is that the files are immutable
that means that when you're writing a
file and you close that file handle you
can't go back and change that file
anymore that file is now read-only in
another sense it's also purpose-built to
handle big data in other words if you're
going to to fill your hard drive or
spill your fill your disk with a bunch
of small files that's going to make a
very important
those 1k files you probably don't want
to have those in HDFS what you're going
to want is you're going to want to aim
at one gigabyte or so files as you can
see I have several different pieces of
paper in front of me as well as some
Legos each one of these pieces of paper
represents something different in these
four pieces of paper represent computers
or nodes within my cluster this is node
1 node 2 node 3 and node 4 this node is
responsible for coordination and we're
going to give names and we're going to
talk about more about what these do in
just a second but now let's focus on the
Legos these Legos each one of these
Legos represents a file for example this
is a red file this is a yellow file and
this is the blue file in this red file
we're going to say that this is 128
megabytes this yellow file is a hundred
and 32 megabytes and this blue file is
32 megabytes as you can see each one of
these files
is a little bit different and and it's
colored different so that we can see
that and visualize how the files are
laid out within our cluster one of the
things that HDFS does different is
instead of dealing with a file 130 to
128 megabytes file all at once what it
does is it breaks it up into what are
called blocks
so this 128 megabytes file becomes two
blocks of 64 megabytes each this block
is 64 megabytes and this block is 64
megabytes in this block and the red file
we have several different pieces and as
we start to go right this out into our
cluster 8 dia HDFS does another thing it
actually creates replicas for us we'll
talk about the reasons for the replicas
later on but suffice it to say this
replica is important now as we go to put
this file into our cluster what we do is
we take this block and by default is
going to be broken up three times into
three separate replicas so here we have
one of our replicas is going to exist on
node 1 another replica is going to exist
on node 3 and another replica is going
to exist on node for looking at our
cluster we have three replicas of the
first block of the red file now let's do
this with a second the second block gets
replicated here on node 2 and node 3 and
node 4 as you can see this entire file
is replicated three different times node
here on node one we have block 1 block
block 1 block 1 3 times then we have
block 2 replicated 3 times as well now
let's see how we can how we deal with a
block a file like yellow file in this
yellow file you remember it's a little
bit different it's not exactly 128
megabytes we have a little bit here on
the end so now what we do is when we go
to break this file up into blocks we're
going to have three blocks now instead
of two because our file is a little bit
bigger it's 132 megabytes so here we
have one replica or excuse me one block
another block and another block let's
replicate this file around the cluster
just like we did with the red red file
here it gets broken up for us and the
yellow file block one is replicated all
around the cluster now let's do the same
with the second block and as you can see
this smaller replica or a smaller block
doesn't actually extend out to the 64
megabytes when we have a smaller end
piece or a smaller file that only
occupies as much data as it needs to so
this third block is going to be it is
going to be replicated around our
cluster like so as you can see once more
the yellow file is replicated in its
entirety three times on different nodes
within our cluster we have block one
here we have block two on these nodes
and we have block three on these nodes
now let's see how we can write to the
cluster we're going to write our blue
file to the cluster and as part of this
we're actually going to start talking
we're going to start saying to this to
this node and it's going to help us
coordinate things and that coordinator
is going to tell us okay I want to write
a blue file to the cluster and the
coordinator is going to come back and
say write that write that file to node
one two and three so what we do is here
in our cluster when we start working
with that the first replica is going to
be placed on the first node and then
remember how it the coordinator told us
to go into two other nodes what's going
to happen is I as the client talk
directly to node 1 node 1 is then going
to talk to node 3 and it's going to
replicate that data over
and then node3 is going to replicate
that data over to node 4 and this is
what's called a pipeline the pipeline
goes like this one of the important
parts or distinctions for HDFS is that I
is the client as I'm writing out this
data the data as soon as it has written
its entirety to the first node it is
going to be respond back and say ok your
data has been written whereas there
still there still may be that pipeline
transaction going on in the background
now let's say we want to read that file
let's say we want to read the blue file
now that we've written it the first
thing we have to do is we have to talk
to our coordinator node and we say
coordinator node where is the blue file
out I'd like to read it and the
coordinator node is going to come back
and say it's on node 1 node 3 and node 4
and as that data comes back is that
information comes back to us that note
is going to tell us in it in the order
that we should prioritize it from so
we're here we're going to read from node
1 and we're going to read that data back
notice that where there are very
separate things that are happening where
when we try to figure out what's
happening within the cluster we have to
talk to this coordinator node first we
have to say where are the places where
we could read the blue file from but
once we start actually trying to pull
that data and start work with that
working with that data we talk directly
to that node we don't want a single node
within our cluster having to have all
this data go through it
that becomes a bottleneck for us now
that we've seen a little bit more about
how this reading and writing happens
let's see how let's talk a little bit
about these functions and these Damons
in more depth in this in this example we
have four different machines and you
remember you've seen these machines
they've been responsible for reading
data as well as writing data writing out
these blocks and the daemon process that
does that is called the data node the
data node daemon is responsible for
reading blocks and writing blocks then
we've also seen how we have this
coordinator node and whenever I want to
find out something that's happening
within the cluster I have to talk to it
that's called the name node and the name
nodes responsibility is knowing what's
happening and coordinating what's
happening throughout the cluster for
HDFS as we just saw whenever I wanted to
read a file it's the name nodes job to
know that where each file is and which
block so on and so forth there's a lot
of information that it that it handles
so once again that coordinator is called
the name node and then the Damon Damon
process is that the Damon process for
these nodes is called the data node that
data node process once again is
responsible for reading and writing of
blocks we talked a little bit before
about how everything is replicated three
times within our cluster now let's talk
about why that's important in this case
we want to have three different replicas
and we'll talk here's an example of why
you'd need that let's say the Hulk comes
into your datacenter and smashes data
node three datanode three is now down
but it was storing several different
blocks in it as we can see there are
some blocks that that are what are
called under replicated and some blocks
are okay for example we have yellow
yellow block three is completely
replicated it's replicated three times
still it's replicated here here and here
however other blocks are now under epic
ated meaning that they aren't replicated
three times for example yellow one is
only here and here and yellow block two
is only replicated here and here only a
total of two times we want three times
what's going to happen is that the name
node is going to realize that data node
three is is no longer with us it's no
longer working and what it's going to do
is it's going to instruct the other
nodes and say you have a particular
block I want you to replicate that to
another node so let's actually do that
so as we were talking about yellow
block one is now under epic ated what's
going to happen is the name note is
going to tell us tell data node to data
node to replicate that block that you
have block one over to data node four
and now that data in its entirety you
remember how I talked about that the
data for these blocks are in their
entirety they're not a checksum or
something like that they're actually
that allows us to copy the data from
here over to here and creating create a
another copy of it in its entirety so
for the purposes of time and I'm just
going to do this very quickly and
manually and you're going to see that
I'm moving these blocks that were under
replicated before two new nodes so these
new nodes allow us to see that we're
back to having all of our blocks
replicated again this is a process that
the name node handles for us and does
automatically behind-the-scenes so where
whereas some of the things were under
Epple the blue file in three different
places there's another important concept
that we needed to talk about of reasons
why we have three replicas and that's
called data locality whenever we want to
run a MapReduce job specifically the map
portion we want to be able to access
that data locally on the system where if
the data does not exist locally on that
system it will have to be pulled across
the network to that node and be
processed this is very important because
as jobs are being run for us what's
going to happen is the priority is going
to be given to those nodes that actually
contain that data for example if I were
to run a MapReduce job over the blue
file the blue file when that when that
MapReduce job is run it's going to say I
really prefer that you run that job on
node 1 node 2 node 3 and let's imagine
we had 10 over node other nodes over
here we would say we really don't want
to run it on those we really want to run
it on these 3 and we give a higher power
for that we do that once more because
this node is able to read that very
quickly it's going to read it over that
vast SATA Drive that we have local to
that machine instead of having to read
it across the network from a different
machine this also comes into play when
we have other Damons that are located
right there with our data node one good
example of this is HBase whereíd where
HDFS works a little bit different with
HBase where as data as data is written
with HBase when it is located with a
data node the first replica is always
written locally to itself first and as a
result of that it gives you some
impressive speed improvement and
performance improvement by not having to
read across the network as that first
replica is written locally it's also
written to other replicas in the
background onto other nodes well I hope
this video helped you understand some of
the details and some of the ways that
HDFS works if you'd like to see
innovative ways of of learning such as
this where we're using Legos in order to
see how all this works for us then click
on the link that's in the description
and we'd love to talk to you about water
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>