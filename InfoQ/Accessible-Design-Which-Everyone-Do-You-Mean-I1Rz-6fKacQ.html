<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Accessible Design: Which “Everyone” Do You Mean? | Coder Coacher - Coaching Coders</title><meta content="Accessible Design: Which “Everyone” Do You Mean? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Accessible Design: Which “Everyone” Do You Mean?</b></h2><h5 class="post__date">2014-07-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/I1Rz-6fKacQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text"> 
DEREK FEATHERSTONE: Welcome
to Accessible Design,
Which Everyone Do You Mean.
I'm going to start right
off with an example.
 
We do a lot of work on
websites, helping people
figure out how to build
them the best way,
how to make them the most
accessible to as many
people as possible.
And this is kind of an
anonymized example of something
that we saw about a year,
maybe just over a year
ago when we were looking
at this interface.
And it's kind of a pretty
straightforward interface
that we're seeing kind of a lot.
It's in vogue right now.
And the idea is
that it's these four
cards that are on the screen.
And there's a call to
action to flip it over.
And so you click on Flip.
And you get more details
for that particular item.
This is a pretty common pattern.
And so we were
going through this,
and the team had actually
done some really good work
to make sure that
this was accessible.
And so they included all the
information in plain text.
It was pretty straightforward.
And each one of these
little, bottom right hand
corners that says Flip has
some text behind it that
distinguishes it from all the
others, because these four
calls to action for
a Screen Reader user
would all say Flip,
Flip, Flip, Flip.
And that's not really going to
make as much sense as it could.
And we've been told
to do that for years,
that we want to provide more
context for a Screen Reader
user.
And so we might do
that with Alt text
if that's a separate image.
We might do it with
some hidden text
we might do with
a title attribute.
There's lots of different
ways to disambiguate
each one of those
links from one another.
And so it worked
reasonably well.
What they had done was they
put on each bottom right hand
corner the Alt text was-- So
you see in the top left hand
corner we have the Ibura sport
coupe car, and then the Romia 3
series car, and then the-- I
can't even say these words.
Milara 305S car, and
the Ibura IX9 sedan car.
So each one of those, each
one of these calls to action
for the Screen Reader was
details for Milara 305S,
and details for
Ibura sport coupe
so that each one of those
links made more sense.
 
So that makes perfect sense.
We're going through that.
They had done a
reasonably good job.
And then we started to test
with Dragon Naturally Speaking,
somebody that uses voice
recognition software.
And the mantra for somebody
that uses voice recognition
software, and they're
usually using it
because they've got some kind
of a mobility or dexterity
impairment, some kind of a
challenge that prevents them
from using the mouse or
maybe even the keyboard.
So they're using voice
recognition for this.
So their call to action
in each case is Flip.
But what does the link say?
The link says Details
for Ibura Sport Coupe,
Details for Milara 305S,
Details for Ibura 9 Sedan,
Details for Romia 3 series.
So the voice
recognition software,
even though we've made this
work better for somebody that's
using a Screen
Reader, we've actually
made it worse for somebody
that uses voice recognition
software.
And this is the
kind of thing that I
mean when we're talking about
which everyone are we actually
designing and building for.
So the solution in this
case is to kind of combine
those two things.
So for a voice recognition
user, their mantra
is see it and say it.
So they see the call
to action, Flip.
So they're going
to say Click Flip.
And it's a call to
action, because you
can see the design makes
it a call to action.
It's got a different
graphic treatment.
It's got a word.
It's got the arrow, the word
Flip with an arrow beside it.
That's basically begging for
somebody to say Click Flip.
Now, it's not that it's
impossible for that Dragon
Naturally Speaking
user to do this.
They have other mechanisms
of getting at that content.
They could say Click
Link, and it'll
bring up a list of all
the links in the page,
much like a Screen Reader
user has a list of links.
A voice recognition
user can say Click Link
and it will put a little
arrow beside each one.
And so to get to this third
one here, Milara 305S,
it might put a number
three arrow beside that.
So the voice recognition user
would say Click Link Three.
So it still works.
They can still get at it, but
it takes a lot more thought
and a lot more time
than it really should.
If this thing isn't a link,
and it's quite possible
that these things aren't links,
then the voice recognition user
needs to go through an
exercise using the mouse grid.
So to click on that they
might say Mouse Grid,
and it'll divide the
screen up into nine chunks
with a number in
the middle of each,
and then they'll
drill down on that.
It might take them four,
or five, or six steps
just to be able
to click on this.
So when we think
about accessibility
we need to think
kind of beyond what
we're used to thinking about.
And I even wrote this in the
description for the talk.
So I'm a developer.
And I now consider myself
to be more of a designer,
but I started as a developer.
And in my first three or
four years as a developer
I thought I was creating
accessible designs and
accessible sites.
And it turns out, as
I look back on it now,
what I was really doing
was creating things
that were Screen
Reader compatible.
And there's a huge
difference between the two.
Screen Reader compatible
and working well--
And I don't mean Screen Reader
compatible in that it just
works with Jaws, or
voice over, or NVIDIA?
I mean, Screen Reader
compatible in that it
works with those technologies
from a technical perspective,
but it's also easy to use.
So we want to shoot for
something more than that.
We want to make this
stuff as accessible
as possible to
everybody regardless
of what kind of
technology they're using,
and we want to go
beyond that as well.
So that's the kind
of things that I
mean when I'm
talking about this.
So this is me.
I'm Derek Featherstone.
I'm @feather on Twitter if
you're tweeting this stuff.
Feel free to email me as well
if you want to follow up,
or if you want to copy
these slides right
away before anybody
else gets them.
I don't know if that's
possible, but we'll try that.
So feel free to email me,
feather@simplyaccessible.com.
I work at this
little tiny company
called Simply Accessible.
There's about nine of us.
And we focus pretty
much entirely
on making things
accessible and easy to use.
And all of these things that I'm
going to share with you today
are based on that experience.
Simply Accessible is fairly
new, but between us on the team
we've got well over 100 years
of experience doing this.
I've been in this field
for close to 15 years now.
And this is what we spend
our time doing every day.
So I really want you to ask
this question of yourself.
When you think
about accessibility
we often talk about
diversity, and inclusion,
and accessibility.
They're kind of all things
that go hand in hand.
And we say very often that we
are designing for everyone when
we're building
things and designing
them to be accessible.
But I really want
you to ask yourself
this question, which
everyone do you mean?
I did a search, and I used
this image as an example
because I was looking for some
stock photos to put in here.
And I did a search
for diversity.
And this image comes up.
And on first blush
it actually looks
like there's some
diversity in here.
There's men in here.
There's women in here.
There are white people in here.
There's African
Americans in here.
There are Asian people in here.
There's people with lots of
hair, people with no hair.
But when you take a look at it,
it's not actually that diverse.
But this is like one of the
first images that comes up
for a search for diversity.
And I think this is an
easy trap to fall into.
And this is something that
I think we do as designers,
and developers, product
managers, company owners.
We do this all the time.
When we say we're
designing for everyone,
we usually mean we're
designing for everyone
that's just like me.
That happens a lot.
This is a trap we fall into.
What's not well represented
in terms of diversity here?
Age.
 
Basically everybody in
this collage of photos
there's probably, in
fact, if you look closely
you'll even see that--
Whoops-- That this guy
is in there twice.
He's like right beside himself
in the top right corner.
I think that's
just an oversight.
But they put in him
like profile and face on
and they called that diversity.
There's no age difference
represented here.
There's no ability
difference represented here.
 
Is there nobody there
wearing glasses?
Let's just assume
that maybe there's
some people there
wearing contacts.
OK, we're going to go with that.
AUDIENCE: I'm just
saying like that's
the most basic thing to
[INAUDIBLE], like the most
common thing [INAUDIBLE].
DEREK FEATHERSTONE: And there's
nobody here with glasses.
 
And even there's
nobody that's got
any obvious religious
belief difference.
Right?
There's nobody here
that's wearing a hajib.
This Asian woman
who's on there twice
is wearing the same-- I'm from
Canada so we would call that
a toque, but I think you
call it a beanie down here.
But she's wearing that.
But that's it.
Like there's not diversity here.
But it was kind
of like out there
as, yeah, this is a photo
representing diversity.
So when we talk
about this we need
to ask this question, right?
What do we really
mean by diversity?
Who are we including and
who are we not including?
We talk about religion, gender,
age, ethnicity, philosophy.
There's all kinds
of other things
that make up what diversity is.
And I think we even want to talk
about ability and expertise.
And so when I'm talking
about ability and expertise
I'm even talking about things
like how experienced are you
with computers?
We're all here.
We're mostly geeks.
 
You know, no offense,
but we're geeks.
Right?
We are.
Do you believe me?
AUDIENCE: Oh, embrace it.
Yeah.
DEREK FEATHERSTONE:
Like we're geeks.
 
When I try to talk to
my dad about technology
and help him with things that's
really challenging for me,
because there's
things that he just
doesn't get because he didn't
grow up with this stuff.
That's just our reality.
So my dad's still like
a novice computer user.
And we quite often aim for
that middle ground right?
We're not looking
for just power users.
We're all pretty
much power users.
We're not necessarily looking
for power users or novice
users.
We're aiming for
somewhere in the middle.
There's a huge group of people
that we're kind of excluding.
I know when we do testing
we work with people
with disabilities all the time
and work on testing sites,
and actually putting
them through the paces,
and not in terms of just
the technical side but also
the overall usability
side of things.
And I can't tell you the
number of times where we've
worked with an absolutely
expert Dragon Naturally Speaking
user, or an expert
Screen Reader user,
and they've been able to get
through things that somebody
that was kind of an intermediate
level or a novice user,
they just couldn't do.
Right?
And we need to make sure
that we represent that.
Not just in the way that
we design, but in the way
that we test, and the way that
we release things to the world.
We have to remember that
we're not all power users.
The number of key
strokes that there
are for Jaws, or the
number of commands
that there are for Dragon
Naturally Speaking, I mean,
it's huge.
But we've worked
in lots of cases
before where we've been testing
right alongside somebody using
Dragon Naturally Speaking, for
example, and in order for them
to get through the
test we've actually
had to teach them
things, teach them
how to use the software
that they don't necessarily
know how to use.
So we want to keep
in mind that we
need to represent that
in everything that we do.
Now, let me give you a quick
rundown of all the people
that we talk about.
And I made myself promise this.
When we're talking about
people with disabilities
and accessibility
we're generally
talking about people
that are blind,
that have low vision, that
have some type of hearing
impairment, a mobility
or dexterity impairment.
And that could be a lot
of different things.
Mobility or dexterity could be
things like fine motor control,
just having low
strength, or maybe only
having the use of one hand.
My grandfather in the
mid '80s had a stroke,
and he was never able to
use his left hand again.
So that changed the way
that he did everything
because he had to do
everything with just one hand.
And I challenge you
to do that someday.
Try to just use one hand.
Use your computer.
If you want to do it with
computer type tasks, go for it.
But even basic things
like cutting an apple,
how are you going
to do that when
you only have the
use of one hand?
It changes your
perspective on things.
So these are the kinds
of things that we're
looking at in
mobility or dexterity.
We're also talking about
a lot of cognitive things.
And I'll be the first to
admit that making things
accessible for people that
are blind, or have low vision,
or have hearing
impairments, or mobility
or dexterity challenges
is way easier
than the cognitive
side of things.
The cognitive side of things is
the least well understood area
of accessibility in general.
And the typical
issues are usually
related to some type of
functional difficulty.
So we have difficulty
with paying attention.
We just had lunch right?
So I know that's what you're
experiencing right now.
We have memory related issues.
That happens after
we're out for a night,
like we're going to go out to
the Gordon Biersch afterwards.
So you will experience some
memory related difficulties
later on.
Literacy issues, routines
and predictability.
These are all things
that have an impact
on the cognitive side of things.
Their impact on
web accessibility
is actually pretty significant,
but we don't really
understand these things
nearly as well as
some of the other things.
We also are seeing more people
talk about vestibular issues
these days.
And so vestibular issues tend
to be things where there's
something with your balance or
with your inner ear where you
actually looking at a web
page, or using a web app,
or whatever it is,
start to experience
a whole lot of different
feelings or symptoms
as you might call
it, vertigo, nausea,
really severe headaches.
It limits the way that
you work with the web.
So we've had people working
with us before testing websites,
and they've actually had
significant issues with things
like parallax effects,
or working on a page
where as you scroll down
elements of the page
start to move around,
or they zoom in rapidly,
or zoom out rapidly.
Have you seen the presentation
tool called Prezi?
That rapid zoom interface,
that can actually cause nausea,
and migraines, and vertigo
type effects with people,
and it makes them not want
to use the web anymore,
or those particular sites.
So those are some of
the things that people
are starting to talk
about now as well.
And certainly speech as well.
There's a lot more
native speech capability
being built into browsers
and into different platforms.
And we would never-- Do you
remember the old days where
you would call in to a voice
recognition system via phone,
and, &quot;For help with
your bill, say billing.&quot;
Right?
And so you'd say billing,
and they're like,
no, we can't transfer you to
the Administrative Department.
No, billing.
No, I'm sorry we don't
have a department.
You would never
build an interface
that relies solely on speech.
You would always have some
kind of a backup for that.
And so when we're
building web interfaces
we need to do the
same kind of thing.
If you're building something
that is speech powered,
how do you build
that in such a way
that you can't assume
that speech will be there?
So let's start to think
about alternative mechanisms.
Now, I went through
that, I promised myself--
And I've already blown this,
so this is not some big reveal.
I told myself that when
I was doing this talk
I wasn't going to talk
about Screen Readers today.
But I've already blown
that, a little bit.
I'm going to try and spend
most of the rest of this talk
talking about things other
than Screen Reader users.
I'm going to try and
talk mostly about
people with low
vision, people that
have mobility and
dexterity impairments,
and people with
cognitive difficulties,
and then go beyond
that little bit.
That's my goal.
I'm going to try it.
I don't think it's
going to work,
but I'm going to try it anyway.
So here's a quick overview of
three types of relationships
that you have on a page.
And I'm showing you this
because of the words
that I just said I wasn't
going to say, Screen Readers.
So we have three
types of relationships
in a page that exist.
So this is a typical form.
We have three types.
We have an explicit
relationship,
implicit relationships, and
content based relationships.
So an explicit relationship is
a programmatic relationship.
So if you look at
this contact form,
where you're choosing
a screen name,
you can see right
there by the number one
I've got a label for that
form field, Screen Name.
That is programmatically
tied to the form field.
That's what we call an
explicit relationship.
If you take a look just below
that Screen Name form field
you'll see a whole bunch
of extra text, some hints.
Your nickname here at Corked,
one word, letters and numbers
only, no spaces or
special characters.
Now, that in this version
is not programmatically tied
to the field.
What it is though
is visually tied
to the field because
of its placement.
There's an implied
relationship there.
That's an implicit relationship.
And there's also another
relationship on the page.
If you take a look at the
error messages at the top,
it says, &quot;There were errors with
the information you entered.
Screen name has
already been taken.&quot;
that's not programmatically
tied to the field.
It's not visually
anywhere near the field.
But if you take a
look at it, there
is a connection to that field.
And the connection is that the
first two words in the error
message match exactly
the form field label.
So that's a content
based relationship.
People that rely on
assistive technology
really need the explicit,
programmatic relationships.
People that have low
vision rely a lot
on these implicit relationships.
Everybody relies on the
content based relationships.
So the content
based relationships
are important to everyone.
People that use certain types
of assistive technologies
have a need for the explicit.
And people with low vision, not
just people with low vision,
but they rely a lot more on
the implicit relationships
that we create.
They also rely on
explicit relationships.
But we're going to focus a
lot on the implied or implicit
relationships.
So let's take a look at
some low vision challenges.
This is a magnification
of the Best Buy website.
It's just like the
website splash page,
where you choose language, and
country, and things like that.
So you see here that the screen
is split into two horizontally.
So there's two big panes.
The one on the top is
the magnified view.
The one underneath is the
native view, so not magnified.
And this is fairly typical.
You might see somebody
that has low vision,
they might zoom in
on the entire screen.
They might not do
this split screen.
They might do the split
screen vertically instead.
They might also do a
picture in picture.
Right?
There's lots of different setups
for magnification software.
This is the software
that's built into Windows.
So everybody, if
you have Windows,
you have access to this.
If you're using Linux you
have access to magnification.
If you're using a Mac you
have access to magnification.
It's built in.
I'm just going to
let this movie play.
You're going to see
a few things happen.
You're going to see just kind
of moving around the interface,
and then we're going to
get passed the splash page,
and get into the
main product page.
So I just want you to
watch what happens.
I'll pause it at
a few key moments.
 
So I don't know if you
just saw what happened,
but it was pretty quick.
A very small motion in
the unmagnified view,
what does that lead to
in the magnified view?
Like that everything
is magnified, right?
The rate at which
the screen moves
is just that much quicker.
So one of the
things that happens,
I don't know how
well you can see it,
but you loose
context very quickly.
You think you know
where you are,
and something happens, and
it happens so quickly you
don't see what you went
past, because instead
of seeing the entire
screen you're just seeing
that small little slice
at any given time.
I'm going to keep
going, and I'm going
to move over down to the
bottom right hand side
where there's a little
language picker.
And I get up to the
language picker,
and it's got a list of about
six different languages
and countries in it.
And as I click the Select
box the list opens.
As I start to move
the list I can
make the selection in the list.
So there's about
six items there.
But I selected one that
was near the bottom, right?
And what happens is
it stays perfectly
in focus in the field of
view in the unmagnified view,
but when I was down near
the bottom of that list
and I made that selection, the
magnified view actually jumps.
So I was on something near
the bottom of the list.
Now suddenly I'm back
up right where I was.
Think about what this means
for somebody-- You know,
we create select
lists all the time
that are three miles long,
or maybe not three miles.
I'm Canadian, that
was an exaggeration.
Three kilometers long.
We get this jumping around
in that loss of context,
and it happens very
quickly and very easily.
So I get to this main page.
I'm going to go up to the
main Best Buy navigation.
And I'm in Products.
And I don't know if you
could see what happens,
but when I'm hovering the mouse
over Products and Services
on the main screen you can
see that this flyout menu
or the mega menu is
starting to appear.
I don't even know
that it's there
when I'm in the
zoomed in version.
So you've got to get
this idea in your head
that people are only
going to see, potentially,
a small portion of
the screen at once.
What does that
mean for something
like this where we've got this
big, visible mega menu that
shows up out of nowhere?
So let's keep going here.
I also want you to
look at-- So I'm
going to pause it right here.
We've got the Products
Mega Menu open.
We're in the Computers
and Tablets section.
And I've got the
Computers and Tablets
kind of subsection of
the mega menu showing.
On the bottom screen, the
unmagnified view, roughly
what percentage of
real estate does
that menu take up if you had
to put a percentage on it?
25%?
30%?
Maybe 35%?
Depends on if this
wasn't a half screen,
and it was a full screen,
it might be even less.
So that mega menu is taking up
maybe a quarter of the screen,
maybe less, maybe more.
But what happens in
the magnified view?
What real estate
is it taking up?
Everything.
One of the problems
that we've seen
in testing with real
people with a mega menu,
or doesn't even have to
be a mega menu per say,
it could be any
menuing system, it's,
again, easy to lose context.
Quite often you don't
know if you're in a menu
or are you in main page content?
Those visual clues
that this is a menu,
that there's a border there,
that there's a border there,
that there's this Computers and
Tablets highlighted state that
shows this is kind
of the active thing
that we're showing, that
we're underneath the Products
section, all of those
other visual cues are gone.
So when we're
looking at that, that
takes up now all
the real estate.
Again, much easier for
somebody to get lost.
This is a typical set of
challenges for somebody
that has low vision.
When they're looking
at a screen they
get to see a very
small portion at once.
I'm going to pick on Best
Buy again just for a minute
here, only because I can.
This is that same homepage.
Now, somebody that
has low vision,
they may have all kinds
of different reasons,
or all kinds of
different, I hate
saying this word,
but conditions that
contribute to their low vision.
So quite often they'll
magnify the screen.
One of the other
things that happens,
I'm not going to say how often
because I don't really know,
but this is a full
screen of light,
and each word-- So this is
just that home page-- This
is a whole screen of white
light that is shooting out
at the person's eyes, and
each letter is a little bit
absence of light.
So you're detecting
small absences of light
in a sea of light.
For a lot of people
that have low vision,
different conditions, that
actually strains their eyes
and makes it harder to read.
So what many people do is they
switch into high contrast mode
where they're white text
on a dark background.
So here's a typical high
contrast theme for Windows.
So we've now switched from a
white background with dark text
to a dark background with,
in this case, yellowish text.
So that's now really
high contrast,
but I want you to take
a look at the screen.
 
Whoops.
Gosh.
 
This is the screen before.
Take a look at it again.
And I'm going to switch
to high contrast mode.
And I want you to tell
me what's missing.
 
All those navigation
items are gone.
The search box is gone.
This happens because in high
contrast mode on Windows
the user is basically
saying I need
to modify this display, the
presentation of this display.
CSS background images
disappear in high contrast mode
in Windows.
This has a profound
impact on what you do.
Now, if you take a look here,
take a look at each section.
Now the user probably
is going to have
a tougher time
reading that content.
The call to action
is still there.
What we look at when we're
looking in high contrast
mode is what's missing?
And I could, in high contrast
mode, I could take my mouse,
and I could put it over
the navigation items,
and they would be there.
But I don't know
that they're there.
I don't know to put my mouse
over there because there's
nothing that's pulling
me in and saying
you need to put your mouse here.
So we're always looking for
things in high contrast mode.
Those background
images disappear.
So what you need to do is
look for this kind of thing,
review every interface that you
create in high contrast mode,
and look for loss of
functionality or meaning.
Is the functionality gone?
In that case search box was
gone, the navigation was gone.
You can still get
around the page.
There's other things
that you can do,
but that core, main
navigation, wow,
kind of an important
thing you would think.
I would hope anyway.
Don't just look for
loss of functionality,
but look for meaning as well.
Is there any
meaning that's lost?
Right, we've seen things--
I'm going to just back up here
for one second.
Take a look at that search box.
We've often seen
things like this,
where that search
box-- You can see it's
just a standard search
box-ish, but the call
to action that makes it
a search box to the user
is that little
magnifying glass that's
in the right hand side of it.
We've often seen a text
box used with a background
image for the magnifying glass.
But it's a native text box.
So what happens in that case?
If this was a native text
box with a background
image for the search, when you
switch into high contrast mode,
just the native text
box shows, and then
the search magnifying
glass is gone.
So now instead of
a call to action
that tells the user
this is a search field,
it now just looks like
any regular text field.
So not just loss
of functionality,
but loss of meaning.
So look for these things.
 
This is a bad, bad interface.
Can I come back to you?
This is a bad interface.
This is a tap faucet in
the shower from a hotel
that I stayed at over in the UK.
And I had a hot
water tap on the left
and I had a hot water
tap on the right.
This confused me greatly.
And I said, OK, I
will not shower today.
I don't like the sounds of this.
This is not going to be good.
This is not going to
end well for anybody.
It didn't make sense
until I saw a tile that
said the left tap operates
the shower and the right tap
operates the bath.
So now I at least
know if I'm going
to burn my head or my feet.
The problem with
this interface is
that the tap is down there
where you expect it to be,
and the tile that has
the instructions on it
is about six feet
away on the side wall.
These two things
need to be together.
This is a design principle
called proximity.
These related items
need to be together.
So let's apply this
to a web interface.
This is another example that we
found in working for a client.
And this was coded to be
programmatically accessible.
So it was programmatically
quite solid.
Screen Reader user wasn't going
to have much problem with this
because it was done well.
However, when you take
a look at the design,
it was actually a
bit of a nightmare
for somebody with low vision.
Now, I'm going to show you the
way that the information is
chunked here.
We have two sets of
questions on the left,
and we have two sets
of Yes/No radio buttons
on the right hand side.
So we've got these
blocks of content.
And we've also got these blocks
of buttons, a Quit button,
and then a Previous
and Next button.
I'm going to give you a really
simple test that you can do.
Some of you that have
been in some of my talks
before, you know exactly where
I'm going to go with this.
But if you can see the
screen, take either your left
or your right hand, and
I want you to hold it up
like you're squeezing a straw.
I want you to do
this, and I want
you to take a look
at the screen,
and go through the motions
of filling out this form
while you're looking
through the straw.
Make the straw as tiny as you
can, and fill out this form.
What would it take to
fill out this form.
I love doing this.
I don't have my
camera, but I always
take a photo of
people doing this.
And I'm being serious too.
Go through the motions
of filling this out.
We call this the straw test.
It's a very crude
simulation of low vision.
It doesn't actually
simulate anything
to do with the reasons
for the low vision,
but it does give you a
simulated really small field
of view, which is what
most people with low vision
have in some way,
shape, or form.
So what was the
motion that you needed
to go through to
fill out this form?
Left, right, left, right,
left, right, left right.
So when we're zoomed in,
if I'm using a magnifier,
this is what it might look like.
You still had the
benefit, if you
didn't close your
other eye, you still
have the benefit of seeing
the rest of the screen
to know where you are.
But if you're zoomed in this
is the kind of scenario,
and this is probably
at about 1,200%,
if you're looking at
this, and then you go
and you find those
answers, what has happened?
You've lost that context, right?
This happens to people with
low vision all the time.
So we move over.
We've got no sense of what
Yes/No set of radio buttons
goes with which question.
You can't find it, but you
have to go back and forth
and maintain it
to figure it out.
So we go back and we do
the other set of questions.
And we go back over to
the right hand side.
And now if we've done
all four questions
we've gone left, right, left,
right, left, right, left right.
Where are you going to go next?
Left.
Right to the Quit button.
This is not a great place for
our primary call to action,
especially when we've created
this left to right use pattern.
And if I go right over to
where those Yes/Nos were,
the first thing that I see
is that Previous button.
We've put the Next button,
and I say we very loosely
because I didn't do this,
but that Next button is
in probably the worst
possible place it could be in.
It's in the last place that
someone would ever look.
What else did you notice
about the buttons?
I'm going to backup
one screen here.
 
So they're graphics
that have text
on them, Quit,
Previous, and Next.
The only way to tell one button
from the others is to read it.
They all have something that's
called the same visual weight.
Right?
They've got the same size.
They've got the same color,
the same shape, the same font
size on them, everything.
Right?
They have the same
visual weight.
There's nothing that
attracts you to one more
than the others.
Now, if I go through this
and jump to the next slide
I'll show you what we did as
a kind of a counter proposal.
We wanted to deal with
this visual weight issue.
We also wanted to deal
with this proximity issue
in this massive left to right
scrolling that we created.
So what we did was we increased
the font size on each question.
We brought all the
Yes/Nos right underneath.
And I literally did this in the
browser as we were doing it.
So I randomly chose a color.
This is not the color that
I said that the client had
to go with or
anything like that.
But we distinguished the primary
call to action, the things
that we actually wanted
them to do, from the things
that they could do but weren't
really necessarily desirable.
So we used layout here to
create the right chunks.
So we still have four
chunks, but instead
of being left to right
they're all above like
stacked vertically.
As people who
create interfaces we
want to keep this
kind of thing in mind.
This is something
that works better
for somebody that has
low vision, because we're
eliminating a lot of
left to right scrolling.
We can now, if I'm zoomed in,
it's pretty straightforward.
We've got, generally
speaking, one direction
of motion instead of two.
We're making a much
easier for a low vision
person to be able to use
this, generally speaking.
I'm going to give you
one more example on this,
and then I'll answer your
questions, I promise.
I want you to think about this
in terms of data visualization
and graphs.
There's a lot of talk right
now about making things
like this accessible
to screen readers.
Absolutely.
It can totally be done.
But I want you to use
the straw test on this,
and I want you to tell me,
compare the value of India
to the Russian
Federation in 2009
for their gross
domestic product growth.
 
So India compared to the
Russian Federation in 2009
for gross domestic
product growth.
 
Now, I see some of you making
your straws really big.
That's cheating.
Keep your straws small.
 
How difficult is that to do?
It's difficult, right?
How do you know
which one is India
and which one is the
Russian Federation?
You have color, and you have
to go down here to find it.
And you've got to go back
up to find that line.
You've got to remind yourself
of which color is which.
You've got to find
them on there.
You've got to find 2009.
How did you find the
value for India for 2009?
You have to go
up, find that dot,
move all the way over
to find the scale, then
you had to go back to
2009, find the value
of the dot for the
Russian Federation,
and come back over to the scale.
There's lots of
ways to alleviate
some of these problems.
Very least of which is repeating
this scale on the other side.
We could take that scale and
repeat it on the other side.
If you take a look
at the point that's
there where I'm hovering, you
can see that in India in 2006
it was 9.263.
So that's actually
a good technique.
We've got this little
tool tip type mouse
over that helps us
see the actual values.
So we don't have to
go and use that scale.
But I've seen lots
of graphs and charts
where the individual
values for the points
aren't actually expressed in a
tool tip or anything like that.
So we need to keep
these things in mind.
What about for somebody
that's colorblind?
 
We have to think of other ways
of doing this kind of thing,
right?
And this is a
horrendously ugly example
that I'm about to show you.
It was more to show that
I can, not that I should.
But you can add different
styles of lines.
This is using a
charting library.
And most charting
libraries have the ability
to say what kind of
line do you want to use
and what kind of
shape do you want
to use for the data points?
So this was using both.
So we've got X's, triangles,
circles, and squares.
We've got a dotted
line, a dashed line.
You could have a solid line,
dot, dot, dash, dot, dash, dot.
It sounds Morse code-ish.
There's different things
that we need to keep in mind,
and that we need to
keep track of here.
So when you're looking
at any interface,
you can do this at
a wire frame level,
you can do it on
a mock-up level,
you can do it on a live site
level, use that straw test,
because what you will
do with that straw test
is uncover layout and
design challenges for people
with low vision.
Right?
We simulate that low field
of view or that small field,
you will reveal so
much stuff in your work
it will surprise and amaze you.
Now, I'm not saying that
you can change everything.
But there are really
obvious things
that you will be able to
change, like where you put calls
to action.
All right.
We see interface
all the time where
there's a Save button in
the top right hand corner,
but we've got a whole
set of editing fields
that come all the
way down the screen,
and then there's nothing
at the bottom field.
Right, the Save button is way
up on the right hand side.
So there was a
couple of questions.
You had a question first.
Yeah, so back on
that Best Buy page
where we had that
Select drop down,
and it kind of jumped
around, there really
is no solution for that.
It's something that
you kind of just
have to accept that
people are going
to lose a bit of that context.
And so that jumping
isn't something
that you can really fix.
AUDIENCE: Architect it
so it's really flat.
And make sure you
provide a search.
And you could also
make sure when
you click on General
Topics that you
put subtitles when you
click on [INAUDIBLE].
DEREK FEATHERSTONE: And so you
were talking about the language
selector right at the beginning,
and the country selector?
So one of the other
solutions, and I
guess I made an
assumption on what
you were looking for in
terms of the solution,
that you were looking for a
technical coding solution.
There is no code solution to
it, but you could certainly
make a lot of design
changes to it.
You could actually
set that up so
that instead of using
a Select you could just
use an actual flyout menu type
thing, not an actual select box
like a form select box.
You could craft your own
with a little expansion
where the jump and the
focus wouldn't change.
If you have more
questions about that
you can come and
talk to me after,
and I'll show you what I mean.
We can talk quickly about
some mobility and dexterity
challenges as well.
This is a pretty typical
like slider type interface.
And we've done a lot of work to
make these very Screen Reader
accessible these days.
So I'm going to let this play.
You can just see how this works.
We're setting some
different levels
here for-- This is how
much you spend annually
at an auto service shop.
And they've got a
rewards program.
And so as you adjust the sliders
to say how much you spend
on a particular type
of repair in a year
they adjust your annual
rewards that you get back
from that program.
And this is a fairly
typical set up for sliders.
We can make this
programmatically
accessible to a Screen
Reader user very easily.
We use a lot of
[INAUDIBLE] to do this.
We call each one of
these things a slider.
We add keyboard
handling to it, and I'm
manipulating all of this
using the keyboard right now.
Well, not right now, but
earlier when I recorded it.
This doesn't work well
though for somebody
that uses voice
recognition software.
These motions, and
I'll change it over
to this alternative
view that we created,
and I can use each
one of those sliders
with voice recognition
software if I want to.
I can look at it and I
can say, Click Parts.
And I can get the mouse cursor
in about three or four steps
to be on top of
that part slider.
And I can then say Mouse
Drag Right, Mouse Drag Left.
And it'll go pretty slowly.
I'm saying this fast, but
this doesn't mean anything.
I'm moving my hand
fairly slowly.
And then I can say
Faster, Faster Faster.
And it'll start to go faster.
But then I overshoot,
and I have to say Stop.
Mouse Drag Left.
And then I go back until
I get to that spot.
Very difficult to manipulate.
It can be done.
You can do drag and drop with
voice recognition software,
but, man, it's just
a hell of a lot
easier to put a text box there.
If I put a text
box there, instead
of me having to fiddle around
with a slider with voice
recognition software, and
saying move faster, or slower,
whatever it is, I can
just say Click Parts,
and put the cursor in there,
and type in the number 73.
It is probably something
where manipulating
it that way with the
text boxes probably
takes maybe 30 seconds in total.
Whereas to get those same
exact values with the sliders--
and, again, this is an
anonymized pseudo fictitious
example where the numbers
really don't matter,
but for a lot of
things the numbers
do matter what the outcome is.
So using drag and drop
with voice recognition
for course level
adjustments isn't so bad,
but when you're talking about
fine level adjustments, much
easier to just manipulate
those with the text box.
So we look at doing
something like this
where we have multiple ways
of solving the same problem.
Now, I want you to use
the straw test on this.
 
What's going to happen as
you're manipulating that slider?
When you're looking and
finding a slider to manipulate,
what happens?
You can't see what you're doing.
So we also always try
to build in for somebody
that has low vision,
something like this.
Now, it doesn't have to be
right on the handle itself,
or right on the thumb.
It could be a
value that shows up
above like we saw on the graph.
But having those
things related together
is going to make it 100
times easier for somebody
that has low vision.
That make sense?
 
I'm going to show you
this one other example.
This is a little
bit of a frustration
with voice recognition software.
 
Oh wait, my volume's not up.
I'll explain it you.
You can see what's
happening here.
I'm going to dictate
into the Specialty box.
 
And I said the word
throat, because I'm
looking for a throat
specialist for medical reasons.
And I told it to click Search.
And it goes off and
it does the search.
I'm going to now manually type
in, and you'll see this a lot
in interfaces that when
I'm typing it in manually
I get an auto populated
list of choices.
Quite often we see people
in interfaces assuming
that one of these auto suggested
items is going to be selected.
You can't make that assumption.
This is like progressive
enhancement, right?
You can't assume that
one of those choices
is going to happen.
With voice recognition
software, because of the way
the events fire, that
box didn't even show up.
So we need to have multiple
means to achieve the same goal.
We need to have multiple
methods for doing things.
Things like auto
suggest you should
allow for any random
text to be submitted.
Don't assume that
it's going to be
one of the choices
from the list box.
 
One cognitive example.
I'm going to let you
think about this.
I want you to read this screen,
and if you can't see the screen
you might want to get
somebody beside you
to tell you what the words are.
And actually, if you
can't see the screen
you have an unfair advantage,
because you need somebody
to read it to you.
So give it 10 seconds.
I want you to read this.
 
Do these words make
sense to any of you?
Individually they do, right?
Now, read it out
loud as a sentence
like it was a bedtime
story, for example.
 
Does it make more sense now?
When I switch it over, and
you'll be like, oh my gosh,
I can't believe this is
&quot;Little Red Riding Hood.&quot;
On the cognitive
side of things we're
only beginning to understand
the impact of that
on accessibility.
Each word can make
sense by itself.
That doesn't mean that it
makes a cohesive whole.
And I'm not saying that
people read things like that,
but think of this two ways.
Think of it as somebody that's
reading and has a literacy
difficulty, where each
individual word makes sense,
but the whole
doesn't make sense.
Or think of it as
somebody that is reading
a different language, or
you're speaking to them
and their native language
is different than yours.
How well do they understand
what you're saying?
This makes for some
real challenges
in cognitive disabilities
in terms of the content
that we create.
 
The last couple of
things to think about
are what are the
impacts-- We haven't even
talked about aging.
What are the impacts
of aging on what
we do with the Silver Tsunami?
What's the impact on
culture in what we
do in terms of accessibility?
We haven't even started
to explore this at all.
And I'll leave this concept.
This is a guy named
Edward T Hall.
And he wrote this book in the
mid '70s called Beyond Culture.
And he studied this.
He studied the
silent language, all
about how people
communicate without words.
And he created this concept of
high and low context cultures.
So in a high context culture
versus a low context culture
it's about the collective
versus the individual,
group empowerment
versus self empowerment,
authoritarian style of
information delivery
versus exploratory.
Accuracy is valued over speed.
Speed is valued over accuracy.
Context is implied based
on being in that culture,
and context is explicit
and self-contained.
And this difference between
high and low context culture,
of course, it is
a continuum, but I
want you to think about
what that potentially
means for the way
people understand
our sites and our apps.
I'm going to show you
four McDonald's websites.
This is the McDonald's website
from the US for the Filet O
Fish.
And this is the
Canadian Fillet O Fish.
This is the South
and East Indian,
like India, Filet O Fish.
And this is the North and
East Indian Filet O Fish.
And I love that
they're all different.
I don't know why on the American
one the cheese is on the top,
and all the other ones the
cheese is on the bottom.
I don't know.
There's different words that
are being used everywhere.
And if you take a look
on this final one,
this is part of what a
high context culture means.
So India is, generally speaking,
known as a high context
culture.
North America and Western Europe
is a very low context culture.
What's different about
this Filet O Fish?
 
AUDIENCE: Looks like
it's on a plate.
DEREK FEATHERSTONE:
It's on a plate.
It's on the table.
Whereas all these other ones are
mysterious UFO Filet O Fishes
that are kind of
floating everywhere.
Now, I don't know if it's
because they haven't got around
to updating the imagery, or
if that's done with intention,
but, man, do I want to
know more about this.
And I've only just started
to learn about this,
and explore this idea
of what a high context
versus a low context culture
is, and its impact on what we
do on the web and accessibility.
So that's kind of it,
because I'm totally talking,
and there's other people
waiting to get in here.
But I kind of care but don't
care, because you guys are fun,
and you're here with me.
So those are the kinds
of things that we
want to think about
when we're talking
about inclusion and diversity.
And this stuff is
totally new to me.
And I love that,
that it's something
that we can explore more.
So ask that question,
and again, which,
when you're asking
about accessibility
and thinking about
designing for everyone,
ask yourself which everyone
do you actually mean?
Thank you very much.
[CLAPPING]
 </div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>