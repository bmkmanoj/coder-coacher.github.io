<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>We Won! How Scala Conquered the Big Data World | Coder Coacher - Coaching Coders</title><meta content="We Won! How Scala Conquered the Big Data World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>We Won! How Scala Conquered the Big Data World</b></h2><h5 class="post__date">2015-04-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AHB6aJyhDSQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">why is this so dark okay thanks all
right why we won we actually
successfully took over the big data
world and this is a talk about why so
yeah we rock so I started doing Hadoop
consulting in 2011 and immediately
started trolling the community because
it sucked basically the the api's were
terrible so three years ago here
actually the Microsoft facility I did
this talk why big data needs to be
functional where I burned my state and
said we should be doing functional
programming and we should be using
languages like Scala that was you know
March 9th 2012 and I made this claim
which is rocketed around the intern
interwebs that Hadoop is the enterprise
javabeans of our time to be fair maybe
it's better to say that MapReduce is but
you know you might as well go big or go
home I suppose so but anyway this is my
claim and really the problem was
MapReduce it you know it to be fair it
did the job for a lot of people you know
it made cloud era guys or might make
them rich who knows anyway so you know
it let people get work done at massive
scale especially you know when you're
talking you know hundreds of terabytes
of data but it was really limited it was
basically flat MapReduce for starters
you just get these two steps where you
input data and do some initial mapping
and output key value pairs and then you
know shuffle them together SIF to do the
final reduction and if that sounds like
it can do everything you want it to do
then you're pretty you're smarter than I
am let's just put it that way because it
turns out it was you know had a lot of
problems one it was extremely
inefficient there was a lot of overhead
where David would be flush to disk
instead of cached for the next stage in
the pipeline and stuff like this it was
never designed to do anything other than
batch remote analysis where I have a
bunch of data on disk and I'm just going
to you know in Mass read it all into
memory and or stream it through or
something and process it whereas today
increasingly people want to do event
stream processing you know they don't
want to wait six hours for their edits
to their web pages to show up in Google
searches they want to see it as you know
minutes maybe so
actually becomes you know time to money
if you will I can throw in a little
marketing buzzword but maybe the worst
problem for us as developers is that it
was just hard to implement algorithms
that is a very limited programming model
and a lot of things don't fit it very
well like if you're doing iterative
algorithms like training machine
learning models walking graphs I mean
almost everything in the Internet
supposed to be a graph right all these
social networks and yet it's hard to
represent and things just graphs and
work with them in that way and the API
was particularly heinous in that talk I
showed you examples of it I won't do
that now for time and because you know I
want you guys not to lose your lunch but
fortunately even then there was you know
hope on the horizon Twitter had started
writing this API called scalding in
Scala that provided a lot of the same
collection api's that we're used to so
you could do flat map and map and reduce
and all this stuff and it sat on top of
a pretty nice Java API called cascading
it was still noisy pre Java eight Java
but at least it gave you idioms of data
flows and you know setting up sources
and syncs and you know doing a pipeline
of steps between them and all of that
was you know hiding most of the ugliness
of Map Reduce so that was a big step it
didn't address the problem of you know
fast streaming of data you could only do
MapReduce sort of stuff so Twitter
realized it was a lot of sort of
duplicate logic they were writing for
their streaming jobs in storm and their
MapReduce jobs so they came up with this
other layer API called summing bird that
tried to give you reuse across those
different things and they only went so
far though and it still didn't fix the
performance problems of MapReduce and it
and else it doesn't scale down very well
this is a problem for developers working
with MapReduce you had to have like a
mini cluster of sorts running on your
laptop just to test your dam code I mean
there were ways they tried to work
around it but it had no real sense of
running outside of Hadoop so you know
you couldn't just you know run it like
you would run a normal Java or Scala
process well to cut to the more recent
times in around 2013 cloud era the
biggest a new vendor recognized
MapReduce is kind of reached the end of
its life let's pick something new that
could replace it it was already this
open-source berkeley research project
called spark that it coincidentally had
started right about the same time hadoop
went mainstream in 2008-2009 and it
solved all these problems now to be fair
especially if you're about to run out
and deploy spark in production you know
it's not as battle-hardened as MapReduce
is so you know always trust but verify
that'll russian proverb that regen you
selected quote which i don't know in
russian but nevertheless see would say
in english but it gives us a couple of
very important things it solves all
these problems but maybe crucially it
has the right abstractions under the
hood upon which you can build other
things and so one of those or two of
those other things actually are not only
just this core API for doing like batch
mode processing but an extension where
you can shrink it down to do mini
batches this gets you close to real-time
processing but you know the sort of
90/10 solution it can't do single event
processing but if it's okay to just
capture say windows of ten seconds of
events and then process them like
batches you know that solves a lot of
problems right therefore but people have
and they've layered on top of this SQL
semantics so that you can write queries
when that makes sense so I'll you know
no talk should be without code so let's
actually quickly go through a very
simple example and this is actually a
complete example it's I've lead it a few
details but this would actually work in
production if you wanted to use it you
know where we start with you know
creating the entry point a spark context
and then layer on top of that a sequel
context in a streaming context this is
the spark way of doing things they have
this this is kind of a bit idiomatic
scholar they have you import the members
of the sequel object sequel context
object but that's just their choice I
don't know he's kind of strange anyway
and you typically use case classes to
represent your schema with the problem
that they're limited to 22 fields and
because this is still in Scala 210 but
you know there's use just use nested
types to work around it
but anyway so that the data set here is
actually it was inspired by a real data
set you can download which is the
records of all flights that have
happened between I think it's North
American airports for the last 15 years
or so it's actually kind of fun and play
with because it has all the flights that
were delayed and canceled and reasons
and all that it's actually shocking how
many flights are canceled when you look
at this data those of you that flail out
like me probably shouldn't be surprised
but it I was surprised
anyway so that's what I'm going to use
for schema and so I'm going to in this
case I'm going to strain the state I'm
going to imagine that this is actually
coming in in real time and I just want
to be keeping running statistics of
what's going on in the air so I'm going
to open up a socket and you know listen
to this data coming in from some server
and then I'm going to iterate over it
and I have a little idiot at ik way that
I like to parse the data let's say it's
just coming in as raw text so I'm going
to parse it into these objects not a big
deal of how that's done but here's the
crucial bit that every time one of those
batches arrives you know like the window
of 10 seconds passes or whatever I can
just start doing all kinds of crap to
that whatever I want to do to that
analysis in this case I just did a
sequel query where I'm going to do a
group by statement which you know for
things like group by and joins it's
really hard to be sequel for concision
now I should say that's you know that
you immediately should say oh my gosh
you've got something that isn't checked
at compile time and that's true there's
a way to get around that problem but
nevertheless the point being that when
you have the right abstractions you can
layer on top all kinds of different
tools and in some of those from now
including machine learning libraries and
graph algorithms you know to do whatever
job you have to do do it streaming if
you have to do it with sequel if that's
what works for your data analysts etc
and then you just start it running and
tell it to stop when it finishes which
could be never it might run forever well
it turns out this is taking the world by
storm
we basically won this battle I feel
vindicated and I take the completely
credit why not
sometimes trolling works but you know
why why does it work well we get these
amazing DSL
I cannot think of any way to make this
more concise to express a calculation
this is actually doing the inverted
index but we won't go into that the only
thing I can think of that would make
this less noisy is to remove the case
key words but of course you know they're
there for pattern meshing so I don't
want to do that but it's just incredible
how it just gets out of your way and
lets you focus on the problem we have
this rich ecosystem of all of these
tools including legacy tools we've had
like the JVM licker IDs and so forth and
even Hadoop which we continue to
leverage we've got some great math
libraries we heard two fantastic talks
today already about spire algebra is
another Twitter library for doing like
mono eights at scale and stuff like that
Helen has done some amazing work
integrating Cassandra and spark and
Kafka she's got this really cool a demo
app called killer weather without an
ef-2 the ki ll you might want to check
out that shows how to nicely integrate
Cassandra as a data repository spark as
your processing engine and Kafka is a
source and sink of data in Kafka itself
which is one of the other hot tools
that's everyone is starting to use now
it's it's a message queue at scale
invented at LinkedIn and it's also
written in Scala
so under 10 minutes that's my story
that's why we won mic drop questions
questions
answers questions answers yeah so when
you say Sparky's thinking world by storm
how does storm people this storm is
interesting there's you really need
storm if you actually want to do
individual event handling the problem
storm is going to face going forward is
that people love to buy one tool that
does it all even if it sucks at any one
thing it does my favorite example of
this is Microsoft Office because and I'm
serious about this I'm I've been around
a little while and I remember when
before office came out you went to buy
you know like Harvard graphics was a
presentation tool and WordPerfect
I know it's sending some of you back on
memory lane here but all these different
tools that were all great but you had to
buy a bunch of different tools it didn't
work together at all then Microsoft came
up with one thing to buy it sucked but
people just wanted one thing so they
bought it and through the other guys out
of business so I actually think people
going to use spark even if it's not the
best tool because it's kind of one tool
to fit everything but but storm is still
the best tool for those individual event
processing problems but then you also
get this problem of code reuse that
summing bird tries to solve of I don't
want to write duplicate logic that's
going to be streaming in in batch mode
and so forth you book anybody else yeah
yeah so not only do you get one shop
once one-stop shopping but you get
integration between the tools so that I
think that's really crucial any what was
there another one somewhere
yeah yeah I think in my example I just
dumped to the console I don't even
remember now but yeah normally you would
write to a message queue right to
Cassandra right to the file system
there's all kinds of basically spark
uses to the Hadoop io layer if you will
so you anything you can do with
MapReduce you can do with spark and as
far as where you're going to send output
or like Mark said send it into some
other jobs like stream it to another
process that's going to do you know more
heavyweight analytics or whatever so
yeah
google it and you'll find it on github
its ki LLR and then whether all one word
yeah or talk to Helen over here and
she'd be happy to tell you about it
anybody else back there right so yeah
caca plus Straka streaming and akka
clustering would be a fantastic
streaming tool and hid bet has not
escaped our notice let me just put it
that way and now that's all I'll say
right now you can read that where you
can read that for what you want for what
typesafe might do but we're thinking
about how all these could fit together
just just one last comment I wanted to
make shameless plug my programming Scala
book is half off at O'Reilly today if
you want to you know the e-book anyway
20 bucks can't go wrong baby needs a new
shoes alright maybe I'll stop thanks
it's called programming Scala second
edition</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>