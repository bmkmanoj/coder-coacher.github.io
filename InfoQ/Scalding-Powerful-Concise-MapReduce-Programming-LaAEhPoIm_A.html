<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalding: Powerful &amp; Concise MapReduce Programming | Coder Coacher - Coaching Coders</title><meta content="Scalding: Powerful &amp; Concise MapReduce Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalding: Powerful &amp; Concise MapReduce Programming</b></h2><h5 class="post__date">2012-05-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LaAEhPoIm_A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we're not scalding it's available on
github you can follow it on twitter and
wanted to thank stack mob and also
Americana for hosting this Scala meet up
okay so the the talk is going to be
appreciated for we're going to talk
about scalding we're actually I'm going
to tell you how we use it at Twitter
when I go into how its implemented the
extra special sauce that you get this
time that you didn't get last time if
you've seen any previous talks is the in
detail algorithm on how to deal with
heavy date askew in your in your data
sets that's pretty pretty big deal I
kind of alluded to in the past I talked
about it how do you walk the graph
Twitter graph given it like you know one
in ten people follow Lady Gaga and like
one in ten people follow justin bieber
right you're gonna like something naive
if you're going to hash Lady Gaga to one
machine and then like you're going to
send ten percent of your edges to that
machine it's going to be a big disaster
and then our garrus only like you know
one in 15 people follow him and so that
machine won't be like loaded at all so
like how do you deal with that and last
we're and talk a little bit about like
what's coming up next for scalding and
the cool thing is like people guilt
guilt us into like doing one thing or
another and you could do that and the
next time we give a talk maybe you're
awesome feature will be the next thing
and there's like now it's like totally
new stuff yeah okay so um this is what
scalding looks like so what's scalding
all about so scalding so a scalding came
about because ah really avi bryant who
is now moving to etsy will be bringing
scalding there they use Scala I sorry
Ruby and cascading now they don't use
skull scalding yet as president but he
didn't really want to write pig anymore
so if you like really into pig and like
pigs awesome you want everything to be
sql-like scoldings probably not going to
be for you and that like that's cool
like everybody can do their own thing
there's a lot of different projects what
scalding is all about is trying to make
the scala collections epi somewhat close
to like a table model of computation
that you would see in a database or in a
spreadsheet there are some people who
are kind of like on either end of this
sure so they're like oh I'm going to try
to make Hadoop look like a like a
database for you that's not what what
scalding trying to do some people on the
other end of the picture where they're
trying to make like Hadoop look like
just exactly like computing on
collections so they're going to give you
a list and you're going to do like
reduces in this list and scoldings not
exactly doing that either right now so
instead we have this picture so the idea
here is that we have all your from so uh
just to get an idea how many of you like
are all like crazy into Scala already
and like you know everything you do is a
monad and like you know everything's
awesome like you know that's good okay
how many of you like I heard Scala like
might be cool and I know basically
nothing about it and it's sort of like
Java and I maybe in the future want to
do that so that's kind of more like okay
we're doing it make sense so the the
functions that you see here have
functions of the same name in the Scala
collections API so flat map is one of
these things that freaks people out if
they're if they've never heard of it but
it's like a really useful concept that
it's like I want to go through a list
and rather than mapping each item in
that list on to something I want to map
each item in that list on to a set of
things and then glue them all together
and why is this so cool like if i gave
you just one function map or flat map
you'd be a fool to take flat map because
i could just take sorry i'm a fool
apparently you'd be a fool to take mad
why because you could just map each
element in flat map to a list of one
thing right so I can simulate your your
map function with my flat map no problem
but also I could do a filter if I only
want to keep half the elements I could
map the ones I don't want to keep on to
a list of 0 items right so flat map is
like is like the canonical map in
MapReduce that you see in Hadoop so we
just flat map you're basically good to
go if you just implement that and get it
right you're fine ok so this appears
here so for some ridiculous reason it
must imagine someone wrote the first
word count example for
MapReduce programming everyone has
everyone does this and because we're I'm
on an imaginative really it's my fault
so this is the implementation in
scalding so we take a text line it might
be like you know all the web pages in
the world and like we're going to look
at these line by line and the
parallelism comes because this map is
fundamentally parallelizable and in
Hadoop you'll have a bunch of nodes 100
maybe a thousand because they do kind of
sucks it's on like a million nodes but
like you know a lot of nodes arm and
you're going to go through and process
all of those in parallel because the
away the algorithm structured you know
that it must be you know there's there's
nothing that can interfere there's no
concurrency problems they're going to
show up so you map each line on to in
this case all the words in that line so
you can do this in scalding because you
can just inline functions right there if
you're familiar with something like peg
or so you're you know i don't know like
SQL you can't very easily just jam your
own functions in here we're taking this
function tokenize which we define here
and it's a really simple function I
don't know it it replaces anything it's
not all these characters with empty
space and then it splits on spaces using
a regular expression just normal Java
functions that are all there in the Java
stl Java standard library on tech on
strings so once you've done this you
take in each each line and mapped it on
to a list of words so if it was like
hello how are you that one line becomes
now a list of hello how are you right
and then you go on to the next line and
you keep doing that so you've glued all
these words together and to use giant
like very simple list of single words
and now there's a function called group
by this works a little bit different in
scalding than it does in Scala in Scala
it takes a function and it says I'm
going to I'm going to look at this thing
in your list and I'm going to map it
onto a key and so like a simple way to
think about this might be that i'm going
to map like you know we want to sort all
the men and women in this room and the
sorting function will return either M or
F right and that's your group by
function and
then you're going to get a mapping that
takes em in the list of all the men and
then F and a list of all the women right
that's how MapReduce works it basically
it's those two things you have the map
this is where you do the shuffling you
do this group and then you do some
reduction on it okay so in this case
what we're grouping on is the word
itself so each word will be sent to a
different list so we'll have you know
hello all the hollows will be in one
word all the house will be in one word
all the usual sorry all the hellos will
be in one list all the house will be in
one list all the users will be in one
list and so on down the line right and
now we're going to a really trivial
calculation on that list which is let me
get the size of that list so now you've
changed these all these sentences into
this map of word how long was the list
but the link the list was just how many
times that word appeared that's word
count right how appeared five times you
appeared two times etc and now you've
got that now you've got this now you've
got two columns now you've got the word
column and the size column and you can
keep doing reductions in sculling you
can keep going you could say we could
say give me the first character in each
word and like you know I don't know
treat them as numbers and add them all
up you can keep adding more and more
reductions and those would all happen in
that same reduce phase on on Hadoop so
this is like awesome if you don't know
much about Hadoop or Scala it's like you
know welcome to being lost but is it's
like it's like it's like how much can
one person introduce one anyone talk so
finally we've got that we just write it
out and boom that's it that's it that's
like the the prototypical word count
everyone a lot of people start with that
that's that yes yeah so let me review
the question the question was what is uh
so what is it let's talk about the types
here so if you're in a scala you're in
two types you want like strongly type
thing and a lot of this is
is well typed so let me answer that
question but then we can move on in a
second talk about some of the bad news
like elsewhere the the type of this
object is a source that represents like
where data is read or write on Hadoop
right so that's pretty straightforward
but in between scalding is a scholar
library it's a DSL for cascading it's a
Java library that you can use in a lot
of voice and that Java library uses it
represents these computations as pipes
basically things flowing around you can
think of these pipes as like lists like
I think of the more like list when I can
operate on them and all these objects
here are now these pipes now the deeper
question is what was going on here so
group by if you're if you're new to
scala this group I took a function and
this function did something and it's a
little bit I mean it's like it's a
slight it almost doesn't decide a hand
if you if you look at it but what group
by does is it hands you a thing that's
weak it's like a builder pattern it
hands you this object which you will
describe the calculation you're going to
build up on the reduction because in
Hadoop we don't want to go through the
data more than once we want to only
going to want to go through it as few
times as possible once is almost already
too many so if you want to take the
average of a bunch of numbers and the
sum of a bunch of numbers and you want
to look at the I fell iment in the tuple
and like take the max of those numbers
you don't want to go through that over
and over again in the sky loss native
library to do that you would be forced
like there's not a really good
composition story there you you go
through it over and over again or you
could write some function that leg goes
through this giant to pole and does all
these different calculations on it
there's actually a really nice story
around it here is that we're taking this
group builder thing that has a bun all
the reduction methods will see it later
and we can do all these different
reductions one reduction is to get the
size size will return the group builder
so you can keep going we could also see
let me get the max and you can keep
going whenever you stop you're done
building up this giant calculation that
will run in the reducer it's not really
running here this is like code you can
compile and you would run it nothing at
happens it's it's code that returns a
calculation so it's it's code that's
like it's the output of this code is a
calculation yes yeah but it's exactly
right yeah so so this will all get
compiled into java bytecode behind the
scenes the things that it passes through
this will get serialized using Java
serialization it'll get shipped over to
all the mappers and the reducers in in
Hadoop and then they're going to put it
through the appropriate parts of this
code it's exactly what will happen uh
Hadoop and cascading handle that portion
of it yeah yeah so it's all ready to go
and by the way I didn't jump up and down
about this any other time I've talked
about it but I probably should there's
this really cool library called cry oh I
don't know if you're familiar with it
but you should really look into it if
you're not it's it's about as fast as
per debuff which is a lot faster than
thrift it is compact as protobuf or
thrift and you don't have to write a
definition for your class that you want
to serialize in most cases if you try to
do something crazy like serializing an
open socket or something it's obviously
not going to work but for most of the
value types that we have in Scala it
just works it's just so you can jam
anything into these things and send them
around you could have case classes you
could do you know a scholar list you
could pass anything around and they're
just magically going to almost magically
going to work yeah
yeah I only did this in this case
actually just to show that you could put
of an anonymous function up there if you
wanted to or you can call any other
function that you like yeah yeah yeah I
mean it's just like it could just be
like bob's your uncle and like that
could have been it too it was just to
show you that like there's no
constraints on what you do you consider
any code around yeah KR y ou cry oh it's
awesome also so so it's also used in
spark if you're interested in Scala and
like in memory MapReduce style it's also
using Casca log which those guys work
with us a Twitter for our closure
version of all of this and also storm
uses it so it's it's and I've heard some
people yahoo use it so it's not like
just some crazy dudes using it well in
addition to crazy just using it I think
we've kind of gone through all this you
can call these functions are you can
define them in line or you can call any
function that you already road so we're
all set to go so this is pretty awesome
so now the data model as I mentioned is
that you have a stream of name tuples
like you can picture this as you're
always calculating with like a
spreadsheet and you've got this code
that like tells you how to modify the
spreadsheet okay so you have column a
one and now I'm going to map onto column
a2 and things like this and it's a nice
model but it's that's where you're going
to see the difference between scalding
and like the standard Scala collections
API so uh we can actually run this and
I'll run this in a second and later
we'll go through more examples but we
have this like little driver script that
allows you to take your jobs that you
write the Scala job it'll in the
background run the compiler ship it over
to our cluster run it if we want to if
you want to run it locally you can do
dash just local and it will run it
locally as well and pretty soon now real
soon now a twin chin or somebody or
maybe you guys well at EMR support it's
like relatively easy to do that there's
some cascading support for that already
people have done that that would be
awesome someone else I did we don't use
EMR so it's not like going to be like
top on
it'll be so easy and so awesome and like
these demos will be so much better if we
could do that so you guys should someone
get on that let's get a trip so how does
how does word count actually run so
pulling some good going into actually
and here is word count it's the exact
same code that we did before right so
nothing up my sleeve and then we i
downloaded alice in wonderland so we
have Alice in Wonderland and I'm sure
everyone wants to know what the top
words in Alice in Wonderland are down
the rabbit hole something like this so
we have sculled this is the slow driver
script w road and i want to run it in
local mode and order a word count and I
want to have I have so that job had to
so it had some args parsing that we
built into the solving and we take the
input R and then we eventually right to
the output are and the way that that
kind of canonically works is just with
dash dashes so we just we run at
so now it's a it's compiling the job
it's linking in some various Hadoop
libraries which may or may not be on
your system and we hacked it up so it
magically uh downloads up from maven
Scala compiler is like super blindingly
fast it's just my laptop that's low if
anyone ever tell you the Scala compiler
slow their line it's only my laptop
that's a joke for people who actually
know that the Scala compiler is crazy
insanely slow and now let's take a
little chick at the words here we go so
ah there they are I think if we search
for Alice it'll be one of the top words
there we go nope that's not the right
one that's Alice ! oh yeah I thought we
did too but apparently maybe someone can
it spot the code the error could see you
want to engage in some real time
debugging let's see so anything that is
not text it's not totally obvious to me
yeah I do to lower as well I think it's
not yeah no no we can open up the repo
and play around we're only splitting on
spaces but we're replacing everything in
that set with space right he's not in
that space right it's because it's not
so it shouldn't anyways there we go
see I mean if we can't get this right I
would be very suspect that this whole
thing is not going to blow up on you
when you try it but apparently it works
somewhat okay okay so that's where we
are with that so so what scuttling all
about so scuttling has like three sets
of functions and it's these are really
like the MapReduce kind of primitives
yep that's a good question for that like
that's so the question was can you run
the jobs from the repple and the answer
is right now you cannot and like so why
is that it would seem like it would be
nice and easy to do things from the
repple but the way that the scholar
repple works is that every line
basically ivan delve too deeply into it
but they get put in as a the body of a
main method in some anonymous object
that gets created at that moment in time
and that model breaks like the way that
we're threading behind the scenes this
cascading flow that we build up and get
and then submit to the Hadoop cluster in
principle it is surmountable but it
probably involves writing our own rep
all like spark is done and that's just
something that we haven't done yet it
would be really nice because that we
have a lot of them we have it you know
we have several implicit switch are a
scholar feature that are very useful
it's free people out that enable a lot
of the dsl and if we had our if we do
implement our own ruppel we can make
sure all those implicit SAR imported by
default and that would make it a little
bit cleaner and you get like probably
replace it would be nicer to replace the
skull dollar beat are yours is
threatened to do this so you can now him
about it you might actually do this so
so there's three sets of functions that
you have there's really two that really
matter and last one is built on top of
the other one and you have the map
functions so if you're just used to like
like the idea of MapReduce from like
maybe you saw in Python or you saw it in
list there's this map phase that's
totally paralyzed and then you reduce it
down to one thing you go through a big
list and you get one value so how does
that picture if you're not familiar with
Hadoop going to do so the idea in Hadoop
is that usually you can shard the
reducing by something else and you don't
reduce down to one value you might
reduce it down like I want to do a bunch
of mapping and then group by a bunch of
days and for each day I want to reduce
things down or I want to have a bunch of
products and over each of those say a
hundred or thousand or a million
products I want to reduce some things
down so in Hadoop that's the picture you
do the map you define some kind of
grouping and then on the grouping you're
going to do some kind of reduction and
that's where you see all the standard
you're going to see the standard you
know some you know average take head
sort whatever that you probably know
from your normal functional programming
there's a pretty decent API on the
github page I'm not going to get through
all of it but we'll see some of it here
um but so each of the functions you see
where the group's you see represented up
there like the map like functions the
grouping functions and the joint
operations so as I've said about 20
times usually if you're familiar with
the collections API and you can imagine
how you structure your calculation using
Scala collections you could usually just
like say ok how would I take that over
to scalding and look up and like oh
there's that function there's this
function there's scan there's take
there's fold there's reduce all the
things that like if you're used to
functional programming they'll all be
there the only thing this different is
the notion of named fields so rather
than just having a map you're operating
on this job is giant couple it's sitting
in the background and you when you say I
want to map you say I you ask you tell
me which fields do you want to map on
and then I'll take some subset of them
and give them to you and then you're
going to map and then you're going to
give something back to me and I'm going
to change that temple around so that's
the whole picture so here you see an
example where we can use by index let me
get the first three elements out of the
two ball and then I'm going to map them
on two tuples name these three things
source 0 destination blah blah blah so
there's two contexts in what we in which
so you can kind of read that arrows
we're mapping this onto that thing there
the other way that we use the arrow is
with adjoining operations and a join is
again it's lifted from an ocean like
databases you don't see this in the
Scala API but
you should we should have a co group
this group by there should be co group
right why not co group if there was co
group in the the API everybody would
have a much easier time writing up
MapReduce jobs but I join if you're not
familiar with it notice how much
everyone's familiar with but you're just
saying on this list I've got a bunch of
keys and on this list some of those keys
appear also and everywhere where the
keys up here in both lists I want to
make a cross product and it just I mean
it's a pretty simple mathematical
operation but it's very common in
databases and so if you're simulating
something like a database or databases
calculation in a dupe it's very useful
and so this syntax here means the source
on the left hand side I want to source
zero I want to everywhere on the right
hand side PR which is some other thing
that kind of comes in I'm going to map I
want to join those two together and then
output tuples that form this cross
product it's pretty straightforward I
mean it's it's the same thing you've
seen in SQL it's co group in Pig it's co
group and cascading there we go that's
how it works boom so I've mentioned
cascading a couple times so what's
what's up a cascading what is this
relationship with scalding so the cool
thing about cascading so there's like
three big scalding dsl's for Hadoop
there's like like like I heard like oh
we're Twitter so we're the big one so it
doesn't seem like but apparently we're
the big one if you're going to use one
it's scalding apparently we've won but
if you don't use scalding there's
another one called Scooby which they
have their own flow planner their own
mapping of Scott lata Hadoop primitives
there's also one called scrunch from the
guys at Cloudera and they have a project
called crunch and scrunch is like after
I told Josh wills how awesome Scala
would be for his crunch library he got
religion and scrunch and so it's awesome
to the difference is cascading has had
years of use I mean like people have
been banging on it for years and many
many people are using it's an active
community is really well tested the 1.2
branch is like really rock solid it
a little bit hard to find problems with
it major problems at least you'll know
it feel it's very safe library it's
going to wherein about to release 2.0 we
still have see a couple bucks here and
there but it's a great library it's
really well tested it's super optimized
it's high performance so scalding is
based on that you're not like you know
when you're using it you're not using
like some crazy guys flow planner who
cooked it up last week and it'll be
awesome real soon now it like it works
it's in it's very fast it's got a fast
local mode which I just showed you that
was not using Hadoop at all so you can
use cascade you can use scalding or
cascading on your cluster you can then
write the same job and run it locally
and that's really handy to be able to
use the same concepts and how you deal
with data for the cluster and locally
it's a real pain if you work with some
other systems you might use something on
your cluster and then bring it into map
sorry are or maybe you know excel in
some cases locally that's like it's like
this impedance mismatch is no fun one
cool thing is that it has a flow planner
that's portable especially now so
there's two flow planners now the Hadoop
flow planner and there's a local flow
planner but you can easily write a spark
flow planner which I really want to push
Chris quincel concurrent to do or we've
also talked about doing a real-time flow
planner on top of storm so you could
have streaming like batch operations
there's in transactional mood in storm
which you're familiar with it it's a
kind of a stream like I do like a real
time I do anything calls it and it would
be possible to bring cascading to that
and then you could use your exact same
jobs in all these different cases that
would be awesome the bad part is if
you're really into like functional
programming and you're going to type
safety and all that cascading stupa
models basically an array of objects and
so even though scalding cares about
types it takes your word for it like
when you say that this is like a
function that takes a double to shrink
to something really it's saying okay I'm
going to go get a double on a string out
of this cascading to pole but that
cascading tuple it can't can tell the
compiler weds got inside so that's an
unfortunate part about it that's one
thing i don't like about it the other
thing that is like a little bit outside
relative
some of the other systems is that
cascading fueled model as I mentioned
before like it just doesn't mesh well
it's like a record model that a nun type
record model that doesn't match well
with a strongly-typed like collections
API like scholars collection is API or
like what you might see in Haskell or
something that having been said it's
like it's really productive like so we
have a lot of people use it every day we
have a relatively large team they're
using a day in and day out they don't
like lose a lot of sleep because like it
doesn't have like like a lot of strong
functional purity to it and it like it
basically works with the model of like
tuples and naming columns and everything
it's like meshes well with how people
think so I haven't been that like
motivated to like do that much about it
so this is just a grep through one of
the repos just account but we've got
like more than 60 in production jobs and
those are jobs that like if they don't
run we get alerts and like bad things
happen that are running on the system
we've got more than 200 just like ad hoc
jobs someone just like hey you know how
many times to justin bieber say i'm
going to you know have a beer in the
last three days I don't know probably
didn't say that actually this is going
on the web like it's gonna make me I'm
in trouble all right no we don't do that
kind of stuff but I mean like it's
public data anyways I mean I can see how
many times justin beaver said it but
like that's not what the few hundred
jobs are but like you get asked a
question you need to answer the question
so like and they work so ah the one
thing I don't want to give you the
impression though is that somehow it's
our only tool at Twitter I don't want to
misrepresent we've got a lot of people
analytics who I know and love and they
love Pig so we have a lot of tools there
the Cask log guys I mentioned before
they're using cascading there's another
Python dsl inside of Twitter being
developed by cascading boots so we're
using a lot of cascading but we also use
pic and a little bit of hive so um I
want to talk a little bit about the
implementation issues in a minute we're
going to switch over and do the hands-on
I think I'm gonna hand it off to our
garrus at that stage but before I do
that we're gonna talk a little bit about
like actually i might even like
throwback the audience it looked like a
lot of people were
new to scala and if i go in detail on
like how like the implicit work which is
like a weird scala feature weird to most
people who don't know scala yet I think
we might just like like not get the most
use out of our time so I think I might
skip that unless people are really
interested in like how does the how do
you implement a DSL in in Scala to like
match with like a Java API is that like
something like is there like some
serious interest in that or should we go
to the hands-on hands-on okay so anyways
these slides will be available so if
someone can see this implicit stuff so
there we go arm so before yes so here's
what we're gonna do we're going to talk
a little bit about the the group built
their stuff how the reductions work and
then we'll go to the hands-on so this is
what we're going to see so this question
came up earlier what is the type that
this grouping operates on and the type
that it operates on is this group
builder okay and that keeps happening
that's kind of fun if it weren't
annoying okay so in this case we're
going to take some a bunch of numbers
we're going to group them up by this
value X but after we do that we're going
to do a series of calculations on them
the first one is this function it looks
lame you wouldn't do this on locally
because you just it looks makes your
code look muddy but if you're going to
do good to the list once you don't want
to have to go through and calculate like
go to the list five times to calculate
all the moments you care about so we
have this function size average and
standard deviation okay so what this
thing is going to do is it's going to
take group thing group numbers by X
let's take Y and then let me add on
three more fields into my to Paul how
many values for y saw the average value
for Y in the standard deviation for y so
now we've grown the to pull out from one
element it had X in it now it has four
elements X the number of Y values for
that X the average number of what the
average value of y for that value of x
and the standard deviation of Y for that
value value and then we can do it
it's mine it was as much as you want I
apologize for the cut and paste code
this was this from a unit test but you
can do it as much as you want after this
we've got for now we got five six seven
columns and now we add one more we've
got eight at the end I say actually I
only want these five and then they get
written out in those five columns so
there you go so we can do all these
parallel reductions at the same time so
you might have like this one object you
might ask a bunch of questions about it
you can keep doing these by adding more
and more elements to the tuple inside
the grouping so one function that we
have like the main function that we have
in cascading a primitive that it offers
you to do this kind of parallel
reduction it can be referent can be
thought of as a MapReduce and then
followed by a map so what's going on
here is once you set up the grouping you
might first do some kind of prepare tory
phase and then you do a standard reduce
on them so you're combining a bunch of
things together but then when you're
done you might want to clean up so
that's the map the reducing the map so
what's an example of that so the one I
just put up sighs average standard
deviation I want to thank the average of
why I wanted to get off all the moments
let me like the first 10 moments how do
I prepare the first step is I take Y and
I splat it out to all the five moments
of wire in this case it was three
moments does your earth power that's one
the first power that's why the third
second power that's y squared now I do
normal reduced which would just like sum
them all up and then how do I finish up
well once I finish up I take one I get a
Naga to count I've got the sum of all
the wise and I've got the sum of all of
them squared and I'm sure at least
ninety-five percent of you if I gave you
those three numbers can compute the mean
and the standard deviation the means
easy you just divide the sum of wise by
the count serie v ations slightly more
complicated but not much and I'm sure
you figure it out so that idea that you
want to first set up then do the reduce
then clean up very powerful how powerful
almost everything we have is implemented
this way if you want to count something
that satisfies a predicate what's the
setup phase well if it satisfies the
predicate map it to one if it does
mappa 20 what's the reduce phase the
reduced phase is obvious you add you do
just some some is a very common reduce
function what's the last phase well
that's kind of common a lot of times you
don't have to clean up so we have an
identity function is the man that is
super annoying if I were just like eight
feet tall it would be more convenient
but so at the end there is a identity
function so same thing with for all are
all these predicates true for this thing
how do I say well I take the function is
it true for each of them that's the Map
Reduce and all the way through what's
the cleanup no cleanup average here's
like a very complicated algorithm that
is a streaming average that's more
stable for giant lists so if you add
giant numbers to small numbers and you
want to avoid roundoff errors this is a
slightly better algorithm for it this
one actually has non-trivial steps on
all of them you first set up by mapping
x21 and acts you do this reduction which
is associative and commutative and then
finally the cleanup is non-trivial you
take the second element of the tuple out
almost everything that we have is
implemented this way why do we do it
this way it can be pushed to the mappers
in many cases it's more efficient if
you're all into combiners and MapReduce
and all that sort of thing you can just
grip through the code it's like almost
everything is written in terms of that
function so it turns out to be a really
valuable function so now I'm going to do
the hands-on all right so let's take a
look if you actually download scolding
from github we have a tutorials
directory that contains a bunch of
introductory material that you can use
to get a little bit more familiar with a
code we're just going to walk through a
couple of those just so that you can
take a look at like the V anatomy of a
scolding job and how it's actually how
you can actually write your own too all
right so how does the how do you
actually write a scolding job yourself
basically all the jobs look something
like this you said
some input source one or more you set up
an output source that you want to write
to you do some basic processing on the
tuples that are contained within that
source and then you just write the
output out in this particular case we're
not doing anything interesting at all
we're just reading an input file from
like a little data file that we have in
here and we're writing it back out
without doing anything the main thing to
look at is this text line definition
here this is one of the standard types
of sources that are supported in
scolding natively the only two types
that we support out of the books right
now our text line on tsv text line just
basically reads text one line at a time
and gives you two fields one field is
the line number the other field is the
line and the tsv is basically a source
that gives you a set of fields one per
tab separated column inside the five of
your reading we also have additional
types of data that we support there is a
another repo that you can download from
github called elephant bird and what
that allows you to do is allows you to
read data which is saved in HDFS in
protobuf or thread format so if you guys
are basically using robot for thrift you
can use that there's schemes cascading
schemes that allow you to read data in
that format too and the main thing to
realize about a source is that there's
basically two components that the damn
No
yeah there's basically two components on
the source and that has to do a lot with
how can skating works one component is
the type of the data that is stored in
HDFS and the other component is the path
structure inside HDFS what does that
mean a lot of the data that we have is
processed daily so for example we we
have a root directory in HDFS that
contains let's say for example the tweet
and then we have a timestamp let's say
by hour or by day or depending on what
your schema you're using that saves the
data for that particular hour day or so
on scolding contains a relatively easy
way to manipulate date which is actually
a library that we're probably going to
spin off eventually in its own in its
own repo that lets you pass command line
argument for like in start times and end
times and you can limit the amount of
data that you're looking at for that
particular time region you want to look
so we have this notion of a time path
source and we have a notion of a fixed
path source fixed path searches means
read everything in this HDFS directory
time path source means read everything
in all the directories that are within
this time range in this case text line
is just a fixed path source it's just
basically what we're telling scolding
here is go and read the data that is
contained contained inside tutorial /
data hello to txt and whatever
processing you do just write it in
tutorial / data output 0 dot txt is this
clear okay now because this particular
example doesn't do anything interesting
we're just going to go and read it and
read the second one which is a little
bit more interesting
alright so in this case we'll look at
like the first sort of you know non
trivial example of doing something keep
in mind that these types of texts
lineart of type source source is like
one of the main objects that you deal
with in scalding the other main object
that you deal with scoping is this
notion of a pipe a pipe is the object
that has all the methods that Oscar was
talking about like map and flat map and
filter and so on the way to transform a
source into a pipe is by reading it so
when you called ode read on a given
source we're doing is you're basically
taking the data that is contained in
that source and converting into the
subject that you can call all the
difference map map and reduce methods
own for example here the only thing
we're doing is we're taking this text
line we're reading it this will give us
a pipe which will contain the line
number and the line itself and then
we're saying all I want to do is I want
to get the line so only the text that is
contained contained inside that line and
write it out as an output
here we're doing something a little bit
more complicated we just want to take
every line and reverse it what this will
actually do is it will it will replace
the field called line which is the input
field in this mapping operation with
this fuel called reversed which is the
output of this in line method which all
it does take the line reverses it and
replaces it in that pipe so again like
the difference of this my of this map
with like the kind of map that you would
call in a scalar collection is that you
basically pass it to arguments the first
argument is this this coupled to that
basically tells it what the input field
days and what the output field days and
the second argument is the inline
function that tells it what operation
perform on the sub set of tuples
generated from this does does this make
sense alright
so a lot of times when you're writing
your jobs you might want to actually
pass input arguments and the way that we
do that is we have our own little arc
parsing library which is which is shown
in this art object so the way that this
is done is you can basically treat this
args object as a map and when you are
trying like for example in this line
where I'm picking up the input in line
47 I'm not using a fixed name for the
input for the input file location but
I'm passing it in through the args
object the moment I i call the key input
on the args object I am Telling the flow
planner that when you call when you
broke basically this job you have to
pass a dash dash input parameter
otherwise the compiler is gonna freak
out and it's not going to work for the
output I'm still writing in the same
place like just in tutorial data output
3 and this happens for for this
particular implementation of job like I
I just you know I can i can add more
arguments if I want to and that's the
way that I deal with input argument
there are other subclasses of job that
will constrain you to also have to pass
let's say a date as a command line
argument and this arts object basically
gives you the flexibility to either
require something or optionally set it
now none if it's not there I don't know
if that makes sense
alright here's a little bit here's
basically an example that shows you how
a group by works so in the first three
lines of this job I'm just splitting the
string by word so this would be like the
first step 202 a word count let's say
and here is the grouping that Oscar was
talking about when Oscar writes his jobs
he generally likes to use the underscore
notation for the for the anonymous
function that you pass the grouping here
we're making it more explicit that you
know this is a group builder object that
you're passing around and all we're
doing is we're doing a size and again in
the output I'm not even though i'm
reading from a text line here i'm
writing to a tsv so this is the tsv that
has two columns one column being the
word on the second one being the count
so here's an example that shows you
having you can actually join in this
case we're actually picking up the the
native dictionary file that exists in
omaxe well I guess no linux
distributions right too so all we're
doing is we're reading this up as as a
number on a score no actually sorry
we're good so here's what we're doing
here this is a file you can actually
look it up
yeah so this is just a file that has
words and an scores I don't see the
scores though oh the scores up in line
numbers okay that's cool and what we're
doing here is we're taking the the field
numb with just a line number and
converting it into a score in this is a
very contrived example but what we're
doing out at the end of this operation
is we just have a score and a word and
this is our first pipe this course pipe
then we're getting against the input and
we're flattening it out to like to a
word and this step here is the one that
is actually doing the work but this does
is it joins one pipe this course pipe
with another pipe the the one that we
got from the from the text line which is
the one that we call the join with
larger on and on the joint keys actually
the word itself and finally we're
grouping by the the line itself and just
getting for every line the sum of all
the scores of all the words that appear
in this line you can imagine doing like
more complicated things here anyway any
questions sorry we are the two inputs
the first input was the dictionary file
in in usershare dicked words and the
second one is actually easier to find
you could pass anything in there any
more questions so the question is what
the development process is for this it
depends I think what Oscar it ends up
doing a lot is we actually I followed
two is we subsample our data sets into
small chunks that we can actually fit in
our laptops and we ran a lot of these
jobs in local mode and so you have a
small subset of the data that you're
gonna run over you first run it on local
mode on your machine see that everything
works and then you then you shipped over
and try to run it on your cluster the
good thing about
this and I don't know if it became very
clear when we gave the talk or like
throughout this talk is that because of
the way that cascading works you
essentially have three stages to your
job the first one is compilation the
second one is flow planning and the last
one is actually when like the running
phase so in the compilation face you can
like you know catch like stupid errors
or whatever like stupid bugs when your
job doesn't compile in the flow planning
stage what the what the flow planner
will actually do is we'll see that all
the fields of all the tuples that you're
defining create a flow that makes sense
if that doesn't happen at that point no
job will be scheduled and it will fail
at that stage you can also get failures
in the in the retaining the running
stage if you have like something like a
nullpointerexception or some other or
bug that we have in code or something
like that but the good news is that most
of the errors will be picked up in there
either in the compilation page or the
flow planning stage so that helps a lot
like that helps you save a lot of time
yes so flat map takes two arguments the
first argument is a topple two of from
fields to two fields and the second
argument is an anonymous function that
goes from a couple of the same size as
your as your input field with the
exception being when you have a field
like just one input field with the
exception being when you have only one
input field in which case instead of a
double is going to be one field and the
output of that anonymous function has to
be an iterable so what flat map will do
is it will create at Apple which will be
as created by the subset of the fields
that you define your from field and
populate the two fields with with the
with the output of your anonymous method
that doesn't make sense so the question
is does flat my create the number in
this a number of rows the answer is it
doesn't yeah the the answer is it
doesn't it creates so for every row that
is processed it creates as many rows as
the output of the anonymous function
creates so
so if you're anonymous function creates
gives back a nun it's going to create
zero rows I want so the question is is
the pipe gaand is the what happens when
you operate the pipes are always
immutable so you can kind of think of
them as frozen in time they're never
going to change so when you take this
pipe and then you flat map over here
you're creating a new pipe that is going
to be larger or smaller who knows
because the flat map can shrink or grow
the number of elements what I was just
typing while our garrus was was talking
about flat map was an example in the
scala repo where I took a list of two
things you can think of that as a pipe
that has two things there's two strings
you all everybody and is a lyric from
lost and what I did was I did this the
normal the word count flat map that we
keep seeing up there did the exact same
one and you see that it returns a list
that says you all everybody is a lyric
from lost if you count that if I do dot
size on it you see there's eight things
now so we started with two lines we flap
mapped it out wound up with eight lines
the total analogy of this we could we
could also do this with a but I could
have done so here's a man so I can
capture this as on top of all the stuff
this awesome hi tech computers doing
it's also playing some felonious monk in
the background anyone's listening to but
there we go so there's this it's not
really a pipe it's a list but now we do
the flat map on it flat map you know
line-to-line split Shh same kind of code
that we've done and now let me capture
this in a new pipe Val new pipe this
would be the kind of code that we would
write in scalding all the time now new
pipe when you take a look that's the one
that has a size of a and if I group you
know you can do a group by and in Scala
as well I can group by i don't know like
take the the word and get the 0th
element out of that oops
and that makes some big mapping whatever
but all that's there but now notice pipe
is still untouched pipe still looks the
same and that's semantics those
semantics still worked the same in
scalding as well so that data I could
also do something like pipe dot make
string and glue this all together into
one big line and that didn't change the
fact like actually that was a bad
example because that you get the same
result whether you did it on the first
or second pipe but whatever yeah all
right let's leave this works so wait a
minute okay so let's try to run this
example as we said like the first input
that we're passing is this dictionary
file and the second one that we're
passing is you're right it did finish
compiling is this one this hello to txt
file let's take a look at this it just
has four words so it's not necessarily
that interesting but let's run it
anyways so one mode that we're doing
here is we we don't need to compile the
whole world every time we want to run a
job something that we do to make
everything a little bit faster is we
keep around like an assembly jar that
we've built with all the necessary
sources that we need and every time you
run like the driver scripts called RB
what it does is it compiles a thin jar
for this particular skala file for your
job adding that into the classpath like
your big assembly chair into the class
but so that makes compilation time much
faster for your little job that you want
to run and you want to run a little bit
quickly so yeah so that means that yeah
so that basically makes things a little
bit better anyway so so this is the
output the line goodbye world if we sum
the scores for good
Mayan world will get 2320 below 220 230
2,500 sorry guys I can't speak and for
hello world will just get 315,000 975 so
yeah that's the output any more
questions oh that's a very good question
what's a scolding gem script about so as
Oscar explained one of the things that
we're doing is we are converting
cascading tuples into Scylla tuples just
make to make the whole dsl feel much
more Skala like and that actually
requires doing a bunch of implicit for
each one of the tuples from 1 to 22 so
there were two ways to do this either we
would take the same code and copy paste
at 22 times or we write a little script
that generates a code so that's what it
does and in fact one of them like more
let's say experimental features that we
are about to release is this whole
notion of an abstract algebra this is
something that all scar works on a lot
the idea being that if you define the
the notion of over of a mono it and the
notion of a group and the notion of a
ring then the operations that you need
for each one of those is like you know a
zero and a plus and then you need like a
x + 1 and all these things so you could
for example define a plus operator for
two tuples right I just some although
all the different fields inside each one
of the tuples together and that's it
right i just called + on each one of
those and you can have like nested mono
aids and so on and so forth so this
would be very useful for doing things
like reductions on multiple fields at
the same time so but in order to do that
again you need to do all the mono
implicit conversions for each one of the
top holes from top of 12 double 22 we
had to generate a bunch of code for that
too so because like you know our
scripting language of choice is Ruben
the gem scripts are Ruby script does
that answer a question cool please so
it's not really like so the question is
if there is any best practice in terms
of the number of joints that you do
within a job
actually what ends up being a little bit
more critical is how much data is Q
there is in a given join that you want
to do the thing is if those joints are
different joints that are happening the
cascading flow planner is going to be
smart enough to break those up into a
very like a different number of
MapReduce steps so if you're doing too
many what will end up happening is the
whole flow will take like a lot of time
to run but nothing bad will happen also
keep in mind that none of the like if
you actually want to access any of the
data and actually read it and do stuff
based on it like you want to have some
logic or you want to run a loop where a
job will can will rerun if like some for
example you're running something like
page rank for example then you can't do
that at the flow planning stage you have
to wait for your flow to complete and
then access the data read it and decide
whether you want to keep going so that
could be a limiting factor now if you
have issues with data skew we're
actually in the process of implementing
SQ join we have this notion of what we
call a block join that yeah that that I
could talk about if you want to but what
that basically does is it helps you do
joins of pipes that were like some of
the keys would pass into the same
reducer and like that would cause a lot
of the data to go to the same reducer
and everything would take forever to run
oh so how does scolding know if an
operation is not associative well the
only like we have a couple of operations
that are not so if you actually look at
the group builder code we implement
logic for how to do a particular
operation in a way that would be
associative or in a way that would do
everything at the reducer and the moment
you add an operation in the group
builder that is not associative at that
point we cancel any maps I'll
aggregation we put everything at the
reducer so we keep track of that and the
moment you try to do something that
would break the associativity just give
up and say okay that means we need to
push everything to the reducer an
example of that would be something like
a fold left that would need to go
through
re element inside your group you can't
do that from a pro map side right the
like let's say that the main like
functions such as sum or average or
average some average deviation or
MapReduce map those retain a social DVD
so as long as you're using those you're
good there are other methods like fold
lift or scan lift or sort which other
ones do we have they're fixing the
library no you can't yeah and there's
another list of functions that are not
associative on the moment you use any of
those you just broke enough activity so
yeah that's a really good example
actually we rarely and maybe never deal
with data that is text in Twitter like
almost all the data we deal with either
thrift store protobuf the thing is that
we haven't pulled that as a dependency
inside the scolding project itself
because we didn't want to make it too
heavy but there is another project
elephant bird and it's quite easy to to
pull in that and and and use that to be
able to read and write to to let's say
for example lgo compressed thrift and
program just to answer your question the
the code looks exactly the same like
nothing changes you just make a few
definitions about your source and then
the rest of the job looks exactly the
same yeah it's basically a different
implementation of a source here that's
all it is yes
yeah so yeah you can take them and Sam's
gonna love this so Sam Ritchie is one of
the guys from Cass clog and he works at
twitter also and we're always like you
know trying you know Sam sitting there
trying to convert you over to using cats
clog which is like a closure version of
like cascading and then we're over here
or maybe me more than are yours isn't
the zealot that I am maybe but I'm
trying to preach the gospel of scalding
and but we've agreed to collaborate on
some subset of things so meat locker
cascading cryo we have like a whole like
github / cascades a bunch of projects
and a lot of those are just like salmon
us and Chris pencil from concurrent and
so Sam some like you know former Olympic
rower like CrossFit crazy dude and we
were joking about something some about
putting cryo stuff in the freezer and
we're we have to give things names and
people hate code names and it became
meat locker and it's so awesome that
meat locker gets brought up it's like a
big question this is awesome it's
Twitter secret strategy to destroy
Facebook that's really the answer he
didn't just say that anyway yeah so yeah
doesn't answer your question it's it so
what we're doing is we're basically
breaking off pieces that are common to
projects that use cascading but are
written in different languages and we're
just trying to use these around so
that's that's the part of serialization
that Sam pulled out into its own project
here was an example of how would you go
about like doing thrift like and so here
so we have these this is this is code
that's not been released yet there's no
reason why I shouldn't be or couldn't be
but it's just not yet and so if you want
to implement your this yourself you can
see it's like five lines of code and you
basically say you wanted to find a new
like in this case we make a trait for
lgo compressed protobuf and we say that
the h there's a scheme this is an ocean
from cascading is the lco protobuf
schemes and these guys are defined
somewhere up here these are defined in
the elephant bird project that is open
source
so you can download that that's another
Twitter project and it's like five lines
of code to adapt it to being so the
schemes are cascading schemes but it's
like like I said I mean it's literally
five lines of code to make it a scalding
source and then you're good to go so we
could release this file or whatever but
and this is just like our users
mentioning if we want to deal with dates
we've got some logic around that but
it's pretty straightforward that's
actually so the question is is there a
way to define a mapping from the tailor
protobuf fields into into cascading
fields awesome question the answer is
yes you can do that in one line of code
we have methods called pack and unpack
what I'm pack does is it's a little bit
dirty but it works so I like it what it
does is it uses reflection on the field
name to call the right Gator on on the
protobuf object or the thrift object and
takes it out and put it into the field
with that name the the good news is that
it doesn't do that at runtime it does
that flow at flow scheduling time so it
will check whether those getters and
setters exist in your in your object and
if they don't we'll fail without
scheduling any job so it's dirty but
it's not as dirty as it sounds so that's
what unpack does it gets out the field
and pack will do the opposite like if
you've named your fields the right way
it will actually use the cascading sorry
the thrift or put above setters to set
them into the object so you don't need
to write a bunch of boilerplate code to
instantiate the object put everything in
and put it out you just do it like that
so you have to operate that way you can
these objects can just pass along the
whole thrift object to you the tuple the
fields can contain a whole object so
like our like you know log in trees
might have like 100 like there might be
a thrift object with like a hundred
fields inside they don't have to be
flattened out in two tuples they cannot
you know you can pass those objects
around through your pipes so it's not
like pig or something that only has
primitive elements in the tuples you can
also have your thrift object right in
the Drupal so the pack in the back is
there when you want to flatten it out
but often is the case that you want to
just compute directly on the object
itself cool thanks guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>