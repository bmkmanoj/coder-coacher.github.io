<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction to Forecasting in Machine Learning and Deep Learning | Coder Coacher - Coaching Coders</title><meta content="Introduction to Forecasting in Machine Learning and Deep Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction to Forecasting in Machine Learning and Deep Learning</b></h2><h5 class="post__date">2018-05-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bn8rVBuIcFg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone my name is Fran
I'm a data science lead at uber before
joining Ober about three and a half
years ago I did a postdoc at Cal Tech
doing approximate quantum dynamics since
starting at uber I founded a couple of
our sequence teams including our
real-time anomaly detection team
forecasting platform team and also
recently our natural language processing
efforts so here the three key takeaways
for this talk I'll be touching on some
of the popular forecasting methodologies
we're going to talk about the importance
of back testing in order to compare
various different forecasting methods as
well as prediction intervals which are
critical in order to understand and give
actionable business insights and then
finally I'm going to also give you a
glimpse of some of the cutting-edge work
that we're doing in the forecasting
realm at uber so forecasting is truly
ubiquitous no matter which business
you're in whether this is in finance in
manufacturing or also for example
meteorology not surprisingly also at
uber we have a plethora of different
forecasting use cases and here I'm
showing a three of the many forecasting
use cases that we have the first one
being marketplace forecasting here we
are predicting both supply and demand
and a cup of other quantities as well at
a very fine granular spatiotemporal
fashion and we do this in order to
direct driver partners into high demand
areas that will be arising shortly the
second use case I want to touch on is
our intelligent real-time anomaly
detection stack anyone in their career
has been an on call in this room that's
quite a few people it's pretty painful
setting up alerts getting woken up in
the middle of the night just to find out
that it's not a true system issue we
have been working at uber at cutting
edge forecasting techniques that
successfully can give extremely high
signal-to-noise ratios and that we can
track various different a time series
both in terms of back-end as well as
consumer facing time series at scale
this is a very large scale problem we
have over 500 million metrics that we're
tracking at uber
the third use case I want to call out is
or hardware capacity forecasts these are
particularly important because Eber is
such a large business nowadays we cannot
just double or triple our hardware for
special events such as Halloween or news
even call it a day
that would be fiscally totally
irresponsible at the same time we also
cannot under provision because this
would potentially two outages and would
might lead to eroded trust of our writer
and Driver partners
so really it's about the Goldilocks
principle not too much and not too
little hardware this again is a very
challenging task as we have to forecast
minute by minute multiple months in
advance
often special events that only occur
once a year and given that uber is such
a young company this is a very
challenging overall so let's how do we
tackle forecasting problems the first
thing I typically do is some exploratory
analysis whenever I can I eyes the data
that I'm working with one of the things
about time series is that typically have
some underlying patterns here's some
airline passenger data from the 1950s
over a couple of years and as you can
see you have some underlying trend going
up into the right my favorite kind of
graph and then there is some seasonality
that overlays that in this case an
annual seasonality so forecasting
methods need to be able to capture and
model such underlying patterns so what
are the prominent forecasting
methodologies that exist there are two
types of classes one are the classical
statistical approaches that have been
around for decades you might have heard
of ARIMA as well as hold winters I also
included here the feta method which is
typically a less well known technique
but has won the M 3 competition very
famous international forecasting
competition it is computationally very
inexpensive and we found it to be
extremely useful for uber time-series
now in recent years also machine
learning and deep learning techniques
have been coming in to the forecasters
toolkit more and more for example the
quantile regression forests a cousin of
the well-known random forest
as well as recurrent neural networks
have been promising on this field
recurrent neural networks have been
particularly useful if a lot of data is
available and you don't have any
interpretability constraints now which
of these and many other models will be
the best one for your forecasting use
case depends on a multitude of different
factors including on how much historic
data is available and what your business
constraints are for example does the
method need to be interpretable so there
is no real way to actually forecast
which of these forecasting techniques is
the best
and so one actually needs to compare
multiple different approaches so how
does one do that note here that we have
to do chronological testing the ordering
in time series is extremely important so
you cannot take out a chunk in the
middle of your time series and train on
the before and after data of that and
test in the middle that would be
cheating and what we need to do is we
have to train on a time series of events
up to a certain point and then tests
subsequently so there are two major
approaches the sliding-window approach
as well as the expanding window approach
and the name really says it all
in the sliding window approach you take
a fixed window of training data here
shown in black that you move forward at
every single pass and then you test on
the orange data now for the expanding
window approach which is particularly
useful if you have very little data
available you actually expand the
training data from pass to passes and as
indicated here in black you don't drop
any of the data points and again you
test on an orange window that is fixed
now in terms of the evaluation metrics
of comparing various different time
series methodologies there is quite a
few out there both absolute as well as
percentage ones the one I want to call
it in particular that I think is very
useful is to compare to a naive forecast
so what's in the e forecast that
basically means that you assume that
today's value will hold for tomorrow for
example and so you have a nice baseline
that you didn't can compare more complex
algorithms to so we have a blueprint now
for understanding what's the the best
forecasting methodology is for your
forecasting use case but wait there is
more in order to make good actionable
business decisions you also need to have
an estimation of what the uncertainty is
around your forecast and here is where
prediction intervals come into play
prediction intervals basically give you
the probability of your forecasted value
to be within the forecasting prediction
intervals versus outside of them
so an 80% prediction interval would mean
that you have an 80% probability that
they would be within the prediction
intervals versus exceeding them now in
this example that I'm showing here
hypothetical example you can see that
the point forecast shown in purple are
exactly the same between both of these
scenarios
however the prediction intervals are
extremely different from one another in
the left-hand-side graph they're much
much more narrow whereas in the right
hand side graph they're much much more
broader so now if we come back to
hardware capacity planning and you want
to have an equal certainty of not
exceeding your hardware limit you would
have to provision much more hardware in
the right-hand side graph so now that
we've covered some of the fundamentals
of forecasting I would like to touch
briefly on some of the cutting-edge work
that we're doing in this domain in
particular we're focusing on event
forecasting at uber events are extremely
frequent within the uber ecosystem
whether it's concerts holiday sporting
events whether that is happening and
they can have huge effects on our
business metrics here's an example of
how thunderstorms in a particular city
cost 3x demand so the problem is with
the classical approaches and statistical
approaches that have been around for
decades is that adding exogenous
variables is often not possible one of
the exceptions is a remote and even
there we find that it is not working
very well so enter recurrent neural
networks this is a neural network that
as the name suggests can deal very well
with sequences the way it does it is
that it takes in the previous state s t
minus 1 and feeds it into the next state
s of t and so therefore it can retain
some of the memory of the time series
now if one wants to capture the
Perle hierarchical structure of time
series often one takes multi-layer
approaches as shown here on the right
hand side in the next slide I'm going to
show the architecture that we've been
using to do extreme event forecasting at
uber at least one of the use cases that
we have as well as also estimating
prediction intervals here we're using a
flavor of recurrent neural networks
called long short-term memory which we
can retain the memory over extended
periods of time in a pre-training step
we use an encoder decoder approach in
order to do automatic feature
engineering this allows us to have a
scalable approach as the human need for
future engineering gets greatly
diminished we then use this learned
embedding concatenated with external
features such as weather concerts and
various different other things and feed
it into a prediction network in this
case a multi-layer perceptron in order
to yield a forecast more details can be
found here on the right hand side as
well as in our publication that is cited
at the bottom so let's look at some of
the metrics that we we got out for this
new approach as we talked about
comparing to the naive approach in this
case last day is something that we
should be doing so you can see this in
the first column
we've also compared it to quantile
regression forests and a villanelle STM
method the last column is our model as
you can see we're greatly reducing the
average error within the new approach
which seems extremely promising
in addition we've also developed new
techniques to forecast uncertainty for
special events such as for example
holidays as you can see here first of
all note how closely we're getting to
the actual values in orange with the
predicted values shown in blue and then
over that we've laid the 95% prediction
intervals and you can see they very
nicely encapsulate the true values so to
close off I wanted to also point you to
some of the publication's in our space
we've been actively publishing both blog
posts as well as papers that you can see
here and if you're interested in
learning
about the mathematics that underlies all
of these forecasting techniques I highly
recommend an open-source book by Rob
Heineman and with that I would like to
close and also mention that if you're
interested in doing more real-world
applications in forecasting at uber we
are hiring a data scientist engineers
and product managers and we also have an
upcoming tech day on April 19th where
we're going to talk about our uber
engineering tech stack including natural
language processing use cases and more
deeply about forecasting as well thank
you very much for your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>