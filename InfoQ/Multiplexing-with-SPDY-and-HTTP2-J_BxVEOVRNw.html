<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multiplexing with SPDY and HTTP/2 | Coder Coacher - Coaching Coders</title><meta content="Multiplexing with SPDY and HTTP/2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multiplexing with SPDY and HTTP/2</b></h2><h5 class="post__date">2014-09-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J_BxVEOVRNw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I wanted to talk to
duplexing with speedy and http2 and the
way that this changes the way that we
optimize for the front end and
performance in the browser so the idea
is to get a clear picture of what this
actually means at a network level so
we're going to just look at how it
changes the way that the network sends
data and then the other part of the talk
I wanted to just discuss es6 modules
it's an area that I work in myself and
how as HT b2 is gonna affect es6 modules
I'm gonna use the term speedy and http2
interchangeably because they refer to
the same specification HTTP 2 is the
next version of HTTP coming out and they
based it on the speedy implementation
which was created by Google so as speedy
gets iterated and the new versions come
out its eventually going to become HTTP
2 so they own the same process and just
to tell you a little bit about myself
I'm guy Bedford and I work on various
open source projects related to ear 6
modules and package management in the
browser so the big piece of news that we
got last month was at Apple's developer
conference one of the things that they
announced to a speedy support in Safari
so the next version of Safari is going
to have full speedy support and this is
great news from the perspective of
adoption because that now gives us a
full house in browsers so we can expect
now that's towards the end of this year
we will have speedy supported in all
modern browsers and going forward that
means that it really is the future
workflow in terms of how we optimize our
apps and so over the next couple of
years we can expect it to have
widespread adoption rights and the the
key argument is within this world of
HTTP 2 bundling itself is no longer
necessary so a lot of the ways that we
optimized for the front end today are
designed around HTTP 1 and that's going
to be changing in this world of a sheet
so I want to go back and just understand
the network level of it to be able to
look at this if you study computer
science it you know all this stuff
really well that's great
personally I didn't so it's the sort of
stuff that you just take for granted in
web development and it's assumed that
you kind of understand this whole
massive stack so tcp/ip it'sit's the
basic protocol when we're dealing with
the web and you go to a website that's
how you looking up this this website and
this was created in 1974 it's a 14 year
old protocol that we're basically just
driving the whole of the Internet which
is pretty amazing
these dudes get the name fathers of the
Internet
which is a really cool title so Vince
Cerf and Bob Kahn who created it at
DARPA and what they really worked out
was how to get the right abstraction
around to create this large
decentralized internet and that was
based on the IP protocol so they
realized that if you want to have a
large decentralized internet where you
can just have lots of computers talking
to each other you need to be able to
have a wait an abstraction that can
allow this and the abstraction they
settled for was the IP protocol the
analogy of it is just like mail sending
letters in the post or something so you
write an address on it an IP address
send a bit of data it may or may not
arrive it may or may not be corrupted
you don't know where how it's going to
get there all the rest so you just send
it out for this magical thing called the
internet and I had to put a cat picture
on the slideshow somewhere so right but
that's not like ideal if you want to
actually be able to send data around you
need something a bit better than that so
that's what TCP comes in so based on
this abstraction where we can send these
bits of data out and we don't know if
they can arrive or not how do you make
something reliable so the way the TCP
tackles this is you break up your data
into lots of little chunks called
packets and for every packet you send
you make sure that that packet has an
acknowledgment that it's been received
so if I'm going to send data to you I
break it up into a load of chunks send
you those chunks and you acknowledge a
receipts of every single one of them
individually so I know that you've
received the message
we have a checksum to detect corruption
and that's kind of it now if you did a
naive implementation of this and just
sent one packet waited to be sure that
it had been acknowledged and that's
called an ACK in this diagram so this is
a diagram with time going down you send
the data wait for it to be acknowledged
and that takes latency time so you're
looking at 50 milliseconds 150
milliseconds and then I send another
piece of data wait for it to be
acknowledged it's going to be incredibly
slow the point is you want to know how
many do I send at a time before I wait
for enough acknowledgment to send some
more and those are the key ideas that
kind of build into the TCP arrow so how
do you send these packets at maximum bit
rates and how do you avoid overloading
the network so if I just send out like a
hundred packets at once they might all
get dropped and the network can't handle
it so I want to send out a certain
amount work out how many are getting
acknowledged and then make sure it's
sending at the right rate the other
thing is TCP is a very polite protocol
so you don't just start sending loads of
data to someone first you have this
handshake where you say can I send your
data and you establish a connection
first you could just start sending data
to someone but it's effectively that's
not a way to build a stable network and
you can also share some protocol options
so that's called the synchronization
packets and it contains other protocol
options in it so I send a request to you
and say I'd like to connect to you you
send a request back and say yes I
acknowledge I received that I would also
like to send data to you and then I
acknowledge that so that's why it's
called the three-way TCP handshake and
at that point the connection is open and
we can now send data to each other
it's a duplex connection so I can send
data to you and you can send data to me
so yeah and then there's one more part
of the TCP algorithm I want to go into
very briefly and that's tcp slow-start
so when we were talking about the idea
around how do you know how many packets
to send initially on the network what
you do is you have a set number of
packets initially and this is actually a
hard coded into the operating system so
it says initially I'm just going to send
10 packets
and wait for those to be acknowledged
that I'm gonna double the number of
packets that I can send so you get this
exponential growth in bit rates and this
is like the kind of idea so I sent ten
packets initially normally about 1300
bytes of packet so I split up my data
into those ten chunks and then as they
get acknowledged I start to send out the
next packets and then as those get
acknowledged I start to send out the
next ones and I double each time so that
I'm increasing my bit rate so that's how
it kind of works out the best bit rate
for the entire network and that's called
tcp slow-start once it starts to get to
the the right rate for the network it
goes into these other states like if
packets start getting dropped it can go
into every error recovery congestion
control and like various states that
allow it to kind of work out how quickly
does send the packets so that's why if
you do a large download you'll see the
speed kind of picks up gradually and
then a kind of wave is about as its
dealing with these different flow
controls and that's all handled in TCP
so we don't have to think about it the
other thing just about packet size
packet sizes have increased I think it
used to be closer to about five or six
hundred bytes and initially we used to
only send two or three packets
originally so here we've got ten packets
being sent initially and it was only in
2011 that there was an RFC to change it
from three packets to ten packets as bit
rates around the world have increased so
these kind of like fine level controls
of the TCP algorithm itself have been
updated over the years on this 40 or
older group so then when mr. berners-lee
came along and started developing the
web the idea was you would share a
document that was built in HTML and you
would have this aged HTTP algorithm to
share it and he just built a directly on
top of TCP so let's just have a look at
an example page load today
and how that gets shared over a TCP
network so you here I've got a page and
we've got a stylesheet and a script and
the stylesheet itself contains an image
inside of it
so once we've loaded that starter you'll
see there's an image and then we'll load
that image so we initially have the TCP
handshake that's one round-trip and then
we send a get request to load this index
page itself and then we start getting
the response for that and so this is
running over that TCP protocol I now see
that I've got a stylesheet so I send
another get request in the same
connection and the point is that
connections can be reused but I can't
actually request the style sheets I can
only request the style sheet because
I've actually finished loading that HTML
page
so I can only request one thing at a
time on a single connection so I can say
give me the index page and then I can
get back the index page and I say give
me the style sheet and I get it back on
that connection there's no way I can
request to resources at the same time on
a single TCP connection for HTTP one
when I want to load that script because
I'm already loading the style on this
one connection I actually have to open
up another connection to load the script
so now we have two separate TCP
connections and this was not the way the
TCP was designed to be used the creating
a new TCP connection is incredibly
expensive for servers and for the Opera
and for the browser itself so browser is
limited to a maximum of between 6 &amp;amp; 8
TCP connections that you open up when
you do this kind of thing and that's
where we have performance issues when
you have a lot of separate files the
browser knows about this so it's already
done the handshake so the other
connection was sitting ready to go but
it also needs to be warmed up through
the tcp slow-start to find out what the
right bit rate is for the optimum
delivery and in this scenario the
stylesheet contained a banner image so
then we sent a separate request to get
that so we just use one of the open TCP
connections so the two issues we have
here is firstly we've got a new
connection for every single resource and
then we'll have a separate round-trip to
load that banner image and those are the
two issues that we have and those are
the two things that we optimize for when
optimizing for the front end so the idea
with HTTP 2 is let's just have a single
TCP connection and just merge those two
instead of opening a new connection just
send the request over the same
connection and like it really is as
simple as that so how do you do it well
you just assign a stream ID so instead
of just saying I want to send a get
request for this file say I'm sending a
get request for this file this is my
first request and then when you get a
response you know is that part of my
first request or part of my second
request and you literally just give a
stream ID to the different requests and
then you can know which ones which and
you can share the same pipe and then you
don't have a connection limit so that's
really it you give each requested an ID
and now we have multiplexing so that's
that's really all there is to it I think
the only thing that's really surprising
about HTTP two is the fact that it's
taken 20 years to get there
25 years and you can also have stream
priorities so you can say effectively at
the packet level these streams are being
interleaved so I'm sending one packet
for stream one and then one pack of the
stream two and they're alternating and
you can set priority so maybe we'll send
five packets for stream one and then one
pack of the stream two etc right so now
let's go back and look at the
optimizations we make on the front end
and how they relate to HTTP 1 so the
sort of optimizations we do today are
inlining and sort of trying to get
everything into one request as
effectively if you imagine this ideal of
looking at something like the Google
home page where everything is just in in
the index page itself and there's no
separate requests and it's very fast and
that's the one side of the extreme and
on the other side you've got like a
single page with separate requests for
absolutely everything and in HTTP 1 we
sort of trying to move to this ideal of
everything just being one requests in
HTTP 2 it's no longer the
that we need and we can move away from
that and just let things be as they work
more nicely for us on our front-end
architecture front-end development is
complex enough without having to do
network optimizations so the two reasons
that we bundle avoid hitting the
connection limit and avoid round trips
with HTTP 2 the connection limit problem
no longer apply it's the only issue we
have these round trips we still have the
fact that I'm loading a stylesheet and
then I'm seeing that it has a banner
image so then I'm sending another
request for that image and that that
round trip of discovering resources as
you load them becomes the new problem
and there's another thing that HTTP
describes which is called server push
and let's say idea that well we could
actually replicate that idea of like a
single page that contains everything by
having a way that this when the server
first gets the request it just sends
back everything you need so it knows
you're going to need these files so it
just pushes them down to you and it says
these are the other three streams would
probably need just now the browser
stores it in its cache and as soon as
there's a request from the browser side
it just uses the version from the cache
and this is great when I first heard
about it I was like well server pushes
is actually what we really want that's
like amazing so it's as I say identical
to what you would have got with the full
inlining approach it's exactly the same
network profile but you're getting
browser caching which we don't normally
have with inlining and that kind of
leads on to this idea that you could
have in the future of like smart web
servers imagine a web server that knows
HTML it knows your resources and it can
trace your critical render path set the
stream priorities in HTTP to and and do
all your optimizations for you
automatically sort of it seems to be a
really nice idea you could imagine see
DNS of providing the sort of features
where you just put on the HTML page as
you write it in the CDN does the
optimizations for you the the server
push and the order priorities and stuff
isn't quite possible yet today
and there are techniques that should be
able to do that but it will take a while
for that to become widespread and also
it will take a while for http/2 to have
wide enough adoption for us to use this
kind of thing
speedy push itself is I've used them in
a few applications in production and it
can be unreliable at points it is still
an experimental protocol so at the
moment
speedy push is actually something that
today is best not to implement but I the
situation's changing all the time so
next month it could reach a point of
stability where it's it's not giving
those issues so what can we do today
with HDTV two approaches well the focus
then I would say actually turns away
from speedy push towards hinting
approaches and prefetching approaches
because that allows us to be able to do
the optimizations so what I do is I just
say I know that the stylesheet contains
this banner image so I'm just gonna
stick a prefetch tag in the head that
says you're gonna need this banner image
just now and that just cuts down that
round-trip because you can tell the
browser immediately these are the things
you're gonna need and this is the these
general techniques are called hinting
and it's the primary optimization you
want to be using in a world of HTTP - so
you want to be making sure that
resources are known as soon as possible
we're literally putting them into the
head of your page it can also be sent as
a server header it was actually disabled
in the current version of Chrome I have
no idea why but it's something that's
been I mean this is an old problem so
it's it's actually prefetching like this
is supported across browsers in older
versions of ie as well but Chrome had it
disabled they have recently Rhian abled
it for Chrome 38 so prefetching is
within the class of hinting types of
optimizations so the idea is you find
out what the deep resources are in your
tree that you would have to wait
round-trips to load and you hit them
upfront
because you're putting them in the HTML
page itself there's only a one
round-trip cost so in comparison to
server push in tech techniques aren't
actually that expensive at all and they
work completely today ilya grigorik
whose name comes up a lot in this space
it's almost impossible to kind of touch
on it without mentioning him he's
recently worked on a resource hint
specification which is specifying these
prefetching and hinting methods for
browsers going forward into this HTTP 2
world and that's something you can check
out he only released that last month
actually so to just summarize how the
transition into HTTP 2 looks the first
step is enabled speedy today so speedy
without server push the implementation
is very stable it's used on sites like
Google Twitter and Facebook you can
literally just switch it on for your
sites so you just start using it and it
falls back to normal HTTP when it's not
supported so you can just start using it
in sites today the only real cost is the
cost of an abling as a secure server so
if you haven't already got a gps-enabled
that's the only real performance loss of
enabling speedy today CD ends that
support speedy if you use cloud flare or
MaxCDN they both support speedy
completely so you can just enable HTTPS
and then switch on the speedy support
for them and if you use servers like in
genex or apache there's lots of
up-to-date server implementations as
well and as I said that the only real
cost is moving to enabling TLS again
ilya grigorik has a really good article
on how to optimize TLS in a pattern in
Ingenix
and if you focus on that kind of level
of optimization or use a CDN like cloud
flower MaxCDN you can make sure to have
really fast front-end performance and
then in terms of how we approach the
optimization once you've turned on
speedy an enabler for your site's
continue to use your current build
processed as you do it today but as new
performance scenarios arise it becomes
this kind of gradual path that will sort
of be moving over the next two to three
years or so where you can start to say
well actually we don't need to put
everything into one file and concatenate
all our scripts together and it starts
to become okay to have separate files
you can start to think in terms of
hinting over bundling and this should it
be a media performance benefits for
enabling speedy anyway today because
most of us do have sites with getting
into 50 requests that are potentially
going to be heavily optimized by that
and so it's this this transition it's
it's this gradient that we can move
along and and not to be seen as
something that's going to suddenly
arrive and change everything but just a
slowly we can change the way we think
about optimization PageSpeed has some
really good ways that it can selectively
optimize so if you use PageSpeed it can
selectively concatenate your scripts and
do minute like compilations based on if
you using speedy or not so we can say
this is age 21 so we want to sense the
compacted version this is a should be 2
so we don't need to so that's pretty
good so that's really all I wanted to
say about HTTP 2 and the other side of
it is to explain how it applies to years
six modules and what it means for
workflow is around es6 modules just to
give a very brief introduction so here's
six modules it's a new syntax for
writing modules in JavaScript and it
gives you this import and export
statements so I can actually import a
JavaScript from another file and I can
export certain variables and functions
and classes from my own JavaScript file
and this is the currently in the es6
specification which is the next version
of JavaScript and it's said to be
finalized next year but the draft is
pretty much
it's a complete draft in terms of
features at the moment but it's still
undergoing review and finalization in
towards next year so in this example I
mean firstly what's what's been really
great about working with es6 it's
actually like a whole new way of writing
java scripts you can do it's a really
nice language to work with so in this
example I'm learning a subclass from
another module just like common jeaious
or if you use requires yes I'm loading
the module from the same folder so if
this file is called my class no js' I'm
loading subclass module genius in the
same folder here and then I'm able to
import a variable from that a class and
extend that class and export that from
this module so when browsers implements
years six modules how does that loading
profile look we have this dynamic loader
to load in the browser called system and
I can write system that import and in
the name of the module that I want to
load so this will send a request to the
my class no js' it then loads that file
and sees that it's dependent on the
subclass module it's loading and it then
sends a separate request to that so we
have exactly the the the tiered deep
dependency loading so you can see here
so it's loading the first one and then
the second one and perhaps you can see
where this is going but the the primary
optimisation for years six modules in
the world of HTTP two which will have
major adoption around similar time
scales as es6 modules is the ability to
trace and hint a module tree for
production which I'll show you shortly
and if you want to use es6 modules today
there's a project called system J's
which actually allows you to be able to
write code like this today so I can load
a common J's or MD in the browser today
so I include the loader and then I get
the system global that I can then use to
import and I can type the name of the
module and it'll then go load that
module see its dependencies load those
module
and it runs exactly the way that is
specified in the es6 module
specification but it adds compatibility
layers to make it easy to use so as I
said the production workflows is we can
create a bundle so you can compact all
our scripts into one but what I'm really
interested in is the ways that we can
approach tracing and hinting modules
into unity production so if you imagine
a large application with lots of
separate Java scripts running on it
between different pages of the app and I
want to be able to load some scripts on
some pages and other scripts and other
pages the optimization around that you
effectively have to create sub bundles
for different areas of your app and that
becomes a very complex problem if you
can just deal with modules being loaded
separately it suddenly becomes a lot
simpler thing to deal with because I can
just load the modules that I need on
this page and on another page I can just
load the modules I need there and the
browser cache gets shared between those
pages so I don't need to worry about how
I bundle or optimize we're allowing the
network to handle optimizations so I
wanted to show you a very quick demo of
how we can do this today the demo I'm
going to show users JSP m and this is
the reason why I'm using JSP m to demo
this is because it's an easy way that I
can show that year 6 modulo 2 production
right so I've already set up this
application I've got a main page
and I'm loading the systemjs loader
which gives me the ability to load es6
modules but right now what I'm doing is
loading common Jess so the next thing
I've got in the page is a configuration
file which has been automatically
generated by JSP M so I didn't have to
make that myself and that just tells it
where to find modules and then I've got
the system that I can now write and
start loading a module file with in this
case I'm importing a module from NPM
and it's called voxel demo and so when I
run this import it sends a request to
that common J's module file it loads the
common J's and sees its dependencies and
then separately sends requests for the
dependencies and and resolves the module
so I'm just going to load this up here
and show you what the network profile
looks like
so it loaded about a hundred and thirty
modules in this case and it's a 3d
library with lots of stuff going on and
you'll see in the network tab we have a
tiered loading so I'm loading initially
I load the first module having loaded
that module I load its dependencies and
then in parallel I can load all the
dependencies of that module and you see
I've got more than six requests going at
the same time because this is running
over speedy so I'm able to request ten
modules at the same time over the
connection I discover their dependencies
and I then download those dependencies
in parallel and it's sort of going down
the dependency tree and the deeper the
dependency tree that's our key
performance cost so to make this work
well in production what we want to be
able to do is flatten that dependency
tree and load all 130 of those modules
initially in one round-trip and JSP M
provides the functionality due to do
this it's called a dependency cache so
just like we would do prefetching or
hinting I inject the full tree
information into the head of the page
and as soon as there's a request to the
module I know that the dependencies
upfront so I know I need to download all
these modules at the same time and the
client is able to pull because it knows
what it needs so I'm just going to say
def cache this module and it traces the
tree and injects the full dependency
cache
I've got disabled cache here so when i
refresh the page now you should see that
it loads all of those resources in
parallel as a single request and then we
get a much quicker page load because
we've now optimized for the speedy
connection so you see it loaded 130
modules in parallel because it's a
multiplex there's no problem with doing
that each of these modules is now cached
in the browser so if I had another page
of the site that shared modules with
those I can just load the modules I need
and I'd get them from the cache so it
makes the the problems of bundling large
module applications a lot simpler when
you allow the network to handle the the
optimizations so that's ok
so takeaways I think just to start using
speedy is amazing so there's no reason
really not to enable it unless you
haven't already got SSL enabled and then
put in putting more pressure on
performance measurement and optimization
around speedy it's it's something that
that can grow and I'm putting pressure
on the server push implementation as
well would be great because not enough
people are putting pressure on it I had
a bug and posted a bug on it and was
able to get some progress on that but it
would really help to put more pressure
on implementations the only way we move
things forward is by using it and it
sits by implementers using these systems
that things are able to get better and
performance optimized and there's fine
grained optimizations at the protocol
level that will be made as people start
to use it more and more so yeah check it
out and thanks very much for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>