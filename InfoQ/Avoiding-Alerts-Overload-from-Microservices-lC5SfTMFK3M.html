<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Avoiding Alerts Overload from Microservices | Coder Coacher - Coaching Coders</title><meta content="Avoiding Alerts Overload from Microservices - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Avoiding Alerts Overload from Microservices</b></h2><h5 class="post__date">2017-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lC5SfTMFK3M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning I'm going to start with a
bit of audience participation because I
know people like that especially early
in the day but don't worry it's not that
bad I just want to know how many people
here have been responsible for
supporting a production system which is
why you're here
of course ok so this probably looks
familiar and with the threading I think
this is something like 400 email alert
it's very hard to work out what the
problem is here and after I took this
screenshot I actually checked and these
aren't alert for my servers some other
team copies and config put my mailing
list on it they're not even for my stuff
another example in slack the problem
when things go wrong is sometimes you
get told about it a lot so you know in
the space of an hour we couldn't pretty
much do anything else in this channel
other than look at things telling us
something was wrong final example and
this is my phone on the day of the dying
DDoS attack so the FPS is dying my team
weren't fixing it there was a sort of
shared services team that was looking at
dying but I was getting consistent
number of alerts from page of duty and I
couldn't stop them because pager jutsu
uses dying and I couldn't get into the
GUI and do anything to stop it so I've
actually found a path going 84 94 so
knowing when there's a problem isn't
enough and what you need is a succinct
understandable alert that tells you what
you need to do you only want an alert
when you're going to take action
anything else is just noise
but how do how'd you get that so as I
said I am a principal engineer at
financial times and I'm lead the content
platform work there and about three
years ago we started building a new
version of that platform and we're using
Microsoft is architecture and we are
well we started off building a lot of
Java much Micra services and then we
need to go because it's a lot smaller
the kind of services where it would
pretty much now go is our preference we
are running docker on core OS on AWS we
have multiple decks doors so we've got
MongoDB neo4j elasticsearch and we're
using Casco as our messaging queue and
we currently use
fleet Vulcan V and homegroup home
written services to do our orchestration
in classical management but we are
moving to kubernetes currently as you
would expect this is a very simple view
of our architecture and there are four
main concerns that we have we take in
content from all the different content
management systems in the FT and that
might be a scores blog posts videos and
we transform it into a common format and
as part of doing that we run it to a
concept extraction pipeline to work out
what companies and people are mentioned
in that content so we can say this is
about Apple it mentioned Samsung but to
do that we need to load all of the
information about companies and people
so we have millions of companies
millions of people that we know about
and finally we make all of that
available through API so monitoring this
system talking about launching this
system is talking about what it's like
to move from monitoring one of this to
monitoring were conservatives it's also
a little bit about what it's like to do
develops because with micro services you
can't really spare the time to hand this
stuff over to operations every time you
change it so you are going to be doing
at least some level of support which
means you over people that will feel the
pain when you write shoddy monitoring
and alerting so micro services basically
make it all worse I'm sure you all know
what a microcircuit is it's an efficient
device for transforming business
problems into this tributed transaction
problem and the services themselves are
simple I actually do really like working
with micro services because I can take
ten minutes and understand any of the
services in the system I can make
changes to the architecture fairly
easily deploy independently quickly
reverse things out if anything goes
wrong
so yeah micro services are great but
there's a lot of complexity around them
now your challenge is to understand how
all the micro services interact with
each other to produce your functionality
so why do they make monitoring harder
well a really simple answer there's just
a lot more services so when I was
preparing this talk I went and counted
how many services we had running in
production so we have 99 functional
micro services so 350 running instances
once we've got two production regions
the answer almost all the services
interestingly we've got 52 non
functional services by which I mean
things that do backup configuration
deploying provisioning load balancing
monitoring so that's a ratio of two to
one which is actually was quite
surprising to me but we have five
thousand five hundred and sixty eight
separate services so if you monitor each
of those services just one check once a
minute that's over eight hundred
thousand checks that you're running
today and that's one showing what every
minute that that's quite a lot but
that's just talking about functional
checked so what about system checks well
when we started work on this we deployed
all our services into separate VMs and
for each of those VMs we monitored 20
different things like CPU usage free
disk free memory and TCP connections and
when you work that out so for 16 million
checks a day if we carried on like that
what that means is a one in a million
issue hits us 16 times a day and that's
a bit mind-blowing to think about that
we don't actually do that we basically
thought okay we need to reduce
containers reduce the number of VMs we
have we reduce the number of checks
because quite a lot of those system
checks the things that I wasn't going to
fix I was going to see an alert and then
contact our network team and ask them to
fix it but we actually have 92,000
system checks today but it's still the
total number of checks were running a
day's is nearly a million so there's a
lot of checks and it's attributed
distributed system we're not doing
impressive calls anymore all of our
calls are over the network which means
that network latency other services
being redeployed there are lots of
reasons why your checks might fail once
or many times and the final thing to say
is that services aren't completely
independent so you might aim to do most
of your communication asynchronously
extreme messages but if you've got a
service that writes to database it's
likely making that call synchronously
and we also have because we store
different parts of our data in different
data stores we might take content out of
a document store
and the metadata fat content out of a
graph database and that means that we
end up with a system that might look
like that and when something goes wrong
to one of the data stores you definitely
get an alert from the service that's
using it but if you aren't clever in how
you do alerts you get an alert from all
the services that use that service and
when that happens this is what it feels
like so you need to change how you think
about monitoring if you're doing Micra
services you need to take the time to do
it properly so how can you make it
better well we've been doing it as I
said for nearly three years so I can
tell you some of the things that have
worked for us the first thing I want to
say is that you need to build a system
that you can support and you really want
to do that from the beginning because no
one wants to turn around a year in and
go right I have to go to 50-plus micro
services and make a change in all of
them to give me the thing that that I
actually find out I need so they're
obviously basic tools you need to
support any system so for example you
need logging but you can't jump onto a
box and tower logs anymore because the
thing that you're interested in probably
happened across five different services
on multiple different boxes so you need
log aggregation and we do that in Splunk
and folks pretty handy for us because
you can do attribute queries so you can
find out what's your 95th percentile
response rate but you can also track
individual events but now it's not about
just writing the log to disk so you know
that you expect to have work but when
you're writing from a docker container
to the file system and sending it out to
some cloud service that's going to
import it into log aggregation things go
wrong we discovered we're running our
docker containers as systemd services we
discovered that there's a limit of
$1,000 per 30 seconds by default if you
don't change that you lose logs it's
surprising how long it takes you to
realize you've lost a lot of looks if
it's kind of intermediate and if you're
setting up an alert that says did we
have any server errors in the last ten
minutes well if your logs don't make it
there until 10 minutes late you are let
never fires you don't know what half
so logging not be important but you need
to think about all of the steps that are
now put in place if you do dog
aggregation monitoring is also obviously
something you need to do when we started
out with this service we used Nagios and
it's really simple reason we use that we
were using F T's platform as a service
and it gave us nagisa free it was quite
opinionated though so we could provision
and deploy a VM to deploy to a new VM in
in ten minutes
and we get these 20 system checks but
there were some limitations there wasn't
the service level view so as I had
resilience for a particular service and
I did a zero down long deployment it
wasn't a zero alert deployment because
now girls didn't know that they were
related and would alert for each one as
I deployed and that is really annoying
because it means when you do a
deployment you have to go and put things
in maintenance mode or we have to go to
acknowledge stuff or else your first
line operation seems thrown up and go
what just happened and and lots of the
checks that have been done by default
with things that I was never going to
fix and I really hate going alerts that
I'm not going to fix because it's just
annoying so when we move to the
container stack we didn't get an address
by default sorry we were basically
thinking well what do we really want to
do and with interesting hearing Jonas
talk on Monday about how micro services
are a system you know your set of your
set of services is a system because
we've taken that approach we have a
single aggregate health check service
that looks across all of the Micra
services of our system and has some
logic in it to say whether we're healthy
or not so we care about each service so
as long as I'm able to serve from one of
the nodes I'm pretty happy and so this
is the output of our health check and
basically we we've got two of Mostar
services and we'll only we only alert if
we've lost both of them we also care
about our VM but we've decided to really
minimize what we check and we've written
the service the Jets does things like
checks for disk free space TCP and if we
have a problem we add a new thing into
this so we're taking the approach of
let's monitor stuff when we've seen that
it caused us a problem rather than let's
imagine all the things that might cause
us a problem and monitor them so the
Virgin checker at the bottom is checking
the version of
core OS that we're running and the
reason that's there is we stopped doing
automatic deployments of
the new stable versions because they
weren't as stable as they were meant to
be but we also don't want to just turn
around and say oh actually we're really
out date so we go sort of orange in a
nagging way if so we need to upgrade so
and we care about we do care about the
unhealthy instances because you want to
be resilient but I care about this in a
way that means please don't wake me up
at 2:00 in the morning if you could just
like send me an email or a message that
said this doesn't look completely
resilient that would be quite nice and
when you've got this monitoring we've
got this aggregate health check we do
need to to bring the information about
that monitoring from our various systems
together so we've got two regions
we've got pre-production which we also
care about and we needed to find some
way of looking at it one way and that
this is not gonna be an action we never
heard of because it stands for Silvano's
awesome warning system and it was the
first thing that we had maybe three
years ago and it was written by one of
our integration engineers he just turned
up in work on a Monday morning having
just created this app for fun and it's
using blinky tape which is a
programmable LED strip and he's
basically programmed it so it checks
various different servers and it goes
red if there's something wrong and if
it's if everything's okay it's got like
a knight rider' blue light going back
and forth because otherwise people
tended to say is it broken and and it
used to run on a Raspberry Pi which was
quite cool and we got featured on the
Raspberry Pi blog but don't tell them
but broke and now it's just running on
an old Windows machine which makes make
it not cool and so why does he do that
well the first thing was frustration
about all these emails he had a filter
he never saw them anyway but it was just
so hard
annoying and then we had got screens up
but they were small and the angle was
bad so you really struggled to see stuff
and then we we had that really bad idea
of having a carousel of things that we
were monitoring so what happens is you
see red out the corner of your eye you
turn around the page is moved onto the
next page and then you stand there just
looking waiting for the right thing to
come around so we didn't really like
that so having this up in the office
it's really obvious to everybody
and it's obvious to non-technical people
if it's read someone will always come up
to me and say did you know that was read
it's also really good for demos because
you can you can basically if you're
demoing readings you can take down the
system and it goes read and everyone in
the audience things that's amazing even
though clearly I could do that
like programmatically and so the next
thing we had after saws is dashing which
is a framework that Shopify wrote
although I think they've stopped
supporting it now but it basically
allows you to have a web page with
widgets in of different colors and you
can write widgets of different types the
FT if you're all the things that if you
normally use it for monitoring or
integrated with this so you can call in
a glass box you can look at the result
of a Pingdom check we use Pingdom for
web your on monitoring you can look at
the results of a Jenkins job and you can
also you can also call our I've got
health check and this is my team's
dashboard and it's up on screens and we
don't rotate through that's just above
our heads all the time well we also have
this is the STS first line operations
dashboard so they really like dashing
and if you don't have something shown on
dashing they will basically keep going
on and on at you to write something that
puts it up on there and it is ordered in
terms of importance so the most
important services the ones that have to
be up all the time are in the top left
going down to the least important the
bottom right and they are color coded
platinum gold silver and bronze I don't
know who decided that platinum and
silver were two good colors to put on
the same thing which is obviously they
are entirely the same and but that's
really useful you can look at it and you
can dig in and if you've got that open
in your browser you don't click and be
taken to more information about it so
that provides a really good view of how
healthy is my system right now so talk
about logs and monitoring the third
thing is graphing and we started off
doing most of our graphs off of logs but
actually there's something there wrong
with that because you're using a side
effect of logs that people wrote for
other reasons so I think it's much
better to decide the things that you
care about and record them so what are
the metrics that you really want to know
about and so we thought well actually
we're going to start doing that we have
got Java and go
there's a go port of Koda Health metrics
library so it's really easy for us to
have a very consistent view of metrics
whether we're writing a go we've got go
app or a Java app we export graphite
which I think you can agree is pretty
ugly but graphite just receives all this
stuff it you can then build stuff on top
of it and graph on it and katana is
actually really nice to use it's very
quick to configure the dashboards load
really quickly and this is a dashboard
for one of our API so we're interested
in what's our error rate what's the
client error rate and you can add lots
of different things I took the
screenshot and then I realized that we
we hit a swap from one data center to
another halfway through it's quite easy
to see and this is our system stats
graphed these are the things we tend to
look at when something we know we've got
another we know something's wrong we can
go and see our was the CPU going up
gradually or did it suddenly spike and
finally that helped aggregate health
check that I showed you
outputs information so we can see where
things go wrong so the top line here is
our production environment great bottom
line is a team environment where they
were trying out some sweeping changes
that as you can see were really
unsuccessful it's quite pretty the
interesting thing and something that
took us way too long to realize is that
if anything goes wrong in any of these
things you're using to tell you about
your system and you don't know the
health of your system so you need these
need to be exactly as available as your
system or more available so if you're in
the situation where if Splunk fell over
at 2:00 in the morning no one would fix
it until 9:00 your system is not
available overnight someone has to be
able to support you for that so you have
these basic tools but there are things
you need to do to make them usable so
clearly if you are doing metrics you
need to have some sensible metrics that
you're recording if you're doing log
aggregation you need something that will
let you identify an event and all the
things that are happening as part of it
so we use transaction IDs so any service
that we have in in in my platform has to
look at incoming requests or messages
for a header extra crest ID that will
have a transaction ID if not
supplied it the service will generate
one it is responsible for passing that
on for any calls that it makes and for
logging it in all the logs so we've made
it easy for the languages we use so in
Java there's a servant filter and we use
NBC to stick the transaction ID into
thread-local and it just is that output
on all the love for go it's there's an
HTTP handler library and we pass context
around which is we thought would be
annoying with it actually not a big deal
at all but basically what this means is
I can go to flunk and say I know about
this transaction ID what happened and
it's some the field is extracted so you
can do turns out guide at transaction ID
equals blah and that's really handy when
you're trying to work out what went
wrong and it's really useful therefore
to put the information about transaction
IV in in all of your alerts if you know
it for monitoring you need your services
to report on health and the FT has a
health check standard so that if you've
got a web service you should have on
underscore underscore health you should
say whether your service is healthy and
it returns 200 if the service can run
the health check even if the health
check fails I was on the wrong side of
an argument about this where I would
said that it should return a 500 but the
argument is 200 says I was able to run
my health checks and if you want to know
whether a check failed you have to pass
with Jayson that comes back and the
Jason looks like this so generally
speaking it has a link to some some
guide that will tell you more about how
you can fix it it talks about what the
technical impact is it talks about what
the business impact is and and we have a
chrome plug-in to make it look a bit
nicer if you're trying to look at it we
can aggregate this stuff and because
it's a common standard you can you can
produce all kinds of tools that teams
will use and because you produce the
tools teams will do it because they get
a benefit out of it so the final thing I
want to talk about with tools is it's
really useful to know about problems
before your client does and I'm thinking
particularly about publishing at the FT
because we don't do that many publishers
a day it's probably less than a thousand
and they tend to be grouped and also
they tend to happen a lot after the
development team has gone home so people
in our Manila office will publish a lot
of articles late at night for the
morning morning traffic and I don't
really want that to be when we find out
that we deployed something that broke
publishing
so what we do is we have synthetic
publishing we've got one old article
it's a real article is one that we think
we're very unlikely for anyone to change
for any reason ever so it won't
interfere with our monitoring and we
just republish it once a minute with a
different timestamp and then we just
check to see if we've got the latest one
so well that will tell us is when
publishing goes wrong we'll know
immediately we don't have to wait until
someone does it for real and we've done
that for quite a few of our quite a few
of our publish flows and I think we'll
probably aim to do it for most of our
things it kind of replaced acceptance
tests for us because it's a lot easier
to think about doing synthetic
monitoring actually in microcircuit
architecture so the first thing with
about having building supporting from
start the second thing is is working out
what matters what's the stuff that
really matters so it's the business
functionality that you care about so
this is the FCS homepage this list here
is edit or EQ rated it's here are our
top stories that you would be interested
in and I can guarantee that if this list
is updated and you don't see it on the
home page in minutes someone is going to
phone me up so that's what I care about
and we care about every single publish
for our publishing because basically
journalists really hate it when they
write something and it's not available
for people to read so every single
publish we monitor and we say ok has it
been published within a certain time we
have a service that does this monitoring
it listens for the initial notification
of publishing and make sure that it's
available through all our API it's
within a set time period and the
dashboard lets us measure how we're
doing all of the things that we care
about and for our API so we have a lot
more volume you know we're dealing with
millions of requests a day and we tend
to look at 2/10 Altos 95th percentile
response rate so we also care about
errors so response rates important
errors are also important and it's not
just 500 errors because if you suddenly
get an increase in client errors that
you're getting it's probably because you
screwed up
so we released some code that made it
the basically said if you are going to
do a search using our API you had to
provide a content type header
application data we've never required
that before people have been quite
happily searching for ages so we'll
don't with HTTP spec compliant we
suddenly got a huge number of alert and
we caught it because we were checking
for that and we have a dashboard for our
read availability and
like you to spot the obvious problem
with this dashboard which is of course
that if you're monitoring says 99.99
you're in a really good state if you're
monitoring says 100 you're monitoring is
broken I have no idea how it is that we
know hundred percent on all of these
dials because that is suspect and the
thing that's interesting about all of it
is it's the end-to-end that actually
matters so we do back off and retry so
actually it's the first request the
first attempt to do something fails but
the second attempt worked I don't care
about the failure that happened first
and I don't want to get an alert about
it because basically the thing worked so
I might want to create a report so I can
see what's going on with these things
but we did that and actually what mostly
happened is we never looked at it so if
it eventually works
we generally became quite pleased if we
see a lot of failures maybe we'll look
into it
you can get along go a long way with out
how to arabela and the final thing
though is to check all the services that
are involved in a business flow and
specifically the ones that are doing the
monitoring of it so you do want to know
if the publish availability monitoring
service is broken you need to know
because otherwise again you don't know
whether your systems working and the way
that we do this is we categorize our
services to say well they're involved in
this business flow and they can be
involved in multiple ones so I could say
well when we publish a list of content
this service is part of that flow and I
can make a request to our health check
to say tell me the health of this
particular slice of business
functionality and that means that you're
dashing can actually be divided up into
specific different business cases so
people can say content publishing is
working but list publishing isn't
working and that's that's really handy
for everyone that's talking to you about
the health of your system because what
they're telling you is information that
really helps you narrow down on the
stuff that matters and our alerts are
tied into that they call these health
checks with a category so when we get
into that we know what the thing is that
that's gone on so my third point is
about cultivating your alert and
you need to make your alerts good
because basically the person who's
responding to this alert may never have
done anything with the service this may
be completely new to them or they may
not have looks for the code for six
months it might be 2:30 in the morning
when none of us are our best unable to
really work out what's going on and it
could well be you in the future so you
do this through future use benefit so
review it make sure that it makes sense
to make sure that links work make sure
that it's very clear what you need to do
so this is an example of a bad alert
message that we used to have so I like
the title because it's really it shows
it was written by Java developer method
API response time five M alert five M
stands for five minutes because
originally ran every five minutes it
wasn't running every five minutes by the
time I looked at this alert but the name
hadn't changed it might result in
articles not getting published I don't
know about you but I don't care if it
might I want to know if it did and it
had this amazing technical impact
section it was a mandatory section but
people just put any old rubbish in there
when it might you know something's gone
wrong it doesn't help me
so we rewrote this so now it has phases
and it tells me that it's about failures
of publishing from our method content
management system that's very very clear
and I know which UUID so I know which
article was the problem and I know which
transaction so I can tell if it was
republished multiple times and a lot of
our articles get revised and republished
I can tell which thing was the problem
it's possible I may not have to do
anything because it's been republished
since and it's fine and then finally
there's a link to run book and that run
books have more information you can
change that separately from the alert
and in this case the run book has a link
to a Jenkins job or you can stick the
UUID in and republish and the only
reason we don't automate it is because
our editorial department ought to but
it's really easy to recover and I can do
anyone can do this if you don't have to
know our system at all so you need to
make sure once you've managed to keep
your alerts down and they're actually
real things that you can't miss them
because it's really annoying to have
someone say well this thing's been read
for front half now what you've been
doing and and how you do that depends on
your team but what we've ultimately done
is we have an opt cop rotor we're from
our team of developers and testers two
at a time spend a week being the people
who need to respond when things go wrong
the reason we did this is because
actually when you're deeply involved in
design discussions or coding you don't
notice you don't notice like going off
so you need to have someone who's
totally focused on is that thing read
has it been a message to come into slack
and when they're not doing the work on
on any sport they're dead they're doing
small changes and tooling and stuff that
helps a little bit inspired by the
Google fre thing so you have the opt
cops and need to make sure that you're
using the right communication channel to
make it easy for them to pick things up
and obviously it's not email but for us
you know we're bit hips though you can
slack you need to get early we do really
like it and we make sure that all of our
alerts go into slack and when you put
them into our team Channel and I was a
real advocate of this because my
argument was if you don't put them in
the channel you'll all you're all in and
people will just ignore it and and
actually if it's getting annoying to you
you have two choices you either make
sure you get fewer false alerts or you
basically improve your broken system but
I've changed my mind because it just
gets in the way of other communication
that you're having so now we have a
dedicated channel that's literally
production alerts and responses to that
and so the description of the channel
says who are this week's up cops so we
know exactly who to go to and they are
in there talking about stuff and one of
things we do that I really like if we
use emojis to react to alerts to say
what we're doing so it's there right
with the alert normally we put eyes if
we're looking at something the green
tick if we've fixed it it's a great ik
if we didn't have to do something to fix
it we had to have a little bit of a
formal agreement about the emojis
because we had one too many cases of a
dancing lady where I just Han stood on
and just kinda I don't know what that
means it's really obvious so it's not
enough to just get things back up and
running you need to work on what went
wrong and try and stop it happening
again one thing we do for our op cups we
ask them to fill out failure reports
whenever we have something that we think
is
trusting so it doesn't require anyone
else to tell us over the problem it
doesn't mean that anybody had to have an
it had an impact on someone but it's
just oh well this is interesting how did
we fix it and we have a template so that
people just have to fill in these
sections so the summary might say Amazon
s3 went down in US East one health of
our clusters would be probably
screenshots of what happened for us in
our metrics and we talked about how we
fixed it talked about the impact in our
case we've got a second region fantastic
and action and so maybe the actions
would be actually a u.s. region and
everything went red and everybody
worried although we'd failed ever fiying
automatically so maybe we need to stop
things going red when we're not serving
traffic our region and then what
happened tends to be a lot of
information too to help us work out what
happened and we tend to get easily the
timeline because we are in two locations
and our ops cops would get one from each
location so actually they're always
talking in a slack Channel anyway so we
just can grab all the information that
we were and put it straight into into
the report so yeah this is always
interesting because I do think if you're
doing like services you have to do some
extent of divorce if you do you have to
do something sent of support but I'm
really resistant to having to support
something if I don't think it's
something we can support now so you have
to be looking and saying this data store
is flaky and we don't want to be woken
up every night because it's likely we're
going to take the time to fix it and
it's not enough to write good alerts in
the start you have to keep looking at
them and one thing you have to do is if
you didn't do anything get rid of the
other and it this is something that my
developers really struggle with they're
really worried that if they get rid of
the alert that we might not find out
about something my experience is we tend
to find out from many different sources
when things go wrong and so I spend a
lot of time going write that alert with
indenting delete it and you need to look
at it because things change we recently
moved our run books from one location to
another so all our alerts are pointing
to a location that now redirects
somewhere else well probably we should
fix that
and it's good to get people outside the
team or new people on the team to review
them because they might point out that
you've made assumptions about what
people understand by reading this or
that so that's that's really handy and
obviously if you didn't get an alert and
things went wrong you had to do
something so you know you can look at
what happened and say well what would
have told us well the ratio of
successful to failed and that responses
would have told us and I think making
sure that alerts would fire is part of
fixing whatever issue you've just got
and it might that often the case for us
is not that you need to go and write a
new alert is that whatever flow of code
just happened didn't trigger the
existing alert and this would be things
like a micro service that returns 200
even though it actually failed if you
make it return the right take this code
the alert will catch it and you'll be
fine and it's harder at the system
boundary so we've got several content
management systems that tell us when
they're publishing something and one of
them just stopped telling us and it took
us days before we noticed and we asked
them if they could add some monitoring
to check that they're sending that
notification but we also wanted to do
something ourselves but then you start
thinking about it you think well our
publishing is kind of intermittent and
we could say tell us whether there's
been a publish in the last day but I can
guarantee you that alert the fire on
Christmas Day because no one's
publishing stuff on Christmas Day so you
don't you want to be really careful
about these kind of belton basis things
but you do need to know what's happening
in the in terms of things that come into
and out of your system you do need to
know if the load was stopped working I
improved some logging because it was
really badly worded but it turns out all
are alert for publishing failures relied
on this really badly worded logging and
it would have been nice if they'd been
some kind of a test that would fail when
I tried to run tests on the code to say
you know what this word is quite
important we're using it we haven't done
this but it would be nice and you need
to deliberately break things because you
think that your alerts will work and
then when you try it it's never quite as
simple so f.c launched a new website
around October last year and in the
run-up to it we
spend some time practicing because we
knew he had first-line support that
would have to support our system you
know very well and we wanted them to be
able to do the basic stuff of working
out with our problem and turning things
often on again before before escalating
it so we did some drills and we did it
in production because I actually think
it's much better to do it in production
because people pay attention to that so
I talked to our editorial team and
agreed that I could have a 10-minute
slot where I would deliberately break
their ability to publish stuff they were
all in morning conference it was fine so
I just went down the sat with them and
turned some stuff off and I told the
people who led the teams that we're
going to get the other so this is going
to be the first time my team fixed it in
a minute I had to tell them to back off
and leave it so we had to warn everybody
let the first line deal with it but the
second time and nothing went red and I'm
correct okay I really thought that we're
going to go red so we realize our alerts
were not giving the right information
and we've also in the past at the FT
when when we weren't running so much on
AWS we've deliberately turned up a data
center so we told everyone's going to
happen we've got everyone's coming on
Saturday and we've turned something off
and that has been incredibly good
because you think you're a zillion and
then when you're actually two days
beforehand you suddenly go I wonder
whether there's a dependency there that
we can't get around and we found quite a
lot of things like we had them we're
like oh yeah publishing will be fine
apart from the father's somewhere deep
in the Cask a library was a time out
that was going to wait 10 minutes before
failing and so therefore editors would
be stuck to him it unable to do anything
not good
um I am sure people have heard of the
chaos monkey Netflix also have a hot
entire simian army surge of testing
resilience and we have something called
the chaos snail and it's called the
chaos snails because it is smaller than
a monkey and because it's written in
shell and I absolutely I didn't write it
I think that's just brilliant and it
just it just basically runs us through
on a VM and kills processes log
somewhere else what it did and you can
work out with you or alert so that's
pretty handy and I do I think it's
recalled um things are going to change
and actually does the benefit of Micra
services you can make lots of changes
independently without affecting
everybody else and get get it out there
really quickly I mean we deployed the
production
my platform deployed to production 1,400
times last year in the previous the
previous platform went 12 times a year
150 times as often that we're releasing
code and the chances of having a problem
when we release that code it is greatly
reduced I mean the 12 a year ones I
think we bomb we have three a year that
would be a nightmare that might involve
rolling back and horrendous pain and now
it's like well maybe we've had three in
the year where we've had to roll forward
three out of 1400 and but that means you
need to be constantly paying attention
to the stuff that's being sent out
because out of date information is
really painful so we spent an hour
trying to work out why one part of the
website was with CAD stale content we
just couldn't work it out and then
eventually we found out that the website
team had switched from one API to
another but they haven't changed the
documentation so we thought they were
using our API and they weren't it's good
if you can automate updates because
clearly automations always makes things
easier and we're looking at having our
run book information held with our codes
so that we can basically as we deploy
the application we can call an API to
update the run book and my ideal
scenario would be to be able to run
tests to check that that's still good so
perhaps we have something which
documents and documents our API
endpoints and we run a test so they say
if we've removed an endpoint and the
documentation still refers to it the
test will fail I think that's picked
that powerful and you need to find ways
to share what's changing because we've
got maybe more thirty-five developers
and testers and we're working a lot of
different things so you're never going
to know everything that's going on in in
a microcircuit environment I think and
so you have to think about how you tell
other people about it so we do lots of
showcases we basically do walkthrough -
for any big new bit of functionality and
we're constantly sending updates of what
we're working on to try and make sure
people know what's going on and so in
summary to avoid having too many alerts
you do need to you need to think about
right from the start so you need to
build in the things that will make your
life easier and you need to work out
what matters and concentrate on that you
need to continue to
cultivate your alerts and not be afraid
to turn them off that's the hardest
thing so micro service architecture
sells like you move fast but there is an
Associated operational cost and you need
to decide whether it's a cost you're
willing to pay and when I say you it's
not just you you can go hey great
Microsoft checks are really nice but if
your product owners and your directors
and your delivery leads are complaining
because you're spending 10 percent of
your time doing operational tasks you
can't do it you have to you have to
basically convince people that you can
move fast but you have to spend some
time on all of these surrounding things
and we've been we've been really lucky
with that I mean the reason that we're
spending time now to bring Cuba natives
into our system is because we basically
said it's hard to support our
hand-written closely who'd have thought
writing it would be a bad idea but
basically we can send that time because
we convinced our product owner this is a
good idea
it really helped that I said hey you
know what we can actually turn our lower
environments off overnight if we use
this because that's a 70 percent a very
ws cost so that was an easy sell but
you've always got to have the time to do
these in prison so that's it thank you
we do have over 10 minutes of questions
so I'd love to take a few questions I'm
sure thanks that's a great talk plenty
of things to think that you're running
microservices or thinking about running
micro services so anyone else and hi I
was wondering about when developers are
writing micro services do you have some
checklist that they need to fulfill
before the Rattata production because
you have to add information to different
kind of monetary system support this
week and a lot of thing so you will
never accept them until they fill it out
is that something that you provide or is
it just in the DNA off of your company
so there's actually two aspects where
the FT so we have an engineering
checklist that talks about the whole
lifecycle of work so from everything
from do you actually need to build it
could you buy it through to what you do
at the end of life but it's very light
on talking about what happens during day
to day development during a system
that's doing kind of development we have
a checklist in our team too so we we
have the general have you looked at the
alerts have you updated the planning
guides have you done a walkthrough
so yeah chapter checklists are really
powerful for this stuff I don't really
mind how you've put a health check into
something but I do need you to have done
it so yes we do in that work quite well
and our QA basically are responsible
pretty much for making sure that every
single checklist has been revisited now
also brilliant are doing the check for a
minute so I think I'd say I'd say that
across the team some people are really
good at picking up the alert I've got
missing information and some aren't you
just use the skills people have got that
answer yep cool any other questions at
all
going once going twice
Oh so we saw last week the trouble you
can get yourself in if you host your
Status page on the infrastructure that
it's monitoring what are you doing about
that yeah so we kind of aren't because
dashing is the dashboard called our
aggregate health check so the dashboard
will go red if it can't call it so
actually do have someone is going to
know that is that our trust has gone
down yes we have had problems where
things go wrong and you can't even get
on to workout was what's happening but
we do have enough separate calls in
there we have Pingdom calling our health
check so as long Pingdom is up and
dashing is that we've got two different
sources that are calling the same resort
on our system and that will go red if
something goes wrong that's a final
offer if not all those laughing it's
tough I was interested when you had the
example of one database failing and
that's causing alerts from everything up
the respect that was affected by it yeah
and that's kind of caused from the lurtz
tool yeah you also don't want that but
how do you make sure that you know that
if you've got a problem here that
there's all these other things that
could be affected by it so that storm is
something that used to happen toss a lot
because we had just done every service
alerted if it's response time with too
long or if it had errors logged and we
removed most of those actually and we
our ideal scenario is to only have
monitoring through these services that
know about the business functionality so
for publishing as an example it's only
that publishing monitoring service
that's going to alert if something goes
wrong and we might then want to go and
look at the status through metrics or to
logs that anything that that should be
that but it's quite hard to do so we're
kind of in the middle of it final offer
I'm around all day if anyone what's up
chat thank you once again they're really
great awesome
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>