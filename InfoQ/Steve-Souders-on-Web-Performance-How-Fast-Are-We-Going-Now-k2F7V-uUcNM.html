<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Steve Souders on Web Performance: How Fast Are We Going Now? | Coder Coacher - Coaching Coders</title><meta content="Steve Souders on Web Performance: How Fast Are We Going Now? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Steve Souders on Web Performance: How Fast Are We Going Now?</b></h2><h5 class="post__date">2013-04-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/k2F7V-uUcNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we're doing today that was a great
introduction from Michael I have to
admit I'm on a web hi to this morning
despite the fact that I had to skip my
workout and drive an hour and 15 minutes
in rain I passed a really bad accident
but I'm still feeling really really good
I love this conference this is the third
time I think that I've spoken here and
there we go there's my kids too and my
kids okay I don't need a network I love
coming up here before I came up i was
talking with eric from shout 'let yeah
about tracking analytics on social media
very very exciting opportunity he got me
so pumped about that that I wanted to
announce I'm actually retiring from web
performance and joining Sheldon april
fools it is very excited I'm not joking
about that but I would really love web
performance so we're stick with that I
saw zero and mentioned 0 is here i love
them from there from New Zealand I know
craig walker maybe he's here so
hopefully I'll get to connect with him
encourage people to go down there but I
really wanted to give a shout out to an
I think she's right about what she was
saying about how this event has grown a
very community driven event and she and
her crew do a fabulous job so could we
have a round of applause for an
so today I'm going to talk about that
quixotic mercurial web that we all know
in love I worked on this talk over the
last few days quite a bit and had some
fun discoveries and that's what I love
about this job is I've been doing this
for eight years and I can spend time
researching for a talk and find
something that I was completely
oblivious of that is pretty important
just over the course of a few hours so
let me go ahead and get started here
there we go we've got that oh and i
wanted to point out the slides are on my
website right now if you go to Steve
Souders com I have all my talks listed
in the slides or next to them so you can
download the slides now or later later
might be better and all of us trying to
download them now so I don't know about
you guys but for me I don't like slow I
don't like waiting I find it really
frustrating here's where I was two weeks
ago with my 16 year old daughter at DMV
standing in line and they need to adopt
some best practices it is not a great
experience I think go all agree slow is
not a lot of fun right whereas fast can
be really excited right it makes a smile
and laugh it's exhilarating we'd like
doing it with friends it can be
beautiful and it just puts a smile on
your face right
how many people like fast okay so what's
this have to do with the web that we all
know and love well it turns out that
people want fast on the web to right now
how does that come about well we
certainly are familiar with these very
prominent ad campaigns here's one from
comcast talking about xfinity and it's
got speed lightning fast all over it
here's a great series of commercials I
love where the very bed pan guy is
interviewing these kids what's better
one thing or two doing things at the
same time or not faster is better is the
bottom line and it's not just our
connections right it's the devices
themselves they're getting dual cpu quad
cpu those cpus are faster so we're
giving you faster devices as well and so
these the ad campaigns are feeding this
desire for having things fast this in a
visceral desire not to have to wait for
stuff right and we can see that in this
chart i pulled from joshua Bixby's blog
post last week these were actually these
data points are from four different
studies if you pull up the notes in the
powerpoint deck I've got the links to
all four of them and they just show how
the user expectations for how long
they're going to wait for a web page to
load are getting shorter and shorter and
shorter we've got to deliver those pages
fast we're going to drive the users away
create a negative brand image in their
mind right we're going to have high
bounce rates so we've got these major ad
campaigns advocating the desire for
Speed users or reacting to that they
want things faster and companies also
have produced these case studies where
they show that making things faster is
better for their bottom line so I've got
about a dozen of these that I went
through actually last time I spoke here
just for a touch on four of them really
quick here's one from being where they
saw that if a pay
search result page was two seconds
slower they lost four percent revenue
per user here's one from Mozilla making
their landing page two seconds faster
increased Firefox downloads by fifteen
percent here's a great one from
shopzilla making their site faster
increased conversions by seven to twelve
percent depending on which country
you're in and here's a more recent one
from Barack Obama calm Kyle rush will be
speaking at velocity coming up in June
and he made their site sixty percent
faster and saw a fourteen percent
increase in donation conversions right
so speed does have an impact on how
users engage with and embrace your
website and your brand so how fast are
we actually going right what I want to
do today is I want to talk on various
components of that experience that web
experience and talk about how we're
doing over the last year or so in that
area so first let's talk about
connection speeds so I'm going to pull
data here from this awesome quarterly
report that comes out from Akamai Akamai
as probably everyone knows is a CDN
number one in the industry tremendous
worldwide presence tremendous number of
websites major websites using them and
for about the last five or six years
they produce this quarterly report they
call the state of the internet and in
that they include connection speeds
broken out by major cities states
countries and globally right so I went
back over the last few years and created
these charts so here we see global and
US average connection speed over about
the last five years so I just want to
look at year-over-year if we look at
year-over-year the average connection
speed globally has gone up four percent
and in the US it's gone up eighteen
percent which is pretty significant
especially if you consider that our
connection speeds here in the US are
more than twice as fast as they as the
global average
so that's significant progress there if
we break it out specifically by mobile
we see even greater improvement now I
want to talk a little bit here about
these pseudonyms that are used Akamai
doesn't want to reveal stats for
specific mobile carriers so what they do
is they assign each mobile carrier a
pseudonym us one might be sprint or
t-mobile I don't know but across all the
reports that carriers name is kept
consistent and so what we can see is for
the three major carriers us one two and
three year-over-year their average
connection speeds have gone up 68% 131
percent and thirty percent so if we look
at connection speeds year-over-year
we're getting a four percent increase
globally in eighteen percent increase in
the US and on mobile depending on your
carrier 32 131 percent faster so that's
pretty good right pretty good progress
to me mobile could always be faster I
think Michael will agree with me how
about browsers so I don't want to open
that Pandora's box about which browser
is the fastest more here i'm looking at
year-over-year comparisons and looking
at trends the trajectories that we're on
and what we see is that browsers as we
get new releases browsers are getting
faster those new releases are faster
than the old weasel releases thank
goodness this is a really tremendous
study unfortunates by a year and a half
old it's from Gomez data which is part
of compuware now and they haven't
revived this study they haven't updated
this study unfortunately but this data
is based on rum data real user
monitoring which means it's measured
with JavaScript inside the page of their
customers and they're gathering that
data from real users who are engaging
with various types of pages from
different network connection types on
different types of hardware and devices
all around the world
so this is very very accurate ground
truth data and what we see is that as we
get new versions of Chrome Firefox or
Safari of ie those page load times are
getting shorter and shorter faster and
faster so that's a really good thing we
love seeing that but that's a little old
it's by a year and a half old so I want
to also talk about browser performance
in terms of javascript javascript is a
major component of how fast or slow a
web page is I did a study I talked about
recently where I turn javascript off and
on and compared the performance of the
Alexa top 1000 and with JavaScript
turned off pages were thirty percent
faster so JavaScript is a major
component of how fast a page is going to
appear for users and what we see here
again is the trajectory I don't want to
debate about which is the fastest
browser just talking about trajectories
here as we get new releases of browsers
they're getting faster if you look at
the SunSpider benchmark if you look at
the v8 benchmark again that trajectory
is going in the right direction so
browsers overall are getting faster as
well so that's a good thing how about
page wait okay so here I'm going to pull
data from the HTTP archive the HP
archive is a project that i run i
started about two two and a half years
ago where twice a month on the 1st and
15th in fact at two a.m. this morning
the cron job for the first kicked off
the I crawl right now about 300,000 of
the world's top URLs and I gather
information from that a lot of it is
related to performance I gather the
number of requests the size of those
requests all the headers for those
requests did they do caching to the div
gzip and also capture screenshots of the
page loading timing information so i
encourage you to go if you haven't
already to go to htp archive.org and
check that out so here's the chaat chart
that I have for trends i got the URL
there in the bottom and this shows that
year-over-year comparing March fifteenth
last year to March fifteenth this year
Paige has got 327 k bigger thirty-two
percent so that's pretty significant but
I actually don't want to look at this
chart and the reason is because it's not
comparing apples to apples as I
mentioned today and for about the last
four or five months I'm crawling 300,000
the world's top three hundred thousand
URLs but a year ago in March I was only
doing 70,000 we're growing the number of
URLs as we get more hardware as we
enhance the database schema on our way I
want to finally reach the top 1 million
URLs so we're not really comparing
apples to apples and it's possible that
by changing the sample set of URLs it's
having an effect on this data and I
think that's definitely true what I've
seen I spend a lot of time looking at
this data and today so will you what I
find is that we have better performance
best practices as we are closer to the
top 1000 and as we go out to the tail of
websites those performance best
practices drop off and so one
performance best practice is to have
fewer requests and a smaller page wait
and so it might be that one reason why
this number is going up is because we're
going farther and farther out to that
tail which might not have as high
performance websites so luckily there's
a way to get around that potential bias
and do apples to apples comparison and
that's by in the UI you can select to
look at all the URLs or you can pick the
top 1000 URLs so here's the chart for
total transfer size and total requests
for the top 1000 URLs worldwide based on
Alexa com and so what we see is over the
last year the top 1000 the average page
weight has gone up 368 k has gone up
forty-four percent so that's really
really significant that's a huge amount
of growth it's gone from about 830 k to
about 1.2 meg really really tremendous
growth
which isn't really a good thing right
this is going to be harder for
performance to deliver that in a timely
way I don't have mobile stats here we
actually have a mobile version of HTTP
archive that uses iPhones right now
we're only doing five thousand URLs top
to top five thousand URLs there so
I well we could look at that chart too
and see what that says I think we're
going to see similar growth there as
well so that's hard on a mobile device
which is already have already has
challenges and also people paying for
those bytes in their data plans so this
is quite a bit of growth what accounts
for this growth well I know just from
looking at this data frequently that
images are the biggest component of web
pages today we see that the number of
transfers the transfer size the number
of bytes for images went up from about
490 k to 640 k 150k increase year over
year that's thirty percent and I also
know that javascript is a really big
component and bite for bite javascript
is much more difficult for performance
because it has to be parsed and executed
it takes CPU as well and we saw that
that grew from 165 k to about 210 k so a
37 k twenty two percent increase year
over year so that's significant that
represents a significant performance
challenge and if we look at the major
content types that i'm tracking if you
go to that URL you'll see charts trend
charts for each of these we can see
again the total went up about 368 k
year-over-year images went up 150 k
javascript 37k flash HTML CSS went up a
little bit not that significantly but if
you add these up it doesn't make sense
all of these major types that i'm
tracking only add up to 208 k but
overall the average for total page size
went up 368 k where's the other 100
DK so i discovered this friday when i
was working on the slides it's like hmm
this is interesting so i started poking
around what I did was I added a chart
which didn't exist Friday but exists
today if you go there for what I call
other so the way that I track these
different content types is I look at the
content type response header right and
often that will say text script or image
jiff or some text HTML and sometimes
it's not clear what the content type is
trying to say so for those ones that
aren't clear i classify them as other
and when i started the project two years
ago even a year ago when I looked at it
classifying those as other was fine
because it was a small number of K it
was a year ago was 16 k well guess what
over 1 year that other category has gone
from 16 k 2 165 k 149 k increased nine
hundred and thirty percent this is
interesting this is why I love working
on the web okay who is interested in
figuring out where this is coming from a
nine hundred percent increase are you
kidding me what is happening okay so
what I did all of this stuff is in my
sequel all of the code and the data is
open source you can download if there's
a few hundred people worldwide who
downloaded on a regular basis the dumps
and do their own analyses on it so I've
got that my sequel database at my
disposal so I start doing some queries
so I tried to find the content types
that were either showing up a lot like
they had a large number of resource
requests across this top 1000 or they
were huge and here's the top 5 that I
found so if we look video.flv so if you
remember before I was just tracking
flash I was looking at swift files right
so pretty clearly I should really change
that chart to save video and these
pretty obviously are easy to track video
x flv video mp4 to throw in there but i
wasn't doing that be
four and a year ago and earlier these
hardly existed at all let me point out
what let me explain a little bit there's
a lot of numbers up here what these
numbers mean so in each of the table
cells the first number is out of these
top 1000 URLs that I hit how many
requests had this content type so for
example for application Jason about
thirty percent of the URLs had one of
these requests there were 299 requests
out of the top 1000 URLs the average
size for each application Jason response
was 4k and that's transfer size and if
we take the average across all the pages
how much did it contribute to the
average page weight and so this is
pretty simple math it's about 300 k I
mean it's about 300 requests at 4k each
that's 1.2 1200 k if i divide it by a
thousand it's 1.2 k per page on average
so round it off it's about 1k it
contributes per page 2 to the average
page weight and if we take the Delta
between March thirteenth march two
thousand thirteen and march two thousand
twelve the Delta and caper page is 1k so
we can see the growth of each of these
content types that really didn't show up
very much before we can see how much
those have grown over the last year to
contribute to the average page week and
the top two are these video formats now
they're not being used that much a year
ago there were seven resources this year
there's 29 but also the size of them
increased by 3x and the size is huge why
are the huge I don't know and but now we
have four or five times as many of these
video formats and the size is it has
really increased if we look at mp4 it's
gone up 10 X so the average amount of
bytes that they're contributing to the
average page weight is really huge then
we have so but it's pretty clear what
those are those are
then we have text plane which has grown
significantly as well it's gone up about
twenty-five percent a number of requests
but the size again has gone up for x and
application octet stream to be honest
with you I can't look at those I spend a
lot of time looking at these stats I
can't look at those and tell you what
those are for are the images that they
just have a incorrect content type are
they some kind of binary data I don't
know my first guess is their spots right
because they're fairly significant size
increase so let's explore each of those
first we'll look at text plane oh yeah
so if we add up these top five we had
150 k that was missing if we add up
these top five they add up to pretty
much 150 k 136 k so there's some other
mystery content types but they're not
contributing that much these are the top
ones and actually Jason although the
size has doubled its not contributing
that much to the average page wait so
the video are what about text playing
and application octet stream let's look
a little further at those so I do some
more queries I look at the actual file
extension I look at the name I look at
the site that's hosting it and I can see
that for text plain it contributed 35 k
per page to the average page wait 31 k
of that were for files that were video
for k were 45 files that were fonts so
the end there were some others there was
some Jason and JavaScript and HTML and
XML but they were pretty small i mean
the 31 and the four add up to the five
so there's some rounding here that's
going on but the other other responses
really weren't that significant these
are the significant ones and if we look
at application octet stream you can
probably read the handwriting on the
wall 19 k per page this content type
contributes to the average page wait 15
k of that are for video files for k of
battle for fonts so what we're seeing is
these mystery content types that I
weren't a text plain and application
extreme are largely for video files and
these other content types that I wasn't
tracking a year ago they weren't that
significant have now grown a lot and so
if i factor this back into the original
stats that i showed and create these new
categories instead of flash i'll call it
video and i'll add fonts and we can see
other now has gotten a lot smaller but
images have had tremendous growth video
has actually surpassed javascript as the
number two largest number of bytes on
the average page and javascript has
grown significantly as well and all the
other ones have grown but those are the
top three so this was a really really
exciting discovery for me video is not
something that i pay a lot i've paid a
lot of attention to in the past if we
look at these euro year-over-year stats
looking at the transfer size across the
top 1000 URLs there was significant
growth an increase of 368 k forty-four
percent images are still the largest
number of bytes per page and they have
significant growth but video went up a
hundred and fifty percent an account for
a large number of bytes on the average
page so this is something i think going
forward that at least I and I think all
of us should look at to see I don't know
a lot about video compression I mean
these files are 1.7 to 2.2 2.3 Meg each
is there some video compression
technology that would help to reduce
those sizes the other thing to keep in
mind is the way that this HTTP archive
crawl happens is we have these headless
browsers in this case we use IE 9 for
the crawl today there's no user
interacting with it it's not like the
user clicked play so just by loading
these sites it triggered the download of
a to Meg file does that make sense does
that happen on mobile I don't know I
just discovered this Friday I've only
done a little bit of Investigation but
people want to know these are things
that we need to figure out and my guess
is we don't have a lot of
well-established best pract
is about around video out there it is
accounting for a large amount of the
data on the web and there are some best
practices that we can identify and push
and evangelize for adoption okay so when
it comes to into page wait things are
growing fast it's getting harder and
harder for performance how about quality
of craft that's what I'm all about
evangelizing these best practices
Michael gave a shout-out to why slow we
have PageSpeed is another tool that's
out now and PageSpeed actually is
integrated into ie9 running the HTTP
archive so this is another trend chart
that you can see the average PageSpeed
score across these top 1000 URLs looks
flat anyone disagree it went from 82.4
284 well you can stretch and say that's
a positive increase it's in the right
direction but I mean it's pretty flat as
a guy who's evangelizing best practices
this is pretty disappointing right and
in 84 I'll be honest with your page
speed is today pretty lenient in giving
out grades in 84 pretty much you should
get an a like if you're paying attention
to performance you should get an A a 90
or higher an 84 is not a great average
score for the top 1000 so this is pretty
disappointing and there's lots of other
kind of quality of craft charts that we
can look at in the HTTP archive number
of Dom elements is one of the highest
correlated variables to page load time
and we see that the number of Dom
elements the dotted line on the bottom
is increasing right from 1215 to 1330
doc sighs you can't really say what what
would be the preferred direction for
this to go on one hand you'd want the
average size of the main HTML document
to get smaller because that has to be
downloaded unless people are doing
document flush that has to be downloaded
to kick off all the rest of the page so
if you made it smaller it could be
downloaded more quickly
but people might be adopting best
practices of inlining small snippets of
images or JavaScript or CSS so we might
expect to see it grow here we kind of
see neither again it's pretty flat it
went from 24 K 2 26 k so we're not doing
either of those best practices we're
kind of just having a gradual feature
creep sort of behavior for the average
document size again not very promising
how about a number of domains in this
new stat that i just added a few months
ago max requests on one domain okay what
does that mean okay so number of domains
this is the number of unique domains or
DNS lookups that the page has to do on
average so that went up now dns lookups
take time maybe not a lot of time 50 to
200 milliseconds but they take time and
it went from about 15 to 19 and this is
largely due to the proliferation of
third-party content on web sites so
that's not great for performance we're
having to do more dns lookups but the
other one is also pretty disappointing
so this is if you look at all of those
19 domains how many resources are
requested on each domain this one might
have three this one might have one is
just a beacon for some analytics and we
look at the domain that has the largest
number of resources on it and that
number is the max request on one domain
we track that most used domain we track
that number of requests on one domain we
take the average and again was flat i
went from 40 to 41 so this isn't good
because we know that most popular
browsers only download 6 requests in
parallel / hostname so worldwide the
average to fetch something that's you
know a few k is about half a second so
we can do six every half a second so we
can do six imagine imagine I have six
fingers the people in the back can't see
my fingers anyway I have a sixth finger
april fools
I appreciate the minimal reaction there
ok I know some people are still awake
thank you so imagine undo it six and
each of these take half a second if the
number one domain most use domain has 40
I've got to do 6 12 18 24 30 36 40 right
I've got to do seven of those and this
just assumes they all take the same
amount you're going to have some that
are faster some that are much slower but
they all take that same amount I've got
to do seven about half a second sets of
loading six at a time and that's going
to be about three and a half seconds so
there's no way worldwide these pages can
get under three and a half seconds right
and we want them to be like under a
second or two if you look at that user
expectations chart if you look at where
people start leaving your site what
drives conversions you got to get under
two seconds there's no way these sites
are going to get under two seconds
worldline right so a technique for this
is something i call domain charting
where your domain might be shelled it
calm you might create dub dub dub one
shallot calm and duct taped up to
chocolate calm and split the resources
to is the best numbers preferred number
lots of studies have shown that two to
four is good too is a good safe bet
split it across that and so instead of
having 40 on one domain you'll have 20
and you would have to do 6 12 18 and
then just two more so it gives you a
possibility of getting under that to
second time right we're not seeing that
happening people aren't doing that okay
what else could we look at how about
making things cacheable add an expires
header you know I wrote that in high
performance website six years ago the
percentage of resources that have
cacheable headers is actually dropping
this is the wrong direction to be going
in and this is all across the top 1000
websites how about turning on gzip that
went up one percent from seventy-six
percent of 70
seven percent again pretty flat pages
with errors errors don't really affect
performance that much it could if it's a
JavaScript or a font file or a
stylesheet that's taking a long time to
time out to generate that error it can
affect rendering but then pages with
errors have stayed about the same
redirects are costly again I was talking
with Eric about that right before the
conference and the percentage of sites
that have at least one to redirect has
gone up so if we look year-over-year at
this quality of crafts really no
significant change in fact a lot of the
trends are going in the wrong direction
so this makes me very sad okay how about
if we look at something that's closer to
what we actually mean by how fast the
web is if we actually look at how fast
pages are loading now this is a tricky
thing to look at I'll use Joshua Bixby
as kind of a scapegoat here love joshua
he has a great blog I read all his blogs
I tweet his blogs very informative
president of a great company strangely
at work sat just got acquired a month or
two ago by rad where he's still blogging
he wrote this blog last week top
ecommerce sites are twenty-two percent
slower year-over-year right kind of
startling headline surprising you know
have they come to html5 dev com to learn
about these performance best practices
but I have to say I question the data
behind this and it's for the same reason
that I pulled this chart out of the HTTP
archive if you go there today you won't
see this chart as because it was too
noisy so this is average page load time
and start render time for the crawls
that we do a year ago we were doing
70,000 and with 70,000 pages and we load
each page three times so a year ago we
were doing about 200,000 page load time
measurements now we're doing about a
billion I mean a million of those we're
do
nine hundred thousand plus we have some
other websites people have added so
we're doing about 930,000 measurements
and we're taking the average across a
really large amount of data across three
hundred thousand different sites that
have a lot of different behavior about
how they're structured and the content
that they have and yet look at how
bouncy this chart is especially
certainly there's a very hard to explain
spike around februari one but even you
know around the september-october
timeframe a 1.7 second jump up and then
a 1.3 second drop down just for one
crawl to the next that's hard to explain
these like if you if you and pat meanin
the guy who runs webpagetest and works
on HTTP archive with me we spent hours
and hours looking trying to explain this
there are a lot of you know good
potential explanations we're in a data
center that data center showed signs of
being congested we had a lot of we
increase the number of user agents that
we're testing they have to get their
requests for jobs that has overhead
associated with it we're capturing video
of screenshots loading all of these
things we investigated and really it's
hard to identify but when it comes down
to is this data is based on synthetic
measurements we're scripting user agents
and just because of the scalability of
the job this synthetic environment that
we've constructed has too many biases in
it it's it's too limited we're only
using one browser we could use others
webpagetest supports chrome and firefox
we could use others but we just don't
have the processing power to run all of
those and get to this 1 million URL so
we only use one browser we're testing
from one geographic location here on the
west coast it might be that sites are
you know that on this geolocation we
were going through some peering
connection that was overloaded again
we're using one simulated network
connection in the real world people have
different network
shins they're going through different
ISPs and mobile carrier networks and so
and the other thing is we're not doing
any statistical statistical smoothing
techniques on this so really this
synthetic measurement you could improve
the quality of that by adding more by
increasing the coverage of the variables
more browsers web page test runs in 30
locations worldwide we could spin up if
we had you know more money we could spin
up other data centers other browsers
other simulate other network connections
right but we just can't do it at the
scale that we're trying to approach and
so I pulled this timing chart and I
think in Joshua study as well it was
using synthetic testing I think it's
just really hard with that synthetic
testing to reflect what's happening in
the real world so that's why I really
love the rum data that Gomez and
Compuware we're using in that earlier
browser chart another problem in talking
about how fast web pages are loaded is
pretty much what people show is the
onload time window download like you see
here and like you saw in that Gomez
chart and really window.onload isn't is
no longer an accurate proxy for the
users perception of how fast a web page
is loading right five years ago when we
were all doing web 10 and we didn't have
as much CSS and JavaScript window
download was pretty accurate it fires
after all the resources in the page are
done loading the images have rendered
the javascript is executed people can
engage with the page now we're lazy
loading stuff we're doing dynamic image
requests using responsive images we're
lazy loading JavaScript we have dynamic
many more dynamic elements on the page
that are rendered with larger amounts of
JavaScript and CSS and so it's really
hard with window donload to capture that
experience I've got to canonical
examples here to hammer this home first
is gmail if we look you know just did a
you know I loaded it nine times it just
did one run on web page test and the
window download the median was at 3.3
seconds so in that sample test this is
what you see at 3.3 seconds if we were
tracking page load time using window
download this is what we would track but
this isn't ready this isn't the page
being ready the user doesn't proceed
this is not the users perception of how
fast the pages instead it's something
more like this where we see at 4.8
seconds again the median across those
tests we have about ninety percent of
the page rendered right that really is
the point at which we should measure
page load time or page ready time right
unfortunately there's nothing in the
browser that tells us that there's not a
above-the-fold render time that you can
get via JavaScript we can do it here
with the synthetic environment and
that's one of the advantages of
synthetic testing but you can see from
this example if we were tracking page
load time it's too optimistic it's
giving us a time that is much faster
than the actual users perception of the
page being ready now on the flip side
let's look at amazon so we see that at
about two seconds the above the fold
content is about eighty eight percent
rendered right I don't know about you
for me eighty-eight percent is pretty
much enough I can start looking at the
prices reading the description scrolling
down stuff like that I can engage with
the page after two seconds but if what
we were tracking was window download
that doesn't fire until five point two
seconds so here we have an example where
window download is much too conservative
it's giving us a time that's more than
2x what the users perception of the
speed of that page is and so what we
really need is we need to move to a
metric a new performance metric default
performance metric that is more about
rendering and less about network
transfer times right because it's the
content that's above the fold that
really affects the users perception that
affects the users
so in webpagetest pat mean and has put
out a proposed metric he calls it speed
index is the average time at which
visible parts of the page are displayed
so it's a number like 3257 and that's
milliseconds and what that means is at
thirty to fifty seven milliseconds the
median pixel on the page was drawn right
so we can use this and if you go to web
page test and you run your tests you can
see what the speed index is for the
pages and in fact we're tracking that in
that chart that you saw we're tracking
that in HTTP archive and if you go to
web page test and you run a test it
produces these film strip so you can
select multiple tests and get the film
strips all next to each other that's why
I did here with gmail on the top and
Amazon on the bottom and below each
screenshot is a percentage number that's
the percentage of pixels above the fold
that have been rendered at that point
according to this speed index metric so
we can see at one second gmail is
actually a head it's got that progress
bar that's twenty-eight percent of the
pixels amazon has zero but at one point
five so somewhere between one and one
and a half seconds gmail has stayed the
same amazon has jumped from zero to
fifty seven percent at two seconds gmail
is still the same amazon has jumped to
eighty eight percent so this is a really
really good way to track how your page
is loading and what that perception of
performance is going to be and also in
web page test he plots this for you over
time so again it's a great way to
compare pages what we can see is the red
line the amazon line is rendering pixels
to the page at a much faster clip right
so what I really wanted to do was to
produce so those are some stats about
some synthetic testing and some you know
the need for some stat that takes into
consideration rendering what I really
wanted to do today was published some
stats based on a very large data set
that I have tremendous tremendous faith
in unfortunately we were going to do
that blog post today and last week we
realize today was April Fool's Day today
is a great day for a blog post if it's a
joke blog post it's not a great day for
a serious blog post so I want you to
look for that blog post tomorrow what I
am allowed to do is reveal some high
level stats and not the details overall
from this data set that I trust the
early we saw that median page load times
worldwide based on a huge data set got
three and a half percent faster and on
mobile they got thirty percent faster
and hopefully that will whet your
appetite and tomorrow you'll look for
this blog post from google and i'll be
tweeting about it so let's look at our
scorecard connections getting faster
love that especially on mobile browsers
going in the right direction love that i
have confidence in that that's going to
continue to go in the right direction
pages are heavier I have confidence
that's continued going to continue to
happen but it's not a good thing for
performance so we got to pay more
attention to that and watch that with
concern best practices are flat or maybe
even trending down and that's really
disappointing for me that's direct
feedback on the quality of work that I'm
doing so I need to beat that drum carder
and we all need to pay attention to
these best practices practices and drive
the adoption up and page load times are
faster and we'll get more precise
numbers on that tomorrow so what are the
takeaways overall I think the web is
fast enough and pretty much we're done
we're not done no one here is done we're
not leaving this room until we make the
web pastor okay what are the real
takeaways this video growth caught me by
surprise I don't do a lot of video a
year ago it wasn't registering besides
flash flash has been well contained over
the last year and this snuck up on me so
certainly I need to improve the charts
in HP archive to incorporate these new
content types that are growing at a
surprising clip and we need to develop
some best practices around video
compression how we autoload video or not
what's what are we doing on mobile that
really concerns me this week I'm going
to look at those stats on mobile we need
a better metric than window download
that's what pretty much everyone is
looking at and it's not really a good
proxy for what the user experience is so
I appreciate Pat's work on speed index
we're going to look at that more we're
already talking amongst the w3c web
performance working group about a metric
that we could have available in
JavaScript in browsers that reflected
above the full content rendering need to
evangelize these best practices better
all joking aside they do work they're
not that difficult we're working a lot
with large content providers content
conduits like GoDaddy dreamhost
WordPress that will be one way but the
other way is for individual developers
to maintain their familiarity with these
best practices and to push them within
their teams to areas that I just wanted
to mention quickly that i'm looking
forward to in the future for helping us
with performance one is now even on
mobile devices we have multiple cpus
dual core quad core and the more that
the browser can offload some of the work
it has to do to another thread working
on another
GPU is going allow us to do more things
in parallel as we load pages so I'm very
excited about the opportunities there
certainly we can imagine something like
web worker a web worker being able to
run on a separate CPU but i think the
browsers will figure out ways to go
beyond that and another area is caching
Michael mentioned this in his talk very
excited last week to see Jonas siccing
from Mozilla put out a proposal for
improvements to app cache and looking
for a feedback on that so I think if we
can improve caching and even prefetching
if we make browsers more intelligent
they can know what the user wants before
the user indicates that and get that
content local I think that will provide
us with fester experience on the web so
that's it I wanted to put in one plug
for velocity happening in June we'll be
talking about this type of performance
data there are lots of tremendous
speakers including Kyle rush
about Barack Obama calm and that's it
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>