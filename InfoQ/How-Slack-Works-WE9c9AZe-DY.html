<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How Slack Works | Coder Coacher - Coaching Coders</title><meta content="How Slack Works - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How Slack Works</b></h2><h5 class="post__date">2017-10-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WE9c9AZe-DY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name's Keith and chief architect
at slack it's a pleasure to be here at Q
Khan I probably the less grueling
commute to get to Q con than than most
of you our office about four blocks away
and Sid stole my my pull the audience
for who uses slack lines so now that I
know that a lot of you know what slack
is this feels a little forced but you
know when you talk about slack a lot of
times the new stories that people write
these days are about new features like
voice calls or sometimes they're about
the sort of you know some of the
strategic things that might be more
interesting to developers like platform
a lot of people really enjoy writing
about BOTS for whatever reasons a lot of
stuff gets written about bots but for
the purposes of this talk I'm gonna be
focused on the sort of a core original
use case of slack which is persistent
group messaging right so the product
category that we're trying to define is
group messaging right so a little bit
like you know IRC right self self
curated channels of people with the same
interest groups talking in a real-time
way and I talk in a conversational sort
of setting around a kind of virtual
water cooler instead of like you know
you know common thread or an email
thread where it's reveals long-form but
also persistent right so if you leave
work at five and get back at nine you
can catch up on whatever happened in the
ops channel while you were away and this
is sort of a really simple vision to
kind of state it turns out that
executing it well and executing it at
scale in a way that you can operate is
pretty challenging so you can expect
here today is that we're going to go
through a little case study of how a
typical slack session works so we have a
user and their client connecting to sort
of slacks infrastructure running in the
cloud receiving messages sending
messages and all the sort of internal
pieces of slack that interacts with also
talk about so much Alan jizz we face
right now
right so slacks a very new product it's
only been in operation for a little over
two years at this met this time it's
been operational and up for all of its
users that whole time so it's still busy
being born and I'll talk a little bit
about our plans to address them and what
state they're in and I've done a couple
of Toxic Yukon before you know with
different hats on and I've done a lot of
these tech talks and there's this
tendency to get frozen into talking
about your thing as if it's perfect in
part because like it's kind of you're
insecure right you're talking about the
work you do all
all day long you want to make it sound
like you work at a cool company full of
smart people or doing hard stuff which
is true the more often than not but it's
also a little bit of a fib right there
are lots of things that aren't perfect
there's lots of things that if you had
sort of perfect hindsight you'd go back
and do differently and I'm gonna try and
not paper over them to to egregiously so
a few numbers to get us some sense of
scale here in terms of raw headcount of
users slack is sort of mega scale and
not Giga scale and so there are millions
not billions of connected users right
Millions with an atom we have peak
connected users of two and a half
million and I'm sure somebody will
correct me if this turns out to be wrong
I think that's the largest sort of
instantaneous number of open WebSocket
connections of anything out there I
might be wrong about that I look forward
to finding out if I am
what's more unusual about the scale and
sort of load of the way that people use
slack it currently isn't so much the raw
crush of bodies although that's getting
more and more challenging it's actually
just how intensively they use it all
right so we're not something that people
kind of log into and write a few
messages read a few messages and leave
for those part people stay connected
during their entire workday so for
instance the average user is actually
intensively using slack alright so
actively in focus reading channels
writing messages and so on for two hours
away for every workday and they're
connected on an average of 10 hours for
the average workday there's also this
impression that everything that slack is
kind of a Silicon Valley thing or
something that it's all you know start
at people using it 50% of our daily
active users are from outside the US so
you know Europe Asia Africa and we'll
see how that trend goes over time and a
little bit about sort of our style of
solving problems and the kinds of things
to expect from this talk so a lot of
times people when they talk about their
consumer oriented web stack it's a
little bit of buzz word bingo over at
the so reason this reason that reason
that we're using that we plumbed
together these two things that way
there's gonna be a little bit of a
different kind of talk from that
perspective if I were to pick with
single adjectives for the house style at
SLAC it would be conservative
so the vast majority of what slack is
doing and what's challenging about what
it's doing are storage and memory and
compute and networking handling failure
these the hard part of these things are
essentially hard the same way they were
ten years ago and the things that have
come along in the intervening ten years
while sometimes better are also ten
years younger and you know full of ten
years of bugs that were not that
interested in being the people to find
out about on the other side of this
spectrum we're willing to write some
code so for instance when we get to
talking about messages a lot of times
people expect
slack's message bus to be described as
you know we set up Kafka and such in
such a way and we tuned it like this to
get low latency and so on and so forth
and to be clear there are places that
can be successful with that style
solving problems however our experience
so far is that the effort and
operational know-how and monitoring and
alerting and so on that goes into
getting an off-the-shelf piece of
software to do exactly what you want it
to do sometimes is better spent telling
the computer what you want it to do in
the programming language of your choice
so our message bus to go back to that
example again is a bunch of Java that we
wrote and while that means that we
invest software effort in building and
maintaining a lot of software it means
that it actually does what we want it to
do and that we have in-house experts
that understand essentially all of it
and along these lines we also have a
taste for minimalism there are
relatively few core pieces of
infrastructure that come from outside of
slack that we depend upon so for
instance if you're proposing that we
introduced some new dependency and
Kassondra we might have a long
conversation of the form well why can't
you do what you're trying to do with
Redis or my sequel or some other system
that we've already got thousands and
thousands of server years of experience
operating successfully so I had to go to
Comic Sans for this one because this is
sort of a abusively crude picture of
what's going on at SLAC but it's not
entirely deceptive so out here on the
edge down at the bottom of the screen
we've got our clients right so there are
happy hopefully slot customers and
they've got some floor ality of devices
running slot clients right so maybe
there is an iPhone now at the lower left
running our iOS client maybe there
a laptop running are our native Mac OS
client or a native Windows client and
you can see that there's kind of two big
gateways into the backend of slack that
these things end up cooperating with one
of them is is the web app stack and web
app is a big restful application that
I'll be talking about more in more
detail in a second and the other ones
the real-time part of our serving
infrastructure which I'm representing
the message server here a quick word
about I'm gonna be repeating this point
over and over again a couple times
there's this like fun fact about slack
that media stories really enjoy
hammering home which is that it started
out as a game company or as a pivot from
a massively multiplayer online game and
usually people just mentioned this as
like some funny thing right it's like oh
they were a game company haha isn't it
weird how like cookies crumble sometime
that's true but it actually does kind of
come back in certain ways and comes back
to be meaningful in certain ways and in
a certain way if you kind of squint
until your head the actual architecture
of slack resembles the architecture of a
massively multiplayer online game right
so you kind of have your world that you
operate in which is your team and in
order to kind of make that world seem
both persistent and sort of
interactively mutable with other things
in the world you end up making a pretty
thick cache of what's going on in that
world and then you've got some way of
getting hopefully low latency updates
for the changes in that world so that
kind of mental paradigm of oh it's kind
of like an online game actually explains
a lot of things about slack like it's
like why we have loading screens for
instance that's the same reason that
video games tend to have loading screens
so we're going to illustrate all these
moving parts here going to exercise all
the bits on screen here by working
through a worked example of a user
logging into slack and listening to
messages and writing some messages so
there's a element of this that's just
the internet right so when you go to you
know my domain name.com dns does it's
magic you know a bunch of Cisco routers
are in the way ELB etc and you end up
somewhere in slacks web app code
somewhere on a dub dub dub machine that
slack operates and we call you'll hear
me go
back and forth between dub dub dub and
web app and the contraction web app
because the github repo is is named web
app and the first thing that they do is
this method that we call our TM start so
they're gonna use our API our TM in the
context of slack by the way stands for
real-time messaging so super occurs in
the API a lot kind of trips off my
tongue a lot we say our TM starts so
much that I'll be saying our TM start a
lot as well
and so a real-time messaging start this
is saying I'd like to start a session
you present some token some auth magic
happens and so on and a word about
what's actually happening on the inside
there what's running there at that web
endpoint is a PHP monolith right is an
app law is a bunch of app logic a little
less than a million lines of code at
this as I said here talking to you
that knows where all the business
objects that describe your team actually
reside right there out in some database
somewhere and if you kind of look at the
DNA of this thing you can recognize it
as a you know very competently executed
lamp stack that has had to scale out so
for instance it's memcache wrapped
around my sequel my sequel really is
doing queries for instance that's not
all just point queries the actual engine
we happen to be using for PHP is the hip
hop virtual machine which is is
Facebook's JIT for PHP it gets things a
little bit faster but it also gets us
the option of using a gradual typing
system for PHP called hack lang
but from 100,000 feet yes it's really
really PHP and usually when I when I
delve into this sort of whole
complicated and interesting conversation
ensues and I desperately wish we had
time to go into this whole interesting
complicated conversation and that
conversation usually starts with I like
slack but I hate PHP with a raw passion
that burns inside and I don't know how
to reconcile these two facts they're
confusing to me and I wrote a blog post
for engineering blog recently that
basically makes the weird claim that
those are both true right that it is
simultaneously true that PHP is
essentially a bad programming language
for almost every reasonable definition
of bad and then it's a very good choice
for your application logic for something
you're going to serve on the web if
you're really curious about this I
suggest you dive in there but
interest of talking more about how slack
works I'm just going to leave that
thread dangling for now and ask that you
take me at my word on it
so now here's where things start getting
a little strange and there start to be a
few forward references so your team all
the business objects that represent your
team really are relational data inside
of slack there are a bunch of tables
that contain things like messages and
users and channels and they're designed
in sort of a you know relatively
normalized form and it is also the fact
that we're charting by team it turns out
right so team IDs get you to a database
where your team lives probably lives
with some other teams unless you're
really really active or really huge team
or both that when I say shard by team
people tend to assume that what that
means is you take the team ID and you
hash it and you project that hash onto
some address space right there you take
that hash and you mod it or you you know
Kodama it or whatever on some set of
shards that's not what's going on right
the actual layer of indirection that
goes from a team to the shard that it
lives on is arbitrary and it's arbitrary
for basically administrative reasons
it's arbitrary so that if a shard gets
hot we can split it manually or
automatically in some cases we're still
sort of in the transition from manual to
automatic splits so that metadata of
which shard a given team lives on lives
on another my sequel database that we
call the mains because they're a pair
but it's one logical database and that
thing knows which domain names RepRap to
which teams and which teams map to which
shards and not a whole heck of a lot
else the process of sort of getting
metadata off of the mains has been a
recurring theme in our infrastructure
over the last year for reasons that will
become really clear and with that
information in hand we go off and start
actually constructing the payload for
the response to our TM start so remember
I'm claiming that you know slacks a
little bit like a massively multiplayer
online game RTM start is kind of the key
frame that bootstraps
the simulation of it right it's the
thing that tells you here's the state of
the world here's all the users here's
all their profile pictures because for
all you know while
client was disconnected everything
changed right for all you know the last
time I've connected to this team it was
only two people and now it's 2000 people
and they've all changed their profile
pictures for the election and whatever
else right so our team start gives you
back this big blob of data that's
essentially here's where we're at and to
do that we do lots and lots of sequel
queries it's the most egregious
offenders on this take something like
thousands of sequel queries so thousands
of trips back and forth to this shard
and yes fixing that is something that
we're you know constantly working on so
a couple words about I've kind of been
whistling past the fact that there are
these little pairs being Illustrated
here I'm gonna keep whistling past it
for a second longer here just bear with
me
so both pairs are across two different
data centers right so barring sort of
world historical scale crises hopefully
these things will be available for reads
and writes given a single data center
failure and reads and writes you say
Keith reads and writes that's
interesting yeah hold that thought for a
second so my sequel is an unfashionable
choice a little bit like PHP not quite
as bad a reputation as PHP my sequel
definitely does what it says in the
outside of the box and it's very
carefully engineered but still not we're
used to hearing when people talk about
their scale-out storage engines and
probably the most important reason that
we're using my sequel is just the
collective history in the universe of
the thousands of server years of
operating my sequel without it losing
data we also actually like the data
modeling discipline that relational
imposes on us we don't wish my sequel or
a document store we don't use it as if
it were a document store the way a lot
of people scaling up my sequel do and
also they're just a lot of in-house
experience operating it and writing code
for it and writing queries against it
and diagnosing performance problems and
figuring out you know when it makes
sense that a secondary index and so on
but don't think of this as some big
arrow in the quiver of the no sequel
debates because actually we don't you
know we throw away a lot of the strong
properties of my sequel in the way that
we're using it specifically
we use it an active-active way we do
master master replication so you
actually can write to either one of
those heads so each one of those heads
is is replaying a statement based log
and producing a statement based log from
its partner and if you have if you you
know go out and hold a stethoscope up to
one of these logical you know it's a
single database logically but they're
separate machines they really will be
sustaining right traffic on either side
of this and there really will be some
some lag in the replication is that
crosses from one machine to another
so we've taken my sequel and made an
eventually consistent store out of it
and we do this because basically we want
to be right available in in the case of
failures and we want master promotion to
be automatic in the case of failures now
you might be wondering what about
conflicts what if we actually wrote the
same row on either side of these things
where we did something else inconsistent
like we tried to insert the same value
conflicts do happen
so basically when application authors at
slack are writing query is especially
update queries to this system they need
to bear in mind the possibility of this
and it's where some that comes up in
code reviews a lot is like is this
actually safe in the event of you know
serious master master lag a replication
lag if this right happened on both sides
would it be safe and the majority of our
conflicts that do happen we're able to
resolve automatically basically with
application knowledge right we know what
it's trying to do and so we're able to
resolve it but there are human beings
who work at slack who have to resolve
conflicts that are nonzero rate and this
is one of those things that uh you know
when I talked about sort of things that
just aren't pretty but are true that's a
thing that's not pretty but is true it's
a thing that is not going to scale
forever that we're currently doing a bit
of a kind of application level
programming trick for making these
things a little safer is a lot of these
are kind of racy updates of the same
structure right so you know you can
imagine that I've got my laptop out and
I've got my phone out and I'm trying to
sort of change my profile picture and
both of them to try and catch slack in
the act and get you know a be a to
happen or something
for the most part rights like that are
are are what you're going to read what
you wrote even though we're doing this
master master business because there's a
best effort
to kind of get read what you wrote
semantics using the team ID so the
lowered a bit of your team ID is your
preferred head all right so even even
teams want to write on the left head and
odd teams want to write on the right
head as it were and of course they'll
fail over to actually doing the right
song on either side so you still have to
think about all these conflicts and
stuff but in practice it's actually
pretty hard if things are humming along
in a healthy way to try to write the
same road twice okay so we do all of
these eventually consistent queries that
let us construct the RTM start payload
and we return it to you and it's got a
lot of information it's got the identity
of every channel on the team it's got
the identity of every user in the team
it's got the membership of all the
channels it's got you know where the
reed cursor has moved ahead and all
those channels since you lost examined
them and blah blah blah blah but the two
pieces that we probably care about for
our purposes are it says okay you know
you did start a session I'm kind of
using active user for right now and
here's your WebSocket URL right so
there's a little WebSocket URL there
it's some host name at slack messages
com
and then the actual applications named
WebSocket and then some funny hash of
something or other and we'll talk about
what that funny hash is doing in a
second so the contract here is going to
be now that you've gotten your key frame
right now that you've gotten your state
of the world at the time you started
your session we're gonna keep you up to
date by trickling cache updates through
that WebSocket connection all right so
that WebSocket connection if you're
using it as a chat app one of the really
visible of things that's flowing over
that WebSocket connection is just
messages going back and forth right so
that's where the write amplifier that
actually takes a message you send
broadcast it to everybody that's where
that thing lives and you can see that
operating but going on behind the scenes
is actually a lot of sort of quiet
oh gosh so-and-so changed their profile
picture okay cool I'll show the right
profile picture and things along those
lines also presents information we'll
talk about presents briefly too so
diving in a message server I kind of
gave a little preview of this briefly
message server is an actual bunch of
Java code we wrote
it's message in life is sort of twofold
right so one is that it's a right
amplifier so things that happen on the
team need to be shared to everybody
that's actively using the team and it
also needs to be sort of communicated to
the future it needs to be communicated
to all the people who aren't here in
space but will be here later in time and
so we need to store these messages
somewhere we were talking about all this
discussion foregoing discussion about
sort of master master replication and my
sequel and shards and the left half and
the right half and so on that's all
application logic message server doesn't
know anything about it it's in a
different language
there's no unholy bridge between Java
and PHP that we've tried to forge it
really side calls into a web server in a
way that's not so different from the way
that you'd inside call over you know
port 80 from the big bad internet so
that it can actually persist the message
and I'm making it sound like it's really
simple so you might be kind of like is
this literally just kind of 40 lines of
code that you know opens oh you know
it's a wrapper around a WebSocket
library and then a for loop over the
people that are connected well no so one
of the things that I talked about there
was that there was a contract here that
you're gonna get your updates over the
WebSocket you might have noticed there's
actually a little race here right so RTM
start gives you a WebSocket URL time
elapses things change the world moves on
a little bit maybe some messages even
get exchanged and then you go over and
start connected and start talking over
the real time connection to the web
server to the message server rate so
that little hash of weirdness that came
along with your payload that's actually
a little cursor in the event log that
the message server keeps the message
server keeps an in-memory buffer of
recent events and that things actually
bounded in real time I think it's 30
seconds so in practice you kind of have
30 seconds to go take the ticket that
RTM start gave you and connect to the
message server and when you connect with
it it'll catch you up with whatever
you've missed there's also the fact that
we're doing persistence across you know
a network boundary possibly a data
center boundary so it's actually
possible that we're not able to persist
a message for a while and then what
should we do well you have the option of
sort of the sort of clinic
distributed systems answer is whoa like
we're not in an okay state we're not
able to actually send your message right
now that is probably not the most
helpful thing to do in this situation
though right Network weather does happen
and usually network weather is you know
very brief seconds later we will be able
to persist it so there is both an
in-memory and on disk queue that we try
to recover that the message server
operates so it you know persists locally
and replay is the locally persisted log
if it you know crashes and comes back up
or if there's a you know something
really terrible happens and it takes a
long time for the message for the my
sequel and web app to be able to talk to
it by the way we really carefully
monitor the depth of those those queues
if those queues start building up that's
a really powerful sign that sort of
something is badly wrong with the system
and it could be load anywhere from oh
gosh the peril of presenting on your own
laptop here yeah and it could be load
anywhere from the web server my sequel
or other systems that I haven't been
able to talk about here yet also the
vast majority of messages in a large
team or presence or I mean by presence
is the information that so-and-so opened
or closed their laptop so you know your
intuition is probably most of what
happens is you're sending chat back and
forth gets the chat application well
know most of what you're sending is like
the stuff that drives the little green
dots next to people's names when they're
online that might seem wasteful but this
is actually a really really critical
feature like going back to the online
game analogy for a second those little
presents dots are a lot of what makes it
kind of feel like a virtual water cooler
in a virtual place where sort of people
are you know you're kind of using your
brains location hardware to think about
slot channels sometimes and the reason
that messages that presents messages
proliferate this is a little weird but
it's true is that this is basically N
squared in the number of people in the
team so if we actually tell every
connected user about every other
connected users present status whereas
like other ones are kind of narrow cast
two channels and stuff then it's just
going to be the case that ultimate you
know in the long run you're gonna be
dominated by people opening and closing
their laptops because they do that more
often than they post a mess
in general that goes to every single
user there's another big box that I
haven't talked about yet which is the
job queue let's work deferral mechanism
I know job queue is not a very inventive
name but that is what the code base
calls it and so I'll stick with it here
be one of those things would be fun to
make up a name for someday we physically
manifest this thing in Redis so there's
all kinds of stuff that you kind of
don't want to do I have a background as
operating system person by the way so I
find this analogy just irresistible in
an actual web request it's a little bit
like the top half of an interrupt
handler right you kind of you have
real-time you're sitting there occupying
a slot and like Apaches set of processes
it can handle you want to get what
you're doing out of the way pretty
quickly so that this server can go on to
serve other users but there are things
that just inherently take a long time a
really extreme example is an import or
an if you're just importing you know
years worth of data from you know some
competing chat product or something
that's going to take hours no matter
what we do but we want to be able to
kick that off using the web application
and we want to tap access to all the
sort of you know application logic that
I've been talking about before but we
don't want to just sit there squatting
on a open socket over port 80 for hours
at a time so that happens to this job
key mechanism if you're kind of in the
top half trying to get your requests
finished in a timely manner you can
write a memo to the rest of the system
to to do work later and we do the actual
physical storage of that in a set of
Redis clusters I'm calling it blood job
queue by the way it's actually multiple
queues so you have different sort of
levels of service class some performance
isolation by running them in different
queues different pools of workers and
the pools of workers are sitting there
actively pulling for work to do they
could use pub/sub but they don't yet but
actively pulling for work to do and
pulling stuff off of there and by the
way this is kind of this is one of those
things that seems really the fun thing
to take away from this slide if at least
if you're me or if you ever end up
working at slack is that link unfurling
which is like what we call it when we do
the little cartouche around a link right
so you're in some channel you type HTTP
colon slash slash you know twitter.com
slash some username or whatever after a
second or two hopefully a little box
shows
with you know that person's profile
picture and sort of a blurb about what
Twitter is and a little bit of their bio
and whatever that all happens in this
job key mechanism you want that to be
there because right sometimes
external URLs fail sometimes it takes a
long time to curl something else
sometimes that sites just under load and
if you just retry again in five seconds
it'll be just fine but I mentioned this
because if I'm sort of stuck on the
subway and I'm trying to make sure that
the site's still up and I didn't break
it with you know the code I just pushed
or something like that I just linked
infer or something in a message to
myself and if I do that I have just
exorcised the hell out of everything
I've talked about so far right I means
that the web app it means that the web's
up it means my sequels up it means the
message server for my team is working it
means that Redis is operating in some
sort of vaguely good way it means that
job workers are finding stuff from Redis
in a timely way and actually doing the
work that's on them it means that the
messages that go down through the
message server that tell you that job is
completed and tell you what to show now
are actually functioning and so on okay
so we've kind of worked our way through
our career little picture here we've got
the mains we've got the shards we've
we've talked about message server we've
talked about web app how they're the
kind of two big gateways for sort of the
bulk you know session initiation and
termination and then the actual kind of
fine-grained low latency session
information so there's a whole lot of
stuff that I'm omitting here so we don't
use ROM isequal all the time memcache is
there for a lot of performance-critical
queries it's basically providing a
materialized view of some queries that
are important that keep recurring this
is currently totally manual right this
is currently totally done by people
noticing that oh gosh we spend a lot of
time on this API endpoint why is that oh
it turns out it's this query is it easy
to cache is it safe to cache yes it is
and so they build it that way you know
the old-fashioned way by hand so there's
no kind of magic framework that just
automatically caches queries for you yet
I didn't talk about a service that we
call CVS for the computer data service
one of the as the the essence lacks
backronym all right so after slack was
founded our founder Stewart Butterfield
made up the acronym searchable link to
archive of company knowledge but
honestly slack was just a cool name and
that was
plausible backronym for it but haven't
talked about search at all yet we do
search and it involves search quality
and that involves the sort of usual
pipeline of search quality tools and m/l
models and weights for linear models and
things like that are currently provided
by the cdf thing over a thrift interface
we're using it for more and more things
it's the bridge between sort of stuff
that we have to construct in our data
warehouse in a batch way and real time
services I didn't talk to you but you
are rate limiting a lot of it there are
lots of little pieces of this picture
that I drew where if you poke them hard
enough really terrible things will
happen so a lot of those things are
guarded by if you know some version of a
finger and the wind for what's a
reasonable rate to use them at and if we
start exceeding that we shed load and
start alerting people and by the way the
quick back the quick version of the the
nickel sketch of the solar
infrastructure is that we use solar its
power and search those search indices
are updated in real time by job queue
jobs because solar itself sometimes is
sticky and sometimes gets under load and
it's team partitioned which is a
complicated choice but it sort of fits
in a framework of lots of the other team
partitioning choices we'll be talking
about so feel free to press me on this
if you're curious about more later so
there are bunch of cool things that fall
out of this the way that we've built
slack so far one of the unifying themes
that's a little unusual if you're coming
from sort of a scale out
consumer-oriented
backend is that there's been team prove
it partitioning as a pretty pervasive
decision and as long as teams are sort
of the dimension you're scaling along
right as long as that's kind of how
you're growing this works really well
this lets you scale teams really really
easily so if you ever go to slack comm
and you sign up for a team we're
essentially provisioning an instance of
this little box in Aero diagram I keep
showing right we find you some shard to
fit your team on we slap it there we
find some message server that seems to
have some spare capacity we just tell
you that's the message server that
you're going to be using we write a row
out to the mains that says hey this team
exists here's its domain name here's its
name and a few other sort of pieces of
metadata pointers to which shard and
message server shard and so on it uses
and we're golden and you know slack
unlike a lot of the
a lot of sort of recent hot mass
adoption consumer products we actually
charge our customers right people
actually pay us money and so this is
actually a fairly nice layer of
indirection to have from that point of
view right if you have an angry customer
who you know probably is angry because
there's such great users of your product
right they're probably angry because
they use you a lot or they have a lot of
users I was having a bad time of things
being able to just say okay we'll put
you on some nice fire-breathing platinum
coated hardware is a nice economically
viable store decision to make okay so
let me critique this not all is is roses
for us right now we have a number of
challenges like everybody else and a lot
of these challenges do arise from sort
of some decisions that we made in this
architecture so we'll walk through a few
scenarios or on the system that I've
just outlined which is and this isn't
like some weird punchline by the way
like that really is how slack works so
I'll walk you through a few scenarios
that are difficult to deal with with
this kind of infrastructure the means
you might have wondered why I mentioned
them it seemed like sort of a kind of
detail you might want to skip there a
database there they're a pair of
databases but they're still a database
and terrible things can happen to them
and they turn out to be a single point
of failure I've been talking about RTM
start a lot and emphasizing how it's
this really big bulky thing that kind of
tells you everything you need to know
about your team that turns out to be
really messy for large teams if you are
a thousand people who share an office
together and the people doing sewer work
on the street outside your office today
happen to cut through your fiber line
and people get things working again at
3:00 p.m. and all thousand of you try to
reconnect at the same millisecond that
also turns out to be really difficult
for this infrastructure to handle and
just sort of diving into this some more
you might think well you know the man's
sure like it seems like a single point
of failure logically but their two
physical machines what's the big deal
it's true that if one of the mains fails
its partners gonna take over if both if
both of them fail well we've wrapped a
lot of memcache around these things so
certain number of people are still going
to be able to login
but as soon as they're down the set of
people who are actually able logging to
slack is sort of dwindling quickly so if
you're not a hidden memcache for your
team's row from the mains slacks
essentially down for you and this might
sound really far-fetched because you
know you've already sort of sustained
one failure you said they're in two
different data centers like why did the
other one fail well you know how many
people have had the experience of trying
to sort of protect a system by
replicating it and have the load be
contagious right so those kinds of
scenarios where the reason that one of
them failed was because it was under too
much load well now you have twice as
much load on the other half and that
one's probably not long for this world
so this has happened the worst sort of
outage in in terms of sort of affected
users in slacks history was almost a
year ago on November 23rd and was of
2015 and was an instance of mains
failures so this is still something that
keeps us up at night I talked about our
team start sort of being big let me put
a finer point on it it's not just big
it's actually quadratic so as you add a
new user to your team there is some
probability that that user is also going
to add a channel and that the users that
are already there are going to join that
channel so it turns out channel
membership is naturally quadratic in
team size right you sort of have a
rectangle that's M channels high and end
users high and it's moving up into the
right getting bigger quadratically every
time you add people to your team so if
you're in a big team we're sending you a
quadratic chunk of data when you start a
session and this was sort of hard to
notice initially just cuz the constant
factors are pretty small but you start
getting into the thousands and tens of
thousands this becomes really noticeable
and making this even worse as sort of
that mastery connect scenario so imagine
we do have thousands of people each of
them doing these order of N squared RT M
start operations there's kind of n cubed
load on the system and this decision
that we made of focusing teams on shards
means that that n cubed load isn't
spread across a nice ocean of sort of
cloud capacity it's concentrated on that
single pair of of databases that
constitute this particular shards master
master pair
so we're just gonna lay down and die
it's a bummer it turns out though it all
ends here their jigs up if you guys like
you know have a job for me I'm pretty
smart I'm good at programming computers
sometimes no no of course not
of course not we're not giving up there
don't worry never so with the mains
we're very lucky so currently they're my
sequel like everything else there's
essentially another instance of that of
that sort of my sequel master master
pair of design pattern they do very
simple queries against the mains and
very simple updates against the mains
it's almost the updates are almost
always adding a row or modifying an
existing row the queries are always
point queries so you don't need some
incredibly special properties or some
incredibly strong properties from you
know pick your favorite scale out
storage engine to replace the mains with
why hasn't happened already well it
hasn't happened already in part because
we've gotten really good at protecting
the mains through a combination of
memcache and the kind of rate limiting I
was describing earlier but also because
it's just a really hard thing to change
when we finally kind of work up the
nerve to do this it's going to be a very
difficult and delicate operation to do
it in a way that doesn't actually take
all of slack down and we have ideas
about how to do that of course you know
well you know T the rights and
double-reed and do you know run the
whole playbook for how you deploy
stateful services that are scary it just
hasn't bubbled to the top of our list of
priorities yet a little more to say
about our team start for large teams so
first of all partly incremental work has
just been astonishingly successful at
this so even though there's this like
bummer kind of big o story about it
being quadratic the actual distribution
of latency is here is not unacceptable
p95 is around 221 milliseconds right so
barely perceptible p95 is you know well
over half a second but we're not you
know with that N squared there you might
think that there are people sitting
there you know growing old while they
try to connect to slack and that doesn't
seem to be happening mostly and we're in
the process right now of actually
changing the story around the client
architecture I've been telling you right
so instead of having a complete view of
the
we're in the process of migrating all of
our clients and there's several of them
and they're in different code bases to
be able to understand a partial view and
give you separate api's that let you do
partial queries of channel membership
and I'm using the channel membership
example that's really easy to understand
but that kind of quadratus it e as you
add users keeps popping up in lots of
natural places my fairy example this is
just emojis right so slack lets you add
custom emojis you can name an emoji give
it a little 32 by 32 thing it might even
be animated and it's relatively natural
as teams grow to like go through this
life cycle where they kind of have emoji
bloat and then some administrator
decides to clamp down on it or some sort
of no fun person says hey this is work
why do we have all these like animated
emojis of parrots and but until you
reach that point like emojis are also
one of these kind of quadratic things
and I spent a bunch of time looking at
emoji performance when I first started
back in January and then I masse
reconnects I'm I get to introduce a new
service that we're calling flannel and I
keep trying to come up with some like
clever name for what the what flannel
means like some sort of pun on channel
or like maybe like Spanish you know
Mexican desserts
like the flan or something we've got
nothing it's just named flannel I'm
sorry and what is it
well flannel is a nap level as an
application aware edge cash right so
it's an edge cash that let's unpack that
it's a cash so it has some of the
information that we need to start and
run these real-time operations it knows
about slack or it's specific to slack
it's not some generic thing it's not
operating on sort of arbitrary restful
things or something it knows that the
thing that it's trying to do is
accelerate slack connections and
sessions and we don't necessarily run it
that close to the data center we're
running it on you know hopefully nice
gear with good network connectivity but
we'd like to be able to run this you
know in a closet that we're renting from
somewhere far around the world hopefully
so I sort of before a picture of the
real time part of the stack looked like
this and we're in the process of
changing its that it looks a little more
like this so a couple of observations
about this first of all this looks like
a really silly change to be making right
I've added a hop in a layer of
indirection presumably we have to pay
money for those computers you know why
are we happy about that well
that layer of indirection is doing a
couple of things for us first of all
it's keeping load off of the actual web
back-end so if a lot of the functions
that sort of RT M start used to take up
are now happening when you connect to
flannel we're actually able to serve
that you know in a way that scales out a
little more straightforwardly you notice
there's two of them right so we can
actually scale edge capacity for flannel
separately from scaling the back end all
right so we don't actually have to make
some big painful migration to scale out
all the shards for instance right we
don't have to find you know we don't
have migrated to MongoDB or something
like that which would actually be a
challenge by the way I didn't kind of
get too deep into this but we really do
use sequel the database engine does work
for us we're actually relying on the
query planner to be smart there are
joins and stuff right we're not using it
like a document store so being able to
cache the application level results of
that throw closer to the user it
logically provides a nicer place to
scale out a little bit and this is not
all pie in the sky I did an engineering
blog posts like back in spring about how
we were planning to do this at that time
we were calling flannel slack D it's in
its creator since then it's actually
gotten started on it want to call it
flannel so we're calling it flannel and
this this thing actually exists
so slacks internal team the team that we
actually all live off of all 650 slack
employees are using it there are a few
other teams that were using it for
because it sort of addresses really
critical problems that they're
experiencing and with any luck it's
coming to you real soon now as a version
of these slides that's kind of was
premature and jinx did so but this is
not sort of a you know sometime in 2017
when we get to it this is real code that
we're in the process of sort of figuring
out how we're gonna deploy okay then so
let's take a little breath here think
about some of the things we learned it
pains me to highlight this but of course
there's a lot going on that I wasn't
able to go into here I'd be incompetent
to answer a lot of interesting questions
about the client technology but we do
have clients and they're complicated
software to you know if you look at sort
of the distributed system that is slack
in terms of the number of cores the
amount of storage use the number
memory you know the flops the bandwidth
the vast majority of it is in phones and
laptops not in computers that we operate
or write you know are able to administer
we do voice calls there's a whole bunch
of infrastructure on the back end to
make voice calls work well and a bunch
of Technology and clients also to make
voice calls work well and there were a
bunch of interesting trade-offs there I
haven't told you how we backing this
stuff up how any of this information
ends up in a data warehouse I very
briefly thumbnail sketch search Ivan
talk about how we deploy code we do that
about 40 times a day to the web
application and the infrastructure that
supports that and lets us do that with
confidence and lets us convince
ourselves that we're not breaking tests
and that you know the core workflows
inside of slack still work with all
those changes to enable that kind of
high velocity is is itself very
interesting monitoring alerting is also
a critical part of running slack so you
know there's a really rich kind of
smorgasbord of stuff that I could be
talking about but it seemed like the the
interest gauge for people who want to
talk to me about slack are usually more
interested in sort of this basic kind of
you know what is the 100,000 foot
picture view of it but there are many
many careers worth of stuff to learn and
do and problems to solve here so we did
the hundred thousand foot view as
promised we talked a little bit about
our big PHP monolith and why you
shouldn't hold that against us if you
are currently inclined to we talked
about the unusual way we use my sequel
well we've kind of used my sequel to
make an eventually consistent store
that's highly available for rights under
failure talked about how our real-time
stack works and how we defer work we
talked about a bunch of challenges we
faced a lot of them around sort of how
slow reconnects are and RTM start and
the fact that failures and reconnections
are often correlated in time and we need
to handle that and we got to explain a
little bit about flannel which is a big
part of our solution to that story so
this is work in progress there's a lot
to do there's contact info for me on the
slide obviously like you know I would be
remiss if I didn't mention that we're
hiring and with that I thank you for
your time I'd be happy to take questions
okay so if anyone has questions please
raise your hand and then we'll hand out
Mikey all right what happens when you
get the third data center how was the
active active and replication work in
that case yeah so one of the interesting
things about this master master thing is
there's actually no obvious way to go
from N equals 2 to N equals 3 and the
answer is that is one of the like
bridges that we actually need to cross
so a current possible answer to this is
that we just sort of go more
conventional route and do master/slave
so maybe there is a right head data
center and writes just have to make
their way back to the right capital of
the world maybe we can have that right
be per shard and we try to make it so
that you know you're geographically
close to the right head for your for
your particular shard I could speculate
we haven't really built it yet
but it's a solid observation that
there's not a totally obvious way to
sort of make the master master stuff
work if you're thinking why can't you
just kind of make a ring of three and
say you know here's a B and C a replays
to B be replaced to CC replays to a it
turns out that doesn't work and and ask
your local my sequel expert if you're
curious about why okay so the bin logs
don't get replayed so they don't echo
forever so you the one from a doesn't
make it to C thanks for this great
presentation I have a question about how
do you handle hot spot for your charts
yeah great so the question is how do you
handle hot spots for your shards
currently we handle it by alerting and
having people scramble to split or shard
the top and if that doesn't work we we
handle it by paying for even fancier
hardware to run the shard on and by
hiring you know hopefully smart people
like yourself to come work for us and
come up with a solution that's going to
work better and be more automated and
hopefully not break everything about the
application in the process does that
answer your question
all right honesty yes so smoking so you
didn't think about security so lots of
teams shared lots of documents through
slack and maybe some confidential
information like bats or and stuff like
that so how do you make sure that these
information lies in your servers and
only our team acts as these service not
only even your team can have access to
these data even if they're wise to see
my sequel database or the encrypted
message if you encrypt them there's my
first question my same question I know
that you won't slack to replace the
normal email flows between the team so I
had this case that one of our team
members just left the team and all the
information that he wrote to us
including descriptions conversations
documents that he shared with the team
is gone how would that replace the email
if it's gone
okay that's those are a couple
interesting questions the first one said
I should say by the way that I'm like
not an actual InfoSec professional so my
understanding of this stuff is
especially secondhand in amateurish
however one of the advantages of sort of
the team charting approach that we
didn't go into too deeply here is that
it does make attempts at intrusion a
little more legible right so it does
make sort of attempts to exfiltrate a
team's data or something a little easier
to see one of the things that we're is a
little opaque from the outside that
we're more careful about than some of
then I think some of our competitors is
that people don't have access to the
production environment right I don't
have access to the production
environment I don't have the you know I
can't run queries against the production
my sequel shard there is you know glass
I can break in case of emergency that
enables me to do that for instance but
that's obviously heavily audited and the
people in our security team have tooling
that they refuse to tell me about that
they claim is keeping that stuff on
lockdown and that's kind of one of the
things that's different about a paid
product that we're asking people to
trust their company's information with
in the case the second one where my
understanding is basically you're saying
that somebody left the team and they
kind of took all their information with
them when they left the team that is not
how I understand the product to be
supposed to work right so when when
users disappear they still kind of have
an identity
the product and things like your DM
threads with them should still be there
files they created should still be there
to the best of my understanding and
share it in the same places but if you
have a repro of this and if you're able
to share any details at all you know hit
me up and I'll try and dig into what
actually happened there one more
question
great presentation thank you our
engineers like engineers happy with PHP
so it depends on the engineer the
question if you didn't hear it is our
slack engineers happy with PHP so it
depends on the engineer right and so I'm
coming from like 7 years at Facebook so
I of course like feel like this is you
know the old sword right this is I know
exactly what the what the scary parts
are and exactly how to use it so it
doesn't break when you're coming to it
new there's a bunch of things that are
actually that are actually surprising I
think by and large once they've adapted
to our code base in our conventions
we're using it well I think most
probably wouldn't say they love PHP the
programming language but they sure like
deploying 40 times a day and it turns
out that's there those two are sort of
fundamentally in tension in an
interesting way so I'll sell my blog
post again if you don't mind if you look
for taking PHP seriously everyone for
coming and another round of applause for
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>