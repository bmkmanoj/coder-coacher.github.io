<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Data Infrastructure at SocialCode | Coder Coacher - Coaching Coders</title><meta content="Building Data Infrastructure at SocialCode - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Data Infrastructure at SocialCode</b></h2><h5 class="post__date">2013-09-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mTxqIleyrIc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello so I'm will Larson and I'm the VP
of platform technology as social code no
one knows what that means including
myself and my co-workers so you guys
think any kind of a VP of engineering
what I want to talk about is building
data infrastructure and so I I'm a I'm a
manager these days so I tend to think
about things a little bit differently
and it's not until you talk to someone
who hasn't thought as a manager that you
realize how foreign you're thinking has
become so when I think about companies
and I think about technology and I think
about Python or anything I really think
about growth and change and what it
means so we talked a lot about changing
companies so as you go from one office
to three offices one office to three
times as many people talk a lot about
grabbing more products go from one
product three products 10 products what
happens there but we don't always talk
about what happens when your data
changes something we talk a lot about
scalability going from a one small
database to a larger database to a
different new database back to the old
database and what huh we necessarily
talk about the social dynamics and how
to scale a data platform in more of a
kind of team oriented with so that's
what I want to talk about tonight's how
do you build a platform where your team
can grow with the data not just from a
technical perspective but actually as
people work so what I wanted to do is I
want to talk about where we started out
kind of the stack that we had problems
we had how we had to solve them today
and kind of why we chose the solutions
that we did this should look pretty
familiar to you on this is probably the
most vanilla vanilla deployments you're
ever going to get if you're running a
Python infrastructure and so it kind of
goes like this running on Amazon of all
your servers up there have a couple of
web servers are taking the requests from
your clients for us we're running Django
on unicorn your servers are behind a
load balancer got an ISO big hosted my
sequel cluster got a Redis and then you
might be doing some offline processing
with celery so this is where we started
I think if you went to most Python
startups you're going to see something
incredibly similar to this it works
really well works incredibly well but it
starts to grow and there's all sorts of
interesting topics I'd love to talk
about how you grow different elements of
it how you grow the performance I grow
the scalability whether you go to kind
of service-oriented architecture but on
there's also a lot of problems with the
data that you run into very relatively
quickly so three problems that we ran
into and our pretty Universal is that
when you have a giant my sequel database
you run into a lot of schema problems we
work with Facebook and Twitter data and
one of the great parts of that is that
they change their data constantly then
we have literally no control over this
and they push it out a convenient times
like friday at 5pm or accidental bug fix
at six on saturday that doesn't fix
anything but break something else so you
can design a beautiful kind of schema
it's going to work really well but if
you're working with third-party data it
doesn't matter because you don't
actually have any control over it um so
one of the things that we've dealt with
is as ours database has gone from you
know one terabyte to larger than a
terabyte um is this how do you run a
schema migration when it takes 12 hours
and you can't go down because you're
running ads for clients and your clients
don't really care about your database
don't really care about your platform
they just need you to run ads we're
gonna do a make good and then I'm going
to pay them back half the money that you
made that day then you're going to yell
at another problem you ran into is not
just literal data silos so you have data
silos sometimes we have three different
products and people aren't really
communicating with each other and
Twitter product and your facebook
product you can't get the data together
and your data scientists hate you
because you haven't thought about how
they need to work but we've actually had
data silos that are siloed just by the
nature of the performance of the
database itself which is so we get some
new data but our indices are so large
our indices are already you know 80
gigabytes or 100 gigabytes they can't
actually
all through the tables that are storing
the indices or the indices to actually
get at it so you have all this great
data but you can use it and so your
product becomes kind of horribly twisted
over time in this way and then another
thing that has happened as you grow is
and you have kind of a single database
is that you have a bunch of different
users you have an API that needs out of
real time up to the date up second data
they also have a data science team it
needs kind of high wait-and-see
high-throughput solutions you only have
one database you don't really know what
to do so so this was definitely us for a
while we all suffered through a lot of
it and then we've spent a lot of time
thinking about at the same time we went
from you know one office to three
offices from one product to three
products and from one database to about
20 or so databases but how do we solve
this problem how do we make it so that
we can share data in a pretty convenient
way so we thought about this a while and
these are the requirements of came up
with so first we thought it was really
important to decouple publishing and
consumption so what happens when you
don't do couple publishing consumption
as each publisher has to know about the
consumer which means I have a really
high coordination cost that means if
someone else and the team wants to bring
up a new server they have to talk to
everyone else who's publishing data
which means that they're not going to
bring up a new server because they're
not going to talk to everyone else on
the team they just get really slow or
you get really silo it very literally
because everyone does it everything else
kind of on their own stuff another thing
is you really wanted ability to explore
data cheaply we have a very large data
science team and these are slightly
they're not developers but these are
people were incredibly fluent with
sequel these are people who are much
better than I will ever hope to aspire
to be with sequel but but they can't
write a great Python scripts if their
code goes into production you have a
meeting about how you screw that went up
oh and you need the ability for them to
query it pretty easily third schema
changes as I mentioned we don't have any
control over our schemas at some level
so we need the ability to change really
frequently or we miss out of products to
our competitors who can change their
schemas more quickly finally actually
four is not finally if there's five
working on it so something that we've
dealt
before is it's fine if things fail it's
better if they don't fail but when data
solutions in court infrastructure fails
it needs to feel really loudly and
sometimes you run into experience where
you all the sudden are missing a month
of data or three weeks of data and you
just can't get it back um I used to work
at digg and we once had our we set up
our Hadoop cluster incorrectly and the
name cracker the name though it went
down and we lost every single piece of
data for six months so what would have
been nice is if it had given us a
visible error or the person had not
screwed up but instead what happen is it
couldn't save the new the new config
file to disk so it just never never did
anything until was restarted by accident
months later failing loudly is really
valuable oh you don't really like to go
talk to your board about how you lost
six months of data a little bit
embarrassing the finally kind of data
store lock in there's all these sorts of
new databases coming on and you know
every every six months or something new
that's pretty cool like rethink DB
recently seems pretty awesome then like
two years later they actually work and
and it's a great often two years later
they work so we didn't want to get stuck
with my Seco we didn't want to get stuck
with postgres if you don't want to get
stuck with manga or cassandra we want to
have a lot of flexibility about how we
could as new databases come out keep
iterating trying out new solutions
cheaply so this is what we ended up with
and it's um I've been told this is not
the cleanest diagram but I tried very
hard so when you start out on the the
left you have services and so if we go
back here this entire slide is one of
these small little boxes in the services
we have a bunch of services that are
doing their own thing maybe one teams in
Seattle maybe one teams in DC and
there's doing their own thing not
talking to each other they don't need to
they're pushing data into a data input
service this is HTTP we're using JSON
schema to validate all the documents one
of the great failures of data
warehousing is you can collect all the
data but not have an easy way to
validate it and then you have all sorts
of data you can
use which is an actually data it's just
kind of a pain in the ass so we have all
the data coming from all these different
services flowing into this input and the
data input which validates the document
then accepts it and then it pushes it
into Kafka and soak off i think is one
of the most exciting databases i work
with i think if you go to the website
they call it pub sub reimagined as a
commit log and i don't know if that
means a whole lot to you it doesn't
necessarily mean a whole lot to me but
next slide I'll talk about why I think
talk is probably the most interesting
database have worked with in a while
even though it's written in Java sorry
it's obligatory um so then we also use
zookeeper we don't use zookeeper because
we are in love with zookeeper although
zookeepers phenomenal we mostly use it
because it plugs in out of the box with
Kafka and it gives you some really great
properties talk very briefly about
zookeeper but the most important thing
zookeeper gives Kafka and also our input
and output services is just coordination
on the ability to kind of say you know
I'm we have these consumers this is what
data they've seen talk a little bit more
about that in a second then we also have
kind of a shared data alquiler what this
has done is this is abstracted all of
the consumers from all the inputs it's
all so abstracted all the servers and
all the databases from the protocols in
the storage layer in between this means
we had throwaway Kafka we throw away
zookeeper and we could switch to a
totally new storage solution and all the
services all the back ends are totally
abstract form it and that's again HTTP
service where you can subscribe but we
also do some bulk loading into s3 which
we can translate into redshift and so on
which is a little bit annoying so why
Kafka so I think like I said cough is
the most interesting database data store
that we worked with in a long time and
it's a little bit peculiar so if you
start the new data bit of a new project
someone's going to say I want to use
Redis his Redis is amazing and I have a
lot of love for etis I've used it a lot
and it's done some bad things for me as
well but um
the best feature about red is that
developers want to use and usually say
no if you can pub/sub so dead simple it
started up and push messages in have
subscribers come and go and it just
works and it just works as long as your
consumer never goes down it just works
as you don't need to know if the data is
actually they're just works if you don't
actually care about date well I mean it
just works as long as all the consumers
can take all the data just as quickly as
all the publishers are pushing them in
so I'm really what coffee does is it
does pub/sub right so it decouples the
speed of publishing from the speed of
consumption and the way it does this is
data comes in and it writes it onto disk
and then each consumer has an offset the
offset tracks how far each consumer has
seen on a given Q which means that you
have as many number of consumers as you
want and each consumer just past have a
single offset stored for it within
zookeeper so very cheap consumers come
and go if you screw up you can set your
counter back and replay data and keep as
much as you want or as much as you can
afford it also that the new versions of
zookeeper actually have replication it's
new and 08 which means it probably
doesn't work but we're testing that and
it works sometimes um I would wait a
little while on that one another thing
gives you as horizontal or scalability
add more nodes get more performance
right now you can generally what they
say is you will get to performance on Io
before we get to performance on CP or
memory with Kafka and that's what we've
seen even on network I oh that is not
disgaea even on Amazon where the hard
disks are usually pretty awful we've
seen pretty pretty remarkable
performance there finally zookeeper a
lot of new databases suffer with kind of
distributed coordination problems we dig
we were early adopter of Cassandra which
is now pretty stable but at the time we
adopted it had a lot of flaws it was
just very early we had a lot of flaws
for adopting a technology that early for
our production data this mizuki / just
does the coordination really well it's
one of the one of the thousands of
implementations of packs that
a solid robust just works so one of the
other things we did is excited to use
HTTP and JSON schema um other you might
say why don't you use a bro avro is very
similar to HTTP and JSON schema and
basically the reason we did is that we
are a Django shop we mostly use Django
tasty pie and it just was easiest for us
to keep using technologies we're
familiar with so the input and the
output services were actually
implemented in Django using G of n and
unicorn for some scalability there other
thing is JSON schema gives you some
really nice flexibility you can
dynamically load schemas off of a web
server you just put the URI in there and
that uses that schema to validate and
also you can have things like this where
you have a minimum or a maximum and you
don't usually get something like that we
can valley ate at that level with a
schema for free so you know how our
lives improves and I would say yes our
lives have improved in a lot of really
important ways so the first thing we've
done is you've been able to bring up a
ton of specialized databases these have
this one monolithic my sequel database
that we couldn't change had a lot of
flaws um but now we have a bunch of
smaller databases that have a about
three minutes got a that just serve a
specific purpose and the team that needs
it creates it the team that needs it
maintains it so instead of having to go
talk to a team across the country with a
three or four-hour time zone difference
just get the data out of the centralized
store bring up your new server subscribe
to the streams and you're done also it
means that we've been able to move all
of our data into a single data warehouse
pretty easily we've moved it all to
redshift on amazon and this flexibility
of decoupling and the consumption from
the publishing meant that we didn't have
to go talk to every team get them to
push their data in just kind of worked
out of the back it's really not how that
phrase guys run through it finally
paradigm flexibility is that now we have
the ability to kind of try a bunch of
different approaches for different
problems so our data is on s3 going to
map reducers against it also from s3 we
can push in
redshift but we you know people want to
try new technologies we want to use
neo4j to see if that works well if we
could find a better way to do kind of
replication and stability there and this
gives us the flexibility to bring up new
databases actually load test them test
the performance without committing
months or longer I've seen years going
to migration projects that don't launch
so it's been it's been a pretty big
improvement over our lives yep and
things we've learned Kafka bindings in
Python are not good don't use them don't
plan too far ahead we've radically
simplified from what we started out with
that that diagram diagram needs to be a
lot more complicated we've
systematically cut more and more
components out get up
anyway we'll just run with that one and
then the final one is that we initially
wrote everything in Python in unicorn
and that worked really well up to a
certain point but as we went from trying
to do you know thousands of messages to
tens of thousands we just had a
scalability wall that was related a
little bit to python and unicorn but a
lot to deal with that just the bindings
for Kafka and Python we're very immature
and had a lot of challenge getting g
event hooked all the way through we had
a bunch of java developers on staff and
we actually just switched over to java
entirely for the entire stack and that
entire stack is a simple service that
accepts documents invalidates them in a
second service that kind of is a layer
over to Kafka so it actually took us a
couple of months to do that on to get up
the performance level but actually just
validate the concept took a couple of
days so I think one of the nice things
about using HTTP as an interface and
drop out change languages use the right
tool if the right thing dev really
wanted he's Erlang which on I love
Erlang but I am don't let that much so
now we will jump into questions ah so
you mentioned dumping data that doesn't
meet the validation phrased during input
isn't that a bit extreme so it's an
interesting question we went thought
about this a lot so the alternative to
dumping data that doesn't validate well
is that you push it into a good
invalidate queue and then you have
another process which you can see the
errors as that Q gets large and they can
rerun kind of a corrupted code or a
migration code on it its own it's
interesting I think what I felt and what
the team felt was that by pushing things
into that cue you'd have you wouldn't
fail loudly or dunvale quickly
ultimately the person needs to fix the
problems the person publishing the bad
data and not necessarily team
maintaining like the shared piece of
infrastructure we felt this was a very
violent but clear way to indicate that
something was wrong to that person
immediately I think he can go either way
and it just kind of depends on the
the different dynamics you're looking
for and there's social dynamics if you
have someone who has to then go talk to
the developers were pushing bad data in
and so on it's a good question so you
mentioned that would be the same
question how did you feel about the
level of DevOps required to go from your
starting architecture to the last
architecture oh it's a great question so
we do all of our work in chef I wrote
the first version of the chef so I'm
pretty I'm pretty much and happy to jump
into the DevOps stuff it wasn't too bad
we just spun up new nodes that the real
pain that we had was configuring
zookeeper and Kafka to be fault tolerant
and testing whether it was actually
fault tolerant we we ran in there there
are some dragons there as they say um
it's not perfect but it was pretty
reasonable I own we do have a very
strong tech ops team and we were able to
rewrite our infrastructure from the
start and chef so it's all managed we
ought all Jenkins builds it's pretty
it's pretty standard but it's pretty
clean so for us it wasn't too bad but it
home I wouldn't want to do it and a
bunch of other stuff the same time so
yeah I think there's an so does all data
from the local data stores get pushed
into the input channel so I think there
was a long discussion mostly in my head
about what is data and what is kind of
just stuff you put in a database um and
I think they're different things so
basically what we decided is data is
stuff that's actually meaningful and
usually coming from a third party unless
it's convenient for it to not come from
a third party I would say this is kind
of a problem that we're still working
out
so you spoke of publishers subscriber
offsets yeah so I can try to explain how
Kafka uses offsets but basically you
have a log in in the log I'll talk about
08 07 slightly different but not an
interesting ways so you have a log and
have a bunch of messages in it and the
first matches message is one the second
message is two three four five for every
consumer you store which is the last
message they have successfully received
and then as they get more messages you
move you increase that counter you'd say
give me 50 messages and we'll give you
50 if they're available then you go from
five to fifty five and that's how that
counter kind of decouples the publishing
and subscription does that seem like
that answers that question I hope that
answers that question but feel free to
ask afterwards I love to talk about it
more the last question oh yeah so if
you're a django shop did you have any
issues with South migrations so there's
a really exciting Lee helpful message in
South which is basically you're using my
sequel go away Oh which
it's it's not that funny um yes it on we
have had a lot of issues with south it's
better than everything else we've used
but it's still pretty painful so love to
answer any other questions if anyone has
them thanks for letting me spend your
time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>