<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Chaos Architecture | Coder Coacher - Coaching Coders</title><meta content="Chaos Architecture - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Chaos Architecture</b></h2><h5 class="post__date">2018-05-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LCDgnLI-Ll0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is actually an interesting
anniversary event so exactly seven years
ago in 2010 the very first presentation
of the Netflix architecture was at KU
con San Francisco and it was right here
and in the tour in this building but
Randy Shoop was doing the architectures
you've always wanted to know about track
and he'd been to cue come the year
before it's my first time to come to Q
Khan and I came and I explained what we
were doing and pretty much baffled
everyone in the room I think so there
was people thought we were pretty crazy
at that point but it was this is you
know this is a great conference we
really like the way Q Khan is run and
it's always been one of my favorite
events to go to so I'm gonna run through
a whole bunch of stuff and I'm gonna
lead into chaos architecture but first
of all I'm going to talk a bit about
what's going on in kind of the evolution
of business logic as we've gone from
monoliths to micro services and then
talked a bit about the cloud native
principles since I got 50 minutes like
also to do some intro and then about
halfway through I'll switch over start
talking about how we can think about
applying chaos architecture chaos
engineering to these cloud native
architectures they call that Cal's
architecture because architecture is
cool right so so business logic started
off pretty monolithic and then we came
up with these nice micro services and
now we're transitioning to functions and
so why did that happen so the thing is
that if you go about ten years ago
monoliths were the thing that was what
we built and we built it from good
reasons because the network's we had
were slow and the inter process of
communication we were using was mostly
XML and a bit of soap and all those
nasty things some people are probably
flinching or twitching hearing about
this stuff it was big and heavy and slow
and if you try to divide your
application into lots of pieces you
divide it into two and they were pretty
much stuck because by that point it took
too long to respond so the problem was
that it was too inefficient to break
everything into real chunks so we had
service-oriented architecture but the
services
ended up as big monolithic bundled up
services that did too much stuff if we
try to break it into the individual like
single function level components it
would run too slowly so there were a few
attempts to do Tru SOA most of those
just like died because they couldn't
keep up so then about five years ago we
we sped things up a bit we have ten
gigabit networks instead of one that
helps
we have JSON rest we're actually able to
break our application into chunks which
are kind of do one thing at a time right
you get your boundary context now as a
micro service that does one thing it's
got a few upstream consumers a few
downstream dependencies and you really
want to just have a single thing that it
tries to do right and that means they
more testable they're easier to reason
about the reason here when they go wrong
is much easier to figure out what went
wrong because this one service that does
one thing is behaving strangely so
that's that is part of the whole sort of
migration here and then we get to this
is actually a simplified model of one of
their sort of but the time we go to
Netflix we were building things like
this is this is a service the home page
now it's kind of yeah there's an API
call at the end or something and there's
a whole bunch of stuff going on and the
things on the right-hand side of the
storage based services and the stuff in
the middle is all the different stages
in between so we're building these micro
service systems and then we started
noticing that a lot of these services
that we were building like we're like
one corner of the monolithic application
was a library that did some queuing
thing and now we've got we built a micro
service that just does queuing at night
well we don't need to build that service
we can just run it as a thing so we
start migrating to have standardized
services so a lot of the off-the-shelf
pieces turn into standard services so
you start using sqs because it's a queue
and I don't have to implement it you
start using Kinesis because they don't
want to have to run Kafka and scale it
and figure out how to do that I can just
call kinases and all these other
different things so you end up building
a fairly large chunk of your application
that used to be been bedded in this
monolith and then got broken out has now
become whittled away so what's left in
the middle is the business logic which
is the thing that
surely you were trying to build in the
first place so there's less work because
there's less stuff to write but you've
got more dependencies around the edge
okay now these business logic we started
to break into individual functions and
this is kind of where we get to the
server list model because now instead of
having the same function the same
functions built into each micro-service
we just implement it once and you just
pass events back and forth between them
so this is starting to come along but
the way I've drawn this they're all sort
of grayed out right because they're not
really running and the interesting thing
is that when something comes in it wakes
up these services and you know you only
wake up for the things you need and then
they go away again
right so this is pretty high level I
know most of you know how surplus works
but like this I give this kind I give
this talk to sort of managers at big
companies so that they try to understand
what services but the real point here is
that when systems idle it shuts down
across nothing to run and that's why
we're tending to see huge cost savings
with the service architecture and
there's much less code to write so it's
actually faster to write and you're
using you're gluing together building
blocks that are already scaled and
reliable you know sqs or something so
you're you're writing less and it tends
to work and scale more easily and cost
less to run so there's this double win
here that it's faster to write and
cheaper to run and we see that's one of
the reasons I think server list is
particularly interesting so it's
different cases if you hit a surplus
function enough times that you kind of
want to run it for permanently rather
than invoking it every time so let's
look at cloud native architectures and
I'll talk a bit about the principles
here so what do we mean by cloud native
well let's think about data center
native you have a data center and every
now and again you you send another rack
of systems into it and the system sit
there for years and yeah but what you
really wanted to do was shift it out of
there and put it up in the cloud so
there we go I have a I have a company
that this is the best thing about my new
job I have a company that makes slides
for me and I said make a forklift truck
go into the cloud and they came up with
this
my graphic art skills are woefully
inadequate as you can see by looking any
of my older decks but yeah this is cool
so the point here is that instead of
paying upfront and depreciating over
three years you're paying a month later
for the number of seconds you use
because know if you notice a month or so
ago we stopped charging by the hour we
know we charge by the second you have to
pay for the first minute after that it's
per second so we've now got really fine
grain for instances lambda functions are
per 10 milliseconds so it's you're
really consuming just what you need you
just have to get good at turning stuff
off so the point here this is the first
principle pay four years last month
instead of what you guess you're going
to use next year all right so that's the
first thing let's think about how we get
there if old world you file tickets um
wait but what you really want to do is
make it self-service on demand just make
a call and then you know sometimes
somebody's out having a coffee or at
lunch and nothing happens to your ticket
but you can just keep calling the
service everything happened so what we
really wanted to have here is deploy and
do things by filing a ticket you don't
want to do that you want to make an API
call self-service just get stuff and
that is the other kind of benefit of
this cloud native architectures right
that they're demand driven self-service
automated instead of moving from lots
and lots of tickets you want to move to
having a tracking ticket that records
that you did something but you're not
asking for permission the ticket isn't
asking other people to do things so your
ticketing system turns into like one
ticket for a deploy which records the
stages that that deploy went through as
it happened right so you've got your
audit trail but you're not you know
maybe if you're deploying a new finance
system there's a some manager has to
sign it off to say yeah I approve this
update but it's generally you can get
down to a single approval for a deploy
and that's sort of the I guess the
current state of the art now if you're
trying to get there and you're going to
sounds like an alien from another planet
and we have this nice waterfall that
takes a year what I recommend people do
is measure the number of tickets and
meetings per deploy right and write that
down and make a little graph of it and
then make tell people
this is a bad thing and make the numbers
go down right so that's a sort of a
strategy for exposing the fact that
there are so many meetings and tickets
and you want to have less and you can
drive it down over time so that's
another principle so here's another one
we've got all this infrastructure
scattered all the way around the world
and if you want to deploy something in
Latin America or or Japan or or you know
Germany you just it's just a different
drop-down on the menu and you just go
and do it and that's cloud native
whereas if you want to build a data
center in Brazil anyone to serve a
shipped hardware to Brazil knows how
hard it is to do that I mean some
countries that they're more difficult
know this but whatever it is you've got
to hire people and find a building and
takes a long time but it's a drop-down
on a menu it caught it's no difficulty
it's no harder to deploy in another
country than it is to deploy in your own
country so this is another principle
that you get to build instant globally
distributed applications by default if
you want to do some long latency testing
here set up a machine in Oregon and a
machine in Dublin or Frankfurt and run
the crest it's no harder than setting up
two machines in the same region right so
this is this is a another new principle
all right so if we look at an individual
couple of regions and the sort of
typical data center architecture gonna
get bit topical here let's have a
hurricane Hey Hurricane sandy comes in
floods New York in lots of hurricanes
this year
so so you failover to Chicago or
something but the way we build it in the
cloud is we have regions with zones in
them the zones are between ten and a
hundred kilometers apart so they're not
in the same flood plain they're not in
the same blast at you know fire area
there's they're far enough apart but
they're close enough together for
synchronous replication that's why the
zones are closer to our are no more than
100 kilometers apart right so you see we
write stuff you want it to go to all
three zones you don't have to wait but
across regions there's more latency it's
typically asynchronous right so if you
do a data center migration to cloud and
let's say you have a my sequel primary
and secondary and you just copy the
image that you had in the data set
to the cloud you're only using one zone
you haven't really got any additional
resiliency here I wouldn't say that's a
cloud native thing it's like a forklift
what you really want to do if you're
moving from a data center is replicate
the data across like a replicate copies
of your application if it's horizontally
scalable replicate the data across three
zones and then you've got a much more
resilient system and you know if there's
another hurricane comes in or something
and it might might affect one zone but
it's very unlikely to accept to to take
out all the zones in a region so it's
more inherently resilient so another
principle here is you should distribute
over zones in a region by default
alright let's talk about elasticity it's
pretty hard to get above 10% utilization
that on somebody earlier today said yeah
ours is like 8%
right that's that's their average data
center utilization across all time and
in the cloud I think you should be
targeting you could argue the numbers
but I think probably 40% is plausible
you know you kind of want to average you
know target 50% an overall average maybe
you'll get 40% right but it's many times
higher and you don't run out of capacity
because you just scale up more right so
the effect here would be if you had a
thousand machines in the cloud on
average right and your average capacity
in the clouds a thousand machines you'd
need 4,000 machines in the data center
to do the same workload that's the four
to one cuz you're running these machines
40 percent busy these machines 10
percent busy that's the same capacity
right except the cloud you could if you
need 10,000 machines you can go get them
so you want to order scale predictable
heavy workloads because you're going up
and down but for the lighter workloads
that go all the way to 0-2 so if you're
looking at I want to deploy across zones
by default that means I need at least
three machines of every type in fact you
know the pattern of Netflix we said we
want at least two machines in each zone
so we went a minimum deployment with six
machines and you could get fairly small
machines but it's all six machines and
if you have something that doesn't need
six machines you should use a server
list and just go down to have you know
plan to just create the machines when
you need them and it's a lot cheaper
okay so here's another principle turned
it off when it's idle many
times higher utilization huge cost
savings and avoids capacity overloads
okay all right so quickly the delivery
the way deliveries are done in the cloud
developer build something you want your
time to value to be really short movie a
day I'd say state-of-the-art right now
if you write some code you develop a
function you say okay this is a good
change you hit save you you save it into
your build system it should go build it
it should deploy it it should put it in
a canary test to see if it looks good
and by the time you come into work the
next morning it should be deployed in at
least one region and maybe over the next
some period of time it deploys globally
but that's kind of I'd say state of the
art for time to value is I mean I wrote
a line of code how soon did a customer
get some value for that code that's a
day right most people some people this
is years right but I mean this I think a
day is good it's probably you know you
could go faster but that's I think a
reasonable thing to do well the point
here is that you're keeping old versions
of things and that you don't replace the
previous version of the system like this
is the old way of doing it works so hard
to get a you know get VMware to give you
somebody to push buttons on VMware to
give you a new VM it took you months to
get once you've got it you would update
it you'd use chef or puppet or something
to keep updating it so you do replace in
place and when you were doing things
whereas here is like I can get a new
machine in a minute so I'll just get
another machine alongside the old
machine and run side by side so now you
can run multiple versions at once and
you just need to be able to route the
traffic between them so here's the other
principle immutable code automated
builds ephemeral instances containers
and functions Bluegreen deployments and
versioning your services okay so that's
kind of what I mean by cloud native and
this is the full set of principles just
to revise them right so paying as we go
self-service globally distributed cross
zones and regions high utilization and
immutable code deployments may be other
things but if you're doing most of these
I think you've got a cloud native
application and if you're if you're not
you're probably just forklifting some
data center stuff all right so I'm gonna
move on now and talk a bit about chaos
architecture and I like to describe this
as four layers to team
an attitude so I'm going to go through
and explain what these layers are kind
of what the teams do and what the
attitude is first of all infrastructure
and services you basically want to set
it up with no single point of failure
and you've got all of these zones around
the world and regions and whatever it's
easy to do but what it really means is
you get you get this region and you
deploy multiple things and if you zoom
in on that you see everything's
interconnected and you've got multiple
versions of it and you zoom in to one of
those and there's more versions and
you've got more reading zones so you're
you're using lots and lots of
replications at multiple layers as you
go in but the key point here is you're
just trying to get to have no single
point of failure and what I really mean
is there's no no single point means it
has to be distributed right if there's
no single point that means you're
building a distributed system and if
there's no single failure it's got to be
a replicated system so we're building
distributed replicated systems and we
want to automate them and you know cloud
is the way that you build that
automation so what we're talking about
here are systems are distributed
replicated automated cloud and that is
how you can go and deploy something that
is going to give you the infrastructure
reliability that you need all right so
that's infrastructure so there above
this is a lot more interesting code
switching interconnect level because if
you've got data in more than one place
you have to figure out how do you get
the data there how do you keep it in
sync if there's some data replication
going on there's routing between the way
you get customers or traffic has to be
routed to more than one place because if
the place goes away you've got a route
around it so let's just sort of show a
little diagram here so this thing went
away and those customers get routed
somewhere else okay so that's traffic
routing avoiding an issue but then you
know I've heard I was I was telling
someone earlier in in the previous
session we had a years and years ago I
was at eBay and we had a data center
outage and it was a big scramble to get
it to failover it took about a day or so
to get everything back up and then it
took a month to fail back off afterwards
because we didn't know what we were
everything after the failover was such a
mess that we couldn't actually figure
out how to get it back safely
it wasn't a simple switch there and
switch back so this is you know if you
fix it you've got to go put all the
customers back in and then you go to
this anti entropy recovery you've got
your data is now out of sync so you have
to resynchronize it and that's typical
Danty entropy algorithm that's so I'm
gonna ask you a question here who here
has a backup data center the backup user
okay what's the best description of it
is availability theater you've never
ever failed over to it I get some good
embarrassin it's there if you're trying
to sell chaos engineering to you to your
management chain see if you can get the
CIO to CEO to ask the CIO if they've
ever failed over to their data center
their backup right that's one of those
embarrassing questions that will cause
them to go fund a chaos engineering team
that's a theory anyway try it or
infrequent partial testing your app by
app you do do a failover to the other
you know data center you know if you do
that you can kind of get you know there
are some compliance rules and industry
regulators that's kind of the minimum to
pass audit if you're if you're a bank or
something like that right if you do
individual failed overs but or regular
tests during maintenance window so at
the weekend when you're not doing
anything you actually do it and can tie
a failover to prove that you don't have
to do it you exercise people do that
some people do that that's kind of a
good practice or do do frequent fail
overs in production to prove that and
nobody can tell you're doing it right
and you know we were hearing from Dave
harness sort of every week or two
Netflix actually shuts down an entire
region and moves all the traffic
somewhere else it takes them less than
ten minutes and nobody notices it's
happening and if and if it fails they do
it more often right this is this is that
continuous delivery principle that jazz
humble came out with if it hurts do it
more often if you heard that so the
point here is if you keep doing this
frequently enough it will stop hurting a
norm and you just end up with if there's
some issue in any part of your system
you just switch away from it and
everything
it's fine so here's the problem you want
to route
updates and customer requests to
specific regions and services you want
to replicate data and reroute requests
during incidents and the switching
mechanism this is the hopper needs to be
more reliable than the things you're
switching between this is one of these
principles of highly available systems
if you put a switch between two things
the failure rate of the switch has to be
vastly better than the failure rate of
the things otherwise you're just making
it worse right
the combined reliability of the whole
thing is dominated by the fact that the
switch is less reliable and the things
you're trying to switch between and what
we find is many cases there are these
you know you get some small failure in
your system and the system starts to
respond to it and then you say let's
failover and then it turns into a
massive outage because everything
collapses and all you're switching
software you find every error code path
that hasn't been tested in your system
right that's the problem so the only way
you can exercise those error code passes
by regularly running these tests and
exercising it and that's kind of you
know I keep going on about Netflix but
they really are state of the art here I
don't know many people that are doing
that frequent fail overs I heard of a
bank once that flipped every weekend so
right even weeks you're on this data
center was the primary in odd weeks this
data center was the primary and they
just flip back and forth every weekend
and if there was a problem mid week they
just did the flip and kept going now
that's I think a good state-of-the-art
way of doing this if you're in that kind
of environment okay so that's the
switching layer let's move up a layer
applications so what happens if the
application gets an error it wasn't
expecting or it gets a slow response or
the network connection drops yeah it
does it crash does it fall over does it
write the wrong data to the wrong place
you don't freak you know you see all
kinds of nasty behaviors so that's a bit
of a problem we'd like to be able to
test these things better but with
micro-services it actually gets a little
bit easier because we're monolith it's
really hard to test because it's got it
does so many things that it's got so
many conditions that it's basically
going to be difficult to test all of its
error cases but a micro service does one
thing it takes one input and it gives
one output and it has a bunch of
dependencies you can basically reason
much more about what it's going to do
you can put circuit breakers in there to
limit that that data source you can have
bulkheads prevent it spreading
there's an architecture there was a talk
and I think it was I think it might have
been the kook on London event I think it
was Q con London from Starling Bank
I think where's yo Starling bank did a
nice talk and I helped review the thing
and he had this recruit ibly long
acronym for his architecture and I
noticed that part of it was ditto so it
became this is now the ditto
architecture do idempotent things to
others so that means that all of your
requests are item potent means you can
do at least once delivery of everything
which means you can multiple the liver
and it doesn't matter and all of your
audio transactions become nice nicely
behaved events and that way you can
scale it becomes incredibly resilient
and reliable you can reason about the
system if you have something that has to
be delivered exactly once you've built a
system that isn't going to be have
trouble being available because you have
to if you're not sure if it got there
you have to kind of freeze the system to
figure out what's going on that's the
kind of consistency problem of exactly
once delivery if you can set it up to
always be able to multiple deliver and
not have a problem then you can melt you
can deal with it much better and so
really what we're trying to avoid it
avoid update and delete write those
change the state append and and write
but fine you just have just appending a
write log so you just build these logs
of what's going on in the system and the
current state of the system if I can see
all the logs I can figure it out all
right this is basically double entry
bookkeeping it and 4,000 year old
algorithm invented by Babylonians with
clay tablets and it seriously that is
it's one of the oldest algorithms and
it's because they couldn't rub out the
clay tablet so you just had to append
right so the reason that we have updates
and delete is because computers were too
small a few decades ago and you know
memory wasn't enough memory or disk
space so we optimize by updating stuff
in place whereas if you're doing
anything that matters you should just
build logs just keep logging maybe you
purge it up every now and again but but
but update is a terrible thing because
if you're caching and pointing to
something and it change you didn't know
it
changed right that's the problem we're
trying to avoid you delete by writing
tombstones over things it's a sort of
the Cassandra way of doing deletes all
right so that's the application people
we've all seen this right systems fine
it starts behaving it's slightly
strangely and you go in and go why did
it do that you start rebooting bits of
it and you if there's a knob you turn it
the wrong way if there's a button that
says do not push this button you push
the button and the people will screw up
a perfectly good system that's very
highly available and there are lots and
lots of examples of this in the real
world and so what are we going to do
about it we have to train the people and
I have a nice analogy here how many
people here have never had a fire drill
like pretty much anybody's ever worked
in an office has been through a fire
drill where you go and stand in the
parking lot for you know ten minutes
while they count everybody that got out
and make sure that it won't go down to
the building and make sure no one tried
to use the elevators and all that kind
of stuff
right we know we know we're supposed to
do this and then every now again you
really is a fire and it saves lives and
like just before the first time I gave
these slides there was a huge earthquake
in Mexico and two hours before the
earthquake there was a public earthquake
preparedness drill in Mexico City it's
like everybody in Mexico City was
trained in how to respond during an
earthquake two hours before the real one
happened and you know it's a real one
because the Earth's moving few feet side
to side underneath you right it's not
just the hey we're sending off a siren
right so but his this is the question
right who runs the fire drill for 80 if
your infrastructure and your
applications are on fire whose tests
what you're supposed to do you didn't
train everybody and how to respond to
that so this is this is where I think
that is one of the sort of prime
functions of the chaos engineering team
and I you know got a link to the the
Netflix or a third book on this subject
you've got people applications switching
and infrastructure and the cows
engineering teams job is to have build
tools which exercises all of those
layers and here's some of the tools so
there are people running game days so
that it's exercising what to do during
an outage can you find all the
information you need you know how to be
on a call you know on the con call where
we're discussing the outage there's a
set of behaviors there that work if you
don't know what you're doing you can
actually disrupt the call and there's if
an efficiently managed outage incident
call is a is a really powerful way of
getting the into the outage to be really
short and if you don't you can make it
worse the simian army there's all of the
open-source and the chaos monkeys and
things like that you're building failure
injection tools that can put things in
the right place and the cows automation
platform chap is the Netflix there's a
blog post on it which is basically
they're automating their failure
injection been they're automating the
process of choosing where to inject
failures and the operation of those
failures and if you start noticing that
the failure your test your failure test
is causing a customer visible problem
then you back out of that and then
Colton's built gremlin which is sort of
a application a productized version of
this so you can go go find a gremlin and
go and I still don't know you still
don't think you have a competitor out
there so your category creator it's a
brave place to be in a market but I
think we need more tools here there is
actually Ross miles as doing chaos that
I oh there's a there's a github account
they're starting to build some
open-source pieces that that start to do
little bits and pieces of this so they I
think this is this is the it's an
interesting time and if you want to get
involved in building stuff this is a
great opportunity to contribute
open-source tooling and concepts and
ideas it's a really emerging area but
there's another team that does something
similar and some of you have security
teams to protect your systems but the
more advanced companies have a red team
whose job is to break into your systems
to prove that the security teams doing a
good job right you heard a lot about
this from Shannon lights into it she's
if go go watch her presentations on this
they yeah
red team comes in every Monday morning
it just breaks into all of the stuff it
into it because all those stupid things
developers did last week right it will
go find every buffer overflow that
introduced and every whatever and they
get into the system so they build tools
there are tools for this too
one of the neat one says what's the the
human level right kind of like game days
so save stack Ava is actually its
open-source package which just simulated
spear phishing email attacks on
companies right so it generates emails
that have links which basically go in
and let you and say hey you should click
on this link for this special offer
thing and if you click on is there's no
bad person don't click on links right so
the true so the Netflix reduced to get
these emails that were the the the
security team would send in and these to
leave USB keys lying around in the
cafeteria and if you plug them in it
again so the bad person you're not
supposed to do that supposed to if you
find USB key lying around should not
just plug it into a random computer so
you have to condition the people right
this is the Alissa's part of it and
you're one of the weak links in the
system and there's a bunch of other
tools here and a few companies doing
stuff so these are the four layers the
two teams and the attitude here is
you're trying to break it to make it
better right so let's let's think about
risk tolerance so who is at risk for
what and what kind of risks are you
trying to guard against and you can kind
of choose a little bit here where the
downtime is a bigger risk than getting
the wrong answer right do you want
consistency and security which means
that you have to be able to stop if you
can't guarantee this is the kind of the
cap theorem right basically if you can't
guarantee the state of something the
only safe thing you can do is just stop
the system and that's down time and that
might be the right thing to do if your
mistake could cause you know a trillion
dollars to go to the wrong place or
something right you should probably just
stop and make sure that the system comes
into a state where you can safely do the
next thing but in other cases the
biggest challenge the biggest risk is is
downtime and you can actually provide a
degraded service which is actually fine
so you want to be more permissive and
one of the things
again going back to my time at Netflix
we were one of the hardest things was to
teach people about permissive failure
and conceptually this is if it's our
fault that we can't tell whether you
should be able to let do something or
not then we should let you do it right
if we're not quite sure if you're a
customer in good standing you're trying
to watch a movie because of some failure
in our system we should just let you
watch the movie right because it's our
fault right you probably are a customer
in good standing so if I just can't tell
most of the time you are right the
probability that my down time occurs
exactly the same point or someone
fortunately trying to get into the
system is vanishingly small so so you
want to be permissive on failures most
of the time right so if you can't renew
a ticket or there's some security system
that's in there you're gonna look at
what is the actual cost of giving away
the thing that you're currently trying
to protect right so that that's the
concept here it means you can get much
more available systems so a really
powerful technique for coping with
partial outages of various parts of the
system another thing is is this incident
lifecycle you know something's gone
wrong what are you gonna do you want to
try and mitigate what's going wrong so
it doesn't spread you want to restore
the system into some basic bit into some
working state and you want to adapt the
system so that it doesn't do it again
right that's basically the cycle and
that's kind of the anti fragile feedback
loop if you read the book anti fragile
by Nicholas Taleb he has lots of good
examples of like working out you go to
the gym to work out if you work out too
hard you'll actually break something and
end up in hospital and not be able to
walk the next day that's too hard right
if you don't go to the gym ever and then
you have to run for a cat to catch a
plane or something and you have a heart
attack that's probably a bad thing too
so there's a level of working out which
is good level of exercise most people
have too little exercise right I know I
do but the running for planes is
probably my main exercise at this point
in my life but you want enough exercise
that you get slightly stronger each time
you do it that's the right level of
exercise for an anti fragile system and
that's what
we're trying to do with cows
Engineering's introduced enough that we
don't break the system so we're trying
to break it to make it safer and this of
this a bunch of research and things on
here and this is actually like because I
said break it to make it better but
really we're trying to make it safer and
safety is there's a whole lot of stuff
going on in safety and this this whole
area about the new view of safety this
is about industrial safety this is
people operating nuclear power plants
and heavy machinery and industrial
processes and what do you do when people
die or get hurt or something goes wrong
and Todd Conklin is a you know one of
the leaders in this space he does
training classes but he has a really
good podcast called the pre accident
podcast and if you want to know more
about this area go listen to this
podcast add it to your podcast list
really interesting he's he used to be
the safety guy at los alamos live in in
New Mexico and he's still based down
there so he knows all about stuff like
that that big nuclear issues so that's
that's interesting I actually went on
that podcast once and did a whole did a
talk about two years ago about the chaos
monkey and that whole idea and it was an
you know he was quite interested in it
so it's not just industrial every now
and again he has software people on so
John wall's Paul's been on there that
journals were introduced him to me and
John Osborne has left Etsy and he is now
doing a whole bunch of work in this
space and there's a thing called the
Stella report and it's URL is Stella dot
report because that's and that is
actually the URL okay
and it's it's a whole discussion about
how to build safer systems and how to
deal with the the failure modes and
reporting and things in this space and
and really the the core ideas I got from
this was from reading drift into failure
by Sydney Dekker and that's a really
good book to read unless you're on a
plane or you have a loved one in
hospital if most of the examples are
extremely disturbing if I had to put it
down one sending a red rat on a plane
because it's just full of plane crashes
and people dying for in hospital
but anyway but it's one of the
inspirational books and Sydney Decker is
one of the inspirational figures behind
this whole area so I think this is this
is a new emerging area but this is this
is the this is the core concept a
failure doesn't have a root cause in a
component failure or a human error right
if a component or a human error causes a
system failure
it's a failure in the system it's not a
failure of that human like humans do bad
things all the time and systems don't
fail just because this one time you did
that bad thing it actually tipped it
over the edge you shouldn't go and
notify that person or or you know or
replace that company would you know deal
or replace that component it's because
there wasn't enough margin so the point
here is you're trying to build systems
that have a safety margin which is
capable of absorbing human error and
absorbing failure and so the question
then is how much margin do you have and
here's a little example if you're a
blindfold on us and if I was playing for
all that up here right on the edge of a
cliff I would be going like okay I think
the edge of the stage is here okay I can
feel it all right that's where the edges
I can back away from that a bit right so
I've just established a safety margin of
about four feet all right in that
dimension I could go that way and feel
my way over there and discover this okay
I have a fairly reasonable amount of
margin here right that's what we're
talking about and then I can fall over I
can still take a step this way and I
know that I can take a step this way
right because I've established I have
imagined to do that so that's that's the
concept we're trying to get here and I
was trying to come up with an analogy
for it hopefully that works so this is
the hypothesis that we're going we think
we have a safety margin a certain
dimension like we could take you know
50% more traffic and we'd be fine or we
could this service could fail or you
know this thing could go wrong and we'd
still be up that's the safety margin so
we think that so then we carefully test
it by like pumping you know moving
traffic from somewhere else or
overloading something or pushing pushing
something up closer to that limit and if
you notice things actually falling over
like I find the edge of the unexpectedly
find the edge of the cliff earlier than
I
expecting I like stop quickly and back
out right that's what we're trying to do
we don't want to cause an issue so then
this came up in the in the panel session
earlier and so how do you select a test
there's so many places you could inject
failures what are these dimensions and
where should you be trying them and
lineage driven fault injection was a
talk one year ago a cue come here there
was a talk about it by Colton and Peter
Alvaro it's a really fascinating idea
which is that you track effectively you
trace from your edge you take your
business transactions that matter say
your Netflix case I push play or you
know or I'm signing up for our webs
signing up for a service the signup flow
you just follow the path right this is
an important business transaction what
is the lineage of that transaction
what's the dependency chain and you just
work your way through that chain
injecting faults into that chain in a
way that you're trying to make sure that
that business transaction is going to
work so you're not testing everything at
random you could kind of let's call it
fuzzing right when you just test throw
random things it's probably good to do
some random injection but this is this
lineage thing is that you're trying to
find a key piece of business traffic
that you're looking at and then you try
to inject faults into that place to make
sure it doesn't work out go wrong right
so summarize what we really want to have
then is really experienced staff who
have been on it play out and you know
maybe you don't have real outages there
are planes don't crash every day but
pilots are trained in what to do if
there is an incident right so that
training is really important so you want
your stuff to be experienced in the
processes involved in dealing with an
incident you know your applications to
be robust so you want to put them in
test environments and simulate failures
and various kinds and you want to do
that in production too you want a
dependable switching fabric meaning you
exercise all of its Hara handling
failure modes code paths right you know
what it does in all these different
circumstances it may still go wrong
because it's hard to test these but if
you aren't testing it at all you're
going to be really screwed right and
then you want a redundant service
foundation all your services and all
your capabilities you want to make sure
that you've got those distributed out
so that's what we'd like to have and
that's my clown Dave the building mole
so I've got a few minutes left for Q&amp;amp;A
that's awesome any questions
hey Agron who is that permissive model
is there a chance that you are giving
more ice to the root cause of the
permission mode sorry is there a chance
that you are giving more ice to the same
people who caused you to go to the
permissive model by breaking your system
equiping I'm sorry I didn't get so if
there is a hack attack on your site if
there's an attack on the system yes yeah
that made you just you cannot authorize
the customer Oh authorizing it okay so
now your allow people in without
authorization yeah all right okay and it
caused this yeah so have any new loop
you know if you couldn't tell whether
this is a valid customer or not and they
want to do something that's relatively
cheap for you to give them like say
what's your movie and your system is
basically down that could make that
decision you miss just let them watch
the movie because most likely they are
like a good customer right so I think
it's okay but I mean you don't want
people to go in there and sort of you
know move a billion dollars in in some
other account right so there there are
cases where it doesn't where you want to
be put the hard control in there you
want to just stop the system right yeah
but I think in most cases people tend to
err on the side of blocking if they
can't be sure
whereas they should actually think
through could I be more permissive in
this failure you want to log that you
did it then maybe you'll do some some
fraud analysis later to see if you were
exposed but there's a good paper by Pat
Helen called memory guesses and
apologies and this is the apology phase
right and it's like I couldn't figure
out what to do I'm going to punt you to
punt you to a customer service or just
like say sorry or just try and deal with
it okay another another question
you went out there
thank you for the talk I just wonder can
you clarify what we mean by turn off the
service for saving utility fee oh is
that what you meant yeah for hardware
like servers yeah well I'm yeah and what
I mean by turn off I mean you could save
power in a data center by powering them
down but that's what I meant was on a
cloud you basically it's cloud native
right so what I meant was just stop
using them you know D allocate the
resource you use you know I need you
order scale down right so I have a
hundred machines and they're worth load
the load traffic hogs I run it on 50
machines so that there I keep it keep
all the machines busy by shutting them
down and if you've got a QA environment
you know how you know this 168 hours in
a week
hopefully you're not working more than
about 40 or 50 of them so you should be
able to shut down your entire QA
environment for 2/3 to 3/4 of the week
because there shouldn't be anyone using
it right so that gives you back a higher
utilization over time right yeah well it
saves you safe to you bill I mean if
you're on a cloud you're paying by the
by the second so right you just stop
paying rent for things you're not using
that was the point yeah a question I
want it back there
hey thanks so when your graphs you had a
lot of different connections between all
of the nodes how do you inject in
between those nodes do you have to
always have middleware in between all
the different services such that you can
throttle that's a good question I think
number of different ways of doing that
the Netflix way of doing it was to build
a set of libraries basically or most of
the Java based services at Netflix were
built from a template you just started
with this you know base server that you
copied and then you added your own
business logic to it and it had a
standard HTTP request handler which had
additional code that could inject
failures into it and then the outbound
side where it made requests again there
were hooks for injecting failures into
it so there's part of just now you
injected your business Lodging to the
middle of this framework which was a web
server it was a tomcat server something
like that so that's that's one way of
doing if you use the spring cloud
framework in Java you can pull all of
those all those services in ribbon is
the main one that Netflix has a newer
way of doing this is to use a service
mesh the ISTE or service mesh which is
now part of the cloud native computing
foundation uses envoi envoy which is a
that's the actual service mesh point
that comes from a be and being I think
no lift lift don't go over to mix yeah
lifts and so what that mesh gives you a
point where you can get in there and
configure it to inject failures and
things like that and do it that's a so I
think this is a property of the service
mesh so this mesh can be an additional
process which is running alongside your
server where you're just sort of talking
to localhost to get to it or it can be
embedded in your application if you're
using a library that supports it I think
with Java you can do some here because
it's bytecode you can do some injection
of code to do things so I think things
like app dynamics where you can inject
the monitoring into an application that
wasn't instrumented but or you could
just hard hard instrument stuff so the
number of ways of doing it I think those
are the common ways I've seen yes you
can do it the network layer as well if
you're going through you know on you
at the packet level as well it's a bit
harder to coordinate that time for one
more
yep anyone else this is the precarious
run down the side so W is it self is he
doing any CalSTRS - yeah AWS well Amazon
has a Chaos engineering team AWS does a
whole lot of different testing but you
know we do have the basic principles in
there don't we don't we don't have a
centralized team because each service
team manages itself but we do a fair
amount of testing and yeah the reason I
ask this is that like what's appearing
to me is that most of the time of like
doing effective chaos engineering is a
statement on complexity architecture
like how well you can understand and
manage complexity which is I don't think
it's just contained in application layer
like it so it's up and down the chain
yeah I guess it's often so much more
philosophical question but I think the
the point here is that if most systems
are right now is so fragile if you poke
them at all they fall over I think
that's kind of an awful case so so if we
just get incrementally better bike
training the people in how to deal with
is the starting point right if you just
just exercise people so they know what
to do during an outage start there and
then start getting individual
applications if you've got to micro
services you've now got a boundary
context that does one thing you can
start reasoning about it you can say
what happens if that one thing does
something wrong you can cause it to you
can inject failures into that one
service see what happens you just work
through your system so this is that's
not there are still systemic problems
that will occur right if you've got your
timeout set wrong you'll get you'll get
congestion collapse you'll get
thundering herd's you'll get all kinds
of other larger scale problems and you
need to test for those as well but yeah
I did a talk at craft conference in a
couple of years ago where I talked about
some of the issues like timeouts and
stuff like that so by the way if you
want more my slides on micro
services they're on my github account
Adrienne Khosla don't get hub there's a
slides thing and a big big file there
few hundred slides worth of stuff all
the things like up to about a year ago
when I joined Amazon that whole Dexter
one last question
is there a point when chaos engineering
could turn into a DDoS attack towards a
whole system like how when does that
occur how do you how do you make that
balance how do you make that decision
that we should not go further you have
to well you if you're doing it right you
should be monitoring the system so that
if it starts to look bad you back out
and hopefully by backing out it stops
but you can tip a system over there's
hysteresis
there's classic cases where systems
aren't restartable right if you actually
take your system down and back up again
it doesn't come up again that's a common
property of systems that have grown over
time like it 'no system only works if
the caches are warm for example so you
have to if you kill your caching layer
then you can't re warm it and then
you're down right so you have to be
pretty careful about some of these
properties but it's good to know that
because then when it happened when you
do it during the day when everyone's
there and watching rather than you know
you know 3:00 a.m. on a Sunday or
something one of the you know like a one
more Netflix thing which is that the
most annoying thing about Netflix was
that it would tend to break at 7 p.m. on
a Sunday night because that's when
everybody wants to be at home watching
TV with their kids is that's the problem
that you're sitting home watching TV and
then it goes down because that was the
peak traffic every week and you know
when you've got a week on week growth
rate where every week is an all-time
record like having the peak be enough
time is a pain so you want to create
enough chaos during the week on a
Wednesday when everyone's at work and
everyone's fresh that that's the model
where that's when you want to be
exercising the system you don't to be
doing stuff out of hours and having a
bunch of people would sleep-deprived
trying to make important decisions it's
always a bad recipe for these things I
think the thing I'd add there as well is
one of the principles of chaos is
containing the blast radius and so one
of the ways you prevent ddossing
yourself as you start small what's the
smallest thing that can teach you
something and then as your trust and
confidence in the system grows you
grow it so eventually you're running a
production scale but you're not starting
there but if you poke one back-end
database and you've got your timeout set
wrong it could ripple all the way
through and take out everything so
that's a it's gonna happen one day you
should do it do it live do it when
you're ready for it okay I think we're
out of time so thank you thank you
everybody thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>