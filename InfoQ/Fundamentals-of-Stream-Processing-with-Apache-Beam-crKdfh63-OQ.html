<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fundamentals of Stream Processing with Apache Beam | Coder Coacher - Coaching Coders</title><meta content="Fundamentals of Stream Processing with Apache Beam - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fundamentals of Stream Processing with Apache Beam</b></h2><h5 class="post__date">2017-09-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/crKdfh63-OQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so good morning my name is Francis Perry
and I'm Tyler Aikido and we are both
engineers at Google and also committers
on the Apache beam project and we're
here today to talk to you about string
processing and more specifically this
unified model for both stream processing
and batch processing but this unified
model within apache beam that we've
developed over a number of years that we
think really makes it relatively easy to
think about stream processing and you
know build stream processing
applications that are robust and you
know able to give you the kinds of
results that you want all right so we're
gonna start today by talking about data
shapes what kind of data are we
interested in processing after that
we're gonna move on to the second
section where we're gonna look at the
four questions that underlie this model
what where when and how then we want to
explain to you why those are the right
questions for asking these solving these
types of problems and how they make it
very intuitive to reason about a very
complex set of use cases and then at the
end we're gonna wrap it all up by taking
a little more concrete and looking
specifically at what this incubating
Apache bean project is all right so
let's get started with data shapes so
throughout this talk we're going to be
using a running example and in this
example we're a mobile gaming company
and we've just launched a new mobile
game for users across the world so we've
got users all over the globe and they're
frantically doing some sort of
mind-numbing task on their mobile phone
candy killing works better what our kids
are doing these days
yeah we're not kids every time they do
this they're earning points for their
team and we'd like to analyze those logs
and figure out what's going on how our
games being used and so as we do this
you know we're generating some user
events some data so each each of the the
colored squares in here represents um
some score for a team or something like
that and as we start out our you know
our app is new so our data is relatively
small and and it looks something like
this maybe now our game gets a little
more popular so our data set gets bigger
and then it gets really really popular
and our data starts to get so big that
we have to start introducing some sort
of structure to it to keep it manageable
so you know maybe we start siloing it
into these daily groups of data so that
we can we can deal with the fact that
you know our popularity has massively
scaled up our data size but really this
kind of organized from
structure is just a cheap way to
represent an infinite stream of data
right once our game launches and users
start addictively playing our awesome
game they're never gonna stop and those
events will just keep streaming into our
system for processing
but given that we've got this massive
scale data we're doing this all on top
of distributed systems and as you know
distributed systems being distributed
can introduce them ambiguities so let's
look at some points that were scored
right about eight o'clock in the morning
to understand what distributed systems
are gonna cause here so this red score
was happened right around 8:00 a.m. and
arrived in our system for processing
almost immediately and the second one
here this yellow one also happened at 8
a.m.
arrived a little bit later of 8 28 30
something like that maybe there was
network congestion who know some some
some blip in the distributed systems
caused it to show up later and over here
we've got this green score this happened
at 8 a.m. the user crushed the candy or
whatever they were doing but didn't make
it our system till closer to 2:30 in the
afternoon for processing so perhaps this
was a score where a user was online or
playing our game on their phone and c-26
be on a transatlantic flight with no
elbow room right in airplane mode it
wasn't until that plane landed and that
user reconnected to a network that they
were a their phone was able to send that
event to our system for processing so
now we've got this unordered infinite
data set how do we process it so some
kinds of processing are actually pretty
simple
so element-wise processing is anything
that looks at an individual element and
can process it by itself so this would
be something like parsing translating
filtering any of those types of things
but most other types of things you would
want to do involve you know some sort of
aggregation of multiple events counting
joining things like that and so you have
to be able to combine these multiple
events together whenever you're doing
this kind of aggregation you need some
way to take that infinite stream of data
and chop it up into finite chunks so
that you can actually complete your
aggregation and a bit of results the
simplest way to do this is is to just
say you know as my data arrive I'm gonna
kind of chop it up into these fixed
intervals that are just just contained
whatever data arrives in that that hour
of time or whatever it is when it shows
up and will process it there and but the
drawback of this
if you remember that element that green
score from shows up about 2:30 we're now
going to process it with other scores
from the two to three o'clock window and
that means it's being processed out of
context that depending on our algorithm
this may or may not be a problem but it
could lead to incorrect results so what
we really want to be able to do is to
process events in the context that they
actually occurred and so so that's sort
of what this diagram is trying to show
you know as inputs arrive we want to we
actually want to shuffle them into the
right space to process them within that
context when they occurred so if you
look at this red element here it arrived
in the system just after 12:30 maybe it
also happened pretty much immediately
then so we'll keep it in that 12 to 1
o'clock window the green one on the
other hand arrived around the same time
as the red one but it actually happened
much much earlier so if we just
processed it in that that 12:00 to 1:00
o'clock window when it arrived that
would have been the wrong set of you
know the wrong context for that event so
what we actually want to do is shuffle
it back into that 11 to 12 window and
process it with all the other events
that happened in that period so
conceptually this kind of reordering
makes sense but to do this in practice
we're going to have to formalize that
difference between event time and
processing time so we're gonna sit be
seeing a bunch of graphs that look kind
of like this throughout the talk the
basic format is you've got these two
axes of times we've got event time so
this is these are the times that events
actually happen and it's on the bottom
access that the x-axis in blue then
you've got processing time which is you
know the time that the system itself
observes the events when they come into
the system so when are they actually
processed and that's on the y-axis in
green now if distributed systems weren't
so gosh-darn distributed and everything
just arrived in our system immediately
we'd see everything arrive on that ideal
line where it's event time and
processing time we're indistinguishable
but reality looks a little bit more like
this this red squiggly line on
processing time this is slightly to
delay it off of an end time and what
you'll see is that delay is variable it
changes over time depending on the
behaviors of that distributed system and
we call this distance the skew so we
need to track this in order to reason
about completeness and correctness from
the system so so within the beam model
we refer to this red line as the
watermark and it's a it's a statement
that you know no event times early
this point are expected to appear in the
future now this watermark can be either
perfect of heuristic a perfect modern
mark is possible in a few rare
situations where we have perfect
knowledge of our input data so an
example of that might be if you have
sequential log files likelike Kafka that
neha was talking about you know if you
know a static set of log files ahead of
time and you know that within a given
log file you know event times are
continually increasing then you actually
have perfect knowledge of all of these
events and you can construct a perfect
water market but our more common in the
distributed system is that we have to
use a heuristic watermark the system has
to make its best guess of when all the
events for a given time have arrived and
the data is complete so this is
especially true in the case of mobile
events we talked about mobile mobile
devices go online and offline all the
time they may stay offline for large
periods of time and there's just no way
for the system to know about this and so
given that there's just no way to
provide a completely accurate prediction
about when you're gonna see all the
events for a given point in time and
what do we start dealing with the
watermark we have to deal with cases
where it's a little too slow or too fast
so if it's too slow the watermark is
basically holding up our results this is
going to introduce unnecessary latency
into the system we're gonna have to wait
a really long time because maybe there's
just one more late element coming even
though we have the bulk of our data and
we can't give you any result if we're
holding on waiting for that that flight
to land then the other problem is that
they can be too fast and when you're
using these heuristic watermarks doing
your best to estimate completeness it's
still an estimate and when that estimate
is wrong you may end up getting events
that show up late and you've got to
figure out what am I gonna do with these
am I gonna throw them away am I gonna
incorporate them somehow
so this is the environment we're in
these types of infinite out of order
data sets and we'd like to do our data
processing within this environment so we
think this can actually be pretty
intuitive if we break it down into these
four questions so the four questions are
what are you computing so this is
essentially you know what what
transformations do you have in your
interview system are you
you know summing integers you know
counting up total scores you're building
histograms or you're computing machine
learning models this is essentially the
question that you that folks have
traditionally answered using a classic
batch processing what what is it that
I'm actually calculating once you
figured that out you want to think about
how the event time is going to affect
results so does the time each event
originally occurred affect how you'd
like to create the results are you a
grenading results based on event time
fixed windows or sliding windows or
bursts of user activity
the third question independent from the
event time is when in processing time
does the system actually materialize
these results do you want exactly one
answer for you know a given window when
you think that all the input for that is
complete or do you want to see
speculative results for things build up
over time and then what do you do if
there are late data in the system do you
do you want to just throw that that
those late data away or do you want to
incorporate into the results somehow
just when as the system is processing
things do you actually create results
and then finally how do refinements
relate so if I choose to omit multiple
results over time as more data arrives
in the system and my results become more
complete do I want those different
results to build on each other or do I
want them to be independent and distinct
so now we're gonna dive into each
question in a little more detail and
we're going to do it over the course of
an example as we build a pipeline so the
first thing we have to figure out is
what we're actually computing so we
talked to briefly earlier about these
different types of transformations right
we have those element wise
transformations we're just looking at a
single element in isolation and you can
think of this very much like the map
function in Map Reduce very it's very
easy to parallelize because of that
independent computation right so you can
farm it out over as many machines as you
like so then we have aggregations these
are these are counts these are joins
things like they need any time that you
are combining multiple elements together
and this this tends to correspond more
towards the reduced portion of a
MapReduce
now there's also a set of operations
that are really just made up of other
operations so things like joins counts a
lot of these are basically just built up
with more primitive operations but it's
useful to roll them up into a composite
operation and not have to worry about
the details of how
it's actually implemented alright so now
let's actually see a code snippet for
our gaming example so the code snippets
in this talk are pseudo Java
they almost compile but we wanted them
to fit on a slide so we sort of allied
in some of the details and so what we
have here are essentially three three
different statements to begin with we're
gonna read a collection of raw log lines
from some IO source and store that in so
so in beam a P collection is essentially
you can think of a representation of a
potentially massive scale collection in
this case it's a bunch of strings so
we're we're saying we're gonna read from
from some IO source this raw set of log
strings so then what we'd like to do is
do an element-wise transformation to
take each of those log lines and parse
it into a more structured format so here
we're gonna take each log line and
create a key value pair extracting the
team and the number of points scored for
that team and then once we have these
these individual scores that have been
tallied for forgiven teams we're going
to take those and we're going to apply
this composite transformation some
integers by key and we're gonna sum
those up so this is going to give us per
team totals of scores all right so now
let's look at some sample data so we can
start seeing how this executes again
same graph as before so we've got event
time in blue on the x-axis and the green
axis showing processing time and that
ideal line is shown here where there
everything would arrive if there wasn't
a delay in the system and then all these
these circles with numbers in them are
essentially individual scores for the
team we're gonna we're going to go
through and sum these up and you'll
notice looking and looking at a couple
of these like the the three here that
we've highlighted this one this event
occurred and then was also observed by
the pipeline relatively quickly so it's
it exists relatively close to that ideal
dashed line the score of nine however is
more like seven minutes delayed right it
happened just after 12:01 arrives in the
system for processing just after 1208 so
maybe this user was playing our game in
an elevator or subway somewhere with a
temporary lack of network connectivity
and this graph isn't even large enough
to start to show the types of delays
that you would get with folks playing
the game on the transatlantic flight or
something like that
so now we're gonna start animating this
to show how processing time alright show
how things
execute so what you see here is time as
progressing as this thick white line as
the computation is going we're summing
up the scores we encounter into that
intermediate state that's tracking the
white line and remember this is still
just classic batch processing here and
so the system continues to build up the
sum until it's seen all the inputs and
once it's finally seen all the inputs
then it generates the the final output
answer which is when the rectangle turns
blue and here you'll notice we're
covering all of event time with one
result so we're not really paying
attention to when the events happened
we're just summing them up so this is
basically equivalent to traditional
batch style processing so so let's now
see what happens when we go and start
playing with the other questions so the
first thing we're gonna look at is when
doing this is the question that talks
about event time so windowing is going
to let us create independent results for
slice of finite slices of event time and
so on this slide we have three examples
of relatively common patterns of one
doing so the first one is fixed windows
this is just essentially saying we're
gonna take take time we're gonna slice
it up into these these fixed divisions
that you know ahead of time they apply
across all the keys you know hourly
windows daily windows those kinds of
things a sliding window so something
like every hour
give me the last 24 hours worth of data
so the cool thing about sliding windows
is they actually overlap so a single
element can conceptually be in multiple
overlapping windows and effect I can
contribute to multiple results and then
the last example we have here is session
sessions are really interesting for a
couple of reasons one one is that
they're based off the data themselves so
you don't know a priori what what a
session for any given user is going to
be and then the other interesting thing
is that they're unaligned
they're not the same for for each given
user in the system because because I
didn't define upfront a session is
essentially something that tries to
capture a burst of activity or you know
a sequence of events that are somehow
related typically typically in time and
so when you're talking about like user
interaction over a website naturally
different users are going to have
different bursts of activity and bursts
of interaction and so cheston's sessions
themselves inherently capture
information that is that is part of the
data itself
not just something that you assigned
ahead of time so that makes them both
very powerful but also very complicated
to build yourself
now using windowing it's very common
when you're processing these unbounded
infinite datasets and streaming process
and stream processing but it's actually
also something we've done for a very
long time and traditional batch
processing however in that situation we
usually use something like a composite
key and don't tend to think of it as
when doing but the same kind of
computation applies so let's now go back
to the code and we're going to start
adding in you know these answers to
these other questions so we've we've
alighted the first two steps of the code
because those aren't going to change for
the rest of the pipeline so we're just
looking at that last step where we're
doing that integer summation and so what
we have here then highlighted in black
is the code that we've added and so all
we all we're saying is you know once we
have our input that's been parsed into
these these teams score key value pairs
we're gonna go ahead and window it in to
fix windows of two minutes and then
we'll go ahead and apply our summation
transformation so now if you look you'll
see that we're computing for independent
results each for two minutes slice of
event time as as before this is still
batch processing so we wait until we've
seen all the inputs before we provide
any any results and that's fine for
bounded datasets but that's obviously
not going to be practical for unbounded
datasets because we can't wait until the
end there is no end so what we want to
do there is start decreasing the latency
for individual results and to do that we
have to look at the third question the
processing time so to answer this
question of when in processing time
we're gonna want to start using a tool
called triggers so triggers are a way
for you to dictate when results within
your pipeline are going to be emitted
the triggers are often going to be
relative to the watermark which again is
that systems best guess of data
completeness and event time progress so
again let's go back to our code and
we're gonna insert a triggering
statement here that simply says I want
to trigger each of these two minute fix
windows when the watermark passes the
end of it so that when the watermark pad
reaches the end of that window that's
basically a state and from the system
saying we believe that all the input
that you're ever going to see that is
relevant to this window has been
consumed and so it's it's safe to go
ahead and get
what we think is a complete answer so
now that's look at how things execute
with this triggering so the graph here
shows a perfect watermark this is what
happened would happen if the system knew
exactly when the data for a given event
time had was complete and so the the
watermark is represented here by that
green dashed line and you can see it
essentially follows the the curve of the
the data as they arrive and once the
once the dashed line passes the end of a
window then the system is free to
materialize the results so we first get
a result for the first window and then
very quickly afterwards the watermark
traverses to the right we get outputs
for the other two windows and here you
can see one of the drawbacks that comes
from waiting for late data right that
first window we're waiting a long time
after that windows passed just for that
one late element 9 right so here we only
have two elements but you could imagine
a situation where the vast bulk of our
data has arrived but we're just waiting
on one or two that late late elements so
we're getting introduced in this
unnecessary latency into our system so
let's go ahead and now look at the
second graph on the right so this is you
know same input data but this time we're
using a heuristic watermark that's with
the solid green line and you can see
that being a heuristic it's it's less
conservative than the perfect watermark
on the left but the the big omission
here is that the watermark doesn't take
into account that that value of nine
that in this case showed up late so here
we've got two problems right sometimes
we're waiting too long to get any
results out of the system and sometimes
we're moving on too quickly leaving
elements abandoned behind so we can
address both of these problems with
using more advanced rigor so back to the
code again what we what we've done now
is we've added a two modification
structure we basically said well you
know we want to we want to solve the the
2slow problem we'd like to have early
speculative results over time so we had
this with early firings clause that just
says you know every every minute of
processing time if you have anything new
to report to me go ahead and give me an
update that way I'm going to get
continuous updates to my results over
time we've also added another clause to
ask for late firings so here if we
expect our elements to be relatively
telemon's to be relatively
we might decide that every time we see a
late element we'd like to go ahead and
update our results so if we go back to
the animations now and look at this you
can see it looks very similar to what we
had before but for for certain windows
you know as time progresses we're gonna
give early updates anytime that we've
had updates or we've had data that it's
worth reporting about for more than a
minute and those are all annotated as
early once the watermark passes the end
of the window if there's something new
to report about again we'll give an
on-time firing and then on the the graph
on the right where that nine shows up
late you'll see we also give that
immediately that that late firing saying
you know we you know we we've given you
an on-time firing we thought the value
of this window was five but then this
nine showed up and we need we need to
give you a new update we need to tell
you it's actually 14 but every time we
choose to give you multiple updates we
have to decide how they're going to
relate so if you look at the last window
we have a speculative firing with three
and then we get some morph elements in
with a total of nine right before the
watermark now we can choose how what
we'd like that final result to be in
this case we've accumulated so we're
giving a final score of twelve and
including the results from that earlier
speculative result but that behavior is
actually configurable and that brings us
to the final question in the model so
this last question how do these
refinements relate in order to walk you
through the the different options we
have here we're gonna have a little
example here on the table so the idea
what this table is we're gonna imagine
that we're getting four elements in that
we're gonna process so that's what the
three the five the one and the two are
and as they arrive we're gonna have
three trigger firings we're gonna have a
single early speculative firing we're
gonna have a single on time watermark
firing and then we're gonna have a
single late firing and we're gonna see
what the actual materialized outputs for
those firings are with depending on
which accumulation mode we've we've
chosen so the first mode is discarding
mode and here you only include values
that have come in since the previous
firing so you can see we our first
firing includes that three but our
second firing only includes the new
elements that have come in and then if
you look at the bottom then we have we
have two rows down there last observed
in total observe so the lassiter serve
tells you what is the last thing I saw
from this this this pipeline and it gave
us the value of to which all B's
not the right correct total sum so this
is this is sort of the shortcoming of
discarding mode but if you're sending
these outputs into a system downstream
that is also itself doing summations
then you get exactly what you want that
that downstream system is going to take
these individual disjoint outputs and
sum them up to give you the the right
total so you get that 11 in that case
now another option is accumulating mode
which is what we had on the previous
slide so here the total running sum is
emitted every time we do a firing but
you can see now the last observed value
is correct but however the total
observed may over count so again
depending on your what you're doing with
this data downstream that if that system
isn't able to handle these multiple
firings you could end up with incorrect
results the last example we have is
accumulating or attracting mode so in
this case we're still doing the
accumulation that we were doing before
but in addition every time we have a
refinement of a previous result we not
only give you the new updated result but
we also give you a retraction saying hey
you know previously we said like for the
second firing we'll give you the 9 this
is the new value but also previously
we'd said that the the result was 3 well
we're gonna get we're gonna say you take
that 3 back here's here's sort of a
negative 3 all right so let's go ahead
and change our running example to use
accumulating and retracting mode so
again just a small code tweak and then
back to the animation here you can see
this looks a lot like what we had before
where you know as results evolve over
time we give these speculative updates
and new values but you'll notice each
time we provide a refinement to to a
value we also in addition to in addition
to giving the new value we give this
retraction that tells you you know
previously we'd said you know a certain
value now we would take that away and
this is really important and like if you
have a system that's that's performing
multiple aggregations in a row you know
that that early firing and then the
later on time firing or late firing if
you wreak your data downstream those
might end up going to different places
and you can't just assume that you can
overwrite the previous value with a new
value you may need to actually have a
retraction for the first value sent to
one place and then this new value sent
to another place so being able to have
have both of these both of these sides
of the coins sent throughout the
pipeline or sent downstream that's
really important for having correct
everywhere so those are the four
questions the what where when and how of
data processing right and we think
that's a really intuitive framework for
understanding these complex infinite out
of ordered data datasets and how to
process them so now we'd like to try to
convince you why this framework is so
powerful so we have here five five
reasons why we think this model is
really powerful well just kind of walk
through more quick
the first one is correctness this isn't
something we've historically thought we
could get from streaming systems right
so this is this is a pretty big deal
yeah so you know many of you are maybe
aware of sort of the the the history of
streaming systems and how there's for a
long time then this this baggage of
streaming systems not being able to
compute correct results you know having
having inherent systematic things built
into them that make it difficult to
actually get correctness but if you
build your systems properly and you also
approach the problems properly that's
not actually true so let's look at an
example here that's going to highlight
this a bit so again remember that the
the processing time the time things
arrived in the system is really at the
whim of the distributed system that
you're dealing with so depending on the
day depending on the time the wind
direction right your results might
arrive in a slightly different manner so
what we have here you know we've got our
ten events and the white the white
version is what we've been seeing all
along right but you know if things had
happened differently or if we're
reprocessing data and you know the the
stuff everything percolates through the
system slightly differently they might
have arrived in the order that you see
in the purple version the thing that's a
really note here is that the expositions
for all of these events is the same here
the events the events occurred once
right they always occurred at the same
point in time it's just when we actually
observe them with the system may differ
depending on how the system sees the
events or when we do the processing
things like that let's look at the
impact of those changes right processing
time he's gonna look look like this so
this these graphs deserve a bit of
explanation so what we're doing here is
trying to explain to you what it looks
like when you use processing time to to
to compute windowed aggregates of your
of your inputs so in the light gray you
can see that the the orderings of when
these events actually happened but
process when you're doing windowing
within processing time what's what's
really effectively happening is as an
event shows up you're you're throwing
away whatever event time that event
actually had and saying you know what
actually I'm going to just pretend like
the time that this event arrived in the
system is its event time so that's why
all of these events are effectively
being shifted over onto that ideal but
the results of that you can see is that
depending on which order they show up
and you actually get different results
so here I'm an undefined behavior of
your distributed system it's actually
leaking through and affecting your
results what would really like what
would be ideal is for regardless of the
ordering of when things arrive we end up
with the same answers so let's look at
what happens if we aggregate based on
event time right so here you'll see that
we do have different intermediate
results those speculative firings beyond
time the late firings are giving us
different results because the data is
arriving in a different order however
the final result for each of these
windows is identical regardless of the
input ordering all right so next up
power so what we're really getting at
here is that with with the beam model
you can do really powerful things with
without a whole lot of effort so as you
may recall earlier when we were talking
about session windows I'd mentioned how
those are those are really powerful
concepts that allow you to really learn
very interesting things about your user
data but they're also really complicated
to implement and so let's talk quickly
about what we mean by sessions so what
we have here is we've looked to identify
the points that we have for a given user
and we're dividing it up into two bursts
of user activity so you can see the user
was sort of playing for a while here
took a two minute break got distracted
by something then came back and played
our game some more so there are many
cases where we'd like to treat these as
two different sessions of user activity
and process the elements based on the
session in which they arrived so doing
this yourself in code is actually quite
a lot of code word there's there's blog
posts out there that kind of explain how
to do it on various systems and it's
non-trivial but the way that you can do
it within the system like beam where
we've we've carefully abstract it apart
these different questions that you're
answering allowed it to swap in and out
answers to the questions means that you
just walk in here and you throw in this
one-line change of saying you know we
used to have fixed windows in here well
now we're just going to say you know we
actually want to window by sessions
everything else stays the same and now
you can see that as we're going we're
really building up these sessions over
time as elements come in in close
conjunction with each other we're able
to treat them as a single session so you
can see we're starting to build up
sessions we start merging things as we
process new elements and what you see
here is that late element of nine is
really key and getting the correct
results here because we only have two
sessions but until we see that late
element it looks like we have three
sessions all right so next composability
so we've shown you a bunch of examples
where you kind of ask you know ask the
four questions and answer them but you
actually can ask these questions
multiple times and that that just
extends further the amount of power you
have with the system so what we have
here is some code that's calculating the
length of a user session so we're when
doing in two sessions trigger at the
watermark and then we're just
calculating the length of each section
each session we're not trying to sum the
scores right now just figure out how
long that burst of user activity was so
this this length of user activity is a
reasonable proxy for user engagement so
this is a metric that's commonly used
just to see help you know how long our
users you know playing our game crushing
candy killing works so remember these
graphs we've been showing you are perky
so Tyler and I as two users playing the
game each have our own graph here right
because we each have distinct sessions
our own patterns of playing this game
and this is oh that's fine so so once we
calculated session links then the next
thing we're going to do is go ask the
questions again and this time we're
gonna say okay well we've computed
individual session links for all of our
users but in order to really infer
something interesting about our system
let's go and we're gonna window those
those sessions into fixed windows so we
can sort of group together sessions
within a given time period and then
we're over that time period we're gonna
compute a global average so we're going
to say with you know within this fixed
window of an hour or two minutes or
whatever you know what is the average
session length and this will give us
over time a view of user engagement as
it evolves as our system changes
so you can see here we could use this
for something like tracking new rollouts
so we just pushed a new version of our
game to prod and what's rolling out and
our users are starting to engage with it
if over time then we see a sudden
increase in the length the average
length of a user session then that new
game that new feature that we rolled out
must be really engaging right it's
causing users to not get distracted and
to focus on our game for a longer time
okay so now to move on to the last two
so so flexibility the system gives you a
lot of a lot of power to to choose
exactly what the shape of your output
looks like we walked you through so far
six different examples so here we've
covered everything from your classic
batch job processing all the way up
through some very complex stream
processing and what's more moving on to
the last point the modularity we've done
this with really quite minimal code
changes you know very small changes that
are still maintainable and leave your
leave your code readable and and allow
it to evolve still over time as needs
change
so here the core algorithm that we're
running was just integer summation right
it's very simple but this same kind of
modularity applies to much more complex
algorithms so the bulk of your code the
bulk of what you're doing is you're
really complex user and logic that will
all stay the same as you tweak these
other three questions and so there you
go five reasons that we think the four
questions are awesome all right so we've
got these four questions but that's no
fun unless we actually have a framework
that's concrete right well you can get
your hands dirty and start playing with
them so that brings us to apache bheem
so to begin with we'll look and see
where this model and then eventually
Apache be came from perspective so so
for us it all began back with with
MapReduce so the original MapReduce
paper was published back in 2004 and it
fundamentally changed the way that we
think about massive scale distributed
data processing
now inside Google we kept working after
MapReduce and it evolved into a number
of new systems and as we were building
these systems and using them internally
we continued to publish papers so we're
sort of writing these papers lobbing
them over the wall but really just
focused on our own
internal needs externally then as you're
all well aware an open source ecosystem
flourished around it initially you know
these ideas around MapReduce but then
you know adding in all sorts of other
innovations and other ideas and really
turn into this this massive ecosystem of
open source software that you all
utilize today so in 2014 Google
announced Google Cloud dataflow which
was part of our cloud platform and it
was a product and a fully managed
service based on these years of internal
data processing now there were really
two parts to cloud dataflow there was
the SDK and the programming model
originally called the data flow model
which is what we've been discussing in
this talk that you used for constructing
your pipelines and then a no-no Nobbs
managed service for executing them but
we didn't want to stop there we there
are two things really we wanted to share
this model more broadly both because
it's awesome
but also because we realized looking at
the success of open source ecosystem
that users really benefit from having
this larger ecosystem and having
portability across systems and the
ability to choose amongst these other
open systems so Google along with a
number of our partners when I hadn't
donated this programming model and SDKs
to the Apache Software Foundation as the
incubating project Apache bean so today
Apache beam includes essentially the
following three things firstly it is
this this conceptual beam model that
we've been talking about that that the
underpinnings of which are these this
what where when how a model of thinking
about data processing then we have what
we hope becomes a very large set of SDKs
that let you actually express the
concepts in this model so we're starting
with the Java SDK which came almost
directly from Google's original dataflow
SDK modulo a very large package renamed
and we're also working on a Python SDK
which is on a feature branch in the
Apache bean repository and then lastly
the part that actually makes this
portability thing happened is that we
have runners for a number of existing
distributed processing backends so we
have runners for Apache flink Apache
spark and Google Cloud dataflow there's
a direct runner for local development
and testing and then you know as part of
you know community involvement and
actually building an ecosystem around
this within the greater open source
community we now have two other runners
in development contributed by folks from
these two communities
a patch of gear pump and Apache apex
with apex is either as both of those are
currently in future branches has apex
it's gonna merge in any day now apex is
eminently moving into the main feature
branch or the main branch of beam
because our goal with Apache beam is
really to support three distinct user
communities right the first is end-users
these are folks who have data they have
business needs they'd like to write
their logic and process their data and
they want to do this in the environment
they're familiar with the languages
their company is already chosen the
distributed processing backends are
already using either on-premise or in
the cloud the second is is SDK writers
so these are the folks that want to make
the beam concepts the beam model
available to folks that want to develop
in new languages so you know you want to
you want to have a go SDK or you wanted
to have a Visual Basic SDK you know go
for it and then finally we have the
runner communities and these are the
folks who you know are part of and
deeply involved in a distributed
processing backend we get flanked SPARC
apex whatever and what they'd like to do
is add support for being pipelines so
that they can take the concepts and the
programs written against them and go
ahead and execute them on their back-end
so having a vision and reaching it
lowered are two very different things so
so be mentored incubation in very early
february of this year and then very
quickly after we dumped 48,000 lines of
initial code into the apache repository
so what follow then was a period of
chaos deagle deagle off' occation
generalization you know a lot of work
around making this into a an actual
system that can be you know worked on by
a large community outside of google in a
very general way so we've been getting
into the cadence of doing incubating
releases we've grown some new committers
in the community working on things like
like your pump and in Apex you've seen
us do a lot of API changes as we try to
stabilize the API is between components
and get ready but we're really reaching
the part getting to the point where you
can this promise of portability is true
where you can take a single beam
pipeline and smoothly run it across any
of these runners and then once we start
once you get to that point and you have
folks actually interested in picking a
runner to Toronto
then you now have this other problem you
have to address of how do they
understand what different runners can do
so that brings us to the capability
matrix so what we're trying to do with
the beam model is really generalize the
semantics of this style of data
processing right we're trying to come up
with the best most intuitive way for you
to represent your computations and so
what we've done is we've we've
categorized features within the model
and these are these are broken out into
categories that match the what where
when how questions know that the color
sort of match the colors we've had
throughout here and for each of the
features on each of the columns it shows
you you know for the given runners that
we have what level of support we have
and you can see in general most folks
within the industry are moving towards
supporting everything within the B model
one thing worth calling out here is that
the SPARC column is is missing a few
things there that's primarily because
this is built on top of SPARC 1 X so as
SPARC 2 L matures which is bringing with
it a lot of the stuff within the B model
into the SPARC itself we expect this to
change and also the folks that are that
are working hard on our SPARC runner are
also working at building in support for
this within beam on top of SPARC 1x so
you'll actually be able to do things
with beam that you otherwise would have
to do manually and require much more
effort on top of SPARC our next once
they once they manage to flush out all
of those columns alright so I think
we've got really different takeaways
here for different folks if you're just
getting started with stream processing
and you're still trying to figure out
the concepts and how you can think about
these style things I really encourage
you to go check out Tyler's blog posts
on streaming 101 and 102
they're basically a much longer more
detailed and slightly snarky ER version
of this talk but they're a lot of fun
and then if you you know want to learn
more about beam come to our website
there's lots of information there both
if you're interested as a user as an sdk
writer or as a runner builder and if
you'd like to join the community there's
our mailing lists and we would love to
have you come and help us build you know
the sort of future vision of data
processing the way we've seen the
community grow over the last few months
has just been amazing what happens when
you take something it was designed in a
very hermetic environment and you
generalize it you bring in the power of
new ideas
so we're really excited about the bean
community and its growth
all right so we'd love to take questions
either a here offline we'll be around
all day so thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>