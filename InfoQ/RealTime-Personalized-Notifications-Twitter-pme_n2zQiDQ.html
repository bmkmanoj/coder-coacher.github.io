<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Real-Time &amp; Personalized Notifications @Twitter | Coder Coacher - Coaching Coders</title><meta content="Real-Time &amp; Personalized Notifications @Twitter - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Real-Time &amp; Personalized Notifications @Twitter</b></h2><h5 class="post__date">2017-08-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pme_n2zQiDQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">a couple of weeks ago I read an article
they talked about how recent IPL cricket
auctions were life-changing for several
cricket players that list of players
included Ben Stokes who's one of my
favorite British cricketer and he was
quoted in the article saying I was
following it on Twitter I didn't
actually see it life I kept on
refreshing my notifications I thought
people were tweeting and then I realized
that Pune had got me why am i sharing
this anecdote with you all well for one
I can talk about cricket in this country
without having to explain myself and as
you can tell I love talking about
cricket but more importantly it ties
very closely to our jobs to be done
framework and we want to keep our users
informed about the world the other thing
is it also showcases one of the prime
use cases of Twitter where people come
to Twitter to follow a live event and to
hear the real-world commentary around it
and for both of these use cases my team
plays a very central role and we we
enable these use cases by notifying
users about what's happening in their
world in real time the key to highlight
here is their world and real-time and
during the talk today we will focus on
why these two are important my name is
Saul Apothic I manage notifications team
at Twitter I'm here today with Gary lamb
who's the lead engineer in my team today
we will be giving a high-level overview
of notifications then we will talk about
some of the main challenges we face
scaling it and then we'll go over how
our infrastructure solves them after
that we will go over the evolution of
notifications and broadly there are
three categories that we will cover
today we will start with triggered
notifications and then we will talk
about personalized sign out and
recommendations so that's not before
before we start let's do a quick show of
hands
raise your hands if you have used
student notifications in the past it's
awesome
but for those who did not raise your
hands I'll have to find you and start
sending you notifications but for your
benefit I will give a quick recap of the
recap of the four main channels we use
to notify our users the the first one is
notification timeline this is an in-app
experience you go to your timeline to
see a reverse chronological history of
all your notifications it acts as a
pivot for the product then there's push
it's a heavenly use channel it ties very
closely to the core of Twitter which is
students live and these notifications
are real-time one of the most common
force notification is account
notification through account
notification you can opt in to be
notified every time someone tweets and
that's that comprises of a huge volume
for push and then there is our G channel
some of you might not be aware but
Twitter actually started as a group send
SMS service and so SMS is still relevant
for us apart from some product cases we
use it for sending login codes and 2
factor authorization and whatnot and
then finally email it's the oldest
messaging channel channel on internet
and it's still pretty relevant for us
now let's switch gears and talk about
some of the main challenges that we face
and they have a bearing on how our
architecture has evolved over time the
first one I want to highlight is
notifications our bimodal what I mean
there is if you were to plot the number
of notifications received by users for
all of your users you see a power law
like distribution where there are two
major category of users there are users
like me who get a handful of
notifications on a given day and then
there are these
have users they receive a ton of
engagement these tend to be your
celebrities your politicians news
reporters and people who have gone viral
so as a product this creates challenges
that have bearing on the infrastructure
so for instance for these heavy users
they they get ton of engagement so they
get kind of notifications and you want
to make sure we deliver those to them in
real times on the other extreme people
who do not get a lot of notifications
are the ones who do not tweet a lot they
are the consumers and for them the
challenge is to figure out the right
content that or right notifications that
will bring value to their Twitter
experience as all of you are aware
Twitter is a very spiky product here's a
good example of that and this is from
2013 so it's slightly dated but this was
from in Japan they were he broadcasting
in an anime called castle in the sky and
something must have happened during that
where suddenly all the Japanese users
took to Twitter and started tweeting it
and it created a tweets per second
record for us back then
and then we started receiving hundreds
of thousands of notifications or tweets
per second and as you can imagine every
time there's a tweet spike there is a
corresponding notification spike related
to that footer is highly asymmetric
unlike some other social networks you
have cases where people like Elon Musk
who have millions of followers so every
time
Milan must tweets we have to send out
his free to all of his followers and
just like to expand out there's
notifications pan out so every time we
tweets we have to find out all the users
who we need to notify and then deliver
that notification to them in real time
so to round off
an overview of our challenges how I'll
briefly mention the four main challenges
that we face and by no means this is an
exhaustive list there are lot more both
from product and infrastructure point of
view but I want to highlight them today
because we will also talk about how we
solve or how we address some of these
challenges latency is the most obvious
one Twitter is life we want to make sure
that from the time someone tweets to the
time you get notified is minimal we
don't want to introduce much latency
otherwise it takes away from the
experience I spoke about notification
spikes and find out again we have to
make sure that we not only scale up to
handle billions of notifications every
day but we can also handle these sudden
spikes heterogeneity is something that's
there for a lot of distributed systems
and it's the case for notifications and
what I mean there is if you look at all
the calls that our services make
notification services make you can split
them into two major categories there are
there are calls that are serviced via
cash or a fast data sort which tend to
be anywhere from few milliseconds to
let's say tens of milliseconds and then
there are external calls that we make to
Google and iOS which can take anywhere
from half a second to a second and when
you have these two sets of calls it's
not as simple as just horizontally
scaling up you have to be cognizant of
how you are coupling or actually not
coupling them and then finally for any
app like Twitter we need to make sure
that we are resilient and for that we
need to make sure that our users can get
their notifications even if we were to
failover and in general we need to make
sure that we can handle multiple data
centers so let's talk about how our
infrastructure addresses some of these
challenges and now for the sake of time
I will be very I will give you a very
high-level overview each of this topic
is something I can talk for the entire
duration but that's
give a quick overview so this is our
push architecture which is used to send
push SMS and email notifications and at
a very high level we see notifications
and then there is some business business
logic that we apply things like make
sure your settings are being honored we
check for spam abuse whatnot and so a
bunch of business rules are applied and
then once a notification passes all of
them it's then sent out for delivery and
then it eventually gets to your devices
so for this architecture the the three
main challenges are of course we want to
keep latency low because push as I said
is a live use case live channel we have
to be able to deal with spikes which
happen all the time and the problem the
spikes is you cannot fit it when they
happen so I used to work at Netflix and
there they could rely on predictive auto
scaling but we cannot do that because
you have no idea when the next spike is
going to come and then I mentioned the
heterogeneity so how do we deal with
latency and spikes
well we rely on horizontal scaling but
that's not enough we need to make sure
that when we scale up or when we have a
sudden a surge of a spike or
notifications traffic we don't bring our
upstream services down because as I said
notifications are asymmetric and so it's
very common for us so we have millions
of events in our queue that are all
referring to the same user and if each
of these processes start calling for the
same user that services we will be down
so we is lie on lots of short-lived
caches usually when you have you put
caching in place you optimize for cache
hit rate for us it's totally fine to
have cache misses in the beginning and
given that these spikes are short-lived
short-lived Cassius Cassius totally
suffice our use cases in terms of
heterogeneity there are two main ways we
to them so as I said we have different
types of notifications and different
types of calls and we have we rely
heavily on priority queues to make sure
that for instance your login codes are
not being delayed because there's a
backlog because Katy Perry just tweeted
so we need to decouple them and we do
that via priority queues and then the
second thing is we make sure that cause
that take that have similar latency
profile are queued separately so if
there is a delay due to a Google outage
or slow down we are not delaying other
notifications so that was the push
architecture for notifications timeline
we use a pull architecture and at a very
high level same thing notifications are
received via by our right path they're
typically received asynchronously but we
have some synchronous use cases we apply
a set of business rules and again in
this case instead of pushing them out
for delivery we stole them in a backing
data store and they are cached and these
sketches are long-lived caches and the
idea there is every time you visit your
notifications timeline it should be
served out of cache it's very rare that
it's not served out of cache because
it's a very costly operation to generate
your timeline on the fly so that the two
main challenges here are again latency
which is obvious we want to make sure
that we are not taking forever to load
up your notifications timeline because
otherwise you'll switch to something
else and the other thing is multi DC we
need to make sure that in the event of
fail failover or what not we can deliver
your notification timeline to you and in
a timely manner so from a latency point
of view we use a custom implementation
of Redis and I'm not sure if you guys
attended Yao's talk on Monday she talked
about our caching architecture very good
talk if you check it out
as I said these we rely on long live
caches and we also store all of our
notifications in Manhattan which is our
real-time distributed backing store and
it's built for availability because
that's the most important thing for us
from a multi DC point of view we of
course to cross the seer application but
that's not enough we need to make sure
that if you you were to move from one
data center to the other your
notifications stay the same otherwise it
will be a very jarring experience and so
for that we have we siphon off a small
volume of notifications and it's off in
a separate of a synchronous queue we
make sure that we compare notifications
across data centers and make sure that
they are the same and if they are not
saying we need to figure out what's
going on we also have maintenance job to
make sure that our caches are not
growing out of bounds and just doing
housecleaning which we don't do not need
to do while we are handling real-time
events if you are building a
notification infrastructure there these
two things I want to highlight which you
should think about building in one is
you want to make sure that it's very
straightforward to add any notifications
otherwise your experiment velocity will
be very slow so most of our new
notifications are pretty much self serve
or templated the second thing is we you
want to ensure that your notifications
and your experiments are driven purely
on the backend side and the reason I
mentioned this is as you can imagine
footer is on all all possible apps and
if I have to add a new notification and
if I have to chase down iOS theme and an
Android team and make sure that I figure
out when is their next release and they
are never aligned and before before you
know you have wasted months trying to
add a new experiment and so we do not
have we used to live in that world a
long time ago and we have moved away
from that and most of our new
experiments are now server-side
and it is responsible for our high
experiment velocity so from an
infrastructure point of view before we
wrap wrap it up I want to mention these
three key takeaways again there was a
talk on Monday where this really good
talk where we talked about sink what is
a sink and that's a trade-off we make
all the time and for distributed systems
we should always rely on asynchronously
a synchronicity is your friend try to
make sure that you have as minimal
synchronous operations as possible
because it's very hard to scale things
up
so in our notifications infrastructure
we rely on very few synchronous
operations and wherever possible we try
to move away from them similarly we make
trade-offs between write and read path
and the idea there is you need to be
cognizant of what you are doing at right
time there are certain things you can do
in right time but you shouldn't for
instance when we cache stuff we only
cache our IDs we do not cache data for
them because we want to do that at the
real time so that we give you the most
up-to-date information similarly on the
flip side when we show you an aggregated
time line most of the aggregation
happens at read time there is no point
doing it at write time and then we
talked about multi DC but and this stain
is you need to be very even if you
starting off of an app with just one
data center make sure that you are aware
of how you would scale it up to or how
you would evolve it to have multiple
data centers now let's go over the
evolution of Twitter notifications so we
started off with this category of
notifications which are by far the
highest volume they're called triggered
notifications as you can imagine they
are triggered by an interaction on
Twitter so for instance if I were to
tweet and if you were to reply or like
it will generate a like notification or
reply notification so again these are
the highest volume notifications by far
as I mentioned we generate
billions of them every day we need to
make sure that not only we do that but
we can handle spikes and I thought about
by modality in the beginning but trigger
notifications are highly bimodal a small
minority of user receive majority of
these notifications a quick example of
how they flow in our system someone
tweets goes through a right pass and
then there we apply all our business
rules and logic and then once it passes
that it sport document onto two
different paths one path is real-time it
leads to your device with a push and
other path is where it's stored and then
served out of cache so why why do we
need personalization why I triggered
notification is not enough the reason
they are not enough is there is a lot
more happening on Twitter both outside
your graphs and within your graph that
triggered notifications do not cover
they cover a subset of those use cases
but they are not enough for instance if
I'm a left-hander warriors fan and I
follow follow warriors account and then
there is some controversy during a game
and if warriors account doesn't tweet it
I would never know about it but if
couldn't knows that I'm interested in
Valle yes they could notify me in real
time with a upcoming trend and whatnot
as I mentioned in the beginning we
notify users about what's happening in
their world in real time we cover
trigger notifications cover the real
time part of it but there's lot more
that we do with personalization that
Gary will talk about so I'll hand it
over to him Thank You syrup so this is
where I'm going to talk about
personalization personalization comes
down to notifying our users about their
interest you only want to hear about
things you're interested in and there's
two ways we do personalization at
Twitter the first
we call personalized Vento previously we
talked about fan-out when Elon Musk's
tweets we have to send it to all the
followers that have subscribed to them
but Elon Musk tweets about space because
he's the CEO of SpaceX and also tweets
about electric cars because he's CEO of
Tesla you might not be interested in
space but you might be interested in
that your cars so we only want to notify
you when Neil almost tweets about
electric cars and this is what
personalized spinel does I'm going to
walk you through the personalized
fan-out algorithm so we mentioned we to
keep track of a user's interest we all
want to notify you about your interest
so we keep track of your recent
engagements on entities let's break this
down engagements are like tweet replies
when you tweet about a particular topic
we call that an engagement or where you
like somebody else's tweet we we bunched
this up into something we call
engagements entities these are hashtags
accounts or can be something more
abstract like a particular topic it
could be movie sports so we keep track
of your engagement on these things we
call entities to make this more concrete
for me for example I'm currently in
London
so I've tweeted about hashtag London and
this is being Q Khan I want to keep up
with everybody's great talks as in
favoriting and liking and retweeting all
of your tweets about Q Khan so I have
recently engaged with London and Q Khan
so those are my recent engagements so we
keep track of these engagements as a
proxy to what your interests are and we
only keep track of your recent
engagements because Twitter is
constantly changing the data on Twitter
what people are talking about on Twitter
are constantly changing as well as your
interest you maybe haven't been
interested in the Super Bowl or the
Oscars a month or two months ago you're
not so interested in that right now so
when we keep track of your most recent
engagements the second part to
personalize find out is your followings
so when we mean followings my followings
are the users I follow
you follow a lot of people on Twitter
but not every single person you follow
means the same to you there are some
people that you really want to hear from
and there are others that you might not
want to hear from them so much so for
every user we want to keep track of who
are the very very top followings who are
the people that they really really want
to hear hear about so for me for example
I'm a Twitter employee so Twitter is one
of my top followings and to come London
I want to hear about everybody's talks
York on London tweets a lot about it so
I follow maybe hundreds of people on
Twitter but in personal i span out we
would only use the top view in this case
would take top two in the simplified
example so to recap we have your recent
engagements on entities and we also have
the top followings and I want to walk
through an example of what happens when
Q columns and tweets and when
personalised inal decides whether they
want to send it to me or not so first
thing when you come London tweets we
extract out the entities from the tweet
in this case we're looking at hashtags
so hashtag London is the entity would
then look whether I have recently
engaged on hashtag London which I have
because I'm at UConn in London and then
we look at the author of the tweet and
whether the author is in my top end
followings so then we decide hey we'll
send this to Khan tweet to lamb Gary and
the reason this is the hard problem like
a lot of problems that Twitter is
because there's asymmetric if you look
at Katy Perry she has millions millions
of followers to make it clear what we're
not doing is looking at Katy Perry and
sending it to her top 20 followers
because then there will be a huge fight
to become the top 20 years of 2025 okay
Perry and we upset a lot of people what
we're doing is we're taking everybody
that follows Katy Perry and seeing if
Katy Perry is in one of the top
followings so you compare Katy Perry to
someone slightly less popular like me I
might only have one or two people who
are in my talk who want to hear from me
so how do we solve this so we solve this
by colocation and no network lookups
okay
what does let's deep dive into this what
we do is we shard by every user so each
user belongs to a particular shard I
would be longing to say chars one star
would be long into short too and on each
one of these charts for every user we
keep track of the top engage their
recent engagement and the top followings
and they're all on the same chart what
this means is when we want run the
personalized standout algorithm we don't
have to do Network lookups to other
services everything's done locally so we
don't incur any latency by hitting other
external services they also scales well
because we've charted the recent
engagements and charted your followings
so we can scale horizontally that way
and Pokeno here is all of these charts
listen to the same copy of the firehose
so each one of these are running their
personalized final algorithm on the
users that belong to that shard
so let's walk through an example of what
happens when Katy Perry actually tweets
so Katy Perry three skitty here I lost
the bet for best picture I wanted
glowing light so this was tweeted during
the Oscars if a lot of you might know
during the Oscars they announced that
la-la-land
won best picture and then they kind of
they made a mistake there and then they
announced that moonlight 1 so this Katy
Perry tweeted this out before the fix
the mistake but back to this problem
what we have supposing you live in a
world where you have two sharks you have
sharp one and you have shark tooth and
we're going to look at two of Katy
Perry's followers Buller a and follower
be so far where a is not short one and
on short one we keep track of follower a
engagements in this case spoiler a has
engaged with a lowland and Katy Perry is
one of the top followings on shore to
follower B is a fan of moonlight and
they've also follow Katy Perry's lemon
cell followings so what happens in
personalized fan out is again the first
thing we do extract those entities from
the tweet
in this case the entity is moonlights a
movie we then send that tweet to every
single personalized standout chart so
all of these charts get it and they run
the algorithm and as I mentioned before
we look at the engagement only follower
B has engaged buffoon like so we'd only
send this the follower be and not
follower a so in this way we're able to
notify you about people you care about
and only about the interest that you
care about so we talked about why we
need to co-locate our data I'm going to
deep that now more into how we make this
efficient and the key is data
pre-processing I'm going to go a bit
backwards on this slide I'm going to go
from your right down to the left so
first off the recent agents are entities
we store that in memory the reason we
store in memory is your recent agents
are short-lived as I mentioned before we
keep you're not interested in something
from two months ago so we keep maybe a
couple of hours up to a day's worth of
your recent engagements in memory which
is good
we don't persist it as any for anywhere
else because it's not useful anymore
after day or two this data isn't really
going to be useful but the problem comes
when your shard goes down we work out of
a data center there's maintenance jobs
things go down all the time all of you
are probably aware of this and when you
go down then you lose your in memory
recent engagements at this point we
can't just go oh sorry we're not going
to send you notifications right because
all the Katy Perry fans will get really
angry so what we do is we need to make
sure these personalized Dino shards come
up very very quickly and the way we do
it is we read from a Q this Q has the
last say 24 hours of recent engagements
and on startup we've consumed from this
Q very quickly to rebuild this in-memory
engagement graph and there are two
things that we do to make this efficient
the first is batching by batching your
engagements together you reduce the
message overhead and this typically
gives us a huge speed-up
the second thing we do is if you ever
use the fire hose and you looked at the
Twitter JSON data it contains a lot of
metadata so it has things like what
clients the user used what the iOS is
Android which countries perhaps they
tweeted from what time of day the actual
tweet pecks there's all this metadata in
there that aren't relevant for personal
life I know
so we extract only the key pieces of
metadata for example maybe just a user
ID and the tweet ID that they've engaged
with so with that's why we call it a
slim firehose and its batched so in this
way we're able to launch personalized I
know shards in a matter of minutes if
they ever go down and the other good
thing about this is the entity
extraction so figuring out on the tweet
which movies or hashtags
it is sometimes can be quite an
expensive process and we do this into
the slim engagement service what this
means is we do the entity extraction
once and then we send that result to all
the personal personal life span out
shards which means we save on that
computation and you can put some more
expensive computations in the entity
extraction so this is recent engagements
the next is how do we preset process the
top followings so to find the top
followings for every user we run an
offline machine learning algorithm and
this is based off your historical
interaction between the two users for
example whether you're likely to like
that person's tweet whether you have
similar friends follow similar people on
Twitter we calculates this offline
because you're your friends and your top
followings don't change that often
friendships are built over months and
years even in this modern age so running
an offline allows us to make it reliable
and scale correctly and after we've
calculated the top end followings for
every single user we partition it and we
partition it in the same way that we
partition our production record service
charts so if we have n rec service
shards rent personalised I know shards
sorry that service is what we call it
internally personalized final shards
we would have n partitions on HDFS and
we pre pre partition this so when we
bring up a shard supposing we bring up
char three it just copies that piece of
data from H DSS on to its local disk and
after we've copied it to local disk we
don't actually load it into memory
immediately this is because your
followings graph is typically pretty
large you don't want to waste all that
memory on something that is so large so
what we do is we lazily load it into
memory as its required so in this way
the recent engagements as we bring up
the shard we process back from akhil
built it up very quickly the top end
falling we simply copy from HDFS on to
the local disk of that shard so the key
takeaways for personalized fan-out is
co-locate your data once you've
co-located your data you no longer have
external Network lookups you can scale
horizontally that way but in order to do
this efficiently you really need to rely
on data pre-processing preprocessor
engagements make sure that sufficient to
process your followings graph so you
partition your correctly but the third
key takeaway here is real-time
personalization is actually very
expensive we have to do a lot of
pre-processing to make sure this all
works efficiently this impacts our
iteration speed if someone has a new
idea they have to write the group they
have to write a preprocessor pipeline
pre process this data before they can
experiment with that and the other thing
that makes real-time personalization
so expensive is we're listening to the
firehose things are spiky on Twitter
we've talked about that a lot if you
look at Hillary Clinton her when she got
nominated for dozen product as a
Democratic nominee her mentions and
people were interested in it spiked many
many different times many times the
average volume so in order for
personalized fan-out to scale to this
volume we need to over allocate on
capacity to make sure we handle this and
because this is very unpredictable we
waste a lot of resources by having to
over allocate
so this brings us to our second way of
doing personalization which is
recommendations in personalized I know
what we saw was we send you the best
content from the people you follow but
sorry I've mentioned earlier your
there's content on Twitter that you
don't explicitly follow that will be
interesting to you there are a few
examples on that slide on that slide
over there but what we're trying to do
is look at your interest look at your
followers and friends look at what
they're interested in and try to
recommend you good content for example
if I didn't know about Q Khan but three
or four of my colleagues came to Q Khan
tweeted a lot about it
maybe you're interested in Q Khan too so
the goal here is to find content you
love even though you don't especially
follow it and the reason why we have a
second way of doing personalization is
because it's scaled a lot better and in
this recommendations world what we're
going to do is relax the real-time
constraint what we're going to do is not
read from the firehose but instead
control the own load by ourself what
does this mean so we have a very tight
loop and in this loop we actually go go
over every single user on Twitter the
number of users on Twitter is much more
predictable than the number of events or
tweets so in this way we're able to
predict our capacity much better and by
controlling the load ourselves this
allows us to scale and be much more
efficient with our resources but again
as identified what we're sacrificing is
latency as you've been tapping our
Twitter no longer are we sending out to
you in a matter of seconds but there
might be a delay of a couple of minutes
because we're looping over every single
user at Twitter and what loop service
does is it triggers another component
called fatigue fetch and rank so fatigue
veteran rank is another service that I'm
going to walk everyone through right now
first off fatigue in this stage we're
able to figure out
where the user lives on that bimodal
distribution that we talked about are
there someone that receives a lot of
notifications in which case we don't
want to send them anymore nor are they
someone that who could do with more
recommendations to help them get the
most value out of Twitter and we have
what we call a history store we keep
track of your historical open rate for
notifications whether you like these
notifications whether you engage with
them so we we know whether you want
notifications or not and this works well
for both the user because they don't get
notified if they don't like
notifications and it's also good for us
because we don't waste computation
capacity calculating and doing things
for users who aren't going to see it in
the end once we pass the fatigue stage
we know at this point we're likely
wanting to send a notification to the
user in this fetch stage what we do is
we look up many many different candidate
sources so a candidate source is the
interface that we defined and it's very
simple that the user ID is as a key and
the value that returns from it is just a
list of possible notifications that may
be relevant to that user
so in this batch stage we fetch from
many different content sources so we're
more likely to find something that's
relevant to the user we would have a
separate content source for each type of
content we would recommend for example
we would have one for hashtags we would
have one for photos we would have one
for follower
recommendations and at the other good
thing about this is if a person has a
new idea and wants to experiment with
the new candidate source for example
maybe they just want to recommend good
technical presentations they could
create a new candidate source for that
and plug it into a pipeline very
seamlessly no longer do they have to do
a lot of pre-processing and all of this
data group logic that's inflexible like
we saw in personalized I know and
typically these candidate sources can be
backed by other services a typical one
we use that Twitter is grass yet so
grass yet is a real-time graph
processing library that we've
open-sourced a Twitter check it out on
github is that essentially what it does
it looks at your followers and look
the tweets of your followers and is of
the random walk across these to find the
best content for you not only can we
back canada sources with other services
they can also be generated offline so we
have scouting which is our offline
MapReduce pipeline framework that we use
our Twitter to generate this data and we
can store that in Manhattan and we can
then load that out from a data set so
these candidate source is super flexible
and they're easy to plug in into the
fetch stage once we fetch all the
candidates we're going to rank them with
a machine learn model if we have 100
candidates we're not going to send all
hundred to the user that would be
impractical and the user will just be
overwhelmed so we choose the best one to
send to it and we bait do this based off
your historical engagement whether you
like photos more whether you're like
hashtags more and we also look at the
social proof for example if six of your
friends have liked a particular photo
versus another photo that only two of
your friends have liked we're more
likely to recommend the one with six
that six of your friends have liked and
once we figured out the best
notifications to send we send this out
to our throw notification infrastructure
so this is what's our talked about where
we do the notification timeline and then
push notification I want to go one level
deeper we mentioned our machine learning
model Twitter is a data and
experimentation driven company so we
need to collect data and data is really
what drives our machine learning models
so each one of our services we log our
information back to HDFS for example in
defects in the rank stage we would log
perhaps the score that the machine
learning model gave all the candidates
and say that HDFS
we also instrument all of our clients so
when you open the new location or
whether you give us an explicit signal
that you don't like notifications we
have this information on HDFS as well
what we do we turn these into machine
learning features and labels they feed
in not only to notifications but
potentially into your rank timeline your
ranking timeline as well and then we
generate a new machine learning model
off of this new collected data so in
this aspect as Twitter is constantly
changing we're adapting our models
to reflect that so buy this in the in
this way we're able to deliver you
relevant recommendations so to sum up
the recommendation section we relaxed
our real time constraint and we took
control of our load we can predict our
load because we're not subject to the
spiky nature of the firehose anymore
this is really more personalized because
we have a diverse set of content sources
we're looking at many different content
sources compared to personalise panel so
you're more likely to find something
that relevant to the user we run that
through a machine learn model and to
parity you really need data I want to
bring everything together we talked
about today so at the beginning we
talked about our notifications
infrastructure this was our scalable
infrastructure that powers the
notifications timeline and allows us to
send push email and SMS to our users and
then what we walk you through is
actually a sliding scale of three
systems from the real-time to the
personalized so in the real-time world
we showed you trigger notifications
these are based off your interaction
where users have an expectation of
delivery and we send these out in real
time then we talked about personalized I
know you might not want to you want to
be notified with all your interest from
people you follow we saw here we're able
to do real-time personalization but it
comes on the cost of in flexibility and
very expensive capacity and then we move
to a world where we're in
recommendations where we're extremely
personalized where we can deliver
extremely relevant notifications to the
user but we have to sacrifice on the
latency so the key takeaway I want
everyone to walk away with today as
you're building out the new architecture
think about where your product or where
your use case lies from the real time
for the personalised world you have to
use it there's a trade-off between all
of them and hopefully what we show you
today will you to make a better better
informed decision about that so thank
you very much for listening follow up on
Twitter and I guess we'll open up for
Q&amp;amp;A right now
- Cindy hi thanks for the talk what
programming languages you use for this
stuff so we are predominantly a SCADA
household of the back end and financials
in the u.s. color yeah thank you for the
presentation and I actually work on a
news recommendation infrastructure and I
was interested in terms of the candidate
sources to say you have a candidate
source that does collaborative filtering
for you and then you have your we
ranking step where you integrate all the
sources yeah so do you pass along and
features from your candidate sources so
you make sure that maybe you have a very
interesting answers but it's being run
very low because you don't have proper
representation for that your revenge
instead yeah so typically we don't do
that because your candidate your
features that belong to your candid
sources are very specific to that
candidate source and you're trying to
rank across multiple candidate sources
those features won't be that helpful so
you really want teachers that are common
to all of your candidate sources all
right thanks for the great talk and you
mention that do you rely on the data
processor of your entities right so how
hard is to do it and how hard is to
define or to find your entity or not
wait sorry can you repeat that your
question how hard is for you guys to
define what is an entity and precise
that in order to have the right
automation yeah is that so
entity is like hashtags and authors are
pretty easy to expect but you can
imagine nothing to an interest it's a
pretty hard problem and sort of there's
a lot of people working on that Twitter
the Torah is full of entities I mean
everything a user as an entity so an
entity could be following another entity
or hashtag is an entity or image could
be an entity or interest could be an
entity so we have a taxonomy of interest
we have all of that predefined and if
you were to add something it's I mean
some of these things are they don't
change on a day to day to day basis like
you might have different hashtags but
they're still a hash tag in a great talk
a couple questions
one was see you pick a candidate source
you mention hash tag for those accounts
to follow right yeah and then you pick
one and then you rank the results for
those and you mentioned that you don't
have features across because they're all
sort of different entities and they're
going to be normalized differently right
but some you know in search for example
they usually just slot in and you say
slot once locked in slot three and then
they pick three three three or something
and they rank individually and the main
so why don't you guys try slotting so
notifications
tend to be more focused on the
chronology so like the value you get out
of it is more about chronologically
sorted reverse chronological sorted once
so we don't want to just artificially
inject diversity for the sake of
slotting but if something is highly
relevant to you
then it'll be injected in I mean so you
give up blending so for example I can't
get a blended photo plus hash tag
because you're you're afraid that it's
going to mess up your ranking so
blending happens in real time because if
you blended it you'd have this problem
of you would go like photo hash person
for the hash person but then there have
different ranking normalization so that
would mess it up so that's why people do
slotting right they'd say here's all the
photos and you'd have the top three
photos you here's the people top three
people here's the hash tag top these
hash tag and they're using different
ranking algorithms and
so but so we try to be that kind of way
to avoid non-normalized no absolutely
that's a good point and I think we
try to do a lot of personalization so we
see that if someone is not engaging with
let's say people recommendations we stop
sending those to them altogether so we
don't have it predefined but we do some
of what you are saying but it's highly
personalized since we collect all that
slightly
so essentially you will try to first
show them a people recognition then you
get the feedback they're not doing
anything
every so often you need to keep
reintroducing the rights if they get
interested in yep so you are sort of
doing that startup doing that but not in
a fixed order yeah
and we don't like notifications is
special because you don't get you can't
display a full search page of
notifications right you get very few
chances to send it so you do want to
send the best one at that particular
point in time thank you
hello here so there are like two pubs
completely different ones the whole path
and one one is the push pop do you in
any way somehow cooperate those two
paths because or do I care about
situation when you push a notification
and it isn't
yes visible on the timeline that can
happen because they are completely
independent paths so you might keep it
but do you even care about it we do care
about it we want to make sure that every
time you get pushed and you go back to
your timeline you see it in your
timeline so that's the ideal experience
and that's what we strive towards but
there are cases when they can be out of
things but it's very rare okay but do
you somehow coordinate those two things
yes yes we do so the notification right
path that we mentioned earlier it falls
off on two different paths so it
basically makes sure that when we send
something out for push delivery it's
also stored in the cache at the time so
it kind of is the gatekeeper for that
okay
hi there you mentioned multi DC can you
quickly talk about some of this stuff
running in parallel and multiple DC's
and these all your own infrastructure or
some of the public life as well I can
take you that so on Twitter we run our
own data centers so everything is
internal to Twitter and we do have to
run things in parallel because users can
switch between DC's yeah so every you
have to have Mead onion you have to be
able to serve from that serve your
notification town I out from different
DC's is everything where you the whole
population of Twitter are you running
that all in parallel and every DC or are
you sharted across BC similar we would
you would have to have enough redundancy
to make sure that you are resilient
enough yeah so your event might arrive
in one vc but then get replicated on to
the other one but we do have the
duplication
you're talking about the labeled data
other if you can go back but so as
people interact with your phone you're
getting sort of feedback so you can
determine so so that feedback is - we
want is to determine if they're
interacting with a candidate source what
else how else are using it and they're
using a building global model so let's
say a bunch of people click on the move
like thing now that should increase the
relevance globally right are you using
that to feed a global model that will
feed into the personalised models for
anybody so I mean there's many models
running at the same time like we have a
model for the top-end followings that
would use some of these features the
rank for this specific slide on the rank
it's a global ranking across different
candidate sources so we usually for
candidate source ranking yeah so we
gather all the candidates so if you have
a hundred candidates some of them might
be hashed as some of the might be photos
and you kind of need to decide what we
want a hashtag or a photo recommendation
okay okay
thanks everyone thank you we'll have to
end this here as we continue this track
in this room 11:50 and if you have any
questions feel free to ping us and we
can talk offline</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>