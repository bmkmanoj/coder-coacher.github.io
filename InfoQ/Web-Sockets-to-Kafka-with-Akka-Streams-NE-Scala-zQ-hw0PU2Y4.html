<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Web Sockets to Kafka with Akka Streams - NE Scala | Coder Coacher - Coaching Coders</title><meta content="Web Sockets to Kafka with Akka Streams - NE Scala - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Web Sockets to Kafka with Akka Streams - NE Scala</b></h2><h5 class="post__date">2016-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zQ-hw0PU2Y4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright so I'm ready is everyone it's
449 scheduled for 445 so this talk is
designed to be a really quick overview
of doing something that's at least
minimally complex to be interesting with
a co streams I've done a few and like
where I spent a lot of time getting into
how akka stream works and then I just
like oh here's how you parse a CSV file
as a stream and no one really cares
about that they can do that in a lot of
simpler ways so for this talk I'm going
to build a really small web socket or
WebSocket server that just accepts web
socket connections publishes all of the
WebSocket messages received via that
connection to Kafka via a single stream
and this is something you might use if
you want to collect metrics from your
front-end pages from JavaScript or
something and yeah so it originally
counted it under 100 lines of code every
factor it a bit move stuff out to
classes if you counted all the important
bits and still wouldn't be but don't
hold me to that right so first akka
streams right not going to go over all
of it the basics it's built on top of
akka uses actors to actually run stream
processing graph nodes it's uses the
reactor stream standard that something
people from type save Twitter Netflix
other companies have been working on
might like fingers crossed get added to
the Java standard library it's really
big initiative the whole point is that a
lot of people use it you'll have a lot
of interoperability by using it so you
can use streams of kafka messages
streams of messages over WebSocket you
can do high-level I oh you can write a
server that's just basically a
transformer first but that consumes a
stream of HTTP requests and produces a
stream of HTTP results you can do
low-level network i oh you can actually
just stream over a socket UDP TCP you
can stream to the input or output stream
of a process you can work with the files
with or sorry without processes like you
can actually call grep and like throw a
bunch of strings at it and have it
filter it if you need to do so or you
can create just a little runnable that
mimics grep rapping over the input
stream and cruising to the output stream
of the java process and you can work
with files like I said but you can also
work with s3 local files in both cases
they're just streams of bytes right one
of the cool thing about cool things
about akka streams is that you can sort
of abstract / exactly what's happening
you just have a source that produces a
stream of elements of some type you have
a sink that consumes elements of some
you have a flow that transforms now
elements of some type and some other
type etc right so let's get started so
here's the goal like I said many web
socket connections single kafka topics
something that consumes all of those
streams of messages and here's the event
class that will will be working with
well I've emitted all the serialization
deserialization boilerplate but we both
basically both be using or be using JSON
in both cases so over the wire from web
sockets it's pretty simple right json is
the native format of javascript so we'll
just be taking events in that format
then we publishing them to Kafka in the
same format JSON that's a lot simpler we
don't need to work with Avro protobuf
thrift although we could and that will
just help us reach do that stuff so
boilerplate as always right so we're
working with an actor system first
that's where we're going to get the
actual actors in which we run the akka
stream streams will be using the
dispatcher from that as an execution
context and the new thing the thing that
you probably don't recognize we'll be
using a materializer that takes the act
akka actor system as an implicit
parameter and it uses that when you give
it a blueprint describing your stream
processing graph and these are all
immutable functional composable etc
blueprints it will create a actual
running stream processing graph using
the actor system right so these will
always be in scope if you wonder how
something's working and it doesn't seem
to have all the context required it's
probably pulling Zhan the end from this
implicit context right so first we're
going to need some service to publish
the kafka obviously we need to publish
the messages received by a WebSocket to
Kafka we're also going to want to look
at Kafka and make sure the messages get
there and later we'll be doing a little
low testing so I had some trouble
getting everything working with a screen
size this is most of this is boilerplate
though really so published for example
it creates a sink using the reactive
Kafka library and just sort of abstracts
over a lot of like the producer
properties and wraps incoming messages
in producer message of E which is just
the class used for messages that are
produced that holds various metadata
keys etc and it can serialize anything
that has an implicit type of rights so
that's just used by play JSON to say
I know how to turn in an instance of
type T into a JSON object and then vice
versa for consume anything that can be
read from json it can be read from Kafka
and there's a little more boilerplate
there with consumer properties the topic
from which we read messages the group ID
under which we read messages
deserializer etc right so I've also
added some little ascii art graphs that
are designed to show for what these
different stream processing graph
components look like at the conceptual
level so publish events from the kafka
client you can see there that's a sink
it consumes events objects of the event
type that it showed you earlier
publishes them to Kafka and consume
events it is a source it consumes events
from Kafka and amidst them to be used by
some other component in the stream
processing graph now these aren't
runnable graphs yet they're just
components of graphs you notice they
have like hanging inputs or outputs we
won't be able to actually use these to
create a running stream processing graph
so ideally we could just for every
incoming WebSocket connection and
connect it to publish events but that
would be very inefficient because then
for every connection we would have to
create one Kafka producer there'd be a
lot of wasted resources there so we're
going to do instead is use source Q
right so it's very simple trait all it
is is something that can be offered
elements of type T returns a future
boolean true if the element was added to
the queue false if it wasn't fails if
the underlying the thing underlying the
queue has failed so we're going to do
we're going to create a Kafka publisher
graph that consumes element by element
spice or skew and that publishes them to
Kafka then we're going to run that graph
this is also snipped slightly off anyway
this is the same as the publish events
that you saw earlier righty right right
there and right so what that does is it
constructs a buffer size 1024 if it
overflows we'll use a back pressure
strategy and connects that Q by that
buffer to the kafka so you can see the
ASCII art graph here source Q event
messages or events go from the source q2
Kafka with
as part of the source Q source right so
then as I said you can't see that here
cuz it's yes so this creates a runnable
graph so this is very similar to a
process in Bruno's earlier talk sort of
or more like a task but anyway it's a
thing that can be run that gets you a
source q Levin more like a task actually
so then in the next line we run it we
use the aqha streams materializer
implicitly and we get a source cube
because that's the materialization
parameter of the runnable graph so we've
created we've defined a graph blueprint
that looks like this then we've run it
to get our actual source q we're going
to be throwing things in that source
queue to publish elements from web
sockets and I'll explain what that's
necessary later right so first we want
to sink that will write every element it
consumes to the source Q we do that by
just taking a flow of event mapping over
it with map async which takes a function
from t2 fuge from a to future be in this
case we're just using source to offer
and then using n then to register
listener basically because if that
returns false that means your message
has not been and queued and we could
also fail based on this but that's for
this I've chosen just to do this and
yeah so we can also add throttles
buffers etc to the queue writer for
example to throttle the amount of
messages published by any WebSocket
connection per minute for again for
simplicity that hasn't been done and we
take that map async currently flow and
connected to a sink that just ignores an
incoming elements because we don't
really care about the boolean values
except in that they indicate that a
message was not added to the buffer
right so this point have created a sink
that consumes elements of type event and
public the source q events and correct
so next thing we need to do we need to
parse messages that arrive via web
socket so for that we have a flow of
message that's the representation of
WebSocket messages it can be binary it
can be taxed in both cases they can be
stricter streaming so we're just really
interested in strict text messages and
for anything else we've chosen just to
ignore them and here you can see parse
messages just uses the JSON parser to
turn these text messages
into JSON into events and then gets them
it'll throw an exception if the parsing
fails so this graph component it's a
flow from message to event will be
composing that with our sink of a bend
and some other stuff that we haven't
seen right so here's where we do this a
lot of the composition so using flow
from sinking source all that does is
just take a seemingly disconnected sink
and source combine them to make a flow
and sup and that flow contains parsed
messages and then source cute all right
so anything every message coming into
that flow what we said to parse messages
and then send to the source q sink and
then there's source top maybe which
never actually emits anything that's you
is just to keep the connection open on
the other side right it's now we need a
flow to handle incoming requests that's
routes seems like I've got about eight
more minutes but it should be enough so
all it does this is a bit of spray
routing dsl stuff that's not really
that's kind of out of scope for the talk
but there's an implicit converter from
that to a flow of HTTP request to HTTP
response and we're just using that as I
said earlier to create basically a
server that's really just a flow from
request to response so we run that then
we get our server that is passed to HTML
button handle and the reason we have to
use the source queue and all this
complicated stuff instead of just
creating a more complex graph that
handles merging is because we don't
actually have control over the
materialization of the of the WebSocket
handler flow it's this method this
method provided back HTTP response to
any requests to the ws path with an
attempt to promote that to a WebSocket
connection using the website handler
flow to handle every message right and
I'm just using a simple front end page
all that does is send one thing per
second to one test message with a random
client ID and a timestamp and Kafka
listener that I'll be running in
parallel that just logs the messages
from Kafka rights so we're running the
streaming upload server and we're
running the kafka listener and I also
ran the street the load test in the
background so there's some messages and
Kafka hasn't seen yet let's hit
localhost a few times and so the page is
incredibly send
it doesn't actually have anything except
JavaScript but you can see on the
console that is repeatedly sending
messages and you can also see on Kafka
that it is repeatedly receiving messages
so the purpose of the load test though
is like this is this is kind of cool
right like I made like a toy project I'm
kind of wanted to I was curious to see
how many how many concurrent connections
that could handle like whether this is
something that I would like use demo
something or whether this is something
that could actually stand up to like
production loads so this load tester it
uses it uses the WebSocket client flow
also provided back HTTP I've emitted the
full load tester code and this actually
is going to be running for a few minutes
so it's the perfect time to ask
questions you can see here that this log
shows all the web socket connections
that have been accepted and here it will
log how many messages send and how many
are received right so this is kind of my
experiment and fitting a more complex
thing at a lightning talk and glossing
over a lot of how it actually works or
the deeper things about how akka streams
work how materializes work what
questions do people have is there
anything I cried so here we can see it
said 600 5536 events and received 600
5536 events so the load test sends
messages via web socket to the server
and then listens to Kafka for those
messages and upon receiving all of them
terminates so we can see like it was
able to send six sixty thousand messages
within a reasonably short time span not
just like one or two I believe I have
about four minutes left if anyone has
any questions anything i lost over that
I shouldn't have so either I did really
well or no one understood what I was
talking about cool
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>