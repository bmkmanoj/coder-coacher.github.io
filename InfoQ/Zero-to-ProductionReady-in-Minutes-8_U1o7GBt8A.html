<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Zero to Production-Ready in Minutes | Coder Coacher - Coaching Coders</title><meta content="Zero to Production-Ready in Minutes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Zero to Production-Ready in Minutes</b></h2><h5 class="post__date">2018-01-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8_U1o7GBt8A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thank you all for choosing to come
listen to this talk about how we help
enable developers to take applications
from zero to production ready at Netflix
I send you to mentioned my name is Tim
Bozarth I am here representing and
sharing the the hard work and
achievements of the runtime platform
team at Netflix so I'm guessing that
since you are here at this talk and here
in this track you're all interested in
some way of thinking about how do we
help level up and how do we help enable
enable our engineers to be more
effective and more productive this time
goes on and that's really exciting for
me because the vast majority of the work
that we do in our team and really our
focus in the last couple years has been
this exact concept how do we help make
our engineers more effective and more
productive so the talk I have today I
broken into four different sections the
first one is where I take a little bit
of time to share some of the background
about Netflix's platform machinery and
sort of our infrastructure
the decisions we've made and also some
of the challenges that precipitated and
then how we have reacted from those
challenges really the work that we've
done in the rest of the talk
so the second section will be talking
about how we're trying to encapsulate
and make best practices easy and make it
easy for anybody to access sort of the
cumulative knowledge that we've gained
over these years the third section is
this idea of how do we get rid of
handwritten client logic client library
logic this might not make sense right
now but I hope it'll make more sense as
I sort of describe and show you what
that architecture looks like and how
these things played together and then
the final section is a little bit less
technical but it's still really
important it's a bit of an exploration
of how we in our own team and in our own
platform space have given up and worked
away from some of the homegrown
technologies that we built and that made
us successful and learned to stop
worrying and start adopting shared
solutions in the form of open source
technologies or other commodity
solutions so let's just dive right into
this first one right so it was the best
of times it was the worst of times for a
little bit of background you might have
seen a slide like this somewhere else
but this is just roughly some info about
Netflix's scale to sort of frame set the
stage right we've got about a hundred
million users a little more than that
over a thousand developers might even be
over 1500 now 190 countries we serve
decent
bandwidth and then we've got more than
500 microservices currently in
production and hundreds of thousands of
VMs so fairly large from an
infrastructural scale and moderately
large from a human scale so we have you
know thousand-plus met up 2,000 plus
engineers some of you might be larger
than that some might be smaller now the
way that we do this then what I have
here is a hilariously simplified
architectural diagram of our system but
it's still fairly representative so I'll
kind of walk through this right now this
pointer probably yeah I can't see that
doing um so right up at the top we have
our users so whether it's a phone or
your laptop or your xbox or whatever the
case may be right you've got a Netflix
application you want to browse around
and find something to watch and press
the play button well what happens then
is any and all of those communications
while you're interacting with that
product pass through the internet pass
through our CDN really cool stuff going
on there and then pass into what we call
our streaming stack so the streaming
stack is all of the machinery necessary
to help provide that experience that
product experience and the streaming
stack is really broken into three basic
parts the first part is the thing at the
top
we call it the edge so there's a single
proxy which handles all inbound requests
filter those down and points them into
the next layer the next layer is what we
call the API layer so that takes all of
our internal stuff and translates it
into a communication protocol that's
good for communicating out be at rest or
something else and then you get into the
meat of what we call the service mid
tier and when you've heard about Netflix
or read about Netflix the odds are this
is sort of what you're thinking about
right this is this gigantic web of Micra
services almost completely Java but this
this is really the heart of the business
logic to heart of how the product
actually functions and how we deliver
what we give you and then of course
below that is infrastructure things like
our long-term storage and volatile
storage metrics dream processing all
that jazz because I don't like
incomplete diagrams there is also this
offline job thing which is all of our
batch processing which is massive
there's from a from a VM and
infrastructure scale there's a huge
amount of stuff going on here this is
like how we encode all of our different
streams into the many different encodes
that we have and how we do
recommendations and all of our general
algorithm offline work but today for the
most part the section I'm going to talk
about is this one
right here so everything going on in the
mid-tier those lessons that we've
learned and the reason why I'm talking
to you again is because I'm from the run
time platform team and the run time
platform team is ultimately the group
that is responsible and helping look out
and helping enable engineers across the
organization create and build these
applications productively in our
ecosystem okay cool enough of that
background so let's talk about some
investments that we've made over the
years so starting back when we very
first began to transition to the cloud
we started realizing the importance of
some of our core shared platform
machinery and this is a place where we
invested an enormous amount we were one
of the fairly early people to to really
go completely cloud native which meant
we also ran into a whole bunch of new
novel problems in the process so we
invested in platform we invested in
platform libraries platform components
and general strategies on how to help
make us successful things like how we do
service location and how we do dynamic
configuration how we do resiliency how
we generally handle our PC and
communication between these services
really really critical when you're an
ephemeral inconsistent non trusting sort
of environments that may just disappear
at any moment we also invested in
client-side resiliency the belief that
if we can't if there's something that's
trying to talk to another service and
that other service isn't necessarily
there that we should still do something
useful so we had this huge investment in
platform over the years and this
continues through today with the idea
that if we're good at this if we're good
at platform we can have a highly
available product and if we have a
highly available product we can do this
thing we call winning moments of truth
and this is really critical to our
business this is sort of an idea that
Netflix uses and talks a lot about
internally so winning moments of truth
it hopefully it's kind of intuitive the
idea is that anytime somebody opens up
their product opens up their phone or
whatever the case may be their
application that they can browse around
find something that's interesting to
them press play and it just works right
that's that moment of truth so by being
highly available and investing in this
infrastructure we can win moments of
truth which ultimately get us money
because people are willing to pay for
that service that makes us keep going
and that's fantastic
right dun-dun-dun but this giant
investment in in availability this
obsession with highly available services
it created problems now the problems it
created are not ones that our customers
experienced but their problems that our
engineers experienced their problems
that we experienced so let's talk a
little about these challenges because
these challenges really formed the
bedrock of what we've been doing in the
last few years and ultimately what I'm
talking about so the first challenge is
this it's hard to take advantage of
evolving best practices and this is
really true in a micro service ecosystem
right as it scales as you go from two
services to ten to a hundred to several
hundred to thousands over the years
capturing the best practices as they
evolve with time and helping integrate
those best practices into how you build
your service it's hard I mean it's just
straight apart the next one is this so
owning client-side logic is complex and
stressful so if you are owning a library
that people use to interact with the
thing as opposed to a service there's a
different life cycle between these and
again I'll talk about this more a little
a little later as I get into the section
and then the final one is this is that
the non Java experience at Netflix it's
hard because as I described even today
the vast vast majority of that big
mid-tier is still in Java but of course
there's other business functions which
are appearing people who may want to try
to capture some of the value of those
services which exist in that big
monolith that aren't in Java they're in
something else right and so there's not
they're not really necessarily
accessible now the challenge with these
kind of the trick about these kind of
challenges is that they evolve over time
and as is the case for almost any
problem that develops with scale is it
it's not necessarily obvious that you're
having a problem is that it's something
that sort of just grows and you begin to
get used to and you begin to start
thinking well this is the cost of doing
business right this is what it takes to
develop at Netflix this is what it's
like building a Java service but the
reality is often those things are just
taxes that you are paying that are
accumulation or a result of the
decisions that you've made so a couple
of years back we we basically stepped
back our team again the
team responsible for creating many of
these problems creating many of these
investments in platform in the way that
people used them started to recognize
some of these challenges we were paying
in our productivity and started to
really focus on productivity so we had a
recognition and the recognition kind of
formed a few key ideas which then moved
forward the first one is this it's at
availability as table stakes right at
this point in the game thinking about
your service availability the machinery
that the machinery that we use to do so
the place that we were and even really
many of the things that are out there in
the world and machinery that any of you
can use helps you make availability
table stakes the key was that we needed
to start focusing on productivity and we
needed to really recognize productivity
as a first-class concept that was the
one of the core values that was one of
our core competencies as a company which
leads right into this next one is that
complexity particularly in client
libraries is the mind killer
so again all of these investments in
reliability and highly available systems
sort of layered on more and more
decisions more and more things that
every engineer had to be responsible for
this is something probably many of you
are familiar with but man this this idea
is so important is that complexity
ultimately is this thing which which
really cripples you and then the final
one is that Java is not in fact the
whole world right we've done a pretty
good job in taking over Java but there
were a handful of other things popping
up that were really critical to the
business so again we have this polyglot
based system as more and more things are
becoming important in key to the
business right we have to make sure that
we're supporting those people and also
enabling their productivity
and again right that's why I'm here
talking to you where the runtime
platform team trying to help enable all
developers across the company
productively create and build these
build this software in the Netflix
ecosystem cool ok so that's enough about
best practices or excuse me about about
our history let's actually get into the
meat of this thing in best practices so
let's talk about so best practices how
did we do this the idea for how we tried
to encapsulate and help present best
practices in a useful way
was founded almost entirely in the idea
of generators so I mean this is the
least of the first question what are
generators well this is sort of our
loose definition of them basically
generators are something that give you
an application on the done on this paved
road
in a matter of minutes which are then of
course begs the next question what's a
paved road hopefully this is kind of an
intuitive concept basically a paved road
and the way that I think about it is any
combination of opinions any tools or any
platform machinery the way that you
build software the operational lifecycle
of that software when this is something
that is supported by central teams and
when central teams can say if you build
your application in this way you're
gonna have a good time right you're
gonna need to worry about so much less
than if you want to build it from
scratch that's the idea of the paved
road if you want to go off it cool more
power to you but it's so important for
central teams to be thinking about this
great ergonomic smooth experience so
that's the paper up
so why generators hopefully this is
again fairly obvious right to help make
it easy to actually adopt understand and
build these production-ready
applications get something on the paved
road so the way that we did this quest
by adopting an awesome open-source tool
called yeoman maybe some of you are
familiar familiar with it maybe some of
you aren't it's basically just a really
simple workflow definition system you
can define workflows you can define
automations you can put these things
together and then you can just you know
run them from your CLI and makes it it
makes it really easy and intuitive so
yeoman this great little thing we didn't
have to build we could adopt paired with
best practices and a whole bunch of best
practices that we learned over the years
so let's talk about that so I'm gonna
tell you a story this is a story of what
it used to be like to build applications
at Netflix and my guess is that all of
you know this story in some form or
another the specific actors in the story
I'm telling might be different than the
ones that you deal with maybe today
maybe in the past but my guess is that
the themes are the same so the story is
that of building a new application of
getting started and wanting to take
something from you'd basically get a new
application out the door so what's the
first thing when you build you do when
you build an application well you
probably searched your knowledge base
right you search alright how do I go
there yeah how do I make a new
application we use confluence if you're
lucky right you probably find a page
someone in your team wrote if you're
really lucky to update it in the last
year right then when you make that
search you also probably found a page
from a team like ours being like hey
here's
best practices here's what you should do
you probably found one by the security
team and by the performance team and by
the analytics team and maybe by the
tools team and a whole bunch of other
people so you take all of that
information and you synthesize it into
the perfect understanding of what a new
application looks like or you copy and
paste your last application and you move
on okay so then you have to make a place
for your code to live we use stash maybe
use github or git or whatever you might
use then you have to make a place where
you do your builds again we use Jenkins
if you're feeling really ambitious maybe
you'll actually connect those together
so like your build status reports back
into your code review tool or and you
think that'd be pretty sweet you have to
set up how you test you have to think
about how you actually deploy your
objects or commit your objects and bake
them then you have to think about
deployment so right we use spinnaker an
open source tool it's pretty cool for
our continuous deployment maybe use
Travis or something else and this is
where you define right how do you
actually push this thing out to
production like if you're in multiple
regions are you doing multi region
phased window deployments or whatever
the case may be and awesome you are
ready to push to production or are you
because if your projects important and
your service is important you probably
need to start thinking about metrics and
dashboards about alerts and all your
baseline alerts you need to start
thinking about your security
integrations that are you communicating
with all other services correctly and
this is roughly the point where it is
completely acceptable to break down and
start crying because this process can be
and can be miserable it can be
incredibly time intense so we talked to
engineers around around Netflix and sort
of asked how long does it take for you
to get started and to take an
application and actually be ready to
deploy this thing so to sort of have all
of your operational stuff and all your
code set up and the answer was anywhere
from two to three days to two to three
weeks so that much time to really have a
fully composed application that they are
confident will be successful in
production before they even write their
first line of code so let's look at how
this works with generators and I have a
video here so we'll see if the AV
systems are smiling upon me does that
work Oh beautiful great so what I
actually did was just open up my
terminal and ran this thing I hope you
can kind of see it so I ran the
generator first thing it does is ask you
what type of project you want to
I selected one of the types it asks you
a series of questions like what does it
want to do on your behalf most of those
things that I just talked about oh you
you know defaults to all of them and
then it asks you a few basic questions
things like right what do you want your
base class to be named who supports this
project what's this thing gonna be
called in in your repo and that's it so
that's the sum total of the questions
that we ask takes about a minute to run
after that and I'm going to show you
what it produces okay so what the
generator produces and the way that
we've approached generators and
hopefully the way that any of you who
are thinking about this can approach
generators is really divided into two
sections the first section is code so
the first section is how your code works
and then the second section is the Eco
as an integration with your eco system
so we'll first talk about the code this
is a screenshot of literally the output
of the generator that I ran when I when
I recorded that video so what we have
here is a fully functioning application
yes it is a hello world application but
it's also a fully integrated hello world
a hello world application it is
registering with discovery it has all of
our dynamic configuration and
configuration hierarchies it's odd it's
talking to the to the metric system it's
talking the data pipeline and
persistence everything that's necessary
is they're all an engineer needs to do
is basically define their API so go in
and define how their API is structured
and start writing their business logic
the rest of its done and it's basically
just an encapsulation and putting all of
these components together that we've
learned and that you know that work for
us that work for Netflix that we've put
together into this equation so that's
the code comported code portion the
second portion is all the stuff I just
talked about right all the other things
in that story how you set up your repo
and your Jenkins jobs it's been a career
and all that jazz the thing that's cool
about this is that generators allow
allowed us and allow anybody to talk to
the teams that own and are responsible
for these individual components to learn
the best practices that they have
figured out over years of being immersed
in these things to take those ideas and
sort of wrap them up in one nice little
bundle which can be a piece a part of
that generator story and then can
actually you know we people can realize
can recognize that value can get it when
they start and potentially can continue
to use it as time goes on so when you
put these two things together right the
generation where person's just ready to
start and then the full ecosystem
integration what you get are happy
engineers right because they just got
between two and three days back and two
and three weeks back or whatever your
own experience and story might be but
wait there's more
and this is actually the part laughs
okay this is the part that's really
close to my heart
this is something that the engineers and
the people consuming this don't
necessarily think about but if you're
concerned about the health of your
ecosystem or your a central team this is
really important and it is consistency
because as again right we have a lot of
different micro services we have these
principles of freedom and responsibility
anybody can build in any way that they
feel is necessary and if you don't
provide guidance if you don't provide
that paved road a great way to get stuff
done what happens is fragmentation and
that fragmentation eliminates the
ability for you to have leverage as
central teams to start sharing common
solutions across the organization and
what's so great about generators is that
they allow us to create that consistency
while maintaining autonomy while making
sure that those engineers are not
disempowered because what we are doing
is installing components and this is
sort of a meta point but it's one that's
actually very important in as a team
who's been building and responsible for
platform machinery over this evolution
this this is one of the most important
things we learned is that what we want
to do is set people up with components
not a pass and when you're dealing with
a really broad and diverse ecosystem
customers that are extremely
heterogeneous and how they build
software components are the thing that
we have found have been the most
successful because what a PA's does is
provides in some form an abstraction and
if for some reason one of those people
out there who loves 9 of your 10
opinions but doesn't like that 10th
opinion needs to change it and that
opinion is behind the abstraction they
have to throw it all out so generators
let you actually piece these things
together where yes they are responsible
for a hundred percent of the code they
ship that means everything there is
there's and maybe there's some stuff
that they really don't know but right
that stuff's not breaking down those
things are they'll come with the
commitments from the central teams that
they're gonna work in the appropriate
instrumentation so this meta point is
something that is really important to me
is really important to our
organization as we figured out how to
actually leverage and support a large
organization that's quite diverse okay
so hopefully that at least is a good
overview of the way that we've
approached encapsulating best practices
and helping people get started let's
talk now about this next section this
idea of goodbyes handwritten clients so
if you have ever worked in the context
of the micro service ecosystem you are
probably familiar with how chaotic
system to system interaction can get
right just simply talking or interacting
with another service doing so reliably
in a way that you can predict right all
of these things are really challenging
so it Netflix right we've been doing
this for a pretty long time and we've
been moderately successful with it and
there are sort of two things that I
think of as foundations to how we've
been able to navigate these very tricky
waters of this traffic jam successfully
these two are DS of this the first one
is that at netflix every service owner
is responsible for delivering a client
library for that service so what that
means is like if you if you want a
service you also own delivering that
library so that anybody who's interested
in interacting with your service just
consumes that library and then calls a
native interaction right so this is and
I'll show you in more detail what this
means but this is this is something we
do this is something I thoroughly agree
with and there are other ways of doing
this but this is one that I think is
great and I think it's one that's worked
really well for Netflix so that's
principle number one principle number
two is this is that our clients defend
themselves from failure so that in the
condition at which some service a
downstream service is not responding
correctly in that large distributed
system with complex distributed failures
if something's not working we basically
expect it not to work and we do our best
to try to provide something useful for
the person who is trying to interact
with us so we have things like fall
backs and circuit breakers and a bunch
of really multi like multi-layered
caches and all of that jazz right these
are the this is this concept of
defending ourselves from failure again
this is something I think is awesome so
let's look at what this actually means
okay good you can see the gray so what I
have here is sort of an overview of what
it means as as an owner of a service so
the way I've done this is I've tried to
color code it right so you have a
service and you're responsible for the
client anything that's blue is something
that you
the owner of that service would be
responsible for it so let's expand open
what the client is to make this a little
more real so the client is really broken
into just a few layers the first layer
is your RPC internals because of course
you have to define how your client
actually interacts over the wire like
what's your wire protocol what's that
stuff on top of that you have a couple
things you have to deal with
serialization and deserialization
because of course you have an object
you're passing over the wire and you
need to translate that into something at
least understandable and then you have
the platform integration components for
in our case there's a bunch of stuff
here right again common metrics caching
discover how we do discovery how we do
load balancing fall backs tons and tons
of stuff and then on top of that you've
got that final layer which is any
bespoke client business logic you have
if for any reason you needed to
transform the information that was sent
over the wire to make it more you
interact like interaction friendly or
user friendly this is a layer where you
can do that so this is how this was in
the past right this is what it used to
look like when you own all of this now
the next bit of course is on the server
side and it's pretty much the same thing
but mirrored right again RPC internals
serialization deserialization platform
integration and then the whole reason
why you exist in the meat of all of this
which is your server logic right this is
this is the reason why you're actually
building that service and again because
I don't like incomplete diagrams I'm
noting that dependencies are there right
this is Java or whatever language you're
in where you're delivering a client it's
basically the same thing so this is how
things worked for a really long time as
you're responsible for both of these
with sort of no additional machinery to
help you share this burden so what are
the problems right well there's three
I'm gonna talk through each of these so
the first one is that when you need to
make API changes right it's a bummer
anytime you have a subtle change or
especially a breaking change you have to
now coordinate the delivery between your
service a thing whose delivery cadence
you control in your library I think it's
worth taking a second to note why this
is so important so there are services
and there are libraries and we're
responsible for both I'm some of you may
be responsible for both or one of the
other but the lifecycle of these two
different types of things are
fundamentally different
right when you have a problem in your
service and you want to fix it what do
you do track down the problem you write
some code you push that code you deploy
it and you celebrate right that's the
end of the day everything is great your
problem is solved when you have a
problem in your library you find the
problem you track it down you push your
fix you deploy that code you celebrate
no that's not what you do because that's
not that's only the beginning you don't
get to celebrate until the last person
out there stops using the version of the
library that had the problem in it which
now means you have this miserable
longtail where you have to start working
with people and begging and pleading and
doing anything you can to try to
automatically eliminate that library
this stinks right so the idea here is
that the more code the more
responsibility we have in that library
lifecycle the more pain you're exposed
to the more we want to try to do to help
shift it to the server side so that's a
problem the next one is that there's a
lot of hand written RPC related code RPC
n platform related code so again you're
still thinking about serialization
deserialization you're still thinking
about like how that object looks
whatever that is and how you integrate
it and the ecosystem your common metrics
all of that jazz right this stuff was
handwritten because these clients are
handwritten and then of course the last
bit is there's not really a great
cross-language story is we're a Java
shop well we used to be a Java shop
right the mid-tier is a Java tier and if
you are writing your java service and
you're writing a Java client you are a
happy camper right up until the point
where that person in node is like oh wow
that's service I sure wish I could get
information from that how do I talk to
you and like what do you do right do you
then go start building a node client
something you don't know do you tell
them good luck or something right these
are problems the key is that these are
solvable problems and this is this
really became the foundation of the of
the transition and the technology we've
been making in the last couple years and
that transition was founded on two
things two big pieces this is the two
technologies that we adopted that have
really been instrumental to how what
we've become
it is G RPC and protocol buffers ERP C
itself actually ships with protocol
buffers of course this is open source
it's Google's project and these two
things are really powerful and while
they ship together each of them have
independent responsibilities so
buff's responsibility is this is to
allow you to strictly define the
interaction model for your service so
you've got a micro service and you want
to communicate how people can interact
with you this is how you do it
G RPC amongst a whole bunch of things
also then allows you to seamlessly
integrate that service all of that
machinery into the broader ecosystem so
these two things together gave us two
big wins the first is code generation
but not the kind of code generation I
was talking about earlier not the kind
of best practices code generation but
actual like literal server style server
and client code generation and then the
second one is a new abstraction layer
and it's a really fantastic abstraction
layer so let's talk about each of these
two and how we recognize these so with
code generation at first there is the
proto right and with that proto that
thing that stands there we kind of like
bang some sticks on the ground and we
can actually start to generate code and
G RPC has these things built in so let's
take a look at what an application or
what a server and a client look like
when you have proto and when you have
code generation in place so here's that
same diagram but now modified to
actually show the new blue right what
it's like for people who've been
building services in the last year or so
so the first thing to note is yes in
fact there is a new box so there's a new
area of responsibility down there in the
bottom left which is that you need to
define in a in a in a single autonomous
unit what your actual API is that the
proto file for your service you have to
define what your interaction model looks
like
so that's the proto from that though we
can run code generators and the code
generators out of the box are capable of
generating your entire client for you
with some really awesome backward and
forward compatibility promises so
there's a whole class of changes that
you don't need to worry about it also
then it's capable of generating your
server stubs so you can actually you
know have some of the service stubs
implemented for all that communication
all serialization deserialization
handled all the platform integration
handled with the next slide and this is
just this like this this really
incredible thing it leaves you with
basically only focusing on writing your
server logic so all you have to do is to
find that API and then implement out
your server logic the rest of it comes
out of the box this is this is really
great
so again because I don't like incomplete
diagrams I'm including one other little
thing in here which is your bespoke
client logic of course you can still do
that right you're free and responsible
if you want to you can wrap that
generated client with anything you can
imagine
please to hoto like I think we've
probably been approached in the last
year than many people who've been
building these services a few people
thought they needed to and after a
little talking and sort of understanding
more what it's capable of you they
realize they haven't needed to so at
this point in time we don't have a
single rapt client library and why is
that important well because if you wrap
it and you start putting client logic
out there you're right back in that
library delivery right back in that
library cadence again you're back in
that lifecycle that's so hard to manage
or you're begging and pleading so that
is what cogeneration what proto really
enables so what's that next thing that
abstraction layer well it's called
interceptors and interceptors basically
allow you to encapsulate common patterns
so any of those common things that exist
or occur with any communication that you
do over the wire any service to service
communication interceptors allow you to
implement in a really awesome flexible
way that are outside of the users normal
concern domain yes again it's part of
their application code it is shipping
but it's in the RPC internals themselves
which almost no one messes with and with
the internal teams being extremely
diligent around how these things deploy
are you know or take responsibility for
and provide guarantees that they
continue to work so what are some
examples of interceptors well here's a
handful these are things that matter to
us some of this stuff these some of this
machinery the concepts are open-source
we've talked about it in the past right
fallbacks advanced caching how we do
retries injecting failures into the
stack hedged requests to circuit
breakers on and on and on there's a ton
of these things so let's actually make
this a little more real because I can
tell you that interceptors are a cool
thing but they may not be obvious how
these really impact the day-to-day
development lifecycle of engineer so I
have a screen shot I have a screen shot
that I mosaic so hopefully you can't
actually read it this is a screen shot
of a completely typical application this
is just one of the random ID's here to
your apps I mean I I like selected it
with the dart board basically where I
went in and saw
that they have some caching and this is
it so this is a cache implementation
it's actually one third of the code it
was all handwritten I if I showed it all
it would've been too small to even make
out but basically this implements a
two-tiered cache I think this one uses a
like a guava cache and then one of our
persistent caches we call it Eva cache
so they had to write a lot of this code
for any service that a person creates
right if you want to have caching or if
you want to use any of your custom stuff
or you had to write some custom code
again something probably most of us are
familiar with that's a bummer because
again this is client code that's out in
that lifecycle you got a bug in it well
welcome back to the client lifecycle
what's it look like now with G RPC in
front of us so this is it it's seven
lines of code in total and it's broken
into two pieces one is in the proto
definition itself so right up in the top
you can see we basically said this
method like if you've got your foo get
video method you're saying okay this is
a method that's cashable until you
define it as cacheable and then you can
define your key and in your key
definition you can use any variables
that are on the request object like
interpolations of those variables and
cool you're good to go and then the next
bit is that what G RPC is allowed and
what we've been able to do is build
simple client configuration wrap around
it where now if you want caching you
literally write a single line of code
per cache so in fact this is not one
cache this is a three-tiered multi-layer
like right through read cache where the
first thing it does is it checks your
request variable so we have an
interceptor implemented for request
variable caching another one for the
guava cache and then another one for
that kind of custom internal one the CB
cache again this functionality what this
functionality is now exposed via
configuration all of those challenging
things that had to be hand written you
can expose via configuration when you
have this kind of this kind of layer
this kind of abstraction in place and of
course G RPC loves languages I think I
took the screenshot this weekend this is
like the various languages that they
support so if you have a proto you can
then generate your code that that client
library that fully supported client
library and all these things ok so let's
step back a second because I don't want
this to be a sales pitch for G RPC
although that works great for us
the reality is proto and g RPC was
the decision that we made after we
compared a variety of different
technologies but regardless of your
stack well actually not quite regardless
of your stack there's really three
components which if you have you can it
you can basically achieve any or parts
of these values and those components are
first in IDL right you have to be able
to define how your service interacts and
interfaces with the world it can be
proto I mean there's a bunch of them out
there but proto's the one we like the
second thing is you need to have some
kind of code generation which takes that
ideal and transforms that into useful
code meat and then the third thing is
you need some concept like interceptors
and you could call them a variety of
things it could be interceptors or
filters or mix-ins or basically any
layer that you have that can apply logic
at the data can apply common patterns to
any request occurring across the
ecosystem that's really it right if
you've got those things so that's it for
the technical portion so I hope that the
sort of the description of how we've
encapsulated best practices how we have
helped eliminate the need to handwrite
clients while also still achieving that
great value of providing clients to your
users so that they can just use native
language constructs is in some way
helpful or at least relevant now this
last one this last one I hate to say is
entirely obvious so everything that I'm
gonna say here is obvious it's also
really hard and I've lived through this
a couple times so I can see so what I've
got here is a picture of diminishing
returns it's probably something all of
you have felt at some point or another
or you have a project you have a
technology you have machinery you built
at some point that was amazing right it
saved the day and helped enable you to
become what you are today you have great
pride in it and as time goes on as you
continue to invest in it you realize
that this thing is no longer quite as as
fresh as it once was
it's you're investing more and more
energy to get any unit of done to do any
unit of value done and you know so this
is just the lifecycle of ultimately
anything the sad irony is that most of
us don't actually notice there's a
problem until we've gone over at the
other side and we start getting into
that like dreaded area of the more you
invest the worst things get right sadly
we've done this a few times
but the key is that you at least are
thinking about this you're starting to
ask yourself those questions so there's
an idea and this is something I talk a
lot about with my team something I think
a lot about is like with every step you
make comes the decision to take another
step and that as you are doing it as you
are investing in that thing as something
smells a little funny or maybe even as
nothing smells funny at all you are
actually making a step to continue to
invest in the same technology to double
down or do what to do whatever it is
that you do and if you are doing that
purely because that's what you're used
to doing you are susceptible
is a powerful force it is also a
terrible strategy
it's an incredibly dangerous strategy
too because it's so easy it's the one
that is it's the direction that you're
already going and this was so important
for us to recognize that we had
organizational inertia with a handful of
our investments these things that are
maybe maybe even open-source like things
that we've open-source things were proud
of things that were really great but we
have outgrown with scale and with as
sort of the needs of the business have
evolved so there's this one principle
that we think of a lot in Netflix it's
this idea of favoring commodity right if
it's not our core competency don't build
it right we don't run data centers
because it turns out Amazon is way
better at building data centers than we
are and that is fantastic
right that's allowed us to actually
focus on building a product the same
thing is true for many other elements of
what we do right it turns out that
building internals of our PC right
building actual wire protocols is
something that a handful of other really
great people have done and that we
didn't need to do anymore
even though the ones that we had built
had carried us as far as they carried us
right there was just more that we can do
by actually setting down the things that
that helped give us that pride and help
to bring us to where we were and
starting to look at other alternatives
to recognize that we were needed to take
another step ok so let's wrap this thing
up if there's one thing that I hope all
of you recognize it's that everything
I'm talking about here is done I know
that it sometimes it's a challenge with
with talks and there's a desire to
really like get out there and talk about
things that you're just starting on but
everything we've done here is is really
complete now more
10% of the traffic in efflux is using G
RPC I'm hoping that fairly soon that
number is much larger and we've got some
really great trajectories we've had over
800 projects in the last year with
generators more than a hundred of these
things are deployed and everything I'm
talking about has been the default way
that we've been doing business for the
last six to twelve months why do I
mention that because I hope that least
you can gain a little confidence that
these concepts have been tested at post
at both infrastructural scale and also
human scale and so far they've worked
really well this has really been that
sort of the path that's paving our
success in the future so what can you
take away well this is it if there's
three things that really resonate with
me here these are them right
cogeneration it's long and short-term
solution that's cogent from the
standpoint of projects but also if you
can literally n if you're building micro
services please please be thinking about
IDL's in some form or another they will
save you they will make you happy in the
long run and that last one is just don't
build stuff you don't have to if there's
great open-source projects or commodity
or shared solutions that you can pay for
do it if you can thank you so much for
the time I hope this is helpful
cool and I do we have time for questions
thank you Tim questions I'll also be
here since I think it's lunch next so
all right you're competing with lunch
yeah hi so we kind of in the step of
going to generation two of our platform
thing and what we we've kind of noticed
that how fast we can get going and
that's really nice but what we have is
it's the problem at it the platform that
we have and how we set it up for the
developers it's starting to bog us down
in runtime where I mean it is
dependency-based so updating the
dependencies and I mean it's true even
if you generate that you yeah how do you
how do you do their regeneration and how
do you make sure that the components
that you build and provide that they
stay fresh and how do you introduce new
functionality and keep them up to date
and how do the developers like it and
how do they handle it okay there's a lot
of questions mixed up in that one so
without trying to answer how do you do
dependency management well in Java
because I don't think anyone's answered
that one well yet I'll try to touch on a
couple of those ideas so cogeneration in
the way that we do it and I'm going to
talk about from the best practices
standpoint rather than from the IDL
standpoint so I think that's what you're
really talking about cogeneration is
broken into two parts the first is the
ecosystem integration and then the
second is the code the ecosystem
integration we can actually do some
really cool stuff where you can continue
to generate with time sort of delegate
to the generators how you're how all of
that machinery is set up but the runtime
side is a little trickier so we do not
attempt to allow you to like have a
project that you've implemented all your
business functionality and then you run
the generator again and like somehow
magically decouple the machinery that
you have like your platform components
and some of those core runtime pieces
from your business logic the division is
not good it doesn't exist that that
would
be a pass right if you want to if you
feel like you can absolutely a hetero or
homogeneously across-the-board update
those things you're sort of thinking
about something else now that doesn't
mean that there's not in a deep deep and
broad investment and how do we actually
help people evolve forward how do we
help make changes either to the core
platform components or the dependencies
that they take and dependencies in the
core platform components actually have
different ways that we do them at the
ways that we manage these things at
Netflix so the core components well it
comes with blood sweat and tears mostly
from our group which is that when we
make changes which there is no good way
to automate or to try to propagate
across the ecosystem we figure out how
to transform them we built we've had in
the past we invested in building some
machinery that does automatic migrations
so like it'll find code patterns and
migrate those code patterns submit pull
requests so that people can actually
take that pull request test it see if it
works
apply it move forward be happy in cases
where that's not possible
hopefully those are not super broad we
then actually work with teams to try to
run change campaigns or figure out how
to do those what that really means is
that when we make changes to the core
low-lying infrastructure we have to be
extremely cognizant of the distributed
cost that those changes make which mean
this stuff has to be really solid really
well thought out
and done with forethought now
dependencies on the other hand
dependencies change all the time so how
do we do that well that's one of the
domains and we have a tools group that's
really focused on dependency management
dependency visualization helping with
dependency migration how you lock
migrate versions and all that jazz doing
version reconciliation they've got a ton
of machinery and a variety of different
components some of it opens some source
some of it not it's not particularly my
domain expertise but all of that
machinery is then plugged into the
actual development lifecycle of any
application that somebody's building so
I hope that in some way touches on your
question I mean it's a big question so
I'm happy to talk more afterward if
you're interested
which team or how does the ownership
work for the interceptors is that the
runtime velocity yep absolutely so again
the the reason why that makes sense to
us is because in the past we we built
all of the direct machinery and the RPC
layer that was actually exposed to
people in their day to day lives this
was just a perfectly logical extension
of that as we then removed that stuff
from their primary concern like from the
obvious domain of concern that they have
yeah hi what strategies do you employ to
support multiple languages with
generators and what balance do you
strike with opening up the floodgates
versus trying to support your developers
so this again touches on two sides
because I talked about two classes of
generators the class of generators that
encapsulates best practices in the class
of generators that generates code
fortunately one of those sides is easy
in fact for multiple languages the code
side is really easy because they out of
the box with proto and G RPC together if
you write once you define your API
regardless of what you build your server
in if it's in Java or whatever you can
actually generate a client in native
languages for interaction and all of
those like I don't know it was like
twelve or something all those different
languages so that part's fairly easy I
don't want to glaze over an important
note there is that if you have
interceptor logic that logic those those
filters those interceptors do actually
need to be written in multiple languages
or you can use things like a sidecar
like if you're looking at something like
Envoy and a service mesh you can
actually kind of scoot that off to the
side the other half though the best
practices how do we do that well we
haven't gotten very far in the other
languages and capsule ating best
practices yet the reason why is because
we don't really know the best practices
yet and when I say we I mean sort of
abroad we that's the Netflix we also the
runtime platform we we the runtime
platform back sorry runtime platform in
Netflix really understand the best
practices in Java but we are just
growing into the server components of
other languages including nodejs of
which I think there was a talk yesterday
about what's going on in the node.js
infrastructure right up at the very edge
we even have a fairly decent number of
Ruby services I rail services in one
side and they're just now
of figuring out what are those best
common practices as those begin to
solidify is the point where there's
enough convergence and enough knowledge
that then we'll figure out okay how do
we actually approach encapsulating those
best practices together in a good
defined paved road style way so what's
your service versioning and D Co
strategy like older versions tend to
have a long tail I'm sorry I missed a
couple words there commissioner so uh
what's your service versioning and D Co
strategy yeah older versions then tend
to have a long tail how do you manage
that
sure versioning strategy I freedom and
responsibility it kind of varies so the
way that we handle it internally is we
do our best to use semantic versioning
and take fairly serious semantic
versioning concepts we do our best to
really make sure that anybody that's
using IDL's also understands at least
the principles of them because again
there's some really great backward and
forward compatibility promises that that
proto and GRP so you make
decommissioning well it's a handful of
things sort of as I described earlier
how do you deal with the pendency is
changing dependencies in the need to
eliminate libraries it's there's no one
answer it's a multi-pronged approach so
the first is like you can use you can
actually submit changes so you can try
to do sort of distributed refactoring
concepts we have a thing that we employ
at Netflix that we call the quarterly
deprecation cycle which is and basically
every quarter anybody who has one of
those things that's in that long tail
that for some reason that you just can't
knock it off they can basically define
that that version and anything - it as
this is no longer supported if you are
consuming this and if you're running in
production you can't expect it to
continue to work and then those
communications like there's a bunch of
scanning to determine all of the
dependencies across the whole ecosystem
messaging goes out to those people
they're told hey you're behind here
please do your best to update I know
maybe you haven't touched the service in
six or nine months but at least just a
remake and redeploy like there's a
handful of different components that try
to help when you have that large
distributed complex tons and tons of
dependency based ecosystem
guff I'm sure the question is do you
have do you have any default
interceptors for D RPC that everybody
has good to consume yes well has to so
the question was do we have default
interceptors that everybody has to
consume has to doesn't really exist at
Netflix but yes so is there stuff that
everybody does consume yes so state said
you you you actually just touched on it
so request context there are a few
things in there so we do distributed
request tracing and then we also take
certain action on request Conte on
request information which passes down
the entire stack so yes every single
service that exists in our mid tier has
request context either forwarding or
interaction the ability to write to it
and what we call salp it's just our
distributed tracing machinery beyond
that what's that know so the question
was does everyone have to define it
themselves and the answer is no it
figures it out automatically
so the interceptors that we have
basically no one information is there
and are already pre prepared to pass it
along I think the only person that
actually does there's only one layer
that ever interacts with those things
directly and that's the proxy right up
at the very very very very top they set
some stuff but even from there there's a
handful of defaults that they don't even
need to think about by the fact that
request context forwarding and
distributed tracing exist across the
whole ecosystem they just propagate
naturally so cool it's lunchtime you
guys can go I'm actually just gonna sit
up here if you've got questions and
would like to chat more I'm more than
happy to talk thanks so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>