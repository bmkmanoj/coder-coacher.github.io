<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>H2O: An Open Source Prediction Engine | Coder Coacher - Coaching Coders</title><meta content="H2O: An Open Source Prediction Engine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>H2O: An Open Source Prediction Engine</b></h2><h5 class="post__date">2014-05-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oPpii4WZ90I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thank you good evening my name is
Michael mahalo and I'm from his data
thank you for irritation from three and
I would like to speak about how to make
a better prediction and we've helped of
Scala so the first the first part of the
talk will be about our scala api which
is still in the progress and we are
trying to make better better and better
and then cliff will explain some
technical details how to how to tune JVM
to its limits so why are we focus on
better predictions you know Naruto there
is big hype about Big Data lot of people
are interested in there a lot of
companies connecting big a lot of data
so they are trying to participate in
this world and make them better and make
them better made them better by frame
for which is called h2o h2o yeah it's
open source framework which in fact
provide provide in-memory computation
and friction engine and I was asked to
play a short video which will introduce
which will introduce oh where is it
which will introduce each toe ah here is
it so I will refresh the page and it
just one minute video explaining all the
aspects of h2o so your business has
massive amounts of customer data stored
in Hadoop that you need to analyze the
more data you analyze the better you can
predict how to serve your customers
but organizing and analyzing this data
requires complicated math and modeling
this data to come up with better
predictions usually takes days to weeks
and is often error-prone so what's the
best solution quite frankly it's h2o
from 0 X data the industry's first open
source prediction and math engine that
will enable you to make better
predictions and build accurate models
faster and as makers of the parallelized
modeling and scoring in Geneva 2 we got
you covered here's a real world example
let's say your business is trying to
understand the best product placement
for optimal customer engagement so you
want to model the interactions of your
customers on your website to make better
predictions on what they want to do next
well h2o allows you to model all of your
data with better algorithms using mini
machines this way you won't have to
sample a smaller data set for
performance reasons h2o then allows you
to score hundreds of models in
nanoseconds and deliver better
predictions to your business the reality
is data is messy in so many hours of
data science go into moving big files
and lunging missing features and that's
why we design each two O's data console
to make those task a piece of cake with
in a familiar interface simply put h2o
makes big data science well simple so
when you're ready to mine the true gold
hidden within your big data you're ready
for h2o
okay so so there was a short marketing
video about h2o I liked the video
because it's really cool with a lot of
presentation a lot of animation and soon
okay can you see that okay so i will go
back to the slides and make some
technical details about each toe it's in
fact job part 4 so we are running on GV
m and we are doing in memory computation
we are not like Hadoop we are not
storing any temporary data to HDFS or
some some disputed storage but we are
loading data to the memory and then make
quick fast computation over the data
with lot of performance tuning this in
fact supports different styles of
programming but you know a lot of people
for this kind of computation are using
MapReduce now so it's also our paradigm
which we use but internally views
fortune framework from dagli and the
whole platform is accessible via
different clients one client its use for
testing is the Python client which
access hto rest api for doing the
operation and then we have also our
clients our client which use the same
way how to access all functionality of
h2o but you know there are some
limitation of rest api which we cannot
cross so we also provide java api which
is little bit lower level API how to how
to compute over the data and that was
the reason why we also want to introduce
the Scala some scholar I access to this
API because if you look at an example of
our internal API it was designed or it
is
design to be performant and I really
allows you to access whole data
distributed across the cloud in
low-level way so a lot of user are
confused by this API you can see there a
lot of weed word like Chang or set 0 at
0 and so on so we would like to cross
this low-level API and provide something
higher higher level and another should
be scala api we should we should
encapsulate this low-level API to nice
domain-specific language which we called
Shalala according to some restaurant in
mountain view which says soup so it's
favorite restaurant there so it was nice
nice idea how to how to call the
language so in fact sha allah is scala
dsl or internal dsl based on scholar
image which make abstraction over our
low-level computation api and the goal
of this api is just to make writing
MapReduce or computational tasks easier
and also manipulate live data in easy
way but still we want to stay in GBM we
don't we don't want to publish the
operation via rest api because we have
already have this api we would like to
stay in JVM and make the best of JVM and
then there is also a nice integration of
repeal into our HTO with you which
provides very how to play with this sha
la la so what are the basic concept of
this shalalala dsl we play with Tabler
tabular data
frames and if you are familiar with are
you can you perhaps know that frame is
abstraction of data which which consists
of vectors or columns each column has a
name and data type and obtain the data
so we do the same approach how to
represent data in sha allah and and we
call that distributed frame and in fact
the data in the frame and distributed
across your cloud i will speak about
that later and perhaps cliff or clarify
this in this technical talk then this is
this frame distributive frame is the
first class entity which we need to
operate over and then we need to provide
expressions how to operate with with the
frames how to access the columns how to
access the data how to make the basic
operation over data and how to make
operation with the frames and also the
language should provide easy access to
our low-level API so if there is some
functionality which you cannot solve
with sha la la you can still switch to
low-level hto API and just implement
your functionality via API so for free
operations I will just summarize several
of several of them and then show more of
them during the demo but they are
divided into few steps you need to load
data parse data then you need to select
columns or manipulate with the columns
then you need to store data into our
internal store hto is using internally
dkb store which is distributed over the
cloud so dsl also provides the functions
how to access this decay in store and so
there are simple operations and in this
case you are motivated by our because
what of data scientist knows are and
they can develop simple or simple they
can develop not only simple analysis in
our so we like to provides the similar
commands in our squad dsl that if that
arise come look at the program in sha
allah it can read the program and decide
what is the program what the program is
doing and then we have of course some
distributed operation so we provide
currently just three of them you can
still switch to our low-level API but
currently we provide a map just for
mapping one given operation given user
method over the mate over the data
filtering and collecting and you can see
that the api is still little bit ugly i
will speak about it later that there are
some limitations which we need still
cross but we are still working on that
so how how it is implanted internally
it's small magic more about the magic
will be in the in the torque of Klaive
but i will just clarify what the scar
dsl is doing in fact the basic thing is
that in h2o we are using our own class
folder which is responsible for lot of
things for optimization data structures
or generation parts of data structure
which we pass or
on the cloud and for some typing things
so we need to preserve the semantics of
class holder in cooperation visca so
it's one one thing then we need to
translate the scholar scholar a code or
the dsl code which user types we need to
translate to our internal ja api so this
is quite easy task but still we need to
live in the world of h2o which say there
are some limitations regarding what you
can pass around the cloud and what you
can do but still it's it can be done
quite easily and also in our low level
ja API we really focus on preserving
primitive types and computational
expressive style everywhere just to
avoid unboxing boxing so all the
translation from scala part 2 java-jar
part has to has to preset or Prentice
types so one example how filter
operation is implanted I will go to the
slide and filter operation takes a in
fact factor which is map over all the
videos in the frame it's one kind it's
one kind of the further operation which
we provides and you can see it's little
bit agree normally with expect here some
kind of closure but currently we have to
be nice to our class holders so country
disclosure is encapsulated in our data
structure and the code is translated to
roll of
our HTO API I wrote this example in
Scala just to see what is what we are
doing internally but the same version is
you can find Jack code so in fact we are
we have madhurima produced ours which
runs around the cloud look at the data
and run given operation the Maverick
stars as its Mario da so it has to
operation MapReduce we we are taking
classical mob reduce approach so lapping
over the values the values is a chunk
and chunk in fact it's part of data
which is distributed over the cloud and
it contains the rows from the data set
and under data you can over this data
you can do a lot of operations and then
for example here there there is no
reduced age because you don't need to
reduce data it just needs to compute
something in this case we are filtering
the values so we are just appending to a
new vector the values which satisfies
the predicate and then there is some
technical magic that you have to
preserve the naming of the vectors in
resulting frame the types the domains of
the vectors and so on more about that i
think will be in the talk of the off
cliff and I think it's time for demo so
I will try just this so what I did i
launched one instance of h2o h2o so I
launch Victor machine with some flags
and by default H to expose rest api and
we have some
app UI to access the web api so i will
go the browser and show you that I'm not
lying that there is something running is
one one way how to access h2o and I can
look to the store which is what is
inside and there is nothing nothing is
running and I can look to the clouds
tattoos so I have i right I'm running
the cloud of only one machine here on
this machine so I was go back to the
console and I will try to pass some day
some data sorry
so I I bar a simple data set which is
one of the favorite data set design for
fraction and in fact the data set
contains eight columns and four hundred
six rows and you can just to believe
that I load something i will go to whip
you I and show you that there is
something some data load it and it
contains eight columns and lot of rows
so now I can start to play with data so
i can look at the first column so it
contains the name of the data object of
the car in fact the data set contains
different types of cars with their
properties like year of creation wait
Power number of cylinders economy and so
on so i will try just show you several
operations over the data set i can look
for example of number of cylinders or i
prefer i prepare several examples here i
can make
look at the last column which is a year
you can see that the year is just small
number and i would like to rely to
transform this column to a new column
which will contain the full year dated
from a year 0 so I make transformation
and in ripple they are just showing 10
lines of the data set but just show you
more lines you can look at all the lines
of the vector which we created there is
a important point that at this at this
stage we are not creating any frames so
if we are go if we go back to the
inspect or store view you will still see
that there is only one frame one data
frame which was created and I also can
access this view from the console so I
can look at the number of the keys so
there is only one key which is
representing one data frame which I load
it but I can also look at our internal
data structure so access are our
internal java api so i will say show me
all the keys which are created in the kv
store
so you can see that there are lot of
technical or internal keys which are in
fact representing the parts of the data
which are normally distributed now they
are just store in one GBM and also the
part of data which represent the vector
which i created in ripple i can also
make a booty in vector from filtering
all the cars or accessing run victor and
making a boolean vector saying this car
has more than four cylinders or less and
internally it's in fact simple map
operation taking initial frame and
saying accessing the second column and
asking the second column if it contains
more than four cylinders or not and the
result is a new vector containing and
containing containing of boolean values
and I can in fact compare both columns
both created vectors and just make
just look at the difference between the
vectors and you can see that I'm not
lying that both vectors are the same and
in fact internally the operation of
booting predicate is implemented in the
simple map call and the same for also
for filtering so if I would like to
filter all the cars I can use the same
the same call just using the filter
operation and the filter filter
operation will go through whole data set
and look at each row and apply the brake
aid at each row and decide this row
should be included in the resulting data
set or not so if I compute number of row
of this filter operation okay it's
variable f6 so you can see that there is
195 cars which has which have more than
four cylinders so this is one way how to
manipulate with data but we H doe also
provides different kind of analysis I'm
personally involved on drf which is
distributed random forest implementation
pression algorithm which which we
provide so I will try to run the run
that over the car data and make a
prediction about the number of senders
so i will create some small data
structures so i will in i will create a
new frame which is called source which
will contain only selected columns so I
will skip the first column with the name
because it has no practice father
and I will skip the column with Sanders
because this is my response which I will
be preg think later and then I will
include all the combs so I created a new
frame in fact I can save this frame to
dick dkv if I want so let's call the
frame source hex this hex means that
it's our internal data format and by
this call I just save the created frame
to DK v so let's look here and you can
see ok there is some source to the hex
frame which contains only selected
columns and all the rows then I will
take the second column with the
cylinders and create a drf model based
on this data set by this column saying
take the source build Randolph Rose
model over the source and the response
column which is spread which is an
objective or direction is the second
parameter of the call and the last
parameter is the number of number of
trees which will be generated in the
resulting random forest I switch on the
logging so you will see that it's
something the engine is doing something
so i will launch it and you can see thou
several
several a lot my messages saying okay
and we are starting drf computation with
these parameters and then we are
building a collection of decision trees
and computing the some errors over tea
and some properties and the result of
this cold is drf model and it contains
all the data which you need to store for
this kind of models I was doing
regression so I can take the model and
ask the model what he thinks about the
given data and I will be lying little
bit here because I will use the same
data as i was using for training so it
will give it or the model should give
good very good prediction so I will
launch a scoring part of the model and a
very values the source data for the
scoring and there is one point which I
have to stress that the source it does
not contain the response cone so really
model has to take us to look at the data
and ask all the trees with the trees
things about the data and in this case
I'm accessing our Java API so the model
is the object published by h2o and the
score method it's not it's not a rapper
from scholar but it's really java
thought taking taking a frame as a
parameter so i JT depression i was and
the result operation is the frame but
here i leaked
the internal frame so I will
I will create this distributed frame and
you can see the result of the operation
so we say the model thinks about the
first row of the data so let's look at
the first row of the data the model
things that I am is some car with name
emc ambassador has eight columns and in
fact it has 8 com so I would like to see
what is the general error for this
prediction so I will compute the squared
error for each row so i will just take
one vector containing the response and
the bracket vector i forget this
response so i compute for each row i
computed of in fact square error and now
i'm interested in the sum of all these
square errors so i will launch Map
Reduce operation overhaul data and
compute the sum of that for this i have
simple operation prepared here and what
i am doing i'm collecting data over the
cloud in this case i'm collecting single
double and single double is computed for
each row in the in the in the given data
set in the given frame and it should
produce the sum of all all the all the
all the rows you can see here that i'm
using reduced operation which in fear
can avoid in the future just by using
nice types which will implement the plus
operation by default so there was a demo
of the dsl and if i go back to the view
web view you can look that there is some
entity which is called dr f underscore
and big string and this is not a frame
this is not a vector this is a model
which we generated by calling dr f api
and in fact is simple view of the model
which contains all the information and
also we can access
the code of the model so we can look in
fact a generated code of the model which
can be used later for some correction so
I will go back to the presentation and
just just tell you something tell you
something more about future plans in
fact the goal is to have the API which
is similar to scouting API but we still
we are still battling with some
limitation interviews by h2o as I
mentioned this corrosion and JT
generating of byte code and we would
like to support more distributed
operations more high-level disputed
operation you can still go to our
low-level API and share ideas is to
introduce algebraic operation as those s
they're introduced in archbold so if you
are interested in participation you can
simply get close our repository because
it's open source you can switch to the
hto scallop range and just play with
dead or very simple simply contribute or
there is also release which is Smalling
at the bottom of the order of the slide
which you can download and just try hto
scholar repel and try to try to play
with that so that's all from me and
cliff is waiting for his tall thank you
it's getting late so I will go quick and
then we'll go time for QA and so I will
skip through a bunch of stuff that's
sort of boilerplate here hope it's not
auto forwarding okay yes it's auto
forwarding okay so I don't know what you
did let's back up a little bit here okay
so this is sort of obvious we're open
source you can find something to get
there's a get that github but also 0x
datacom right on our front of our
webpage and you already know where a
platform for doing math so let's roll
along so I'm going to dive into some
details on the insides of the Java
implementation and the reason for diving
down there is to show you why we're
getting speed so when I say speed I I
mean that most of our math operations
are completely memory bandwidth bound on
the data which means it's it's
milliseconds per gigabyte as you do more
math per row that mummer about balance
might change might be doing more FP
versus memory but typically we're like
really really fast so when other people
say yeah would you big hatem really fast
we're usually 10 X 100 x I mean rural
we're fast so the basic notion that
McKell is showing you is that we have a
notion of a vector it's an array um but
it's a big array and it could be much
much bigger than a Java int that's why
the length is listed as a long and it
supports fast random access from any
node to any piece of data but we're
geared to running to linear access pass
over the entire data that's where you're
gonna get your most speed out so a
taxonomy there is a big vector it could
be a very large vector conceptually it's
a Java primitive or double but actually
it's compressed internally the
compression schemes are quite aggressive
and I'll talk about in a minute because
that's part of our speed and so we're
often seeing to 24 x better than gzip on
disk so better than gzip on disk in
memory while getting speed out of it the
vector is distributed across the cluster
and it's kept in the Java heap not off
heap and that gets us really fast access
to it from Java which makes for very
convenient programming
but it means you have to watch out for
GC and we've done that we've taken our
care that we can spill the disc is
needed and because we keep the data in
very large arrays of primitives typical
GC costs are very modest for a 32 gig
heap a full GC cycle is usually under a
second and they come around from time to
time but under a second I don't care
where as new gen collections are and
they're there blazingly fast a 200 gig
heap will have full GCS in there over a
couple seconds within the the same heap
will have a collection of vectors what
was typically called a frame and the key
notion here is that we are aligning the
rows within the JVM across this way so
that if you're talking about all the
elements of that car data set what year
how many stones that car have was its
weight displacement all that data is
local and one JVM and you can do some
math on it at you know memory bandwidth
speed so then yeah so it's a frame and
essentially you can think of this as
this a tabular data or it's a struct of
arrays instead of array of structs
that's the conceptual view of it within
that big ol pile of data we breaking up
the vectors into chunks where a chunk is
a thousand to a million elements
depending it's stored compressed and the
compression strategy is sort of key to
speed in a lot of ways basically more
compression is good because we get more
data / cache miss and it's how long it
takes to drag the data in from Emory
that sort of matters on things they're
all aligned going across as I mentioned
before and so you can essentially I
think you're looking at a struct that
you can read and write and toy with
directly like it's a struct in Java you
can write to it as well as read from it
and we'll follow the Java memory model
rules down the line all the way that
chunk is also the unit of execution so
one CPU will grab a chunk of rows and do
whatever math you're asking for all
across all those rows all in one set and
that means that it's also single
threaded on those rows and so there's no
synchronization games being played in
the
code that you're looking at you're
guaranteed single-threaded access to the
rows that you're you know hand it to
that granularity is big enough to cover
all our control over heads all the
management costs of launching a threaded
deal with a pile of data and small
enough to get good fine-grained dated
parallels amount of it because that 11
CPUs does that chunk all the other cpus
we grabbing their own independent chunks
and will typically light up all the
cores across all the machines in the
cluster and it'll do the math and one
big pass of the data and be done the
within this kind of taxonomy hto is
handling all the communications and data
management its handling all the
communication between the nodes as you
do reduces you have to roll things up
across the cluster and so on within a
single node we're using Dudley's fork
joining to do the chunk by chunk
execution where you're typically
expecting few thousands of chunks more
or less depending on this volume of the
data and that's going to be round robin
however fork join does it across however
many you know 4 8 16 CPUs you have we
typically see we get really good
behavior out of fork join except for the
cases I'll talk in a minute so there's a
taxonomy there's a frame it's a 2d table
where the cross is columns typically
hundreds can be thousands can be
hundreds of thousands but the rows will
run on to the millions and billions so
however much fits in all the RAM in your
cluster with in that collection a column
we call a veck internally it's a column
of data it's a big array that's working
up the chunks thousand to a million of
the pop I chunks a collection of
elements elements conceptually a Java
double how it's actually stored memory
varies according to compression strategy
of the moment but you can ask for back
as a double value or as an int and we
also support like the notion of missing
elements is really crucial for a lot of
the data science stuff so that said the
the platform as a whole can be
considered a couple different layers
there is this layer where they use the
rest in JSON that McHale talked about
earlier that's the layer where you treat
the box as a big box for doing the
cluster as a big box for doing math
load this data run a logistic regression
run a random forest get some results
cycle background add columns drop
columns manufacturer features run
another model score it go again you
might be driving that from the internal
h-2a console you'd be driving it from
Scala but you might be driving from our
or Python or Mathematica or whatever
here I'm going to talk about a little
bit more on the innards of the system
the data parallel coating layer and this
would be for people who are implementing
stuff at the Java level and so Mikhail's
doing scala to java and we have other
engineers doing our to java and i'm
implementing what other people are
implementing new math algorithms so
whether it's k-means or you know
gradient boosting or whatever that's the
layer you would talk to the system at
and then there's more complicated stuff
you can do if you want to get sort of
heads down in the system so I'm gonna
get one or two examples of the java but
this is a scholar group so i'm going to
leave it at that and then i'm going to
go dive into some of the gusts of the
system and and why things go fast and
then i'll stop and this will be like a
whirlwind tour of things really quick
because it's getting late we'll just
take a Q&amp;amp;A after that so here's a here's
sort of the simplest MapReduce tasks
that does something interesting and this
is an example from linear regression I'm
just going to accumulate the sums of
squares of a particular vector and so
I'm going to make a MapReduce task it
has a map and reduce map is a function
which takes a thing of type A and
produces a thing of type B in this case
type a is a double type B is also double
but it doesn't have to be and the next
slide will change it up a little bit and
reduce takes two things of type B and
makes one thing out of it and that's it
collapsed it reduces your data right so
this is a piece of code that will run at
the memory bandwidth speeds across the
cluster in like five six lines and
compute sums of squares across the
cluster you know sort of just like that
I can have a more complicated example in
particular i can have state here I've
got my type a is a pair of doubles and
my type B is this Java object called an
LR pass one which has three internal
fields and the map call is go do
something with all the fields it's go
scribble on
and the reduces take of this end of that
so that this is implicit and that that's
the other side of it and it will redo
the reduction and that will run across
your cluster doing all the right things
so within a node the reductions
obviously between two java objects and
squishing them together but across the
cluster one of the java objects cut
serialized and shipped over the wire and
then they got reduced on the other side
i'll touch this a little bit and then
we'll move on for efficiency we do a lot
of we do batching so a whole chunk is
passed into a map call it once and we
typically right at this layer but you
can write this one line it'll work but
at this layer you can see that there is
a loop over a chunk which is a thousand
two million elements and the app calls
here are doing the decompression
strategy which might vary from chunka
chunka chunka chunka right but this is
typically a few clock cycles to
decompress and pull data out the new you
math and go on the rest of the code was
same as last slide so that's sort of it
for the whirlwind java side of things
and I was going to dive into some
technical concerns so these are all
slides I wrote in the last half an hour
so pardon me if they're a little brief
or a little raw so so we're concerned
with big and whenever we're concerned
with big you're concerned with speed
because typically the problem is Big got
slow so how fast is fast typically we
have to see all the data and it's big
and we also typically have you know less
math than the memory bandwidth it takes
to drag the data in so if you're doing a
logistic regression you're doing what's
called Graham matrix it's a bunch of
floating-point ops for every row that
comes in if you don't have very many
columns it's not that many
floating-point ops and the cache miss
time from x86 to memory if far exceeds
the cost of doing a few floating-point
ops so you can decompress in the shadow
of the memory bandwidth right and more
decompression is better or more
compression is better because that means
you got more data in one cache miss so
currently we have about 15 different
compression schemes and we can drop
another linen and half a day or so and
they vary all over the map
there are obvious ones or just take the
data without compression but then you
might do a bias and a scale off of a
thing or you might say it's oh it's
bulli noleene so it's a bit set or it's
very sparse data so it's a run length
encoding going on there's a bunch of
different compression strategies and the
they're picked / chunk we inspect all
the data at the time we load it and
decided the best compression strategy
for this few thousand rows and we keep
growing them as we go the only thing
that's key there is all the
decompression schemes take a couple of
clock cycles to go if you compress the
data and then you can go to you lots of
time to do your math on it after that
serialization so series ations is funny
thing you might think with big data that
serialization is a key issue when you're
doing with a cluster and it is but for
not for the obvious reason you can't
send the big gate across the cluster or
you're just going to drown in individual
node in the data the assumption the
guiding assumption have to start out
with is there's just more data is going
to fit on your one machine so you just
can't ask for it all in one machine
you'll want to pass it through the one
machine either you want to keep it in
memory so you're not passing around the
big data but you are passing around tons
and tons of small data things that are
plain old java objects that are part of
doing your math there are accumulating
histograms and sums and variances the
building a grand matrix or they're
building no partial dependency graphs or
whatever they're building but doing
something with the math and that's
getting pasture on the cluster so we
have to have a fast way to send pojos
around the cluster since we already had
a bytecode Weaver in place for a variety
of reasons we wouldn't head them through
in serialization the bike rode Weaver
and it does sort of the fastest possible
thing you can imagine it writes the
fields via unsafe directly into direct
byte buffers there's one to buy token at
the start of any send of a nested Java
object that defines the entire layout of
the whole thing and after that is just
the data and then we compress that as
well because typically we have more CPU
the network bandwidth and so it's faster
to compress and send a compression than
it is to send the raw data the the
direct byte buffer access is all just
loads and stores and nothing else
there's no sports in there at all it's
literally take this field and lift and
stor it with compression as you went we
write into streaming asynchronous now
buffers there's multiple shared tcp
channels full app level recovery and
retry you can pull a cable on the
cluster and add it back back in five
minutes later and things will just
recover we do make a decision to send
small stuff via UDP and big stuff via
TCP and that's because there's a lot of
small stuff and UDP is usually very very
reliable as much faster than TCP and
that zone and turns out that TCP while
everyone knows UDP is unreliable that's
what the you is so we have to have a
reliability layer for that turns out TCP
is unreliable as well and in a couple
minutes in my labs or ec2 or ever I can
cause TCP to fail and when I say fail I
mean silently the sender will open a
socket write some data and close the
socket and the receiver will get no data
and no arrows will be thrown in either
side the date will just go into the bit
bucket be gone so you think of TCP as a
reliable communication channel in fact
it is not and that only happens under a
heavy heavy load but I can drive that
system to the maximum possible load you
can get and under that kind of scenarios
TCP channels drop silently so I have a
reliability layer built in for UDP and
for TCP it's the same player um a little
bit more on MapReduce a map call is made
once per chunk typically I have a couple
thousand per node you know more or less
an amount of data you've loaded but if
you've piled on a lot of data you'll
have thousands maybe millions and not
millions once you get really big boxes
and we're using fork joint for the fine
grain parallelism more on that in a
minute as well the reductions a little
different than the usual Hadoop
reduction people think about and setup
us running all our maps up front and
saving the data to disk and then the
shuffle and pull all back and do the
reductions we don't ever go to desk so
we reduce early and we reduce often
every pair of maps when they're done
immediately a reduction happens if
another pair of maps are run and they
reduce when those two reduces are done
they reduce again immediately so the
reductions happen sort of incrementally
on the fly as the data is as the math
has worked so you always keep the data
size if your final
crunch down to the smallest you can fit
in memory you know subject to the
constraints of running as much in
parallel as you can once you're done
doing all the reductions on a node it'll
go course over the wire and eat a log
tree roll up a people nor the log tree
roll up is every pair of nodes and the
cluster consider themselves buddies and
in one of them's the parent of a tree
and the two tree parents or our buddies
and go up again you're going to the
layer standard binary tree thing so that
the time it takes to do a fast math pass
will be a log tree walk across the
cluster then everyone does their math
and a log tree walk back over and you'll
be network latency bound sort of the
limiting and that's one of the reasons i
went to you to be packets for small data
if you're doing a small roll up a small
result like you're computing the
averages and standard deviations or
linear regression kind of thing you have
a small result it's all going to run in
UDP packets and that is your memory that
is your latency and getting that job
done fork joint experience I had some
some hard thoughts on this one this is
kind of fun stuff for joining sort of a
new framework in Java 8 but it's been
around for a long time and Doug Lee's
been working on it for a long time and
have a lot of respect for Doug and he's
done some really great stuff and I think
fork join really is good but it has some
interesting gotchas to it the good stuff
first so there's a learning curve and
once you're over it it's actually easy
enough to write and use we use it all
the time there must be hundreds maybe a
thousand or more uses a fork join
scattered throughout the code so it's
full-featured it's flexible it keeps you
know all the CPU is busy I can have huge
piles of tiny tiny jobs running around
or handfuls of big jobs it all seems to
work really well so in that sense for
coins good some hard experience comes
out of that blocking threads while
you're running on fork join is hard on
it because he has to realize a thread
has been stolen for blocking it usually
than I Oak Hall and start another thread
and so sometimes you get threat
starvation issues and working around
that some what's painful the recommended
thing is to rewrite your code
a style that uses with our counted
completers and now you're doing
continuation passing styles how many
people are here into a continuation
passing style is oh pretty good okay so
third half basically writing
continuation passing style and java if i
don't want to have thread blocking
issues and that's kind of painful to do
hmm you'll find that buried throughout
the code that happens in various key
algorithms because it's just too hard to
get it right and too painful to leave it
sitting the way it is there's no
priority queues there's is one big fat
happy q &amp;amp; fork joining but you have to
have thread priorities for a variety of
reasons in particular you've got a web
server and the web pages don't have any
priority they can't serve us a page you
can even a status update a-- you know a
polling bar moving across your browser
because you all CPUs ore swamp doing
work the web server can't get it you
know kind of clock in edgewise right so
we built priority queues into the system
built over the fork join layer and
that's also crucial and making the key
value store go fast kind of buried under
the hood I think Mikhail mention it we
have a key value store I'll claim it's
one of the fastest one gonna plan it I'd
love to go measure that but a cache
hitting put or cash hitting get or 115 a
nose basically hash table lookup but a
cache miss is going over the wire and
back and to keep that from causing a
circular chain of cash missing asking
for somebody else to do something around
the cluster there's priority queues
built into the system for the key value
store in exchange for that the cave east
or even when you cache miss is nothing
more than the direct ngaio UDP socket
back and forth round trip to kind of get
stuff done a few of the things I think I
going to pay attention to by default and
fork join exceptions are silently
dropped the thread that takes an
exception doing a piece of your work
just like goes back to the idol queue
and like wakes for the next job to show
up and that usual symptom of that is he
doesn't complete his job because he
threw an exception he doesn't tell
anyone he didn't complete it because he
just silently dropped it and and in the
end what happens is that all your
threads suddenly go idle and the job's
not done and we're all waiting for some
way
sky to complete something but he's never
going to win away so use just the
maintenance disaster you just have to
catch them track and log all exceptions
and and so that when the scene hangs you
can go back and look and see well who
threw an exception so we with a step
further after that and said I can't take
all the lodge exceptions and pass them
around the cluster roll them back up so
they get wreaths roan at the call site
of the original MapReduce task so
somebody says new MapReduce task do all
of a vector in big mouth whatever fine
that'll throw an exception at you
that'll come back and say hey node yadda
yadda yadda were there through this
exception and here's his stack trace and
it's just like a huge help in debugging
things try completes one of the key
things to count right it's part of the
learning curve once you figure out how
it works it's not so bad but in the
beginning you'll blow the counts on try
completes too many or too few and jobs
would complete out of order earlier late
or not at all so on so forth early on
when using fork join we did a lot of
fork bombings of ourselves including the
point where the CPU that the box was
unusable we had to reboot it so you just
have to watch that cap all your thread
pools and maybe throw warnings out
saying hey hey somehow I'm asking to
launch the thousands thread is this a
good idea right and when you cap thread
pulls you then can get into deadlock
issues because all threads are actually
blocked waiting for some other event
which is on another node whose all
threads are blocked waiting event
happening on your note and so in that
situation you have to have to have
priority queues which fixes some of them
or go to the CPS style to fix it again
so anyhow despite all that I would use
for throwing again it when it works it
works really really well and they're
just these edge cases you got to know
about to figure out how to work around
it okay and that's actually the end of
my quick past few things I have some
more java examples which i can step
through and i have some summary slides
so let's summaries real quick and then
we'll go QA and I'll pull up other
slides that people want them um you know
the real summary here is that most
simple java just works in those
MapReduce calls it runs at memory
bandwidth speeds parallel distributed
reads and writes and pins all just go
you can do conflicting rights and we'll
follow the Java memory model which is
pretty loosey-goosey
so you might not get what you want but
it will be you know honestly the JMM and
we're writing big data analytics
state-of-the-art algorithms running
distributed and this slides now old I'd
say that we're solidly solidly working
at 200 gig data sets and we have
definitely tested tera byte size data
sets before so you know it's it's the
big data and we're doing big math on the
big data at fast speed and that's it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>