<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Spark: A Coding Joyride - QCon SF 2015 | Coder Coacher - Coaching Coders</title><meta content="Spark: A Coding Joyride - QCon SF 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Spark: A Coding Joyride - QCon SF 2015</b></h2><h5 class="post__date">2015-12-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lfuJhtd0Dis" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright welcome looks like we have a
full house who would have thought the
sponsored solutions track could pack a
room this is excellent so our goal today
is spark a coding joyride and what does
that mean well our goal today is number
one to showcase sparks ability to
process big data so what we're going to
do is we're going to take a
straightforward example we're going to
extract information using an RDD will
just read a file and do standard rtd
style programming we'll talk about what
an RDD is as well well then demonstrate
some of the higher level api's for
example the data frames API which in
spark 16 has substantial the performance
improvements over what we used to be
able to do with our dd's we'll talk a
little bit about that well then do some
visualization so we can start extracting
patterns and doing exploratory data
analysis will even create a simple
machine learning pipeline to create a
model that can do predictive analytics
based on our data so that we can then
begin predicting future outcomes based
on past data points and along the way I
really hope to give you a sense under
the hood of some of the things that
makes Park ten to a hundred times faster
than hadoop mapreduce so that's your
goal for today before we get started I
thought I'd introduce myself so my name
is Doug Bateman I'm the director of
training at new circle new circle is a
data bricks partner we have partnered up
with data bricks to help provide their
entire training curriculum and catalog
been working with Java back since 1995
so that was Java 1 point 0 if anybody
remembers I always thought it was funny
back then you'd see job postings for
five years experience with Java and of
course this is Java one point 0 but I've
been working as a software architect as
well as trainer during this period of
time so that's a little bit about me
these are some of the projects I worked
on including Microsoft Azure been
working on various Android applications
and development and we also do training
new circle on HTML spark Java and Python
and I work in all these areas as well
but I'm more than just a professional
guy I also like to have fun so given
that we're on the west coast here I
always like to break the ice a little
bit so here's picture of me sailing this
was up in Vancouver Canada I recently
moved down here to San Francisco so one
of my hobbies is sailing I also enjoy a
little bit of rock climbing i picked
that up for my wife we're actually
expecting our first child in March thank
you but before we go any further I
wanted to just quickly pull the audience
because one of the things you just don't
know coming into a presentation is what
type of audience to expect so first of
all there should be how many of you guys
are new to spark excellent how many of
you guys have used spark hands on at
least little okay and how many of you
guys have more than one year of
experience on spark excellent okay so
for the audio that was almost the entire
room was new to spark so this is perfect
and I will aim the presentation towards
that so when we talk about spark the
first thing we want to understand is its
goal a unified engine across data
sources workloads and environments so
let's take a look at that in a little
bit more detail here so first of all we
have a unified engine so spark or is
really the hub of spark and it uses this
ingenious in memory of abstraction known
as a resilient distributed data set so
though how many of you guys are familiar
with to do all right so we've got a lot
of Hadoop guys in here excellent so in
Hadoop you guys are very familiar we do
a map operation we write back to disk we
do a reduce operation we write back to
disk and typically we're using the
Hadoop file system HDFS to do these
operations and one of the big reasons we
write to disk is because you're doing a
big job and you don't want to lose your
entire job in the event that a single
machine were to fail now with the
resilient distributed data set sparkle
or allows us to do the same type of
thing but in memory and so it's a very
different runtime architecture and we'll
look at it in just a little bit later
but thats parkour now dealing with spark
or is very much functional programming
you're doing MapReduce type operations
we'd like to have a much higher level
API and so built on top of spark or are
four primary engines first of all spark
sequel which has both a programming API
where you express your queries using
what are known as data frames or a raw
sequel API and this actually uses an
underlying query optimizer to rewrite
your optimization your query to run on
top of our dd's but much much faster
it's capable of pushing down parts of
the query down to the database so that
the filters are done on the underlying
data source or Park a file rather than
in the MapReduce operation it's also
capable of offloading work into native
memory to achieve far more memory
density than you get with ordinary Java
objects just to give you an idea a
string in Java is encoded as a Unicode
value utf-16 plus it's got a length
field and a hash code field and overall
the very short string may be up to 64
bytes or longer 64 words I should say
whereas if you compare that to what you
can do with natively in memory I could
fit quite a bit information with just
one byte per character much more
efficiently and when you're dealing with
in memory operations that matters so
this gets optimized with native memory
and performs faster we then have things
like spark streaming which allows us to
do micro batch jobs on top of spark that
would allow us to actually process
streaming data very much the way you
might process it on something like
Apache storm but with the compute power
of the spark engine we've got ml lib
Emma lid is typically used by data
scientists and what it's capable of
doing is training mathematical models
based on very large data sets how many
people here have a data science
background a few of you guys in here
very
good so this is ml lib we can do linear
regression we can do k-means algorithms
classification machine learning is
really the future of computation in many
ways given enough information we can
learn about anything Google learns which
emails you consider important we can
learn which websites typically
associated with spam which web pages are
more likely to contain the information
you need Facebook learns which friends
are more likely to be set trendsetters
you can analyze networks to figure out
which people are really leading the
industry in terms of new announcements
that everybody else follows and retweets
and then lastly well I just kind of
spoiled graphics this last one where I
talked about analyzing these types of
networks is the engine graphics which is
really vertice edge databases so these
are the four engines on top of spark
that are built on top of spark or which
is basically a much faster compute model
than what you get with spark MapReduce
those of you how many of you have worked
with hive so you may be slightly
familiar with the capabilities of sparks
equal sparks equal in fact uses the hive
query language but it uses the spark
runtime and it's capable of connecting
to your existing hype tables so built on
top of spark or we have both the rdd API
that's that resilient distributed data
set that dysfunctional programming and
what it's known as the data frames API
which is a much easier and simple API
for processing big data and I'm going to
showcase both of those today and then I
talk about spar expanding many data
sources so spark can pull its data from
postgres SQL or mysql are really any
data source that offers a JDBC connector
we can also pull our data in for
Cassandra or HBase Hadoop HDFS amazon
ec2 we can read in JSON files we can use
elasta search parquet files which is a
very efficient Hadoop based file format
for reading database tables
so we can connect to a whole ecosystem
of different data sources process it on
spark or and the beauty here is that we
could then deploy this into many
different types of environments so at
data bricks we like to use Amazon ec2 so
that you can quickly launch your
application and say you know what I need
a hundred computers but i only need them
right now and you know what i want to
run on spark 16 I don't want to wait six
to seven months for my vendor to update
to 16 and then have my IT team install
it because 16 has all this off memory
heap stuff so basically by running
inside of the cloud the idea data bricks
is that you can always be using the
latest and greatest spark and still
connect to any of your data sources but
the engine runs in the cloud you quickly
bring up the machines when you need them
you shut them down when you don't none
of this I think I need 100 machines and
I need to order them and then have IG
set them up before I've done any type of
benchmarking now just to give you an
idea of the success of spark spark is a
hundred percent open-source Apache
project and it's used in production
today in over 500 organizations from fig
fortune 100 really small shops and this
is just a list of some of the companies
that are using apache spark today and
the list is grogs if you're wondering is
spark real is it going to go the
distance the answer is absolutely it
already is so just to give you an idea
about the activity going on commits in
the past year hadoop mapreduce is fairly
mature not that much change to core
hadoop mapreduce yarn which is a big
scheduling application that allows us to
basically manage resources in a cluster
quite a bit of work done hdfs sparked
very much embraces a embraces HDFS HDFS
is not going away anytime soon let me
have a patchy storm which is a big
streaming engine we look on top of all
of that the activity going on spark is
very much an active project to give you
an idea of scale the largest publicly
acknowledged cluster in the world is 10
in China with over 8,000 notes the
largest publicly acknowledged single job
is alibaba which it processes a petabyte
of data and as you see later on spark
off data bricks running spark awesome
one the terabyte sort competition and
after they finish clobbering Hadoop and
the terabytes or competition went on to
process a petabyte of data just for fun
it's also got very high streaming
throughput so a terabyte of data per
hour at Gina our janila farm and as I
mentioned in 2004 it got the world
record for sorting a hundred terabytes
of data so let's look at that world
record here so 2013 Hadoop has the world
record for sorting 100 terabytes of data
they create a custom cluster of over
2,100 machines and they're able to very
impressive sort 100 terabytes of data in
72 minutes 2014 spark comes along with
only 200 machines running inside of
Amazon ec2 spark sorts the same amount
of data in 23 minutes and then goes on
to sort a terabyte of data in 10 times
at two hundred and thirty minutes for a
petabyte of data I'm sorry a petabyte of
data in two hundred and thirty minutes
same resources truly impressive results
in terms of scalability and throughput
spark is saturating the capabilities of
the underlying hardware now how does
spark really work let's look at some of
the terminology here so when you start
getting used to spark and you start
working with spark the first thing you
want to be aware of is when you launch
your application you're launching a
driver can either be a shell which will
be using today or it can actually be a
standalone program and that driver will
reach out to whatever resource manager
you're using it can be yarn to acquire
several machines and they can be
executives and this or these will become
our executives what's going to happen in
the drivers going to break up our work
and farm it out to these executor
machines now this is very different than
what you saw on Hadoop because these
executives are going to be living for
the lifetime of our application which
allows us to cash our data in memory so
we'll farm out these individual tasks
that'll be processing what we call
partitions of data on each of these
outlier machines so the executor zuv the
workhorse the driver is the director it
drives the process it is our application
and for the duration of the application
which may be many jobs these executives
will remain up which is very different
than what you would see with a dupe
where the processes or short lid for an
individual operation now to get us going
here let me make sure Oh before I get
into my demo and one of the energies one
of the key abstractions so I'm going to
pull up just some of our course we're
from our spark training new circle and
data breaks so we talked about this idea
of spark or really being our DD zor
resilient distributed data sets so let's
look at how those compute model works
and why it beats Hadoop inter- MapReduce
civically in terms of performance so
let's say I've got my data in HDFS in
spark what I'm going to do is first of
all read that data into what we call a
resilient distributed data set these
will be partitioned out different
executives will page in the partitions
that they need to do their job but
initially when I define this our DD
nothing gets loaded immediately it's
done lazily so you'll see this dotted
line nothing's in memory just yet I then
start defining my computing processing
pipeline so for example I might do a
filter transformation so here we have a
log file log data and I just want to
filter out the errors those of you who
have worked with a dupe are going to be
familiar with this typical map operation
or in this case it's the filter
operation I'm just filtering out only
error messages now
you see this little Optimus Prime that's
a transformer we're doing a
transformation okay once we've done a
filter operation notice that we have a
lot less data we may wish to coalesce
our data into smaller partitions so that
our jobs don't finish so far our
individual tasks don't finish so fast
the more partitions the more tasks this
is a good thing it's more parallelism
but if you have too many partitions and
your individual tasks are finishing in a
matter of milliseconds at that point
your scheduling delay can become
significant so generally speaking more
partitions as more parallelism as better
until your tasks are so fast that the
overhead starts to become significant so
you want your tasks to finish half a
second rather than a few milliseconds so
we might reduce the total number of
partitions with a coalesce operation and
then at some point we call what's known
as an action how many of you guys are
familiar with the streaming API in Java
8 so you guys are familiar you're
building up a pipeline but nothing runs
until you actually call an action a
terminal operation and those Java 8
pipelines we're doing the same thing
here in spark we've really created a
stream of operations but nothing is
going to run until we call this terminal
operation here called an action and what
the collect operation says is take those
results bring them back into memory in
the driver this is one possible action
other actions might be save it to disk
in HDFS but bring it back in the memory
in the driver now let me ask you if I
had six terabytes of data in memory my
drivers got three gigs of RAM what's
going to happen I get a nice big out of
memory error this is because with
collect I'm saying bring this back in as
an array in memory so typically you're
only going to call collect if the
results are small otherwise you want to
to a file somewhere or continue to do
other operations sum up the values do
some type of reduce operation et cetera
now let's look at how this thing runs
when I call an action only then does
anything run and if you start reading
the documentation for spark you'll hear
this phrase lineage or dag you execute
the dag or materialize the rtd what they
mean is this pipeline that we've been
defining which forms a directed acyclic
graph in other words everything flows
downstream there no circles and you run
it to get the result so it's going to
execute the dag when we call an action
and not before so at that point it's
going to go out read the file do the
filter do the coalesce so here we go
we're going to read the file do the
filter do the coalesce bring the data
back into the driver and when we're done
it releases all that memory it's just
run a job it brought it into memory
here's what's different than what to do
all of the communication between these
stages is in memory nothing gets written
out to disk until I call it an action
that says write to disk in this case I
didn't even bother writing to disk I
said bring it back to the driver but I
could say save his text file and write
it out to HDFS so all of these
intermediate operations are happening in
memory and then you say but Doug what
happens if the computer fails do I lose
my work well with spark what will happen
is let's say the computer that's
executing this part of the pipeline here
let me run a little arrow program if the
computer running this part of the
pipeline we're to suddenly crash and
actually in this particular case because
of the coalesce
that whole thing there is going to be
one task in my pipeline and the computer
running this task were to crash no big
deal if the computer running this task
crashes the driver knows well this task
was running on this input data set let
me go ahead and start this up on another
executor in my cluster when one becomes
available or maybe one executor is just
taking a really long time and spark has
no idea if it's hung or not it'll
speculatively re-execute that task on
another executor and wait for the first
one to finish and in this way we get
resilience while still getting the high
performance benefit of being in memory
and what we're done the memory is
released now you say wait a minute what
if I didn't want that memory released
what if I'm going to call a count action
but i also want to save it as a text
file and maybe i want to do a filter and
collect that result well if I don't tell
it to cash what's going to happen is
it's going to load this data and do the
count release the memory load this data
save it to a text file release the
memory load this data do a collect and
release the memory well clearly this is
undesirable we'd like to get a
long-lived benefit in memory so we have
the option of telling it to cash by
calling the cash operation to cash this
result long-term in memory so if I cash
this in memory by calling the cash
operation then they get the benefit of
not even having to go back to the
original data source now you want to be
careful when you cash things when you
cash things in spark
very very important don't cash the
original data set cash is far downstream
as possible if you cash too much you
lose the benefit of cashing you really
want to cash a small data set so that it
fits mostly in memory and so one of the
biggest things we see when we do audits
of production systems is that they
didn't cash intelligently they either
cash too much or not at all and of
course it really comes down to what's
the series of jobs your application
wants to send and spark has no way of
knowing what future jobs are about to
run so this is why you as the programmer
provide that insight by calling cash the
other thing you need to remember is to
uncashed it and cash is really a
shortcut to persist a memory so the
opposite of cash in the API is called
unpurchased gets everybody the first
time they go how do I own cash the
answer is you call unprocessed now I
want to check in with my audience here
how are we doing in terms of hitting the
sweet spot of your interest good once
i'm coding we want to see that coding
excellent so this is the picture that i
wanted to show you now that we've done
that let's go ahead and do some coding
and see spark in action so we're going
to do first of all hourly
straightforward example based on the
audience and then later on i'll point
you to a more complex example you could
explore on your own but let's take a
look at this power plant demo so how
many of you guys are familiar with what
are called peaking power plants all
right so California in the middle of the
day it's really hot and they need to
turn on some additional power plants to
meet peak demand based on the
temperature humidity etc they need to be
able to anticipate how many of these
plans to turn on and more importantly
for each plan how efficient are they
going to be because outside temperature
that's really hot out the plants don't
run as efficiently because they're
exhausting
sure sire second law of thermodynamics
so what we have is a data set we've got
atmosphere temperature in centigrade
exhaust vacuum atmospheric pressure and
relative humidity and we've measured the
output of all of these power plants
based on today we have these readings
and we got this output next hour we had
these readings we got this output and
what we'd like to do is to predict the
power efficiency tomorrow given our
prediction for the temperature and
pressure and so forth so step one we do
what's known as an extract transform and
load operation step 2 we're going to
explore and visualize the data and step
three we're going to do some machine
learning so let's jump in now to our
demo so right now I am firing up data
bricks oops data Brooks runs entirely in
the cloud on Amazon ec2 I come into data
bricks and the first thing I want to do
is I come here to clusters and I say you
know what I need a cluster to be able to
run this job so let's see here q con
demo and you know what I'd like to have
eight machines and I'd like to be
running spark one dot five and i'm going
to use on demand instances so that
nothing gets killed in the middle of my
presentation by amazon click confirm and
this is going to start launching inside
of amazon ec2 i could bring up a hundred
two hundred node cluster now at this
point what I'd like to do is go into my
workspace q con san francisco 2015 and
what i'm going to do is just make a
clone so that you guys can still keep
the original wait for that to finish
cloning there we go I'll go into the
power plant I've got an example here
that I think is right for you guys with
power plant there's a more elaborate one
where we're processing a much larger
data set in Wikipedia there click stir
data trying to predict how many clicks
they're going to get and detect
anomalies like big traffic on a day
let's go into power plan for now so step
one I need to do an extract transform
and load operation so let's see here
very first thing I need to do is to open
up our text file and read it so I go
spark context i'd like to read a text
file please in which text file will all
copy paste this path oops copy paste
this path here oh I do have text field
alright text file and we'll read in the
text file let's see how our clusters
doing all right it's still launching
it'll let us know when our clusters
launching its bringing up an Amazon ec2
instance and once we've read that text
file the very next thing we'd like to do
is actually take a look at what's in
this text file so I'm going to go raw
data a raw text rdd dot take and we'll
read in just the first five rows of the
file the Amazons begin a little bit
slower today so we're going to keep
going and we'll come back and run it
when we're ready so to give you the
spoiler of what will see the first line
of the file is comments about the
headers the next line of the file is a
comma separated values file so what I'd
like to do is using our DD programming I
would like to extract all that
information from the file into nice
little Python objects that represent
each data point so what I'm going to do
is I'll start with my raw text our DD
and I'd like to filter in this case I'm
using the Python API we could just as
easily use the Scala API I find that for
people who don't know Scala they prefer
Python for the demos the wiki ml I've
got solutions in both Python and Scala
for anybody would like to see them and
what would I like to do well I'd like to
filter out give it a line
if the line starts with a 80 atmospheric
temperature that's the header line of my
file that defines the columns let's see
it still bring it up my Amazon ec2 okay
so what we're going to do we're going to
filter out lines that start with
atmospheric temperature and then what
I'd like to do is for each line split it
up by commas turn them into numbers and
place them in my Python object so I'm
going to want to do a map operation
convert line to row or line to power
plant row and finally that gives me back
my rtd so what I'm missing at this
moment and let's go ahead and print that
raw data rdd take five now we need to
make this function convert line to our
DD this is standard functional
programming I'll go ahead and define it
here in Python convert line to row so
give it a text line I want to return a
new power plant row so lying dot split
based in this case they actually not
comma separated values their
tab-separated values so we're going to
split it up so this is gives me my cells
and then let's see here for each cell I
want to create a power plant row object
so power plant row cells 0 and I might
want to let's actually read this out
happens for York temperature is as a
floating point number cell 0 and so
forth where do we got vacuum pressure
floating-point cells one what else
atmospheric pressure floating-point
cells to relative humidity
floating-point cells three and finally
the power efficiency floating-point
cells for and then I want to return a
new row object return power plant row
which I define right up above
atmospheric temperature vacuum pressure
atmospheric pressure relative humidity
and power efficiency boy amazon is not
being very friendly today not good for
live demo I should have fired this up
five minutes before class I thought how
cool would it be for you to see me fire
it up we're going to keep going just in
the interest of time and then come back
in and actually run it so what we're
doing is we're taking line to text
filtering out any line that starts with
atmospheric temperature because it's a
header label we're then going to take
the next line split it up based on white
space convert them to numbers and throw
them into a Python object standard
MapReduce functional programming for all
the Hadoop guys in here right nothing
unusual about this and then we might
take back the first lines and see how
they look ah here we go my cluster is up
run it
first time it's got to bring up some
processes nothing like a live demo this
particular file is not too huge the the
wiki ml though is leave half a terabyte
and of course the entire wikipedia which
we do do in our labs in our training is
much much larger so here we go notice
what I'm getting is a header separated
by tabs followed by a line that's a
string of tab-separated values but this
is not very pretty but I'm running in
Python so i can go lines and then go for
line in lines print line there we go
notice it formatted a little better
because I'm still running and I got the
full power of the Python programming
language available to me running in my
shell can you guys see this okay I can
make my font a little bit bigger so at
this point we've said we want to split
this up and turn them into power plant
row object so let's run this guy again
and oh we have a job failure does
anybody see any spelling errors I don't
see one so let's look at our error
message come down here in the middle of
our nice stack trace that's one of the
skills we have to teach and training is
how to read these stack traces and it
says error occurred while calling run
job could not convert string to a
floating point number atmospheric
temperature so here's what I did wrong
somebody spot it i filtered out lines
that start with atmospheric temperature
don't I want to filter lines that don't
keep lines only that don't start with
atmospheric temperature so I did the
exact opposite to see that let's just
get rid of this map operation run it
again and now the first 5 lines on my
already do you were the headers alright
well we'll go back and fix that we're
going to come here and we want lines
that
do not start without mysterio
temperature we run it again there we go
now we've got in memory Python objects
that we could begin to process now the
trouble with Python objects of course is
that we're stuck doing this MapReduce
style of programming so what we'd like
to do I go into the second stage here is
move beyond that style of programming
and begin to use what are known as data
frames so let me reconnect my cluster
rerun the work we just did and this time
I'd like to actually take a look at it
so I'm going to convert that our DD we
just made called what was it called raw
data our DD raw data our DD and I'll
convert it to a data frame
and now what I have in my disposal is
the full power of a query engine that
can run either sequel or
programmatically written queries so i
can start saying only give me things
that were on days where the temperature
was greater than 20 degrees let's try it
so I now have a power plant data frame
now the first thing I'll do is I'll want
to register it here as a table so I'll
come down here and i'll say power plant
dot register as temp table and i'll call
a power underscore plant that registers
it to my sequel binding and at this
point i could start running queries like
select star from power plant where
atmospheric temperature is greater than
20 degrees i could of course do an RDD
filter operation but this is much easier
and more concise now for those of you
who don't like sequel i can do the very
same thing in Python power plant dot
filter where the power plant atmospheric
temperature is greater than 20 degrees
this is similar to something called
pandas or sequel alchemy I'm basically
expressing my query is still a
declarative query executed by the same
query engine but this time I'm
expressing my query in Python and it
still builds the same abstract syntax
tree and runs it but let's go ahead and
display that and again you see that I
filtered out only jobs that have a high
atmospheric temperature let's roll down
a little further and get to even more
interesting stuff how am I on time by
the way I've got about 10 minutes 10
minutes alright so I'll fast forward
just a little bit I'll do one plot
not so yes I could describe the power
plant table for example I want to get
into the machine learning aspect so
we'll jump down a little bit let's say I
wanted to visualize my data I could do a
scatterplot so i could say select PE as
power in 80 as temperature from power
underscore plant and i could actually
change it to be a plot you notice I've
got a very strong linear relationship
between the outside temperature and the
power efficiency of our power plant
let's go a little bit further I'm going
to jump ahead just a little bit in the
interest of time
so you'll notice that atmospheric
temperature let me zoom out a little bit
very nice line exhaust vacuum a little
bit less clear there's something going
on here it's kind of linear maybe not
this is the kind of analysis a really
good data scientists would do be to try
to convert this into a line by deploying
transformations then we look at
something like atmospheric pressure
there's a semblance of a line but a much
weaker correlation and then you get to
something like relative humidity and I
have no idea what that's supposed to be
it's just a cloud so relative humidity
evidently has very little impact on the
overall power efficiency of our power
plant so at this point we're ready to
actually start doing some machine
learning and in the interest of time I'm
going to go ahead and jump straight to
my machine learning solution notebook
instead of live coding how many data
scientists in here again just a small
number of you guys okay so just kind of
show you what you can do to get a real
application out of this so we finished
visualizing our data let me do run all
again notice the older version of the
notebook had showed way fewer data
points they've now improved that to show
a lot more data points which is really
neat go ahead and hit run all and while
we're running everything I'm going to
scroll down to linear regression so what
I'd like to do is to build a linear
regression model of our data so the
first thing I'm going to do is I'm going
to take my original data set here and
I'm going to split it up with an 8020
split i'm going to use eighty percent of
those data points to do training twenty
percent then to do predictions and then
i'm going to evaluate how good are my
predictions it's a very common machine
learning use case and if i want my job
to be reproducible let's say an RDD
partition goes down and i do this random
split over again I would ideally like
that partition to have the same data it
had before so one of the best practices
is to give it a random seed any
seat at all but a random seed so that
it's deterministic in the event that
something needed to rerun so here I've
given it a random seed you'll also
notice that I've asked it to cache the
data frame in memory to allow my
iterative algorithm to run faster now
one of the really amazing things about
machine learning is that it benefits
enormously from having our data in
memory rather than on disk because what
machine learning is doing is iterating
on this data over and over and over
again basically trying to find the
perfect linear regression model in order
to scale it's doing this using what's
known as gradient descent rather than an
ordinary matrix multiplication in order
to be able to scale to enormous amounts
of data so here we go I'm going to say I
want to make my predictions go in the
predicted PE column when I do my
evaluations i'm going to do training
based on the original PE column i'm
going to allow my algorithm to iterate
no more than a hundred times and the
first thing i'm going to do is take my
data frame and turn it into a
mathematical vector and then i'm going
to run linear regression and then i'm
going to give it my training data and
out pops a model let's run this
notice how quickly 15 jobs just ran in
spark because all that date is now
cashed in memory you guys were all
thinking why spark going so slow when I
was running stuff up above weren't you
tell me the truth how many of you guys
have gone why was it so slow answer I
hadn't cashed it was going back to HDFS
each time are actually in that case
amazon ec2 s3 storage this time it's
cached in memory it ran 15 jobs like
lightning to 15 total iterations I then
could take my test data and get the
predictions and let's take a look at how
we did should I take my test data I run
predictions and take a look some of them
are quite close others that's pretty
close to here we go this one's five away
this one's seven away not too bad so a
good data scientist would look to tweak
their model to improve performance but
notice how fast and how easy it is to
program on sparc using either Python or
Scala and then I could go even further I
can actually print out the equation that
it learned why would be our power
efficiency and so it learned i just
multiply the atmospheric temperature by
this the vacuum pressure by this notice
how little contribution vacuum pressure
and other things have compared to
atmospheric temperature we come over to
relative humidity and it's got almost no
contribution no here we go what's this
one as atmospheric printer has almost no
contribution at all now part of that's
because of its units and then I could
actually do some evaluations I can do
some nice visualizations scroll down a
little bit I can see for example that I
get a nice bell curve on my error this
is excellent data result and I could go
even further and see how much of my data
is one standard deviation two standard
deviations or three standard deviations
away from its prediction so as a data
science
I can very rapidly build and develop
models and they get much better
performance and throughput than I would
expect in Hadoop so with that said I'm
going to wrap up and then take questions
so who are we again the presentation is
a joint sponsorship between data bricks
and new circle data bricks has done
about 75% of the commits on spark
because it's basically the original team
at Berkeley that made spark so spark is
an enormous ecosystem but a lot of the
core talent driving spark is at data
bridge new circle is training partners
with data bricks we do training in spark
big data android java python and a whole
host of other technologies if you liked
this style of learning through hands-on
demonstration really getting a sense of
a mixture of the power spark what's
happening under the hood and actual live
coding this is what we do in our
training it's three days of very intense
hands-on but hopefully this has been a
valuable experience for everybody here
thank you guys very very much for
attending I can take other questions
support for neural networks and deep
machine learning so there's a website
called spark dash packages org where
some people have tried to do deep
machine learning with neural networks
and of course she'll realize a simple
neural network is just a well a
level-one neural network is really just
a simple linear regression but once you
add deep neural network chaining that's
when it becomes more advanced go to
spark dash packages org anybody else yes
what happens if the driver crashes so
this is a great question and the answer
to that depends on how you've launched
the driver if it's a nightly job for
example you can run the driver in what's
known as supervised mode where the yarn
master or the standalone cluster master
would detect that the driver failed and
relaunch the driver and the entire
application on the other hand you know
if it's an interactive shell you launch
it but yes when the driver crashes you
lose your work unfortunately at this
time there is a technique known as
checkpointing where you actually
checkpoint your work so
even if the driver were to crash you can
recover where you left off and this is
very commonly used in streaming
scenarios question here not very
difficult at all so if you were to set
up your own cluster you can do that on
what's known as spark standalone mode
which comes it's called standalone
because it comes with spark and you can
quickly set up a cluster like right out
of the box if you just have a bunch of
machine just launched the executor 'he's
you'd launch the driver and tell it you
know where the actually the executives
register back to the drivers so you
launch the driver than you lost the
exact lesion tell them to register with
the driver very easy though you can also
actually run spark on yarn in which case
you're then using the yarn master to do
all of us work that takes a bit more
work so the nice thing about standalone
is that it comes out without having to
configure a third-party product like
yarn question here Peter and then Matt
so what we didn't get to go into as much
detail so the question was what happens
when spark doesn't necessarily have
reused data from the purposes of caching
so remember this picture that I showed
much earlier let me bring it back up
again it's the rdd slides this picture
now one thing that's actually happening
in this picture that we didn't get to go
into because of time constraints today
is something known as pipelining so what
happens is he in Hadoop each of these
individual stages would be their own
process running and dumping to HDFS
which spark is going to do is realize
that all of these things could actually
be done locally between each of these
individual transformations so it's going
to read in a partition in this case
let's say I had one executor how would
this run it's going to read in in this
case two blocks from HDFS do the filters
do the coalesce and then of course do
the collect operation and it's going to
do that all in memory on one partition
then it might fire off another task to
work on the next two partitions and so
forth so what actually happens is even
if it does in cash in memory you're
getting the benefit of what's known as
pipelining through this analytical model
you're also getting what they're called
shuffle
so if you have a more elaborate
operation where it's actually what's
called a multi-stage operation between
each stage it actually dumps out
intermediate results to a file which
creates a second level of caching Matt I
think I said first yeah ah so spark
supports Python Scala are and Java and
starting 15 job at eight so what you
really want is to choose the language of
your choice Scala is actually your first
choice if you guys are good with Scala
you understand scala where you take our
quick new circle class on Scala the
majority all the API is available
Scarlett's Park is written on Scala with
that said maybe you guys are a Java shop
you can use Java now I discourage you
using Java 7 because it's very verbose
with inner classes you really want
lambdas and then we find the machine
learning community is more comfortable
with r and python so we teach it in our
in Python for that type of audience does
that answer your question Eric and how
is optimized is it from pulling from a
sequel source so that's a great question
and there's two optimizations that you
want to be aware of one of them is if
I'm doing a dataframe query and let's
say I do a filter with my data frame and
it looks and goes you know what I could
do that filter in my in Oracle itself or
maybe i'm doing a join but it looks and
says i'm joining two tables from oracle
it pushes the joint and filter into
oracle first second it can actually
create parallel jdbc readers so if it's
a sorted data set you can actually say
give me the first thousand to this
partition the second thousand to this
partition and then the individual
executor czar running in parallel but of
course your Oracle database under the
hood needs to be able to support that
level of parallelism so usually we only
do that when we're writing is something
like Amazon redshift where you can
handle concurrent queries so what do you
have to do with spark from a management
perspective and then you specifically
mentioned business intelligence so if
you're a team that's doing exploratory
data analysis and business intelligence
skip our dd's out right go straight to
data frames are td's are going to be
your fallback data frames are the
preferred way to go if you can use data
frames use them are dd's are for only
things you can't do what they
two frames and then it's going to be as
straightforward as using sequel now from
a management perspective what do you
need well you want to make sure that a
you know how to set up your spark
cluster so there's a skill set there be
your team are they familiar with what
language are or Python or what a Python
Java and C sharp so you would choose
whether you want to use Java 8 I don't
recommend Java 7 or preferably Python
for that team if they're very fluent in
that and then just sign up we have a
one-day spark overview and we have a
three-day spark developer we're also
starting out to write a class
specifically for exploratory data
analysis and bi which would go into
lesson to the programming side and more
into doing visualizations and sequel
queries they bi guys can do it as long
as they can write sequel they have to be
able to write sequel that's the one
caveat now you can also make spark a
JDBC provider so your existing tableau
tools could connect to spark as a JDBC
provider in which case spark is writing
the JDBC but not now you're actually
running your familiar tools and still
connecting to spark oh absolutely would
take advantage of parallelism the sparks
equal execution engine will take care of
the parallelism thanks guys very much
for coming I know there's food out there
I'll take a few more questions you're
welcome to come forward and ask
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>