<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Concurrency &amp; Garbage Collection - Considerations as the JVM Goes to Big Data | Coder Coacher - Coaching Coders</title><meta content="Concurrency &amp; Garbage Collection - Considerations as the JVM Goes to Big Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Concurrency &amp; Garbage Collection - Considerations as the JVM Goes to Big Data</b></h2><h5 class="post__date">2011-05-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8BwXijVmvKk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming all of you it's
actually a privilege to see a full room
I was the first one to come here which
was good thing to do but see seeing all
of you here at this hour
sorry for late start but we'll make
hopefully make it worth your while
that's that I'm a JVM guy and have been
looking at Java problems for a long time
looking at how James scales how things
break when it scales so everyone has
looked at and I've met a lot of you how
many of you have started Java before
year 2000 and how many of you have
started looking at Java in the last five
years cool so there's a lot of Java has
taken a lot of a very long journey over
the last 15 years and the 15 odd years
and on the server side things have
changed dramatically concurrency has
become more prevalent more modern more
rework more required you don't think
about writing programs that have no
threads anymore right so and then there
was suicide
containers that were abstracting most of
this from us what we have today is all
of that logic is in your on your Eclipse
on your IntelliJ station so you cannot
ignore the constructs of the Java memory
model and and for a long time Java
memory model was broken and we'll see a
little of that
it's luckily doing much better now and
in the post content collections a lot
more good things have happened but
really this is not to talk about Big
Data it's talking about JVM issues
behind Big Data and all the big data
players are suffering these today all
Java apps have some of notions of these
problems but specifically more so Big
Data and when I speak so
thanks AUSA as a slave this is actually
a slide from a who remembers this
picture theory ago yeah dr. Seuss where
the bottom most part of the stack is
actually sees the place called Mack and
he's basically screaming dude anymore on
top right the trailer head synchronized
is the locking paradigm that's what
people have always been using will will
double click what's happening behind
synchronized how locking is working
there also propose ways to look try and
build collections that are wide
synchronized and some of you have looked
at it and if you've seen Brian gets
booked concurrency you will actually see
a chapter dedicated to that but we'll
just look at how to build a non-locking
collection if you had the leisure to go
build one yourself right and there are
some ready-made solutions that you don't
have to build but we'll double click on
that and so say Sasson there's a
possibility of a of a talk of internal
je viens if i know whom he's talking
more that as well flick-flick he
actually wrote that collection so if
he's doing the talk make sure you you're
there it's it's it's a beautiful talk so
then you have collections general
collections have they how they play and
we'll fast forward to serialization UUID
and then to garbage collection so the
talk is going to be a little jerry
because there are sections in it and
they're focusing on separate sections so
but these are all issues that we see on
a daily basis when when you scale your
collections okay in your app so and then
if you have any any questions along the
way you that have piled up feel free to
lift your hands stop the flow of that's
all that's fine a synchronous Q&amp;amp;A is
fine so i don't think we'll get to be
able to talk about IO itself but
hopefully
a call-out to the tools of the trade
people looking at Java everywhere have I
mean there's no no performance or no
metrics without a tool tools are the
reason why we are here
humans build tools so there are
different levels of these tools these
tools are kind of ordered in an order of
how how much impact they have towards
the end division is your light weight at
the top of this is you're looking at
what Jamie's doing if you want to look
at what is happening while you are
running your app J console is a life I'm
sure everyone used J console visual VM
gives you a view on how the GC is
playing how the various things how the
objects are growing so on support people
how many of you use your kit say speak
for yourself itself it's a it's a great
tool z vision is Azul JVM it's a JVM -
it's a JVM wandering - baked inside the
JVM it's a lightweight but it so that
also less available if you look at the
JVM observation tools TI di and P I
again this is JVM tooling interface so
it's tools are not the ones that you get
from tooling vendor you can wrap one
your own the JVM ti we won't go into the
depths of it but you can actually write
a small tool that measures how your
objects are being laid how of your
allocation rate is you can put a - Java
agent and it will spit out those
information to your logs so some of the
new big data vendors are actually doing
that so they put a small Java agent that
measures your memory it's stuff like
that so so so TI has traditionally not
been exploited by end users it's
beginning to be place where people are
getting their hands dirty as well what
so s doing again d trace or profile we
tuned perf perfect actually
o profile very well packaged by Red Hat
so you can actually sudo apt-get install
perf and you can actually see how what
the instructions said we'll see a few
outputs of this vtune is Intel
we tune which also does a lot of
deep-dive of how your program is running
what your cache misses are how you're
doing your how the instruction sets are
being laid out one cool thing about
retune that I would call out is you can
actually run the program on your laptop
and say how would this run on a big
4-way Nehalem processor so you can
actually say ok you could do a map from
a planet like this here on this Intel
chip
how would it run in production for
example so so there are some interesting
tools that the nuances that are brought
to this by the body always guys and the
hardware guys so of course everyone
probably ran into ganglia how many of
you use ganglia or have heard of it sure
production monitoring you can also sell
send alerts from your project here so if
you the people who are trying to do
massively parallel programs you're
trying to debug a Hadoop program you can
send a small alert through a ganglia
Jiwon and I would come up back to the
main system even if this program died so
that's something to look up sure
Nagios again another popular tool
netstat else off yep the list goes on
but these are kind of traditional and we
kind of expect people to be using those
on a daily basis in production this is
actually a a Matt Groening production
from The Simpsons synchronized it's an
eye chart synchronize under the hood
when you do a java.lang synchronize it's
as if you are zoom but it's in the
function or inside the code what's
really happening is actually so so if
you look at the top right corner that's
actually how your objects laid out right
that's your object mark word seen on the
left hand side you have the bit fields
you can say based on what you store in
the bit fields your logic flows
differently so for the vast majority of
off locks they are never actually
contended so you're basically racing
through synchronised unconsented logs
are cheap so historically people would
say a wide synchronized like like real
dirt right
but if your code does not actually
contend the the sink rat it's not so bad
you actually go all the way down right
it's when you when you actually contend
that's when you see the first like rate
law come in right and then of course you
need to inflate lock that's when you see
a big heavy monitor so behind the scenes
all here synchronized you actually have
a monitor and monitor that's thin until
it's actually considered the first
contention then you see inflation of
that monitor and that's you actually
that's when you're actually seeing the
effect of the overhead of a real lock
right now what came up came about in the
in the last few years actually being
able to buy a Stalag say you have it
you're a thread you own this lock and so
you could it's very high likelihood that
you're going to run through the code
before someone else needs to be get it
so the whole concept of biasing was
quite expensive the the certain circa
2006 debt lapse and and the son son team
came along with a store free biasing
ability to buy ask without a store and
the the crux of that is is coming at the
last one so if you store the thread ID
at folk and age it's a buy a stable lock
so you have a thread ID so your next
time someone is trying to pick it up
you're gonna buy us a lock to live with
this thread and let it finish let the
other threads loop a little let it
finish and you probably get to the end
of the end of the program before you
need to actually swap the whole lock so
that's that's the fast part so the fast
part is no contention to unlock and then
you try to buy a slogs
without or you can so and then if so if
it does it does get a contention you
actually have to force the thread to
give up the law then you bulky bias
everything so so the crux of that has
been that achieved a store free biasing
by actually having a thread ID in an age
so and that's kind of the the the new
nuances and so for a long time if you
people have remembered having used
thread biasing or used biased thread
locking
now it's a default in 1.6 so this all is
is what's happening behind the scene of
a synchronized statement needless to say
locks are expensive if you have a highly
contended program locks are expensive
we'll see how we'll still have that also
plays into the Java memory model so Java
memory model historically I mean you
cannot go talk about Java without
talking about Java memory model how many
of you heard of the double check lock
locking scheme and how many of you have
have been told it's broken
okay so all of that so so double check
locking came across where people have
been taunting about lazy initialization
of objects and when you do lazy
initialization you want to make sure
that for the odd case where you do need
to fully make this object you want to
lock and make sure it's done in run time
in real time
now the what happened in oh-six was
people thought oh four actually Brian
guess and and Nick missing the name
became about this concept that volatile
who was actually not maintaining the
ordering of threads so in some sense you
were you were not guaranteed order even
partial order within the program so
double check lock was broken and so say
that that was considered the most intact
locking mechanism up until that time and
then we suddenly realized that the
memory model was not ordering things by
causality so if you look at this so post
Oh sakes which is all the jaebeum's
after that have been patched is that you
see that if you maintain partial order
in this thread a and then make sure at
the lock transition everything else is
also ordered partial order in this way a
partial order on the second part gives
you global common order so global
ordering so instead of forcing global
ordering which is a very hard memory
which is a slow memory if you say it's a
slower process before ordering your your
logic basically you're locking almost
serially throughout the program they
by asked themselves to do partial
ordering it's also volatile the
semantics of volatile may have changed
and made sure that when you declare a
volatile variable it's guaranteed to go
back to main memory right and it's it's
it's no longer being stored on your
thread-local buffers they are flushed
every time you write it goes flushes and
every time someone reads it you clean
from main memory now there are some
performance nuances around it you
actually lose some performance by that
but we are assuming that you want that
ordering in your system and that
ordering this bitch is very critical
especially in the multi-core systems
where you are running lots of threads on
and you do have lots of CPUs to take
advantage of that so so the whole
volatile concepts redefining volatile as
a as forcing order written how things
how memories accessed has made sure has
brought at least that happens before
causality into into the JVM and that has
given more to that has actually saved
could encode the order across across the
JVM so data races are less likely
because your memory model is busted
every module is now actually been fixed
and so you don't need to see data races
if you have not introduced answer right
then you see future tasks how many have
you most of you have probably used
future tasks behind so said yeah so the
last five years of of programming in
Java is so different from the first 15
10 years right because you now you have
all these nice little cool collections
you give queues blocking queues and and
and ways to use them and so some of the
new Big Data projects actually have used
they have soldered in two thousand five
and six and seven so these are new code
bases and their code is so if you
occasionally look at code that was
written in 2000 like Tomcat has about
400,000 lines of code
and then you look at the new code bases
which are doing much bigger tasks than a
traditional Tomcat of less than like
75,000 80,000 lines of code so so 5x
shortened code much more work the the
ideas they're using future tasks and
locking key long queues and all the new
collections that have been invented
after which which allows you to do a lot
more a lot less code and do a lot of
runtime typing so but but the reason
this thing is here is that future tasks
actually uses simple counters simple
cast castable counters are Tommy
counters to take advantage of creating
order so for example if you know that so
if a simple way to think about this is
say if you know that a will happen
before we serve in other words you're
incrementing I so you have I I plus plus
right so you're incrementing I so if you
I becomes from what goes from 1 to 2 you
know that that's actually going forward
so let's say that's going forward in
time you can basically connect that
series of evens which are going forward
in time with your time to deriving to
your own time so in other words you're
piggybacking on something that is
guaranteed to go forward in time with
your own logic so in future tasks you'd
say ok that's it I know that's going
forward so I can also increment my own
thing so in some sense you're not
putting you're not forcing Europe your
variable to be volatile or you're able
to be synchronized
you're just deriving forward motion by
piggybacking on some other thing that's
moving forward so that's kind of the the
the concept behind behind it so
iterators use this all the time and
that's kind of the reason that's a plus
now look at jsr 133 that's a very common
that has that brought all the concurrent
collections into the JVM so here's
another take I don't know how well it's
shows take from actually reminds me of
the old-fashioned slides which used to
use present but this is showing that a
local on a the comp the city is
comparing lock performance between lock
and atomic integer right so what's the
performance difference between locking
and using a re-entrant lock or an atomic
integer I think the program here was a
pseudo-random number generator and and
so for high contention as a thread local
law it's actually faster than the other
than your atomic integer or reentrant
lock where they actually exceed is
actually in your moderate contention so
if you you don't have contention at all
then you're you're better off with the
other other side so it's kind of a taken
from the convinced we practice practice
Brian gets talks of feel free to go a
bit it
java.util concurrent also has locks so
don't forget that Java Ito convened also
has lost its actually original from a
from IBM to actually that does show juc
statistics so there is a monitor or even
behind Java code so don't assume that
just because we abused Java two
concurrent locking is gone
here's a tomcat in production that we
managed to catch how the the session
manager locks under contention this
actually real code went on to go and fix
the hashmap on the top it's just highly
contended so if you write your program
make sure you run it against a large
load high concurrency load to see where
your locks are oh it's it's it's
important to know as a designer where
things could be so it's not a surprise
we actually the standard manager we we
actually patched Tomcat at that point
and and that ventilator
but you actually look at the code it's
very trivial you look at once you double
months you know where the lock is you
open the code you know that's a trivial
thing usually very trivial thing instead
of being in production and knowing that
you have only 30 threads are working
this is very easy way to do this this is
actually the CV Shonto from a supervised
norm docking collections
yeah so one of the things that we
realized is um dolls law actually
trembles Moore's law if you have a large
single threaded part of your program
Moore's law is not increasing megahertz
anymore it's increasing more cores right
so muchi law is still in effect you're
actually getting more transistors but
they are all parallel so what we need to
realize that if you're not going
parallel you're going to be in trouble
so we actually looked at an example of
state actions and key value pairs
earlier if you looked at that object
model how the Harvard object is it
prints inside the word the mark word it
was a state and actions what to do and a
key value pair so the one of the one of
the key things that we learned trying to
trying to do concurrent programming is
that if you can actually parlay your
program to a state and action you're
you're able to look at go back to finite
state machines the on a meta theory that
we learned in our undergrad if you go
back to that state where we were looking
at okay this is the state this is the
various possible things for it if you
and these are the various values that
you can move to so once you come to that
level of understanding of your program
multi-threaded programming become
becomes very natural
otherwise you're constantly looking at a
cold fragment and trying to see how what
variables to lock up right so and that's
that's a different way of looking at
synchronization come up with parallelism
requirements very early on in your in
your coding practice so then you can
actually look at unbuilt product in some
sense and come up with an algorithm that
would say okay this this thing can
transition this many ways so in this
case if you look at a hash map and we
often do this as part of our offer for
new people when they join our company we
tend to ask them to design one one that
doesn't lock right so if you look at
hash map these are the five four
patience does get put delete and resize
right you you need to store your data
somewhere right so you have byte arrays
so you sub byte arrays of course out for
the most excellency wants to start with
so let's use that as your data to put
your data where you put in data so you
have a collection of binary and then how
do you how do you want our wide locks so
you know there are some Atomics in the
JVM and that's Cass provided by the
hardware for you for sheep and then
there are so you you want to use the
cache so this this implementation by
clip click actually uses no logs in
Neuville aside so it's a very good open
source source Porsche has this code it's
actually part of Cassandra it's also
part of a lot of other big collections
that I've seen recently where you can
get a completely scaling high scale with
large number of threads you get a lot of
you get the throughput that you expect
what we found so the crux of that
algorithm was basically okay you have a
for the most of the year reads so if you
doing just puts you don't actually need
to lock for a long time it's about the
interesting thing was unless your hash
is broken there you're collisions but
for the most part your puts don't need
any locks on the way it's your band
you're putting and getting from the same
contended hash key that's when you're
actually trying to come to a lock right
said so if you use cache you so that's
the most content file the other part is
when you are actually resizing so if you
hit the full limit off your byte array
you need to have a big larger overflow
byte array copy this forward and then
while you're copying over you have
another small segment which is critical
section so during your copy you you need
to backfill your rights and then use
that overflow buffer March the two
buffers so in some sense this is just an
interesting approach to doing
collections and actually now it's very
common to see non blocking stacks
non-blocking link lists and so on so
so did the copyright so this is the kind
of the resize algorithm so this is kind
of pseudocode on the slide but real code
exists and real code actually speak
stuff that speak very well and runs very
well so we've seen we've seen other
parts of the system lock up because of
this hash map because it's really lock
free skate free all right who has not
seen Java or heads right have see versus
Java Wars for range pretty widely across
different age groups of coding so what
is the cost of an eight-car string right
all you wanted was an eight-car hello
world right not even world and java
takes about 56 bytes to write hello
world right that's a lot so where does
that go it goes into the headers the
fields and part of the headers that we
talked about earlier pointers and then
you have length actual data is there
right so and then you're still pad it so
that you can of course there are some
optimized string representations and
swing collections but that's what you're
using when you're using a java.lang
string tree map actually tree map is a
very popular ordered collection right so
ordered set gain tree map has a figure
if you used a double double tree map an
interesting thing is the tree map has
this tree map dollar entry which
basically it's not amortized so
sometimes you use a collection you're
more ties your your cars up front so if
you used a kind of like a linked list
right so you basically or an array you
actually basically the overall structure
has some cost and the actual for
alignment cost is is cheap so after some
sighs you're okay using that collection
the problem with tree map is that you're
using the stream a 40 byte array almost
every time so so you get to blow up of
clearly five acts so that's a large
overhead to think about what collections
you use when you have smaller sets so
it's really if you need the ordering and
sometime
using a real double-double like a
primitive double-double it's much faster
but if you need the ordering done for
you for cheap you probably have no
choice but to use this but maybe maybe
you can write a small function that can
do it if your set is not out of bounds
so that's that's kind of the crux of
this look at memory profiles so actually
there's a there's a the average
collection size roughly three so one
thing we did was study all the size of
corruptions anyway here's a memory
profile this is Cassandra the core data
structure for it is a concurrent skip
list and the concurrent Skip list also
has an overhead that's reasonably well
amortized but it is expecting odd range
it does ordering for us and we kind of
want it to order but if you look at the
actual it's one of the top things in our
heaps so that's what takes the byte
array it's use byte array for
presentation that's one of the top top
item there so so which collection are
you covert and so they're a bunch of so
if you're think about concurrency Google
collections and we attend these are very
common good collections out there but
watch out for per element cost so if you
write a small micro program so in this
case you can actually get a rough idea
like a small so before you jump to using
your collection of course write it but
make sure measure it see if it the
collection is growing out of bounds as
you add elements or if it's if it's
coming down to a if you are monetizing
your costs early on so it's kind of a so
the trade-off is is the collection
actually per element cost or not so
that's kind of your big big thing to
look for primitives are cheap and but it
can be hard to manage so if you share a
very large collection then sometimes you
do want to use a real collection as
opposed to primitive but the average
collections are really very sparse and
they're roughly size three so for three
sized collection don't put a whole huge
huge big large collection
that we jump quickly to civilization
peace right so serialization has gone
really I mean Java IOC life serializable
is really really slow and historically
it has been designed to be slow in some
sense because Java expected things to
move from one platform to another
platform or any other platform right so
they made sure that all the protocols
are kind of everything the entire object
is flattened and and all even
information required and wyman to
information required is flattened so you
kind of take the whole thing across and
DC lies it it's really slow so done true
it's kind of like true colors true
platform right so how can you do better
so if you use transient you can ensure
that certain fields that are actually
that need to be serialized that are the
ones that you want can be shipped across
so you don't need to flatten the entire
thing another to the other externalize
abode so if you use an external I
suppose you can write your own role your
own see you Lizer you can say okay all I
need is the ID of the stuff stored in
database and now I don't need the entire
object because I can recreate it here
right if I need to ID don't utilize
everything and ship it across so that's
kind of your external izybelle you could
create your role your own to Eliza book
now if we will hear the gory details of
arrow and Google protocol buffers and as
we go forward but this lack of good
serialization at the core Java has led
to almost the new the new evolution of
Java software they're saying which is
the thrift arrow and Google protocol
buffers based systems and that's kind of
the heart of where Big Data started
again instead of suffering like the the
old of logic WebSphere stacks of systems
suffered they are my core becuase and
and put it inside their overall
scalability challenge the new structures
gave up the old serializable and to come
to this techniques a common benchmark
that's out there is the city's benchmark
and does put if you look at the
roll-your-own it's still on the top the
Java manual and somewhere down here is
Java built in really slow like many
times over slow drift is somewhere here
protocol three protobufs drift and tip
compact it or six is actually much
faster than all four did some
benchmarking recently as well but yeah
it's across the bowl right somewhere
down here it's JSON also isn't the
common popular one it will use it more
often than you expect XML another so
know the spectrum this is actually the
cost so this you're measuring this
actually a benchmark from github very
reasonably common much more surely
benchmark this is may metrics here the
whole object
stylization transfer and dislocation
cost it's the whole full cycle and some
of them are better on allocation some of
them are better on deceleration so some
new ones exist average somewhere here
with arrow there can that cutting has
poor a lot of work around arrows if you
hear him talk any of the new recent
conferences definitely attend and all
gory details of a bro inside the crux of
it is being able to separate your schema
off your object from the actual
representation in this utilization so
one comment and I didn't realize there
is a easy way to demo it on commenting I
usually do it like dump a skill ization
see you last hello world object and look
at it and see what's inside that if you
do a hex editor and look at it people
actually have the entire object fields
and see and then there is a string hello
world so it's very very insightful in
the sense that if you look at the arrow
dumped stuff types are all runtime so
you don't actually
have to predefine all your types you
don't have the predefined and the data
is untaxed you don't need to tag your
data so you can actually get it across
without having to change and all the
fill IDs are are reassigned so you don't
actually have to manually assign manual
assignment or fields causes is there a
prone at the same times also less
effective but then you can ship them
from one gym total gym that you might
have a different version so you use
spill on the schema mismatches if you
had a scheme in this match so if you if
you came up with use one side of the
data the other side did not have the
right schema you could have a mismatch
so so there are some issues with it but
and since there are only runtime checks
you don't catch them during compile time
so all these checks type checking is
postponed to runtime so you may actually
catch it in real time in production and
so that's kind of one of the cons to
that but I mean if you if you're using
the Hadoop family of staff or HBase and
there's a ton of good classes of
programs that are using amaura Google
protocol buffers so protobufs came along
around the time and disparate caching
became very popular everyone started
writing your own protocol buffers and
try to compress a set try to then use
the copies so here you decide you can
predefined your entire message format in
a dot prototype so so if you look at
this schema you take a class if you
every class you create a full product so
once you have a dot proto you can then
generate the other side the class
tomorrow so the interesting thing with
protocol buffers is you actually have
all the types of the fields just like a
database would to have the pins field
matchings so you can basically you ship
the data just as key value pairs and
based on ID retreat we recover it from
the file so very
very database approach that approach
there are data oriented approach to
doing select a new situation reasonably
very fast and still yeah so there is
Android uses for the call before the
cross for most of Google Java products
BigTable and others in the fusing about
the person back swift to sound our users
drift and drift actually is pretty
pretty very layered approach so in some
sense this is almost the closest to Java
in some sense because it actually does
the entire from your client to server if
you shift it shift an object across you
would actually take it through the
entire stack just like you do do tcp/ip
and ship the entire all these layers of
that across and it's really created for
you on suicide all the way up this time
so it's kind of separate it separates
the structure from the actual protocol
and transport so in some sense it is
very portable the same time it does have
that overhead it does have all the
layers does have T socket and you file
transport which you have to manage it's
new you may update so it wraps around
existing socket creates catch and so it
creates exception handling around it but
it's a very so it doesn't differentiate
between socket or file it's it's just
transferring things across IO channels
so it's really a cleaning and I of
protocol expecting things to go across
the wire out of the box so there's no
nuances around it of course so you could
compact it make it compression so you
get some performance over there still
regardless it's it's a it's a heavier
system out there but it is is the most
portable amount
with that it jumped to the next fragment
of this stuff which is UUID so how many
of you have used your new IDs of late so
the good news is we don't use anymore
basic condom call and random and created
ID and as humans its unique you end up
having to use UI d if you have fused
code recently turns out the default
actually behind the scenes I mean so
leashed Lee Schultz is the 128-bit
that's the actual definition for your
idea created might be one so a UID is
actually if you break down has these
various nuances to it right so you have
four parts to your to UID and so this is
the classic put this function up here
because the to string is the most common
thing to call on your UUID people pretty
you re then pretty string out of it and
then use it in program it's a time low
time made the high version and a variant
sequence this is kind of telling you
what what definition of you idea using
and four is the one that you use for
relations if you - that's kind of it for
go back it's so the the basic default
UID is using such a transform and so
it's expensive
also users turns out the default
pseudo-random number generator which is
actually in Sun JVM it tries to get a
seat for your debut random slash dev
slash random even on ec2 is a file
system called right so if you go to the
VMware layers and call make a fire call
so if you are running happily in memory
your call the first for every new ID is
actually goes to the file so you could
disable it by saying EGD file does not
that does not exist never go back and
does about 20 40 percent better because
it actually uses the local garden mix
remember generator which is faster but
fast-forward people today use tiny
varieties time tons of time actually
it's a pretty the current clock time is
pretty good seed to start from right so
using that to create your time you're
adding much faster there lots of
alternatives out there it's about ten
times faster because the time call is
actually a system time release call
right there's an actually open source a
couple of these are open source projects
go down code or another from pre
progressed quarter as well and they said
call that you can look at this links
offline but these are pretty fast
implementations for you IDs that are
used in someone open source projects but
big Gator here's a quick run on on a
regular program that exactly output a
perf top so firfer has a the package
perp that we called out earlier actually
has a program called perf table which
gives you break down how here's how your
program ran and so it kind of gives you
the kernel time on this one is much
higher than the current time on that and
that's basically the JVM I serve here
say Francois program somebody use time
base you a dish where you can much much
faster it's a much of developed mais
program of java four times less kernel
creation a long time and so on
definitely better instructions per clock
causes if you're on AC to try and see
that you don't
they didn't know it yeah peaceful table
project Klein is initially had that I
have not seen the code comment yet so
yeah alright that's that's you Eddie's
hot section yeah so the tip string so so
this thing is Department called time
based varieties actually have a two
string that's optimized make sure itself
strings to string to string theory turns
out shrinks are I mean we could go we
could do it full talk on just java.lang
string because we use it every day and
not until recently byte arrays but not
part I mean character arrays person
betrays a bunch of a long time into
categories so pirates are in and it's
actually new flag that was in the reform
through benchmarking that is compressing
your strings can actually do pretty good
things to your memory so this week's are
very compressible right so the classic
lza algorithms work very well on it so
there's a new flag that came up in very
big huge ad case we've used some of it
as recently well can't watch for it
because I thought actually put things in
production with it yet so but it does
give good performance that fits append
performance a patent actually anyone who
has used open source portals like
Liferay or other style portals find that
they actually have custom implementation
for ends or a why depends completely
actually profile your code and reduce
the number of referenced you find that
your home substantially better and it
said there are a lot of new ones around
append and so strings and worse a string
buffers the immutable strings versus
mutable string buffers lot of different
or heads for that Google has a bunch of
guava project which does a lot of good
good work with Joyner so if a patent
stands out so they have a simple jointer
with a lot of joint stacks very cheaply
turns out if you actually find so turns
out if you actually have have now the
empty strings turns out actually to
check for empty strings if you actually
instead of to use the spinkle model you
actually get much better performance
because the JVM actually goes through a
lot of checking to see this reference is
actually looking at not so skip nulls
and newsprint also if you use null
something default you can actually get a
lot more performance from for your gyres
there's actually a splitter which also
uses behind the scenes eggox like things
not using real remix java.lang string
actually ends up using lyrics by itself
so which is a much more heavier
collection use so it's an interesting
question right so if you have a string
and you want to let's say so different
different implementers over the course
of last few years came up with different
ways to stuff your strings so the 64-bit
string for example if you're mostly not
filling up your data 32-bit
representation and compressing the rest
of the part can actually get you a lot
of lot of performance so one of the
things that if you use any standard
profilers you'll find the first thing
the break on is java.lang string and the
reason is they expect an internal
structure for your string and the common
nuances there is when you touch strings
especially when you depend on inter
structure of your string you end up
having to folk the bytearray yourself
and you're off on your own in some sense
for the most part it's encouraged to not
touch the internal aspects and play with
by trade yourself because when the
global see global system Murray copies
come and play your throne are thrown by
surprise but that's had a lot of like
I've run into this and terra-cotta start
using it they were they expect a certain
layout for the thing and you can
actually as a program look at it and
make sure you're using the right team
and all fusions and play with them but
but you see different results when you
use a different JVM or switch your JVM
underneath but yeah so that's assisting
in terms if anyone's internals is is
optimal if you don't change your
jaebeum's often yeah now references call
from code from Q Khan last year or so
that I would go into garbage collection
if you have any questions this would be
a good spot to stop but it's also good
as you go forward you'll see a lot more
GC focused content
so best practices best practices for GC
start with for burst GC so oftentimes
there's a misconception that GC logs I
turn on you see logs and GC problem got
worse or logging I hear problem got
worse we looked at all these GC logs are
cheap and they are cheap even in
production so be be reasonably verbose
at having them be frisbees put your foot
down if your ops team is not giving it
yourself feel free to be a benchmark
that part and that's actually reasonably
cheap it's not coming in the way of
doing GC for the most all for the most
part actually there is a bit expensive
ones and this error here and so but
that's when you're doing tuning hand
tuning and then they're actually doing
census for your object layout so
possibly for your question a tool based
out of the free list census can possibly
derive what part of that object layout
is strange but it's an interesting it's
an interesting topic but turn on logs
and turn them on very explicitly so if
you get back to the core of GC there are
sarahfey if you really want to
understand you see think of your app as
as in very abstract fashion your app is
when it's running is doing a lot of
allocation right and an allocation is
creation of new objects and then you
have kind of your system defined size
you have you have memory right so you
have size your allocation rate so
allocation rate is filling up your heap
so let's say you have one gig of hay and
you're allocating at hundred Meg's per
second you have 10 seconds of runway
right we really look at your size is you
have yeah I have fed if I did not worry
off any GC I can run this app till 10
seconds that's that's kind of like Faust
principle
right then while you're allocating
you're also changing these objects so
there's what's called so all the
allocation and the mutations these are
all called mutators in GC parlons when
you hear someone say mutators that's
your mutation that's very allocation map
now if not all you're allocating is
staying there so your life heap maybe
like 256 Meg's so then if you have one
gig heap you're allocating at 100 Meg's
a second you know you have 10 seconds
before you run out of memory you find
out that some of these objects are dying
while they are before even 10 seconds so
if for example your objects are dying at
by hand at half the rate you know that
you're always having a live heap of
roughly half your total gigs right you
have a fight whole MV so that's kind of
your trajectory of how your objects are
how your memory is runtime see there's a
runtime equilibrium 100 Meg's per second
the creation and 50 Meg's per second
they're dying so you kind of have an
equilibrium around half a game right so
if you now that we've laid that out now
you want to make sure that you clean out
this the ones you'll only have half the
heap free if you're cleaning up that 500
Meg's right so and so that's where
garbage collection comes in play the
garbage collection is trying to see
which part of your heap needs to be
removed and cleaned up and and trying to
tragic try to walk through the entire
heap have some kind of route set so you
know this this these pieces are always
life your class loaders you say your
basic classes chawal java.lang object
and the basic you hear of poem Jen and
stuff right
so the basic classes and the collect
class around it it traverses all of that
make sure there's no marks all the GC
yeah bowl objects so the marketing part
and then actually please cleans it out
at some base some different algorithms
right when it cleans it there's
different methods to do it but while
it's doing it so let's say it's marking
this here
some of these collectors don't want you
to change what is just marked so let's
say you've marked the spot and suddenly
you've freed that part right so so you
want to hold out your materials from
doing things while you are at either
mark scavenge or or the final clean
stage right the mark sweep and clean now
so at some minor portions throughout
this entire GC cycle you have to stop
things from mutating and that's where
stoppers comes so the entire GC problem
can somehow be translated into three
core free principles which is basically
you want to sustain a very higher
allocation rate and keep it within the
heap size you have you have given it at
a very low cost times right so you want
to give a low cost times and try and see
what kind of size you want to put so and
and try and sustain a very large live
set and possibly a very large allocation
rate so that's kind of the the three
fighting parameters if you oh there's a
fourth parameter and that's overhead of
GC itself so you want it collectors
don't talk about it right so because
they don't want to advertise this part
may be the fourth parameter and thus you
don't want to have a very highly
overhead GC so for example you don't
want to have so if you had two gigs room
so let's say you want you gave it one
game and you created a two gig key
behind the scenes then you have much
more room to play with then your actual
runway is not ten but twenty seconds so
now you can actually play with how you
do the GCS somewhat more reasonably or
you can put all of that in the other
side get very low pass times or stuff
like so you can play it much more easily
the game is easier if you have lot
larger heap behind the scenes that would
be a large space overhead or if you're
lots of CPUs or if you if you if you're
able to run half of your CPUs then you
have CP overhead so behind the scenes
there's a fourth parameter which is how
much overhead can you how much how much
of CPU and memory
can I take away and that's kind of so
when you're tuning GC you hear hundreds
of free hundreds of parameters which are
actually X X star X X stays the real
parameters are here and and that's what
you want to choose
that's what you want to understand of
your application and then you're tuning
falls into place right people tend to
get to the perhaps we the hard
introduction and that's what's true for
me as well for GC is you come from these
bazillion parameters that are changing
their meaning every version but what G
one understand is is the core your
application load your runway how long
your application will run and what is an
equilibrium size for your app and and
and what's the past tense you need maybe
it's okay for you just pause once in a
while if it's a batch program or maybe
your online program and you just cannot
live with anything large pause and I've
seen past time requirements for some
apps that are in the 5 milliseconds
range so for the for a large exchange
for example every millisecond is is
actually almost equal int of a
denial-of-service yeah so there is false
time requirements at the lowest end
which is the low latency guys to pass
time requirement where I'll just restart
the JVM once it hits the GC so I've seen
both sites right now sizing so of course
some of this is simplified and some of
this is only applicable to the classic
hotspot jvn sanjayan
right but when I create a new object so
GC is actually so now let's talk about X
and Max right the first X is in is
introduced here but the real so if you
if you got the last two slides the rest
of the slides are actually not so
important the last two slides were the
most crux of the problem because I mean
you could come up with an algorithm
which doesn't need to go through these
details so sizing so the GC heap so we
took this one gig heap we could have
laid it out in two different phases
right so what this implementation is is
proposing is let's put all youngjun apps
in one
this is the young John Eden and saguaros
vision so it it cuts the cake of one gig
into two parts young John and ol Jen
right so and it's through some mechanism
I'm gonna promote them into the Oh Jen
right
the assumption is that as is a common
assumption which is infant mortal
applications or objects but basically
they say that lots of your objects will
die before they get promoted so if you
ran your collector on this one and
cleaned it out very efficiently you
never need to worry about so you can
basically run two parallel collectors on
each of them with different algorithms
and as long as this guy is is very
efficient and fast you don't observe a
lot of things all over here and since
this guy by definition these objects in
origin have a lot more connections they
move from here to here would be much
more expensive right so that's kind of
the be the designers ideal dream right
oh I have these objects all of them so
let's say we created that hundred MB
they all die instantly
like one second we draw hello world a
hundred times and vanish right in that
case that instantly finished so they
come here immediately collect themselves
a garbage some collector just cleans it
off it's done right so that's so when
you're doing a new objects let's say
you're doing this new string it would be
created here say or any object you're
creating it in the Eden today so it's a
all your allocation rate is a function
of this one so if you looked at
allocation rate and you want to amplify
how quickly your application grows like
rapidly that's your Eden size so you
want to amplify the slicer right well
and this is xmn by the way so this hole
size is x-men anything that gets created
here or flows into a survivor space
hoping that it doesn't need to be
committed yet right but if you have very
well cash behavior let's say you have a
like a bunch of well cached objects they
are guaranteed to go here they're going
to live longer right so when you create
state that's very large or stayed that
needs to be reciting your memory for
long you end up with lot of promotion
so the dream that the jvm designers had
we're all objects will die instantly or
very soon actually did not work out in
reality code
besides longer actually class pants
became bigger
we actually have much larger objects
which are in your palm jam
so there's another smaller space here
called Pompeian which is supposed to
never be collected right even that has
grown to be very large
everything has grown that you actually
by default from what then these days
right
so if you use a default from watching
promotion mechanism so there's another
threshold that the tenuring threshold
here governs how things go from here to
here but that's that's so the default
ones promote very often and fill up the
origin as well right
sure there is a adaptive size policy
which basically says okay I don't know
if I made the right dish and change it
for me right so actually the default is
not so the xmn actually clearly says
this if you said this to 256 Meg's this
whole thing would be to visit Meg's and
this would be the rest right that's
actually crisp cut but often and the
default behavior is you set a ratio you
say and there is a ratio default ratio
if you said that that ratio is over
overwritten the ratio kind of tries to
give you a small size see how fast you
fill up and then changes everything with
it with the same ratio so it keeps
changing it with as you keep building
the stuff the problem with that is every
time you change it you basically resize
not just that you resize these survivor
spaces with it besides everything and
then shift objects during this process
so one of the things we found was that
actually was detrimental to predictable
performance so one of the things before
so that so if you're trying to find a
predictable size you want to cut this
too or do some experiments that adaptive
resizing actually causes on say the
unexpected behave side effects of that
will be seen in the in the past times so
I guess by this time we're aware that
past times are what we are always
trying to force to be small so yeah so
there are some so it's a heuristic and
sometimes works and it works very well
when there's a lot of infant mortal
objects
what we found in theory in practice is
exactly the opposite is that if you want
predictable behavior from your collector
and you're trying to do experiments to
tune setting them the sizes upfront
right and then changing them to tuning
them to your workload actually is more
predictable experiments set of
experiments than letting the adaptive
size policy work for you but on the
other hand people also use new so saying
in that I don't know if I actually have
a slide on that but you can tune the
ratio so you can say minrray men new
sighs - max new size so you can actually
say tune this ratio from here to here
and not beyond so you can kind of say
that don't go beyond 256 Meg's through
that mathematics but it's kind of
reasonably tricky check some masks and X
M s so M s is basically the starting
size right so you start with let's say
you gave it a one gig even though
sometimes when you launch it one gig JVM
even though you don't have one gig keep
you actually pass right so you get a
Java - version and you're like what
happened basically it launched with
maybe 10 Meg's because it was a
short-lived JVM and so so XMS is the
initial size so basically once you set
that you there's no resizing for the
entire heap so this sum of all these is
going to be always one gig so it kind of
cuts that out from the OS make sure you
have that one for one gig so so when
time comes to actually allocate it it's
not swapping so that's kind of the so
that's the initialization size and the
actual maximum size so given that so if
you say that set so now you know you
have one gig to play with and a new
ratio of so and so by the way there
again took the min and Max ratio so if
you use a new ratio or set them to the
same value min
max you do cut you do get the same
effect of an XML which is basically a
fraction of it and it's not so if you
put min and Max you get a resize Auto
resize for for the new gen but that's
it's also a common that's a common
technique people use as well next slide
so now that you picked sizes you get to
pick a collector there's a lot of them
and there is the Azul GC as well which
is not available in the Sun format but
it is in pure pure software now so a
pretty fun book on the corner everyone
everyone I've probably probably 200
copies or maybe under under estimating
there's very few copies of that soul and
very few copies fully read it's a very
academic authentically academic approach
to garbage collection I've actually met
a few people finished reading it and
Richard Jones is actually a good friend
of the Azul community made him but
fundamentally it gives you a very very
good ride and lays the land on what are
the challenges involved in GC tuning GC
designing so whether it's Marc sleep and
so but anyway so for the son JVM guys
these are the options right if serial
parallel parallel old and concurrent
mark and sweep of course g1 which is
still in experimental mode for the
sunlight collectors you have the Azul
JVM which has its own pause let's G see
none of the tuning before and after will
be required because that's all
auto-tuned it is a low cause there's
almost sub-millisecond pastimes on this
at very large chip sizes so it's three
digit k-keep sizes millisecond or sub
millisecond pastimes and and that's all
on x86 hardware so you start with a
hardware solution but the gory details
of that will get to I think at
last talk of the same topic someone
asked me what is the secret thoughts
behind the Azul collector and the quick
summary a guard was that's your that was
that's my my mother-in-law's version of
garbage collection which is collect all
the time to collect food collect all the
time and there's no dishes right so it's
correct all the time no fathers you're
able to look at us but but let's get to
the crux of meat here cereal GC so it's
very cereal linear the new cereal old
the forces when you hit that limit when
you hit the GC you are seeing a stop the
world pause right fatso then came came
along parallel GC which basically does a
lot of threads parallel scavenge
so you parallelized collection and then
there is a serial old 0 parallel old GC
so that there so when you did the OL gen
collection that was always cereal right
and in the default path it's very
popular very widely used actually so
while news
then came panel G&amp;amp;G see which paralyzed
the OL Jam
MS again classic Varney
so to uses parallel GC on your new gen
it uses CMS on the origin and really
when the concurrent market sweep fails
it does a serial all gen so sometimes in
your logs you see concurrent more
failure that's when she comes in g1 is
actually g1 would probably be like 2005
version of the Azul collector which
basically uses pages as your say doesn't
it no longer believes in the infant
mortality it looks at pages and sees
where how density or phase are and picks
the sparse page and collects it that's
cheapest ready to collect and then of
course it deals with the problem of
moving a popular page it's a popular
page since a popular object turns out to
be connecting to every other object so
moving a popper object has a lot of
nuances and that's where g1 has its
flaws but picking a collector
the default I think is palapa parallel
and CMS is a good option that's popular
yes
so g1 actually is circa 1 620 is is
available but if you still need to use
the flag unlock experimental options and
use it correct
stability is still a big challenge there
but of how it works in so on so forth
there are some use cases where it should
work with no flags I mean it should be
so G 1 is is designed with having zero
flag tuning hopefully that works up
reading GC logs is actually a topic in
its own and a very good topic actually
when you see these right so these are
kind of so this this is so now we are
getting into the nuances of CMS which is
kind of the most popular collector of
the day parallels you see I think is
well understood and people know it still
has a full GC in the seed roll part so
reasonably well understood let's look at
this one so when you see Fuji senior
command line your CC logs even knows
that's stopped the world right you have
very poor so and then there is a stop
the wall part in the CMS which is your
initial mark which is actually stop the
world and that's what I was describing
earlier is the initial mark through the
rescan and beaker if weight references
and remark also stop the world so if you
see those in your log files look for
promotion failures so I tried to promote
from yung-jan to all jan and there was a
promotion failure that's a common time
and actually to a full GC after that you
see so the the interesting thing is the
CMS collector is actually reasonably
conservative on its promotion and since
that if you have 10 Meg's of objects
you're promoting from Eden into the
Elgin it it kind of on an app it expects
on an average one more of 10 max is
coming so it makes sure you have twice
the size
indeed in the origin so turns out when
you do a two hundred Meg's of transfer
you need four hundred Meg's of already
if you've used half of it it once four
hundred it triggers a full GC and all J
so so that's the promotion failure and
then of course concurrent more failure
tries to collect the Vol'jin
concurrently and then fails and that
there's a CMS origin collection this how
it looks like and gory details so this
is the promotion failed bar new
promotion failed and then of course a
country mode this is a real just cater
main customer five seconds of full past
time has happened here in this case not
not that common now that I'm oh now that
part of my move from Azul I've seen more
of these in my customer base but that's
that's the that's the full the GCPs from
people look at this problem you don't
something you can disable collection of
poem actually so that there's a flag
there people and then you can also tell
promotion now not to be that
tunics a mess so the next three sites
are probably going to take us through
tuning CMS specifically because that's a
common problem these days the the well
introducing a very new item their most
problems of GC are actually solvable the
ones that are very hard to solve are
actually fragmented is actually
fragmentation and and so so if you give
a get a quick idea of fragmentation
let's assume this Rubright when we came
you came one of the first person to come
in it was an empty room so we could fill
in a hunt of tens of friends or for each
cave they could sit on a table 70 Nava
under the hand you actually have a lot
more sparse spots in there so if
a team of four a gang of four comes into
this room they would have to split
because they don't have four consecutive
seats under that so so that's the notion
of fragmentation you have fragmented
this room this actually is a very high
residency and thank you for coming again
but but coming in at smaller sips one of
one each came this fragmentation is not
a problem for us so if people came one
and since we are objects are one see th
those object those people can sit on
those chairs there there's no problem
so fragmentation is a problem when you
have large objects it's our irregular
play sized objects are coming into your
heap
it's definitely a problem it's it's it's
a big problem if that's happening it's
not a problem she have uniformed sized
objects so so the crux of of tuning this
collector is basically to a wide
fragmentation if you can fragmentation
is the goriest
the most difficult problem that said how
how do collect aeration collectors deal
with it
did you a full GC they clean up all the
things move everything order them all
into one side then you have large open
blocks again you can allocate again
freely allocate rapidly as well right so
you coalesce blocks around you so you
can move people to the side kind of get
more space here and create a size of
four or five so so in some sense kala
Singh of memory blocks into and
collecting doing compaction cycles and
compaction cycles are all stopped evolve
and that's in in the traditional GC
collection installation son collectors
and that's why fragmentation and
compaction are all or and compaction is
not so CMS the Sun doesn't come back and
so that's a big problem for CMS rapidly
fast-forwarding there are other
interesting jurist expect in to see CMS
the CMS initiating occupancy factor
which is when do i trigger
the old collector when I know the
initial occupancy is this much on the
new one right so that's so if you
usually the default is something like 65
and that change is based on how your so
the diffraction of occupancy so you can
set this and make sure that's what's
used instead of using a heuristic which
is again adaptive technique a heuristic
built for certain assumptions and
doesn't work for the worst-case 23 times
again a function of so size your
generations well so that your min GC
times are know that their function of
your live set not your total heap size
you can have 10 gig but if your life
heap size is only half a cake you're not
going to be really gain system GC is
another common someone actually use
system got GC the reason to come to this
talk so for them if you use this flag it
invokes a concurrent system that you see
not a real stop the wall GC GC threads
so the trade-off in CPU memory comes in
around a number of threads so parallel
GC threads of size 4 so that's another
common technique to get 4 threads
running in Pablo the default is actually
derived from number of CPUs on the
system and it's a pretty complicated way
of deriving it but that's what it is so
so it falls off what what was found is
that if you you can't have 30 for 32
threads and get performance off GC falls
off and so there's a 8 is the kind of
the flipping point if you have more than
8 threads or HCP use you end up using n
minus 5 by 8 as they have so so kind of
16 assume that you get scanned at 16 if
you have 64 threads
so the strategy is to tune your min GCS
and then you can let a lot of allocation
happen in the Eden so that's kind of
here if you know your Minji Caesar low
then you have a lot of allocation rate
possibly there but that's so this is
your CPU trade-off so you could do a lot
more soup threads and you're taking
basically you're taking stealing CPU
cycles from your application work to GC
so that's here or this one fragmentation
so you have an app that runs very well
up until it suddenly up until suddenly
something happens and knob turns and
performance caused degrades it induces
you try to do a full GC from j console
everything is happy and you continued it
wrong right so that's that's the classic
fragmentation use case we talked briefly
in detail about this so the real trick
is that when you have so basically this
so if the ganga for did arrive here this
whole table is actually not available
for allocation it's considered round off
error and you kind of let to find it you
and trigger a whole GC so the so if you
use a compacting collector fragmentation
is no longer fall and that's where i was
mentioning compaction which is heavily
avoided by all the collectors the azul
collector compacts always right it's
it's compact always and and got more
interesting nuance to this very parallel
highly concurrent collector so it does
both generations in parallel in complete
concurrency so kind of the window of
actual past time is very low so the eden
is collected in parallel that the old
gen to completely concurrent collectors
running for you all the time and so and
all the time and all the time it's not
triggered by events in the in the
application what's happening
continuously so it's kind of a the only
compacting collector which which proved
the paradigm which hit the nail nail the
opposite side it's basically don't the
wild compaction do it all the time so
doing it all the times we found that the
past ants were actually relays and then
you don't need to worry about all the
ways parameters
so promotion so if you delay your
promotion
so as you promote you're basically
landing your objects in this all genpei
and corrupting all the pages at
different paces if you promote less
you're fragmenting less so in some sense
that's kind of the real solution that
people have found is to be able to tell
the uniformly sized objects
she used arrays or primitives you have
some control on that buts in java
parlance exactly very hard object pools
were around for some time again all of
them have their own issues and nuances
so off late and so the the the last few
slides of this topic can actually are
actually topics of their own we have
seen it topic recently by Todd lip gone
from that's based in which first done a
complete talk after this which is
basically go off heap right so we'll
touch a little of that how do I build
off it but so we've talked about
compactions of the world and why it's
still yet unsolved on Orcas on hotspot
they're working on it but clearly this
is the way to go on that front if you're
suffering fragmentation talk to those
who guys there's tools again let's look
at some tools here J rocket Mission
Control has a very good tooling on GC
you can look at the tour over all pauses
some more pauses in different
generations can look at compaction you
can go get to the gory details of how
long the mark took at every level so
this probably aren't the best
tooling I've seen and some people are
actually getting to going back to
reusing the j-rock machine control to
use it to get kind of an idea on how
your objects are laid out you can
actually double click and get what type
of objects they are so in some sense it
can still answer your question but it's
a derived answer it's not a direct
answer it's a good to good
you think to talk to your kid guys you
may actually pick it up
this was a this actually thing in
production that we revisited and
basically applications suddenly
transitions to a backtracked it was
reasonably well you kind of think it's
doing okay it's some stability and
suddenly goes back into that back to act
nothing changed in that occasioned rate
of the behave of the of the of the app
itself so somewhere around forty percent
full it flipped into into a
fragmentation wall that said although
you thought you had half-empty it was
not you actually we're full because of
fragmentation so you cannot use the free
memory because you have way too many
holes and that's that's kind of a the
most the most common problems we see in
GC are here and these are the hard
problems of the day yep so just actually
from production clustering running
Cassandra so again there are some
interesting tools here still J console
visual GC logs
actually a tool that came around your
two thousand by the team that works in
the same I could make as the GC Richard
Jones book that we talked about
it's called GC spy which kind of gives
you kind of your memory in pages and see
how it's laid out kind of forgets that I
am NOT it's kind of worked for me on JDK
one six but I can't watch for it using
the introduction but it's it's a
nevertheless the core is out there it's
open source we're gonna use so but this
people have been talking I'm sick of the
GC stuff let me just go build my own off
heap so the no GC movement if you all
like the North Sea Coleman or even
there's no sequel guys are trying to
build off heap stuff right so crux of
that off heap and and I'm kind of giving
you the initial tantalizing pieces so if
you really go off of this topic you off
of this talk if someone asks you I'm I
don't worry about GC I'm gonna put it on
on outside the heap you'll have
something to to communicate with those
guys
but there are a lot more golden eagles
there the crux of that is actually a
bytebuffer
right so when the the buy before
allocate direct and novice circa 2000
right very early on java kind of assume
that you create stuff outside your heap
right so you kind of we can do it Jane I
kind of the that's almost around the
time of GN I you could create a slab of
memory outside it doesn't care if it's a
file or it's really in memory so you can
put put a map file or actually chunk 16
gigs of heap 16 Meg's of heat outside
the designers of Gillian's and at the
time it son okay assume though and and
I'm I'm kind of guessing here
that side the heap now suddenly the JVM
is not transferable right it's not as
portable so this was kind of always a an
orphan child in the notion of how you
deal with it right so now you have these
outside long objects that are going to
be how do I clean those guys so you
don't have a memory control on it so if
you look at actual design like partners
who is a software and when was read
software fundamentals and kind of the
Boehm time very early on how do you
build good program code they they came
up with this notion of virtual machine
so the JVM is not the first virtual
machine that was invented the notion of
washing machine has been touted as the
way future will be in 1970s and 80s
right and the the crux of that is that
use virtual machine as a paradigm to
build build build your apps so then you
can virtualize the file system around a
die around it the network around it and
so that's how the original concepts of
programming models were created and and
turns out one of the things that clearly
said is that don't don't corrupt that
view by adding a slight workarounds
right because then it's no longer a
virtual machine in sparse virtual wash
real no longer apply the same paradigms
or programming techniques of building
this right and that's kind of the
the the view point of its which JVMs
have looked at at off going off here is
that that thing is now no longer
manageable it's outside I can't I want
to make sure it's very small right so
the first immediate thing that they did
was make sure anything that screener
outside Heep is very small but with the
with the new I am gonna build a big
large cash upside the heap you're you're
heading into that territory and if
you're aware of that that's and managed
that very value you're probably fine but
that's that's the notion you are that's
where you are so you actually have to
tell everything else that's happening on
that machine which you have no control
on can be impacting you as well so you
can trigger swapping for example of
something else kicks in so there are
other things there but nevertheless J&amp;amp;A
and allocate direct allows you to do the
two down dynamically load like a c
program without having to incur the cost
of pre declaration like Gianna the
question is is this an IO based or yes
yes so this word so things got better so
now your whole an aisle package has
gotten much much better in a recent
version so we can actually even venture
to do something like this and but early
on that was kind of like the first use
case of creation and J&amp;amp;A itself is again
very clean and if you run a simple
program you'll be impressed
oh my god I can actually on an hello
world and see you in the same JVM and it
works but really it's not
there's no compile time declaration so
you will have potential runtime failure
so we be aware of that but it does work
for limited use cases in the labs the
people who are going that way even
Cassandra is headed to try and do some
off heap analysis and see the existing
performance gains have not been as big
yet let's talk about that here is just
to consider and so that's kind of the
how do i D allocate memory on this the
crux of that is that you resort to
finalization finalizar
which basically are kind of the last
gasp
calls from JVM or that's where so all
the all the references to this outside
heap are maintained and kind of like in
a local in JVM site so the clearance so
let's say even if we freed that memory
they need to come to this object which
reference referred it and drop it off
this object is actually car relegated to
finalization which means it's the last
so unless you're a full GC you don't
actually see that memory freed up so if
you have frequent full GCS and some
people have worked around by creating
systems or GC calls periodically then
you can actually clear that out so this
is a very tough bug ID four four four it
means it's like mid early two-thousands
very old bug has been there for a long
long time and finalizes are one threaded
so again so this or the other nuances
there if you're aware of those and so if
you have done a gives a program for
example in Java you would have seen the
effects of something similar already
like as you keep doing this so if you're
gzip is not clearing it up so if the JVM
is not clearing out that programs memory
use periodically then you would see that
you actually stack up behind a slow
single-threaded program and kind of
cause a notion of a of a of a GC of a
memory leak it's also big it's a weak
reference that's using that which ends
up being the final item you can have a
workaround to that you have a max direct
memory size so you can actually limit
the damage it can do in case and so
there's a new flag that came in actually
pretty recent flag but you can set the
max to edge and resize so you never
actually go beyond that and cause the
overall JVM so so yeah the notion of
having to trigger in system GC to avoid
in the first place if system do you see
kind of defeats it but there are
certainly use cases where you actually
need to go that route because of
fragmentation
and hopefully yeah so this is other your
trigger assistant you see in a wider
league last part which I kind of
expected not to be here and that's why
it's virtually you got here virtually
there is it turns out to be a long talk
right on virtualization there are a
bunch of interesting interesting new
wants us to be aware of and you think
about GC think about Java thread
priorities it's one common thing right
so thread priorities are very scheduling
becomes pretty much an art because the
VMware is relaying your your priorities
for you all over again
there's a common ballooning driver it's
a common philosophy that's taught on how
you manage memory in the VM world and
for gbm's it's clearly the way to do it
is to disable it you don't want to get
your memories reused to some other app
while your app was kind of not busy
essentially if you don't want to balloon
your heap on to other processes time
time itself is an issue on has been an
issue it's kind of been salt in the
latest be aware your sex kernels and
with the latest hardware but TRC and
older hardware and older if you are on
older VMware solution it has it's very
relative so the counter PSC is not so
not all counters not all performance
counters are and counters are
virtualized so you cannot measure things
where they're measured on real hardware
so worth being aware of that is is worth
while not all GC ec2 instances are the
same as well see different you get
different performances with the same ec2
instance
again the latest Nehalem hardware gives
you virtualization for direct path IO as
well as vtd and me TX sir but but if
your i/o if you're an i/o one app be
aware that if you're not using this you
have a substantially different
performance basically it allows you to
bridge from your VMware guest to is
bypassed the hypervisor go directly to
the network interface and that's what
this is doing so if you have it are you
intense your app try and use that if
you're a virtualized so that's kind of
the big big picture out there JVM still
happens to be the most popular platform
of the day there is no question about
that Java may not be the app you're
using to develop I may not be the
language you're using to develop your
app but if you're using any variant of a
runtime the JVM is running most likely
your app on the top so so different
languages but they're all trying to go
back to using the JVM which is pretty
old and has matured as a platform there
are these are the kind of common
heartburns
at large scale and there are we have
touched on some solutions on those
shares so I think the question is is it
still still get keep still the
limitation for sunji see if you have if
you have a low latency requirement right
so if you have a low latency requirement
that is still the truth if you still
have maybe you've seen it grow to three
and a half cakes for for some workloads
but it's still the common case that
small heaps are better than large chips
but if you have a stateful app for
example such as Cassandra bear where you
want a lot of live set if you have a
very large life set we have found that
larger ships have been used by customers
with periodic possum and we try to
reduce that full GC occurrence
- once a day or once every two days but
reduced you're kind of managing the
problem because the solution while the
solution is being worked on by Oracle's
rule and other guys we are waiting for
those to become mainstream and use them
for all shapes but if you get it to
physics Kiki
you know you can do a lot more today I
kind of expected not to be talking about
GC in 2011 honestly I mean I I mean I
was designers like designer like you
linked j2ee apps in 97 98 and suffered
GC in production I was like wow this
problem is to be solved right so having
put a decade of work on it I kind of
hope that problem is gone and we not be
talking about this at all and happily
talking about stuff that's even more fun
but yeah so we cut the algorithms for
working this out are in the works I
think in all the GCS right in all the
JVM and all the GCS rocket the Sun
Oracle Sun so my guess is this problem
is is is on its last leg post and Oracle
is a very customer centric company just
like the others so customers have
suffered this problem it's not a it's
not a problem that's it's not a vacuum
problem it's not a problem looking for
customers it's actually a real problem
so there are two routes and one that I
present both of them one was we go off
heap and build our own I think there are
some GC JVM changes that need to happen
to make dafif work well and that's the
kind of like not relegated to finalize
urge but make it a mainstream thing but
the other side so there are some
designers are going that route the other
route is wait out 1-7 hopefully she sees
solved their g1 is this promising to do
that or maybe the Azul GC we can so
widely
openjdk worship like that it can be
widely applicable but the algorithms are
there I mean this field has been
researched for four decades and there's
the baby of you kind of snuck ourselves
into the first implementation second
implementation and now we're trying to
carry that parallel a GC version into
perpetuity I think this problem needs to
see Bill is very likely on its last legs
at which point my guess is that the
average except I mean memory has become
infinitely cheaper and very easy to get
a 1564 people wanted launched at 64 gig
heap and so and that and you want to
have it like if you're running something
like a world's TV you won't have all of
that in memory locally and run it
without full GC pause so Mike my
suspicion is that it's it's very likely
going to be a problem that you shouldn't
be even looking at and and JVMs are
enough problems even with the new Scala
and others are all going to be JVM baked
so this problem is not not a long-term
poem so so for the young people who
often come and try to want to spend time
trying to change ECI just tell them
don't worry about it
Patricia is one of them yeah I mean it's
that's how I started and and and and
really having looked at the problem the
different approaches it's really a an
implementation child problem it's not a
real problem problem it's not physics
it's not it's not definitely not rocket
science and so and I think that that's
kind of the learning off the azules
story is that even without the hardware
we are able to sustain actually without
how do we do better and since in the
notion of faster faster CPUs have
actually helped free pass relatives and
the new hardware has caught up right the
Nehalem sir I have too high a QP I have
memory bandwidth they did do better so
so all the architecture we proposed
and kind of front round and test run if
you will our hardware of today where you
have eight cores
you have a very fast memory controller
on chip you get the high bandwidth so
you have four such Nehalem chips you
have a pretty good system today yeah so
the question is how if I'm a mobile
phone developer and would how would I
write how do I tune my GC or what
collector can I use and what's the wall
doing they're so small JVMs actually is
a so well understood problem but the
small JVM is being able to tune GCS
actually said well people attack the
problem very early on and it's actually
quite solved problem there are different
vendors metronome is actually a ivory
metronome is very good at that and it's
actually also growing into being the
began one but that problem the real-time
GC guys which is the low level low and
smalls kind of like kind of a small size
JVMs that problem is removal salt even
traditional collectors do very well at
small tip sizes so jiwon kind of is also
g- goal is to go from four to sixteen
gig size but if you're suffering GC at
the small size that's g noble and that
should be tuned so so that problem you
should actually do look at the playing
it through the logs put it in this excel
spreadsheet and and find the right size
and you're probably done yeah so the
truth sticks met and less size that's
actually a well understood problem and
there are a couple of different
collectors it's actually one collector
GC company in in germany actually which
actually focuses just purely on that
small GC small mobile and android
environment you see but traditional all
the other collectors should do a
reasonably good job there are l GC
should probably work for you alright
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>