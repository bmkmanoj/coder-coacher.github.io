<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oracle to PostgreSQL Migration with ora2pg | Coder Coacher - Coaching Coders</title><meta content="Oracle to PostgreSQL Migration with ora2pg - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oracle to PostgreSQL Migration with ora2pg</b></h2><h5 class="post__date">2011-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w23DrHi0lD0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming everybody my name is
Kevin Kemper I worked for consistent
state and today we're going to talk
about order PG I'll do a little bit of
an overview we'll talk about where you
can download it there are some
prerequisites we'll go through a high
level look at the install we'll spend
most of our time looking at the order PG
config file and then I'll throw out some
some ways that we've used it in the past
so order PG is based on Pearl DB I
underneath the covers you need a working
connection obviously to both Postgres
and Oracle you can actually run it in a
way where you only use the DVI to
connect Oracle and then it dumps flat
files for you and you use P SQL or
what-have-you to load your data into
Postgres order PG will dump the full
database schema you can identify
particular schemas or you can do them
all from Oracle it will dump tables
views sequences indexes it'll even do
stored procedures and functions however
the stored procedure and function
migration is a to use their words a
work-in-progress so it's not perfect but
it's actually not bad it's it's pretty
good we've used it a couple times and it
gets you 80 to 90% there in fact the
last time we used it we had to make one
change to the code and it was across the
board but it was in the way that the
functions called other functions we had
to make a single change and everything
works out of the box which was nice
likewise you can export the full data
sense for the tables you specify you can
specify individual tables you can
specify a where clause and only dump
part of the tables on and on and on
there's this
particular tool has more options than
then you could believe we we probably
use 10% of the options at best most of
it a default values works really well we
haven't tested a lot of the you know you
can do this and that in certain
scenarios but in general it's it's
pretty solid tool so if you wanted to
download it you can get it from
SourceForge likewise if you go to the
second link here the hora de 2 PG dot
Darryl D net that's basically the order
P G homepage and there's a lot of good
information on that page it talks that
the documentation is out there the
general approach to usage is out there
and obviously the links to to jump over
to SourceForge and download it are out
there in terms of prerequisites you have
you need a valid Postgres install a
valid Oracle install and at least
version 5-6 of Perl likewise just as a
side note the oracle install and the
Postgres install do not have to live on
the same boxes where you install order
PG so you can have Oracle on box a and
Postgres on box B and you can even put
order PG on say box T and use it to do
all your conversions from the same box
which is the last customer we helped
with this
that's exactly their plan because
they've got 70 some database servers
they're going to migrate and they don't
want to go through the the process of
installing or 2 PG and getting it
working on every single box do it once
and do the migrations from there
likewise perl dbi needs to be installed
and working the DVD Oracle package needs
to be installed and working the DVD
Oracle package is a little tricky
meaning it generally works but you have
to fiddle with the the parameters a
little bit so making sure that the
obvious of the Oracle home is that in
the Oracle said it's at and so forth all
the main Oracle Prem
there's on top of that you need you have
to point it to the Oracle client
libraries you cannot point it to the
Oracle the server-side libraries plus
you probably will have to change the
permissions on the Oracle client
libraries so if we in our case we were
running the function or running the the
tool as the Postgres user on the server
and of course the Postgres user doesn't
even have read access to the Oracle
client library directory because those
are owned by Oracle so we had that we
had to do some permission resets not a
big deal we did a change mod and added
our forever just read for everybody from
the Oracle client library down and
everything worked from that point
forward in terms of installation that
the download is a tarball
so uncompress it you CD into that
directory and like any any other perl
module you execute the capital m-make
file dot PL as as perl and then run your
make and your make install there is a
make test that make test is pretty
involved and it's fairly difficult to
get it to be successful all the way
through because of all the different
environment variables you need to set
for the tasks so we actually skipped the
make tasks we did the make and make
install and wrote a quick little test
script to make sure that that the DVD
oracle was working to connect to Oracle
and that DB d PG was working to connect
to Postgres and we were good to go after
that so likewise once you do the install
it's actually going to give you an order
PG perl module in your your perl site
repository in user local bin it's going
to give you to order and Ora to PG
binary and it it creates a Etsy
directory in the directory you
uncompress your tarball and it puts an
order PG config file in there the order
PG config file can live anywhere because
you use you specify it on the command
line when you execute the tool so the
config file itself is significant in
size we'll go through some of the key
variables but I haven't covered all of
them so the one of the most obvious
sections is the oracle variables we need
oracle home we need the oracle DSN the
username and the password the oracle DSN
if you specify it like we have here DB i
: oracle host equals host cid equals sid
name that works about 50 percent of the
time it depends this is completely
dependent on how they've configured
Oracle in the Oracle side so for example
in a number of the servers we converted
we had to specify it as leave off the
host and specify the SID as host name at
sid dot schema sign it sort of thing
most oracle DBA s are pretty familiar
with the different syntaxes for
connections the different syntax you
would use for a DSN an oracle DSN
connection string and if if the Oracle
DBA s that you're working with assuming
there are some if they can give you a
command line version of how to connect
to the Oracle database you can basically
plug that in as your connection string
and it'll work
obviously the Oracle user and password
there is a user grants
variable by default it's set to false if
you set it to true
it tells order PG that you're not
connecting as as sis DBA or somebody
with the equivalent of super user which
means that it'll hit different system
catalogs to try to pull the list of of
information that it needs to build to
generate the DDL for you and to extract
the data so we obviously ran as an
Oracle super user as this DBA so we
didn't have to worry about it but in the
case that you don't have that access you
can still leverage this user grants
variable set it to true and then you can
log in to Oracle as an enough as an
authenticated normal user as opposed to
like sis off or sis TVA and it will
still work for most functions the areas
where you're going to run into trouble
is if you actually want to pull some of
the system catalogs which in our case
with one client in particular that was
pretty important to him not sure why but
they wanted to pull the system catalog
data as well
export schema is again it's off by
default which basically says export the
create schema command there's a schema
schema name variable schema equals blah
and if you set that to a schema name or
a number of schemas separated by spaces
those are the schemas that are going to
be exported when you run the order PG
tool likewise there's a Postgres search
path that you can set i we did not need
to tweak this everything we pushed when
either into the schema the originating
schema which already had the path or
tables went into the public schema which
is default likewise this this type
variable is probably the the primary
variable you would tweak so order PG
works by it looks at this type variable
and based on what you have in here
that's sort of the the operation that
it's going to perform for example if we
have table as our type variable it's
going to effectively dump DDL it'll dump
all the create table statements likewise
it'll dump the alter table statements
but it won't dump the indexes there's a
separate actually my bad it does dump
indexes likewise if you set it to data
it will dump the data but it'll dump as
it as insert statements if you set it to
copy
it'll dump the data as copy statements
so on and so forth you can dump views
when you do grant it dumps all the
permissions sequences triggers functions
so on and so forth
likewise there's a variable called
tables by default it's commented out if
you don't specify anything it dumps all
the tables in the specified schema or
schemas or if you're dumping data it
will dump data for all the tables in
that schema or schemas however if you
put a table here or a list of tables
separated by spaces those are the only
tables that order PG will dump
there's annex the opposite of that is
the exclude variable so you can say I
want to dump everything except for the
list of tables that I give it in the
exclude variable there's a a skip
function or variable that basically says
I want to skip foreign keys primary keys
unique keys so on and so forth all this
will do is say effectively don't don't
dump these constructs for me dump
everything else but what I specify the
data limit is by default set to 10,000
is that ten thousand a hundred thousand
which means that the each copy statement
is going to dump a hundred thousand rows
and then it will add multiple copy
statements to the file you can tweak
this obviously just be careful about
tweaking it too high because you would
really want to force your machine to get
into a swapping scenario it's not so
much of an issue if you dump to a flat
file and then you want to push that over
to say a Postgres database server that's
dedicated box lots of RAM and run your
copy statement from the command line
it's more of an issue if you have order
PG connect directly both to Oracle and
Postgres and do a memory to memory
migration if you out
there's a modified struct parameter
basically this one says if I specify a
table I can give it a list of columns
and those are the only columns for that
table that it will dump so if you've got
a table with 47 fields but for the
migration you only care about two of
them you don't have to dump them all you
can just dump the two columns you care
about there's a replace tables this one
it's interesting
thus I kept it in the presentation but
we never actually used it you can
specify the original table name and a
new table name effectively all it does
is a rename for you I can see where this
would be valuable but again we didn't
use it
replace columns does the same thing you
specify for a table you specify in the
per in the parenthesis a series of
original name : new name original name :
new name and it'll rename the columns
within the table for you if you specify
the Postgres connection parameters then
order PG by nature well go ahead and
connect to Oracle and pull the data and
automatically connect to Postgres memory
to memory and push it straight into your
Postgres database if these are not
specified or if they're commented out
the default behavior is for order PG to
dump two flat files or a flat file and
you can control we'll get into that in a
few slides here whether it dumps to a
single file or dumps multiple files for
you the PG DSN PG user PG password
standard pearl dbi connection strings
likewise you can have a ware parameter
so in the examples we have here the top
one where one equals one or if we put
some of the where clause in there we
could specify a where clause that
applied to all tables or you could say
where and give it a table name and then
in
in square brackets give the actual where
clause and you can have multiple table
names and where clauses separated by
spaces so it'll apply those where
clauses only to those individual tables
there's an output parameter you can set
the by default the output is set to
outputs equal and that's the output file
where it's going to want to dump all its
data if you don't have the Postgres
connection string variables set if you
set it to outputs equal dot GZ it'll
automatically do a tart GZ for I'm sorry
it'll do a gzip and likewise if you set
it to be z2 it'll do a be unzip however
you have to make sure that you have the
Perl compression the B zip to
compression package installed by default
Perl knows how to do gzip but it doesn't
know how to do B be zip - if you decide
to specify B zip - then you also need to
specify the path to where B sub 2 lives
and the B zip parameter likewise the
output directory by default is going to
be your local directory wherever you run
it from however if you uncomment the
output door and set it to some directory
that's where all your output files will
go there's a number of parameters that
allow or 2pg to try to defer constraint
checking so there's an F key deferrable
which will defer all foreign key
constraint checking and there's a defer
F key option that defers all foreign key
constraints during a data dump I'm not
exactly sure what the difference is but
it's nice that they have the ability to
defer the constraints because if not you
get into some odd situations likewise
and we'll cover this a little bit later
- there's ways that you can tell it to
separate everything what we did just to
jump ahead a little is
be dumped all our DeeDee Alda desk and
we dumped our indexes and our
constraints into separate files then we
did all the create table statements and
we would do a manual review of the DDL
to make sure it looked okay and then we
run it against the target database and
then when we did the data migration we
would do a direct memory to memory
straight from Oracle into Postgres and
that seemed to work pretty well and it
to be honest gave the client a really
high level of confidence that you know
they have the ability to take a look at
the DDL and make sure that everything
looks kosher there's a drop F key and a
drop indexes parameter that says drop
all the foreign keys before the data
import starts and then replace them
after the import is done and likewise
for the indexes drop all the indexes
when you're doing a copy or a data
migration and then replace them after
it's finished which pretty powerful
parameters because of course we can get
better performance by dropping all the
constraints and indexes shoving the data
into the database and then recreate in
all the indexes there's a PG numeric
type variable by default it's set to 1
which says it'll replace the the numeric
Oracle type with either a big int or an
integer depending on how big the number
actually is if in there are certain
cases where you want to set that to 0
but in most cases if it's strictly
numbers then it's a good idea just to
let Postgres change it to an integer or
to a big int we found that in 99% of the
cases there I think there were three
instances out of about 1,500 tables
where we had to come back and modify
what order PG had selected based on that
criteria in our case there were a couple
of numbers that should have been begins
that it actually converted to integers
we ran into some issues with that
this parameter the datatype parameter is
basically a way for you to specify the
mapping yourself instead of Latin or to
PG do the mapping for you so you can say
I want a date to be specifically a
timestamp or a timestamp without
timezone or whatever you want it to look
like I want along to come across as a
text and so on and so forth so if you
specify the data type variable you can
specify the mapping yourself one note is
that mapping will be applied to all
tables you can't specify just a
particular table unless you go into the
tables variable and set it just for a
couple of tables and do a run and then
go back in and pull those tables out and
put them in the exclude tables list and
then do another run with one with the
mapping on your own and one with the
order PG mapping there you can set the
the default language likewise there you
can enable the PL PG seek welcome
conversion convention in this case we
left it at the default and we didn't
have any issues with it but in again if
you look into the documentation it talks
about if you're having issues and your
your your data migration flat isn't
working right and your number running
into a number of problems where the the
data won't load on the Postgres side
then this is one of the options to try
is turn this off by default this is
trying to make sure that the the oracle
dumps become as compatible as possible
with not only Postgres but with PL PG
sequel in terms of the stored procedures
in particular there's a files per
constraint and files per index variable
which says by default they're 0 so if we
were going to do a dump and we're going
to dump the flat files everything would
obviously go to the output sequel file
however if I said file
our index and set that to true then
every individual index
I'm sorry every individual table would
get a new file with all of its indexes
in that file and likewise if you set the
file per constraint for any constraint
so effectively you're going to get a
table and all of its alter statements in
a single file and then all of its
indexes in a single file so you'll have
a propagation of output dots equal files
named per table name but it is
beneficial in terms of it gives you the
ability to have a lot of control over
how you do the migration and what order
you do the migration and so on and so
forth
we found that the tool works really well
but it's not necessarily a good idea to
point or to PG at an Oracle database and
just say convert the whole thing top to
bottom because there are small hiccups
along the way sometimes you need to
tweak the DD all a little bit and the
other thing we ran into is if we tried
even just for the data migration if we
said do copy statements 100,000 rows at
a time and do the entire database table
wise top to bottom that we ran into
memory issues so we and we'll get into
this in a couple slides here we actually
set up a script and we would pull the
list of tables from Oracle and we would
loop through all those tables and we
would effectively run the data migration
one table at a time
it was automated so it was still a
single function that the user had to run
however it was much more effective
because it would run a table and then
basically exit the loop and come back
and run another table and we all the
memory issues went away there's a file
per table which again I like the other
parameters we just talked about the file
per constraint and file per index this
one would say when we're dumping either
data or when we're dumping DDL we will
get a single file per table likewise
there's a file per function
which same thing every function if
you're going to try to migrate stored
procedures and functions will go into a
separate file we dumped all our DDL in
terms of functions and tables into
single files so we dumped all the
functions into one file we dumped all
the tables into a single file but we
dumped our our constraints and our
indexes into a separate file so we could
sort of manually control dumping the DDL
without any indexes or constraints do a
review and apply that and then migrate
the data and then come back after we've
done a review and apply all the
constraints and indexes and it works
pretty well truncate table is just that
it will truncate all the tables before
the data migration you can set the
client encoding if you're having issues
which we actually set this because the
last client we did this for was using
some off-the-wall encoding that is
pretty non-standard so we had issues
even though their database was set
without encoding the migration was
trying to force it back to a Latin one
and it just wasn't working out a couple
examples I'll walk through so again like
I mentioned we would migrate the DDL
first and we would do that to flat files
then we would migrate the data and then
we would come back and migrate the
functions so in terms of just a
walkthrough of what we did first we
wrote a script that would generate a
table list for us and the script was
nothing more than a wrapper script
around order PG so we would set that the
oracle variables we set the schema
variable we left the tables to blank and
we set the type variable to show tables
which actually isn't in the list in the
comments but if you read through the
documentation it talks about you can set
it to show tables and it will show you
just a list of tables likewise there's a
show schema variable or value that you
can set it to which will simply dump a
list of all the schemas for the Oracle
instance you're connecting to which is
whole nother kind of because the schema
and Oracle isn't really a real schema as
far as this Postgres guys think however
it will give you a list of names that
means something to the Oracle DBA s so
in any case we had this rapper script
and we would run it we would execute
order PG and give it the config file and
basically redirect that to a file that
contained it contained only the list of
tables and then we would migrate the DDL
based on that table list so effectively
we would set all the oracle variables
set the schema would leave the tables
blank and set the type to table and in
this case we would set the file per
constraint and file per index both to
one so what we would end up with is an
output sequel file with all the create
table statements and then per table we
would end up with a file with all of the
alter statements and a file with all the
create index statements which made it
pretty easy to walk through do a quick
review of the DDL make sure that it
looks appropriate and then execute the
create table portion of it against the
Postgres database so again we wrapped a
script around this it would execute
order PG and it would give it the proper
config file we would send the output to
a log in case there's errors and then at
that point we would go in and manually
run the DDL against the Postgres
database likewise and we didn't do this
we left the views and the grants and the
sequences and so forth as part of the
main DDL file but you can split all
those out as well and then we came back
and we would migrate the data so again
we would go back to that original script
we wrote to generate the table list and
we gave it the show tables and we
redirected that in this case to a a file
name called tab last so after we applied
the DDL to Postgres we would set in
addition to set in the oracle config
settings and setting the schema in our
case we did have a particular schema we
were leveraged
and we would set the type to copy and we
would assign the proper variables to the
Postgres connection settings what the
script would do is it would walk through
the tab list file and it would
effectively on the fly plugged the the
tables variable with the table name one
at a time and then it would execute
order PG for us and in this case we
effectively got to do the copy memory to
memory we didn't have to dump the data
to disk but at the same time we're
minimizing our use of memory if you well
and we found that by doing one table two
time in this way we didn't run into
memory issues and at the same time the
the old the DBAs that were going to take
this and migrate the rest of the servers
they only had to run the one command
they didn't have to go manually tweak
the config file and change the table
name and manually execute it for all
1,500 tables in the database they just
had to run the one script it would loop
through the tables and migrate the data
one at a time and then likewise we would
execute the shell script that would
migrate the the remaining DDL ie the
indexes the alter taemin's alter table
statements and so forth and then as a
last step we came back and did the
function so we went into the config file
and created a separate config that had
the type set to function we set the file
for function to one so we got a separate
file for each function and it gave us an
opportunity to review those functions
before we executed them against the
Postgres database and we didn't dig too
much deeper than that however there are
variables that allow us to dump
procedures and triggers separately we
dumped the procedures and triggers as
part of dumping the the initial DDL ie
we dumped the triggers as part of
dumping the table statements or we
appended it to that file and then we
dumped the procedures as part of when we
were dumping the functions in order pg
of course
converts those two functions because
Postgres native generic Postgres of
course doesn't support procedures it
supports functions in our case it worked
out great
we there were no there are no particular
issues in their application that
required the or that caused the fact
that we no longer have a procedure or a
package we didn't have any issues with
those now being functions not really
working for the app anymore
I could see where there might be some
scenarios where code would have to
change if you are leveraging the true
Oracle package syntax in this shop in
particular impact both times we've used
this we didn't run into that so
apparently I don't know how widespread
the use of that syntax is and the it
apparently it doesn't seem to be all
that wide so if if the call of the
procedures and the packages and the
functions are basically what a typical
DBA would do sort of an C syntax style
where you call the individual functions
within that package in the application
then this should work out of the gate if
not there will be some application code
that needs to change and that's kind of
it for Oracle to order PG it is a big
tool I would recommend you take a good
look at the config file read through all
the comments in the config read through
the readme file there's a lot of
information this tool has more
flexibility than you could imagine and
you'll probably only use 10% of it like
we did but it does work</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>