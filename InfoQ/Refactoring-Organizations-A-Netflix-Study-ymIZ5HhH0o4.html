<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Refactoring Organizations - A Netflix Study | Coder Coacher - Coaching Coders</title><meta content="Refactoring Organizations - A Netflix Study - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Refactoring Organizations - A Netflix Study</b></h2><h5 class="post__date">2017-12-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ymIZ5HhH0o4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so late 2009 I joined a team and moved
over from another role at Netflix to
join a team called the electronic
delivery and I stepped into a little bit
of controversy when I did that decision
had just been recently made that was an
architectural and organizational
decision and some people had some strong
reactions to it and I heard about that
pretty quickly when I took on this role
but let me set the stage for you just a
little bit in 2007 Netflix had launched
on one device who is the most popular
and most ubiquitous device at the time
which was Windows and launched on
Windows Media Player and fortunately
within a short period of time we were
able to expand that to the Silverlight
platform which actually allowed us to
add Mac support tomatoe all of our
engineering friends very happy Netflix
had also launched on the Roku player
which was the first set-top box they did
Netflix streaming and scored a deal with
Microsoft we were able to get Netflix
onto the Xbox 360 which had a large
installed base had a great great sort of
network stack and capabilities and so we
were able to really leverage that
platform quite a bit Netflix also was
expanding doing sort of turnkey
implementations of an STK of an SDK in
various seee devices like Samsung and LG
blu-ray players D TVs and also a variety
of other CCE manufacturers the user
interface was primitive and that
primitive user interface involved going
to the website adding a movie to your
instant queue coming back and watching a
movie it was a very very bare-bones
implementation that was bolted on to the
Netflix DVD infrastructure and
experience you can see the foundation
here in the early versions of the
website where play buttons started
getting scattered throughout add to
instant queue buttons you see very much
how this was sort of an add-on and
actually from a business perspective it
was it well as well initially there was
no charge for streaming and then there
was a nominal charge by the time we
rolled around to 2009-2010 and the
infrastructure was also very simple and
very primitive
it was http-based there was an
application stack that had user
interface components security activation
playback capabilities and an underlying
platform called an RDP which was the
Netflix ready device platform and it
talked over HTTP using a protocol an XML
RPC style protocol and everything about
this implementation was very sort of
custom and crafted like a custom ticket
based authentication system custom
response codes and everything that was
designed within this took sort of a
lengthy process to design each of these
for every version of the protocol that
was going to be rolled out and that it
was plugged into a simple harbor based
load balancer in the Netflix data center
we were still in the data center at that
time and had one application the Netflix
content control protocol service or in
CCP which then had its own database and
plugged into the Netflix legacy
infrastructure what was also happening
was there was this massive amount of
work coming because streaming was the
future of Netflix as a business and
there was a growing demand for more and
more engineering to grow that part of
the business and so we very much had
this feeling like a steamroller was
coming to run us over at the same time
there was a project a service called the
Netflix API that had originally been
developed to drive our DVD business the
idea was to let a thousand flowers bloom
which meant we would have a lot of web
applications built around an API that
provided content metadata so that people
could build interesting apps which
hopefully would drive traffic back to
the Netflix service but it didn't really
turn out that way it didn't really turn
out to be very successful the API was
also built in a very similar way there
was a web app connecting over HTTP but
you'll see that the technical stack was
actually a little bit more modern it was
rest-based it use JSON for its schema it
used HTTP response codes which are a bit
more standard or becoming standard at
the time and because of the nature of
the app it was using OAuth as its
security mechanism and a proposal was
made by the leader of the Netflix API
team that we go ahead and
add API into the mix there was a team
that was ready to do some new work the
API itself and the thousand flowers
effort really wasn't going anywhere so
this team was sort of ready to dig in
and take on a new challenge and it
seemed like an interesting idea to
explore and so the idea here would be
that the UI experience the discovery
experience would be supported by the
Netflix API and it would plug in and
then be playback security and platform
functionality all the rest of the
functionality would continue to be
supported by NCCP and if you look at the
assessment from sort of a dispassionate
perspective there were some pros and
there were some cons to this approach
one was separation of concerns we can
have a whole separate team focused just
on the discovery experience more
bandwidth because it was a team that was
available to jump in and help out and
the thought was that this would drive
faster innovation which Netflix is
always hungry to do but on the con side
the reliability the API platform was not
incredibly reliable it hadn't been
designed for scale fetching metadata for
these apps was not sort of a massive hit
on that and so there were a lot of
concerns about the stability of the API
platform and whether or not it could
hold up and be more like the utility
application that NCCP had become the
other concern was that there was a
header heterogeneous architecture
differences in protocols and security
mechanisms and work styles and finally
concerns were expressed about the lack
of domain knowledge of the API team
taking on the streaming space which was
which is a very deep space the security
aspects alone were fairly deep and so
this turned into a little bit of a cage
match between the two teams neither one
really wanting to give their side the
electronic delivery team wanted to hold
on to the user interface experience and
thought that one service here was the
right path and the API team thought the
opposite that they could provide more
value to the business by partitioning
that off and at the end of the day it
took our chief product officer Neil hunt
to play referee and make sort of a
top-down decision to decide which path
are we going to go down and we
ultimately decided to go down the split
path
any thoughts on what the reaction was
after that all right how many people to
show hands that think that the
electronic delivery team was happy with
this decision anybody think they were
happy with it oh one or two people okay
you're an optimist this was the reaction
pretty much
there was concern a lot of concern about
the future of the product experience the
impact on our members the reliability
all of those things there was also some
anger about the way that this happened
it felt like a land grab to some of the
people involved I would say that really
what it comes down to when I think about
what's underneath all of this and I've
seen this happen even amongst the best
leaders and the best engineers there's a
form of tribalism that is at play the
idea that our team can do this really
well but we don't know about those other
teams we don't know if they're gonna do
as good a job as we are and very
commonly when you ask teams about each
other they always think that they do
stuff really well the other teams maybe
not so much and this is I think fairly
Universal and the problem with this
tribalism is that it creates walls and
this literally this experience and this
tribalism and the way that this all
unfolded created a hard wall between the
NCCP based team electronic delivery team
and the API team and that wall persisted
even for several years after I came in
and took on the electronic delivery team
despite my best efforts so this is an
example this gets to the heart of
Conway's law and with somebody mind
raising their hand and sharing with us
what Conway's law is it's fairly well
known anybody want to volunteer
I saw a part of a hand go up here we go
I'm sorry I couldn't hear you can you
shout out there you go it's that
relationship between organization and
architecture and typically your it
implies that your architecture is a
reflection of your organizational
structure and communication mechanisms
but this is my favorite description of
it and it really hits home here you have
four teams working on a compiler you
will end up with a four pass compiler
and this is this is really true this
that really does happen in the real
world and this is why there are walls
these this is an example where those
walls come up between each of those
teams where the architecture is driven
by the organizational structure rather
than a solution being determined first
and then allowing those organizations to
contribute and so my premise for today
is that Conway's law describes
dysfunction and I think we sometimes
just talk about it as a fact of life but
I think I'm gonna take a stronger stance
on this and say this it should not be
this way that you actually end up with
sub optimal solutions if you follow
Conway's law we have to embrace an
architecture first approach architecture
must come first before organization and
not only that technical analogues and
technical perspectives will ultimately
help you drive better organizational
solutions they can take some of the
emotion out of the situation some of
those fears and concerns and all of
those things we were just talking about
but if you take nothing else away from
this top today I want to encourage this
thought this thinking this philosophy of
selfless leadership I'm sure all of you
please all of you play a leadership role
in your organizations and selfish
leadership helps you get to the best
possible solutions and what that means
is that when you're making a decision
about architecture or about organization
you think about the company first then
you think about your team and then you
think about yourself you're still
important but if you're really trying to
do the best thing for your company then
you want to go and do these things in
that order so Forte's today
program I'm gonna do a little bit of
introduction I haven't yet introduced
myself and I'll do that in just a moment
I want to spend a few minutes just
talking about a framework around
refactoring that we'll use throughout
the presentation and then I'm gonna go
deep into my experience managing the
electronic delivery team and at the
scaling challenges that I that I had to
work through and some of the work the
outcomes that came from that and I want
to spend a few minutes acknowledging the
fact that if even if we do take these
technical perspectives and apply that to
organizational problems that people are
not computers and there are times when
you have to factor that in even if you
try to take a detached perspective most
of the time and then I'm gonna come back
full circle to this story that I just
started with and I'll get another bite
at the Apple to sort of take a look at
Conway's law and potentially have an
interesting reaction to that so by way
of introduction my name is Josh Evans I
was at Netflix for about 17 years
I just left recently in 99 I started
with the DVD business versus an engineer
then as a manager I was able to help
foster the DVD e-commerce experience
over to streaming and then in 2009 I
moved over to manage what was the
electronic delivery team but I really
didn't like the name and we ended up
renaming the team to streaming
infrastructure so I'll use that
throughout the rest of this talk and
this is essentially all of the
infrastructure that enables playback not
the actual streaming of bits but all the
surrounding functionality whether that's
digital rights management manifests
delivery for URLs so that you can go
fetch content from a CDN device
activation all of those things and then
in 2013 I moved on to run a team called
operations engineering and the
philosophy of this team is that all of
your operations wherever possible should
not only be automated but engineered and
infrastructure should be built to make
it work as well as possible and not only
for your customer facing member
experience and the production
environment but also the environment
where your engineers are working every
day so you can think of this as sort of
an analogue to the DevOps sre space and
now I'm taking some time
it's kind of thinking about next steps
for those of you who would like to tweet
as we go I've put my my Twitter handle
up there feel free
now the reason Netflix is relevant here
and I think all of you know this is
because Netflix is very successful it's
a very successful business and so there
might be patterns here that would be
useful Netflix is a global leader in
subscription internet TV with a growing
slate of original content a lot of it is
globally licensed there are a hundred
million members strong now have just
happened recently in a hundred and
ninety countries localized in tens of
languages on thousands of device types
and what all of you probably know like
Netflix was an early adopter of the
cloud and AWS and micro services and
they also have a very unique company
culture which is highly relevant to what
we're talking about here let's dig in
and talk about the framework a little
bit so who can tell me at a fairly high
level why do we refactor why do we
refactor code what are we trying to
accomplish any volunteers say it louder
more efficient code anything of anybody
else just yell it out make things better
okay
remove redundancy that's a good one too
elegance that's a that's a controversial
one there but I won't touch that one for
now
so make things better we refactor to
improve or sustain something typically
its functionality
sometimes it's engineering velocity you
want to make things better or I want
mine make my code more maintainable make
it easier to make changes without have
to worry about breaking things in ten
places and then also functional and
operational quality so that would mean
either quality from a traditional QA
perspective or performance reliability
those key metrics we care about for our
service architectures and these things
tend to happen when we scale as you add
more functionality as you get more track
you're gonna find breaking points that
encourage you to go refactor because now
things aren't working so well so you've
lost it now you're just trying to get
back to where you were before or you may
be trying to make an improvement and so
scale is critical here and there's two
dimensions of scalability that I think
are relevant for this conversation
one is functional scalability which is
essentially the ability to add features
easily with minimal effort the other is
what most people think about when you
talk about scalability which is load
scalability the ease with which you can
add resources or remove resources as
your load shifts around and I'm going to
propose a couple of quick definitions
for what I would call organizational
scalability this is the ability for an
organization to easily add people and
domain responsibilities in response to
increased work and complexity but also
the ease with which an organization or
team can adopt can adapt to shifts in
business strategy can you be nimble can
you quickly move over and do something
that you didn't anticipate and in terms
of when we refactor it usually feels
something like this when you hit this
tip this tipping point things just get
harder you feel like you're pushing a
rock uphill and more specifically common
tasks become difficult and strategic
efforts become impractical or impossible
in terms of how we refactor we typically
apply patterns whether they come from
object-oriented design talking about
inheritance or polymorphism or
composition in microservice architecture
patterns like auto scaling or horizontal
scaling or systems engineering concepts
like I don't know a parallel processing
might come to mind and let me give a
quick example of how natural it is to
think about a technical perspective and
apply it back to an organization I think
we do this in the engineering world
fairly fluidly and here's a great
example the Netflix culture deck has one
slide that sort of captures the essence
of what Netflix culture culture is about
with the right people and
of a culture of process adherence we
have a culture of creativity and
self-discipline freedom and
responsibility and freedom and
responsibility is so ubiquitous in the
vocabulary of Netflix that it gets
contracted down to F&amp;amp;R man that's how
they refer to it we want to do that when
I go do this thing sound a little crazy
but F&amp;amp;R and as Netflix moved from a
monolithic data center model to micro
services in AWS they also adopted the
philosophy espoused by Werner Vogel's
you build it you run it and you build it
you run it you could think of as a
concrete implementation of the
abstraction of freedom and
responsibility
that's how natural this is because in
the engineering world that's exactly
what it is you are go for free to go
build a great service tier you're free
to make code changes at 12 o'clock at
night on a Friday but you should be on
call and have to deal with the
consequences of your actions that's the
freedom part and that's the
responsibility part so you can see this
is really very natural you can think of
inheritance in all kinds of ways and how
that might apply from a culture
perspective so now I want to dig in and
talk about scaling teams and I'm going
to take you back to 2009 when I did take
on this team called electronic delivery
and we were still in the early days we
had these devices in production plus
many more smaller ones and we had to
support all of those so obviously once
something goes into production it
becomes that one more thing you have to
keep up and running and we had some key
platform development improv and progress
and keep platforms or these custom
bespoke
applications that Netflix builds
in-house as opposed to providing an SDK
to a seee manufacturers knew the
integration themselves and when I first
started there were two projects in the
works that were still being developed
and getting prepared for launch one was
called Vega
it was the disk based piece ps3
implementation the very first
implementation for that device and the
other was kind of a strange project this
was called this this platform here is
for the Sony
bravia systems it was called Bibble and
it was their own sort of proprietary
framework and we had decided that the
strategic relationship with Sony was so
important that we ended up doing this
very custom implementation to make that
happen and it was a lot of work
couple of engineers were working on each
of these in parallel and then no more
than about a week or two into my tenure
running this team a gentleman named
Anthony Park who is still at Netflix he
was at the time director of key
platforms he came to me and said hey
we've got this project we want to start
enabling subtitles and alternate audio
as a feature for streaming and I thought
it'd sound like a really cool feature
and I said yeah sure no problem let me
go find an engineer I found one I said
okay you can have this guy Bend and
he'll come work with you well no more
than two days later a gentleman named
John fund came to me he was running the
link project that was the code name for
the disk-based we implementation and he
said hey by the way I don't if you know
this but the second you're done with
these other launches we're in line and
we expect to get resources so we can
launch this within a few months so it
was a very tight schedule and an
immediate turnaround on that oh and by
the way I think there's a lot more stuff
on your plate than you're realizing so
big picture when I came to realize and I
realized what I had taken on was that
there were many parallel tracks of work
that I needed to be able to accommodate
Netflix wanted to be on every possible
device that they could get their hands
on that made sense product innovation
was going to continue as normal Netflix
does that in a relentless pace by the
way we were gonna roll out in Canada in
2010 so get ready for
internationalization because that's
gonna take some effort oh and cloud
migration at the same time you know we
already decided in 2009 that we were
gonna move to the clouds so we also had
to do that and as I mentioned earlier
service reliability we've got a service
running people expect it to work and
reliability was a bit of a challenge for
us in the early days so we had all of
these things that we needed to be able
to do so I suddenly had a moment of
panic how many engineers do you think I
might have had just guess how many
engineers were supposed to
support all of this just throw out a
number Brenda we're in the order of
magnitude here that's good six I
inherited six really good engineers but
there were only six of them so this was
me that day I went home I with ya
obviously not me this person has more
hair than I do but that's the way I felt
I went home and had a bit of a meltdown
I was like I got this new job I got
these new people I'm working with I'm
lying on the floor my wife is talking me
down like okay I think I can deal with
this but I indulged for you know about
an hour I was like oh what am I
gonna do what would you do in this
situation what's the first thing that
you would do rather than raising hands
just shout it out okay maybe one at a
time say it again
anybody else prioritize good anything
else delegate to other people it's kind
of hard we got six people they're all
totally loaded interesting higher more
good good good you got the gist of it
let me tell you what I actually did so
going back to our sort of computer
science perspective I'm going to start
with the analogy of a task queue so we
had all this work we had to prioritize
it so let's get it in the right order
let's dump it into a thread pool which
happened to be the six engineers that I
had and then we get our completed tasks
and so we did this prioritization
exercise service availability always at
the top of the list you got to keep the
things running you're already
responsible for and then really all we
could take on and I wasn't even sure
that we could do all of this was to
continue to focus on these device
launches new game consoles new versions
of the apps for those game consoles we
were going to go down loadable see
expansion so getting onto more and more
of those LG and Samsung devices oh and
mobile hadn't even really kicked in yet
but we knew something was coming you
know the I the iPhone had been out for a
little bit we kind of knew mobile was
gonna be a growing space and what we
couldn't get
to was pretty much everything else audio
and subtitles international support new
codecs that we want to do to make things
more efficient and the list would go on
and on
so clearly the next thing I needed to do
which one of you already shouted out was
I needed to scale up my team I needed
more people I had great engineers but
there's only so much you can do with six
engineers and so the first thing we need
to do is figure out well what's the
profile of the work that we're doing
what are the roles that we need and how
much do we need of those kinds of roles
how much throughput do we need it to be
able to provide what would what team
structure would support that and I had
what I would call a monolithic team and
I think for any of you that have been at
a start-up this is essentially the way
teams start they start monolithic there
was easily a manager
maybe there's no manager in a start-up
scenario and a relatively flat
organization where work sort of happens
ad hoc and the next project comes along
and whoever's available those and jumps
in and starts working on it which is
great when you're small but not when you
need to scale and so just as in the
monolithic world and from an
architecture perspective you have to
decompose your organization just like
you would decompose a model with
monolithic architecture and typically
what you're doing is figure out what are
the distinct modules and services you
might break your monolith into how do
you partition the work because typically
you're challenged with a model with is
it isn't scaling you have to figure out
how to split it out can you do it's
horizontally can you break out different
kinds of workloads into different apps
you have to be aware of your
dependencies and what are you going to
break what do you want to continue to
support and you ideally want everything
to be sort of loosely coupled so your
your services can function independently
and be maintained independently so you
can be more agile and so we did that we
took a first stab and broke the when I
now call streaming infrastructure
didn't really like the electronic
delivery name and so we ended up
renaming the team to streaming
infrastructure which made sense at the
time and we broke it into two worlds one
focused on the seee manufacturers the
core sdk which was called an RDP the
Netflix ready device platform that's
general protocol design all of these
sort of basic platform functionality
that you
one for streaming and then those
features would get ported into what we
called our key platforms those are all
those de spoke custom applications that
Netflix was building in-house and it was
generally device and partner oriented so
we're able to think about our
dependencies the key platform
engineering leads were different from
the people we worked with for the more
generalized features so that worked out
you know reasonably well but this is
really just the beginning we were still
missing some pieces to the puzzle and
the next piece came in as I was starting
to understand what the operational load
looked like and we had some problems
with overload there was a vicious cycle
that was happening where we had one
engineer who is really good at doing a
lot of different things
this gentleman's filler Philip Fisher
Ogden some of you might know him he
actually has my my old job right now and
Philip was great at a lot of things he
was great at development he understood
the features and the infrastructure
involved in supporting the streaming he
had built out this great thing called
server in a box that allowed us to
easily test the streaming functionality
he was a good project manager and I was
sort of thinking like hey if I need a
manager this guy might be really good at
it but the fly in the ointment was he
was really good at troubleshooting
because he knew how everything worked
and he had a natural talent for
discovering what was broken and we
wanted Phillip to do this work right
because you build it you run it right so
he built NCCP he was one of the main
engineers on there so we did want him to
be on call it made sense for him to do
that but the problem was is that he was
overloaded there was a high risk of
burnout
he was already trying to find strategies
to try and offload some of his work or
make sure at least didn't get called on
the weekend by putting some dashboards
together and checking things on Friday
and he was making very slow progress on
his other things because anytime
somebody couldn't figure out what was
going on in production he was the guy
that got called and we had some
stability issues in the early days in
the data centers so that happened often
enough that it was pretty distracting
from a systems perspective this is a
classic scenario of what I love thread
starvation if we want to use that as an
analogy where you've got a queue of
tasks but some of
tasks are super expensive or very very
frequent and they happen so often and
they consume so much of a shared
resource imagine a lock of global walk
of some kind that the other work simply
wasn't getting time and wasn't moving
forward and Phillip was our shared
exclusive resource in this case now this
is only part of the problem because the
other part of the problem was context
switching and in context switching from
a systems perspective you typically have
one thread executing you'll get
interrupted for whatever reason and then
you have to go save out your state you
go idle and then the other process has
to go fetch state to go bootstrap itself
and then it can start executing and then
the whole process reverses back and
forth each time you switch context now
we know what the system's level this can
create gross inefficiency but for humans
it's even worse because if you've ever
worked on a project where it's very very
complex it can sometimes take you 20
minutes to half an hour just to get your
mind into the right state of mind to
start writing code and if just as you're
starting to get into that you get
interrupted you're not really going to
be able to make very much progress and
so both of these challenges were at play
here and the solution here is thread
pool isolation and distributing your
locks and distributing that high
interrupts expensive work and spreading
it across your thread pools so that it
is not as much of an overall percentage
for each one of those pools and doesn't
dominate the workloads as much and so we
essentially did the same thing at the
organizational level we deepened the
troubleshooting skills across the team
and build some dashboards and tooling to
make it possible for people to be more
self-sufficient when they're on call we
distributed the escalations so that we
didn't just have Phillip responding
whenever notes people couldn't figure
out what was going on and we also came
to realize that we needed to engineer
our operations because the burden on
everybody doing on call was just too
high we needed to make it easier to do
those kinds of things and so we added
one more component to the organizational
structure
which is a team at the time called
insight and tools but you can think of
it as an early precursor to the sort of
DevOps function as we think about it or
what I like to call operation
engineering because I think it's a more
descriptive term and so we had that
component now added to the set of things
this was a huge amount of work we
realized we really needed to invest now
then there's that pesky thing of cloud
migration cloud migration is really
distracting and it really it changes a
lot of things that make it difficult to
do both cloud migration and product work
at the same time especially if you have
the same people working on both of those
things or the same organization and the
difference is that the workloads
actually are very different so when
you're doing cloud migration it's this
sort of systemic end point by end point
service by service migration from one
place to another requires a lot of sort
of methodical work it's very
infrastructure intensive and a lot of
that work happens over a period of time
and yet product work is really about
rapid iteration it's about a/b testing
it's about trying out new things it's
about adding incremental functionality
all the time and these two workloads are
so radically different that it was
essentially impossible for the
electronic delivery team at the time
this was just before I joined it was
impossible for them to do both plus the
throughput issues which we've already
discussed so another analogy and this
one is from micro service architecture
is a pattern around header how to handle
heterogeneous workloads imagine you've
got member traffic coming in and it
needs to hit some service within your
middle tier infrastructure first you're
going to hit a cache if you get a cache
miss you'll go hit the the service that
supports that cache which might fetch
data from a database and then backfill
the cache so that you can go and have
that available so you get a nice fast
read from the cache on the next hit
imagine you also have batch processes
that need to access that very same data
and it's hitting the cache it's falling
back to the same service but then it can
take it down because batch services are
very different workloads they're very
spikey they can come in and just hammer
the crap out of your service if you're
not careful and God forbid that happens
at the same time that you're taking peak
traffic from your members and so once
batch can batch comes in and takes out
your cashing tier it'll take everything
else out because falling back to your
service in your database those aren't
going to perform nearly as well as your
cash so it'll overwhelm all of them and
before you know it everything's down
batch isn't working member of traffic
isn't working you're dead in the water
at that point and so the classic
approach to handling this is to
partition your workloads so in this case
for this service in service in question
the cash layer was replicated and there
was a dedicated cash just for batch now
you could go and say that if that fails
and go ahead and stop calling or you
could decide to I like to think of it as
a zipper you can go all the way down the
stack and really create totally separate
copies if you really need to and so
that's what we did to get cloud
migrations started we partitioned and we
chose Silverlight as the first platform
to be migrated to the cloud to do a
prototype on that essentially figure out
okay let's just get one thing done and
the partitioning was easy we did have
another team somebody called that out
before that could take on some of the
work that was our platform engineering
team the problem was they didn't know
anything about streaming they were
building generalized infrastructure and
so part of the solution also involved
domain portability how do you get the
intelligence the knowledge the
algorithms that are known in one space
and move it over and we essentially took
one of the leads the what presumably
been running the team prior to me and a
gentleman named ramjet maven curve and
he moved over right into the platform
team and embedded in that team and he
took all the knowledge and expertise
that he had about streaming and brought
it into that team so this is a team that
was great at building infrastructure
that was ready to build generalized
infrastructure for cloud migration for
all of Netflix services and ran Jewett
was there to help them figure out how to
make that first step now the other thing
that was really cool about this was that
Rand jet came back to streaming
infrastructure after he was done and he
took
all that knowledge about the
infrastructure and the platform work
that generalized work about how to get
something stood up in the cloud how to
build a service in the cloud and you
brought that back to the team so I like
to think of this pattern as engineer as
a library sort of following that
metaphor of using a pure science
metaphor here and so the last piece here
was to add a systems function into the
team and that right initially was cloud
migration other things sort of rolled in
afterwards the next problem was staffing
we needed to get from about six people
to somewhere around twenty four twenty
five and that was my first guess
ultimately it ended up being more than
that and I became the bottleneck it was
only one of me and I'm pretty good at
recruiting but you can only one person
can only recruit so many people so
quickly and so I chose to do something
equivalent to cloning and parallel
processing I didn't have time to hire
four managers and then have those four
managers go out and find people so I
just did the most expedient thing I
found for people for victims who wanted
to try management and I asked them to
step in and take these things on now
some of them actually they really wanted
to do it but but they needed a lot of
ramp up they needed to learn how to
recruit they knew that the technical
expertise to run the run these domains
but they'd never recruited before they
needed to be ramped up we worked very
closely as a team to try to figure how
to accelerate the growth of the team and
by 2012 we actually were pretty
successful we had staffed just ahead of
the curve to be able to keep up with the
demand on all of those fronts within two
years we had completed our cloud
migration for all the services owned all
those endpoints for doing streaming
using all of the the different legacy
devices we launched in three different
regions Canada Latin America and the UK
there was a massive explosion in devices
including one year in 2010 where we did
a whole bunch of different gaming
platforms than downloadable versions of
it and
by the way iPad iPhone Apple TV all at
the same time pretty much or within all
sequential that we did major product
improvements we did finally build that
alternate audience subtitles feature set
and they kept building on that and
here's the agility piece we had staffed
up enough that when Netflix decided to
take the CDN in-house
initially the CD ends were external they
were third-party CD ends there were
multiples of them that were actually
performing the streaming for us we
decided to bring it in-house and we had
enough resources and leadership in place
that we could quickly pivot and provide
a cloud response service resource to the
CDN team so that they could integrate
with the Netflix infrastructure so I
think that was a fairly successful
effort and I want to just take a minute
to talk about this what I call IQ versus
EQ and this is the acknowledgment that
people are not computers that these
analogies only go so far when you're
thinking about applying computer science
to organizational challenges and in
order to be successful doing this you
kind of need to be able to pivot between
two states of mind one I would call IQ
which is task oriented logical literal
detached impartial and somewhat
autocratic if you can logically just
think your way into the right solution
and you can just go boom I know what the
right answer is let's just do that
now on the EQ side things get a little
more squishy it's feeling oriented it's
emotional its social and political
sometimes it's empathetic you think
about how things affect people and it's
a bit more democratic or you can even go
as far as consensus oriented now you
need that one of the interesting things
to think about is when you're think
about organizations depending on where
you are in the process of figuring
things out you can really go wrong by
skewing too hard one way or the other if
you're too autocratic and you simply
make a decision and you leave people out
of a decision-making process you can
alienate them you can lose staff you can
make bad decisions about how people you
know will feel about certain changes but
if you're too EQ oriented
you can get paralyzed by worrying about
what everybody thinks about the decision
you're gonna make
and so you have to really figure out
where you're gonna make first strike the
right balance so if you want to overcome
tribalism if you want to get beyond us
and them you need to be able to move
back and forth between these two modes
for IQ you wanna when you're designing
something you really want to just purely
think about it from an IQ perspective
what's the right solution what are the
options that we have it's a very
rational illogical exercise and so is
evaluation if you're gonna go and assess
is this the right design should we
consider other once an implementation is
also a rather dispassionate thing at
least it should be and sometimes people
get very passionate about every aspect
of engineering but on the EQ side
there's two things that you really want
to emphasize and think about whether
you're gonna do a Ryoka tech chure or we
architected
is that the inception has to be planned
and there needs to be trust you need to
plan you need people need to be in the
loop they need to be signed up for doing
the work that you want them to do if
there's gonna be an implied
architectural change that spans teams
all of those teams need to be on board
for the most part and then when you
socialize things you also want to make
sure you're being careful so once you've
made a decision and you decide you're
gonna go ahead and roll that out you
want to do things like well I'm doing
this organizational change we should
probably tell the people who are most
affected by it firstly they don't find
out from somebody else and then of
course you want to go tell other folks
what's going on so that they know who to
go to now that your organizational
structure might have changed and I would
argue that what happened in the earliest
story when I first kicked this off was a
flawed inception the decision was
top-down there was no partnership
between the parties there was not a lot
of trust between those parties that had
been built up at the beginning and I
believe that that's what foster a lot of
that resentment and anger that came
about that lack of understanding of like
well I don't think this is the right
decision and so now I'm going to take us
full circle and then forward fast
forward to 2012 and this architecture
that we had now lived with for several
years this split between
content discovery and then playback
functionality had become something more
like this we'd moved to the cloud we had
this soup of micro services to call in
to we had a new proxy service called Zul
that did dynamic routing but we still
had NCCP and we still had api we had
some weird things going on now we're api
was calling NCCP and other the other way
around i mean i these two edge services
now tangled up in almost these sort of
circular dependencies and the wall was
still there there was growing complexity
as a result a duplication of effort
which we noticed was kind of really
inefficient and it was a tax on those
engineers who had to work with both
teams imagine you're the device person
trying to write code and you've got
different protocols and different team
styles and security mechanisms that
you're wrestling with the work is almost
double for the device engineers and then
Anthony Park raised the stakes same
gentleman from the first story and he's
he had this obsession with playback in
half a second he was talking about it
all the time he said we're gonna do this
we're gonna go figure this out but the
problem was in order to implement that
you needed to be able to integrate with
both of the edge services in a really
clean way you needed to fetch lists of
movies and you needed to be also you
needed to also get limited duration
licenses so that as soon as somebody
clicked on play you already had a DRM
license you were all ready to just start
streaming right off the bat and by the
way I checked with my boss at the time
he's like yep we're gonna do a lot more
of that so you should be ready for that
and in addition as always Netflix wanted
to move faster and they were noticing
some friction because of these two
services and our reliability could have
been better we had two edge services and
either one of them was down everything
was down or either couldn't find the
content to play or you could find the
content you hit play and then it didn't
work so this is kind of an unacceptable
situation so as a reminder when do we
refactor common tasks are difficult
while we were there and now we wanted to
do this really hard stuff these
strategic things that were gonna make
the customer experience so much better
and they were almost there were almost
impossible this was Conway's revenge for
me you have four teams working on a
compiler you'll end up with a four pass
compiler well we had two teams and a two
edge service architecture the exact same
thing this is Conway's law to a tee but
fortunately we had a better foundation
this time gentlemen named daniel
jacobson came on board in 2010 and
started working on really fixing a lot
of the challenges with the Netflix API
team the team became more mature and had
some rockstar engineers on it now the
platform had gotten a lot better it was
much more robust in fact some folks on
my team were going and kind of wants
some of that they've got some great
technology over there they were using
hysterics for example some of you may be
familiar with that that's the team that
created histories they had a strong
operational focus and so they were
keeping their service up more and more
and the most important thing Daniel and
I had become friends and we had spent a
lot of time talking to each other about
gee we've got these two edge service
teams and shouldn't we be working more
closely together let's try and figure
out how we can make that happen because
it seemed like something just wasn't
right and so there was a moment of truth
I was sitting down having a one-on-one
with our senior architect on my team a
gentleman named Peter stout and I just
asked them this question I said Peter oK
we've got these challenges this is
what's going on we know there's pain we
know we need to fix this what's the
right solution and Peter being the very
astute engineer and general smart guy
that he is do you care about the
organizational implications that was his
very first question it didn't he didn't
even hesitate he didn't miss a beat he
knew about Conway's law he knew I might
have some feelings about that and I did
I was like who let me think about that
for a minute I had to pause what's the
implication for me my team might get
restructured I hadn't I might lose
control of the situation because I might
open up this can of worms and I don't
know where it's gonna go and I'm proud
to say I had a moment of sanity and I
said no we'll figure that out later
there was enough trust
the people involved that I knew that the
right thing would happen and so what
came out of that was a project called
Blade Runner because it was the emerging
of the edge services and this
architecture became this NCCP as
playback became a set of middle tier
services fronted by the Netflix API and
the necessary functionality to do things
like protocol security dynamic routing
the ability to decrypt tokens so that we
could do dynamic routing was integrated
into the appropriate services so this
was a unified architecture that came
from common thinking across both teams
and the organizational structure
reflected it I trusted Daniel so much
that I gave him my team I said you know
what this needs one leader not two we
need to get this under unified
leadership and the playback services
team which is the renamed version of
streaming infrastructure was directly
integrated into this new organization
called edge services we gained some
efficiency right away because some of
those overlapping duplication of effort
kinds of things could get consolidated
there could be one team focusing on
insight and tools not two and that
platform that the API had created and
some of the good stuff that came out of
the issue in the infrastructure team
could be shared across the two and now
that they're in a single organization
there were less excuses not to do that
and we roughly organized around the
technology not the other way around
we've organized around microservices you
see teams grouped around things like Zul
and n ccp around functionality and those
share services so takeaways from all of
this put your architecture first
leverage those technical analogues so
that you can take a step back and be
dispassionate gained the ability to
think like a computer when appropriate
but obviously you also need to be able
to switch back and forth and know when
to be sensitive to the social aspects of
thing
and if nothing else be selfless because
think about what's best for your company
and try to break through those
inter-team barriers that always happen
between organizations if you would like
to reach out to me or follow me here is
my handle on twitter at ops engineering
and also my information on LinkedIn and
not sure about time are we doing on time
oh thank you
absolutely it becomes difficult that's
why I spent time talking about culture
because my ability to do those things in
the ability for every other manager and
efflux to do those things is because
they have so much freedom to act based
on you know their best intentions for
the business in a more top-down and
structured organization you're gonna
have to adapt in some way and if you
find that you want to create sort of a
free and responsible engineering culture
but your company doesn't support that at
sort of the company culture level you're
probably gonna clash and so one you'd
either need to try to isolate yourself
from the rest of that or try to
influence the culture of your company
which can be pretty challenging but it's
at least worth note being aware of the
impact of the culture on your ability to
do that so you don't just beat your head
against the wall and say well I keep
wanting to do this and it's not working
why isn't it working well frequently it
could be because that the wrong people
you know the incentives are wrong the
culture is wrong the people who get
hired and fired and promoted which
ultimately defines your culture doesn't
support what you're trying to
accomplished but I still think there's
probably pieces of this cat that can be
applied you can still think about the
solutions first so you can still
mentally break through those barriers
you can create partnerships with the
other engineering leads in your team and
hopefully or across your organization
and hopefully you can break through if
you can explain these constructs of them
and say look you know we're building the
wrong thing because the organization is
driving the wrong decisions can we do an
exercise where we just do it let's just
do a thought exercise about what's the
right architecture if we didn't weren't
worried about organization what would we
do and then you can worry about
compromising from there then you can go
and say well this team really still
wants to own this service so you
compromise so those are the kinds of
things I think you could try all right
let's take just one more question and if
you want to ask Josh other questions buy
him a beer
not see anyone you're all very thirsty
okay I go with it I get it let's do it
Thank You Josh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>