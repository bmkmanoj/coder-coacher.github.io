<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bias in BigData/AI and ML | Coder Coacher - Coaching Coders</title><meta content="Bias in BigData/AI and ML - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bias in BigData/AI and ML</b></h2><h5 class="post__date">2018-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yM9jUqPDqL4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this morning's keynote is someone who
has worked at Apple at Google he's
worked at Twitter and most recently he
just came from slack the person I'm
talking about is Leslie Miley Leslie's
gonna do a talk today about AI and ml
and the inherent bias that's within our
datasets he's gonna touch on some of the
things that have been in the news with
social networks zero-day accounts and
they're the bots that are being out
there he's going to talk about how
underrepresented groups can be hurt by
AI and ml and then he's gonna talk about
specific things that we can do all about
it so please join me and welcome me
Leslie the stage good morning everyone
it's a big crowd I'm really surprised we
got this many engineers up this early
including myself so before I get into
this a couple of things I have to do I
have to do a selfie this is the age of
social social media and so I'm going to
just do a selfie with all of you if you
don't want to be in this picture you can
leave now anybody leaving no seriously
I'm doing a selfie this is a great don't
worry I only get the top of my head so
nobody knows oh look I got photo bomb
this has been an unprecedented year for
a lot of reasons and and it's really
been unprecedented because we've seen so
much of social media being powered by
bias powered by ml powered by angst
powered by fear it has been one of the
most tumultuous years that I can
remember politically and socially and
and and I started to think about and
think about the the part that we've all
kind of played in that and and so I'm
gonna go through a little bit of it now
so Facebook in 2016
said fake news wasn't a problem right
after the less you know like the fictive
is not a problem on our platform by
October 2017
like ten million people saw ads that
were fake or you know from that were
Russian linked by November that number I
jumped a hundred and twenty six million
I suspect by the end of the year they'll
just say everybody saw them and and I
actually think that would probably be
accurate we've all seen information
that's been fake or false or propaganda
and and and you know we've gotten so
used to looking at ads most of us in
this room that we just like we really
don't even notice them but your friends
your parents people who are outside of
tech have probably sent them to you and
ask them or shared them and you're like
what's going on but you really don't
think anything of it you're just like
yeah people see this information they
send it to me all the time so when
Twitter went on Capitol Hill they're
like yeah we have 6,000 Russian link
BOTS we found on our on Twitter that
generated a hundred and thirty one
thousand tweets between September and
November of 2016 it's a lot of tweets
with 288 million views Twitter has six
to eight million users in the United
States
that's almost assuring that every active
user in the United States saw some of
that and you have to start asking
yourself how does that happen
how does something that you and I can
see that we know is propaganda that we
know is fate that we know is false how
did that get by how did that get pushed
out to millions tens of millions
hundreds of millions of people I'm gonna
like give you a little bit of my history
in this when I was at Twitter I ran
abuse safety security and the accounts
team during an investigative session we
discovered hundreds of millions of
accounts that had been created in
Ukraine and Russia hundreds of millions
and this is 2015 right so I was like I
don't know what these are for but
they're probably not any good I don't
know why they're here we should probably
get rid of them but I left Twitter and I
don't know what happened I would like to
think that they got rid of them
I suspect they didn't get rid of them
all because we see what what has
happened and and once again I'm like
what happens if you have hundreds of
thousands millions tens of millions
hundreds of millions of these accounts
on Facebook on Twitter Twitter excuse me
Facebook just came out and said 200
million of their accounts may be false
or fake or compromised
yes that's only 10 percent of their
active users I know that doesn't sound
like a lot then it's only 10 percent but
that's 200 million there there there's
there's a problem that we're just like
not addressing and we're gonna dig a
little bit deeper because I think that's
the tip of the iceberg in 2016
Twitter came out with their algorithmic
timeline Facebook had been doing this
Instagram is now doing it but Twitter
said they wanted it to ensure that you
saw more tweets from the people you
interact with and they also said this is
a quote from Twitter ensured the most
popular tweets are far more widely seen
than they used to be enabling them to go
viral on an unprecedented scale I'd say
mission accomplished they did a
magnificent job of creating a timeline
Facebook has done a magnificent job of
creating a timeline showing you and
showing your friends ensuring your
family the most popular tweets but
there's a problem with that their media
publishers whether they want to believe
it or not they're more people see
information on Facebook than they see on
the New York Times CNN MSNBC and Fox
combined they're a publisher and their
publisher with no accountability none
they publish it and then they say we're
the platform the system didn't deliver
news the system deliver propaganda the
system didn't deliver your cat videos
they delivered biased information they
told people to go out and protest
against black lives matter they told
black
matter people to go out and protest
against something they told someone to
go and shoot up a pizza parlor in the
middle of the country because the DNC
was running a pedophile ring someone did
that because of fake information that
they got from social media these are the
things that just concerned me and I
asked myself this question what if there
were hundreds of millions of fake
compromised accounts sharing information
what if they were tweeting what if they
were sharing it on Facebook what if it
was on Instagram what do you think these
systems do you think like Facebook or
Twitter x' algorithmic timeline would do
with all of this they would take it in
they would say hey this is being shared
a lot so I'm going to show it to more
people who like this type of stuff would
like this content and yes it's not the
people in this room mostly it's probably
people just your friends your family who
send you these things and I saw this is
it true I get this all the time for my
family it is this true and I'm looking
at some like I don't even know why you
thought that headline was even remotely
true but it looked true to them and
between Facebook's two hundred and
thirtysomething million and between
Twitter's and this number is up for just
dispute they would never admit it
potentially over 700 million accounts on
their platform you have a billion
account they could be sending false
signals into these systems signals that
take advantage of their algorithms take
advantage of our bias and get us to
think different things to vote different
ways to talk to people different ways
Facebook did a great study if you want
to call it great in 2014 where they
started introducing different types of
information into people's timeline to
see if it affected their moods and it
did people would post different things
people would read different things that
it would actually change what they were
doing and my thesis is that once they
found this out they published it it went
out there my question is did they do
anything to stop anyone else from doing
that I think we know that answer today
it's a frightening world when you can
reach hundreds of millions of people
with data that is an information that is
wrong information that is is propaganda
and influence their moods and the funny
thing is they didn't see it coming
Twitter didn't see it coming Facebook
didn't see it coming and they actually
stood up and said it wasn't a problem
until they started looking into this
does this start to sound vaguely
familiar to anyone I mean would all this
information be part of the training data
that determines what you see in your
timeline it is everyday it's part of the
training data and it concerns me because
it was hailed as the next big thing
bringing relevant relevant content and
targeted relevant ad serving these
systems were deployed at mass and at
scale and they worked with little human
input or insight showing people what
they wanted to see whether it was true
or not that's not a world I really like
living and personally it's a very scary
world and so Facebook had to give credit
Facebook said hey we're and hire twenty
thousand people to tackle fake news
twenty thousand people to tackle fake
news one is there that much fake news to
do you need 20,000 people Twitter is
letting - now you see how and which ads
are targeted to you but no one said
they're going to change their algorithm
no one said they're going to change what
they're doing they're going to throw
people at this problem and as I was
preparing for this I wondered like why
is this why is this resonating with me
in in a way that I just couldn't figure
it out and and I had to like do a lot of
reading had to do a lot of thinking said
this is shades of the mortgage crisis
this is shades of people taking a bunch
of information in chopping it up into
little pieces feeding it back out to a
hungry public and not really
understanding after a long enough time
how the system even works anymore why it
works that way
what's even in the system how its
generating its outputs banks we're
trying to understand the risk they had
after the 2008 crisis they had to hire
people to actually look at every
mortgage to understand their exposure
they had to look at every piece of
mortgage that they had chopped up and
thrown into a CDD thrown into a CDO and
a lot of the bank's just threw their
hands up and said we're just going to
write off some number and let the market
come back and then not worry about it
which lucky for them it happened it was
interesting because Warren Buffett
called CD DS and I'm sure everyone in
here knows what it is if you don't it's
a collateralized debt something or
another yeah I don't even know what it
means and essentially it's a chopped-up
a mortgage that gets bundled up and they
get sold as security that we're all
rated as triple-a when really they
weren't because no one knew what was in
them no one knew how they worked anymore
no one knew how they operated and when
it all came crashing down everyone who's
left holding the bag but no one even
knew what was in the bag so why concerns
me is that the next big thing will be
nai ml company it may be Google getting
bigger Facebook getting bigger it may be
something we don't know it may be
something one of you in here are going
to end up creating and I wonder if we're
just going to repeat the mistakes of the
past I don't know I hope not
so I've explained some of my concern
I've explained why this is a problem
anybody wants to talk to me afterwards
catch up with me I'm gonna run out the
door so I don't have to defend any of
this there he said the reason we have to
look at this now more than ever is that
there's a growing and thriving industry
growing up around this these models are
being applied pretty much everywhere
everywhere they're being applied to
self-driving cars they're being applied
to ride-sharing imagine that you know
uber or lyft or some other ride-sharing
company determines that a certain
neighborhood their rides are always
under five dollars are they going to
send people to go pick up they're
probably not or they're going to send
people who are lower rated this is
happening now what does that do for
impoverished people what does it do for
people who aren't advantaged it's just
like redlining in the 40s and 50s this
is happening because no one is looking
at the data where the data comes from
there's no transparency and how the
algorithms are put together I'll get a
little more real this is happening in
sentencing guidelines
ProPublica did a great article on this
and they decomposed someone's algorithm
and the the sentencing guidelines that
software they came up with it was like
this is gonna remove the bias and people
gonna be treated fairly
guess what african-americans were 20%
more likely to get a harsher sentence in
some cases they were 45% more likely to
get a harsher sentence with the exact
same parameters because the data set
that they used to train this model was
inherently biased they didn't recognize
it and they didn't remove it and now
they've built it and it's in 25 states
and it's sending people I know sending
my family members to jail longer giving
them harsher sentences this is real and
this is happening and this scares me
because at some point it starts
impacting us more
than a self-driving car or more than an
election we may not agree with its going
to start making life-and-death decisions
for you it's going to start making
decisions about your health care it's
going to start making decisions about
your mortgage rates it's going to start
making decisions that you don't
understand and the people who are
deploying them do not understand and as
usual we the public will be left holding
the bag because after the mortgage
crisis no one went to jail no one was
held accountable and we got the tax bill
that we will continue to pay and your
children will continue to pay really
uplifting isn't it so what can we do now
we cannot talk about it we can we can
put our heads in the sand
or we can start to have a discussion
around where the data comes from you can
start having a discussion of is the Dave
data over sampled or under sample we can
start bringing in other people to look
at the data one of my favorites is it we
can just be transparent about what it is
we're collecting what it is we're using
how we're using it this is not a trade
secret it's data
anybody can get it how you use it should
not be a trade secret particularly
because it involves people because it
involves places it involves things in
the public domain already and that is
kind of where I want to go next
actionable steps
seek people outside of your circle you
have to talk to other people I mean III
want to I want to like call out Q Khan
for really taking a large step towards
making this a diverse and inclusive
conference it is amazing I'm seeing
people in here so yeah definitely give
them a hand they've done a great job
then I'm up on this stage means they did
a great job thank you
but when you're creating these systems
when you're deploying them find people
outside your circle I know I know a few
people who are doing a people detection
they're making sure they can identify
people identify the right people the
wrong people and I asked who's your data
going out to and they they showed me I'm
like these are all very wealthy tech
people these aren't people of color
these aren't people from different
backgrounds these aren't people in
different body and particularly
California different body shapes body
sizes it's a bunch of healthy California
Tech people that they're doing people
detection on like you have to widen your
data set you cannot just rely upon that
and then roll it out because it's
inherently going to have problems I
talked about radical transparency it has
to be radical you have to put it out
there for people to see it has to be
peer-reviewed if it's not you're just
going to continue to to build into your
data sets your bias my bigger than hire
more women in engineering do it
every engineering team I've worked with
that have had more women in it have been
better engineering teams I just get a
better output it's it's a fact
get over it do it if you still think
it's an issue come I'll stay around for
that if you want to talk to me about
that I got some friends I want you to
talk to and another thing is work on
your empathy and your self-awareness we
can change the data we can make it
transparent we can bring people in but
if we don't improve who and what we are
if we don't develop more empathy and
self-awareness we're just going to
revert back to the mean and that is
something I've seen every time you know
yeah just recently we got what's the
guy's name
Jason called Beck is trying to get back
out there and repair his image right
he's one of the VCS who it was he's
trying to repair his image like first of
all no second of all we're going to
maybe the instance of awareness which
means you probably should just not show
up you know you you you have tainted a
such a large pool that you have no
business going out there and this is
what we like to do though we we think we
can just go and programmatically solve a
problem but sometimes the problem isn't
out there sometimes a problem is in here
sometimes the problems with us and I
really challenge all of you to you know
to this is one of my favorite of Barack
Obama statements and is totally
non-political he said every day he tries
to wring a little bit of bias out and I
think that's a great quote for what we
do for the systems that we're building
is to wring a little bit of the bias out
you can't get it all out you won't even
know about most of it but just a little
every day it's kind like refactoring you
know yeah it is this like refactoring
nobody likes to do it but it's really
good it you really end up in a better
place because of it so refactor your
empathy refactor your self-awareness
constantly consistently says some
sources here bias-variance tradeoff
check that out
it's on elite science it's a great read
algorithm watch and the algorithmic
Justice League which I think it's really
good timing these are sources you can
all go to that will help you understand
how to start attacking bias in your
datasets Europe as always as a head of
the United States when it comes to
protecting people they have a general
protection regulations GDP are in Europe
it's amazing I say go and read it you
will learn a lot if you're not following
that I advise you to start doing it now
it may not make it to the United States
but I think it's the right thing to do
and then there's a fredericka Pell Zell
she's I believe in a Montreal I sourced
heavily from her for this talk she has
an amazing medium post on this that goes
into depth of many of the things I've
talked about there are a lot of people
out here talking about this now let's
not make these same mistakes let's not
build a data ml weapon of mass
destruction and deploy it on people
unsuspectingly and then stand up a year
two or five years later and say we're
just the platform because no one is
buying that anymore
thank you and this is kept going a
little faster than I thought I thought
I'd talk slower but the part that has
concerned me is we generally have worked
without a lot of accountability in tech
we have for years have been able to
craft systems and platforms with little
oversight and I think that has been for
the most part a good thing those times
are changing and they're actually coming
to a close I would rather be self
regulated than regulated by the
government I'm sure most of you in here
would rather be self regulated than
regulated by the government but you have
to start leading today don't wait for
these problems that we've seen with
Twitter and Facebook and reddit and the
other you know platforms that have been
co-opted by foreign governments to
spread false information the only way to
do that is to start attacking this today
start attacking this in your data sets
today you may not be building the next
Twitter you may not be building the next
Facebook but you're building the next
something and I implore all of you to
start thinking outward start thinking
the impact it has on people that don't
look like you people who don't come from
your backgrounds people who don't come
from your schools people who don't come
from your families you know people who
are less privileged than everyone in
this room it is so important and I am
spending time on that because I've spent
20 years in this industry and I've
watched it become a force of change in
the world that today
I'm more embarrassed and proud of I'm
more embarrassed that we built a system
that was co-opted by a foreign
government systems that were co-opted by
foreign governments that put information
in front of us that was not true that
started essentially or amplify the
racial animosity that has been brewing
in this country
decades amplified it in a way that no
one would have anticipated five years
ago I'm positive that Jack and EV and
Mark and Zuckerberg we're not sitting
around thinking oh we're gonna build the
system that's gonna do this they did not
think that but as the system's grew
beyond how they even understood it and
as they brought in people to continue to
scale those systems no one asked those
questions and we cannot make that
mistake again
I always think I should have something
pithy to say at the end but I can't this
is a dark talk as you say it's a black
talk I wanted one thank all of you for
being here and listening to what I have
to say it's always an honor to get in
front of people it's it's it's something
that I look forward to because I think
when you can speak to your peers and
they listen to you you can move the dial
a little bit so thank you for helping me
move the dial thank you for showing up
early in the morning and thank you for
laughing at my really bad jokes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>