<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Crushing Tech Debt Through Automation at Coinbase | Coder Coacher - Coaching Coders</title><meta content="Crushing Tech Debt Through Automation at Coinbase - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Crushing Tech Debt Through Automation at Coinbase</b></h2><h5 class="post__date">2017-11-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KaK2lAj1B-Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everybody for joining today I
know this is the last session of the day
so we'll try to make this a little
entertaining for those of you that were
at the open session earlier we had some
good discussions and there was a lot of
talk about different kinds of technical
data one of the pieces that I'm really
gonna highlight in this talk is what it
looks like at the end of the rainbow or
what are some of the other factors that
we are trying to get to as we remove
technical data how do we want to see our
organizations improve as we step into
this could I get a quick show of hands
for anyone that has a coin based account
right now okay we've got a few people in
the audience so this will be near and
dear to how we're protecting and
securing your environment for those of
you that haven't seen coin based before
we're one of the largest cryptocurrency
companies in the world and what that
means for us is we're in a space that's
evolving really quickly and security is
very core to everything we're doing in
our environment we've recently started
expanding to the UK so we have a small
office nearby that I'll luckily get to
spend a little bit of time on this week
and if you haven't seen our platform
before this is what it looks like so
this is our G Dax exchange one of our
investors is the New York Stock Exchange
and this is actually money moving
through our servers we've scaled up
quite a bit over the last several years
I've been at the company for around
three years and this is not just a
ticker in a ledger this is real
cryptographic money moving through our
services so we have a large and
concentrated store of value that we
spent a lot of time thinking about how
do we protect we look at a lot of
metrics in our environment as well so
from when I've joined the company in
2014 we've sent scaled from one country
to about 33 countries today and we have
large growth plans for later this year I
think that number will tick up quite a
bit we've grown to about 6.1 million
users and I think really interesting Lee
has been the growth of services there
have been a lot of talks at Q Khan about
monolith micro services or what does
life look like with micro services but
one of the other sides to growing the
number of services that we have is also
the technical debt and the complexity
that it takes to properly manage all the
services when I joined the company in
2014 we had one monolithic service that
was coinbase we've since scaled up quite
a bit now running over a hundred sir
today and I'm gonna pull back the veil
and share what some of that those
growing pains have looked like and what
are the tools we build to manage the
technical debt associated with them the
last metric that I'll expand on quite a
bit is the number of deploys per
engineer per month so this number is
actually the average amount of deploys
per engineer per week that we want to
see in our environment we strive to have
every or the median engineer in our
engineering team deploy about once per
day per week we try to avoid deploying
on Fridays to avoid incurring weekend
debt but right now we're deploying on
average or median engineer about four
and a half times per week and we're
really proud of that statistic for the
overall industry that were a part of we
think we're trying to accomplish a
fairly large mission the mission a coin
base is to create an Opel an open
financial system for the world and a lot
of where Bitcoin has been shared in the
press and where there have been articles
written about us have been the large
spikes and the booms and the busts and
if we were to focus solely on those we
wouldn't have a large challenge in front
of us but we're thinking about is how do
we connect this new technology to the
rest of the world so there have sure
been some large spikes over the past
we're currently right now at an all-time
high in our industry which means we're
seeing more load than we've ever seen in
our environment before and we're very
proud of that but we're really thinking
about the long term we're thinking about
not how does the market cap grow
incrementally how does it grow
exponentially and so as we're designing
our services inside we're not thinking
about how do we get twice as large we're
thinking about how do we grow 10x and
how do we ensure that we don't introduce
technical debt today that puts handcuffs
on our team and prevents us from
reaching the scale we eventually would
like to see ourselves out following that
moonshot mission there's for those of
you that follow cryptocurrency on reddit
there's always been the to the moon guy
where whenever something large happens
in the industry you'll always see this
person comment on that thread inside of
reddit and we think a lot about how do
we design for truly massive change in
our industry and that's really why we're
all at coinbase today so I deeply
believe that moonshot players are
powered by engineering the
ah city and the reason we're all here
today is because I believe we will fade
away with technical debt if we cannot
maintain velocity to iterate faster than
other people in our spaces and grow as
our environments do as well we simply
cannot survive and so I'm going to talk
a lot about how engineering velocity can
be looked at measured and improved
inside of our environment I've split
this talk up into roughly three sections
where I'm going to start on that topic
of engineering velocity I'm then gonna
talk about some of the unique things
we've done in our environment to
fundamentally architect out debt from
our from our environment I think a lot
of the lessons we've learned are
applicable to other environments and I'm
gonna finish with a recent test that we
ran that we called scorched earth to
exercise the components of our
architecture that we think help
aggressively remove technical debt from
our world so starting with engineering
velocity could I get a quick show of
hands for anyone that's ever worked in
the government ok um could I get another
show of hands for anyone that's worked
in finance or some heavily regulated
industry the majority of the room ok so
I think this is a telling sign where
I've worked in these environments to
where you show up to work in we want to
be empowered engineers we're here for a
mission there's something we want to
ship out to production but there's some
slow or bureaucratic or churning process
that just prevents us from getting out
to production and this may have come
from a reasonable place maybe there's
enough technical debt that there's real
fear or there's the potential for a
catastrophe if we ship the wrong thing
at the wrong time
but this is what we have to fight
against we need to create an environment
that engineers feel empowered in and
want to work quickly for for those of
you that have ever worked through an
engineering change request process I
found this online for one of the large
enterprise business suites that I think
you'd all be familiar with and what this
does is detail all of the steps required
to make a change through a software
suite that they share and if we break
this down into into a linear step to
simplify this a little more this starts
with the manual process on our tool of
creating forms this then encouraged us
to make the documentation which based on
the system they were using here on
positive as a manual step they then
encourage you to read line in
building on that system is a third
manual step then you have to go out and
get signatures and they mean the
physical kind here and only then can you
do work think about all those barriers
to adoption or the barriers for a great
idea to find its way into life and after
that you can go through their release
process and what really kills me is that
throughout our industry this is really
popular this can be the norm for how a
lot of us do business the the the
presentation this was shared and online
had almost 90,000 views I I don't
suspect we'll get that money on this
presentation but if we can share these
ideas and start to fight against the
manual sludge that prevents our
creativity from finding its way into
production and encouraging engineering
velocity I think we'll have made a great
impact where I think this kind of
process is especially harmful is a quote
I picked up from actually my first job
one of my first jobs ever at a car wash
where they impressed in us that when a
customer has a bad time through our
experience they are the majority of the
time they're not going to complain when
someone has a frustrating time and our
infrastructures or our environments I
think the same holds true and the
majority of them frustratingly will
never return so we're not going to get
the feedback that there's a potential
issue in our pipeline ensure we can do
things to improve that but fundamentally
the default state is we won't know when
there are large problems and this is
going to pull people away from sharing
ideas or away from trying new things and
coming from the government where I
worked in the past to now at a
reasonably large financial start-up
encumbered by a lot of security
regulations for a good reason
I've come to deeply believe that the
velocity that we need to create is what
helps our environments evolve and for
those of you in a start-up or someone
that's not printing money or having a
full monopoly over your business
we need to continue evolving to survive
and it's technical debt that prevents us
from turning that velocity into
evolution in production I want to rewind
over the last few years in our
environment where this is what coinbase
looked like in about 2012 so we started
as a company that was trying to bring
Bitcoin of the masses and we started as
a hosted
coin wallet we we called it we realized
that people didn't always know what that
meant so we then pivoted to now focus on
making Bitcoin easy we then as you can
tell hired a designer and wanted to make
it simple so now we were Bitcoin made
simple who then became Bitcoin safe and
easy this is after there was some very
public security incidents with other
players in the space we started a focus
on security and all the while behind the
scenes here this is where we're starting
to grow from one service to now ten
services and now thirty services our
design team then had some other reef
actors in our in our environment we then
became the place to have a hosted
Bitcoin wallet and right around this
time we realized there is another large
opportunity in our space to not just
provide a tool for consumers but to also
create an underlying exchange and that's
where that tool G Dax which is now
another brand of business force came
from this represented a massive shift in
our internal infrastructure from
starting to migrate off of Heroku and
third-party services into an AWS
environment that was much higher
performance moving much faster and much
much more secure from this point we've
started to spend a lot of time recently
on scaling around the time of the last
screenshot we had a couple million users
we pretty quickly from there shot up to
3 million 3.3 million around this time
we also branched outside of just Bitcoin
and we're now servicing multiple crypto
currencies which was also a massive
shift that migrated us from a lot of
technical debt from assuming there was
only ever one pair to now supporting
multiple crypto currencies like
aetherium from this point we then
focused on scaling quite a bit so we're
now up to about 6.1 million users and
growing now faster than we ever have as
of yesterday we were seeing the highest
loads on our website and databases that
we've ever seen and we think that's
going to continue scaling and it's
having engineering velocity that helps
that proceed at a fast rate over this
period where of evolution has been
especially valuable for us has been
watching the rest of our space evolve
this is a snapshot of some of the other
players in our space and some of our
competitors a few years ago fast
forwarding to today most of those
companies don't exist anymore and I
think our investment in developer
velocity and developer experience has
really helped us survive and continue to
thrive so looking at some metrics in our
environment on what that's looked like
over the past year we move fast we
launched a lot of servers we deploy
really frequently quantifying that we've
launched almost a hundred and fifty
thousand servers last year we deployed
over 13,000 times this is just to
production successful deployments and
we've created about 45 new services in
that period and we're really proud of
that velocity but following one of the
points I brought up at the beginning we
think this velocity is only at the
beginning of the growth we're here to
see looking at some of the other large
financial networks visa today is
processing around 2,000 transactions per
second PayPal is processing around 155
bitcoins only processing around 3.5
there are a lot of wild ideas and new
cryptographic primitives that we think
will help us scale to the the volumes we
like to see and these are gonna
represent other massive shifts we're not
sure what they'll all look like so we
have to design for an uncertain future
and design with flexibility in mind I
think engineering velocity really
requires two pieces under the hood the
first are the tools that we can build
and provide our engineers and that can
be broken down into number of components
but also the guardrails so as people are
deploying into production we don't want
to constrain what they can do or how
they can do it but we want to make sure
it can only be done safely security for
us cannot be an afterthought it can't be
something we look at reactively once
something's in production it needs to be
a core part of our pipelines so security
is what enables us to work not something
that disables us by default we are not
able to work in our environment and it
is the tools and the guardrails of that
security provides that allows us to move
productively one of the first pieces I'm
gonna start breaking down is shipping so
we have a strong culture of shipping at
coin base that is a key word that is
what our founders value this is
something we emphasize heavily and we
emphasize this because we believe
stagnation stagnation is a form of debt
if we're afraid or for or for whatever
reason unable to push changes into
production that's often a great
indicator that there's
kind of debt in our environment and we
spend a lot of time instrumenting to
measure what shipping looks like I want
to talk about our evolution from the
early days of the company where we
started on Heroku which was a great
enabler to help us move fast deployment
was I was very easy one line of code
would allow us to deploy and the
environment was entirely managed for us
when we were running on Heroku we had a
reasonably small engineering team and
this chart is showing the number of
deploys per engineer per month and what
I want to point out here is that the the
bottom three engineers where we have a
number of engineer other engineers
grouped up into the light blue on top
but the majority of our deploys were
pushed out by three engineers our
engineering team was much larger but
only a few people had the permissions to
use this tool because that one line was
so powerful if you had that deployment
access you generally had access to do
just about anything inside of our
environment we then invested a lot of
time building internal deployment
tooling is I think almost every shop
here has done at some point building
your deployment tooling as you build in
your cloud or your environment the
tooling that we built had a strong
emphasis on a more powerful permission
model so we could expand the number of
people that were empowered to deploy
without having to tap another engineer
on the shoulder so they felt that their
changes that things they wanted to push
out to production they wouldn't be
stopped and they wouldn't walk away
dejected and perhaps not let someone
know but they would have their own way
and a codified pipeline to push that out
so as this tool code flow came into the
wild at coinbase
what we saw was a massive increase in
the number of engineers that were able
to deploy to production so this is the
month where we first started to release
code flow for our internal services
actually the month we deployed our
coinbase exchange or G Dax and the all
others became our greatest source of
shipping to production we were really
proud of this this was removing our lead
engineers or some of our leadership from
being the only people trusted to hit
that deploy button and creating a
codified workflow so everyone can start
tackling some of the debt in our
environment and preventing other ideas
from not shipping out to production in
order to ship effectively there are a
few tenants that
we've that we've invested in and we
deeply believe in one is in order to
make this a core part of your
environment everyone needs to be able to
deploy your tests and master to
production once we've gone through tests
we've gone through our guardrails we
want to trust those guardrails and we
allow anyone to then push those changes
that have passed our guardrails out to
production period we don't believe
production deployments start on your
first day we want this to be a very
fearless process we want our engineers
to feel comfortable pushing even our
largest services out so on your first
day at coinbase
we walk you through the process to make
a change and hit the deploy to
production and the deploy to production
environment we want people to be
comfortable deploying to this
environment and lastly having a
blameless culture we spoke about blame a
little bit in the open session earlier
today and that's a word that we really
don't like we think when there's a
failure in production it's a failure of
guardrails it's a failure of automation
we never blame an engineer we do not
want that a part of our culture
accidents happen short but if something
breaks we're gonna learn from that in
the form of better automation or better
testing or better health checks and
continue improving just like the Boy
Scout rule that was mentioned earlier we
always want to leave our environment
better than we found it and we do not
ever want to have a blame culture fast
forwarding a little bit further to what
deploys have looked like as we've scaled
since we've migrated on to this new this
new deployment system you can see our
large spike because we migrated off of
Heroku and we're able to start
democratizing access to our production
environment you can then see other
engineers starting to join the company
over that period you can see mr. green
here start to contribute to other
services inside and so this chart
roughly mirrors the increase in services
and staff that we've seen over time and
we're really happy to see that number
killing up and not stagnating as more
security and regulation it comes into
our financial space when we overlay the
price of Bitcoin over this so this is
showing roughly in a scale of GBP how
valuable coinbase or how valuable
Bitcoin was over this time we see it
growing a little bit but we don't see a
big exponential growth well we see
ourselves spending a lot of time on is
improving our infrastructure and
herring for big changes and it was a lot
of the work that happened in this period
preparing for scow preparing for
flexibility that's allowed us to survive
and handle some of the load we're seeing
today and it gives us a lot of
confidence that as our environment and
constraints change around us we'll be
able to adapt to those as well
poking a little bit at the guardrail as
I mentioned before not all of our
deploys to production succeed and that's
a good thing we want engineers to make
changes that may not always succeed in
production what these mean are our guard
rails are working so when we deploy
something out to production that for
whatever reason is not going to succeed
maybe there is a untested to take a base
query or we're missing an index or
something's unique in production we have
robust health checks that we'll see that
roll out and if there's a failure or
performance degrades or we're not seeing
the same characteristics that we'd seen
on a last deploy we'll fail that deploy
we'll roll it back we'll call that a
successful usage of one of our guard
rails and we'll provide feedback to an
engineer to say hey this didn't work for
some reason maybe you weren't aware of
some debt and some other part of our
code base maybe there's some dark code
in another place that caused some issue
here and we'll start to walk them
through it so rather than being an
environment fearful of that where we're
afraid to push something out this allows
us to move fast embrace some failure in
our environment and still work safely
and these are some of the shipping
metrics behind our growth of services
today we're running at about a hundred
and five services in our environment and
so as that number continues to grow I
think that follows closely Conway's law
like we saw from the last speaker and by
breaking into more services and
providing the tools to do that
effectively we see ourselves
increasingly lopping off debt turning
that into a new well-defined service and
increasing our velocity from service to
service and deploy to deploy one of the
KPIs that we look at alongside of this
is that metric I mentioned deploys per
person per month this is a really
valuable metric for us to assess of the
debt and the fear and the overall
productivity in our bar
and we expose this on our weekly
retrospectives for our infrastructure
team and the rest for engineers as well
this is a metric we want to share with
our teams this is not something used by
management to objectively evaluate how
we're working this is something we want
to use to encourage our staff to build
the systems to maintain their velocity
the target metric that we have is about
four deploys per person per week we
recognize all weeks are not the same so
we look at that over a monthly basis and
we target 16 deploys per person per
month coinbase varies from about 3 to
6.5 looking over the last year and a
half or so I attribute that variety to
new services coming online or new design
periods where we may not have been
deploying to production as often as we
have in the past some of the constraints
we look at here are only successful
deploys only to production in only new
commits we don't measure when people are
deploying to development we don't
measure deploys to staging in this
metric and we don't measure when old
deploys are being redeployed out to
production this is something that's
supposed to look at what our velocity
looks like in how we're evolving if
we're just looking at how old things are
changing or experiments are happening
that's not as effectively evaluating our
ability to evolve in production and
that's really why we're why we're
looking at this and that's the change we
want to see some of the other big
factors for us like I mentioned Micro
services but also ackles access control
permissions the ability for people to
for engineers to feel empowered to get
things done is incredibly important I
can't tell you how many times have seen
environments where engineers would like
something to change but they don't have
the permissions there may not be a great
reason behind that it may be
organizational bloat or they haven't
been onboard ador there's not a
well-defined process but anytime you
don't have the permission to go through
a process that for no good reason is not
codified or automated that's deeply
harming your environment and we think a
lot about how to streamline that
simplify our permissions and empower
engineers and lastly for edge cases not
everything needs to deploy we've shared
some of these metrics online and in the
past people have immediately called out
well you're a financial institution you
must have services managing money or
managing crypto are you honestly
deploying those on a regular basis apps
we have different rates of a loss inside
of our environment and that's a great
thing our web page or our front-end
services might evolve much faster than
our financial processing units in the
back and that's intentional and I think
that's a good thing as well one of the
next places we look at our pipelines so
how many people have a deployment
process where every engineer every
service team has a slightly different
way in which they access their servers
or deploy something out to production oK
we've got a few people and that's a
natural state to be in it's something
that evolves it empowers engineers to
give them access to their services but
this makes it really difficult to manage
those services over time so we've
invested a lot in having streamlined
pipelines to get things done it helps
engineers migrate from team to team to
team and it's deeply valuable for
security where you're an environment
that has many different ways of
deploying that can be fine but now you
need to replicate your security work
some of your infrastructure work some of
your availability work and so by
streamlining your pipelines and really
thinking about your components that can
be a great thing for velocity and
simplicity these are some of the
components that we take a strong look at
inside of our environment and one of the
big shifts that we went through is from
looking at these as individual
components and different services that
perhaps different people were we're
focused on to starting to link these
together and thinking of this as a whole
workflow from starting with an idea
walking through all of these steps and
getting something out to production
wherever there are manual blocks or you
have to walk and talk to a human or you
have to slow down because there's some
process that's not well understood or
defined that is a problem and that needs
to be addressed
this whole workflow is one developer
experience and it needs to be optimized
as such one of the projects I worked on
recently was to start simplifying the
bootstrapping of this process where we
originally had some documentation and
some manual training where we would help
get you up to speed on these services
but what we do now is we have a simple a
simple Ruppel with template projects for
all of our default languages if you want
to start a new ruby project at coinbase
today you can run cf Annette this will
pull down a default repo it'll have a
default soccer file configured some of
our default docker images
our default circle test file again
ignore file some of the simple
boilerplate to help you get up and
running so we can further increase your
velocity through this pipeline this also
like I mentioned bringing security in
line has had a great boost for us where
rather than security and infrastructure
and other teams coming in after the fact
to say well you built this new service
and scrape but actually we need to use
this we can bring them in a start and
make them a core part of our pipeline
this prevents a lot of the the unique
services or the changes from environment
to environment that can start to look
like death if all of your services are
unintentionally designed completely
differently if we're not holding hands
or providing services for our developers
to make steps repeatable as we go from
service to service and we're trying to
go from team a team to contribute across
all of our code base it's gonna be
really difficult and what it looks like
debt to you might not look like debt to
me because I'm so familiar with it so we
want to start standardizing or
recommending recommending those
standards were possible the next part of
our workflow that I think is fairly
common from any of us is a pull request
model we use github Enterprise
internally it's worked well for us but
we all I hope have version control of
some sort one of the next pieces that we
always go through from our polar quest
is then going through a code review and
there are a lot of good tools that we
can use for a separate code review there
are great meeting structures that we can
run these through as well but we've
spent a lot of time integrating this
deeply with our workflow so our code
review is all done on our pull requests
that we've opened inside of github we've
built our own tooling to streamline this
in line and this is where our code
reviewing and our approvals to go out to
production will happen once we go from
once we're working on our code reviews
we'll also monitor that in line so we
have a bot that'll monitor who's
approved what's been approved this ties
back with our deployment systems so
we'll see those approvers in line we
also do this for other parts of our
environment so it's very easy for us to
create other dart components from your
dart code but also dark parts of
infrastructure and I've seen a lot of
environments where you may want to make
some change in your environment
because you don't have the permission
you have to talk to someone in security
or production or net ops to make a
change to a firewall or some other
setting so we spent a lot of time also
codifying our infrastructure to turn as
much of our environment as possible or
any part of your job you need to work on
on a regular basis we want that codified
we want that in our pipeline we use a
tool called terraform inside and a
wrapper around that we've open-source
recently called geoengineering that
helps us easily codify our
infrastructure so we can also code
review and open pull requests to any
environment one of the metrics for
success we were looking at as we started
to go down this project was I wanted any
intern or anyone on their first day at
coinbase to be able to open up a pull
request and propose a change to a
production firewall and see that process
through a codified codified a workflow
make its way out to production so this
is not tapping people in the shoulder
this is walking through a well-defined
pipeline the next component of that is
security and following this streamline
pipeline we bring that in line to every
one of our pull requests that's open is
run through some security tooling that
we've created so we'll scan for
application layer issues network layer
issues or OS or dependency layer issues
and that will all happen directly in
this pipeline we're using github status
checks here we also expose this thread
appointment tooling so it's as easy as
possible to look at by bringing this
into our pipeline this also prevents a
lot of debt from forming once we've
deployed or try to deploy in production
I'm sure many of you have at some point
been stopped by security where you've
worked your way through a project
security companies as well we don't
quite want to want to do something that
way and it comes after you've spent a
week working on it and that can be a
really destructive thing to your ability
to evolve in production we then go
through a fairly normal CI process we
use circle CI enterprising inside but
we've used a number of services over the
years one of the pieces we've spent a
lot of time on is optimizing the build
time the build speed and especially the
build queue inside of our environment
the build queue is something I think we
all have a lot of affect over it can be
one of the easier things to optimize
over your build times this was brought
up at Facebook's disrupt conference a
couple a couple of years ago where you
can look at this kind of problem in a
couple of ways and often we find our
humans or engineer
is waiting on machines and I think
that's the wrong way to look at a
problem or approach a solution and we
always want to invest in machines
waiting on people not the other way
around almost every case results in
machines being less expensive than
humans and empowering our staff to to
not have to wait on our machines running
can be something that can really help
our environment we do this through our
for our CI cluster and investing quite a
bit in a large auto scaling group so
we're using some custom cloud watch
metrics for those of you familiar with
AWS this circle dot run queue container
is a custom metric they send out to
cloud watch we can pick that up in our
auto scaling group scaling metrics and
whenever we see engineers show up in the
morning we'll start to scale up our
capacity and over provisions so as new
tests come in through the pipeline
we'll have capacity of ready to go
machines will be sitting there waiting
on humans not the other way around it
helps us move fast the next piece that
I've mentioned a couple times is
permissions and permissions can be one
of the most dissing abling things in our
environment so oftentimes at larger
works the way we pick up privileges is
from the time we get hired until the
time we leave we slowly accumulate
permissions we slowly get more effective
and it might be that's that senior lady
on your team that's the most effective
person in the room because she's been
there the longest not necessarily
because they're the most senior engineer
or because they're supposed to have more
permissions they've just been there
longer so they're more effective and if
we're not empowering our engineers from
the first day they do they join your
organization we're doing harm to our
environment especially when we want to
tackle debt if we're not empowering our
staff to actually tackle that debt or to
make that small upgrade we're slowly
evolved as new services and new
dependencies are released we're going to
see that a queue across a crew across
our environment so we need to make this
a part of the past of our industry
investing in good onboarding for your
staff really thinking through what are
their permissions your whole team needs
making sure everyone has that permission
I spent a lot of time walking through
other engineering teams at coinbase and
I'm always asking what
there anything you were blocked on today
when you were working on this project
were there any manual steps was there
anything you weren't able to do and I
make a note of that and I'll
aggressively go back and look at why
that was the case and try to
fundamentally fix our permissioning
models inside those pour permission
models can truly handcuff your best
engineers and with those handcuffs on on
when we can't keep our services up to
date when we can't evolve and when we
can't ship that's we're gonna see debts
start accruing and growing inside of our
world the process we follow to truly
empower our engineers in a world where
security is one of the most important
things that we do is we follow a dual
person turnkey model or we think of this
as consensus engineering and I truly
believe that true value is connected is
secured through consensus humans can be
single point failures laptops get
compromised we see mistakes happen and
so we invest a lot in making sure no
single person has access 20 of our
customer funds any of our secure data
but always providing multiple people or
a quorum of trusted individuals the
ability to escalate permission and make
a large change and have a large impact
in our environment the way this is
implemented inside is in three phases
the first phase for a good consensus
engineering permission model for us is
the proposal phase we need anyone to
have the ability to propose some change
to our environment for us this means
codifying as much of our environment as
possible trying to eschew the idea of a
single administrator and having that
person be able to propose a codified
pull request typically through get to
our infrastructure to one of your
applications or any of our source code
okay of course making sure secrets are
out of that we then have a separate
phase of achieving consensus this is the
consensus code review bot that I showed
earlier in this process making sure
multiple people have cryptographically
signed or approved that this proposed
change is safe to deploy to production
and we can control that a few ways but I
want anyone to be able to propose almost
anything in our environment security for
us is in the achieved consensus phase
and lastly I want anyone to be able to
apply anything
to production if someone can't deploy a
tested and passed master branch of your
largest service to production I think
you really have to ask yourself why and
if it's because you don't trust your
tests or your suite that's something
that needs to be tackled directly this
is this is a snapshot of what that looks
like in our environment where when
multiple people have turned the key or
code reviewed and approved the pull
request that unlocks its we can deploy
inside where there are novel
cryptographic applications of this are
where we're starting to use a lot more
Sameer secret sharing in our environment
and this looks like some EMA and
implementation where n people are
authorized in any M of those people are
required to come together to unlock the
key that can make great change and great
impact in your environment one of my
favorite implementations of this that's
a source and accessible by everyone
today is with Hashim corpse vault
product and can I get another show of
hands for anyone that's using the their
vault product today for who's familiar
with this a small percentage maybe five
percent or so so what this looks like in
the underlying implementation we end up
using this cryptography for a lot of our
key stores we're restoring real value
and the form of cryptocurrency on our
machines or in our cold storage is with
an implementation like this where when
you initialize a new vault you define
the number of shares or that's the end
of the number of people that will have
access to this and then the key
threshold which is the M and this is
saying that any two of these three
shares can come together to cryptic
cryptographically unlock some encrypted
key that's encoding all of our data in
the back end once we've created those
shares will then distribute them through
some secure ceremony and as long as
those shares come back together that's
what we're gonna do here is post one
shard and then post a second shard that
unlocks or unseals our vault and that
allows us to work we use this for even
some of our deployment processes inside
we're beyond the normal vault the vault
usage their underlying Shamir's
implementation can be used to require
multiple individuals to unlock a deploy
unlock a service or create some other
great change inside we've integrated a
lot of this through our deployment
tooling so this is really our consent
is engineering tool more this is our
code flow service that powers all of our
deployments at Quinn base one of the
next metrics we think a lot about is
what our developer productivity looks
like compared to the rest of the
industry and for those of you that have
seen the annual puppet state of DevOps
report I think this takes a great
snapshot into how the rest of us are
doing in this world some of the metrics
they pull are how fast people are moving
how fast people are deploying the impact
that has for public traded companies on
their share price and their overall
performance as a company so one of the
metrics they look at is the frequency of
deploy and they find people practicing
DevOps the way they've defined it are
often moving 200 times faster than their
other competitors in the space and 200x
improvements is a great enabler for our
ability to change looking on the top
right for 24x faster recovery times from
from failures that is absolutely
critical to tackling debt in our
environment if we can't respond to it
quickly it's going to keep accruing so
that kind of velocity is really
important for us to keep debt out of our
environment the next thing we find is we
all want to bring great people into our
environments and this is something that
helps us continue to evolve quickly and
we find that in high-performing teams
that are moving faster there are nearly
twice as likely to refer good people
into their environments so also a great
thing for a growth and last lastly for
security 50% less time remediating
security issues it turns out most
security issues are not while they're
sexy nation-state issues they're often
simply not having patched or updated
your libraries and these are things that
we can truly address so working in the
world of finance devops can be a bit
scary as well we've referenced public in
our home page we've moved about six
billion dollars in cryptocurrency
through our infrastructure today and
this kind of velocity that allows us to
remove debt is often at face value
incompatible with a truly secure world
where we want to apply DevOps and
automation so I decided to take a look
recently on what DevOps and security
look like in the growing space of
FinTech so the way I took a look at this
was through a new tool that I shared
recently on github this is github calm
slash would off deploy velocity and what
this is looking at is for home pages of
sites that are published online and it's
pulling all of the HTML assets that
they've shared publicly so if you look
at our site whenever we push you to play
out you'll see a new static asset hash
change and you can roughly see when a
lot of modern websites are deployed
static assets will be cached based on
some hash ID so I ran this for the last
month and looked at some of the other
players in our space and I was looking
at how often are they deploying to
production or how often are some
production assets changing now this is a
very crude metric it has no insight into
the internal workings or the backing
services it's purely looking at one
service or one web component of these
people's environment and we found that
there's a fairly large spectrum of
deploy velocity or deploy rates over
this month but we see some of the
largest players on top are deploying the
most frequently I think there's a strong
correlation component to this where
larger teams are often naturally
deploying faster but where security is
so core to what we're doing
seeing ourselves near the top on this
list is a good indication that we're
doing something right here
we then took this and for all of the
people deploying in this phase we're all
cryptocurrency companies so you can all
see the amount of money we're moving
publicly on our sites so we can see this
for many of the other players in the
space as well and we mapped the
deployment velocity with the amount of
money that they're moving through their
environments and we see here pretty
strongly that the the largest performers
or the best performers publicly in terms
of funds move are all deploying at a
reasonably high rate and where security
is so core for what all of these teams
are doing this is something we were
really happy to see where we're working
in a debt filled environment there's
this also there's this challenge we can
also run into of unintended velocity so
where we cause some public outage or
some public issue or a high severity
issue and we've had our share of these
recently as well and I think when
they're when the right guardrails are
not in place it's easy for debt to catch
up with velocity and cause real problems
and we have to be really careful about
this we saw this happen to one of our
internal services Risa
that I'm gonna walk through where this
is our change list of history of
deployments out to production for the
service and we deploy reasonably
frequently it turned out for all of
these deployments we hadn't yet
integrated the system with the rest of
our pipeline and so what was happening
here is we were building a docker image
it was going through security scanning
and then we were deploying out to
production without any form of CI in the
service it was a component of legacy
that had been grandfathered into our
environment and it turned out there was
a breaking change that was made this was
deployed out to production and for about
48 hours this service was only spewing
errors and it was completely offline and
this was especially bad because when
this was deployed where we normally have
HTTP health checks and were monitoring
for the state of his service reactively
and ideally we want to proactively test
her services but this is reactively we
were also not monitoring for the state
of that service so because we were
missing those proactive and reactive
checks the service went offline without
a warning for nearly 48 hours and this
happened because we encouraged our
engineers to move fast trust that we
have the guardrails in place and in this
case we were missing those guardrails
this was not the fault of the engineer
this was the fault of the guardrails not
existing in the right places
so the way we were able to carve
ourselves out of this problem without
going through and refactoring the
service what we did was we wrote some
functional tasks that were not looking
at all the information is in our CI
system its booting up the whole service
it's making sure the service is failing
with the correct error in our
development stage so we now see these
failures in our CI system without
reactively pushing them out to
production so one of the ways we find
ourselves maintaining this velocity and
digging ourselves out where there are
still components of debt inside of our
environment is making sure we have a
good test phase proactively in all of
our services inside those safe guard
rails are also really important for the
rest of our services and making sure
we've reached a hundred percent of our
environment side so where we have a high
functioning team we're moving really
fast we find ourselves racing around
corners deploying new features and then
things break and this is natural this is
a good thing
we don't want our engineer as a
moving at this pace we want to build a
guardrails to make it safe I think
having debt in our environment often
inspires fear and we need to build
guardrails to contain that fear we don't
need to put barriers in place to slow
down our engineers it's the job of your
infrastructure your DevOps your security
operations team to build those
guardrails to keep your world safe and
automate it really aggressively making
sure you have a blameless environment
inside I really want to emphasize that
guardrails fail with that people do not
in this next phase I'm gonna talk a
little about how we fundamentally
started architecting a debt out of our
environment so I think a lot of this
comes with these things called
snowflakes so a quick search on github
for no idea why this works results in
hundreds of hats treats right how many
people have that service inside your
environment where your founders spun it
up somebody that doesn't work at the
company anymore it has spun up that
server and you're not quite sure if
anyone really understands how that works
yeah I'm seeing some heads and nods home
this is this this is a fact in our
industry this is how a lot of things
work and we're often afraid of these for
good reason you're never sure of
technology when a small change is gonna
take the whole world out from underneath
you okay and this is huge for security
like I mentioned security incidents
don't often happen because of
nation-states or zero days or some wild
paranoid world that we're all living in
looking at the Verizon data breach
investigations report where they profile
thousands and thousands of leaks every
year they found 99.9% of them happened
because the CVE or vulnerability had
been made publicly available and was
available with the patch for over a year
and you simply hadn't patched and this
is unacceptable and this is something we
can all work on in our environment one
of the core things that we do to fix
this is our 30-day fleet rotation
service so what our 30-day rotation is
is saying that we do not allow virtual
machines or servers to live in our
environment for any more than 30 days
we're constantly rotating them we're
constantly moving through them so no
server lives for more than 30 days in
our environment this is what works for
us this for us is a huge forcing
function
Automation where we have that snowflake
server that we're afraid to change
because we don't have the right testing
or automation on it this forces us to
redeploy that once a month every month
month after month and every time
following the Boy Scout rule we're
always making things a little bit better
working towards greater automation this
is always exercising our deployment
muscles so when something breaks we are
confident that we can respond to it and
we're removing that fear of change
inside of our environment you will
always accrue debt if your team's do not
feel empowered to change it and this is
really empowering for us and lastly the
sets of minimum velocity if we're always
deploying at least once a month we have
this constant habit of moving and and
evolving as a company and we want to set
that tone so our team knows that it's
okay to change it's okay to move fast
and it's okay to change as our industry
does as well
this is a chart of what our internal
fleet age looks like this is a metric
that we track our team looks at at least
once a week on average our server age is
around 24 to 48 hours of coinbase when I
took the snapshot we peaked at around 10
days for the median age of our server so
we're constantly burning and churning
and we share this metric in a lot of
places in our environment so we're
constantly showing those percentiles
this is from one of our internal
deployment tools the way we're able to
accomplish this is with strong Bluegreen
deployments throughout our environment
so we run everything in docker
containers but we don't yet schedule
orchestrate our containers largely for
security but also for availability here
so we're only running mutually trusted
containers on the same host whenever we
deploy a service through all on their
own auto-scaling groups and whenever a
new deploy goes out we'll bring up a new
auto scaling group make sure it passes
health checks and tear down all the
roads old servers so we're constantly
retooling our boxes this is easy for 12
factor apps but we're also able to do
this with a little bit more time for our
databases as well so a deploy of
coinbase comm will look something like
this where we have a lot of new auto
scaling groups going up and coming down
the last point I'm going to mention here
is rather than reconfiguring in our
environment we're always rebuilding and
we're always establishing that habit of
moving quickly I think it's key that
that's an implementation detail
your infrastructure that's something
your infrastructure team can own we
don't need to impose that on all of our
engineers and this feeds really well
into a lot of the serverless work that's
happening today this can be a great
enabler for us moving quickly codifying
our environment removing those snow
flakes and tying this into your world so
you can get fully automated deploys on
that I'm gonna move forward to the the
end of the talk and note that with those
architectural changes we've made and how
we've architected our environment we've
really tried to make crushing debt a
core part of how we work we're
constantly cycling our boxes we're
constantly rebuilding coinbase from
scratch and it's something we're
comfortable with it removes fear and we
find its greatly empowering for our
staff the big takeaways that I hope you
can leave with are one that velocity can
be the enemy of debt and we want to
remove debt so that we can improve
velocity in our environment we're able
to do that by destroying snow flakes
snow flakes can be the antithesis of
confidence and empowerment in your world
for us we found that our 30-day fleet
rotation is a great tool to enable that
and lastly crush the debt from your core
having streamlined deployment in the
deployment processes well-defined
pipelines and really thinking about the
overall experience helps you move fast
it's great for security it's great for
productivity and it's great for crushing
technical debt so thank you so much for
joining today I think we're a little
past time so maybe we'll skip questions
here and I'm happy to chat with anybody
if you want to come up afterwards so
thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>