<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scala &amp; Data Science Fun with Folks from Linkedin &amp; eBay | Coder Coacher - Coaching Coders</title><meta content="Scala &amp; Data Science Fun with Folks from Linkedin &amp; eBay - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scala &amp; Data Science Fun with Folks from Linkedin &amp; eBay</b></h2><h5 class="post__date">2013-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mtqqNJLQzfk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi guys I'm like Alexa mentioned my name
is Vitaly am I'm a data scientist at
LinkedIn and work on the product data
science team aplington currently doing
some stuff around also a profile under
sending them better and extracting more
data out of them and getting more data
into them like it appears every other
company in the world we're also hiring
so if your guys interested feel free to
connect with me on LinkedIn and we will
do this talk kind of short in a like you
can see it will be no presentation just
us will be writing a somewhere machine
learning code and what we are trying to
achieve and we're doing it for the first
time so kind of be patient with us
hopefully it will won't have any
glitches and everything will work but
we're trying to do a show how kind of
machine learning and big that it has
progressed so much that now we can do
kind of pretty sophisticated for with
very small amount of course so I will
kind of in order to show show that i
will do it online with you and i won't
be using the repple even though that's
kind of one of the neatest features
about scholar the problem is the atlas
for scouting since you write your job
and only when you actually write the
last line which is the right it's all
kind of submitted and gets analyzed and
optimized and stuff like that so it
doesn't really work well with in a
ripple mode as of now but still and you
will be able to see some neat stuff
hopefully so I will start with again
basic stuff that we and young mention it
as data scientists do it's not just
machine learning algorithm it's mostly
about processing data and
as we all know with napa do you know
there are two different things we do we
met we either mapper that data from one
form to an art transform but that is
actually not the most interesting part
of the process because we do it kind of
you know which is its transformation and
it's pretty easy to do the what L or the
actual more interesting stuff is how can
we optimize what we do on the reduce
stage where we get some large amount of
data and we need to reduce it to one
thing and then the working with example
for me and during this talk will be just
Wikipedia so just to show you i have
here so let's see more so i have this
article's txt that you can see is just
for some reason the first thing is an
anarchy ism and well just text and it
will be just a lot of and kind of tuples
for the first one the first element of
the top o is the article name in
everything else just just text so we use
kind of some parsing library it I know
how well its parts it but it it will be
fine for our examples today and the
other thing is i actually prefer to work
with objects and classes but i will be
working mostly with tuples since i will
be working in local mode here without a
cluster what's one thing about great
about the cascading and sculling that is
based on that it has a very good name
local mode that you can basically write
your code run it locally and just say
with no modifications whatsoever to the
code you can submit it to the cluster
and it will work as well there but there
is this one thing that also young
mentioned it's really stuck increases
hear that he's worked on the ever stuff
and unfortunately i realize just today
that a bro also works now on local mode
so we'll try some other time but as of
now we'll work on top olson
the first thing you know let's start
with something simple and it's a basic
worry that we we do some time we want to
get the top articles or the tub elements
in our data set for Wikipedia you know a
simple tap thing can be let's get the
longest articles in our data set so
let's just start writing the code and
top articles okay so M well articles I
think I have it so um those of you who I
don't know skating it works with a very
spike and results or something called
type pipe so um I prefer to work with
type pipes and it kind of looked makes
it cold look cleaner I think and also I
think one of the great thing what kind
of made me switch to Scala is the the
fact that it is strong type so kind of
avoiding using it will be just kind of
shame so we're basically getting it and
it's can hear they have type esv format
and here with the file type which is in
my case string and string and we get it
from in my case and hopefully you can
see that source main data articles txt
okay so now we have our articles and
like I said we want to find em the
longest the longest article a on
Wikipedia so kind of there are many many
ways to do that and for example and
again let's may be completely example to
show how basic war count in scalding
wars so if we want the workouts and it
will be articles that group by and it's
kind of very much similar to the skull
away
we want to group by the second and we we
kind of have the we have now a string
but the string represents represents the
list so we we need to kind of and there
is a map value that takes every element
and transform it to something else oh we
have the long string and we can just
split it so sorry it's let's take the
second one and let's just split it
according to some ray reg ex and now
that I'm let's see what we got here and
as you can see it's basically string and
RF string and now we can also if we do
length or size on the string basically
this is how we get the because a World
War count in sky in Scouting but like I
said that's not you know that's pretty
basic stuff let's try to get the top the
top article any ideas about how what
should I write in order to get the the
top articles by length anyone okay Chris
do you know
well so we can probably one example we
can take this war count and we can just
order it and so it doesn't have order by
in order to do or by unschooling and we
can just do fingers sorry has values it
gives me if we to type pipe it gives me
back at I pipe maybe I should write it
in this scholar form which is to type
pipe I get my type pie back so and here
we can grow by it again but there is a
group all which basically groups on the
unit so I get em everything a kind of
group of what it does it sends a the
entire data from all all the mappers and
to a single reducer and that's very
handy for soaring so here I can do so by
and now if I sort by the second element
and again and take the top x then M kind
of gives me a well want em but it's kind
of not the am not the cleanest way to do
it because in finger and candor must be
some other way and the subway is a coal
involves something that it's really cool
about scalding and one of the things I
like kind of most about it is it it's
called aggregators and they have this I
know if we call it kind of date and
design pattern but it's basically
implementing custom reducer in just
three steps so let's look at him how an
aggregator looks like so angry Gator
know this one angry air is powerful the
algebra so one word that algebra is
something that
is used a lot with sculling but it's
actually and not part that you can use
it without a scalding as well it's just
the library I'm going to do some kind of
basic algebra self and since it's
developed by Twitter then it has the
word bird in it and it's a trait where
you only need to define three things is
the prepare prepare is again if you
think about it in terms of map we use
it's like the first mapping on your data
to prepare it if for the reduced stage
then the reduce that gets a lot left
element the right element and you
combine it and present is kind of a map
after another map after we reduced can
be just identity or M it can be if you
need to form your data from B to C to
say you want to output it in some other
form then you also implement a the
present so that's the basic trade but as
we will see it's actually a pretty
powerful trade so let's try to kind of
define a class here and top and elements
that gets max which is will be an int
and it will extend an aggregator so
aggregator need to get free free types
so first of all what is the data that we
will we will do it on so we will run on
all all the articles that we have and
will basically what our starting point
will be will be a string so we want to
shrink and but the string of the text of
sort the string of texts the what we
want to get back and what we want to
reduce is may a list a list of all the
elements and let's say want a list of
the type top counts
so we will do it on see we can do it
here yes let's go do it from here
because here we already kind of have the
counts and so let's start with AIDS and
RuPaul what we have from grupal sorry
about that I just need the five-string
in it yes string and int okay and we
want to return a list of the top string
and ends and that's actually enough so
am I will release the third one as
basically the same because in my I'm not
interesting in getting--getting this bed
in some other form actually pretty
satisfied we're getting it as a the top
list so like I said I need to implement
those free function the first one will
be prepared so I somehow need to take
this input and m1 element and to to
transfer a tradition returns forward to
something that I can reduce so in that
case is just easy its list of input that
is how I prepare my data in order to be
reduced the second one is to reduce
itself so here again what we need to do
this is the signature of the functions
you can see it's texts left it takes
right and we want to to get the top
lists according by this count so let's
hoop kind of define an implicit ordering
implicit well or on equals or ring dot x
and which is this soaring will be from
what we have here showing an end to an
end right and we want to or it by a by
the second element by the second element
and we because we are interesting tab we
want a one-day organ reversed so again
we am we kind of define the ordering
here so again now they're a remember I I
have two lists and I need to get back
one list that will have the top a max
remember this class has max elements in
it because I'm interested in the top end
which max is this n/m kind of one way to
do it is just we can concatenate both
strings we can sort them now move sorry
and since we have a new implicit orange
that will work and we can just take the
top max element out of it and the last
function will be the present which is in
this case since i'm interested in
getting the same exact data it will be
just him a reduction which is the
parameter to present so this is my aim
aggregator pattern I prepare my data I
somehow reduce it which is he here that
were all the count went and present is
just an identity function so now after
I've done the group all and I get all my
work were count data and Canaan single
is I can just do the aggregate and what
aggregate expats expects to get is an
aggregator and we and we have one here
so let's say top and elements let's say
the top hundred wikipedia articles and
you can see here that what I get as
return is a
a list of string of int which is exactly
what I wanted and now we can sorry again
get the value side of it sorry the two
pipe and values out of it and kind of to
make it more to get not the list but
every element in single row it's just an
identity flat map so the identity flat
map basically takes the list and break
it down to it a pipe pipe of so up up to
here we have a tight pipe of something
that looks basically like that string
and int and since type pipe is you can
look at it as every element is it it
will eventually be a line in your file
then yes scroll up it what oh sorry
sorry yes okay yeah that I should look
here and what flat map on the list is
how well your kind of all know we'll
just return me string in an end and now
I can just write it to some again TSV
file which will call it the source and
the longest txt okay so just let's hear
em just call this function that we call
top articles and since i will be using
the parameters and we'll be using the
sum of this function now i shall we can
use it as well top and sore top articles
we can just now run it so let's see how
it runs
I didn't make a lot of mistake by school
so the way to run a saw I'm a fan of
maven I kind of didn't get to s bt m I
know it's like the nucleon blog what
i've been using maven for a while and
see like maven and maven has a very nice
color plug-in that it just has scholar
on any actually increase will show you
its console function and this is how you
can i add arguments to your code and
well so I kind of like it so now we just
run it and now trance everything in a
local mode so let's see launcher
aggregated by okay it's ended which is
and with successfully so that's good
let's see and what we got here so I
believe I kind of called longest and as
you can see it's say I'm using only kind
of a sample a sample of the Wikipedia
data in order to and make it run fast
enough and not just wait for stuff but
as you can see the symbol kind of starts
with the letter A and we got what we
wanted they kind of all sorted by their
length and this is the top top hundred
articles by length in my sample so
that's great but it's kind of so
selecting top elements is a very common
pattern and even though you know I
didn't write too much code there is
still some code involved em in here and
we surely we can make it we can do it
better so again just to remind you what
see sorry let's remind you this part is
just
the workouts and this part is
aggregating aggregated over over the
workouts and getting the top element and
this is kind of what I need to write the
logic to get the top element so actually
Twitter the Torah guys was very nice and
they realize that it's also very common
pattern and one thing I didn't mention
that also this reduced function I wrote
is actually not that not that efficient
I'm taking all the ways I'm actually
something then sorting them we're all
time again so for example the first time
I sorry I the second I'm it will heat
reduce the left element will already be
sourced so there is no way in need to
sort them again so one thing i could do
is just merge them that is one thing but
it is actually a better way to do it and
it's using a priority queue so what
twitter did they actually implemented m
priority queue aggregator so let's see
new priority queue to list aggregator
well that's a mouthful yes and not
actually string int and that's the
parameter if you can see it also gets
max so against one hundred and it's om
let's kind of look at it once one second
so this is how priority queue egg you
again looks like it gets max and gets an
implicit ordering so again one way is to
define the simplest edori if you already
soaring ordering something that has an
implicit or and you can just use it and
what one thing to notice here is it
extends some priority queue aggregator
wish and implements only the function
present which present basically thanks
the priority queue in turns it into is
because in most cases were interested to
get back the list of the top elements
but let's look at this the magic that
happens in the priority queue a greater
is just here and actually prairie q a
grenadier extend something else that
called
mono mono an aggregator so mono is
probably the most useful element in kind
of data science it's all we do can
probably explain in ma nodes and if you
get the concept mono it's it's a very
very simple trade it just has zero and
it has a plus and one of the creators of
scouting from Twitter in this guy his
name is Oscar Boykin so he likes to walk
around Twitter and ask people what they
do and explain them how it can be
implemented using model because and at
least up until now he hasn't failed in
his task he could explain everything how
can be so really if there is like one
thing this whole category theory has
this you know this explosive functor
monads which nice but at least in my
practice they're not extremely useful
but more it is extremely useful and
basically what I did here and now with
the prepare and reduce for those of you
who kind of know this is basically a
monoid so instead of where instead of
doing it prepare and prepare and reduce
you can just use a mono if you have one
and you can define one of your own and
just give it a mono it and then the
prepare is basically and that's what
they do here they're using a priority
queue ma node which is again and it can
be very simple a you kind of build it
and do stuff with it but say and well it
has 0 which is the minimum priority
queue and has a plus which is pretty
basic m so the same code will run and
now I don't need all this all this stuff
and actually it will run beautifully
also with this Prairie q2 Lisa
aggregator and all the code will work
the same and instead of just showing you
the same results let me show you some
other cool aggregators a scalding has so
one they have is an average ER is
basically the way
the ways to compute it's also a monolith
one of the best the way the way to
compute an average is to kind of sum and
then count you know you can do it in one
test you can do it in two passes but
there ever jury is actually it's a
streaming mean so it just does one pass
and doesn't hold two values during the
past just hold a single one and you get
an average which is again extremely
useful and in a lot of cases if you need
more than and then just the average tour
is the moments a aggregate which is not
a monolith it's a group but for all
things that can be am honored in here
you can see that worse or the moments
sorry here so again you just fire the
moment a aggregator and you get a couple
of five elements actually it will be six
that gets you the count the mean the
variance the standard deviation the
skewness and kurtosis of your data set
again extremely useful one line just
switch what I wrote there with the my
egra Gator with them with the moments
aggregator and you will you will get all
this data at once at 1am very cool
aggregator hyper log log so for you who
don't know and you can see here there is
also hyper log log mono it hyper log log
M also can be defined as a mono it and
produce those of you who haven't heard
about hyper log log it's a way to a
probability probabilistically to
estimate a cardinality of a data set so
if you have a huge data set and you want
to count the number of unique elements
so actually like soaring them and doing
a unique is a very very expensive
computation and most of the time we
don't we're not interested in the exact
exact number of the unix we have but
with just 12 bits of data and you can
with hyper log log you can get up to a
one percent error in your approximation
which is extremely fast extremely useful
and again it's a mono it just plug it in
em into your aggregate function which
I've written here instead of the Prairie
Clues hyper log log and it works and
lovely so any questions up until now and
then I will increase yes no but a it's
probably a great idea for my next blog
post so maybe I'll write it and share
the link with you I forgot to mention
all the code way will be on github later
we'll also post it post the link in on
the meetup page something else okay well
then let's see what Chris will show us
here I'm probably just going to burn
through some stuff really fast because
we're running a little bit late and I
I'm going to kind of cheat a little bit
because I typed most of this out and
we'll try it in the repple though so i
want to show you real fast one of my
absolutely favorite tools if you use SBT
you probably don't care if you're like
vitaly 9 you're still stuck with maven
for reasons maybe beyond your control
there's this really nice maven scala
console and the fantastic thing about
maven scala console is you have access
to and you can barely see that well let
me basically you have access now to all
the libraries that you depend on in your
code so i can do things like you know
I can import some algebra operators and
then we can wow you've probably still
can't see this sorry let's see is there
I know I know let me actually just make
this guy a little smaller and then I can
shove them up in the page
okay can you guys actually see that no
does it need to go up anymore are we
okay okay um so you can do pretty cool
stuff though actually so like the tally
said algebra kind of came out of
scalding but it was so useful that they
split it apart so you can do kind of
amazing things with with algebra by just
importing operators and it turns out
there's probably a mono I'd for just
about everything you want and it will
get it come into scope automatically
when you import operators so I highly
highly highly recommend that you tried
this out it's really cool you just make
an SVT project or maven project maven
scala console or just run spt console
and you'll have access to all this great
stuff to play with in fact the console
so if I could it sort of digress for
saying the repple is by far my favorite
thing to sort of try stuff out with so
you know you can there's something you
want to learn some new API you want to
play with pop it open in the repple poke
at it you know see what the the edge
cases are see how it works that sort of
stuff and in fact pretty much everything
you're going to see today I did over the
course of playing with the rebel okay so
um so one really really important
question that I need to ask and get out
of the way before I go on because it
affects the rest of my talk do you
pronounce this mahout Orma out does
anybody know the same we have an opinion
like gif gif mahout it's not out really
okay so I'll say my I'm probably gonna
slip to mahout a bunch of times but I'll
try with my out okay so let me just grab
some stuff out of here get it out of the
way so we can whoops so what I'm going
to show you is a quick little example of
essentially wrapping nasty mahou things
with some nice Scala stuff and in fact
the original version i did this and was
in scalding but it's a little harder to
run scalding in the repple at the moment
so i'm just going to do it with
straight-up scala but you can see almost
line by line how it translates the
scalding
so I'm using the same data set that
Batali had so i just made up this nice
little case class to play with so let's
get that guy in there too okay so here's
the kind of where it gets exciting well
it's not that exciting it so basically
I'm just going to read up these articles
there in text format you guys all seen
this before and I'm just going to split
each one out and make an article out of
it so we've got a title in a body
nothing too fancy we haven't done any
really fancy text processing or anything
if I was going to do this for real i
would probably probably go into one of
the NLP tool kits or or use a leucine
one of the tokenizer zor something like
that it would you know you get some nice
stemming and spell checking and all this
fancy stuff that jaan was talking about
before but for our purposes this will
work so let me paste this guy let's make
sure it runs oh dear I probably need a
pace don't know it that's right I
screwed this up tune so live coding what
did I do wrong let's try this again
did I mess up my formatting there we go
okay cool so basically we have a
sequence of articles you know you can
look at our oh and it's got some stuff
it's exactly what Vitaly was showing you
it starts with anarchism of course okay
so no big deal we got some data so what
I want to do is cluster this data and
i'm really lazy I don't want to write my
own clustering implementation mahou 0.8
came out like today yesterday something
like that I don't think they've even
announced it yet it was on their mailing
list but the jars have pushed up in
maven central so in there they they have
this very cool streaming clustering
algorithm so it's a single pass k-means
clustering and it works pretty well
unfortunately somewhere in there they
have this notion of dense vectors and
sparse vectors and some of the stuff
wants to use dense vectors so if I was
going to do this myself probably what I
would do is take all the text and use
something like murmur hash to give it an
int unfortunately the that makes the
space of a you know max in size sparse
vectors and it tries to make them into
dense vectors at some point and then
everything blows up it's pretty horrible
so instead what I did really fast this
is easy to do though and you would
probably do this anyway because instead
of just doing word count you would
probably get tf-idf scores or something
but basically I'm just going to pass
through all the articles real fast so
let's look at this I'm basically going
to pass through the articles I'm going
to grab every single word and stick it
into a set so I basically split this guy
up push all the words make them lower
case that's my genki normalization put
them in this set zip it with an index so
now every words just going to have some
number associated with it now it's a
little nicer for me too this is the the
cheap hash function ok so again nothing
too fancy so let me let's try this guy
out I better use paste again because
nasty things happened last time ok so
you know we can see I'm curious like how
many times the occurs probably a lot
yeah so the curs a lot so if we use some
of the tallies nice stuff we could
probably make some passes take out stop
words to fam you know nice stuff like
that ok not super necessary for now look
ok so let's keep going ok unfortunately
I need some sort of rapper helper
function so what I'd really like to do
is basically take one of my articles I
want to get out this sort of named
vectors so now we're in my out land and
they have these pretty nice random
access sparks vectors so this particular
guy is backed by a really efficient into
double hash map so for us it's
essentially just a you know we say
basically we saw we have a one in you
know position five and we have a two in
position you know 600 million and three
and four us then it's even though this
vector lives in dimension 600 million
and three we only use two things so
sparse vectors very nice don't reinvent
the wheel use my out so basically I'm
just looking up where that guy actually
what is actual index is using this set
quick so this is where is where the sort
of replicas in handy a lot because
basically what i can do is i can go into
the repple and do something like this ok
so now I've got this vector and you know
so you're doing this the first time and
you will have the hell do you set a
vector and you go well Veck what can I
do oh ok I can do a lot of stuff so
you've got set you've got set quick you
try them both turns out set quick as the
one you want ok so let's just get that
whole function in
so this is basically the helper that's
just going to pass through and convert
our articles into mahou vectors because
my how actors is what everything
operates on we better have them okay any
questions so far this is all pretty
standard all right so since i'm not
doing this in scalding i wanted to give
you a flavor of sort of how this
algorithm works so actually what i'm
going to do is I think articles there's
about 4,500 in the group i have so
actually i'm just going to chunk them if
you never seen this function it's pretty
cool grouped so you can call it on
probably like gin traversable or
something basically it groups it into
chunks of 400 and that truncates the
last one so this is essentially me
saying that my date is located in
multiple places my own i operate on each
one separately and then i'm going to
pull all the stuff together so this is
the mappers if you like so let's try
this guy out okay great it actually
hasn't done anything yet it's lazy okay
so that one's not so exciting okay so
here's the exciting part so this
streaming k-means algorithm basically
has two parts to it so in the first part
pretend we have tons and tons of data
say like n points where n is huge so the
first thing that's going to happen is
we're basically going to draw a sketch
of our data and the way we draw the
sketch is we go through all the data or
each mapper the amount of data it has
and it basically grabs the first point
and says hey you're a cluster or a
centroid if you like and now it grabs
the next one and it says are you close
to a centroid I know about if you are
then you are part of that centroid or
maybe you're a new centroid with
probability 1 over the distance that
you're actually away if you are past
some distance threshold that says you're
a new centroid too so it goes through
the idea is that if you have end points
and you have want K clusters in the end
are going to get something like k log n
sort of centroids in your sketch so
these are like sloppy clusters is what
they call them so basically it's a
probabilistic summary of your data so
let me go through this is where the the
nice wrapping comes in so basically you
need this guy he's a cluster so this is
this a you know nasty my help package
okay so so the cluster needs a searcher
and it needs a distance measure and it
needs an approximate size so note that
this size is just sort of a suggestion
and you'll see probably it's it's not a
followed that that carefully so actually
there's a whole lot of these nice
searchers for this size of stuff and for
the sort of giant vectors that we're
dealing with the brute search is
actually going faster than the other
ones but so they have a they have a
locality sensitive hashing they know
there's a bug somewhere in it so it's
really inconsistent they have a couple
projection searches which are pretty
cool but in practice actually the brute
searchers beating it for beating them
for me so which is kind of strange to me
it probably means that I don't have
enough stuff going into it okay and then
there's just a little side effect here
so you can't have a scala talk without
slipping a VAR n so everybody can look
down on you so so basically these
centroids need like some key or
basically so I'm just going to increment
up it doesn't matter at the end of the
day but if they all have the same one it
gets confused so notice then what I'm
doing so I've got my groups remember I
chunk this guy in the groups about size
400 I turned it into iterable because
because par doesn't work on the iterator
so this is where Scala is awesome
basically I'm calling par now so I'm
turning this into a parallel collection
and I'm calling map so what's going to
happen is this guy is actually going to
fork off a whole bunch of processes and
run these all separately right and then
collect them back together and this is
all I had to do it's completely safe
there's no way I can screw it up in this
way and each one of these will get its
own cluster just like
you would have in mem produce the
mappers will do all the stuff they need
at the end it will put them all together
into one big list so let's give it a
shot so then I'm all I'm doing then is
basically just saying okay wrap this guy
in this other thing that it needs called
a centroid this is where it's it's kind
of nice to be able to do all this in
Scala to be honest because there's
always something that you have to wrap
or use or whatever and then just
increment the count so basically add
this guy and then this reindex is just a
kind of a last step that they recommend
doing so let's give it a shot and I'm
gonna before I stick it off where does
this guy live three yeah okay so we can
actually watch it okay we ready so just
to prove that it's actually going to do
something you should be able to see did
it go no it did not go here we go let's
try again area so you can see this guy
if you can see that basically it's I
think I have eight cores it's using just
about all of them you basically get all
this for free it's amazing so if I were
to do this without the parallel that it
probably would have taken about eight
times as long to be honest it's almost
linear in the number of cores you have
here so this is pretty cool so now we've
got all these guys and we could poke at
them if we really want so we could say
like sense dot so I know what to look
for cuz I've done this too many times
today but so and this is where the the
sort of Scala interrupt comes in really
handy too because these these searcher
objects or these cluster objects
essentially have an iterator but if you
want to feed another one in it has to be
iterable which is which becomes a huge
pain so it's actually you can just sort
of do this fancy fancy dance here and do
it like that so we asked for 40 we got
like 200 ish on each of them I'm not
sure why to be honest it's it's brand
new to me to some of this so we'll
probably figure it out in days to come
okay so we've got all our clusters we
want to stick them all together
so we basically just grab all those
points out so again you can't have a
functional top without flat map either
so basically all I'm doing here is just
saying well actually spit out all these
lists into one giant list okay and then
finally there's a so that the end part
of the algorithm then is we did this
sloppy version so now we use a really
fancy expensive version like bulky means
if you can see this guy and then
basically cluster the the sketch using
an iterative process on one reducer so
the idea though is that your data is
only about k log n so it actually fits
no problem into memory unless you're
doing something really really strange so
we can do this guy we get our cluster
and then we need to say something like
de cluster just tell it to cluster them
up and this guy is kind of slow so while
that's while you're watching that any
questions so I'm going to hand it back
off to Vitaly I told you I'd be fast yes
that's a good question i think scala
said it has much much more stuff in it
and I think algebra does maybe a little
easier to use so you know that's not the
best answer maybe but they both have
mono aids I think algebra is kind of
concerned itself mostly with just having
some algebraic data structures like ma
nodes and semi groups and groups and
rings and fields and vector spaces and
then they have this aggregator idea I
don't know what to be on assignment used
Scala said that much so I mean do they
have a lot of stuff predefined in there
as well or do they just is it if they
just have the framework sure ok because
I mean the things that algebra like
Vitaly was saying has some amazing stuff
in it for you already I mean it's
basically got so bloom filters is a mono
dright you just keep slapping into the
filter Kalman sketches am annoyed hyper
log logs and
wyd stochastic gradient descent is a
mano I'd like least common ancestor and
trees as a Mon I'd like Batali said you
could probably go through and ask
anybody what their project is and it's
probably a mano a underlying it and
there's actually a paper i think that
was just came out of Twitter from jimmy
lin who's a professor at maryland which
was basically like everything you want
to do is a mano I'd it the title
something like that like I'm probably
butchering it but it's pretty cool so a
lot of that steps in there for you
already which is pretty nice and it's
the API is super simple like the the
mono API you know you you basically just
have to define + + 0 and for most stuff
you're doing you're probably using a
semigroup so you don't even need the 0
so you just have to define plus and you
know they're not doing any actual
validation you know I can say something
zaman oyed and it doesn't necessarily
have to be and I'll find out you know
six hours down the line in my job but
for most part of you you'll probably
have a model right so the only condition
on the mono it is when you call plus you
better end up back in the mono I'd ok
when I hand it back to Batali this was
your mahout outbreak was I okay so the
necessary thing I what you show is how
we can do similarity between wikipedia
articles for example now find which
articles in Wikipedia are similar and in
order to do this I will use a concept
called cosine similarity which is also a
crease just use it cosine similarity
it's maybe it's best to show so I have
here this sorry this a word were count
data so i just took my took the day said
before and basically wrote it in this
form where i have the article name and
then award and how many times it appears
so we want to some similarity measure
between the wars of each article and
basically it's to take well if
word after shows up in the Clockwork
Orange ten times and it shows up in some
other article also ten times and maybe
those articles are similar em if you
think about it it's kind of not a simple
problem especially if you're doing it on
all of the Wikipedia article so we have
to loop over Wikipedia article and then
the second loop over all the Wikipedia
article and then we have to look for all
the words and we the way cosine
similarity works is that you multiply
the kind of multiply the workings and
then sum sum it up together and I more
formal definition and cosine
similarities in the square root square
root of the sum of a kind of the dot
product so there is a long way and to do
it but since we're short of time let's
show you the very quick way to do it and
and then let's define similarity see
mail or no yeah it's late sorry and so
okay i have my data here i read it and
i'm now using I i'm using the actually
the pipe format because what I'm about
to do next is actually works better with
this format and not the type 1 and like
explain it taking elements in and
multiplying them together is also make
all dot product and that product as a
basic linear algebra operation and
sculling has this great linear algebra
api that we're now going to use so let's
take those words and convert them to
matrix and you see there's we can just
converter to matrix and we need to
define what what is there all what is
the column and what is the value of all
types so what's great about again this
API that
unlike normal matrices you don't have to
be you know convert all of your data to
some indices they don't have to be role
in the column in the evenings it don't
have to be integers they can be strings
yes sorry a way yes I think that's
better right and string string and our
data will be double because well math
works better with doubles and let's take
this stop also we will know how to use
it so now what we have is this very cool
string string double matrix and matrix
has a lot of operations on it and one of
them and kind of one thing that it's
better to do for cousins marry because
if we just multiply we will get just the
the article that are the longest are the
most similar because you know they have
a lot of the same war so the
multiplication will be large so we need
to normalize it and so just let's do
normalize and well Twitter this gives
those two normalization l1 and l2 l1 is
basically having all the sum of the row
equal 1 l 2 is having the square root of
the sum of the squares to be too but
since we're doing a dot product we'd
like some article with itself to be
equal in the similarity to be equal 1 so
now again even the roll just the altar
normalization to write it even scalding
a which is a very very a compact
MapReduce framework it's probably will
take you like five lines or something
like that if you do it in Java god
forbid and it will take you five hundred
and so you know mon line it's extremely
useful and now here come instead of it's
just long to do everything with
everything so let's just take a random
Oracle
which probably it won't be random so M
words we can think it called getro no so
roll yeah what's the role stuff remember
Chris get roll roll what I uh well yeah
that's that's the problem right so r.e.m
like i said slate Matt not get row okay
so let's just get a row and one of the
articles i have there is ali and now in
order to get out is similarity with the
entire set of the rest of wikipedia
which is the matrix we just need to
multiply by matrix but well not exactly
the transpose am and so now that we
basically have a transpose matrix and
again for those of you who are kind of
rusty and linear algebra or is just too
late for you the reason why I should so
the row m is basically it's one by the
amount of features i have which is i
don't know how many words are there in
wikipedia and the tray when I do turn to
suppose then all those wars become the
columns and all the rest of the articles
and will become and we come ansari and i
will get basically in a score a cosine
similarity score for audi with every
other article and the way it looks it
basically again i get a row vector with
string which string will be a name of an
article and a double and that's it so
when we just need to write it now let's
write it TC source data and source sorry
main data oh
data and matrix that text okay and let's
just go sign change it will run it
forget and one thing to remember while
trance again you just take this code and
upload to your loop cluster as is and it
can run on so this matrix API works
extremely smart and so it's also kind of
sparse vector as far as may engage with
library it's all kind of hash this it
distributes smartly between the nodes
and that's all the operation it knows
and ok here it took its 45 seconds to
finish and let's look at the results
what did I have their am called
Patrick's right ok so we see actually in
the clockwork orange is not that similar
to aldi m thankfully in let's see you
know let's just sore it and I think here
is probably one of the best way to soar
it is maybe sort minus k2 minus n no
sorry no no that what it is reversed but
it's a wasn't ski to know what why is in
this world I need the delimiter crap how
do i do the delimiter answered what
Dashti and yes field separator slash t
no that's not it well nevermind em
that's just yeah I know instantly see
you next of kin eyesore it here group
all to pipe
No okay let's just sort oh my god it's
just too late I fell again cosine equals
and typed is it typed pipe that from I
really want to see when the results
right so what did I have there it's a
doubles it string is in doubles right
training double and source main data
that matrix not txt okay what's wrong
with double yes that's wrong double and
now cosine that group whole just going
to show you how to sort global sort by
the second one ah and if you remember I
told you about the same think that a one
of the reason why I can show you it on
the ripple because it's optimizing so
well you can see it here so I do so by
in Reverse which is doesn't actually
sort source it and then reverse it it
just optimized for sorting by the orders
or drink which is smart of them to do
because otherwise it will be well not
very smart yes sorry that values not
right and now we do this sorted pc
source me they swirl it txt
sort
okay hopefully the sort the unix or
wasn't working properly and it wasn't
the right result and the right results
will look better but let's see okay well
that was fast more horrid okay so aldi
we got one which is ok it's a good sign
right and auto racing armored car
assembly and alaga dude okay anyone
knows who august horch is the second
what race em well good thing we have
this happy internet so let's use
wikipedia what was the name agus search
yes I was hurt okay and August search is
a German engineer and something
something what what do you do yes
eventually became Audi which is and that
is from this article so probably just
recognize the world aldi and one thing
one thing I kind of skipped that helped
me to actually achieve this result and I
also did tf-idf on the workouts because
otherwise i would get again a lot of a
lot of articles that has the word a in
the world v in them which is not that
interesting but once you do tf-idf and
again it's installing just couple of
lines you calculate the TF well TF is
the workout right the IDF is just the
document count and you kind of normalize
by it then the word audi and i'm
probably you know germany probably also
the years probably match and stuff like
that and now it becomes and again and
kind of bring me back to the code where
was it that is well besides the row to
normalize that is all the code necessary
that's that thing is i think amazing you
know that just take the role multiply
its well linear algebra for all of this
you folks who haven't listened in their
college during their college classes so
not now you finally know what
it's good for and well that's that's
kind of so any questions yeah it's light
okay thank you guys for coming yes
I</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>