<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MapReduce Basics | Coder Coacher - Coaching Coders</title><meta content="MapReduce Basics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MapReduce Basics</b></h2><h5 class="post__date">2012-11-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eul15SKKuMw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's start by taking ourselves back
to the magical land of the early
two-thousands we are at Google at this
time I was in junior high school I think
and so Google has this thing they have
to do all the time and that is for
literally every page on the Internet
they have to do something like this
which is building inverted index so you
want to know for the word cow if
someone's gonna type that into a search
box every single website on the internet
that has the word count it so as you
know there are a lot of websites there
were fewer websites at the time but
still quite a few and in addition to
this they had tons of general you know
data preparation and analysis tax tasks
that ran over enormous enormous amounts
of data here we go so it's hard to do
that on a single computer even really
powerful fat supercomputers it's hard to
do it on and they are very very
expensive so the basic idea that they
came up with was you get a cluster of
tens hundreds thousands ten thousands of
commodity machines and you get them all
to work together on solving these types
of problems so that is all nice and
dandy but if I want to be doing like
they say web analytics if I want to be
building a search engine I don't want to
have to think about all the oops yeah
I'm using the wrong computer thank you I
don't have to be thinking about all the
complications that arise when you're
trying to build distributed systems you
know anything can fail at any time so
you have to maintain some sort of
consistency it gets very complicated and
very unhappy very quickly so these
people at Google came up with this ID
and this idea was MapReduce so how does
my produce work at a very high level you
as a MapReduce programmer
write your program in terms of two
functions map function and a reduce
function and the framework pretty much
take care takes care of the rest
so what are these functions well let's
get back a little bit later so what are
these functions like what a map function
does is it takes as input some key value
pair and then it just outputs some other
key value pair the reduce function so in
between map and reduce this magic
happens
we're on mission we don't have a
whiteboard doing no it's too small this
uh this magic happens where all the keys
that were outputted from the map
function all the values correspond to
them get combined sorted together and
sent to a reducer so I'm gonna go into
this a little more heavily in the next
slide and so a reducer also gets a a key
with a list of values that correspond to
its input and outputs something else so
let's look at sort of the canonical and
Map Reduce function or the sorry the
canonical MapReduce program is word
count so here we are back as Google or
really anyone and we have some you know
enormous corpus of tons and tons of
documents and we want to do some sort of
frequency analysis on it so for every
word that shows up in those documents
what we would like to be able to do is
output a count of how many times that
word appears in in all the documents so
we start with our map function
and our mutt function is very simple
each each input to the map function is
one of the documents in our corpus and
an output is so we take that document we
read it we parse out tokenize all the
words and then for each word in that
document we are able to output that word
map to the value 1 does that make sense
we're saying there's one of that word in
that document ok so at the end of our at
the end of all our mapping we've called
this map function on every document in
our corpus and we have tons and tons of
these output key value pairs right for
for every single word in every document
that we have we have that map to the
letter 1 so this magic happens in
between map and reduce where for for all
the keys for all the output keys of our
map function the ones that are the same
are grouped together so the word cow cow
cow isn't up to one let's say maybe a
hundred thousand times in our corpus so
we find we go to all those things and a
single reducer it gets cow as a key and
then a hundred thousand ones as the
value so the job of the reducer then is
to just sum all those ones and output
that so the reducer is going to output
cow and then 100 thousand any questions
there okay here's our Java main function
we want to run this word count program
that we just wrote and so this is some
simple Hadoop code to do just that thing
we create a job configuration object
which represents this MapReduce job that
we want to run
we tell it so it said our outputs are
going to be words map to numbers so we
tell it if you can't read it it's a set
output key class to text class that's
Hadoop's
way of saying string and set output
value class to enjoy table which is
Hadoop's a way of saying a integer that
is easy for it to serialize to send like
across the network and stuff like that
then we give it our mapper class in our
reducer class those are the classes that
we defined back here we have this set
input format and set output format
business that's something we'll get to
later in general MapReduce jobs are
reading and writing to this distributed
file system called HDFS and the input
and output formats tell us what formats
the files will be in on HDFS will they
be text files will they be binary files
they have any like special structure
that we have to know about last of all
the path of the files and then we can
run our job and send it off to a cluster
of machines to get it working second
okay so let's let's talk about Hadoop in
general a little more so where does all
this data come from and where does it go
Hadoop is the core components of Hadoop
are these two things HDFS and MapReduce
this is actually changing recently with
yarn so if anyone was curious about that
I can tell you about that but this is
what you need to know for now so HDFS
the Hadoop distributed file system is
this file system that runs on a cluster
and stores all of its data across
different nodes across different
commodity machines it replicates data so
you expect that you know your machines
might die on you
and luckily there will be your data on
other machines and you know it'll heal
itself and replicate that again and so
you use the same machines for HDFS and
MapReduce so that the computation that
you're doing on your data is in the
exact same place as your data and you
know what's great about HDFS is you can
it scales linearly linearly with a
number of machines so if you have like a
4,000 node cluster you can scale you can
ya scale to you know hundreds of
petabytes which is a lot of bytes many
questions about Hadoop HDFS ok so here's
how MapReduce works under the covers so
sort of a high-level diagram in terms of
how that the system works itself there's
this master process called the the job
tracker and the job tracker is what you
tell to run jobs for you it's in charge
of pretty much everything that goes on
it is a single point of failure so if it
dies you will have to manually restart
it to get your MapReduce jobs working
again and that'll run on one machine in
your cluster and on all the other
machines you run these processes called
task trackers and a task trackers job is
basically to hear from the job tracker
that there's some maps or reduces that
it needs to run and then run those so on
everything that we're seeing in this
diagram let's imagine that each of those
little boxes is a is a different
computer there's a task tracker running
on that computer that is running these
processes so when we should we get our
you know enormous corpus as input and
let's imagine for the sake of simplicity
that it's one huge file and each each
line in that file is a document so
MapReduce will and Hadoop will for us it
will split up this file into different
partitions so you know if we have a
hundred thousand lines and 20 mappers
however many mappers lines per member
that is to each one so it reads all the
inputs and sends the relevant data to
each mapper for each line the map
function that we talked about earlier
like that that word count it's called on
that line and that map function produces
some number of output key values and
those were the the ones the words map to
the ones we talked about so then we have
the magic that I have been alluding to
which is the shuffle phase so the maps
are all done the job tracker tells the
task trackers to start a bunch of
reducers on other machines and in
addition to starting them it tells them
okay so these are the mappers that run
and you know they have data that you
might want so a reducer let's say that
top producer is in charge of all the
keys from cow to milk so that reducer
will go out to every mapper that
outputted a cow key and ask ask it
whether it has any ask it for all it's a
cow to one key value mappings um so they
all get from multiple mappers they all
get sent over to that reducer and the
reducer is then able to merge those
together merge you know cow comma 1
comma 1 on that machine with Calculon
Calculon or another another machine then
it supplies the reduced function that
we've defined with cow map to like one a
thousand times or if we did it you're a
better way cow map to you know one or
five or whatever it is the reduced
function runs and then Hadoop will take
care of writing the output of that back
to HDFS wherever you tell it to ok so
the last thing that we didn't cover is
the real reason we need MapReduce in the
first place
and this is more undercover stuff what
happens if a task fails while you're
trying to run your program like one of
your mappers guys so you know in a naive
implementation that your whole job would
be destroyed and you would need to just
start over again
but what MapReduce does nicely for you
is it will rerun that task for you so
you don't have to recompute all the data
just whatever amount of data was
assigned to that particular task needs
to be redone okay so there any questions
about that I'm gonna go into one more
example okay cool
so another canonical MapReduce example
is how is a program to calculate pi so
if you ever heard of the the Monte Carlo
method of calculating pi basically what
you do is you imagine you have this I
know this is more of a rectangle but you
imagine you have this square and a
circle inside of it and you have
millions and millions of darts and you
keep throwing darts at that dartboard so
they'll land uniformly somewhere in the
in the square doing that you can get a
sense for for the area of that circle as
it as it pertains to the square
surrounding it so you know you'll you'll
have let's say out of your 10,000 darts
I'm not sure what pi divided by 4 is but
you'll have some proportion of them that
land inside the circle I mean it's very
it's very easy to calculate when you
pick two random numbers if that is close
to the center of a circle or not and
you'll have some that lie outside the
circle so using you know the basic
formula area equals PI R squared you can
look at the ratio between the area of
the square and the area of the circle
and get a pretty good estimate for pi so
how would we do this how would we
you make this better in mapreduce
anybody be interested in taking a guess
where that MapReduce program would look
like okay I will put no one on the spot
cool
so in this case the way the way we're
doing it this is functionally equivalent
to the way you explained it yours might
actually be even better but we're going
to predefined four for each map or the
number of darts is gonna throw at that
board and then let's say we have like
2,000 mappers and we want to create you
know to find the number of darts as 100
we send we configure a hundred as this
this num to run configuration parameter
and that'll get sent over to our
MapReduce jobs so the mapper looks again
pretty simple what we do is for the
number of darts that we want to throw we
pick a random number we figure out
whether it is inside the circle and if
it is we increment this number and I'm
inside in this case each mapper only
outputs a single a single key value pair
and that's because you know there's no
reason to do more week there's no reason
to do one for each dart we can just sum
all of them to give to the reducer whoa
this is really terrible at this
yeah we're back here with it again so we
just want to go down
let's go I see we touch them again
okay so here is our reducer um and for
this for this PI Monte Carlo simulation
we only have one reducer so every mapper
to achieve this every member outputs the
same key but there may be different
values for the different number of darts
that hit inside so the reducer then gets
all the different values the number of
darts that landed inside the circle from
from the mapper it sums all of those and
divides by the total number of darts
that were thrown and then it can easily
estimate pi as four times that ratio
that's and then it sends a single output
which is its estimate for pi out to be
written to HDFS does anybody have any
questions about that so this is this is
more of a best practice than how how my
produce works this is the distributed
cache so it's very common that in
addition to that whatever MapReduce
computation that you want to be doing
you'll have some auxilary files that
need to be used like for example if you
are actually doing an inverted Internet
index of the internet you probably want
a list of basic words like and that you
don't you aren't actually going to
include in your index this is used for
for joins which we can talk about later
so MapReduce allows you in your job
setup code to add these these files as a
cache to the cache and then the
framework will here I'm gonna zoom in
will take care of distributing those
files to computers that your mappers are
on so they're able to use them when you
need them this is also how it sends out
your mapper jar to all the files and you
can send additional jars that you might
need to use any questions about that
cool
so so what's so great about MapReduce
what's so great is that it scales
linearly with either the size of the
data or the complexity of your analysis
if you use the right algorithms so it's
great for both doing data parallel type
algorithms like the word count that we
talked about where you have tons and
tons of data let's say you know you have
logs for your website
you know terabytes and terabytes and you
want to look for you know see if one a
certain word appears on them Hadoop is
great for that it's also great for you
know these types of analytic tasks like
the Monte Carlo simulation to calculate
pi that we talked about as well as all
sorts of machine learning and so yeah
and so Hadoop is great for both of these
applications here are some other things
that it's good for grubbing through
enormous datasets analyzing your logs
calculating statistics for enormous data
sets running very large simulations
doing extract transform load types of
things where you want to format data for
input and to a database or take it out
of a database put it into another one
great for machine learning great
obviously for building inverted indexes
for sorting and much much more so what
is MapReduce not good for what it's
pretty terrible at is doing things at
real low latency so if you have
something that needs to be run
immediately there's a fair amount of
overhead that goes in setting up a
MapReduce job so it's really meant for
batch processing although there are
frameworks that are trying to address
this a little bit more it's terrible for
highly sequential algorithms if there's
no good way to run it in parallel my
produce isn't going to help you at all
and then a more practical thing that
you sort of think it would be good for
it but it's not great for is joints so
if you're trying to join some set of
data with some other set of data there's
no really elegant way of translating
that to the MapReduce model if you're
trying to join a large set of data
against a small set of data you can send
the small set of data in the distributed
cache out to all your nodes and that
works very well but if you're doing it
otherwise you have to do it it's called
a reduce side join and that uses a
little more IO and everything then you
really want it to be I think that is all
that I have all right thanks a lot
everybody for coming really appreciate
it and yeah please talk to me afterwards</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>