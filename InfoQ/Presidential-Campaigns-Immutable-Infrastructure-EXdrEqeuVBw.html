<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Presidential Campaigns &amp; Immutable Infrastructure | Coder Coacher - Coaching Coders</title><meta content="Presidential Campaigns &amp; Immutable Infrastructure - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Presidential Campaigns &amp; Immutable Infrastructure</b></h2><h5 class="post__date">2017-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EXdrEqeuVBw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning let's start with a quick
story so that's that's kind of weird
right see what happened before midnight
tonight and join the team you can be not
just hashtag I'm with her in spirit but
hashtag I'm with her literally as in you
could win a trip to Philadelphia to be
here on Thursday night when Hillary
accepts the nomination so yeah my team
and I are sitting around the first night
of the Democratic National Convention
this was supposed to be a quiet night we
had scaled out all the services elastic
load balancers were nice and toasty I
was eating a burrito from Chipotle when
you're on a campaign you don't really
know what to expect you ask your friend
leo from a prior campaign what type of
traffic they saw didn't write anything
down you get a lot of graphs without x
and y-axes so you ask the campaign's
comms team how many people do you think
are gonna watch prime time during the
DNC and they tell you million million
people seems a little low so you ask
another team and they tell you 10
million people so you basically know
there's between 1 &amp;amp; 1 million and 10
million people are gonna watch this
thing and you still don't know how many
people are gonna come to the website but
here's here's the good part you did your
homework you had your product teams for
months ask everyone that they could find
which speakers were going to plug the
website right so you can be prepared
senator Al Franken did not give us a
heads up the story of the Franken bump
is the story of Hillary for America
unexpected amounts of traffic and
unexpected times in unexpected ways by
unexpected people
I'm Michael Fisher I am currently a
software engineer at the Democratic
National Committee but before that I led
the site reliability engineering team at
on Hillary Clinton's presidential
campaign we ran in a mutable
infrastructure that grew to a hundred
services in production and 80 software
engineers designers product managers
sres
and other tech folks I'm going to talk
to you about the easy wins and the
challenges of a mutable infrastructure
mostly though I'm here to tell you about
how immutable infrastructure as our
architecture helped us defeat the chaos
that comes on a presidential campaign
but first how did we get here we often
called Hillary for America the world's
only nationalized nationally televised
hackathon there are a few other
organizations where everyone from
members of Congress to Ranji comedians
who sometimes are also US senators to
Lady Gaga plug your site and the
products you work on a hackathon there
fun right it's a great recruiting
technique you got a lot of interesting
developers it's also a terrible way to
run a tech operation on a campaign it
can't be a hackathon when you have to be
ready for senator Al Franken the plugger
site unexpectedly it can't be a
hackathon when you have to scale from
five engineers to 50 engineers deploying
sometimes hundreds of times a day so why
did I join why am I even here I used to
work at blue state digital which was a
tech vendor who was a primary tech
vendor for the Oh eight and twelve Obama
campaigns
I actually worked there in 2012 and got
the witness firsthand the impact that
cutting-edge tech can have on a campaign
it really does a lot so when I learned
that I'd only have to take the subway to
Brooklyn to actually work on a campaign
last cycle I jumped at the chance and I
live in Manhattan so that was still a
pain there's two ways to measure growth
on a campaign people services products
you're building the tech you're building
so this is the tech team back in summer
2015 that's six months before even the
first contest in Iowa people were not
even paying attention at that point so
pretty small team and then we grew
you win some primaries they let you hire
people that's the deal
so this is the New York primary this is
spring of 2016 so a little bit bigger we
like filled out a conference room and
then you have summer 2016 you get even
bigger and I think there were flags
because it was July 4th we didn't always
have just a ton of American flags around
this was I think around July 4th so it
was right after we got the nomination
and right before the Democratic National
Convention after that we were too big to
really sit in one frame so I don't have
any photos so the scale just kind of
explodes right so I joined a Chafee back
in June 2015 just a few months after
Secretary Clinton announced her
candidacy at the time there were just a
few Engineers you saw them working at a
Chafee and then there was an external
engineering team called the groundwork
who are the groundwork no it's not the
Illuminati true to their name the ground
were created a lot of what H if they
built its tech on a few of the folks
there came from Netflix and natural they
brought with them a lot of the ideas
around immutable infrastructure that
we're going to talk about today that
informed the way that we did things on
the campaign they provided us with the
beginnings of our production environment
our first web page
our first Hillary Clinton calm they gave
us some of our core infrastructure
including but not limited to our API
gateway the way that we collected money
if your contribution forms the way we
collected signups a lot of our
engagement tools speaking of
infrastructure what did this all look
like the edge it's a cloud so the edge
was a lot of what my team managed we
used fastly as our CDN it was all hosted
varnish we actually ran more than 3700
lines of varnish in production which is
a lot probably too much fast Lee is
where we stored actually the majority of
our business logic so if you were coming
to any of our web properties you know I
think we peaked at like 40 or 50 domains
at one point you were coming through
fastly and you were hitting our 3700
lines of varnish it handled all of our
failover
a regional failover or caching and our
edge security
from their static front ends the vast
majority of our apps nearly 99% were
static front ends react JavaScript based
front ends deployed to s3 and we're
going to dig into the details on how
that worked and why it was amazing
after the edge right you either went to
your front ends you go to your back ends
so we went to our API gateway and this
is what I was talking about before
there's multiple API gateways because we
scaled it out quite a bit this was built
by the groundwork it was a Python app
that acted as a reverse proxy to all of
our micro services so everything at HSA
was for the most part a micro service
right built behind our API gateway
running an AWS from their service
discovery we loved console everything
talked through console we ran to console
clusters one in our development
environment one in our production
environment console did a bunch of stuff
for us it was service discovery let our
machines know about each other it led
our our services know that they were
healthy did health checks it was also
our secrets and key value store and that
was really big and that's gonna come
into play later and how we got around
some challenges in immutability we also
had a central log service data dog we
had data dog on everything which gets
very expensive the end of the campaign
when you're paying for hosts so
everything had data dog AWS
infrastructure we ran the agent on every
single instance we were running in
lambdas we use the API every single
thing had dated dog instrumentation just
like with people there were too many
services to fit on one screen this
screen goes on and on and on but I can't
take continuous screenshots at this
point in the talk you might be asking
yourself why a presidential campaign
would even have a tech team and that's a
great question because for most of
history they have it so what kind of
stuff did we build and why why did they
build it why am I even here telling you
about it so just some examples
donation platform we took credit cards
ACH payments and more online we had a
way for you to donate to save your
credit card and donate you could even
text us to donate right we would reach
out to you via text and you would just
say yeah I want to give Hilary money and
we charge a card signup tools for
example if you wanted to go to
Philadelphia right you come to Hillary
Clinton comm like one hundred and forty
eight thousand per minute did and you'd
sign up right voter protection tools you
were in line your polling place is
closing oh my god what do I do
well we had a tool for that tools to
find your polling place register to vote
call tool and that's MSN so that
supporters could talk to other
supporters events applications the list
goes on and on how far does it go quite
far so just to give a sense of scale and
perspective sixty elections between
April 12 2015
and November 8th 2016 that's 59
Democratic primaries and caucuses and of
course the general election ad
technology staff everyone from software
develop front-end and back-end
full-stack engineers Sr ease product
managers product designers CTOs WDC TOS
100 immutable backends you know about a
hundred services running in production
by the end of this thing a hundred and
fifty serverless front ends so those
front ends that we talked about on s3 we
had a hundred and fifty of them five
hundred and seventy seven days twenty
five hundred max QPS what we saw earlier
franken bomb 82 million seven hundred
and fifty nine thousand six hundred and
seventy six votes cast for Secretary
Clinton from day one in Iowa to November
eighth so that's that's chaos right how
did we do it
why why did we do it immutable to the
rescue so at a chip day we thought of
immutable as a handshake between
developers and sres immutable it's just
another abstraction layer right I don't
have to care about every detail piece of
your service all I have to do is
understand because of the architecture
we have in place I understand how it
works in our world right and that's the
only way you can
survive when you have a team of four
sres and a hundred production services
right and developers shipping code
hundreds of times a day that said when
you have developers spanning from junior
all the way to senior this can sometimes
be a new concept right not everyone
comes from immutable this is you know
this started back in 2015 so you know
you got to you got to go back in time
and remember people were coming from all
different backgrounds so a lot of people
would ask well like why this is a Packer
build right why would I why would I do
this why would I wait for this when I
can just do this I can just add this age
into the really important production
instance and restart nginx because I
just push my fix right I just I put it
on my two boxes and I'm done
and that's that's a good question right
this was not an easy conversation to
have when you have and you're trying to
get people on board right they just want
to ship at the end of the day though
it's about trade-offs right developers
have constraints right like they their
they need to move quickly they don't
want to use build tools that we
prescribe you know all these things and
as an Operations person and as an SRE I
also have prerogatives right I want to
make sure that the services are
resilient I want to make sure that I
don't need to understand why you're
running nginx I just need to understand
that if the service comes back up and de
NEX starts up and I'm good right I don't
I don't want to know about this and I
mostly want to know how long is it going
to take to fix it and who's gonna yell
at me right and can I just every night
be able to go to sleep and say if
something falls over it's gonna come
back up right and that's one of the best
things about an immutable infrastructure
like the one we had so what's what's
easy build tooling there's lots of
amazing tools out there we're gonna talk
about what tools we use deployment also
lots of amazing tools out there
resiliency that's built in right if you
have micro services orchestrated the way
we did behind an api gateway you are
resilient to a lot of things like
franken bomb but it still takes people
convincing that it's the right way to go
for everything it's hard to make it work
for everything
and it's hard to get everyone on board
with a methodology like this where you
say every single thing we ship is gonna
have a serverless front end and an
immutable back end that's tough so as
far as build tools we used a couple
different things we ran our CI
environment and Travis we use that for
automated tests at deploying to s3 for
front ends we used Packer to pack our
a.m. eyes right so we would first run
through Travis then we would go over to
Packer produce an ami and ship that off
to AWS continuum was a tool built by the
ground work if folks are familiar with
spinnaker it's very much like that it
handled all of our aSG's attaching
detaching load balancers making sure
that health checks for copacetic and
launch configurations it really abstract
at that layer out for developers so they
never had to go into the AWS console
which was huge and then we used ansible
ansible did the final moving around
files and making sure that everything is
good restarting services before things
came up in our development in our
production environment so let's let's
take a step back and let's focus on each
part of the infrastructure some of our
some of the learnings and hopefully
we've been through some takeaways so
starting in on our edge this everyone is
coming for you all the time right so
it's really scary
as it turns out when you raise your hand
and say you want to be President that
attracts attention some of it is
exceedingly negative so we used our edge
to protect our web applications right
it's one of the reasons we went with
serverless front-ends is because we knew
as three can take a pounding but how do
you protect those backends right that's
that's the stuff you really gotta worry
about and who are you defending against
script kiddies and and DDoS is is mostly
seeing what you're defending again
mostly nuisances right you get a lot of
nuisances you get a lot of people coming
by at you know four in the morning and
just pounding you with WordPress
pingback attacks because they think it's
fun and that'll take services out right
so how did we how did we deal with that
how do we defend about against that and
how did we
sleep every night working seven days a
week for a year now confusion as a
defense strategy so this is some some
DCL from our varnish config as it was on
on November 8th and actually it's still
up because Hillary Clinton comm is still
up so you know one of the fun things you
can do is you can let people think that
they actually did take out your services
so Ron you're running for president in
the United States you don't so much care
about people coming from maybe Brazil at
3:00 in the morning right it's not
really a priority unfortunately that's
where perhaps a lot of bad actors are
coming from so you know what we did was
we we really leveraged fast Lee's API is
to be able to quickly and easily flip
switches to for example just turn off
everybody outside of the United States
and what did you get if you were coming
by and you were on a blacklist and you
were hitting us with bad traffic at 3:00
in the morning
you got a 503 so it looks like you won
it looks like you took out our API
gateway but you didn't everything is
still working for everyone we care about
and you think you've succeeded and then
you leave right because your script is
all of a sudden getting 503 s and you're
done right so one other thing you you
run into is maybe you built a donations
API right like I talked about the
groundwork built a donations API
and maybe it works really really well so
well in fact that people use it for card
testing so for folks who don't know what
card testing is you know I'm a I'm a
person and I've acquired a set of credit
cards unknown origin and I want to know
whether they're good in that good or bad
right and so I need an easy quick way to
script against something to find out
whether I'm gonna be able to submit a
one dollar charge or or I'm gonna get a
decline rate people love to use api's
like this for that and so you know we
didn't want to be those people right
that cost money it makes it look like
you're getting donations when you're not
you're getting a ton of declines it's
very confusing it looks
you're getting donations from all over
the world when you can't be so what we
did was we basically at our edge right
we decided to just mess with people
right you just confuse them you just
randomly return yeah I charge the card
or that card is bad and all of a sudden
you were no longer of reliable source
for information about whether the credit
cards work and you can do all that at
the edge right you can do all that
server lists in varnish and doesn't
touch any of our infrastructure doesn't
ever hit our API gateway doesn't ever
hit any donation processing and it's
fantastic sometimes though you do have
to just ban people so the the wall was
the name of our table that we stored all
the the bad IP addresses in right we
would you know if you if you were a
troublemaker right we would throw your
IP address possibly your your entire IP
space into this blacklist and you would
get an eight and error 819 because
varnish is fantastic and you can just
make up HTTP error code it's like they
don't actually exist of reality and and
also if you were a script Kitty with a
wordpress pingback army you would get a
forbidden well let's see what that
actually looks like it looks like that
you've actually got a forbidden taco so
that actually works and it works exactly
as well as you would hope it would so
you know if you are trying to come to
Hillary Clinton calm and you're trying
to cause problems there are easy ways to
keep you from doing that right and you
can do that all serverless on someone
else's infrastructure scaled out
available and it never touches any of
your stuff and that was the primary way
that we dealt with nonsense how did we
keep track of all this though right like
you have a hundred services you've got
25 40 different domains how do you how
do you keep a handle on all that up spot
so we built a lot of bought helpers
you know robots become into our slack
and tell us yeah
here's here's your to x-axis here's your
for x-axis here's your 5x X is whose
here are the URLs here's the people we
didn't I didn't want to shame any IP
addresses they get recycled so those are
grayed out but those are the people that
are causing problems everyone here seems
to be mostly behaving but often we would
see you know one or two IP addresses
with counts of like millions right
they're just causing millions of 5x X's
and then we would ban them at the edge
and they would get their their taco so
you know that was how we kept track of
everything and again because services
were shipping all the time
sometimes we wouldn't even know that a
new service has gone out maybe it
attracted attention maybe it attracted
negative attention so all of a sudden
you know we'd say okay I'll spot 5 X X
and I'll thought would say like hey you
know it's slash you know new service and
we'd be like we don't even know what new
services but someone is abusing it and
they're gone right and that just takes
care of the problem so turning to our
infrastructure let's talk about
turbulence let's talk about static front
ends this is where we started to
convince folks that it was this was the
way to go we had a service called onesie
so when you're on a campaign you have to
give everything a cool name otherwise no
one will use it that's just a tip for
anyone in the future so onesie was our
way of making s3 a robust serverless
origin all right so we have 150 static
front ends those are your applications
like those are what everyone is
interacting with on Hillary Clinton comm
or any of our other domains so you have
to stay organized you have to be able to
move really really quickly and that
gives us onesie so once he was one part
AWS one part varnish and it gave us a
way to route nearly all of our traffic
so nearly all of our traffic went
through all of our front-end traffic
went through onesie so what was one see
how did it work
what can we learn from this so varnish
is amazing because you can just easily
do things like this right you can say
Hillary Clinton comm slash calls
actually routes to a single bucket with
a sub directory called calls right and
you can just do that
all day long and the advantages are
numerous right so the advantage of using
one bucket for all of your front ends is
just organization right
I am keys or standardized no one has to
worry about bucket names this all just
works once he gave us caching consistent
caching very short TTLs on our front end
so that you know we were able to make
sure that you know everyone was on the
same page about how their JavaScript
would cash or where they're in or their
HTML would cache paths and query
parameters varnish super easy to just
munch paths and strip out query
parameters making sure you're not
passing that stuff down to s3 and that's
the kind of stuff we did in 1z and also
I think most importantly let us do
regional failover really easily so you
know most of the world right that that a
lot of people care about lives in US
East one and that's where we lived
also in AWS but you know when you're on
a campaign people start to worry about
everything right so you know you start
having these conversations like what
happens if a hurricane comes two days
before the election and wipes out
Northern Virginia and the federal
government and that's a real
conversation that you will have and
you'll say okay I guess we'll move
things to us West too then also just in
case and so this let us standardize all
of our failover to us West two and it
let us do all of our regional fail or
all of our failover health checking at
the edge and instantly be able to switch
between buckets in many cases toward the
end we were actually running out of both
regions and just you know randomly you
would get us West to or you'd get us
East one and it didn't matter because of
the way that we did our deploys so our
deploys for onesie or surrealist front
ends very very simple the edge again
fastly we've been over that s3 was where
we stored the files
Travis is how we got the files there and
of course everything started and get the
biggest thing about onesie that I want
to make sure folks you're taking away is
static front-ends are only getting more
and more popular right and giving people
a way to quickly push to update their
application again hundreds and hundreds
of times a day often without having to
worry about any other piece of the
infrastructure is super key
we gave I am keys to our we encrypted I
am keys and Travis Travis did all this
automatically for us at the end would
just give right permissions for the
specific directory in s3
that means as a developer I never had to
worry about taking out Hillary Clinton
comm or taking out Hillary Clinton calm
flesh donated I only had to worry about
my little world and as an SRE I didn't
have to worry that anyone else was going
to do anything weird so that's what
onesie let us do and again a ws+ varnish
that's all that was so we talked about
script kiddies we talked about DDoS we
dealt with the old and various traffic
we've talked about front-end
architecture front-end developers can
ship fast right they're happy they just
have to deal with Travis it's great
let's assume then that the remaining
traffic are legitimate folks making API
calls and they really really want to go
to Philadelphia so again here's that
infrastructure and this is where the API
gateway and the micro services come in
and just a reminder about what Frank and
bump looked like right that's the
traffic that we were contending with on
that night so what happened what
happened when that happened it wasn't
good so you know our static front ends
were fine right they they were deployed
to s3 everything was cached everything
looked good to everybody right and
that's that's the number one takeaway
from this from this architecture is that
if I'm a user coming to Hillary Clinton
comm slash sign up and I really really
want to go to Philadelphia
because I'll Franken told me I have to
go you're gonna nothing looks wrong
right everything looks great so what
happened though on the back ends right
like what happened to the API
Gateway what happened to the
microservices so you know depay
despite pre-warming elastic load
balancers that's a pretty sudden
onslaught of traffic and you're
basically going from zero to insane and
then right back to zero because everyone
left right away so like I said earlier
part of having an SRE team of four means
that you have to trust your developers
it means that I don't know about all the
applications that are making API calls
from hillary clinton calm and quite
frankly i don't want to know right that
is the whole contract of this
architecture is you ship i monitor and
we all understand how this works in our
world but i'm not going to tell you
don't put that api call there right i'm
not gonna tell you don't ship that
service you're just gonna do it we
weren't the gatekeepers to production
right so here's what worked that night
the api gateway kept passing traffic to
microservices donations and signups were
built to queue right and that was the
biggest thing is when designing these
micro services that have api's that have
to withstand that and more you have to
separate out the work you're doing right
and so what the model we use when
designing these applications almost
universally was api worker right so the
signup api for example would very
quickly all it was responsible for was
taking that JSON payload from the
front-end and shoving it into an sqs
queue that's it you can scale that right
you can go horizontal on that you can
run that's really fast it's really
efficient no database goes in the queue
right maybe you start to lose instances
because you can't handle the api but
you're not gonna lose them all right so
you're at least gonna have some taking
api requests right i'm fulfilling that
contract at the front end and then you
just have your workers just working
through that backlog right for hours
right just processing the sign up so
those folks and go to Philadelphia and
we did that with donations that was
signups we did that with events we did
that with with almost all of our stuff
so the queues worked the API gateways
scaled the front end stayed up
one of the best things about the
immutable infrastructure that we had and
what I think is one of the best reasons
to use this in the future is that as an
SRE there's not a whole lot you have to
do when this happens as a developer
there's not a whole lot you have to do
you have to understand what's happening
you have to understand that you know you
don't have enough instances right and
you've got to let things Auto scale in
the way that you've prescribed right but
most of what you're doing is sitting
back and you're watching you're
monitoring and you're trusting the
system that you built to do its job
right you know that this is not gonna
work out well for every single service
but you kind of don't care because I
don't have to run around and start
adding instances I don't have to start
figuring out what's going on I know
what's going on I'm looking at my
monitoring I'm saying yeah that's crazy
and the infrastructure is going to scale
and it's going to fix itself in
instances are gonna come back up and
they're just gonna keep working through
that load or they're gonna just start
they're gonna keep taking API calls
right and that's one of the most
beautiful things about that night is
that I we didn't have to we were all
talking right we were all monitoring but
we weren't trying to troubleshoot
because the infrastructure took care of
the problem there's one thing though
about that infrastructure diagram so
here it is again one week before the
Democratic National Convention we
launched a new website exactly what
you'd want to do before you get the most
traffic you've ever seen in your entire
life we went from a static service as
three based site like the beautiful
thing I've described to you because we
wanted to make life really difficult for
ourselves right we went to a no js'
dynamic ec2 backed website of course
with a WordPress CMS in the back because
why wouldn't you do that why would I why
would I why would we have done that
given everything I just talked to you
about for the last 30 minutes why would
we make that decision raise your hand if
you've ever been asked to build a CMS or
like worked with a CMS content
management systems in general yeah it's
right it's like everyone
right I worked with four right like
three alone at H if a right so it's it's
inevitable that you will have
stakeholders who will ask you to build
you a CMS and they want WordPress and
engineering will say that's super cool
we love WordPress but we will actually
want to try a new CMS it's totally
serverless it just uses markdown you put
and get and then it builds in a lambda
and they go cool we'd like WordPress and
then three months later you deliver
WordPress right there's a lot of reasons
this happens WordPress is easy to use
people are familiar with it there's
plugins
I hate plugins right the plugins are the
reason that that developers and office
folks hate WordPress right they also
hate it because it's popular right we
had to ban people coming through with
WordPress being back attacks because
it's so popular that's what everyone's
looking for they're always looking for
WordPress so we decided to do things a
little bit differently we decided to
give people WordPress the WordPress that
they are comfortable with right the
wonderful CMS and instead of using
WordPress as a actual website we used
through JSON API and built a nodejs
server so on the night of Franken load
Franken bump rather Hillary Clinton comm
was this nodejs server how did it
perform what was it like so we call it
secretary of state lists because again
you have to have fun names otherwise no
one will use your services this is what
it looked like actually if I'm being
honest it looked a little bit more like
this because we had to run a lot of
nodejs servers in order to handle the
load okay so what was the biggest
problem to solve when we were building
this service right we wanted again to
build a service that let people put
things into WordPress we want
a persistent data store that was
scalable and not our problem
right AWS is problem hence Redis we
wanted a nodejs server javascript was
the choice because we had surprised a
lot of front-end developers writing
those beautiful react front ends right
and they just wanted to be able to write
back-end code so it made sense and
notice cool right and we had our edge so
what was the biggest problem that we had
to solve it was what is the state of
your state there's too many things
moving along here right and you know I
don't think that these types of
applications are not naturally lend
themselves super well to immutable for a
couple reasons you know the first is
CMS's or old data right and that's easy
you put that in a database but then
you've gotta like produce a website and
when you produce a website you've got
these crazy things like templates and
you have other data from the front end
and you have to combine all that to
create this website instantly across you
know 19 or more node servers and you
have to do all that making sure that
everything is consistent and that if any
service or rather if any instance falls
over that it comes right back up with
the right version of the data and the
right version the templates right that's
that was the problem we had to solve so
each piece of this was pretty
straightforward
we had my sequel as a database for
WordPress that's not a controversial
choice I think by any means scaling it
wasn't an issue we ran Aurora in AWS it
had just come out pretty recently quite
frankly we didn't know how it worked but
it seemed to replace my sequel pretty
well toward the end you know we learned
about its read-only endpoint support and
move toward a lot of that stuff which
was really awesome we're able to
separate out the rights from the reads
which was cool WordPress is a beast
right developers make a decision to use
it because custom fields right everyone
wants these beautiful custom fields this
is great because you're using because
you can give them that right
we used to plugin that stored all the
custom field data instead of in separate
columns it's stored everything is one
big JSON blob in a single column which
sounds great if you're pulling it out as
JSON it also produces some insane my
sequel queries like I've never seen
before so
pulling data out of out of WordPress was
essentially a non-starter without
caching it right so what we ended up
doing was instead of pulling it out of
WordPress we ended up pushing it into
Redis so every time someone published a
page on the site it would go into Redis
and Redis is great because if you're a
no js' server and you are simply taking
templates and you're combining data with
templates and you're getting a page
request and it's /foo slash bar I go to
uh I go to Redis and I say give me the
value for foo bar and I get the JSON
right back right so it's the perfect key
value store for something like this from
there again console and I'm going to
talk about how we use console to solve
our biggest problem in a second and we
stored all of our templeton s3 so that
meant that that if you were a developer
you could make changes to your templates
and push them up to s3 and that would be
it so how did we solve this problem of
the state of the state
so we abused s3 console and ELB health
checks to figure out what everything
looked like at all times right this is
like I think a very big immutable hack
so the biggest fear in the design of the
system from from my team was how does it
hold up to Al Franken right how is it
going to stand up to to an onslaught of
traffic that you don't know about and
you have to kind of assume that it won't
alright that's that was my team's mantra
was just assume that your services
aren't going to and make sure that
you're building them in a way that when
they come back as they do everything is
in sync and all the state is consistent
across your stateless machines right so
that that's one of my big concerns right
second concern from developers is how am
I gonna get templates changes out there
right temporal changes are huge on a
website right digital team wants a
button move from here to here they want
this text change over here this happens
you know on a busy day tens maybe 50
times a day right so how do you do that
how do you get those templates to s3 how
do you get them onto the machines
and make sure everything is consistent
and remember hacking an ami takes time
right so you can't do that so what we
did was we leveraged s3 to store all of
our templates right so you would make a
template change you would push it up to
s3 through Travis and we get tagged with
a version when you published a page or
rabbit rather when you were ready to
publish new templates go into WordPress
and you'd say this is the version of
templates I'd want I'm ready to go and
I'd click OK that would write the
template version to console console was
basically our our highly available
cluster that always told us what the
right thing was it was the canonical
source basically for our state
so console then new ok these are the
right templates that I have how did we
abuse you he'll be health checks so this
is this is actually the code that did it
a bit of an anti-pattern right you'll be
health checks probably should be like
real quick if I service up what we did
instead was we had the LEL be health
check be responsible for basically
controlling all this the state of our
world right we said ok there's two
things you got to be to be in the ELB if
you're an instance serving this website
you got to be you got to be up to guy to
be taking traffic right you got to be
responding to like just slash health and
giving it 200 ok but that's not good
enough
you also need to have the right template
versions so you'll be health check would
kick off every five seconds it would
make sure that the version and console
matched the version of the templates on
the box and if it didn't because you
know cron broke or something which is a
terrible that'd be horrible we should
all just go home if that happens but
sometimes things would happen right they
don't get pulled down there's a some
sort of error in the templates so what
would the EOB health check do it would
just pull that instance from rotation
and take care of the problem so we
basically just co-opted EOB health
checks into managing the state of all of
these nodes servers and and it worked
that was the thing that that was stood
traffic on on
to the DNC perfectly we attained 97 to
90 percent cash hit rate on that service
through fastly just by being really
smart about the way we were updating
things and when it did get traffic it
handled 200 500 QPS no problem always
serving the right templates and when it
wasn't that instance would just come
down and we'd start over and that was
the beautiful thing about about fitting
this into immutable so some takeaways
immutable infrastructure was key to our
technical success I really don't think
that we could have done this any other
way
there's just quite frankly no other way
to not have a giant ops team and a giant
don't have a giant off steam and a dev
team of the size that we had and wanted
to grow to and not have that that
contract in place there's no way I could
know what was going on in every single
service and so this was this was perfect
we're able to move quickly but we were
resilient against failure most of the
time right it takes more effort to apply
immutable to everything you're doing
like you know applying immutable to
something like what you just saw is not
simple right you kind of have to work
you've got these workarounds but it's
totally worth it and ultimately
developers like the handshake between
sre and dev when it comes to immutable
because I'm not the gateway to
production I'm not telling you not to
ship it I'm just saying that thing
better be stateless it better fit behind
that API gateway and if you want a
front-end oh it better be server lists
and then I never have to hear about it
right except for when something goes
wrong but we know how to fix it some
other takeaways on a presidential
campaign innovation is a necessity and
there aren't any hard and fast rules in
tech there's lots of other rules but you
can build really cool stuff it's
difficult to imagine where
infrastructure tech be in four years
could be could be containers could be
something we don't even know but the
next campaign will be leveraging the
most exciting stuff out there right so
this is you know already over two years
ago it's it's I can't even imagine what
people will be thinking about in in
three years and things get going and
what they'll want to use it's very
exciting
and I think the biggest takeaway likely
at least for me is that you can you can
really build cool and work in
public service right so there's nothing
stopping everyone from you know you go
to a presidential campaign you go work
in tech you volunteer on a congressional
campaign whatever it is you can do cool
 you can build services at scale
it's not old legacy stuff it's it's
really cool stuff so yeah I think that's
that's it thank you Michael
I think we have some time for some
questions we do want to keep things away
from the political sphere and towards
the technical sphere so if you have any
questions feel free to raise your hand
you said you're using ansible yeah and
Packer did you run ansible from packer
or is use ansible
somewhere else three times actually so
that's a really good question
so ansible ran in packer so the the
primary way that we got code into the am
eyes was simply by creating a debian
package but you know you could tar it up
whatever we just had a package that
Travis would produce it's the bundle of
all the files and we would send that
over to Packer Packer would put that on
on packet the nice thing about doing
this is actually a groundwork convention
the nice thing about doing this through
the Debian packages is you get nice
dependency management rate tells it
tells when to exactly what to install so
that happens and you get all your files
in place and then you need to do a
little bit more so we would run our
ansible playbook there and then I would
actually run a second time once the ami
was imaged onto the machine we were
running ansible again in production to
make sure we got our production values
so often things would change right
between dev and prod so we'd pull from
console again get our secrets and REE
template yep anybody else
it sounds like one of the big reasons
your handle that huge spike was your
kind of message queue kind of situation
we yeah you know built these things up
in the back back-end was that more
important to handling that spike than
the immutable architecture the immutable
backends or how did that how the amitabh
apart play into that so I think the the
immutable part of that right is that our
services were small and they and we
could basically guarantee that there was
no state on them and that they were
always going to be the same right so I
think obviously you know a core tenant
here in just any resilient design is
getting everything that changes away
from your machines right what I think
helped us survive that night right like
that traffic spike and all the traffic
spikes is really just the promise of of
the machines always looking exactly the
same right and coming up without any
inconsistencies and and really just
nothing to worry about and that it could
just keep churning on whatever it was
doing previously so I think it's a
combination of the mutable I think it's
a combination of you know having service
front ends and I think it's a
combination of just architecting your
applications so that the work that you
need to do is separate from the work
that's happening on the machines and
that you can always know in any of your
services that are going to be subjected
to something like this that they can all
fall out one can just kind of handle
things while the other ones are coming
back and they'll just keep churning
right that's that's the promise of that
of that set up I think cool anybody else
have a question all right well thank you
very much thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>