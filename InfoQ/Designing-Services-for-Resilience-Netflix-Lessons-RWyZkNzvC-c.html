<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Designing Services for Resilience: Netflix Lessons | Coder Coacher - Coaching Coders</title><meta content="Designing Services for Resilience: Netflix Lessons - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Designing Services for Resilience: Netflix Lessons</b></h2><h5 class="post__date">2018-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RWyZkNzvC-c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everyone for being here today we
often consider designing services to
make them more resilient but believe it
or not no matter how many checks we put
in place or how often we're doing the
right thing
there's off there's going to be things
that go wrong in order to ensure that
your services can handle these things
that go wrong that will happen to them
we exercise resilience testing and
experimentation we talk a lot as an
industry about designing for testability
and there have been a lot of good talks
on that in fact even today at Q con and
it's not a very it's not a new thing at
this moment but there are certain types
of tests in experiments that are new
such as chaos experiments and there are
ways that we can design our services
that will make us better prepared to
participate in these kinds of resiliency
tests and these kinds of resiliency
experiments in design discussions at
Netflix nowadays we've moved from what
happens when this what happens if this
fails to what happens when this fails
the bigger the system is the harder it's
going to be to maintain and the higher
importance that's going to be placed on
testability it's no secret that
Netflix's systems are heavily reliant on
fallback paths and in order to make sure
these fallback paths are completing the
way that we expect them to we test them
every day in the form of chaos
experiments so I want to show you one of
my favorite chaos experiments here and
so this is an example of a screen that a
lot of us would likely see I'm guessing
a lot of us would likely see the third
row right now though because you watch
stranger things real if you're if you're
still getting through stranger things I
totally understand if you need to dip
out and I won't look but one of the rows
that we have is the continue watching
row and say that we were running a chaos
experiment for this row to make sure
that you know if this row failed if we
weren't able to load what you were
previously watching we wouldn't expect
all of Netflix to go down right we would
maybe expect a fallback path like this
right we
maybe I expect just that road to not be
there we might expect to just serve
something else there but during the
chaos experiment we ran to test this we
realized something like this might have
happened and because we ran a chaos
experiment on it we were able to verify
that this didn't happen before customers
looked at us like this I'm Norah Jones
I'm a senior chaos engineer at Netflix
and today I'm going to talk to you about
how you can design your services to
participate in experiments like that to
participate in resiliency experiments to
participate in chaos experiments and
through resiliency testing your life can
be made a little bit easier you'll get
paid less your customers you'll will be
happier and your service will be overall
more available I work with a lot of
great people at netflix and I'd like to
thank my team for being here today
Allie Haley Lauren and Casey and letting
me share this experience with with you
all we also got the opportunity to
participate in a book together recently
it's free on The O'Reilly website right
now if you'd like to check it out and I
will have some copies of them at the at
the Netflix reception that we're hosting
on Wednesday after the conference so
let's get started how can teams design
services for resiliency testing we're
gonna go through several different ways
that that we can make resiliency testing
easier on you so one is enabling failure
injection another is enabling our PC
remote procedure calls one is enabling
fallback paths and having ways that we
can discover them
we should enable proper monitoring and
have a key business metric that we need
to look for or several key business
metrics that we need to look for and we
want to have proper timeouts and ways to
discover these as well but let's take a
step back for a second
there's a lot of known ways to increase
confidence in resilience like we've seen
through other talks throughout the day
and
and and other things that we've
practiced in managing microservices in
general so one of them is unit tests so
unit tests work like this it's on a
component level and they're critical for
verifying expected results at a very
granular level we have an input and we
verify that we get our expected output
in place and it's to validate what an
individual component is doing we also
have integration tests and integration
tests work similar unit tests and they
have a similar goal except they verify
that components work well together and
even services work well together so
those components working together have
valid inputs and get their expected
outputs and even those services have
valid inputs and get their expected
outputs
Michael Bryce X said and in his talk in
Q Khan New York he said don't touch your
code locally write an integration test
to test your api's and if you can't
write the test then refactor your code
deciding to trust your test is a
cultural thing and that rings so true
and I believe that's something that we
really believe in here at Netflix as
well we we don't just comment out our
test if you know if they're not working
properly right like that's the easy
thing to do if your tests are not
working properly you might need to make
some changes in your code right and so
that it that is really a cultural thing
that that takes place right you all have
to believe that together and you just
kind of have to do it and so this brings
me to a point with a new way to increase
confidence in resilience and this is
through chaos experimentation
so while unit testing and integration
testing are valuable for identifying
defects they can miss other problems
they can miss problems that we see in
production right with the with the load
of production in place with you know
verifying that that that failing calls
do the right thing that they're supposed
to when they fail so it's impossible to
know that the system as a whole will be
available in fault-tolerant unless we
have chaos experiments in place so we
run these in production and
looks similar to the set up as an
integration test that we saw before
except from calls from service a to
service B we can fail those in
production or we can add latency to them
now I before I explain to you how we
design for this form of resilience
experimentation I want to explain
exactly how this works in Netflix so in
order to do that I need to show you our
key business metric and before you do
chaos experiments it's really really
important that you have a key business
metric that you monitor in place and
Netflix our key business metric is SPS
or stream starts per second meaning how
many people are actually pressing play
at a given second or a given point in
time if you work at an e-commerce
company your key business metrics might
be you know adding to cart or making
sure people can checkout or searching
appropriately but this is something that
we really keep in place as we're doing
these chaos experiments so this is
Netflix's chap I want to take a quick
survey first how many people are
familiar with chaos monkey wow it's
almost every hand how many people are
using some form of a casma get in their
company
some hands so chaos monkey is great but
there's a lot more failure modes that
can happen other than single instance
failures right and and those are those
are things that we should be able to
verify so in addition to chaos monkey
Netflix we have a new tool called chap
and chap stands for chaos automation
platform and it works like this so this
is how traffic looks normally right we
have we have say we have a hundred
percent of traffic going from a service
called API to a service called
personalization so we then want to make
a calculation based on the amount of
traffic that we have right now and
relate that calculation to our key
business metric so the key business
metric that I brought up to you earlier
was SPS so we've come up with a
calculation at Netflix in such a way
that it affects the lowest SPS possible
but still enough to give us a signal
that our chaos experiment is working
correctly so based on that calculation
and relating to SPS we find a certain
percentage of traffic so say in this
case we calculated two percent of
traffic and we split that in half and we
take one percent and we use our gateway
called Zul and we route that into a
control cluster and we don't do anything
with the traffic that's going into the
control cluster we just have one percent
of traffic going into the control
cluster then we take that other 1% that
we calculated and we put it into our
experiment cluster and this is where we
actually fail it or we add latency and
it's really important that we have the
exact same amount in our control cluster
as our experiment cluster because we can
actually monitor our key business
metrics and how close they are to each
other so that key business metric SPS I
brought up earlier we actually monitor
this as a part of our chap experiments
and short it automatically if the
experiment deviates too far from the
control cluster
so we can add a bunch of other key
metrics in there too and short them
automatically based on it or we can just
fail the experiment and let it run all
the way through but the key business
metric is something that we don't want
the customer to have impacted so we have
that shorted automatically and it will
short whether or not it's below the
control line or above the control line
above it may be it may be indicative of
a retry storm whereas below it may be
indicative of some failures so chop
works on top of a service called fit
which which works chop works on top of a
service called fit and it it's our
failure injection testing library and
it's important in order to test for
resiliency that you have failure
injection testing enabled so however
your organization decides to implement
failure injection ejection testing make
sure it's easy for service owners to
turn it on so I'm gonna give you all a
sample failure injection library today
and this is something that I modeled off
of Netflix's at a very very high level
and it it essentially allows you to fail
or or make a service layton and so I'm
going to go through the code with you so
this serves as a skeleton failure
injection library I built it in F sharp
because I enjoy the succinct nests of
putting it on conference screens there's
not a lot of overhead with it and I have
a simple API for the library that can
demonstrate its cheese - so our Kaos
module here defines our main failure
injection we have name which is the type
of chaos we're injecting this is very
important for logging purposes if we
want to truly remain chaotic in our
approach we we could probably not allow
the user to pick the chaos but we do
want to log it and have it be a
parameter our shoot chaos is a boolean
and this determines if we've met our
predefined criteria to inject chaos so
this is important for safety concerns
right so we don't want to inject chaos
if our predefined
hasn't been met and you want to allow
service owners to decide that predefined
criteria to some extent and then chaos
an f-sharp you can actually pass in
functions as parameters to other
functions which is pretty cool so this
is the chaos function we will be
injecting into our normal function and
here's our async arrow and at a high
level the async arrow function serves as
a function that produces an asynchronous
computation as an output and then the
usage of the do bang here is interesting
where in most languages the bang means
not in F sharp it actually serves a
unique purpose of a synchronous
workflows like the one in this function
so the do bang here allows us to execute
this chaos parameter and allows us to
enable execution on other computation
threads as the cast parameter function
is being performed so here are two main
types of chaos we have failed with an
exception right so that could raise a
random exception and then we have
introduced latency and this has a
latency parameter and allows you to put
in a certain number of milliseconds that
you'd like to add latency to your call
and it will sleep for that certain
number of time so then here's some
sample criteria that we can do before we
need to meet before injecting the chaos
and here's where it gets interesting
there's a bunch of predefined criteria
you can put you can suggest it to only
do between nine and five so that can be
a predefined safety criteria you put
like maybe you don't want to cause chaos
after hours right maybe you don't want
to cause chaos when your developers
aren't at work you don't want to be mean
to them they are your co-workers right
don't want them to get paged more than
more than they need to be paged or you
can add other criteria as well it's
totally up to you and so here's the API
so you want to make a simple enough API
that your that your developers can use
it so a is your healthier normal
function and it gets piped with this
little with this little guy here and it
gets piped or injected with the Kaos
functions listed give
that the criteria that we defined before
is met so now that you have sort of a
background on how failure injection
testing works and how our chaos
automation platform works you you kind
of have an idea of where we were at at a
given time so we were sitting with teams
we were sitting with service owners on a
regular basis and we were trying to come
up with what made a good chaos
experiment right and as you can imagine
that was a lot of meetings right and
that doesn't scale it takes a lot of
effort to come up with what makes a good
chaos experiment you have to decide
where to inject your failure you have to
decide if you're doing a latency
experiment how much time you want to put
in there you you have to figure out what
the RPC calls are you have to figure out
what the Associated fallbacks are
there's a lot of stuff that goes into it
and it was very time-consuming
and so we decided to start automating
the creation of these we said there has
to be a better way than sitting with
teams and figuring out what makes a good
Cass experiment let's automate the
creation of these so we had to get all
that stuff that I was talking about
before those RPC calls those fallback
parameters those timeouts retries all
that stuff and feed it into this into
this experiment creator and that brings
me to this point have good monitoring in
place for configuration changes and have
good monitoring in place for your
configurations so all this stuff was in
a lot of different places right so we're
making calls out to like five different
things right now just to get all this
configuration information in one place
and now we do have it in one place but
it's it's very useful for for for
performing resiliency experiments
so we also want to have our PC enabled
remote procedure calls we needed
information on remote procedure calls
whether or not these calls had
associated hystrix commands his York's
commands are our circuit breaker or
fallback path whether or not those
hystrix commands had associated
fallbacks with them we needed to get
timeouts we need to get retries and
again we needed to have these all in one
place my colleague Lauren has this
hypothesis that config changes are more
dangerous than code changes and this
obviously came from something not just
didn't just come from nowhere has anyone
in here ever had a configuration change
that's caused an issue for them before
surprisingly that is almost every hand
in the room right but we don't put a lot
of monitoring on them we don't place a
lot of rules on them like it's it's it's
kind of crazy some some configuration
files don't change and then you you kind
of look at them and they haven't changed
in the last four years and the employee
that changed them isn't even here
anymore and you're like what what do we
do here right and so it's really
important to have like good
documentation around them and and good
monitoring around these configuration
changes and make them into something
that make them into an API that you
could you could call and you can figure
out what these configurations are so
let's talk about RPC at Netflix our
remote procedure calls are called ribbon
externally we call them ni WS internally
and they're the services we've discussed
that need to talk to each other reliably
with minimal minimal interruption from
the network issues scaling events
isolated software problems and other
dangers and this means having a valid
address on the end of a rest interaction
so ribbon is Netflix's inter process
communication and it's a built-in
software load balancers the primary
model involves rest calls and it has
load balancing features it has fault
tolerance it has multiple protocol in
the form of HTTP TCP and
UDP and it has caching and batching we
needed to get those RPC calls but we
also needed to get their associated
timeouts as well so knowing your
timeouts is very very important for
doing latency experience experiments
because we use these this information to
fuel what makes a good latency
experiment and to ensure that we're
running a proper latency experiment so
timeouts when we define timeouts we say
at what point does the service give up
right and timeouts are really hard to
figure out they're they're hard to get
right a long timeout means that you know
the note is declared dead meaning a user
has to wait or it sees an error message
which probably isn't a good experience
and then a timeout that shoot too short
might mean that the note is prematurely
declared dead which isn't good either
so you need something right in the
middle you need a sweet spot and chaos
experiments can actually help test for
that
so retries are commonly associated with
timeouts right immediately retrying a
failure after an operation is not
typically a great idea Netflix we use
exponential back-off on retries to avoid
pulsing and but it's important for us to
understand this logic it's important for
you to for for everyone to understand
the underlying logic between the retries
in between the timeouts and the
exponential back-off and and have that
that logic and that information living
living in a place that you can easily
call and access so again understand the
logic between your timeouts and your
retries a timeout on the first attempt
is likely indicative of a problem with
the node and you want to flag this as an
anti-pattern for a service behind a web
api fast enough is probably between 10
and 100 milliseconds and beyond that you
might actually start to lose some some
customers and some users so this brings
me to my next point
circuit breakers or hystrix and this is
actually first introduced by Michael
Nygaard and his book release it and he's
actually
released the second version of his book
release it - which is out now and I
highly recommend it so he introduces a
concept of circuit breakers and applied
them to software and so a circuit
breaker exists to allow one subsystem to
fail without destroying the entire
system as a whole but once the dangers
passed it can reset itself and restore
the full function of the program so we
apply this to software at Netflix in the
form of what we call hysterics and this
is actually open sourced and so you can
you can access it at any point as well
as along with ribbon what I showed
before so traffic coming from the
calling system to the service you're
talking to that traffic is monitored by
the circuit breaker and once you hit a
certain error threshold it will open
essentially to the point where the
circuit breaker opens it no longer takes
traffic the failing service no longer
takes traffic to the service that's
having problems and it Commedia
return a failure or fail up the stack
and so hystrix maintains this circuit
breaker functionality so as we were
gathering info for the RPC calls and for
the timeouts and for the retries we also
needed to see whether or not those those
our pcs had hysterics commands
associated with them and in order to
determine whether or not something was
safe to fail we had to ensure that it
had a fallback path and there may be
times when you know something is still
safe to fail but doesn't have
necessarily a fallback in place but this
was our way of making sure that it was
safe so if your service is non-critical
I would argue that you should ensure
that there are fall back pads in place
or if you're deeming it as non-critical
meaning that you know it shouldn't bring
your entire website down if it fails so
there's a few different fallback
strategies you can have in place perhaps
it could return a good response to the
last cache value it could return a
fallback service or it can return some
sort of canned response so you know
maybe instead of your recommended
titles we just show you generic titles
right on the screen instead of your
recommendations showing we show you
something generic so instead of the
whole service failing you're just not
seeing you know it exactly as you're
used to so it's really important to know
what these fallback strategies are and
how to get that information this
actually relates to your business
metrics so much because you should know
like when you're critical services fail
or when you're not critical services
fail what's actually happening to them
and if that state changes at any given
point that should be something that
you're able to consume before you're
doing that your resiliency tests so we
monitor our history commands during our
resiliency tests we see our account
successes we see our account fallback
successes so we can actually see these
during a chaos experiment that we run
and then we'll count the failures in the
timeouts as well so based on our
fallback strategies and based on what we
expect from a history command we can
verify that this is okay during a chaos
experiment by looking at these graphs so
with all that in mind I think it's
important to ensure synergy between your
hystrix timeouts your RPC timeouts and
your timeout logic and your retry logic
they all play a huge role together and
what we found when we were gathering
this info was that they were all in a
lot of different places and because they
were all in a lot of different places
there were discrepancies between them
right and so so let's go back to chapter
a second we started doing all this we
started gathering all this info to feed
into chap experiments but as we're doing
this you know we realized that it wasn't
all in one place so we decided to zoom
in and we gave we gave chap a monocle so
we we gave trap the ability to have
crucial optics on it services right and
and see this configuration info so
here's an example of chaps monocle so
this is a fake service
and we can see all this information in
one place now we started seeing really
weird things based on this right we
started seeing when hystrix commands did
not have fall backs enabled that we
thought we were supposed to have fall
backs enabled we started seeing
unwrapped RPC calls like that shouldn't
happen either and then we also started
seeing anti patterns with with retries
on the same server we saw hystrix
commands that were higher than the
overall RPC timeouts timeouts that were
clearly too high timeouts that were
clearly too low and you know this was
this was basically because a lot of this
information was in different places and
people weren't realizing that you know
this logic coincided with each other
right and and so we we started trying to
expose this and as as we saw this
information you know we kind of freaked
out right we're like oh my gosh what is
this right and so we decided you know I
realized after seeing this there
probably isn't always money and the
micro services right like it gets a
little confusing after a while right
it's a lot heart it's very hard to keep
track of all that stuff and that's what
resiliency testing is for we can use a
lot of this stuff to feed it into a
prioritize err and my colleague Haley
Tucker has done a lot of work on
prioritizing these chaos experiments and
she's come up with an algorithm of sorts
based on this this information that we
found through through looking for these
configuration files and so this is at a
high level how the criticality score is
formed but we essentially take parts
from that configuration like maybe an
RPS stats bucket the number of retries
the number of hystrix commands and we
weight those appropriately and we come
up with a criticality score and based on
that criticality score we execute that
chaos experiment in a certain way right
maybe the higher the criticality score
the sooner we won that chaos experiment
and as we're gathering more
configuration information you know we're
getting we're getting a lot more
experiments we're getting a higher
higher level of confidence
of which experiments should be run
sooner based on their criticality score
right and so based on all this
information I want to I want to share
with you all some some chaos success
stories that we've seen and these are
some of my favorite chaos success
stories so one of them was that we ran a
chaos experiment which verifies that our
fallback path works and it successfully
caught an issue in the fallback path and
that issue was resolved before it
resulted in availability incident so
this one was awesome and it really
demonstrated the value of the chaos
automation platform you know we're
experimenting in production we're
experimenting on live traffic but we
have the ability to cut it off before it
impacts customers right but we can see
when it's going to cause an issue at
some point down the line and this this
allows us to pause and take a step back
and resolve it without being under the
fire of a page or duty alert and here
was another one of my favorite chaos
success stories it was while failing
calls we discovered an increase in
license requests for the experiment
cluster even though all the fall backs
were successful
so all those graphs we were looking at
earlier they were all proving successful
but there were a bunch of increases in
these requests and that meant that
whoever was consuming the fall back was
retrying the call which caused an
increase in those license requests and
so Netflix engineers obviously all mean
well and we have stunning colleagues but
and everyone has the best intentions but
distributed systems are inherently
unreliable that's a fact
and your reliability can be built into
the system and ways to test for
resiliency saving you both time and
effort instead of after the fact and I
want to do a little bit of a tangent for
a second and the reason I put those
customer success stories and quotes is
because we work together with the
service teams to do this right we we
don't just lob things over a fence at
Netflix and expect someone else to take
care of our reliability and availability
it's something that we all take great
pride in at Netflix and that we all work
together towards and it's because we
have one goal in mind we we want our
customers to have a good experiments we
want we want our companies
customers have a good experience we want
Netflix to remain available and reliable
and it's important to remember that your
most important customer is your actual
business and so if I can leave y'all
with some takeaways from this talk it's
that designing for resiliency testing is
again a shared responsibility
configuration changes can and will cause
outages and it's important to have
explicit monitoring in place on
anti-patterns and configuration changes
that's it if you have any questions I
will be around thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>