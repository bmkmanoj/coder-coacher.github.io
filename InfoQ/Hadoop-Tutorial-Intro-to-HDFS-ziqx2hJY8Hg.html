<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Tutorial: Intro to HDFS | Coder Coacher - Coaching Coders</title><meta content="Hadoop Tutorial: Intro to HDFS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Tutorial: Intro to HDFS</b></h2><h5 class="post__date">2012-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ziqx2hJY8Hg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello my name is Samir Farooqi and I'm a
freelance trainer and consultant for
Hadoop I'm also the creator of the 2-day
Hadoop fundamentals class for Maracana
and in this demo I'm going to walk you
through the HDFS layer of Hadoop which
is a subset of the to date class the
first thing you should know about Hadoop
is that its origins come from Hadoop
from Google white papers there's three
white papers Google file system Google
MapReduce and Google BigTable which
become Apache HDFS Apache MapReduce and
Apache HBase about 95 percent of the
architecture described in these white
papers is faithfully implemented in the
Apache projects so if best place to
start learning about Google file system
and HDFS is really this white paper
there's some differences where in GFS a
file gets broken down into chunks which
get distributed across the distributed
file system in HDFS a file gets broken
up into blocks which get distributed so
there's some terminology difference
there but the architecture is very very
similar between GFS and HDFS Google
released the white papers with the
architecture of how these projects works
but no code was released so it's up to
up to a lot of the engineers at Yahoo
and now many other companies who created
these Java based Apache projects so the
first thing you should know about Hadoop
is understand how a file breaks up into
the blocks and what kind of what that
gives you so let's say if you want to
read one terabyte of data on the Left we
have the traditional ways way of doing
things where we have one machine with
let's say four hard drives go to the
machine and each one of these hard
drives has a 100 megabyte per second
capability of feeding files off disk so
we take the first terabyte of the file
and we scatter it evenly across these
four hard drives so we have 250
gigabytes of data on each hard drive all
right so if you wanted to read one
terabyte from this machine it would take
45 minutes to read that this is just a
theoretical calculation of how long it
would take to read from a 250 gigabyte
drive at 100 megabytes per second so the
power of Hadoop is you take that same
terabyte and you will scatter it for
example across a 10 node cluster with 10
worker machines and you put one tenth of
a terabyte on each one of the machines
furthermore let's say that each machine
also has four hard drives so you're
putting about 25 gigabytes of data on
each one of the hard drives so across
the 10 machines there are 40 hard drives
and each hard drive is now holding about
25 gigabytes of data and to read one
terabyte in parallel from the 10
machines it would only take 4 and a half
minutes
so by harnessing the power of multiple
machines together you can achieve your
reads or writes significantly faster
since Hadoop comes from Google white
papers it subscribes to a master/slave
architecture you know the competing art
paradigm here in the world of no sequel
is the Amazon Dynamo architecture which
is more of a peer-to-peer architecture
and that's kind of how Cassandra works
but Hadoop is a master/slave
architecture and that's the first thing
you should really know about Hadoop that
the different processes or services
running in Hadoop are going to be
classified as either a master or slave
component so the master components of
Hadoop are the name node and the job
tracker and the
Laith components of hadoop are the data
node and the task tracker we're going to
be focused on the file system in this
tutorial so we're going to be more
looking at the name node in the data
node the name that is a master of the
file system the data node is a slave
component of the file system there is
only one name node running in a Hadoop
cluster using Gen 1 Hadoop or Hadoop 1.0
and there are going to be multiple data
nodes running in the cluster so every
slave machine will run a data node
daemon it will also run a task tracker
daemon for MapReduce but we're more
focused with the data node daemon here
so if you have a 10 node cluster with 10
worker machines you would have 10 data
node demons and the 11th machine would
be the name node which is the master for
the file system um you should know that
the name node is a single point of
availability failure so if the name node
goes down the data nodes won't know how
to make sense of the blocks that are on
there
so you want to make sure that the name
node is running on a dual or triple
redundant Hardware a machine you want to
use something like a raid 1+0 there on
the data nodes there is redundancy in
the fact that every file that gets
stored gets 3 replicas of it and if some
of the nodes crash there should be other
notes that have a copy of the file also
in Gen 2 Hadoop there is an
active/passive
architecture for the name node so you
can run a active name node and if that
goes down then within a few seconds
a passive name hood will come up to take
or those responsibilities however in Gen
1 Hadoop there is really only one name
node if that goes down well your cluster
will not be able to read or write any
files to or from the cluster there is a
secondary name node daemon as well in
Hadoop which is probably the worst name
demon in computer science because the
name secondary named it implies that if
the name node goes down the secondary
name it will come up but that's not what
the secondary name node does it does
more like how
keeping for the name node the name node
which keeps all the file system metadata
in RAM has no capability to actually
persist that metadata onto disk right so
if the name node crashes you lose I lose
everything in RAM and therefore you have
no file system backup so what the
secondary name node does is it contacts
the name node like every hour pulls that
metadata out of the name node and kind
of reshuffles and merges it into a clean
file called a checkpoint that gets
written on the secondary name node it
also gets sent to the name node for
persistence so that's how the metadata
in RAM on the name node actually gets
persisted onto a file through the help
of the secondary name node but don't
think of the second your name node like
high availability for the for the name
node so here's the basic architecture of
how HDFS works we have a name node here
with the file system metadata that I
mentioned the name is currently keeping
track of two files data one dot text and
data to text you can see that data one
dot text is actually broken up into
three different blocks blocks one two
and three and data two dot text is
broken up into two blocks blocks four
and five now the files themselves are
not stored in the name node this is just
the file system metadata that kind of
points the files towards the blocks that
gets broken up into the other metadata
and the name notice things like the file
system permissions liquid what your
users have access to a file also last
access time for a file is kept track of
in the name node and also disk space
quotas and file system quotas are kept
track of in the name node alright so in
this diagram we also have four data
nodes which are the slave components to
Hadoop and we are setting the
replication factor for this cluster to
three that's that in a file called HDFS
I XML and there is a parameter called
DFS replication that is set to three in
this cluster
so let's take a look at how the files
actually gets stored in the name in the
data nodes
all right so file one dot txt which is
data 1 dot txt is broken up with into
three blocks and here's kind of how
those blocks are stored in the data
nodes you can see that for each one of
those blocks like block 1 for example
there's 3 replicas made of it so the
first replicas on this on the second
data node and there's another replicas
on the third data node and there's yet
another replicas on the fourth data node
same thing with the second block you can
see that there's a replica of it here
and a replica of it on another data node
and a replica of it on the first data
node so this leads to HDFS being a
highly available filesystem and it's
also self-healing
it's highly available because if the
second data node here crashes with a
copy of block 1 and block 3 that's fine
because you have two other copies of
those blocks in the cluster and this is
also a self-healing filesystem because
if one of these data nodes goes down
then the heartbeat from that data node
to the name node will cease to heartbeat
and after 10 minutes the name node will
consider that data node to be dead and
then what our blocks were on that data
node will get respond so that the
replicas count 3 is achieved the other
blocks will get respond from the other
two remaining data nodes that should
have a replica of that all right so
that's the first file and here's the
second file data to text you can see how
once again with the second file you were
taking the two blocks block four and
five and replicating it three times and
then scattering it across the data nodes
now there is some intelligence to this
to how the filesystem actually stores
the blocks across the different data
nodes so we'll talk about that in in one
of the future slides the name node also
has an embedded web server that hosts a
kind of basic website showing statistics
about the file system so you can see
and the name node was started we can see
the configured capacity of the name node
you can see how many data nodes make up
this HDFS cluster and we can see
statistics about how what percentage of
the distributed file system we're
actually using this is just a one node
cluster running in Amazon
so the configured capacity is only in
the gigabytes range but the largest
cluster is with HDFS and the world can
serve up to 20 30 petabytes of data
across something like 4,500 machines for
generation 1 Hadoop the first I guess
confusion that some people have with
HDFS is that it's a very atypical file
system so you don't actually format the
hard drives into cluster with HDFS right
you actually format the underlying hard
drives in the slave machines with EHD 3
or ext4
you can also use XFS so say you have a
10 node cluster 10 worker machines and
one master machine running the name node
in that case on every one of the slave
machines you would put maybe 5 or 10
hard drives of size 1 terabyte to 2
terabytes each and you'd have to format
those slave hard drives with exg 3 or
ext4 because to some extent HDFS is an
abstract filesystem so like the blocks
come into HDFS and then they get
committed ultimately to ext3 or ext4 or
XFS ext3 and ext4 are the most common
underlying file systems I see I would
recommend going with ext4 if you're
starting a new cluster and there are
some configuration parameters for ext3
and ext4 that I also recommend setting
like know a time and turning on the
reserved blocks XFS can also be used for
the underlying file system for HDFS but
it's far less commonly seen in my in my
opinion
these are the common HDFS shell commands
so if you're familiar with Linux then
this should look pretty familiar so you
can run the LS command to list the
directories in HDFS you can make a new
directory copy from your local file
system into HDFS you can delete a file
in HDFS you can tail the end of a file
you can change permissions in a file and
this longer command at the end is
actually changing the replication factor
to four recursively for everything
within this subdirectory so let's say
you previously stored something in that
subdirectory with replication factor
three and you want to add your more
resiliency to the filesystem and you can
you can update replication factor to
four or you can bring it down to two or
one after you've actually loaded the
files into HDFS this is a diagram that
shows the best practices approach for
how to architect HDFS cluster so this is
a how Yahoo sets up their 4500 node
cluster so we have three different racks
here each rack has 40 slave machines and
every rack has a top of the racks which
in a Yahoo cluster with 4500 machines
you have something like 110 racks and
eight core switches I'm showing to
coerce which is here and three racks so
every one of the slave machines will
have two cat5 cables coming out of it
going into the top of the rack switch so
that means the top of the racks which
has at least 80 ports on each rack and
that within the rack it's a one gigabit
network so the connectivity within the
rack is one gigabit however the course
which layer is a 10 gigabit network of
which eight Giga bits are dedicated to
HDFS and the other two gigabits are kind
of open for MapReduce administration
work and just a user traffic on
the network this should be a dedicated
network though at their tour at the top
of the racks which layer and at the
course which I'm dedicated for HDFS and
MapReduce you'll see that some of these
racks also have you know master nodes
like the name node or the job tracker or
the HBase master but you know if you
have a hundred racks there was a lot of
the racks will have no master machines
on them you'll also notice that the
master machines are beefier they're dual
or triple redundant hardware machines as
opposed to the slave machines which are
usually just one power supply and there
are single points of failure Zhan the
slave machines but that's not as
critical because if a slave machine
crashes there are other machines to have
a copy of that of that data another
feature in HDFS is that the name node is
rack aware so in this slide we're seeing
the same three racks as before each rack
has you know five data nodes the second
rack has data node six seven eight nine
and ten and you can see that there are
three replicas of block a they're on
data nodes one seven and eight and that
information is as you know kept in the
name nodes metadata so the name node
knows that this file dot text breaks up
into two blocks block a and B and block
a is made ax is on three different data
nodes and Block B is on these three
different data nodes another thing is
that the name node is rack aware that
means that the name node knows what rack
each data node is on you have to
actually manually configure that on the
name node viola a bash or Python script
it's pretty simple to do and now this
name node knows that rack one has data
nodes 1 2 3 4 and 5 and rack 2 has an X
5 data nodes that rack awareness is
going to come into play when you store a
file in the cluster because the name
node won't Minnis stores its 3 replicas
for each block in the file the name node
wants to scatter those replicas across
at least two different racks so if an
entire top of the rack switch failure
happens and
Tyrod goes out of commission there
should still be at least one other rack
that has a copy of your data if there
was no rack awareness in Hadoop then
there is a chance that one rack would
have gone all three replicas and if that
rack goes down because of a failure then
you will lose those replicas for that
point in time that the failure exists in
a smaller Hadoop node though with under
20 nodes you don't need to configure a
rack awareness you can you can configure
all 20 nodes to the same switch and just
you act like they're all in the same
rack which is called the default rack
however as you start to scale out your
cluster you'll want to use a rack
awareness topology script so with that
introduction to HDFS let's take a little
bit deeper dive on how HDFS actually
does the writes on the left here if you
have a client so that's like you know
your laptop that is outside of the
cluster that has a file that you want to
load into your Hadoop cluster so the
file you you'll want to load here
actually breaks up into three different
blocks block a B and C and in the
cluster I've kind of removed some of the
complexity from the previous slides so
we're only seeing three data nodes and
two racks however keep in mind that each
rack does have more data nodes and this
is a bigger cluster but I've removed all
the actors that will not play a role in
the right we're going to do for this
file and we also I've also moved the
name node out of the cluster keep in
mind the name node is RAC aware it knows
that data node 1 is on rack 1 and data
node 7 and 9 are on rack 5 all right so
when you actually want to store this
file with 3 blocks into the cluster you
will basically just run one command
called a copy from local command or
you'll write like five or six lines of
Java MapReduce which will then instruct
the client API for Hadoop on your laptop
to go through the following steps to
actually write the file into the cluster
so you don't have to actually do this
yourself or you don't have to actually
break the file up into three blocks and
then contact the name node to ask
you know where am I going to store the
this block this is done for you by the
Hadoop client API like I said all you
have to do is write some simple Java
code or run a command line copy from
local command but what happens is the
first step is when you submit a file to
the client API on your laptop the client
will contact the name going to say I
want to write file dot text block a into
the Hadoop cluster the name node will
respond saying okay write that block a
two data nodes one seven and nine and
then the client will directly contact
data node one and over TCP 510 send a
ready command to data node one set
basically saying get yourself ready I'm
about to send a block down to you but
also get get data node seven and nine
ready
so then data node one will forward on
that ready command by hopping through
the top of the rack switch the core
switch and the next top of the rack
switch or two data node seven and send
the ready command and ask seven to get
nine ready as well and then finally data
node 9 will let seven know that it's
ready and sound will let one know that
it's ready and one will let the client
know that they are all ready and the TCP
pipeline is ready to take down that
first block so this is where the
pipelined right actually begins you'll
notice that when the client is going to
start streaming the block down to the
clients it is not having to go through
the name node anymore the name node is
really only used to figure out where in
the cluster you will write that block
but then after that the client contacts
one of the data nodes directly to start
streaming the block in so the client
will send block a down to data node one
which will then forward it to data node
seven which will then forward it down to
data node nine and then each one of the
data nodes will let the name node know
that they receive the block successfully
and the first data node will let the
claw
I know that they all received that block
successfully and then the Klein will
move on we did in with the Block B and
Block B will be stored in perhaps three
entirely different data nodes right it's
not like the file will be stored on only
only these three data nodes for the next
block the client will basically
renegotiate with the name node the
location for Block B which might be
entirely different set of racks and data
nodes I want to point out just a few
other things here you'll notice that
we're spanning two racks here this is
part of the algorithm for HDFS basically
when you write a block into HDFS it will
put the first replicas on a rack by
itself in a data node and that first
data notice kind of randomly chosen and
that rack is basically kind of randomly
chosen and then the HDFS name node will
choose a different rack in this case it
shows rack v and on that rack it will
kind of randomly choose two different
data nodes to put the second and third
replicas together on a rack
so the first replica goes by itself on
Iraq and replicas two and three will be
more tightly coupled on Iraq the reason
for that is because when you write from
data node one to seven you have to go
through three different switches but to
go from seven to nine you just have to
go through one top of the rock switch
back down to nine so that keeps the
latency for the write a little bit lower
however this is asynchronous right so
the client does have to write to one
seven and nine before the write is
considered successful if you reduce your
replication factor in HDFS to just one
then the client only has to write to
data node one and then a write
successful ascend back to the client and
the name node that kind of write is
going to be many times faster five ten
twenty times faster than doing a replica
three right which is a synchronous write
across multiple switches and multiple
data nodes when a client is reading
a file the same type of architecture
comes into play the client has to
contact the name note first to figure
out where the actual blocks are stored
for a file so in this case the client is
going to ask the name node what are the
block locations for a file called
results dot txt and as you can see the
name node has in its metadata the block
locations for block a and block B for
that file so the name hood will respond
back saying there's two blocks in this
file block a is on data nodes one four
and five and block B is on data nodes
nine one and two and then once again
like before the client will directly
connect to the data nodes to read the
blocks now this list that the name node
sends with block a being on one four and
five is a ordered list and this is um
kind of recommending the data the client
to contact data node one first to read
the first block if the client cannot
contact data node one for whatever
reason then it will move on to data node
four to read the other replicas of the
block the name node is um intelligently
ordering the list of data nodes based on
how much network traffic each data node
is under but the other important thing
to once again reiterate here is that the
client has to contact the name no just
to figure out where to read the files
from or where to write the files to
however the relationship is direct
between the client and the actual data
nodes for where the write takes place
it's not like the read or the write is
having to flow through the name node to
the data nodes that would make the name
node a single point of failure in a
large cluster you know you can think of
the name node kind of like a bookkeeper
in a library so you know in a library if
you want to find a book you're not going
to go randomly from shelf to shelf
looking for the book that would take too
long instead you you'd prefer to go you
know to a cupboard and you open up a
drawer in which there are a bunch of
index cards which have you know all
the books in the library organized by
like author's last name or the title in
descending order for example right so
you can go open up those index cards and
very quickly figure out where your book
actually is and then that so the index
card kind of points you towards that
shelf right that's kind of what the name
note is doing here the name of it is
kind of like that cupboard with all the
index cards which make the file system
metadata on the name node so now I'm
going to show you a live demo of how
HDFS works
we're running a one node virtual machine
here I'm currently in Linux in this
directory home Cloudera Maracana demo
and i'm logged in to this linux machine
as a user named cloud era and in this
directory marcano demo there are two
files Shakespeare txt and Tom Sawyer txt
these are just a files from Project
Gutenberg so let's take a look at one of
these let's take a look at Shakespeare
text it's just a you know multi megabyte
file that has the entire works of
Shakespeare it's just a text file and
similarly does it there's a Tom Sawyer
file that has the Adventures of Tom
Sawyer by Mark Twain so our goal here is
to write these two files into HDFS now
since this is a pseudo distributed
OneNote cluster all of the demons for
HDFS and MapReduce are running on this
one machine so the first thing we'll do
is just kind of run this Hadoop command
which shows all of the options for
Hadoop we are going to be more focused
on this file system option so let's run
that and we'll see all the options for
the file system so we can do LS we can
run addy you command a move command a
copy command so let's let's actually run
the command too
to list what is in the file system for
so it's Hadoop FS - LS and will list
what's at root of HDFS so we'll we can
see that there's three folders at root
and these are all three direct one of
them is a directory don't be confused
between that and the Linux LS command
which is showing me what I'm what is in
my mark on a demo folder so let's drill
a little bit deeper into the filesystem
let's do Hadoop FS LS and let's take a
look at what's in the user directory so
there's two subdirectories here one of
them is named Cloud area and one of them
is named hive remember I'm logged in as
the user cloud era so my home folder is
user cloud era and if I do LS and there
I don't see any files so there's nothing
in there right now let's make a
directory first so we'll do M Kader and
we'll make a new directory in there
called new der and in here we'll store
the two ebook files so if I run LS now I
should see a directory here called new
der and if I go into the new directory
I'll see I'll see no files before I
actually load the file into HDFS let me
show you the name node GUI this GUI is
not just showing me when the name mode
was started it shows me that there's
about 137 files and directories in HDFS
and 59 blocks
total there's just one data node right
now and there's no dead nodes in the
cluster this cluster is configured
capacity is about four and a half gigs
so when I load the files into the
cluster this number should increment
from 137 to two more it should go up to
139 once you load the files in so let's
do that we do that using the Hadoop copy
from local command so this is a case
sensitive
and we'll do copy from local and the
file was called Shakespeare text and I'm
going to give it the path in hdfs where
I want it to be stored so user cloudera
new door and hit enter and let me do the
same thing with the Tom Sawyer file and
hit enter and then while I'm here let me
just do an LS command and check what's
in user cloudera
new door and there you can see the two
new files I just added in there is
Shakespeare text and there's Tom Surrey
text you can see the user group and
other permissions for that file this
number one means that this file is
currently replicated once in a one-note
cluster this is fine by in a production
cluster you should usually use
replication factor 3 and this is the
size of the file in HDFS now if you go
back to the name node GUI and if i
refresh this page if i refresh this page
we should see the file counting go up
from 137 to may be 139 there you go and
it's gone up just a bit one more thing
in the name no that's kind of cool is if
you go into the Browse the file system
link you can also look at what's in the
file system through here so I'm going
under user cloud era new der and there
we go I should see my two files there
and if I click on one of these files I
can see the raw contents of the file as
well I hope you enjoyed this short
introduction to HDFS if you'd like to
learn more about HDFS please check out
Maracana comm for upcoming dates for a
public class for Hadoop we also do
private classes for companies so if
you'd like to contract us out for a
2-day private class we can do that as
well and this is my email address if you
want to send me any questions I hope you
enjoyed the course and I look for
to seeing you in one of my classes in
the future</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>