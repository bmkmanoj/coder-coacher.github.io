<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>SPDY and the Road Towards HTTP 2.0 | Coder Coacher - Coaching Coders</title><meta content="SPDY and the Road Towards HTTP 2.0 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>SPDY and the Road Towards HTTP 2.0</b></h2><h5 class="post__date">2012-11-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SWQdSEferz8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everybody so we're going
to talk about speedy and HP two point oh
and how those two relate it's a subject
that's near and dear to my heart so
without making you wait alright I'm sure
everybody has had a long day this is
what we're trying to get to with HTTP
two point oh this is the promise line so
specifically the first item on the
agenda for HP two point O is we want to
improve performance which is to say
latency and we will actually talk about
latency and why that's that's very
important as a focus area for C 2 point
0 then specifically we're going to try
and solve this problem called head of
line so that doesn't mean anything to
you hopefully after this you will
actually understand what we're trying to
solve their multiple connections this is
something that we have to do today to
make our web apps perform well hopefully
we won't have to do that anymore once we
have a chess piece point 0 and last but
not least two point oh sounds like a big
thing that's like what are you guys
going to do break the internet change
everything not quiet the intent is to be
backwards compatible with 1.1 don't
worry your app will run an htpc point so
with that in mind I actually want to go
kind of deeper and understand why we're
trying to make these changes right so i
can easily just give you like here are
the things that you will get once HP 2
point 0 is here but the real question I
think is the real interesting question
is why do we need it in the first place
and why are we doing this to begin with
so first of all since we're talking
about performance and latency here some
just like kind of like usability
one-on-one numbers so there's has been a
number of user studies done over time
these results are actually old they're
from Jacob nickel Nielsen but they've
held constant and basically what this
tells you is that when an interaction
happens on a page in your app wherever
within 100 milliseconds we mostly
perceive it as instant it's instant
feedback right you click on a button you
get response great that's what you want
100 to 300 people start to respond that
start to perceive some sort of a
sluggish performance
like the buck the button is sticky and
it really depends on what you're trying
to do and anytime you kind of go over
one second that's when the user has
already made a context switch right
you're you're typing something you press
submit and all of a sudden you're
waiting and now you're like oh I need to
email Bob right and then you lost the
user so that's what we're trying to
solve right so ideally we should be what
then let's say 300 milliseconds and this
is why for a lot of sites like Google
Yahoo Facebook there's this kind of
unofficial 250 milliseconds time that
all of the sites try to perform within
or return a web page will then because
we know that if we do that then it feels
fast and responsive so without in mind
how are we doing today we did this study
with Google Analytics so this is data
sampled across hundreds of millions of
users with real real performance data
off their phones and desktop computers
all over the web and it turns out this
is data as of April twenty-eighth 12 so
five six months ago the median loading
time on the web is about 2.7 seconds and
the mean is 6.9 so it's actually pretty
high and mobile is much worse so roughly
speaking five and ten seconds and that
even those numbers are incredibly
optimistic because this is based on the
latest Android phones right so if you're
in the latest Android phone chances are
you also on a fast network because
you're the early adopter so in reality
these are very very optimistic and we do
we have a long way to go to get from 10
seconds or five seconds to 250
milliseconds right there's like 20 25
bucks in there so why is that right why
do these pages take so long to render
and if you look at data from HTTP
archive.org which tracks a performance
data how pages load across hundreds of
thousands of pages you'll find that an
average page on the web today takes
about 84 requests to compose which is to
say there are many resources for which
we need to make various requests in the
process we connected through up to 30
different hosts on average so 30
computers distinct computers on the
network and all of that combined comes
out to be roughly one megabyte in size
so an average page on the web today is
one Megan sighs and takes 84
requests right and we want this to
render in 250 milliseconds or 500
milliseconds that's that's almost a
miracle but you know we make it work and
some interesting observations in here
you know six hundred kilobytes out of
that one meg is images so if you know if
nothing else optimizing images on your
site may be a very good place to look
for easy wins for how to accelerate your
site and that's true in general on the
web we find that image picking a good
image format is very important so at
this point you should be saying like hey
this is cool you know the pages are
growing in size but we're also getting
much faster networks right you listen to
any ad on TV radio whatever and there is
like 4G LTE speeds and 40 mega plank on
your comcast whatever right it's like
great I'll just get a faster connection
and everything will be good turns out
that's not the case first of all let's
rewind a little bit Akamai does or has
this site called Akamai yo where the
report average connection speeds and a
whole bunch of other interesting data
about general performance of the web so
I just pulled this number for q1 2012
and this is specifically for the United
States and you can see that within the
US an average consumer is basically on a
6.7 megabit pipe right so there's that's
the available average available
bandwidth in the US and why is that
interesting well sorry welcome back to
that and second whole data that was
bandwidth the other component of course
when you talk about what performance is
latency so bandwidth and latency these
are the two major things and here's some
data from the FCC report called
measuring broadband America so what
they're doing is they're deploying nodes
within all the major networks to measure
the latency between your computer and
the network itself right so your
provider so this is just the latency
between you and the core network not
even to the destination site and they're
finding that on average if you have
fibroids fiber at home it's 18
milliseconds for cables 26 milliseconds
gsl 43 milliseconds so this kind of
gives you a feel for what are you
getting in
terms of broadband performance and we'll
see why latency is in fact one of the
crucial metrics for us moving forward
now at Google we also measure this quite
and we monitor this data quite closely
and we know that worldwide the rtt so
that the latency to the Google servers
is about 100 milliseconds on average and
within us is 50 to 60 milliseconds now
this is data from 2012 the same same
measurement from a couple of years back
also had worldwide at 100 but had us at
60 to 70 so we know we've made some
progress we've earned or we warn back
about 10 milliseconds which is very good
news but nonetheless that's that's not
enough and the problem here is that
bandwidth doesn't actually help us as
much as you would think when it comes to
rendering web pages and the question you
should be asking yourself at this point
is why why wouldn't bandwidth make all
of our pages faster and here's a study
that we did this is from a couple of
years back so this is what kicked off
the speedy work at Google a few years
back and the study was very simple we
took a sample of websites of popular
websites and we varied the available
bandwidth to the user and the latency to
the server right so it's kind of two
independent variables and you can see
the difference in how the PLT which is a
page load time right the total amount of
time that the page 62 load was affected
so on on the left you have the latency /
bandwidth we should say we're varying
bandwidth and when you're on a 1 megabit
connection you know on average these
pages took three seconds to load okay we
double the bandwidth to 2 megabits and
all of a sudden the page load time drops
almost x factor of 2 that's that's a
that's a great one that's what you would
expect but shortly after that you see
this kind of very quick decay so after
about 5 megabits per second you're
getting single percentage wins in terms
of page loading time which is not very
promising to be honest right and why
does this matter well think back to the
number that we saw earlier from Akamai
on average a consumer in u.s. already
has over five megabits per second so
upgrading your connection today
probably won't make the pages load any
faster that's a depressing thought now
let's try the same experiment but by
varying the rtt to the server so this is
the latency between you and the server
that you trying to communicate with and
the you know the most obvious thing that
you can see is as we decrease the
latency the performance improves
linearly so there's a direct correlation
there and at this point if you're
familiar with kind of the inner workings
of TCP you're saying well of course you
know that that's what you would expect
and I'm not going to talk about mobile
specifically in this talk but everything
that we are covering here applies
directly to mobile except that the
situation there is much much worse right
like orders of magnitude wars so this is
a snippet I took from the sprint faq
right and they're telling you that on
the 4g network you should expect average
latency of 150 milliseconds right so if
you're one of these early adopters we
went out and got the the new 4g that's
what you're going to get if you're on
the 3g network you're going to expect
they can see 400 milliseconds right this
is pretty brutal thank back to the fiber
at home with 18 milliseconds you're
getting you know 10x difference
basically on your mobile phone or worse
so you know in our tests we didn't even
simulate the performance going over 240
so you know we should revisit that test
and the thing to mention about bandwidth
the latency is that you know improving
bandwidth is relatively easy and I put a
lot of stars there because you know I'm
sure somebody in the crowd will be like
what are you talking about this is
incredibly expensive and it is right but
it turns out that bandwidth can be
increased by just providing more fiber
and fiber is expensive to to just lay
but for example between 2007 and 2011
over sixty percent of the capacity that
was added to our existing infrastructure
was through upgrades of existing cables
which is to say we deployed better
infrastructure on both ends better
modulation all the rest and we got
better throughput through existing
cables so there's a lot of winds that
are happening there and there's also the
possibility that we could just lay
another cable right like if if we're if
the links are saturated between new york
and london we can just lay in all the
links
answer but we can do that with latency
the story is much much harder we have
this thing called the speed of light
rather annoying a couple years back
there was that announcement that you
know we figured out how to deliver stuff
faster than the speed of light turns out
it was a broken cable damn it or a
faulty cable so you know still fingers
crossed maybe we'll figure it out one
day but in the meantime the only thing
you can do is to lay a shorter cable
right so if you have a cable from London
to New York that takes 5,000 miles maybe
you can lay one that's 4,500 miles and
that's actually a win so as an example
just last year there was a project or
yeah last year there was a project
kicked off by a couple of companies
which is called Hibernia Express and the
entire premise of this project is that
they are laying the shortest cable
between North America and London right
and this cable will save five
milliseconds of latency and thus cable
is only going to be used by the
financial institutions because they
figured out that these guys are willing
to pay for this the project is going to
cost 400 million dollars so that you
know doing basic math tells you that a
millisecond is worth about 80 million
bucks and that's not even the most
expensive project so this gives you a
feel right like we were up against some
really tough boundaries here and not
only that but when it comes to latency
we're already within us like a small
constant factor of the maximum of the
speed of light so we can't expect a 2x
or a 10x improvement anytime soon unless
we actually figure out how to go faster
than the speed of light which is a minor
complication so once again why is
latency the problem right now let's
we're trying to render web pages so if
you guys are familiar with HTTP right
there is HTTP 1.1 introduced this
concept of pipelining so the basic
operation of HTTP is to say we have one
connection and you send the request you
wait for the response once we get the
response you can send another request
very simple model pipelining which was
introduced in HTTP 1.1 basically says
hey you can send me multiple requests a
one after the other but I still I will
only respond in order so if you send me
requests for images a B and C I will
and you the entire response a danimals
send you their entire response be and
then see right so there's no
interleaving between these images or any
of these resources that you're
requesting so why is this a problem well
in that second diagram it's you know
it's very pretty because everything just
goes a lot faster in reality what
happens is that we have this problem
called head of line blocking where some
requests so for example that first
request could be a dynamic image or you
know a server action that takes a while
and then everything is blocked behind it
and this turns out to be a huge
scheduling problem so because of this
pipelining hasn't actually seen lots of
adoption it is being used on mobile
browsers today but it's nonetheless it's
hidden miss right because if you
schedule something wrong and keen and
you can only guess how you should
schedule the resources you may run into
a case where you pipeline these second
and third resource and you would have
been better off to wait and send those
requests another connection so generally
speaking this is an unsolvable problem
right like you can guess but you'll
never win entirely so what does our
solution has been well we just open
multiple connections right like to help
with it we're just instead of sending
six requests over the same connection
will open 60 G's six connections and
send a request on each and this is the
deep basically the default number on all
the browsers today this number has
varied over time based on what browser
using but today most desktop and mobile
browsers have this limit of six
connections per hostname and of course
what do we do six connections may not be
enough because you have 30 images on the
page we do the main starting all right
so we'll put different subdomains and
then you'll open 30 connections all
right it's like problem solved beautiful
so why is this a problem let's go a
level deeper right we're looking at HTTP
the problem here is that new connections
also impose their own penalty and TCP
has a feature called slow start it's not
a bug it is a feature where the
observation that the TCP guys made way
back in the you know
that we have the server in the client
may have asymmetric links right you
server may have a very fast connection
the client may not have a very fast
connection and we need to figure out the
available capacity between these two
channels or maybe some somewhere in
between there's a congested link so to
be fair to everybody on the network we
are going to start slow which is to say
we're going to start by sending a couple
of packets and then once you act those
packets move will grow this window size
and then you act it again and it will
grow again this is exactly what is
happening here right we start with three
packets we get those acts where you
increase the window size 26 so on and so
forth so you can see that it's growing
very fast and if you zoom out a little
bit you will usually see this sort of
chart for your TCP performance this is a
new connection being started it ramps
very quickly relatively speaking and
then at certain point a packet loss
occurs and then we enter a new
congestion control mechanism which we're
not going to talk about but this has
important implications on CCP
performance this right here is limited
on latency so let's take a look this
tells us that you know within three rcts
we can deliver 12 segments a segment is
basically sick or 1500 bytes on the
network so we can deliver 14 kilobytes
within three rcts this is why latency
matters think back to our original
problem we have a page that is roughly
one megabyte in size 80 requests if you
do the math it works out to be on
average about 14 kilobytes so although
we have a lot of requests they're
usually very small requests because
we're opening new connections we're
always going through the slow start
phase and Verne even though you have
let's say a 5 megabyte connection we're
not able to make use of it because we
need to go through this process every
single time so we start by sending you
like a couple of kilobytes and then we
wait and then we wait again and the
entire performance profile of that page
is then dictated by the latency between
the server and the client right that's
why decreasing that latency was very
important and that's why mobile is such
a hard thing to get right because not
only are the latency is higher they're
also extremely variable
due to any number of reasons right a
carp ilysm in front of you the signal
bounces and all of a sudden you know
weird stuff happens on the network so
one solution that is currently being
deployed is called sorry it's increasing
the original window size so if i go back
here right we started with this with
these three segments this was the
default since about two thousand two as
of 2010 the new linux kernels are
actually have bumped this to 10 segments
and this is actually a very important
win because with ten segments we can
deliver most of the resources within
that first push before we wait for the
Arts et so if you if you sure on your
servers you may want to check your
kernel and make sure that you have this
enabled because this will actually make
pages go a lot faster for you and your
users so and this is the actual paper
from google that kind of motivated this
update so you know that was kind of a
detour deep into the tcp stack so let's
talk about HTTP and speedy so how do
these two relate i mentioned that we at
Google started working on speedy a
couple years back and we've actually had
very good adoption so majority of the
traffic at Google is now running on
speedy which is really awesome and we
also had a lot of adoption from
third-party vendors as you will see in
third-party sites so because of this the
htp biz a working group actually kicked
off an effort early this year for HSP
two point oh and this is already in
progress if you look at that kind of the
rough timeline here we had the call for
proposal that was earlier in the year
this month we need to draft the first
official proposal for HTTP two point oh
so this is like super exciting stuff for
those of you who care about in a working
groups and RFC but the timeline for the
protocol itself is about 2014 so the
first note is that speedy is being
adopted as a starting point for htet
point 0 that is not to say that HTTP 2.0
is speedy right we're basically saying
look we've deployed this it's been
running for two years we've learned a
lot of things we can use this as a
starting point it made
change entirely or it may be very
similar by the time HP to point out here
is here but the fundamental problems
that we're trying to solve are the same
so this is directly from the working
group mandate if you will you know we're
not going to go through it's a lot of
text basically I group them into three
things make things better build an HP
1.1 and be extensible one of the core
features of HTTP 1.1 is that it is
extensible so we won't make sure that we
preserve that we want to make sure that
HP 1.1 continues to work and you know as
we cover it in the beginning we want to
make sure that we actually address the
fundamental limitations of HTTP 1.1 so
we touched on this briefly but you know
here are some examples of the kinds of
things that all of us today need to do
to work around these limitations for
example spriting images why do we need
to sprite images well connections are
expensive right so what do we do we pull
all the images together and then we have
this giant CSS hack to position it on
the page and in the right place it's
quite frankly it's annoying to do we
know how to do it but we shouldn't have
to write if we didn't have this
limitation domain sharding is a great
example right we're working around the
fact that we have this connection limit
in the browser resource in lining is the
same thing so all of these things will
hopefully get addressed in a sense that
once a speech point 2 point 0 is here we
actually won't have to do this stuff so
life will get easier which is nice so
you know HDHP 2 point 0 will take a
while as you saw 2014 you know these
things do take some time so what can you
do in the meantime well you can use
speedy so speedy is already here it is
supported by a chrome opera and firefox
and it is compliant with HTTP as in it's
not like an entirely new protocol HP two
point oh this is another quote from Mark
Nottingham who leads the HP working
group what we're doing HP's point 0 is
we're changing how the data is laid out
on the wire we're not change
any of the semantics of HTTP so all of
the headers that you know cookies all
that kind of stuff is all there it's all
the same it's just how it's shipped on
the wire so here is a very quick
introduction to speedy we're not going
to go very deep on this and this is
specifically for speedy version 2 so
speedy has gone through we're actually
right now working on speedy version for
speedy versions who is what is being
adopted as a starting point for the spec
so I will this is specifically for
speedy version 2 so first of all the
primary goal of speed is to allow you to
use one connection and get all of your
resources over it that means we can
really open that a window very wide make
use optimal use of your bandwidth and
you know everything should just run
better requests are mapped to streams in
speedy speak so you know if you just
kind of replace that word one connection
you have multiple streams over it in
each stream corresponds to request and
streams can be multiplexed which is to
say they can be interleaved so I can
send you a little bit of data for this
image and then I can send you a little
bit of data for the CSS file JavaScript
what have you right so everything's good
it is binary framed and there are two
types of frames there is a control frame
and a data frame and I can tell you that
having written in HP parser and the
server and a client and having written
it and speedy clients the speedy Klein
took me a week ends the HP stuff took me
like a year to get right so it's it's a
lot easier to work with once you have a
well-defined format so here's an example
right this is a sample speedy frame this
is a frame that opens a new connection
or sorry not a connection a new stream
so what we're saying is this the first
bit says it's a control packet right so
there's just one and then we're saying
this is I'm speaking to you in speedy
version to the one represents that it's
a sin stream which is to say we're
opening a new stream so this represents
the type of packet I'm going to skip the
flags in length but then you have the
request ID so the client assigns or
every request gets its own ID such that
when data comes in we can
actually look at the frame and say oh
hey this is data for request we're sorry
stream number 45 which corresponds to
this image nice and simple and then we
have priority as well so you can
actually say hey I've sent your requests
for these five images but now I've
stumbled into this javascript file which
is very important for me to render this
page quickly give me a higher priority
for this one right push back sending the
image data which is actually very very
nice feature today browsers are actually
sometimes pessimistic they will find the
resources and they will wait and it will
say hey maybe there's a JavaScript file
coming later which is actually more
important so I don't want to block the
pipes if you will so I'll wait and then
kind of race it which is like once again
it's a game you can't win we're guessing
so this will help us solve this issue
and everything is length prefixed o be
fixed which is to say if you here if you
ever built a parser this makes life very
very easy right because the first thing
we tell you is like this is the number
of bytes you should expect if you don't
care about this frame just skip right
over it this is not the case with HP
where you need to parse all of the dam
headers and the body and then you like
okay there's this is the new request so
this will actually make it much easier
and then we have things like you know
sending actual headers so as part of the
sin stream the next thing that comes is
we sell you hey I'm going to send you
ten headers right so we can code that
directly and then we say by the way
here's the length of the name of the
header here's the header here's the
length of the value here's the value so
it makes once again very easy to parse
so this is my terrible artistic
rendition of how speedy works bear with
me so we have three requests that we
send from the client to the server these
are streams that we open and then the
server says hey okay that's cool this
third request that you send me I already
have some data for it so I'm going to
start sending you these these frames
back and then some time later I have
data for another from another request so
i can interleave this stuff i can send
it in parallel or cetera so one
connection everything flows over it as
long as it's on the same domain
obviously and welcome back to the well
actually it's right here so domain
sharding is a problem right so if you
sharted explicitly started your
right to have multiple domains now we
need the great on sharding once we go to
http two point oh we need to convince
all of you that that's it that is
actually counterproductive and if you
put all the resources on the same page
it'll make your life better and pages
load faster so now we're still working
through how to actually do that well as
you know as opposed to just telling
everybody to undo it so there's a lot of
wins that you get out of this
performance there was also some gotchas
especially in situations where you have
really high latency networks will touch
on that a little bit near the end so
another optimization that is important
and that is coming that is definitely in
speedy we'll see how it plays out in
HP's point 0 is we need better header
compression so we know that on average
request response headers those are sent
in plain text in HTTP today you can
compress the body's be a whatever
mechanism you wish but the headers are
in plain text and on average that's 800
bites on the wire right so as an example
here this is very typical use case today
I'm sending this like JSON payload that
says message oh hi to random server
right and here's all of the HTTP headers
that get appended to those like what is
it 16 bytes of data that I'm trying to
send right so the actual payload is like
tiny compared to all of this overhead
and that's where that's what we want to
solve in nhp 2 point 0 so in speedy we
actually just compress all of the
headers which gives us a enormous win in
terms of overall performance in speedy
version 4 we're actually reworking or
trying to rethink how that all that is
done instead of using gzip compression
we're actually doing kind of our own
compressor which is easier for proxies
and others to adopt so we talked about
gzip oh and also in speedy version 4 we
have this idea that we're experimenting
with which is to say a lot of headers
that you send today are the same for
example your user agent probably doesn't
change on every request I hope it
doesn't mine doesn't
so why do we keep sending it right
wouldn't it make sense when you open a
new connection to send a header once and
say hey this is a connection level
header and you know if you care just
apply it to every subsequent request
such that I don't need to send it to you
that's that in itself is a form of
compression of course so we're also
playing with that idea and trying to
figure out how we can make sense of this
this may enable very very kind of
compact representation of headers so now
we're getting into the faun territory
you know some people really
misunderstand server push so speedy does
have this feature where we have this
observation basically where the server
is serving you a file and the server may
know already the file the other files
that you will be requesting right like
I'm giving you the HTML I know you're
going to ask for the header image so why
don't I just push the image to you and
that's effectively what server push is
now this kind of raises a lot of alarms
for a lot of people rightfully so
because they're saying well if you're
going to start pushing resources that
may be a problem because what if I don't
want it right I'm on a mobile connection
maybe I have it in my cash I don't want
for you to just kind of use up my data
all you need is like one bad guy to just
push me like a gigabyte of what evers
right and everything is terrible so the
answer to that is you can cancel it the
client will send a stream a sense stream
packet that you saw earlier and you can
look at that and say hey no I don't want
this you can decline that but also you
are already using server push that is
what we call inlining today right when
you in line we use the data URI in your
page you're basically saying I know you
will ask me for this image or whatever
so here it is right in the HTML file
that is server push that's manual server
push if you will but that's effectively
the use case that we're trying to solve
and with proper server push it actually
works out much better because when you
in line something into the page it
becomes part of that page if you have
for example a small icon that you keep
in lining on every page it will
literally become part of every page as
opposed to being in a cache and you know
you can reuse the cash for all the
requests so there's a lot of interesting
things that you can do a server push but
there's also some gotchas and the really
interesting use cases for this kind of
stuff that I haven't really seen a lot
of people play with but I think it will
come is something like Amazon silk so if
you're using the kindle fire tablet you
are using a speedy proxy the way it
works is when you browse to http sites
the tablet itself has one connection to
an ec2 server or the amazon silk server
and that talks over speedy and then
amazon fetchers all the resources
optimizes them it may rewrite the images
and push them to your tablet which in
theory could accelerate the browsing
experience right which is great and in
theory when you're struggling all of
your resources through this proxy it
could keep a very good picture of what's
in your cash so it could be very smart
about the things that it can push into
your cash so I think stuff like this
will come and it'll be really really
interesting now the other thing that
needs to be aware of for speedy and this
is up and open for debate with H with HP
two point oh right now is speedy does
run over SSL and there are many
discussions as to why that is you know
there are people up for philosophical or
political reasons believe that
everything should be over SSL and you
know frankly it is kind of crazy that
you can walk into a cafe start a sniffer
and you know get a bunch of passwords
from a bunch of people that are just
browsing the HTTP sites that's not a
good story I think we're solving that
slowly but surely but that's not the
reason why we use TLS for speedy
specifically we use TLS for speedy
because we have a lot of intermediate
proxies deployed on the web that you
know operate on all these TCP streams
and HSP streams and many of them
sometimes for good reasons sometimes for
bad reasons may drop this traffic so for
example your antivirus software may
actually look at your HP traffic as it's
flowing and say hey you're talking some
protocol I have no idea about smells
like malware to me drop this connection
and all of a sudden your connection is
gone right and you
and the client can communicate with you
server so in practice even something
like web sockets is deployed you know if
you deploying its scale especially on
mobile browsers you use HTTPS because
that gives you an end-to-end encrypted
tunnel where these intermediaries can't
mock with your data and can drop the
connections in practice we find that you
know eighty ninety percent of the time
everything works just fine but then some
percent of the time you have no idea the
client just can't reach your server
which is obviously not a story we want
for something like google com right ten
percent of our users can access the site
not a good deal so that is the number
one reason for SSL right now and you
know I mentioned WebSockets sdhc now for
TLS two things come up whenever you
bring up two LS first of all you know
speed if we're talking about performance
and a whole point of Speedy's
performance doesn't tell us work against
us and there are specifically two things
cpu and latency so for CPU it is
definitely true that let's say five six
ten years ago the handshake the
cryptographic handshake was a bottleneck
you actually need a dedicated hardware
to offload that too but today all of the
modern hardware has all of the stuff
built right into the hardware such that
it's actually very cheap so this quote
that i have here from adam Langley who's
our ssl guy at google this is from his
blog when we turned on ssl google wide
in our cpu load on all of our service
went out by one or two percent there's
no hardware acceleration resolved on in
software facebook also runs all their
ssl in software so the handshake latency
or sorry the cpu load is actually not
that significant they're optimizations
that need to be made and can be made
within the ssl stack for better
performance like adjusting your ssl
buffers but that's really not the issue
anymore and i hope we can put that one
to rest the bigger issue if anything is
the latency because tcp when you
establish a new connection has the
handshake which is a full round trip
that before any data can be sent ssl
requires up to two additional
round trips which is terrible if you
think about it right on my mobile phone
I need to establish a nacelle connection
let's say my latency is 400 milliseconds
that's over one second before I can even
send you the get request I mean that is
terrible so to address that we have a
couple of proposals out there their
experimental calls called false start
and fast start we try to reduce the
extra round trips you know from two down
2-1 and there's even a proposal that may
actually take it down to zero by
basically remembering kind of state on
both ends so definitely an issue but it
gives us a lot of important
characteristics that we saw earlier and
I should also mention that as part of
speedy so speedy uses this protocol
extension called next protocol
negotiation which pushes the negotiation
of what protocol you're going to use
into the SSL handshake itself so there's
no extra round trips if you think about
web sockets the way that's done is you
send HP request with a header that says
I want to upgrade to a WebSocket
connection which then initiates another
round of a round trip we bypass that
with TLS npn so if you're thinking of
deploying speedy you will need TLS NPN
and that is in openssl by default
nowadays so who supports speedy today
chrome since ancient times we have
support on Android browsers we have
support on iOS so this is a screenshot i
took of chrome running on iOS it does
understand speedy it uses the same
network stack so you get all the same
things which is really nice we have
apache modules there's an engine X
module jetty Nettie so a lot of the app
servers today already support speeding
I'll actually show you some examples of
how to get started a little bit later we
also have big sites outside of Google
with support for speedy so Twitter are
supports speedy we have wordpress
wordpress runs on engine X they were
actually the ones that sponsored the
engine ax implementation of speedy which
is really cool Facebook is not running
on speedy but they've indicated that
they are experimenting with it which i
think is really cool and there's also a
lot of commercial pick up on the stuff
as well so Akamai announced that they
will
forwarded q4 this year which i guess is
now we have f5 with their speedy gateway
etc and of course you know I mentioned
this already if you are using google
services you are in all likelihood
accessing them over speedy no just sorry
which is pretty neat so I figured I'd
answer some questions preemptively
because I know that these questions come
up all the time do I need to modify my
site to work with HT 2 point 0 or speedy
and the answer is no we saw that nothing
really changes HSP headers are the same
we're not asking you to change your
angle bracket syntax or anything like
that but you can optimize your sites to
make better advantage of speedy the
biggest one is probably the domain on
sharding if you will that's the number
one win and then the others are probably
actually on the server so making sure
that you because using SSL you have the
optimal certificate length you're not
patting extra data in there and you have
your see window set to 10 and other
things like that so it's more in the
server than the client I think we
actually covered the rest so sounds
complicated they're dropping solutions
yes we saw right there's speedy if
you're running on if you have an app on
app engine you will inherit the stuff
for free which is really nice but you
know there's a gotcha and there's also a
gotcha right so imagine in 2014 we have
a speech point 0 and in life is
beautiful problem solved right well not
quiet we took a problem in HTTP 1.1 we
solved it by going to HP 2 point 0 on
and now we have as usual with
performance we unlock another bottle
neck and now that bottleneck lives in
TCP and in reality all we've done is
we've moved the head of line blocking
issue from HTTP into TCP so why is that
recall the diagram so TCP provides this
abstraction of a reliable delivery on
top of what is effectively an unreliable
network there is packet loss on the
internet in fact it's on average you
know one to two percent packet loss rate
and what that means is you know we have
a research that we're trying to ship to
the client we push out a whole bunch of
packets or segments onto the network and
let's say one of those packets one of
the first ones gets lost for whatever
reason right the rest get delivered all
just fine but the first one is lost what
does that mean well that means that the
receiver will keep all of the packets
that are coming after it in its own
buffer and wait for the first one
because they can't it promised you that
will deliver everything in order so that
lag between losing that packet and then
having to re transmit it is what
introduces this head of line blocking so
arguably you know this is not maybe as
big as the problem is in originally HTTP
1.1 but this introduces a lot of jitter
and kind of random delays into the
performance so if you're trying to build
a real-time kind of interactive
experience this may actually be a
problem this is the reason why we
transfer game state video and audio kind
of stuff over UDP right because if you
lost that packet that's ok maybe I can
pave over it and do something else so
that is a problem and we don't have a
good solution for it this was not a goal
for HP 2 point 0 to solve but we are
aware that this is a limitation and this
may be a limitation specifically on
these kind of really high latency
networks when you have a user from
Malaysia talking to your server in I
don't know here right in San Francisco
so that's all cool what do I need to do
to actually take advantage of this i
have a website that's running right now
how do i get on speedy and the answer is
it's actually pretty simple so there's
you know depending on which stack you're
running on for example apache we we have
a module that is being developed at
Google called mot speedy it is literally
just a filter that you install into your
server and the rest is taken care of so
remember once again we're not asking you
to modify your pages we're just making
better use of the underlying connection
and how the data is multiplexed so if if
a client comes with a chrome or firefox
browser to this Apache server it'll just
deliver
over speedy no difference there which is
really quite nice and you know to get
started you download the the package in
this in this case i'm using a debian
package you enable it in your apache you
restart apache and you're up and running
which is pretty sweet and also the
apache and other servers are often used
as a front end a terminating node have
in front of your app servers so your app
servers could be written in Ruby it
could be even knowed I don't know if
somebody would do that but maybe there's
a really good tutorial in that just link
and i'll share the slides at the end for
where a guy wrote up a sequence of steps
for enabling apache in front of his
Erlang web server right so you can
whatever technology is on the back end
even if it's a java app server you can
put a patch in front you can make use of
this and what will happen as Apache will
terminate the speedy connection the ssl
connection and it will relay the
messages in plain HTTP to your back end
so your back and won't even know that
you know it's talking in speeding engine
X as I mentioned has support it's right
now it is experimental it is basically
it's a patch although from through the
rumor mill apparently they're merging
the support into core very soon as in
like maybe weeks so today you would
actually have to download this patch it
is an official patch from engine X and
you just build it with the latest
openssl and you're up and running which
is also quite nice if you're running a
node server very simple there's a great
module called node speedy and you can
see here that we're just setting up an
ssl server if you've ever written a note
server this is just your ssl server is
except that we calling speedy create
server and that's all the rest of it
really and the server takes care of the
rest and note speedy actually also
supports server push which is quite nice
so you can do really interesting stuff
with it and you can experiment with it
quite well for java i was going to copy
the pages of xml and markup but figured
I shouldn't so having said that there's
very good implementation for jetty the
jetty guys have done an amazing job
you know at the bottom there is a link
that will walk you through all the steps
you basically a couple of things you
have to add an extra jar to your
classpath which adds this NPN support
for SSL negotiation that's step number
one and then you use their speedy client
or speedy server to enable it on your
Java app server you know there's 50
pages of XML in there somewhere but it
works and now finally you know if you're
the clients how do i how do you even
know how do i debug this stuff so there
are for all the browsers that support it
today there are extensions so you can
install just a few Curie's you know if
you're browsing around the web and
you're curious am i on speedy or am i
using HTTP so chrome has a speedy
indicator you just you can search for it
and you'll find I have no idea how does
it's a little kind of green bolt icon
this is not an official icon I have no
idea where it came from but I think you
know the first implementation of this
plug-in was for Chrome I think the guy
just picked it and now all the browsers
adopted it which is neat so there you go
so you know the it'll show up as a green
bolt in chrome green bolt in Firefox and
in Opera it actually has a dedicated
space on the right on the right side
which is pretty cool so these are just
kind of indicators for when you're
inside the support speedy in chrome and
also in Firefox you can also query from
the command line and see which version
of speedy am I using so for example here
I'm just pulling up my console and I'm
typing in window.chrome load times and
then get this object back which says
that we negotiated over npn speedy
version 3 so I'm speaking speedy version
3 and this page was fetched vs PD and
you know it was negotiated over npn so
there's useful metadata there but this
tell that this now tells you that you
are on speedy but can you go deeper and
the answer is yes absolutely and
probably one of the best tools is the
chrome net internal pain if you guys
have never played with net internals
this is an excellent reason to peek
inside and see how it works so we have
this separate pain for speedy that you
go into and you
see all of the open speedy connections
and I will actually you know with good
luck i will show you this okay so speedy
that I always just like a silly silly
note server that I'm running that is
delivering this hello world message over
speedy right so I'm running this and I'm
going to go to Chrome net internals into
speedy and you can see that speedy i/o
is listed as a speedy version 3 make
that a little bit bigger so we're
talking speedy version 32 speedy that I
oh and I can actually click on this
connection
there we go and we have our speedy
session and then I can click on the
speedy session itself and you can
actually see the exchange that this is
the stuff that flows on the wire so you
can actually see the speedy frames as
their exchange so for example we have
this sense stream remember we talked
about since stream which is the I like
to open a new request to the server and
we're saying it's okay so I'm sending us
in stream here all my headers host
method except headers all the rest of
this is what you would expect out of
regular HTTP requests and then we get
some 40 for data back so if you need to
debug speedy this was probably the best
tool to do that so definitely give it a
try so with that I'm happy to take
questions right as a quick summary and
there's a mic in there if you guys have
any questions but in the meantime a
couple of things right so just to come
back full circle the core goal for ecs
point 0 is to improve latency how are we
going to do that we're trying to remove
the bottleneck in in existing HTTP 1.1
implementation which is a head of a head
of line blocking we want to be able to
mocks multiple requests over the same
connection so we can open that you know
big and wide and get all the resources
as fast as we can it'll have all the
same semantics and in the meantime you
can use speedy to accelerate your site
so one question is well how what should
I expect on my site and the answer is it
really depends on you architecture of
your site if you have if you serve
majority third party content and you
have a tons and tons of widgets you know
you're not going to get as big of a win
but we have seen big improvements the
original goal for speedy was to make was
to accelerate the loading time by forty
to fifty percent and you know we're
thinking we think we're on track for
that your actual results may be may vary
but this is very cool because you can
enable this you know your admin can
enable this on the server and the site
just goes faster you haven't you haven't
done anything to the underlying
so that's the quick summary the slides
are online so if you go to bit ly HSP to
speedy you can find those there I mean
thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>