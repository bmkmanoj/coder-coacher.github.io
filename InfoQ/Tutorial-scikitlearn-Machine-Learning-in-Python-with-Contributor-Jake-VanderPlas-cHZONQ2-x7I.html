<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tutorial: scikit-learn - Machine Learning in Python with Contributor Jake VanderPlas | Coder Coacher - Coaching Coders</title><meta content="Tutorial: scikit-learn - Machine Learning in Python with Contributor Jake VanderPlas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tutorial: scikit-learn - Machine Learning in Python with Contributor Jake VanderPlas</b></h2><h5 class="post__date">2012-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cHZONQ2-x7I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Jake Vander Plaats I'm a
grad student in astronomy at University
of Washington and I'm going to talk
about scikit-learn which is an extension
of some of the Syfy and numpy utilities
that's been developed in the last
several years to do machine learning in
Python and so it's a it's a very cool
tool I got involved because I was doing
some some machine learning and some data
analysis for my astronomical research
and there were some tools that I need in
particular neighbors or tree based
neighbors searches things like that that
I couldn't find in the in the tools that
were available
there's the KD tree and SCI PI but it I
was looking at high dimensional stuff so
the KD tree and scale so I wrote a ball
tree code that accomplished my problem
and then I figured since I spent so much
time on that I might as well make it
useful to other people so I got in touch
with the people who had just started
scikit-learn at that point and submitted
my code and from that they they have
drawn me in and now I spend way too much
time working on this package but it's
great so if you go to I don't know if
you can see the URL up there but it's
scikit-learn org and you can you can see
everything everything that's there I
just want to real quick to show you a
few features of the website before we
move on into the other stuff if you
click on the examples menu up top this
is very similar to matplotlib we have a
whole gallery of machine learning
examples so all of this is done on
various datasets various machine
learning algorithms and if you if you
want to learn about machine learning
this is a great place to just click
around and see what you can find
for today's tutorial I I made a I made
my own branch of this website with some
added tutorial stuff this will be this
will be merged into the main
documentation sometime in the next few
weeks but the the web address is Jake
VDP github.com I don't know maybe I can
write that in and can I make this bigger
there we go
so that's that's the web address right
there and if you if you have that you
can you'll end up on this main page
right here scroll down to the second
section and section 2.3 as I could learn
an astronomical data click on that and
you'll be on this page right here so
there's a lot of material in this
tutorial I don't think we'll have time
to get through all of it today but I'm
thinking we'll probably go through
section the first section here which
well setup in spring I think you have
all that already if you had your little
if you had your USB key well go through
the second section on general machine
learning concepts and then skip through
this practical advice thing you can look
at that yourself and then there are a
few exercise exercises later that use
astronomical data to display how to use
some of those machine learning
technologies so why don't we start if
you click on that says inspiration I
think it is meant to say installation
but I still need to spellcheck all this
or read through it so if if you don't
have scikit-learn installed on your
machine then should probably do that
right now if you have easy install it's
on there pip it's on there if you use
the little the little memory stick you
should have scikit-learn so you can
check this by opening an eye python
whoops i python thing and click
make this little smaller import SK learn
so that's the head namespace for cycads
learn SK learn so we'll be using a lot
of that sort of stuff
so if does everybody have that installed
we're going to be doing a lot of really
interactive things here everyone's good
on that cool so the other thing you need
is the is some data files from this
tutorial if you got your little USB
stick it's in Word I copied this to PI
data on my desk cup if you're in that in
that folder from the USB stick just use
CD astronomy and you should have this
file here with a few different things so
we have data kind of hard to read here
isn't it let's see if I can make this
smaller yeah I can show you that if you
don't have the USB stick you can follow
the example here just do if you have get
you can get cloned this repo yeah you
can get cloned that repo and if once you
have the data there's there's a file
data which this has some of the
astronomical data that we'll be working
with
there's a file solutions and this is the
completed exercises the you know the
teachers answer key and then in
skeletons there's some of there's some
code that we'll be filling in along the
way to do some of these examples this is
everyone managing to find that yeah okay
cool
so this is a first page just has a bunch
about how to how to download the
datasets and things like that if you got
it off the USB key then you already have
the datasets there so let's go to the
next section general concepts so the
idea with with machine learning and
artificial intelligence is what you're
trying to do is write programs that have
they have tunable parameters and that
and that you in these programs these
tunable parameters are adjusted to fit
your data in the best way possible so
one way you can think about this a
common thing that you might want to do
is if you have a collection of points
that you know are of two different
categories you want to figure out a way
to divide them so so that if you have so
this this would be a training sample
where you know this is all class one
this is a class two and if you throw a
new point at it a new observation and it
lands somewhere in the space you want to
be able to predict whether that point is
in class 1 or class 2 this is a really
simplistic example but as the as the
datasets grow large the kind of things
this might be maybe you're looking at
webpages and your attributes are the
word counts on those webpages and your
classes if you're trying to you're
trying to classify this webpage as a
personal blog or a news source or you
know some other classification that
you're interested in learning about
there there are all sorts of situations
where this this general framework can be
can be extended to some really
interesting problems so this is a lot of
times for these examples will work in
really simple spaces just 2d because we
can we can plot that up and show these
bounding lines so in case you're you're
wondering this is a support vector
machine that did this classification and
all these plots in here are linked to
source code so if you click on it this
page that it brings you to will actually
show you
you the source code that was used to to
create that data to fit that data and
then to plot it so that's a useful thing
in this tutorial actually throughout the
psychic learn documentation website any
plot that you see you can click on and
it'll tell you how it's made so it's
pretty useful for learning the tools so
the question is given if you have this
data how do you use scikit-learn to to
learn some things to learn these
parameters so we're going to we're going
to talk about a few different things we
need to talk about how to turn raw data
into numerical arrays because if you
have things like text data where you
can't you can't do any machine learning
on it until you've turned that into into
numbers that the algorithms can
understand we'll talk about supervised
learning we'll talk about unsupervised
learning and the difference between
those two and then we'll talk about some
concepts of overfitting and some some
more detailed things so let's move on to
this section right here we have features
and features extractions so in in
scikit-learn the data model is that you
basically for any data you have an array
X so I'm going to do some random points
here let's make it a hundred by four so
so if we have an array that's a hundred
points by four point or a hundred
hundred by four what we have and in
scikit-learn this represents a hundred
points and four dimensions so the the
first dimension always tells you how
many samples you have so it might be in
our what we were talking about before it
might be how many web pages you have or
how many stars you're looking at or how
many different people's faces you're
looking at and the second dimension
indicates the number of features that
you have so any any sort of observed
feature that you have so as you're using
scikit-learn keep that in mind that if
you can if you can get a data set into
the form where it's n features by n
points sorry n samples by n features
then you can use these machine learning
algorithms so as an example of this
let's use the one data set that's common
for this sort of thing is the iris data
set so essentially we've there's this
data set that's out there that some
scientists took a look at some irises
wild irises out in the forest and
measured some features of these irises
so they have the sepal length the sepal
width the petal length and the petal
width so for each of these each of these
four categories there's a real numbered
value which is length and centimeters
and then the classes of these objects
are three different three different
species basically so the thing we want
to ask is given a measurement of these
features of this flower can we classify
the flower that we're looking at into
one of these three and into one of these
three species so we have some example
code up here and yes this would have
been better as an eye Python notebook I
should have should have gotten on the
ball on that oh great
okay so so what one useful thing we have
in scikit-learn is a number of data set
loaders so you always need data in order
to do this sort of stuff but data is big
so we don't want to ship it with with
the source code so instead we have we
have data set loaders so if I import
data sets and then do a tab complete
where you can see all the data sets we
have there's all the ones that start
with fetch our data sets so let's do
fetch tag complete twenty newsgroups
this is basically cat word data from
different news groups with
classifications based on the word on the
news groups all of any face faces is the
is a whole bunch of images of faces that
can be used for facial recognition yeah
there's several other data sets in there
so you can explore that but we're going
to do data sets got load iris and let's
call this data I'm so I got to put in
equals there don't I so this loads the
iris data sets and we can type data at
shape and see oh shoot it's a bunch
object I forgot about that so we want to
do X equals data X and I should just
look at my look at my crib notes down
here to make sure half this right so
data
yeah so we want X equals data data and y
equals data targets doesn't have targets
sorry about this guys yeah there we go
okay so now we can look at xscape and we
see that X dot shape is 150 comma 4 so
that means that we're looking at 150
samples with four different features
each so those are those four different
measurements so we were looking at it's
150 rows and each row is associated with
a single object in this case a single
flower
yeah that's right and the features are
the four four dimensions of that have
been measured for that flower and if we
look at Y dot shape we see that that's
150 right there so we basically have one
number for each for each each sample and
so these are that if we print well where
did my or my thing go we print Y we see
that these these classifications are
basically a 0 1 or a 2 so this is the
this is showing us the the species of
the training set yeah so we can do let's
do it this way so iris equals data sets
load iris the x equals iris data y
equals Iris target
so looking back at this if we look at x0
the first row of the array we see that
it's just an array of values so in this
case the guess the sepal length is 5
centimeters the sepal width is 3 point 5
centimeters etc the other thing we have
here is target iris not target
underscore names this shows us so for
each of these target values for each of
these target values they are associated
with the species so you have the species
names there so let's think about how we
can let's see okay so did everybody get
that get that iris data set loaded up so
if we want to we can I'm just kind of
going on the on the fly here but if we
want to we can yeah great excellent
magic so if we let's let's scatter two
of these just to just to see what these
look like we're going to scatter the the
first column of the matrix with the
second column of the matrix and let's
make the color equal to that Y right
there and we'll take a look at what the
data looks like so this is the kind of
data that we have this is two out of the
four dimensions of what we're looking at
yeah
so we can look at two of our four
dimensions here and we see are you guys
ready to look at the plot that over here
and what we see is that the they're
definitely they're definitely the
features grouped together in a certain
way right we is these blue points right
here over in this part of the space the
red and the green points are a little
closer together but you might see that
those we might expect that the other
measurements would help us separate
these out a little more so what what
machine learning is trying to do is
basically use those features and do this
sort of thing in an automated way if you
were a person looking at this data you
might draw a line right here and say
everything north of theirs is a blue
point or I can't remember which species
that that's part of right now but for
machine learning we want we want the
code to do that automatically we don't
want to have to look at plots of every
single dimension we want this to happen
automatically so let's do a little aside
here and talk about in this case this
was these were categorical categorical
features or these were like all these
these were real numbered features we
have a measurement for each category for
each feature you might also in some
places have categorical features so for
example if you measure that the color is
purple blue or red you don't have a real
number a continuous variable for that
but you just have categorical feature so
in that case what you what you might do
if you wanted to do a machine learning
algorithm on that data is you look at
you'd have these first four measurements
that we had before but you'd also add
maybe a fourth dimension that's one if
the color is purple and zero otherwise
zoom in here and you you could add a
fifth dimension that does the same thing
for blue or sixth dimension that does
the same thing for red so for example if
you have these categorical features
that's
one way you could divide it up another
way you could divide it up is to do have
purple be zero red be one and blue be
two or something like that in us in a
single dimension and though the way you
decide to do that kind of depends on the
problem and what what you're expecting
to get out so it takes it's a little bit
of art it's not just completely a
science doing machine learning so
there's also sometimes you might want to
extract features from unstructured data
so an example of that would be things
like a text document so how do you go
from a text document to some number of
vectors that you can plug into a machine
learning algorithm there are there are a
few ways you can do this if you you can
count the frequency of each pair of
words so if you if you have a text
document about the white house there
might be the pair the president all over
the place so each time you see the the
pair of words the president you make
that a feature and count how many times
that happens that's the called this it's
the so called bag of words way of doing
it so there are there tools in
scikit-learn for doing this sort of
stuff the documentation for the feature
extraction tools is not quite as good as
the rest of the package but that's being
that's being worked on right now so if
you have text data you can you can work
with that
there's actually some good tutorials
online for how to work with text data so
if you're working with image data for
example if you have a whole bunch of
pictures of people and you want to try a
machine learning algorithm to identify
those faces one way to do it is to
rescale all the images to a fixed size
so you have basically a certain number
of pixels per image and you can take all
the raw pixel values as a vector so you
have a 100 by 100 image and that becomes
ten thousand pixels then you have a
value for each of those pixels so in
that case you would have ten thousand
dimensional data and your
to get very high high dimensions you can
do more sophisticated things like take a
take a transform a wavelet transform
something like that you can there are
other things on here I won't talk all
the way through them but there there are
several different ways to to do into
data extraction on on images you can do
a similar type of thing for sounds you
can sounds you basically resample to a
common grid and you have a floating
point value for each of those each of
those areas so anyway you can you can do
machine learning on basically any type
of data so that's what we're trying to
say here and scikit-learn has tools
available that will help you do all of
that so once you have this data
essentially what what you're doing is
you're taking your your input whether
it's figures or text documents images
whatever you translate that in some way
into feature vectors so for each for
each sample for each object that you're
interested in you have a set of numbers
and you also have labels for this
training data and you use your machine
learning algorithm put those together
and you come up with a predictive model
so you can think of this model is like a
Python object and that's the way I could
learn to implement it and then if you if
you have a new sample so you're looking
you download a webpage and you're asked
you want your machine learning algorithm
to tell you what type of webpage is it
what is it talking about you apply the
same transform to get the feature vector
out of that you plug this feature vector
into the predictive model and the
predictive model has as a tool or has a
method that will get you your expected
label so that's basically the the
workflow of a supervised machine
learning problem I'm going to skip some
stuff here and show you that if you in
scikit-learn there's a certain syntax to
do all this stuff so basically in in
there's a there's a vectorizer method
and I'll show you a little bit about
that that'll translate
training text into feature vectors you
have a classifier and you call the fit
method on the data and the labels and
you get a predictive model use the same
vectorizer to get a feature vector out
of your new document and then you call
the predict method to get the new label
out so that's kind of the the workflow
of scikit-learn
and let me go back up here so anyway the
nice thing about that is that there are
there hundreds literally hundreds of
these learning algorithms available in
the package and they all have pretty
much that same interface so I've talked
to people who said I don't know anything
about machine learning what do I do and
I've by my main advice is usually to
find a data set and figure out the
syntax and then just throw your data at
a few of these classifiers and see what
they do and if one of them seems to work
you know you plot something up and one
of them seems to work then you can go to
the documentation and learn about what's
going on there so it's a kind of a
backward approach you don't need to do
all the theory but the theory is good
too so we have all that in the
documentation but we want this to be a
tool that people can just play around
with and learn machine learning so um
let's see where are we here oh yeah so
this one other thing about these labels
the labels can sometimes be categorical
features so things like what species of
virus it is things the labels can also
be be floating-point values so if we're
looking at categorical features we're
talking about classification which is a
type of type of machine learning problem
if we're looking at continuous values we
want to do something we want to do
it's called regression so for example
one of the exercises later on is a
regression applied to what we call
photometric redshifts so in in astronomy
we look at we look at galaxies and we
want to know how far away they are
and that that distance those galaxies is
correlated with the the redshift
basically the amount that the universe
is expanded well that light has been
traveling to us so you can use these
observations of galaxies and try to ask
what what is the redshift and this is a
really common problem in astronomy and
we it's it's something that's really
really important for some of these
upcoming surveys is finding how far away
these galaxies are so you can do
cosmology learn about things like dark
matter and dark energy so this is this
is stuff that the people are using in
practice so let's do a quick
classification example so we have our
our data here and I'm going to close
that we loaded our iris data we have X
which is a 150 by 4 we have Y which
which is length 150 and we're going to
use something called the the linear SVC
so this is a linear support vector
machine classifier so support vector
machines are a technique that have been
really really powerful in machine
learning and you can do you can do basic
linear models and you can get more
complicated from that so the way we do
that is we do from SK learn dot SVM
import linear SVC so linear SVC is our
class that our class that that
implements this method so oh yeah then
we need to instantiate this so we see LF
for classifier equals linear SVC open
closed parenthesis so we instantiate
this class and we have this classifier
instance right here so if you type clf
you see what it is it's a linear support
vector classifier model it has these
certain parameters the C that constant
is 1.0 that's kind of a weight that you
can adjust
- how how closely you fit points you
have a tolerance in your in your
convergence down at the bottom you you
have certain scalings you can choose
different penalty terms you can choose
different parameters to fit so there's a
whole host of things in there and if
you're if you're curious about what all
these are you can in your ipython shell
type linear SVC question mark and the
the doc strings are pretty good so this
is a linear kernel SVC have all the
parameters there
so in general that'll tell you how to go
on so we have our classifier there but
so the classifier has not been trained
yet we we have this nice classifier but
we haven't thrown any data at it yet so
the way that we throw data at the
classifier is du ciel f dot fit X comma
Y so X is our X is our input data Y are
the labels and we end up with a fit
classifier so if we type clf in this
case code let's look at the coefficients
hit when I do that these are the this is
basically the internal data model that
is generated in that fit that's a great
question I think the so underscore means
private usually and yeah yeah so this is
something that you that's a good good
question maybe we need to think about
the implementation details there but I
think the underscore at the ends usually
means something that was fit by the
model and so if not something that is
not something that you necessarily need
to look at because you're not gonna
you're not going to take that array and
do anything with it but sometimes it's
just nice to look
so they're the parameters fit by the
model that are used in the in the
classification later on so this is just
to show you that these are the
coefficients that we put in those 150
data points and it trained and created
this list of coefficients and these
coefficients are what are used to do the
to do the the classification later on so
let's say we have a new point we don't
know what the classification is so we'll
just do a 2d array X new shape oops
sorry
X u equals no B dot is array so there it
is there's our data and we want it to be
a two dimensional array even though it's
a single point because this this tells
us that we have one point in four
dimensions so if we're if we're curious
what the classification is of that new
point based on this classifier we do CL
f dot predicts X nu and then it tells us
0 so the this is a point type 0 so let's
see iris dot target names so that's the
species setosa so this is just a simple
simple linear classifier that did did
something very similar to what I showed
you before you know we plot up and we
draw a little line and we say this fits
there that fits there this internally
that that's what happened with this
learning mechanism ok so here's another
you might you might be interested when
you're doing this sort of thing rather
than just getting out a specific
classification value you might want to
get a an idea of what the probability is
of these certain classifications and I'm
not going to type this one out it's uh
I'm just going to go through it but you
can use some another algorithm
stick regression there's several other
algorithms that behave this way as well
where when you do your you do your fit
to the data and you get out your
parameters and then you have a you can
predict the probability instead of just
predicting the label you do predict prob
and you get out the for this value you
get out basically that it's 90% probable
that it's the first one it's nine
percent the second one and one one part
in 10 to the fifth the third one so you
can get out you can get out a probable
probabilistic classification as well so
so just in case you want to try some of
these out here's a several different
classifiers that you can use there's
logistic regression there's support
vector machines several different types
there's the stochastic gradient descent
classifier neighbors classifier Gaussian
naive Bayes so several things like that
and you can imagine using classifiers in
email classification language
identification news articles categorized
categorization yeah all these different
things looking at at face verification
and pictures like is this picture a face
is it not and what we're going to talk
about astronomical sources you can do
classification as well so we mentioned
that regression is is like
classification except instead of
learning these categorical features
you're learning some sort of floating
point value as something between between
0 &amp;amp; 1 inclusive perhaps and so there are
there are several regression models
listed here so you can you can try some
of those out if you're if you're
interested so let's talk real briefly
about unsupervised learning now so you
notice that what we had before was we we
took our we took our data we turn them
into vectors we use a machine learning
algorithm with the labels to fit but in
unsupervised learning we don't have
labels so the idea here is that you have
maybe you have a bunch of observations
but you don't know what they are and you
just want to get a better idea of a
better idea of what the structure of the
data is so for example with with
astronomical spectra we take we take
images of maybe a hundred thousand
points and we have we have a whole bunch
of observations of each of those and
we're curious what's what's the
relationship between all these we don't
necessarily know if we don't if we don't
use any physical intuition to know what
these points are how can we learn about
the relationships and this is an
unsupervised learning problem so you
have feature vectors you get the
predictive model and one with the new
when you put a new a new point in your
the same thing feature vectors put it
through the predictive model and you get
some sort of likelihood or clustering or
better representation lower dimensional
representation something like that so
this is this unsupervised learning the
other stuff we were doing with
supervised learning so that's just
another another paradigm of of machine
learning so one one really common way of
doing unsupervised learning is principal
component analysis so yeah why don't we
do this so if we do from SK learn import
SK learn dot P CA import PCA and what
did I do wrong
decomposition
it's a the same data set yeah
so again we have our these are our IRA
this is our iris data set so before we
we plotted just basically two dimensions
of this and we didn't we didn't take
into account the other two dimensions
but with PCA we can we can project it
down so that we can look at all the all
the information so PCA equals let's
let's fit this with two components and
will whiten it which means which means
we're basically moving all the data to
the same scale and just as before we do
PCA dot fit X but this time remember we
don't put in the Y value because this is
unsupervised so we don't know what the
categories are and PCA X nu equals PCA
dot sorry
cy equals PCA transform X so we're
transforming we fit on the data X and
we're transforming on the same data and
I need to type equals shoot there we go
oh sorry meet our we need our we need
our instantiated object which is
lowercase PC a so now if we look at Y
dot shape yeah so the this is if we look
at this model right here basically the
the fit is taking our data putting it
through the algorithm and getting a
predictive model and the transform is
taking our data and turning it into this
new representation right here so you can
do that you can do that transform with a
new feature into the new representation
or a new a new set of data or you can do
that transform with the original data
you trained on and you get this new
representation and in our case the does
that make sense
yeah yeah so our original data is as in
four dimensions and our next this data
is in two dimensions because we chose up
here we chose and components is too yeah
so this is nice right because we we
can't really do a four dimensional
scatter plot but we can do a two
dimensional scatter plot so let's do
scatter y : 0 1 : 1 and our color is the
lowercase Y which is our our
classifications ok so we're good to go
ok so if we do scatter Y again : comma 0
y : comma 1 and then our color is y
right here then we see that this is the
two dimensional projection of the data
taking into account taking into account
all the dimensions and then asking for
in an unsupervised manner what's the
best projection so this is interesting
because we clearly see that these blue
points are completely different than the
other ones and unlike our other our
other plot of this it does a little
better job of separating out the red and
the green points right so this is useful
because you can imagine you know going
from 4 dimensions to 2 dimensions is not
the most useful thing but imagine if you
had a thousand dimensions and you wanted
to visualize the relationships between
the data that'd be difficult but you can
project down to two or three dimensions
and see these sorts of relationships the
other way this is useful sometimes
people use it to speed up routines so if
you have something like say a text
document that you're you're breaking up
and you end up with a million features
your machine learning algorithm might
not scale very well to a million
features some of these scale is N
squared or n cubed so it's it's
difficult but if you can if you can
reduce that dimensionality keeping the
most interesting data then you end up
with something that will run a lot
faster
so that's how this can be useful
so that's unsupervised learning we went
through all that that's that same plot I
think yeah pretty close different colors
but same yeah it's a good question so
maybe one way of thinking about it is if
if you're reading White House documents
then you know that you know that in any
sentence that the word president
president shows up then probably the
name Obama is going to show up as well
so if you if you if you're just looking
for president and Obama and those are
your two features they'll probably be
really correlated if you plot the
occurrences of President versus the
occurrences of Obama it'll be this nice
big line so what that's telling you is
that the second feature doesn't tell you
very much about that sentence so you
have these two features you're measuring
and both of them pretty much tell you if
the sentence is talking about the
president so what the dimensionality
reduction does is it it can look for
those sorts of correlations and remove
the unimportant points so in the case of
PCA if you had a if you plotted a
president the occurrence of president
and occurrence of Obama you'd have this
nice straight line what PCA would do is
it would say well it's it's not this
direction that's most important or this
direction it's this combination that's
most important so it would take that
that one D thing and throw out
everything else and then you reduce your
data set size so that's that's kind of a
does that make sense yeah so there
you're essentially removing correlations
that aren't adding any information to
your classification yeah this is
actually in the this is in the
scikit-learn documentation so we have a
script here and this is part of our
example gallery so I just embedded that
in so it might be a little different in
the notebook style so yeah I talked
about some of this stuff
another dimensionality that you
reduction thing that you can do is are
not not to mention out a reduction but
another soup unsupervised learning
method that you can do is clustering so
for example we know looking at this
because because we've put our colors
here telling us which one of these is
which we kind of know where the clusters
of points are we know this is here this
is here this is here but if we if this
were higher dimensional data if we
weren't able to plot it like this you
might wonder if you could automatically
find the fact that these points cluster
together and they're separated from the
rest so that's another unsupervised
learning technique and one of the more
common ways to do this is with k-means
so k-means is an algorithm that does
this sort of thing and i'm actually just
going to close this just for the sake of
time I'm going to turn on doc test mode
right here
I didn't yeah
so ex PCA up there I called called that
why so let's do that and then we can
paste all this in again and so what
we're doing here is we're calling a just
like before we import something called K
means this is our classifier object we
just initialize the random state here
because K means depends on it uses
randomness and it's in its algorithm and
then we define our k-means classifier
object and then we actually did the
definition and the fit all at the same
time right here so what I did was I
actually I trained this k-means
classifier on the output of our last of
our last unsupervised learning thing so
this was the the pca projection the
two-dimensional projection of our data
and now if we want to where did I
where's this plot 2d come from so yeah
let's just let's scatter again yeah so
here this is what I'm looking for
k-means dot labels that's a that's a
length 150 vector so that this is the
labels that were given to us by this
unsupervised k-means algorithm so just
as before we can scatter our X PCA
: 0 X PCA : 1 C equals our k-means
labels and this is what we end up with
so let's see if I can what we saw before
when when we look at the actual labels
was I want
ruin that
yeah
trying to get two different figures here
I'll get it eventually
there we go so on one side we see our
k-means labels so this is what this is
what k-means thought our points were
without without any indication without
any training data on what the
categorizations were and then on this
side was our actual categorizations that
we know so you can see that as you might
expect k-means does pretty well when
things are very well separated but when
you have two different clumps that kind
of mesh together
you don't necessarily get that that's
because this is a this was an
unsupervised method just basically
looking looking at this data it knew
nothing about the intrinsic
classification simple I told it to look
for three - yeah yeah so that that's
another thing you can you can tell it to
look for however many you want so a lot
of these things have have tunable
parameters that you have to decide how
how you're going to set so it takes a
while but you can you can figure out how
much what yeah so second learning he was
asking about hyper parameter selection
and that's the term for choosing like
for it for instance how many clusters
you're looking for or how many
dimensions you're projecting to where
we're working on that right now on
getting some good hyper parameter
selection tools but in scikit-learn
right now there's not much built in
there's there's a little bit of stuff on
cross-validation but it's not very
mature yet so a lot of that you end up
having to do yourself so this is just a
list of some of the unsupervised
learning methods that are available
there as well so you can look through
that on your own time if you'd like
and so this one's fun this is this is
something that if you go to if you're on
this page and you click this link
there's this little script it's a it's a
little matplotlib GUI that does SVM
pacification so you can click you go
back back it's this little link right
here under two three to four click that
and then you can download this file yeah
this is just a little Python script so
if I save the file then I can let's open
up the new terminal put the file right
here I think yeah so you run this file
this is a an interesting thing that
helps you see how support vector
machines work so the right mouse button
lets you put in one type of point the
left mouse button put lets you put in
another type of point and then we let's
do a linear fit and click fit and you
can see that then it separates them out
this way and that in support vector
machines these points that are circled
right here these are the ones that are
the so called support vectors because
essentially the reason their support
vectors is because whoops I didn't mean
to add that one let's do this one again
click it so the reason these are called
support vectors is because it really
doesn't matter what's happening out here
right I can I add as many points as I
want out here and it's not changing the
classification but this one if this one
is perturbed or if there's another point
added near here then it does change the
classification so that support vector
machines kind of work that way they look
for the nice separable space between
these groups where they can fit a big
region like that so so this is a this is
a linear classifier and if you guys have
this open yeah if we put one point over
here that color then things get a get a
little less neat right so you can see
that it's it's kind of kind of tweak the
alignment and more more points end up
mattering as support vectors but you can
you can play around with this you
self and it's it's kind of interesting
actually to see what you can do so let
me clear that out so we've been doing a
nice linear fit here so we can we can
fit a linear model pretty well that's
sort of data but you might ask what
happens when a linear model doesn't fit
the data so well so if we have something
that looks like this
so in that case actually let me clear it
because it was getting a little messy if
we have a cluster here and then you know
some points there some points there some
points there this is something that
clearly should be separable right we
should be able to find a dividing line
right here but it's just that this is
not a linear problem there's no single
straight line that you can draw that
will divide those points out so in that
case you can you can make the model a
little more complicated instead of using
a little linear model you can use a
Gaussian model that's what this RBF is
right here that stands for radial basis
function so it's basically a Gaussian
model to the data and if we click fit
and see well it did it does pretty well
right so the the Gaussian model fits
some it's very very strange it's really
sensitive to these parameters right here
so we change some of these parameters we
can see what that does what if we do
that a little less yeah so you can play
around with this and see what these what
these support vector machines do in this
classification here so one thing if you
guys have this open right now one thing
that's interesting is asking about how
how can you fit an XOR problem so an XOR
problem is basically like this you have
four points with similar similar points
in opposite corners and the question is
can can this be can this be linearly
separated
can you yeah we can see that this is not
linear linear linearly separable if we
do that let me go back to the
hyperplanes so basically one one Lin one
line doesn't really doesn't really do
much there but if we use a polynomial
kernel for example it's easier to see
these these types of kernels with with
the surface we can try to fit these
things so if you have this open there's
a kind of an exercise listed on this
sheet right here or it where is it so I
did play around with this a little bit
and see if you can construct a problem
here with less than ten points so you
have fewer than ten points total where
the predictive accuracy is 50% and the
accuracy is shown in the in the terminal
where you see if I can get these both
open right here in the terminal where
your where you opened this you can see
the accuracy right there so if I fit you
know you have some there and that
accuracy is a hundred percent did said
you guys want to play around with that
or do you want to want to move on play
Fernando said play so yeah see if you
can see if you can define set with those
ten points where the best linear model
so you're you're clicking right here has
an accuracy of 50% I'm I tried to fit
that's 100 percent that's 80%
you
it starts to look really messy after a
while
so anyway when you if your best if your
best accuracy is 50% that means that
essentially your classifier is doing as
well as you could just randomly guessing
right so if we click over to the next
section here this is a the this next
section is kind of practical advice and
this this has some really useful stuff
in it about basically overfitting under
fitting and and how you can use
cross-validation and tools like that in
order to to understand what's going on
I'm not going to go through the details
of it if you want to if you want to read
about that you can you can check out
this tutorial later but I want to give
you essentially a feeling of what it
means to be over fit or under fit so
this is an example a simple example
where you have house size versus price
and the data points the exes and all
three of these figures are the same and
we're basically fitting a polynomial to
this data and you can think about that
polynomial as being our our function
that our function that's that's learning
the shape of the data right so the
number of parameters you use in the
polynomial is really really important if
you use just a single parameter that
means you're doing you're basically
fitting a line to this data and you can
see that this is this is under fit if
you were to you were to start adding
other points where this data looks like
it it is you would have a lot of error
right because this line doesn't really
fit the data very well D equals two is
getting closer if you if you added more
points along here you'd expect they'd
probably fall somewhere near that line
right so this is kind of the happy
medium and then D equals six over here
is over fit
so what overfit means is that you have
too many parameters for your data and
for that reason it can fit your training
data perfectly but for example if you
added a point right here which would
make sense in light of the the trend of
the data right yet a point right there
that's going to be off by a factor of
you know a thousand from what the fit
says
so it's really important when you're
when you're doing this machine learning
stuff to to get a handle on how well
your classifier is or ruber aggressor is
fitting the data because if you're under
fitting then you'll get you'll have a
lot of error when you try to predict a
new sample if you're overfitting you can
have even more areal error when you
predict a new sample and you want to be
able to land somewhere in between so
just to give you a preview of all this
stuff there are ways to do this with
cross-validation where basically you're
you're plotting the degree of fit versus
the error on on the test set and the
training set you can use that to decide
where where your classifier is going
wrong you can plot these sort of curves
with the training set size I'm just
going through this quickly but you can
look at it later if if you're interested
in how to apply these machine learning
algorithms and practice I'm on two point
three point three yeah practical advice
so yeah if you're interested if you're
interested in learning more about that
that's a good place to look before you
start applying these things to data
because there's some important stuff in
there but I want to go to the stuff that
I love which is in this next section two
three four oh no oh there it goes I
thought my image wasn't loading this is
this is an example so for the last do we
have about half an hour or so left
fifteen minutes something like that for
the last little bit I want to give you a
chance to actually try some of this in
practice and what we're going to do is
use some astronomical data so what we're
going to look at is scroll down here in
astronomy often we want to be able to
distinguish quickly distinguish between
different types of astronomical sources
in order to figure out if we want to
follow them up or not and you know send
it if we want to slew another telescope
over there and get more detailed
information and these are some examples
of
sources it might look better on your own
computer screen it's a little the
resolutions not so good here but we have
on the left this is what a star looks
like in a picture you see that little
dot at the center of the screen this is
what a galaxy looks like so you have
this nice diffuse thing and over here
this is what a quasar looks like and a
quasar is it's essentially the a
supermassive black hole at the center of
a very very distant galaxy and because
these black holes can weigh up to about
ten billion times the mass of the Sun
they have this intense amount of mass
and tense amount of gravitational energy
and they can outshine the entire galaxy
that they live in so the net result is
is that you get these very very distant
objects that are way too small to be
resolved and we see light from them
that's about as bright as the stars in
the far reaches of our galaxy so in a
single image all you see is a little
point source just like you do with a
star so the question is can we design a
machine learning algorithm that will
tell us whether we're looking at a star
or at a quasar this is important because
you want if you're if you're interested
in studying quasars you want to be able
to look out in the sky and find these
and then use your your rare and
expensive telescopes like the the Keck
10-meter telescope things like that and
only use that time on the objects that
you know are already interesting or that
you know are going to be interesting
with a high probability so that's what
this example is about and essentially
what we're looking at the data that
we're going to look at for each of these
is the colors through different filters
so what I have plotted up here is this
is the example of a spectrum so spectrum
is basically this is the wavelength of
light so the color and very small
deviations are very small bins and this
is the flux as a function of that that
wavelength so this is what a spectrum
looks like and if we if we had a
spectrum this for example is a bright
blue star it's actually the spectrum of
the star Vega in in the constellation
Lyra and if we have
spectrum of these objects we can tell
that we can tell the difference between
a star and a quasar pretty easily but
these spectra especially for faint
objects take a long long time to to
observe so instead what we do is we look
in these filters in these pass bands so
you can imagine that we stick a filter
on the telescope that has this width in
in spectral space essentially what it
does is is it integrates everything
inside this filter and returns a single
value so these are the filters from the
Sloan Digital Sky Survey it measured in
the you ban giba and our band I band and
Z band
so for each for each object we have five
different numbers yeah all of the all of
the photometric data yeah and then there
there also is a component of SPSS that
also have spectra as well but there
there's something like how many do you
know how many photometric objects there
are is something like eight eight or
nine million photometric objects and
under 1 million that's more than that
it's it's in the tens of millions of
photometric objects and about a million
spectroscopic objects so we have
basically a data set of million points
that are very well classified very well
observed with this sort of data and then
we have a data set of tens of millions
of points that have much sparser data
much much less data but we can so we
want to use we want to use the well
observed things as a training set so
that we can classify the stuff that we
don't have the good observations of so
it's a well-posed machine learning
problem and if we look this yes let's
take a look at the data right here so if
you are in your where's my stuff pie
data astronomy so you're in this
directory that was on the on the USB
stick
and we have we have some files in in the
data file in the data directory there so
let's let's import whoops
I need ipython it's not going to play
nice with both screens so I'm just going
to go ahead and copy copy this because
it'll make it easy and you can do the
same thing so we get our train data and
test data set train data dot shape this
is a this is a record array so we have
five hundred five thousand training
objects and what we do train data II
type names we can see the different
things that are that are in that they're
not oh yeah if you if you yeah if you
clone the repo then you can go to you
have to have to fetch the data so if you
go to data SPSS colors you can python
fetch data and that will download it
from my website and i already have it so
it's there
so we're going to set up our training
data and for you we can use this code
right here I'm just going to I'm just
going to copy and paste it so we can get
through quickly our Y train our X test
and our Y test so we have the training
data that we're using which is a for for
features the test data which also has
four features and then we have our
predicted are true observed labels Y and
then these these Y tests these are
predicted labels from from the
literature so we can compare our results
with what what people are in the
literature are getting with more
sophisticated values so we're going to
use as a first step we're going to use
the naive Bayes algorithm so this
basically takes these clusters of data
and says this looks like a Gaussian that
looks like a Gaussian and which got
which Gaussian is the is the test point
closest to I'm just going to run through
this too so we we important naive Bayes
we instantiate our object we do the fit
as before and we do the predict and now
we have some predicted values so so if
we want to if we want to test the
accuracy of this we can print something
oops if we if we want to type the
accuracy we can ask how many of these
points how many of the the tested labels
and the predicted labels are equal and
divide that by the total number of
points and we get a percentage so the
percentage is something like 60% so this
is this is a pretty simple algorithm it
seems not to have done very well but
there's there's something that you need
to be careful about here because the
data is the data is very
it has the objects with why test of zero
there's a hundred eighty thousand and
with why tests of one there's only
thirteen thousand so basically what
we're looking at is there are there 186
thousand quasars are sorry 186 thousand
stars but only thirteen thousand quasars
so just looking at the accuracy won't
really tell us what's going on in this
case we're just asking how many of the
points agree divided by the total number
of points how many of the how many of
the classifications agree yeah they both
say star or both say quasar so that's
just saying that the test equals it
predicted so one one thing that's
helpful here is to look at the since we
have this skewed data set we want to
look at the true positives and the and
the false positives and essentially what
it helps to compute these things called
precision and recall so precision says
of the things that we labeled quasars
how many of those are actually quasars
and we can compute our precision here
and we find that that's only about 14%
that we've of the things that we've
labeled quasars only about 14% of those
are correct so basically our algorithm
is just trying to basically labeling
everything a quasar almost and and it
does really well on the recall it it
correctly identifies 95% of those
quasars but that's because it's just
calling everything a quasar right so
that's that's where this precision
recall score come in you don't really
just calculating that accuracy of 60%
you don't really see that this is the
problem that's going on so you can if
you don't want to do all those functions
yourself you can actually we have this
thing in in scikit-learn called metrics
so you do I'll just type it here from SK
learn import metrics
and then we print the classification
report and it tells us this so for the
stars the stars were 99% precise but we
only get 60% of them out so every we 40%
of the stars we're actually labeling
quasars incorrectly quasars were getting
we're labeling them most of them
correctly but we're also labeling a lot
of scars incorrectly so this kind of
gives you it gives you an idea of how
your classifier is doing so did you have
your hand up or no so basically we can
do better and for the last little bit
what I want you guys to do is click on
this exercise one right here and this
will walk you through this will walk you
through a better classifier that that's
based on Gaussian mixture models so what
we have here is in the skeletons
directory let's go we're right here so
in the skeletons directory we have a
couple exercises so if we open skeletons
slash exercise 1 then there are a bunch
of things in here that say to do and
this so basically walk this this will
walk you through the process of creating
a Gaussian mixture model classifier that
hopefully will be much more
sophisticated than our Gaussian naive
Bayes and do a lot better at the star
and quasar classification so I think we
have just a few minutes left 15 minutes
left so if you if you'd like to do this
if this sounds like something you want
to tackle you should be able to to make
some progress on it and I can walk
around and and help you if you're if
you're working on that so the weight at
the top of this page it tells you how to
run
this you go in Python and run workspace
exercise one py and then the first
argument there is just the location of
the data that it needs and it'll it'll
load all the data itself it'll it'll
split it into the training and
cross-validation sets that you need and
it's only these little places down here
that say to do that you get to do these
things so I'll leave it to you and feel
free to grab me if you have any
questions about how to implement this
yeah
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>