<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Disco MapReduce Framework | Coder Coacher - Coaching Coders</title><meta content="The Disco MapReduce Framework - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Disco MapReduce Framework</b></h2><h5 class="post__date">2012-03-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YuLBsdvCDo8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm gonna talk about disco MapReduce
framework so I Travis did a great lead
in this morning saying that Hadoop kind
of is all is the end all in them in the
MapReduce world and I found another one
disco I don't work in the disco project
but I do use it from time to time so I'm
like Peter asked me to do a talking map
reduce I figured I would go this way
because disco is a Python MapReduce
framework um it's actually Python and
Erlang will talk a lot about that in a
minute and kind of how it all works but
um it is it is it kind of a nice
alternative to Hadoop and um has a lot
of the same features so I'm just gonna
I've got a bunch of slides and some
examples that we'll walk through it's
this isn't gonna be interactive I'm I
trying to get some AWS stuff set up and
just ran out of time to make it really
interactive so people you log in and
play with it but um but it's pretty easy
to set up so I'll show you that so to
get started on MapReduce if your has
anyone here first of all use disco
Hadoop okay so I got something to do
people and so so yeah so we'll talk a
little app reduce and get started um as
you probably heard MapReduce is you know
the OS for distributed computing you
know Google actually has a patent on it
which is always fun fun to remind
yourself of when you start using this
that any day now you might you might
have to stop or pay for it but I doubt
that'll happen but I'm it is sold is
something that you know works on large
amounts of data of data distributed
across thousands of commodity computers
and I'm I'm kind of throw that in there
at the beginning to mention this because
one thing I've actually learned and
you'll see it kind of as we get to the
end of this too is that um that
thousands of commodity computers part is
kind of important for really getting
MapReduce um getting the most out of it
but the overall idea of MapReduce is
pretty straightforward you have a data
stream coming in with a number of
records in it oftentimes it's just a
line from lines for a log file or
something
you take it the mapping the mapping step
takes those and returns a list of key
value pairs that are derived from that
from each record there's a potential a
possible sorter or partitioning step
that happens next
Hadoop does a big sort in the middle um
as we'll see with them with disco it
actually uses partitioning and then an
optional optional sort but anyway your
you take the lit you take the M the key
value pairs from your math step and and
bucket them somehow and then
all of those buckets on to a reduce
phase which can go through and either um
you know sum up all the numbers and give
you a single number or it can actually
give you a list back and then you can
feed those lists back into a new map job
and chain them together forever and have
something that looks a lot like Lisp but
is distributed and big so it's um that's
the basic paradigm it's actually really
simple if you kind of look at the data
flow here one of the big things with
MapReduce is you really are trying to do
this on in a distributed environment
across a fairly large across clusters so
so looking at looking at the details
here a little more you can kind of see
them breaking this up into a number of
different mapping jobs on the idea here
is all the mapping jobs are
embarrassingly parallel jobs you can
operate on the input stream completely
in parallel so you have a bunch of
records fanned out to different mapping
jobs which then spit back the key value
pairs partition them and then feed them
into the reduce phase and give you each
reduce phase gives you a result and most
MapReduce engines I'm kind of give you
the results of separate files from each
reduce phase and then you know you just
I thought or Perl or something to put
them back together and this goes no
different than that you end up with
results from each reducer that you work
with at the end kind of looking at it
from the job job distribution
perspective it's it's kind of what you
would expect you have mapping jobs on a
number of nodes um what's interesting is
a lot of them at Hadoop and disco both
um don't really rely on don't require
you to have the map and reduce jobs in
the same node so there's a lot of
network traffic that actually happens in
between these there are ways to kind of
force things to stay locally but when
you're when you're doing this this is
it's assumed that the mapping jobs and
the reducing jobs may be on different
nodes so the streams are kind of merged
at the network level and passed out to
different nodes I mean a lot of the
intermediate results might be stored in
the local filesystem
or the you know the distributed file
system so HDFS or DVF festival talk
about that in a second but the notion of
a distributed file system has actually
underlies all of these and I and that's
kind of a foundation and I think I'm
Travis actually mentioned that HDFS is
what most people are actually using the
noob for is the distributed file system
rather than the actual MapReduce part
and the whole idea of the distributed
file system is on on both both ones for
disco and and Hadoop are you basically
take your input files chunk come up
across a bunch of clusters have a few
redundant copies so you might have three
copies of each file split up
across the across the clusters and then
it can move your mapping jobs to where
the files are so you usually have your
mapping jobs it's fairly large are your
files in fairly large chunks so I think
last time I used to do it was 64
megabytes it's kind of the smallest
recommended chunk size for for HDFS and
often times when I work with this I have
fairly large files in my domain and I'll
just use those whole files um they're
you know hundreds of gigabytes and
you'll just kind of put those across the
cluster and then move the computations
over to them but I'm the whole idea of
that distributed file system is kind of
integral to how this how this works is
that you have the idea is is that you
have a fast note or faster just at
faster access on your local nodes than
you would if you were doing a big San or
you know kind of the the other storage
alternatives and um there are some
caveats there I'll kind of touch at on
the end to that we've learned from doing
this in a number of different
environments but but overall it's that's
that's the basic idea and it's pretty
straightforward and it's easy to easy to
deploy these on a cluster easy to deploy
them on Amazon and they're pretty easy
to work with
um so disco is the one we're gonna I'm
gonna talk about all for the next hour
so and disco is a MapReduce engine built
in Python and Erlang so it's actually
not pure Python the distributed
processing part is all handled by Erlang
so they actually use the Erlang court to
fire off um jobs and all the nodes and
then all the client code is actually
done in Python so you write your mappers
and your reducers in Python and those
get distributed out to out using the
Erlang distributed programming
environments and then they fire up local
Python instances on the nodes and run
everything in Python but the actual
distribution is all handled with the
Erlang engine and it's actually um I've
been through the code a number of times
trying to track down various
installation issues at different times
and the Erlang code is pretty small I
think it's only about six thousand lines
of code it's pretty easy to track
through and the Python code is about the
same - there's not a lot of them it's
not it's not a fairly large code base to
do all this which is kind of what you
would expect I mean if you kind of think
about MapReduce it's not that
complicated and python working on um you
know kind of with the lists as a lists
and tuples has kind of main data types
there's not a lot of a you know you
don't have to write a lot of
infrastructure to move data around using
those those gathers paradigms so this go
actually though the whole pipe
idea works really well in here and when
you look through the code its
implemented a lot kind of the way you
would expect it to when you start
writing mappers and reducers sum which
is which is always kind of neat to see
it does have a distributed file system
called D D F s which is Disko
distributed file system and D D FS is a
tag based distributed file system so
there's no file hierarchy it stores
everything as either files or chunks and
you basically when you when you load up
a file the first time you give it the
default tag for that file and then
afterwards you can have more tags
pointing to those files or the chunks
that you're doing so I'm the example I
have here is I'm one one that we do we
have them I'll talk a little bit about
our domain in a second but we do gene
sequencing is what we do so we have
these files with a lot of sequencing
reads inside of them so I'm in this case
you know I have a bunch of sequencing
reads that are stored and the and the
main tag really is just the the
instrument run name so what where that
came from and that's the easy at the
canonical way to get back to any set of
reads but then on top of that you know
we might have different experiments we
might actually want to um you know tag
them by instrument tag and by the user
the person who put them in so we can
build these nice tag hierarchies group
those by projects so then what you want
to when you want a collection of files
out of this you just give it one of
those tags and it'll go through and
resolve all the files associated with it
so if you want to perform a map
operation across you know all the
sequencing runs and you know Project X
you just say project X as your input and
then it will take care of finding all
the files for you and feed those to the
mapping jobs and it's a it's it's
actually nice I if you've used any of
the other kind of columnstore type
databases we're using tags it's similar
you end up building these schemas for
your keys you know that oftentimes have
lots of them colons inside of them so
you can you know trace down and and use
that but it's it's a nice way of doing
this um it's pretty pretty easy once you
start putting your data in there so
really this is a simple D DFS example
right here as a bunch of code examples
in here I I'm not gonna run any of them
but I try to get as much in here and
this is loading it that's just the DDMS
s command you can chunk it up so in this
case we're loading the Holy Grail script
just into it then you can give it a
command the blobs command shows you
where what when I'm what what objects
are actually loaded into your data
data store and then you can use them to
have a cat X cat so you can actually go
and stream the files back to your
console and there's a whole lot of other
features you can do on here to manage
your files um it does use gzip um if
you're chunking data if you're not
chunking data it stores it in its native
format and that's useful to know if
you're dealing with especially binary
data we actually deal with a lot of
binary data and these set up so we don't
tend to chunk it but it is ordinates
data format when you're doing that way
so you don't have to worry about lions
getting rewritten or anything weird so a
simple example um
word counting is hello world of
MapReduce so if you've ever tried to
dupe or anything else you've probably
seen this example this is um this is
word counting and disco right here so
you can see um
so you can see it's pretty it's pretty
straightforward you define your mapper
and in this case it's just a simple
generator function so we have the map it
takes a line in and there are some
parameters some runtime parameters you
can pass around and in this case we're
just going to go and take each word in
the line so we're gonna partition the
split up the line into a bunch of words
and then for each word we're just going
to return a count of one so that's the
mapping step so what you get out of the
mapping step is a stream of words with
the number one next to them and so what
the reduce step is gonna do is it's
gonna take those words and the count
associated with it and some of those
counts so in the middle here is where
you actually um so you're using actually
we're doing the sort inside here so
there's some this.kb group sorted it err
is actually going to be sorting the the
words for us and then we're gonna get
one word with the counts out you can sum
the counts out some accounts and that'll
give you the total word counts in the
document we were looking at so really
really nice and simple I'm it's Python
so it's super easy to read you can just
write it the way you would all of this
lives in one file so it's it's pretty
self-contained when you run it you just
create a new job here give it a name
give it some input one kind of nice
thing is they have a lot of different
input types that you can use so you can
put um HTTP in front of something but DD
FS in front of it to kind of tell it
where to get the data from and so I know
it knows a couple different protocols so
in this case this is a DD FS load right
here the one that's commented out so I'm
just giving it the Python grail script
tag so that is Becky key value pair I
used for
that one or I can just tell it to go get
it from the source and in this case
it'll just pull it off the web right
right is that for the mapping job it'll
just load it stream it straight from the
web into the mapping job and you don't
have to use DD FS at all and then on you
set up the map job the reduce job tell
it to save some results and then just
wait on it and when it's done you have a
result iterator that you get out and
just print them out so um these are the
results that came out of out of this run
so you can see if you just look at the
top ones there's all the ARDS
from the script which actually I thought
they said Ardmore in the movie but they
did say it a lot of different ways so so
that was them that was an interesting
thing in there um and then you can see
those sort of the work they actually
sorted by count you know those almost
always the most common thing and when
you do word counting and then Arthur you
know the actual label they used for the
script lines was the next biggest one so
that's the that's a really simple really
simple example of using disco and really
that's actually all you need to know to
do most disco applications right there
is just write a map or write a reducer
think Python when you're doing it and
the whole system handles everything else
magically for you which is really nice
I'm gonna do a little aside here and
just kind of catch you up to the domain
I come from cuz the rest of the examples
are all gonna use stuff that I work on
daily it was for me it was a lot easier
to just take a lot of stuff we're
already doing so I work in biotech
next-generation sequencing is what we do
a lot of it's anyone here in biotech or
sequencing okay so one person so I'm so
so really quick the stump speech for
this is I'm really simple um we did the
Human Genome Project it kind of night
you know throughout the 90s about 10 to
12 years to time thousands of sequencers
billions of dollars generated coverage
of one human and only about only about
7x coverage of one person so we have
about 21 billion bases were generated in
that project today we can it's a little
different so we have um high-throughput
sequencing instruments came on the scene
around 2006 and the original ones kind
of took about two weeks generated a
little generated about the same as the
human genome project today they're doing
about 100 to 200 all the way up to 600
Giga bases of data per single
about five to six thousand dollars I'm
depending on the discounts you're
getting there's benchtop ones too now
that basically can do similar things
actually do way more than the Human
Genome Project in about four hours
turnaround time single instrument for
about five hundred bucks so this has
really changed everything on but what's
interesting about this is this it's just
this sheer amount of data so if you can
imagine 100 Giga bases of data is a
hundred gigabytes of ASG CS and T's
right there so you're getting very large
data files off the instruments um you
know we see a company you know one we're
running the big instruments you know we
can run those four times a day and you
know we can generate you know four or
five terabytes a day off these
instruments just off of a single
instrument so it's a lot of data kind of
give you an idea of what what the data
actually is it's it's over here on the
edge so if you kind of think about what
what the way sequencing actually works
is you um take your DNA and you're in
your cell from your sample you turn
chunk fragment it up into a bunch of
little pieces I'm a couple hundred bases
long and then those are what you
actually sequence and then you have to
put all those back together so what you
get off the instrument is actually a
file that looks exactly like this that
has on a little label for each sequence
that it found along with the actual
sequence that it read so on you get
basically all of these really some
fairly short strings you know usually 50
to a couple hundred bases long as what
you get for each read off the instrument
and then all the algorithms that you do
afterwards go and put those back
together and tell you how they actually
map back to the human genome reference
or whatever species you're working on so
it's it's a real big data problem can I
say big with a capital B because it
really is you know we deal with
terabytes on a daily basis so it's a lot
of data and it's it's one of those ones
that um there's no way around it either
at the the amount of data we have to
generates driven by the biology it's not
driven by what the instruments can do or
anything else if you want to ask a
specific question on a human on a human
so say you want to find all the
variations between you and the reference
you have to generate about fifteen to
thirty x coverage of your genome so
you're right away looking at you know
kind of that forty forty five to ninety
Giga bases of data that you have to
generate which when you're actually
working with it you it explodes a little
bit so you've got a couple hundred
gigabytes of data for a run that you're
working with and we can generate that
fairly quickly and this is kind of
slide from showing people at our
instruments can do which is actually
getting better and better every every
every month and that's kind of my point
here is that it's probably better just
to run away now and forget about this
problem because there's too much data
but um but it's we do get a lot of data
and it's kind of fun to work with it I'm
so I'm so I'm going to use it for the
rest of this I'm gonna use some
sequencing data just as kind of examples
of how we can do different things that
disco so this is going to show some
techniques in disco but also you know
kind of show it's not gonna go really
into biology at all but we'll talk about
some of the some of the some of the day
you'll see some of the data in here so
so the first thing that you'll kind of
notice is if you look at you know the
files I showed you before the sequencing
data comes off the instrument it's not
single line based and a lot of them in
most the default for MapReduce engines
is just to feed you one line at a time
and since each step in a map phase is
essentially it has to be an atomic step
you can't really remember what the line
you saw before was a lot of our the data
that comes off the instruments at is
actually span so I'm two to four lines
depending on the format we're using so
the first line is the ID then in this
case you get the sequence information
and you have a plus and then you have
the quality information so you actually
have four lines here that is really what
a single record is so if you want to
work on this using disco you'll have to
write you can write a new input stream
reader that will actually go through and
take this pull out every four lines and
then return it as a tuple to the mapping
phase so now the mapping phase instead
of getting a line is getting a tupple
that has the the sequencing data
reformatted in a way that's pretty it's
easy to work with so taking in the four
lines up here and then returning the
tupple right here and then you end up
with a stream of single single tuples
that have all the information that you
want to work in so using the custom
input streams is kind of the first thing
that you'll find yourself doing often if
you're using disco and have anything
other than mike really straightforward
log data that you're parsing if you have
complicated data that works in that and
I'll have an example in a little bit
that shows how to do this with binary
data too so if there's you can you can
work with pretty much anything you want
and it's really just how clever you are
on writing your input streams so
the next thing is that sort sorting step
that happens in the middle and this is
kind of where disco differs a lot from
Hadoop so Hadoop by default does on its
mapping phase then all of the all the
data is pushed into a sort phase source
all the data and then gives it back to
you and then feeds the sort of data into
the into the reducers disco decided to
do it a little differently um they don't
actually sort in the middle I'm sorting
is completely optional and my default
they don't so they give you all the keys
unsorted so you saw in that first
example with the word the word count we
actually have is actually sorting them
kind of as I got them in the reducer and
of course if you've got a very large
data set you probably don't want to do
an in-memory sort so so that's not
always an option but the other thing
thing that's that's worth noting is when
you're working with the mappers normally
everything gets fed to a single read
reduction step unless you have a way of
partitioning those streams coming out so
and this is what um the way the way that
disco does it is it has a partition step
in the middle so as soon as the map step
finishes so I'll walk through this in a
second um you can it you can basically
feed feed the key from the mapping step
into our partition function and then
return an integer and that tells you
what partition you should be in so what
I'll do then is it'll create the number
of partitions you have it will create a
reduced job for each one of those
partitions and then so you can basically
do some data parallelism on your reduce
phase - as long as you understand how to
partition your data so um so so in this
case right here what I'm doing is I'm
just taking all the reads off of the
instrument and we're going to find the
frequencies of basis at each at each
position in the reads so in this case
the reads um we're off of an instrument
that only produces 50 base pair reads so
we want to see you know did you see you
know do you see how often do you see an
A in position 1/2 and you see a C in
position 1 and then a G and a T in
position 1 and eventually there's the
results right there but it's pretty
straightforward so so the really simple
way of doing this is for the position I
I wrote a map function that goes through
and it just takes the record so it takes
the sequence coming in that's that's the
entry one in the in the tupple that i
fed is the actual sequence and it just
enumerates the position in the base and
returns position and base into the
stream so this is kind of like the word
counting except in except we're feeding
on the position along with with the bay
and then what what I do is um figure out
how many partitions we're gonna do
that's what the NP is and then the key
it gives you is the first thing here is
the first element in the tupple that we
returned from the the mapping mapping
routine so in this case it's going to be
somewhere between 1 and 50 so just um
kind of modding that by the number so if
I give it 50 partitions you know will
actually have 50 reduced jobs that'll
that'll be um be fed into here and then
every every position will have his own
reduced job to go and figure out the
base frequencies at those positions and
each of those reduced steps then we'll
get we'll get a base out of it and the
position and if they're all if it's
actually sort of that way you'll
actually see all of them I'm in this
case I think I haven't set up to handle
seeing any any number of any number of
positions in here so this is kind of the
more general case right here but I'm but
then it just goes and it counts for each
base that it sees you know every time it
sees one at a at a position it just adds
account to it and you end up with the
nice summary plot here and if you've
ever done human genomics you'll you'll
kind of know that this is pretty much
the the way the bases are distributed
across the genome and and what you can
see here this is actually you know more
for internal QC at the beginning of the
reads you can see some bias in specific
directions and and this is just kind of
an artifact of the way that the
sequencing actually works that you um
you preferentially um sequence in when
you go and split up the fragment things
you kind of preferentially fragment in
certain areas and you can see that a
little bit here what's going on and
sometimes at the end - you'll see things
happen there so so that's basically
partitioning an example of how you would
use it in here so so for this talk um I
wanted to go a little further than what
we do so we usually when we don't use
Disko all the time but when we do we
have it running on our internal clusters
and I wanted to see how can we move this
into the cloud can we actually set up
something like Amazon's elastic
MapReduce using disco I'm you know if
you if you kind of Google disco and
Amazon you know you find one or two hits
with people who tried it but they never
posted anything else and on the disco
page it says sure you can do it but
doesn't give you any examples of how to
do it so I wanted to figure out how to
actually set up disco on AWS and be able
to create clusters and use them you
easy to to set up disco jobs and then
pull data from different sources and
actually process them all on the cloud
so so what I did have it's anyone here
who's used ec2 before okay and I'm star
cluster and guys use that so so so this
is what I ended up doing I'm just want
to see who else knows because I was
learning all of this as I was doing this
so I always looking for more feedback on
the right ways but this this was
actually really easy so star cluster is
um basically it's it's out of MIT and
it's a tool for setting up clusters on
ec2 so what you what I'll let you do is
you create your machine you can create a
custom mesquite machine image and then
you can basically tell it to create a
cluster for you and it'll go off and you
tell it how many nodes you want it just
allocates everything for you I'm it's
it's it's actually a nice tool and one
of the other reasons I use it here is
star cluster is all written in Python
too so we're actually showing how to do
an entire stack entire map reduced a QZ
pretty much entirely Python tools but
when you're working with them so if you
work with machine images on Amazon
Amazon has a bunch of default machine
images that kind of have some basic
things on it star cluster provides a
couple machine images that have a few
more scientific tools so ipython is
actually available on there's numpy CyHi
and there's there's some other tools
that are common scientific computing
tools that people use a lot or on their
images but disco wasn't on their image
so the first thing I did here is
actually created an image that had disco
and the few other bioinformatics
libraries that I needed to use on here
and it was pretty straightforward they
actually have just a little command-line
tools where you go fire off a single
node instance so that little s1 is
telling us to do one instance tell it
what you're using is your starting
instance so I just use one of their
default ones is that or is their base
AMI and then the instance type is
actually the size of the the type of
instance on the of compute virtual
computer resource you're using so they
have like the small ones and the large
ones in the extra-large and the high
memory so that's Amazon things right
there so you can kind of pick what what
type of machines you're gonna target on
here and and this one I just use the
small one time I was trying to keep the
price down on all of this too so so but
it's pretty straightforward so you
create you create a single node then you
just log in and then you log into that
node and can
figure it however you want so in this
case I logged in installed disco
installed the other things then you
basically tell it to create an image and
you Amazon has on two different data
stores or two different ways of storing
these you can store it on s3 which is
kind of the standard way of storing a
lot of data on Amazon or you can use
these EBS images and those are a little
faster so I use one of those for this
and so I have an image now created for
the disco for disco that has all of
disco installed and now I can go create
disco clusters out of it so I'm at the
bottom here you can kind of see the
example for starting a cluster using
star cluster um and again it's really
really easy you just give it the start
command tell it how many nodes you want
tell it which instance type to use and
it goes and does everything else for you
and then you um can SSH into it and and
have fun with it
so um once you're on that once you've
got the cluster up and running you want
to go and go ahead and get just go up
and running on it so you're gonna
there's gonna be a few more steps that
we'll do to set up set up disco um the
first one is I do here as I actually um
so and this is kind of showing some of
the configuration and how you can
configure so the disco disco has a bunch
of configuration options that you can go
through you can um tell it how many
nodes to use for redundancy so if you're
copying files out to DD FS you can tell
it to do three copies or one copy or ten
copies you can tell it there yeah
there's a lot of different different
commands in there um but one of the ones
is where all the default paths are so I
am actually just going to change the
paths in here to use some one so that
when you when you have an Amazon image
you have a couple different default
mount points one of them is the actual
image that you mounted and then you've
also got it's kind of your default
scratch page which is most likely the
local drives on the node that you're
running so you get Emma with a small
instance you get a hundred and sixty
gigabytes that you can work with so I am
Tildy DFS to use those I'm the one key
thing when you're when you're doing
Amazon clusters is those are transients
so I'm if you're trying to use this as
AM as a permanent filesystem you'd want
to set up a slightly different strategy
than this so this is just using the
transient drive so every time I ran this
I would have to you know reload all my
data over to it and then you create a
mount point so so create the mount point
for the DD FS this is so disco obviously
since you know not everyone knows about
it is not neuro does
nearly get the developer resources that
I'm Hadoop does so there are a few
little bugs that I ran into every time
I've installed it and I keep meaning to
post the mailing list and help them out
on that but this this one gets me every
time is that as soon as you point your
data to somewhere else you actually have
to go through and create these dbfs
directories by hand and then everything
works fine after that and then to get
Amazon working so the control panel for
Disko is a web app so you have to be
able to point your web browser through
through to it and they use it port 8989
it's the default one so on Amazon you
have to update your security group to
allow in the inbound traffic from that
port and that's a pretty simple step
there and then once you've got that set
up you can go ahead and fire off disco
and it's really simple it's just like
kind of like star cluster it's the disco
command and then you do start and as
long as things were set up right it will
just start and run and then you can hit
the web app and go see and and right
here you can see when you started up the
first time what you end up with is a
disco instance with a single node node
on here you can see them it tells you
how many you know it gives you a cystic
here's um this the numbers there will
tell you how many jobs are running in
differents map and reduce jobs and I
think failed jobs is the third one and
it'll also tell you how much storage you
have available on your devfs volumes so
that little green bar will start filling
up as you start adding stuff to each
node so you can use that for monitoring
but one node Disko clusters not that
interesting so I'm you want to go in
you'll want to go to add some nodes
right away
so in disco the way you do that is you
just go to the web browse the web app
and over to the configuration panel and
you just add some nodes so in this case
um the naming convention on the star
clusters is node and then three 3em3
numbers so node 0 0 1 all the way up to
however many nodes you've allocated and
disco lets you use kind of a arrange
notation there so you can go node 0 node
1 to node 3 and it'll automatically it
guesses the naming convention and fills
in all the nodes for you and when you do
that hit the save table button which I
forgot to do and never understand for
some reason that that didn't make sense
I lost out the first time like where are
all my notes but I'm their user
interfaces in the most thought-out one
it's um again
it's simple it's nice and once you once
you once you've done it once or twice
everything starts to make sense but it
also supports blacklisting I'm for disco
and for devfs so if you want like just
dedicated compute nodes you can tell it
what you basically blackness list a
bunch of nodes until them not to store
files on those you can do the opposite
if you want dedicated storage nodes for
that so there's there's some things you
can do for configuring your cluster and
if everything works you end up with on
your on your Status page now you'll see
you'll see all the nodes that you have
allocated so that's basically it once
you have it up and running now you've
got it we've got a fully functional
disco installation running on you see -
so um what can we do with it let's go
ahead and actually I'm going to walk
through an example now processing some
some more big data so one of the one of
the neat neat public davis datasets on e
c or on s3 is on the some data from the
thousand genomes project so the thousand
genomes project is a big research
project where they're gonna go fully
sequence a thousand individuals and just
have all have all of the all of their
sequence data available for research so
it's looking for variation to variations
or the big one they're looking for but
there's a lot of a lot of a lot of
studies that you can imagine being done
on that on that amount of data it's a
lot of data though so i'm they've posted
7.3 terabytes of data on s3 that anyone
can go and play with and as i learned
very quickly you really have to
understand sequencing and what the data
is to actually work with any of the data
so i made a great press release when
they did it but you have if you actually
look at the data it's kind of a
low-level sequencing data format that's
hard to work with unless you've used it
before it's actually all posted as
binary files so these these binary
alignment files which what some what
they've actually posted on here is
they've taken all the all of the data
for the individuals map them back to the
human reference and they tell you where
all of the reads from the sequencing
instruments map to the human reference
so you actually have the results of of
the first passive analysis is what
they've actually made available to us to
work with and it is a binary format so
it's binary data so we're gonna have to
figure out how to work with binary data
on Disko in this case so what so so it's
a really simple example what I wanted to
do is just basically take all of the all
the genome data they pushed put on here
and just see what's the coverage look
like for all the chromosomes so how
would we go ahead how do we go through
and take take the mapping data do a
simple a simple map
pipeline streaming all of the BAM files
that they have and then create a
coverage map of the genome to see if
we're actually you know covering all all
of what we would expect to see in a
genome so and actually it's not as I
change it from snipped coverage so this
slides a little bit wrong or the titles
wrong but it's pretty straightforward
again this is this is um looks a lot
like the word counting example and all
the other ones that the coverage map is
really simple assuming I'm getting a
record out of the binary file and we'll
talk about that in the next slide how
how we actually did that basically I get
the reference and the read passed to me
those are the two the two things that
are passed into the map map things so
the reference is what chromosome it hits
so that would be you know chromosome 1 2
or 3 or X Y or M or MT it was annotated
in here and then all it and then for E
and then each of those read records is
going to have a position where that
mapped to the chromosome so the first
thing so what I'm pulling so I'm
returning as my um as my key is I'm
returning the chromosome name and then
the actual coordinate on that chromosome
that the read map to and I'm returning
how long that read actually was so
depending on the instrument platform
you're using you might get 50 base pair
reads but you also a lot of them have
variable length reads that you're
getting out of there too so you can have
you know up to 800 bases possibly in a
read so that gives you an idea of what
the overall coverage would be so you can
you can use the start position and the
read length to figure out the depth of
coverage at that air in that area on the
genome from that particular read so so
for the partitioning function I actually
just partitioned it kind of logically
using the domain the domain model here
and just partition it based on
chromosomes so if it's chromosome I just
used simple you know numbering X 24 Y 25
mitochondrial was 0 and then for the 1
to 23 just used in on there so that that
comes from the cutting and pasting I did
to make this fit on the slide it's
coming from key so in here it's key so
it actually is key in here so good catch
see if someone's reading actually
reading the date in the code here but
yeah that's so that's sorry about that
but it the critic key is the crumb so if
you see where I build the key up here I
put
chromosome name which is coming from
reference in the position
so right here I'm splitting it and that
right there should actually be CHR but
this was reform copying and pasting and
reformatting to make it work on here but
yeah so that's where that's going so
this will create um 26 buckets that the
various chromosome data will go into so
in the end I'll have 26 reduced steps
here and 26 some sets of results that I
get out of that I get out at the end
that will show me the maps um and then
the reduction function is pretty simple
this actually hits an important thing in
um in disco that's worth talking about a
little bit so you'll notice I'm
importing numpy inside the reduction
function so one of the requirements in
disco is that the map and reduce
function should be essentially kind of
pure functions you hate you really don't
have any access to any external data
suits so there's no Global's you don't
know what modules can be loaded so if
you're gonna use a module you have to
load it inside the function when you're
working on this so so you'll see in here
I'm importing numpy directly to work
with the data and create and do it so
that's just one of the one of the ways
that it's set up and it actually is the
way under the hood the way it's
implemented is it actually takes it you
know takes the client data essentially
packages it up and ships it all over and
then fires it off in a so it doesn't
have in python interpreters running it
just takes the running State from here
and moves it over there I think is how
they're doing it so kind of like what
ipython does in a way but they've been
really trying to make it so you can have
very minimal Python installations on the
nodes or none at all and then be able to
move data move move the move the actual
code over there execute it move
self-contained code from the client side
over to to all the nodes and do it I
wish I understood a little more about
how it's doing but that's one of the is
one of the important things to know in
here and and and kind of another caveat
on that is the map functions you know
really get called once for each line so
they do give you this params thing and
you are allowed to stick some trance
some persistent data in those but there
there are some some constraints on that
too so the parameters data is really
persistent to the actual node that
you're working on so you can't you can't
it's not globally persistent data so you
use that to like do communication back
and forth between math jobs but again
that goes with the whole spirit of map
and reduce is that all the map jobs
should be independent jobs the reduction
jobs are actually fairly long-lived
these usually don't get restarted you
feed the stream coming into it and these
will these Alivia whole time so the way
I'm the way the way I actually create
the chromosome Maps here is I have a
just a dictionary that gives me the
sizes or the rough roughly the sizes of
all the chromosomes and then I go ahead
and create an array for those that just
has a AZ Rose create a own array that I
have just what counts at every position
in the chromosome so these are can be
fairly large the largest chromosome is
250 Meg's so that can end up being about
2 gigs of data that you that you end up
allocating there and actually on the
small memory nodes I had to drop these
off by three orders of magnitude to make
it fit in the RAM on the nodes but um
but that's the basic idea is create an
array here and then stream in all that
all the position data that comes out of
the mapper so I pull out the chromosome
find the you know I've wanted one per
chromosome in here so what I'm doing is
each reducer will know what chromosome
they're working on so they'll only get
data from the same chromosome so then
when I'm going through here I pull out
the position information and just go
ahead and using just slice slice
notation start at the position and
position plus length and add account to
there and now we kind of know what the
coverage at that position on on the
chromosome was from this and then I when
I'm when I'm done I'll I just returned
that as a string so it kind of works
with the ASCII formats that they like to
ship around from these so the big thing
I kind of left out here which goes into
the next slide is um this is all binary
data and this is one of the fun things I
think with the MapReduce systems is
trying to figure out how can you work
this with any type of data and usually
you can think of how if I have my data
in the record format I'm used to working
I you can think of a mapper that you'll
do and you can think of a reducer but
oftentimes if you're working with
scientific data or data from you know
arbitrary sources you know there usually
is a record format that you're dealing
with and if it is binary you got to
figure out a way to actually work with
that and in this case um it was also
data stored on s3 and I didn't really
want to copy seven terabytes of data
over to my local nodes because that
would have cost me a lot of money
instead I wanted to kind of move the
BAM files over one at a time cash em
stream them as binary files and then get
rid of them right away so so the way the
reader that I ended up writing for this
it looks like this and and again you can
see the importing inside the reader -
I'm getting the temp file on the PI Sam
which is the library the domain-specific
library I'm using one kind of neat thing
with um with disco is all of the mappers
reducers readers a lot of these can be
kind of chained chained together and
readers are really nice to chain because
what you can do is you can take
advantage of all of their infrastructure
and so so so in here
the s3 files are just a just just eight
web address addresses so HTTP strings
that um tell me where the file is so the
first thing that they'll actually do is
they'll they'll use their HTTP reader
and then I get a stream from that HTTP
reader that I can work with right here
so that's so they're taking care of
getting the data from the web or from s3
for me already so what I do is that to
make this work work with the binary data
I just create a temp file take the data
from the stream that they're giving me
read it dump it to the disk and then
open up using the Sam of the Sam tools
library the Python library I open it up
and this takes care of all the binary
form unpacking the binary data for me
and now I just have a nice iterator of
every single read in the file and I just
yield these so now my reader is almost
almost a map step in a way but it's
instead of you know doing a key value
pair I'm just read I'm just returning
the single lines the and they're not
even lines in this case it's really just
a couple that contains the reference
name and the and the read so you disco
makes it kind of easy to blur those
lines between you know mapping and
reading and reducing but it also gives
you a lot of flexibility and if you're
kind of used to you know building
pipelines like this for data you can you
can kind of see a lot of different ways
in Disco where you can you can chain a
lot of a lot of different pipeline steps
together and you know kind of massage
your data kind of independently at each
one and and get around some of the you
know either different problems that
you're running into or just work with
different data types um and internally
it actually does disco does pass
everything around as these lists that
you return from here so they don't those
don't get changed
much as there go as they're going
through the system so that was actually
one of the one of those undocumented
features I did find is if you don't
return tuples in your yield statements
it actually crashes and doesn't explain
why so they actually do it to assume
that the readers after the first one
aren't working on line data instead
they're working on tuples which was an
interesting one but anyway so so with
this basically that's what I do and then
when I'm done I just you know close the
close my reference to references to the
the file I created and then temp file
will go ahead and delete those for me so
I never fill up my local space at all
with the seven terabytes of data instead
I only have the working data that I've
got and you'll notice too that I skipped
DD FS entirely for this particular
example and that was in large part
because there was seven terabytes of
data on the other end that I didn't want
to have to push in and and pay and pay
the store so so the final thing for this
one basically you saw the map the map or
the reducer and the reader and what we
um to put it all together here's the way
here's what the job looks like in this
case I give it the list of all the all
of all the locations at s3 so that there
were five hundred BAM files in the final
example that I used here and give it a
give it the reader tell the reader tell
it how to partition tell it how many
partitions give it the mapper and give
it the reducer and then fire it off and
runs for a long time and then I get back
the results for each chromosome and just
write those out to a text file and I was
able so I'll talk a little bit about
some of the things that can go wrong and
here but um let's see here's the here's
one point one of them from chromosome 12
for one of the data sets so you can
actually see this is um there's a big
spike in coverage in one area but I'm
you know much about kind of genomic so
what you can actually see here is that
you know this is there's always a region
in the middle where the what's the name
few important to one of the guy who
raised his hand for biotech the
centromere thank you
so the centromere is at the beginning it
is in the middle of the chromosome so
yeah if you picture those two um
cartouche type things that meet in the
middle and the pictures that you see
them there's the centromere area in here
and it's actually an area of really low
complexity and it's
also you know what in this case it's an
area that actually picked up a lot of
reeds um it's probably just a priming
bias but overall you see you know nice
nice coverage across the whole across
this whole chromosome right here a
little bit more at the end again the
characteristics of the ends are kind of
like the characteristics in here but if
you were to see that for all of them
you'd see the similar pattern where you
have you know the two at the two sides
the two arms on the chromosome and then
the gap in the middle and and fairly
fairly good coverage all the way across
oh yeah so so so the x-axis here is is
chromosome position so this one is going
up to add 120 million about 130 million
so these are actually I've been did a
little smaller to you actually get it to
work on the small nodes so this is 130
million or so long for the chromosome
and then this is the number of counts
that were observed at that position so
it goes basically no from zero counts in
some areas all the way up to you know
around 200 counts for this particular
data set so that's that's the amount of
depth that they were able to get out of
that sequencing run on on this
chromosome so so it count is basically
when you um when you have a route so so
if you picture with the sequencing when
you fragment up the source material
those fragments are all fairly short so
each time one of those one of those
fragments fragments maps back to the
chromosome you get a count and it's just
I do it based on the length of the
fragment that mapped back so you get
them that's that's how you end up with
the with the with the peaks there and
the depth so so it's a simple example
but it's you know that that type of
workflow is fairly common in genomics is
just going through all the reads
computing something simple and and like
answering results but a few things um
that so IIF reduce from time to time and
you know I kind of wanted to take
advantage of this to talk a little bit
about that too I'm you know kind of what
I found it's good for and what it may
not be but this go first of all it is a
great alternative to Hadoop I know if
you've ever tried to install Hadoop you
know it actually takes time and there's
a lot of configuration stuff you've got
to do to make it work
Disko generally installs a lot cleaner
assuming you don't run into some of the
few bugs that seem to get fixed um every
now and then there's a weird bug and I
thought that was one thing I know cuz I
been through the code a lot the first
time I did this because it didn't
install very well the first time and
there were a bunch of little issues that
they had but at the same time I got to
see all the code for it and doing that
and they actually have done it's it's a
fairly clean implementation it makes a
lot of sense if you're trying to write a
distributed processing engine what
they've done here is actually really
clever it's a really interesting way
they did especially the way that they
use Erlang and Python together so well
the reason they use Erlang because by
default Erlang is really easy to
distribute across a lot of nodes and it
has the whole distributed computing
infrastructure built-in so you kind of
have your master node but know as long
as it knows all the nodes it can fire
off the infrastructure and they've done
a really good job making sure python
plays very well on top of the erling
infrastructure so I think the way
they've set it up is actually pretty
clever and I'm kind of pointing at you
guys out there because it's worth
looking at the architecture they did for
for some of it for doing that and you
know it's not too far removed from how
ipython does some things either so it's
kind of it's kind of a neat little
system I don't know how many people
actually work on it but I as far as I
can tell it's only a couple guys at
Nokia that do this and so it actually
was developed at Nokia and they use it
internally another interesting thing
I've learned doing it so we have a
number of different clusters and some of
them are kind of the traditional
commodity clusters where you have you
know three terabytes on each node and
you have either 10 you know Giggy or
InfiniBand between the nodes and that
works well just fine but we also use
this on just traditional HPC storage so
you know the sans system so we have them
some 8 HP systems and then some some
data direct network systems that we've
run this on and it actually as long as
as long as the disks can feed all the
nodes fast enough it works really well
too so you're not really I actually
found them on the real high-performance
storage systems Disko runs a lot faster
than it does on commodity clusters when
you have the same number of nodes and a
lot of that comes from just HPC storage
is really really fast so our CDN systems
give us about three gigabytes per second
and that's by it's not bits three
gigabytes per second throughput to the
nodes versus if you I even if you even
have a good raid setup on a commodity
you know in Juneau at most you're gonna
get kind of a hundred to two hundred
megabytes per second so we're able to
get really high throughput on that on
the big storage systems but then still
you know use MapReduce is kind of more
the processing so then you don't even
have to worry
DD FS for the storage you just use your
HPC storage system and then if your if
your problem fits well in the MapReduce
framework you can just use that on top
of it so we I've done it in both
settings and I tend to actually use the
HPC storage use case more often than I
use the DD FS what I'm doing this but on
that you know I I was kind of new to the
whole tag based file access and I like
the way they do that that actually has
been a great way to organize data and
kind of do those ad hoc databases um
which if you're playing with any of the
other column stores you know you're kind
of probably getting used to you used to
naming all of your data using those
conventions but it's really nice the way
that they allow you to have a tag
hierarchies they so you have the tags
pointing to different to each other that
can ultimately resolve down to the file
so you can group things nicely and disco
also has on this thing called disco DB
and I didn't talk about it in here but
it basically is a tag based data store
so kind of Kentucky with all the other
column stores um and they claim it's
it's really fast but I've never set it
up so I I can't say anything other than
it does exist in and it looks looks
promising on the MapReduce side um you
know I know this is a Python and
MapReduce talk but um it's definitely
you know a good and a bad thing I I
found I'm it definitely certain problems
it's kinda always like in MapReduce two
GPUs you know if your problem looks like
a imager you know graphics pipeline a
GPU is a great option for you it doesn't
look like a graphics pipeline maybe it's
not the best option for parallelism and
MapReduce is very similar to that if
you're if you're if your problem looks
like a streaming data problem where you
have a clear math stuff in a clear
reduce stuff it's a really easy way I
mean you saw the mappers and reducers I
was able to write they're really simple
especially you know using something like
this go where you can just keep
everything in Python in a single file
it's very easy to work with
of course Python is pretty
straightforward too so oftentimes your
pure Python code that doesn't use
MapReduce looks a lot like you're just
straight MapReduce code so you can I and
that's kind of the next point there is
that I when I when we first started
using this I did a lot of performance
tests to kind of figure out you know are
we actually gaining anything with
MapReduce because my goal with MapReduce
and our environment was to make it
easier for biologists to work with a
really large data they were I'm
struggling with them you know kind of
loading all of these readings into a
Perl hash and watching their you know
program crash after
three hours of trying to load it happens
every time someone starts doing this
that that happens so so so my goal with
this was to see is akin can we give them
an abstraction that's a little easier to
work with it hides them from a lot of
the common problems you run into when
you're working with these you know data
files that are a couple hundred
gigabytes in size and so it definitely
it does make it easier if you can get
everyone to learn Python and as we
learned a lot of analysts to actually
learn are instead so but I'm been
working on transitioning them over to
play with this but on the flip side when
we did the performance tests we ended up
finding that the simple simple batch
processing was for almost every problem
was significantly faster so if you have
a job scheduler if you have Python
installed all the nodes and and you
don't mind doing a little bit of data
you know data parsing on your on your
own end I found that you know it was
usually 20x faster to do it just but
with pure Python than it was to use the
MapReduce frameworks and we tested this
back and forth with them Hadoop also and
found that that generally to be true and
doing some deeper analysis so I did some
profiling on there to find out what was
really going on and it's it's it's
really simple it's just the abstraction
penalty of those map steps so if you
look at getting a single record in each
time and if you actually go and you know
just do it do a trace on them on the
Python stack you've got you know 20 or
so function calls leading into that that
have to get executed every time pretty
for you to get every single data element
and dupe has the same type of stack on
the Java side I'm in a lot of cases
whereas if you're just working on the
raw file streams off of the operating
system you know you basically have the
i/o buffers and that's it and then
they're giving into Python so you have
no abstract your abstraction penalty is
significantly less when you're just
writing simple batch jobs um so you know
well I think the you know well while
these are this is a neat programming
paradigm and it works well in commodity
clusters um I think it's it's always
worth looking looking at what resources
you actually have available and seeing
if the overhead of something like this
is actually worth it or if you're
getting the benefits um it's kind of a
good and a bad thing because I know the
one thing I have learned is every time I
play with this I you know you get sucked
into it it's like when you pull up pull
out scheme or Lisp you know you get kind
of stuck back into that world and you
have a lot of fun with it for a while
then you come back to reality and I find
that this is just as intoxicating as
that it's really fun to start thinking
of everything in terms of map
so it's a neat paradigm and and I think
you know as more people use it we might
see more improvements in making those
stacks run a lot faster but every now
and then you know you do have cases
where you know if you really have
petascale data and you want to
distribute it across you know a
warehouse of computers this is not a bad
way to process it um and I think I don't
have any context into how it's been used
at you know outside of what papers
published at Google or places like that
but that seems to be where the use case
were it actually is really useful is
when you do have 2000 computers and you
actually have a couple petabytes of data
spread across those and there's no other
way to get at it easily when you have
even even with what we're doing when you
have you know 10 or 20 terabytes of data
if you have access to it on a decent
storage system you know you can you can
operate on that just fine and Python as
long as you're kind of smart about it
and that's where you suite where we were
seeing the big penalties for it but I
can see if you've got you know two
thousand nodes and you really want to do
a petascale question you know this is
probably the easiest way to do it so so
that's kind of what I figured on it I'm
I got some references in here but disco
I I definitely encourage if you want to
play with MapReduce unlike Python it's a
lot like I say it is intoxicating when
you get it set up and you fire off 20
nodes and especially when it's working
at Amazon and places like that it's kind
of fun to play with it the MapReduce
star cluster and I was kicking around
ideas too because there were some some
things I so I started the whole Amazon
part of this some on Tuesday thinking
that I would actually have a lot more
running by now and I have some open open
things that I think would be a lot of
fun because when I started the Amazon
one I went to see it has someone done
this already and of course no one had
yet so it'd be fun to take take what
I've got started and and just clean it
up a little bit make a public ami out of
it so anyone else who wants to run disco
and Amazon can do it almost as easily as
they can
Hadoop so if you've seen Amazon has
they're elastic elastic MapReduce that
you know it's just a kind of a front-end
for Hadoop so you you don't even have to
do any work to set it up there we can't
quite get that far with disco but we can
make it a lot easier for people to try
it so I think you know for hackathon
tonight if anyone's interested I'm you
know I mean anything can be fun to go go
and I might just do this with my time
there and get this thing made public so
I can put it out there put a little
documentation as to how to do it I've
never done star cluster plugins either
but that would be an alternative for
doing this I'm as it might actually be a
little
easier with star cluster you can
actually install a bunch of stuff as the
cluster is being set up so that would be
another way to do this and another one -
I don't know if anyone's used EBS
volumes but it'd be fun to try that's
they're kind of there you can have up to
a terabyte of kind of block level data
storage and Amazon and if and I'd like
to figure out how to set that up on here
so you can actually have your DD FS
clusters persistent so the way I did it
it used the trans against storage so we
lost every time I shut down the clusters
I would lose the data but I'd like to
figure out how to do it so you have one
EBS volume per node so you can actually
leverage the whole DD FS file system for
more long-term data storage so those
were some kind of next steps that I
think would be fun to go go on this
thread Benham alright well thank you
everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>