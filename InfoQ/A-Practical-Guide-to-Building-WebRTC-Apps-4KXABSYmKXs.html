<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Practical Guide to Building WebRTC Apps | Coder Coacher - Coaching Coders</title><meta content="A Practical Guide to Building WebRTC Apps - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Practical Guide to Building WebRTC Apps</b></h2><h5 class="post__date">2013-10-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4KXABSYmKXs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I I'm done so like you said I'm with
v-line and we're a provider of WebRTC
cloud infrastructure and as of yesterday
actually an application where you can go
being calm and actually use a real live
WebRTC application and you know I have
lots of scars to show for WebRTC because
I've been working with it actually since
before it was in chrome since Google
dropped the code back in the summer 2011
before I didn't even compile actually at
least I didn't know how to make it
compile so we actually wouldn't added
support to Chrome and built actually
have this is the only record I could
find out but what I think was the first
WebRTC application which we built after
going and hacking web RTC support into
Chrome just because we thought this is
so cool
you know we have to understand what it's
like what it's like to build an app
which you can do with it and two years
later it finally gotten to the point
where you can build real WebRTC apps
that work well almost all the time
to the point where they're you know
working as well as the you know types of
applications that you're used to using
everyday like hangouts and Skype and
FaceTime
there was a long hard road to get there
for everyone involved with WebRTC on
you know Chrome and Mozilla and IETF and
w3c but you know it's basically there
now there's stuff that is left to be
done but it's basically there so my goal
today is to actually talk about
something slightly different than what I
told Chris I was going to talk about so
I apologize about that
but rather than going into just a random
assortment of basically peerconnection
related issues you know I realized
there's actually a number of good talks
out there about that and that you know
what I would probably make more of a
contribution for would be to talk about
some of the areas that you know don't
get a lot of of coverage and you know
two of those in particular we've spent a
lot of time working
on our mobile and multi-party
conferencing and there were a lot of
kind of hard lessons learned there that
so my goal here is to take some of the
things that we spend our you know months
banging our heads against the wall
trying to figure out and condense it
down into you know 15 minutes of
impenetrable slides so before I do that
I just want to do a quick demo because
you have to do demos for ortc
so do you have some devices connected
there let's see so first I just want to
show actually how do I switch over to
okay so this is and what device do we
have here
this is a galaxy s4 which yeah like the
quality is actually not quite as good as
you can get on some Wi-Fi connections
but you know it's pretty decent for that
device do we have we have an iOS device
see
up in here and so you can see you know
this is a what is an iPad for you no not
that much different but you know just
going to show that you can get you know
some pretty you know pretty reasonable
quality on you know basically the
top-of-the-line devices today that's
competitive with you know what you can
get with let's say FaceTime so we back
into my presentation here and what I
want to talk about is what it takes to
make that experience work well almost
all the time across a wide variety of
networks and devices that you don't have
as much control over as you might like
to and so but before I do that let me
tell you a little bit about you know the
current state of the art and web RTC on
the important mobile platforms so you
know as Chris mentioned you know Chrome
and Firefox on Android it actually works
pretty well
there's the Java peerconnection API for
note mobile apps you know all of it is
pretty much production ready iOS it's
unfortunately not supported any browser
including Chrome on iOS because of
apples policies about you can't use any
browser engine except for mobile safari
and they don't support it we hope they
will someday but they haven't announced
anything along those lines so Google has
some objective-c API is that you can use
to embed the web RTC engine directly
into your own application and the voice
support works really well there now the
video support can be made to work well
it's still a little bit of work to make
that happen but that I'm sure that will
get easier in the next few months so the
big question that always comes up on
Android in particular is Drive build an
app for Chrome or do I build a mobile
app and that's a question that's getting
even harder these days because it
appears that you know to an external
observer like me I don't know what
Google's official policy here is that
chrome and Android are converging and
the end result will probably look more
like Chrome than Android so you know you
have to decide we know which technology
are you going to invest in here so if
you are building an app that runs in
Chrome you get the benefit so basically
being able to run the same app that
you're running on the desktop and you
have to build
anyway and you get native support for
the single most powerful feature in
WebRTC is links I can link to something
anyone can click on it and they can call
me they can join a conference or
whatever with native apps though you get
a few things that you unfortunately
still can't get in Chrome for Android
and some of these you can actually get
in Chrome on the desktop but
unfortunately not in Android and the
single most important one of those in my
mind is push notifications because in a
chrome app on the browser on Android you
can't keep it running all the time in
the background so if you want someone to
have a lik link they can click to call
you then the only way to accomplish that
is to have a native application that can
receive push notifications and then if
you're building a communications app it
makes a lot of sense to integrate it
into the address book and you know have
some background services so that you can
switch apps while you're actually
talking to someone and being able to
have some of the native API is that give
you more information about the device
capabilities can make it a lot easier to
tune to your device and there's slightly
better performance so there's a lot of
you know good reasons for doing both and
I think that you know where most people
wind up where we wound up is to do both
and to reuse as much if the
implementation is possible across both
those so basically you know build your
app in html5 and you'll make sure that
your browser-based version works well
also in Chrome for Android and so that
your users can have a really good
experience without installing anything
and then take that same app and run it
in phone gap and give it access to the
native API s and in my mind that's
really the best compromise on Android
right now and on iOS you really don't
have a lot of choice the question is
really is it all native or is it you
know also have some parts that are
written in HTML that are not the WebRTC
parts just the pure user interface part
and I highly recommend keeping as much
of it as possible in HTML especially
because you'll wind up with more
JavaScript session management and
signaling logic than you might
anticipate when you start and being able
to reuse that across the browser in
android and iOS is actually a huge win
and
being able to maintain these things so
the hard part of it all is taking into
account the differences between the
devices because obviously mobile devices
are a lot less powerful than laptops and
desktops and even among the mobile
devices is a really huge range in
capabilities so the high-end mobile
devices are actually closer in
capabilities to the low-end laptops in
some cases they're actually faster than
the low-end laptops and then the low-end
Android devices can you know can be
anything so you have to take this into
account unfortunately when you're
building your applications and the
WebRTC does have some built-in
capabilities to adapt to what the device
the network capable of but unfortunately
they're not quite sufficient so that you
know what they do right now is that
there's this video adapter which is you
know way down in lejeune goal which is
one of the small boxes on on Chris's
slide and its job is to see how much CPU
the device is currently using and reduce
the the resolution and the frame rate
potentially if the CPU is being
overloaded and then that handles you
know you know slowing down the recent
you know decreasing the resources used
by the encoder and then you have a
remote bit rate estimator whose job is
to figure out if the other side is
sending it to higher rate and they are
limited either by the network or by the
decoders ability to keep up and you know
they're unfortunately a couple problems
with this one is that both of these
methods only really kick in after there
are you know visible degradation in
quality and obviously you want to avoid
that if you can if you possibly can and
the other is that because you have the
encoder and decoder competing for
limited resources the results may be
kind of unpredictable and really one of
them can win and on one call you'll have
really good upstream quality on another
call you'll have crappy upstream quality
and really great downstream quality and
it all just depends on timing and other
things that are beyond your control
which is which is unfortunate so if you
really want to have a good experience
all the time you're going to
to do a little more work there and the
way that you know that you're basically
you know you've hit one of these
conditions is if you see that you have a
device that's actually a pretty powerful
device on a good network and the video
is being sent at a constant rate of
around 50 kilobits a second it's a magic
number it's the minimum encoding rate
for the for the vp8 encoder if you see
that then you know that you know you
have a little bit of work to do so the
unfortunate reality is that you're
actually going to have to impose the
limits on the frame rate and the
resolution and the bitrate because the
built-in support isn't quite yet tuned
well enough to do that and the way that
you do that are through a couple of
different mechanisms you can control the
resolution in the frame rate when you're
requesting the media stream with
getusermedia so there's a media
constraints objects to get passed in and
you can pass you know max width height
frame rate and then for controlling the
maximum bitrate you have to actually oh
yeah you know very important thing when
you're doing this is that if you're
actually changing any of these
parameters you need to stop the old
stream before you go get a new one or
else the the new constraints will not be
applied this is a bug in Chrome so be
aware of that and unfortunately one of
the things that you'll learn as you dig
deeper and deeper into WebRTC is that
one of the main API is is one of the
things that the chris mentioned is
something you don't have to worry about
but unfortunately in many cases you do
and that is the SDP and that is a big
chunk of text which is generated by the
peer connection and you're supposed to
just hand it off to the other side and
they pass it to their pure connection
and everything magically happens it
turns out that there's a number of
different cases and this is just one of
them where you need to go in and do a
little bit of massaging because this is
actually the only API you have to
influence some of the parameters and in
this particular case if you want to
limit the bitrate because yeah the
WebRTC will happily send as use up as
much bandwidth as you'd give it and we
are regardless of whether the other side
whether it's a good trade-off
you know it might send two megabits and
as a result force the encoder down to
200 kilobits when you really would have
rather put a limit on it yourself so if
you want to put a limit on it there's
this special FM TP parameter X Google
max bitrate that you can add to your SDP
then this other magic parameter which is
really the only other knob that you get
to influence what the encoder is going
to do which is the max quantization and
it's kind of a hard thing to explain
except scenes that have a lot of motion
in them use up more CPU to encode and
decode than ones without a lot of most
emotion in them even if they're both
being encoded at the same bitrate so if
I wave my hand around and I'm basically
running the encoder it close to the
limits of my device then things can go
to hell so you kind of this a way to
know if you're close to the one is your
device wave your hand around and see
what happens and you know what this
parameter lets you do is basically you
reduce the ability of the encoder to do
that so you force it to you know not
encode things in such a way that they're
going to take more resources to decode
in a somewhat indirect way but that's
that's a result that you're shooting for
when you set this on mobile so you know
now that you know how to set these then
you know the real question is well what
do you set them to and you know really
the the easiest way to do it and it's
not necessarily a terrible way is just
pick something that you know is going to
work on almost all devices and just set
it at that and you know that no one's
going to have a horrible experience
because you know one of the one of the
downsides too is that if you overshoot
what the device is capable of you
actually have a worse experience than if
you limited it to something drastically
less than it was capable of so in some
ways you're better off erring on the
side of using too few resources so yeah
I would recommend something in the
ballpark of 320 by 240 15 Phipps about
foreigner' kilobits that's just you know
magic numbers based on waving my hands
around a lot in front of a large number
of devices
and the other approach which is if you
want to be able to get the kind of
quality that we were just showing on
high-end devices you know at 640 by 480
it's something close to 30 Phipps at
higher bitrate I think we were learning
that probably 700 kilobits then you're
going to have to be able to detect the
capabilities of the device and influence
those parameters and you know then you
will set those parameters to be the
minimum of what the sending device can
encode and what the receiving device can
decode so how do you detect those
capabilities well it's pretty easy for
your native applications because there's
a lot of API is that will tell you what
CPU is in here how many cores does it
have what clock rate does it run at how
big is the screen it's a little bit
trickier if you're running inside Chrome
because there's no api's right now to do
that so you know your best bet is to run
a JavaScript benchmark and I wish I had
a better answer than that but you know
this is web development and we're used
to doing crazy things to make browsers
do what we want them to do and this is
just another one off them so you know
find some sort of numerically intensive
benchmark running in a web worker wants
to give you basically you know figure
out what one court it can do run it in
for workers and in parallel and you can
infer the basically like you know the
clock rate and the number of cores
roughly very indirectly and yes last but
not least never encode at a higher bit
rate than the resolution of the
receiving device as you can you know you
can use the processing power and then
with better in other ways and of course
once you figure all this out you're
going to have a mechanism for signaling
it so that's kind of I'll leave that up
to the reader because signaling is
something that you kind of have to
figure out either way so one thing that
I'll mention before I move on from
mobile is that there's kind of a neat
bonus that you get from optimizing your
apps to Android and that is that there's
a lot of really cool Android devices out
there that kind of barely qualify as
being mobile and if you plug them into a
big screen you can basically have the
equivalent of a high-end telepresence
system like you know there's these HDMI
dongle --zz that are kind of like a
chromecast but with the faster CPU you
plug it into a TV and potentially
control it from you
phone the Samsung has a mirrorless
camera that's got an aps-c sensor and it
runs Android beautiful image quality you
know always plug a tablet into a TV that
sort of thing so there's all sorts of
interesting things you can do an Android
besides just a video on your phone so
you know after my super rushed guide to
mobile and they do the same thing with
multi-party this is something that you
know people just have as an assumption
when you say oh I can do video
conferencing well that means I could
have more than two people right and the
answer is well you know kind of sort of
with some caveats and there's a lot of
different trade-offs to the way that you
ways that you might do it and there's
there's two main strategies for how you
do it and you know if Chris was
mentioning earlier that one of the most
power think powerful things about WebRTC
is that there's no server in the middle
it's all peer-to-peer and that is
certainly true most of the time for two
party cases when you're not relaying the
video but in multi-party there's a lot
more trade-offs there and it's a lot
less clear-cut whether that is the right
way to do it so you have to choose
basically whether you want a mesh
topology where every peer connects to
every other peer in the conference and
encodes and sends a separate stream to
each one of them and also you know
decode the stream obviously from each
one of them or a star topology where
you're sending one stream to a central
server which then fans it out and you
know there's there's merits to both but
before that let me actually just show
you the two different modes so that you
can see what the difference is
and see what room are we in three
okay so this is going to be a mesh
topology and this is a gym and Prakash
Singh back in the office jesse sitting
here in front of the room so one of the
things that you'll notice about the mesh
is that because you wind up sending the
same you know encoding you're Sam
searing multiple times you're going to
wind up you know wanting to have smaller
videos because each of those will wind
up being encoded at you know a lower a
lower quality but this is you know it
works pretty well for up to probably
three or four participants on your
typical desktop machine and you know
it's pretty easy to set up so let me now
let's hop over to the star topology
conferencing see
okay and so here we have the same thing
because we are sending it through a
server we can choose to send a higher
quality stream based on who's speaking
so if Jim can you say something for us
so when he speaks to us switch over to
Helmut Prakash does something and you
know it should cut back and so you're
not receiving a high quality stream from
everyone
it's and you're only sending one stream
to the server so thanks guys
all right and you know you can think of
it as the difference between you know
hangout style and something like we're
running a long time so the latency is
not really related to which one you
choose except that I mean mesh can
potentially be lower latency because
every one happens to be sitting on the
same land you can have practically zero
latency the and you know a star can be
more if you're sending your video
through a server that's really far off
so one of the important considerations
is that you want to have the server as
close to the clients as possible
sorry one of the participants be the
help of the star well potentially with
the new feature that Chris was
mentioning where you can add a remote
stream as one of the inputs to a peer
connection because of the way that it
works it's actually going to be rien
coding each of those streams and it
doesn't scale very well but it can be
done or into the so the benefits of the
masher are obviously there's no server
which is big deal because you know you
don't want to be sending video through
servers if you don't have to because it
cost money to maintain them and run them
and adds a lot of complexity and makes
your life harder it is a more complex
session management when especially when
you deal with you know all the multiple
device capability stuff that we're
talking about but you know it's not
really something that you can't overcome
with more JavaScript yeah the bigger
limitation is it's hard to scale it to
more than about 4 participants because
that's roughly the limit of how many
streams you can simultaneously encode on
most laptops before you start having
problems but the main problem with it is
that it doesn't really work very well on
mobile because most mobile devices
struggle to encode a stream once much
less three or four times
yeah absolutely you know a lot more with
data and that's not a hard limit that's
just kind of a rule of thumb so you know
with the pros and the on the star side
our you know it's a lot less load on the
network as you're only sending one
upstream video and since most network
connections these days are still
asymmetrical where you have more down
streep in with an upstream it's a scarce
resource you don't want to overuse it
because you might you might be able to
send one upstream HD stream but you know
you might not be able to send to and you
know winds up being higher quality on
low-end devices because just you know
they are able to encode one stream
reasonably well but encoding 3 or 4 it
can be a lot harder and potentially you
can have an unlimited number of
participants and there's a lot of other
extra functionality you can do on a
server once it's getting the media like
do recording or speaker detection or
streaming to you know a much larger
number participants potentially the
downsides are that you know that you're
running servers and you know as as you
know you came up minute ago it's very
latency sensitive so if you're you have
users and Asia and your servers are in
North America then they're gonna have a
bad experience so if you want people
around the world to have a good
experience you need to run servers
around the world so you can get very
expensive very quickly and it can
actually be worse worst quality for some
people if you do a lot of conferences on
the same land within the same company
then it doesn't really make a lot of
sense to send your video through a
server out there on the Internet
yes
we yeah we are we are using AWS which I
highly recommend there's I was going to
have a separate section on that but I'll
just give you my one quick pointer on
running on AWS it's great run large
instances or higher so you have a
dedicated NIC so that's people also
often say well AWS isn't great for real
time because you there's
unpredictability in the network that's
you know very true on the loading of
smaller instances and the largest says
it's not so yeah we actually run in all
eight regions around the world which is
you know wonderful thing about AWS it's
very it's not really that hard to do
that so how do you know what is the
server in the middle actually doing well
you know actually you know step back you
know the server in the middle this
concept has been around for a long time
and kind of the legacy video
conferencing telepresence systems
usually call it an MCU some people call
it a media router some systems have both
some systems just have one so you send
it up there fans it out
so how it's actually working is it you
know there's a few different strategies
the way that most of the legacy systems
work is that you the server decodes all
the streams that it's receiving it they
call it mixing although it's you're not
really mixing the video you're kind of
laying them out side-by-side on and
creating a another composite stream that
contains all the streams that were sent
to Inge
sent to it and then you rien code that
composited stream for each of the
clients is connected to the server and
so that you can match the encoding to
the capabilities and the network
connection quality of that device and
this tends to give you very good quality
very low load on the client
unfortunately very you don't have a lot
of flexibility on the client because the
server decides how the videos are laid
out and which ones everyone receives and
this is very processor intensive on the
server another option is that you Rhian
code each stream for each client
separately on the server just more
flexible than the mixing strategy but
it's also potentially even higher load
on the server another approach is that
you actually don't decode any the video
on
server you just restrict the bandwidth
of all the different clients to not send
at a higher rate than any client can
receive at so you find the client with
the worst network connection and you
send it a rate that it can receive and
you just forward the video to everyone
which you know the great benefit of that
it's very low load but it's potentially
very poor quality if someone happens to
be on a very low-end device or on a bad
network so and then the other some more
advanced strategies that you can you can
use to adapt the stream without Rhian
coding without necessarily losing
quality that are supported within the
WebRTC stack but unfortunately are not
currently exposed in the in the API so
you can only use these if you actually
have control over the encoder and those
are temporal scaling and simulcast where
to Perl scaling lets you adapt the
bitrate by throwing away some frames
without introducing any errors into the
stream and simulcast lets you send the
same stream at multiple resolutions
without incurring the full overhead of
encoding twice and the result that
really probably makes the most sense for
WebRTC is something along the line so
what Google does with hangouts and that
is that you send two streams to the
server when I said what Google does with
hangouts I mean with the new version
that's based on vp8 and will soon
support web RTC
not necessarily what they're doing with
the older version which uses a slightly
different approach so you can you can
send two streams to the server and you
know for small differences in the bit in
the network capabilities clients you
just if I have one client that can
receive at 1.2 megabits and one at 1.3
you just force it to 1.2 you don't have
to decode you don't have to re-encode
everyone's happy they can't tell the
difference it's a big difference
then you selectively re-encode and you
use some of these strategies like
something like temporal layering to
adopt further for additional clients so
and that winds up being the way that
minimizes the kind of trade-off between
cost and quality so just a few other
quick points
just considerations on the UI front when
you're building WebRTC applications this
is probably pretty obvious but you don't
necessarily see every application out
there making the video as big as they
possibly can it's something that you
know most of the demos do but in
relation sometimes you'll say oh I want
a header and I want a footer and I
squeeze this sidebar and but the quality
of the experience is more or less
proportional to the size of the video
assuming it's high-quality video and
there's a there's a visceral difference
between the kind of connection you feel
with the other party if they're if
they're video at least uses the full
window and you know and hopefully full
screen you know put a full screen button
in there and encourage them to get into
full screen if you can an HD you know
takes it up a whole another level it's
something that people think I don't need
HD or what difference does it really
make you know it's something that you
don't think it makes a difference until
you do it and you're sitting two feet
away from a screen with 720p 30 frames
per second video and you know that's
when you cross the threshold from video
chat to telepresence where you know it's
up its really can be a substitute for
being there in person so the other one
you know if I can impart one more pearl
wisdom here is that no one ever noticed
this a little bar at the top that asks
whether you the chrome can use your
microphone and camera that's kind of one
of the most annoying things and that
almost every web RTC app lines up with a
solution looks we call this our big
green arrow
so I encourage you to make it modal so
they can't get away from it they have no
choice but to make a decision there so
and seriously you have to do that or
else they won't find it and but you know
do be aware that even in chrome the
position of the buttons varies from
platform to platform now Linux and Mac
and Windows the buttons are in different
places on the screen and on chrome for
tablets the bars at the top and chrome
for phones is at the bottom
so things to be aware of and that's
that's it that's really all that I had
to cover I don't know how we are in time
but I'm happy to take questions if
there's time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>