<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Economies of Scaling Software | Coder Coacher - Coaching Coders</title><meta content="The Economies of Scaling Software - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Economies of Scaling Software</b></h2><h5 class="post__date">2013-07-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/43Pzk2ngyjY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">after doing everyone thank you guys for
coming up this is the economies of scale
in software and I'm abdel-rahman II my
twitter handle is paula matic coder i
would be posting a link to the slides at
the end of the presentation I guess if
you follow me you can give that before
everybody else this is a little bit
about myself I'm a platform architect
that just got me I am a frequent speaker
in a bunch of conferences I mean Jack's
of last year javaone or dev and I am a
open source advocate and a contributor
and I've been pretty active in the
community leading a few of them java
users group tins groups and such that's
my email and I also make my slides from
this talk and out of talks available on
SlideShare this is just tells you that
you can do whatever you want with these
slides and share them with other people
and improving them if you would like to
this is my first time giving this talk I
was supposed to give it the Java one
Russia but like a month ago but that
didn't happen but this is my very first
time I would like your help to help me
improve the quality of the content of
the presentation and if you go at the
end of the talk two speakers speak of
score calm and / Jack's comp dash
scalability you'd be able to I mean
you'll get the PDF of the slides as well
and you'd be able to tell me how much I
sucked hopefully not so let's go so
what's up with the title I chose this
title relating to the concept in
microeconomics of the economies of scale
and the Wikipedia definition says that
economies of scale are the cost
advantage that Enterprise is obtained
due to size so pretty much often
operation operational efficiency is gris
a greater with increase in scale so the
bigger you are the bigger your
organization is designed to be the more
efficient and the cheaper it becomes two
and two to scale this slide is titled
the line is blurred it was a time when
only enterprise applications worried
about this issues of scalability I mean
if you're somebody who's at home once I
mean building a website you said
t-shirts whoever it is that didn't that
wasn't an issue that you were concerned
with but the rise of social and mobile
it not only increased the Internet
traffic but also created this breed of
spoiled users that would ask questions
like I want to see the closest Moroccan
restaurant to my current location on a
map along with consumer ratings and
whether any of my friends has recently
had has recently checked in in the last
30 days these are really hard questions
that take let's take up a lot of
resources so that sits set up the bar
pretty high and scalability all the
sudden became everyone's problem so what
is scalability the most common
definition if you go and look around
online you'll find that people with
define scalability as the ability of a
software to handle an increasing amount
of load without performance degradation
I happen to have a few problems with
their with this definition the first one
is that it comes with the implication
that we could scale these systems
forever that we could that a system for
a system to be called scalable is a
system that can sustain that scalability
forever which is absolutely not
realistic because it fails to recognizes
that there are these external
constraints that we have around us the
second one is that it fails to
acknowledge that being scalable is very
relative to the specific use case that
with the at hand and
also it does not take into an account
two things that it's not about a system
being able to handle the work all at
once but it's about the system's ability
to evolve to actually be able to evolve
to handle the work so a bit of
definition um it's one I put together it
would be more like the ability of an
application to gracefully evolve within
the constraints of its ecosystem in
order to handle the maximum potential
amount of work without performance
degradation this is easier said than
done everybody talks scalability disc
scalability that turns out that it's
just not like a button that you turn on
and off you could literally build an
application that is capable of
supporting 1 million users and you add
one more new feature and your
application starts crashing at 500,000
so the approach that to look at
scalability differently we could look at
it in terms of what we call the
bottlenecks in the sense that
scalability is about relieving or
managing these limitations or
constraints that way we call bottlenecks
again and when we talk about bottlenecks
in Computing's we talk about the usual
suspects we talk about the CPU we talk
about the storage room I oh and we
talked about the network and they all
kind of interrelated with each other so
the rest of this talk is structured
around these bottlenecks to make the
case that one's scalability needs or
concerns are to be addressed in this
fashion as well so let's talk about the
CPU bottleneck so nothing affects your
CPU more than the instructions that it's
summin summin to execute in other words
this is about the very code of your
application this is about how your
application is written so
first of all how what is what would like
a scalable architecture look like and to
define architecture the best definition
that is out there is one of modern
flowers that defines it as things that
people receive as hard to change and he
around this very nice paper called who
needs an architect it's very short PDF
that is very much worth reading so but
in other words an architecture would be
these decisions that you can meet you
and the ones that you are stuck with for
the rest of your life for the life of
the of the software itself so you might
want to think twice and before making
these decisions and when we make these
decisions we are talking about the
choice of the technology like choosing
the right platform or choosing the right
language or languages the frameworks
that you choose to go with all their
libraries and also making the right
abstractions with these abstractions of
technical abstractions or they are
functional abstractions and to make sure
that all technical abstractions are
subordinate to the functional
abstractions not the other way around so
a scalable architecture effectively
emerges from a writing writing good code
so what is writing good code what does
it mean mean it means a lot I mean
that's a whole different talk by itself
means that you should think your
algorithms through and and be very
mindful of their complexity means that
your design would need to be solid means
that you need to understand the
limitations of each one of the
technologies that your that you use in a
leverage their strengths and weaknesses
and they means that you need to practice
TDD and all of that kind of stuff news a
bunch of tools I mean etc it's a whole
different taught by itself so I decided
to add this slide no you stuff mean if
put together like I've been like few
seconds a bunch of books that everyone
like everybody that calls himself
software
an engineer should at least read twice
and the list is very long so write good
code but we write good code when we do
all that and we're really careful and
very disciplined and we still end up
with this if you wondering what the
picture is it's straight up for an
article called the fading tradition of
making Kyle dunk piles they included a
link there for you so you can go in and
read the article it's the Arak Oracle
for yourself so we we end up at the end
of the day with a junk pile all right
but it's still much better than this so
this to transition to this these slides
will pretty much tell you that technical
debt is a reality no matter how
disciplined or how good you are because
of time constraints because of different
factors the availability of resources
you will end up with technical debt one
way or another so all these things that
are quick and dirty you know that you're
not really proud of things that you want
you would have done differently or
should have done differently and all of
that kind of stuff and after a while it
starts to smell I mean in the sense of
code smells of as a martin fowler calls
them but the bright side is that as soon
as as as as long as you recognize it as
dead as technical death your your
technical debt you're you're on the good
side and as long as we keep track of it
and then your factory so for the
fairness you want to be wise and then
think twice and actually and cut the
right corners and don't lock yourself
out and don't may technically dead as a
part of a part of your arka-dal the
architecture of the software that you're
writing I mean you know you don't have
time and you can either do this or that
make sure that you make the right call
and don't make something that you're
stuck with forever so this leads us to
the first section which is scaling up
your application so what is scaling up
when we say scaling up we are talking
about vertical scaling that means that
we are talking about a system with a
single node
and it means that we are adding more
computer or computing resources to this
particular node simply getting a beefier
machine what what scaling up your
application is about it is about writing
code to harness the full power of the
one node by writing code that is
parallel for parallelism which is
writing concurrent code or code that
executors execute in a concurrent way so
most of us are used to writing this code
that runs within web containers by
extending one class or like another one
that that that that becomes
multi-threaded just by extending a
particular class because we depend on on
these frameworks but the reality is that
sometimes that the complexity of your
business logic with demand that you
actually break down that business logic
in smaller step have to execute them in
parallel and then have to aggregate the
data back to get a result within two
together to get a result within a
reasonable amount of time if you
actually go and execute this business
logic in a sexual sequential fashion it
just becomes way too slow so you need to
break in break it down and then spawn a
bunch of threads and for Kim and then
join back to come up with their with an
answer this is not easy I often requires
the synchronizing state if you have a
state to synchronize which is a
nightmare nobody likes to do that so on
the one machine to move a little bit to
talk a little bit about the hardware on
the one machine because of Moore's law
we have been reaping or gaining
performed or realizing performance
automatically so you write a piece of
code and then you take it out of a minha
fitting indicate you take out your war
file and you deploy it on another Tomcat
server there is on a bigger machine and
your code it's faster your code becomes
faster because there are more resources
available available for for for the CPU
more memory or or whatever
so what the problem is we experienced
what is called the end of moral and
Moore's law that means that we just
could not fit it could not build faster
CPUs or faster cause anymore so what we
end up no ended up boy who ended up
creating the one ship would like
multiple cores in it so if you actually
want to take advantage of this you
actually need to sit down and write code
that is that that can run actually on
multiple cores the good news is that
there are a lot of frameworks out there
libraries that makes this easier for
users like for joining Java 7 there is a
car and etc so it makes it all these
abstractions that you could take and
take advantage of but this is easier
said than done because we still have a
lot of challenges the first one about it
is all these like third parties and
dependencies that we pull in that I'm
not open source but they are not written
to take advantage of the multi
multi-core architecture the other one is
that synchronizing state is much much
harder because we're not now worried
about multiple threads within the one
cpu we're worried about multiple threads
across multiple course you can either go
immutable which is not always
straightforward or sometimes not even
possible or you can go functional
because and that's a completely
different paradigm shift and it's a
steep learning curve so it gets more
interesting you might think that
actually throwing more coal of course a
day that would actually give you more
performance you get you realize more
performance by just running your code in
like ten chords instead of two or like
four but the problem is am and those law
tells you that you get to a certain
point when you actually end up
diminishing return literally you speed
up speed up your application you get to
a point where no matter how
course you add you know your coat just
doesn't get any faster so this is what
scaling up the application is about now
scanning out this application and when
we say skin is scaling out we talk about
horizontal scaling which means that we
have a cluster or a distributed system
and then we scan it by adding more and
more notes and what this implies that we
need to actually write code to harness
the full power instead of the machine
that we need to actually write code to
harness the full power of the cluster so
we're talking about a topology like this
that we have a typical cluster that has
a number of identical application server
nodes behind the load balancers and I've
got number and identical and load
balancer in bold so when we talk about a
number in a cluster it depends actually
of how many you actually need and you
can afford you can have a cluster 100
nodes you can have a cluster cluster of
two notes this brings out to them the
the topic of elastic scaling or auto
scaling which means that you actually
the number of their live nodes can
actually very much depends on the load
your application your application is
ender and that new node would be
provisioned if there is more node and
then the the the size of the cluster
will shrink if you actually don't need
that load anymore and that say yeah a
typical practice and when we talk about
identical we we we mean that every
single node every single application
node is cloned off a image file they're
identical the exact the exact same code
base and there are a lot of tools right
there that would allow you to do so you
can use share for you could use puppet
or you could simply just use clone your
code off of Amazon ec2 am I images and
the last component of major component of
a cluster is them this load balancer
which means that we have load that is to
be distribute
it evenly across this this cluster
according to some some kind of algorithm
round robin is typically you know the
standard you know configuration so we're
talking about something like this we
have all of these people over there you
know behind a firewall we have the load
balance so when we have these nodes and
then the requests go through the load
balancer and they get routed to one of
those machines so in a cluster one of
the biggest challenges is actually to
manage state across this cluster and
state session data as an example is
managed differently depending on your
needs the first option is that session
replication it's me means that every
time node a rights to the session rights
a piece of data to this session there is
this framework of this piece of code
that would actually go and replicate all
that data photo in the rest of the notes
so that session data is available for
them and it can be read the second
option is what is called session
affinity or sticky session incision if
you would say that you have session
affinity if actually the load balancer
is actually more active and more aware
of the source of the request and that
the load balancer ensures that the
request from client a will always be
routed to server a so he knows it's
coming from this IP IP and always go to
server a that way you can write to this
session and the next time you can like
read from it and you're in the exact
same machine and there is no need for
replicating it across the cluster which
is a big big overhead the problem with
session affinity is that when the no
dies you lose that session data so it
just end up eventually being routed to a
different server and then you don't find
your data over there and this is can be
pretty bad if you're in on amazon com
and you just added something to your
shopping cart and then you go and you
refresh the page and it's gone it's not
there anymore the other option that you
have for managing
is the shared of distributed session
when actually session is stored the
session is stored in a centralized
location or at least to you it is on a
centralized location it might be in like
a storage that is somewhere else like
memcache or anything else or might be
that it's actually managed by a piece of
software that actually goes and allows
you to look at memory or look at the of
the cluster is just like one piece of
one unit of storage so do yourself a
favor and go stateless because managing
session is just a nightmare across a
cluster so just don't use it at all and
go stateless in the end in that way any
server would do it doesn't really matter
where them or to where the load balancer
will send the request and you can just
do your business because you're not
really dependent on some states or
expecting some state that you had very
written before so now we talked about
parallelism on the one node which most
of you are familiar with now we want to
talk about parallel is parallel ISM
across the cluster right so to be able
to harness the power of the cluster
there are all these operations that are
just maybe deal with a lot of data are
like too much data or for like the one
note to handle we could leverage
technologies like mapreduce Hadoop is
the one of the implementation which is a
programming model that processes data
sets in parallel and and then this show
or execute distributed algorithms on a
cluster so other concepts in relation to
scaling out your application that you
might want to get yourself familiar with
an assess whether you actually need them
or not is the distributed lock managers
of a dlm when actually you need
synchronization of like shared resources
across the cluster I was like a paper
think by them by Google but the Google
chubby you could use uki
it was like an array I mean a list of
other technologies and also you're going
to have to actually worry about
distributed transactions HTTPS comes up
all the time what what would you do to
these HTTPS sessions you have you can do
your options as either to end the HTTPS
session on the load balancer or to
actually use a wildcard ssl certificates
that would allow any node pretty much to
to do the handshakes and also you want
to actually consider leverage in
probabilistic data structures and
algorithms things like bloom filters and
quotient filter filters and etc and
these things I mean if you do a quick
Wikipedia search on a bloom filter these
are just like little simple algorithms
that would answer the question whether a
particular piece of data is a part of a
set or not so it would this would be
instead of actually looking up something
in the database it would be just running
it through a filter in memory and be
able to get you answer and answer right
right away so deployment right now so
take a typical deployment environment
right here in bigger bigger organization
you would have multiple environment your
dev environment testing stage and
production I'm sure all of you is
familiar with this a lot of you need to
actually have some kind of automatic
configuration tool as well and you as a
you you should probably like practice
something like a continuous delivery
which is out of the scope of this
presentation which it means that your
your developers are right in production
code every day and you're deploying as
often as possible 2222 production and
running a bunch of tests to do that in
deployment you want to leverage the
cloud I mean you can there are different
degrees of of cloud computing
you could just run your own cluster of
the ec2 instances and put them behind
the load balancer they will be using the
infrastructure of us or as a server as a
service or you could throw in your war
as a putt as a pass and not have to
worry about that at all so this is um
dealing with the CPU bottleneck which is
just starting to actually write your
applications differently in a different
way and starting to take advantage of
all of these technologies so the storage
but a bottleneck or the i/o is the most
probably significant minutes it's a it's
a nightmare so the first part I want to
talk about is the persistent store we
there was a time when actually we didn't
have to think of what data store what
type of data so when I use the answer
the obvious answer the de facto answer
was oh yes use my sequence let's use
Oracle it was the LG BMS model and which
is a model that has a schema that
guarantees data integrity your data is
normalized you have acid transactions
your data is stored in a way that is not
biased towards any kind of query pattern
and it has this very nice flexible query
language that we can use later to query
it however we want called a sequel the
problem is that our data sets as our
data sets grow we started actually
scaling vertically and we mentioned the
actual database system so we bought
beefier machines which is not cheap at
always expensive we spend a lot of time
doing database tunings doing query
optimizations creating materialized
views and all of these things and we
started d normalizing and the data we
keep you keep hoarding data and to the
point that we hit the limit of the one
machine we just couldn't do anything to
make it better anymore so we attempted
to actually scale out the a DBMS system
by actually a run in relational
databases on a cluster and we have these
architectures this my master slave
architectures we started doing data
arding when you actually take your data
set and then you shard it by some kind
of key like employees from A to F our
node 1 and from F to Z I mean no 2 etc
but we failed that did not work so why
is that this is because of Eric Brewers
cap theorem of distributed systems that
literally tells you that you can only
have two out of the three you can either
have consistent all of the three
consistency availability and partition
tournaments partition tournaments simply
means that you're running in a in a
cluster that is full tolerant so the
relational model is actually designed to
favor CA two favorite consistency and to
guarantee acidity of transactions and
all that and availability you want the
database to be available all the time so
by default or by the cap theorem you
just cannot scale it horizontally so
this gave the birth of Owen caused an
explosion of a wide range of specialized
data stores that we call the no sequel
movement whose goal is to address these
issues then the biggest issues is to be
able to a run your data store within
like within a cluster mostly it's a wide
variety side of the scope of this
presentation but some of them are some
of them are key values data stewards of
column base some of them are document
based and we have also graph data stores
which are which are not actually
designed to run to run on a cluster but
they actually look at the data
differently and they allow you to query
them like a like a graph these are a
bunch of solutions like out there this
is again out of the scope of this
presentation but it comes the concept of
to mean this is fairly pretty new sudden
we talked about polyglot persistence and
what polygon persistent is is is the
idea that is based on a claw
acknowledging that the complexity of
these applications that we are building
and with them cause different ways of
query in the data and will cause us to
deal with different data altogether and
the fact that you could actually fit
these different data and different query
pattern in one model and expect no
problem is just absurd so the solution
is polyglot resistant which is just
liveries use more than one data store
and live leverage them within your
application you know store graph like
data in a graph database and store in
relay data is highly relational that is
designed to be reporting every poor
union like am i single my sequel
database or a relational database and
simply have your application query it
from the different data stores and
aggregated and construct this model to
an impasse it along through your your
business logic here comes the important
of things like parallel ism you know
inviting code that is concurrent writing
good code that is concurrent versus
actually sequential processing so for
more details I gave I think a no sequel
talk last year at the jacks 2012 I think
it's available on YouTube that's the bit
ly URL and if you just google the rise
of no sequel and polyglot persistence
just go to the one that is from the
jacks conference I think it might beat
your instance in there so the second
thing that we want to talk about after
the persistent storage is caching and
caching is simply just a large simple
key value data structure so instead of
us incurring that overhead of data
retrieval or incurring that overhead of
camp computing something every time you
want every time for every single every
single request we simply cash that and
we make that available cash it in memory
we make that available or somewhere and
we make it available so since we catch
we can't catch everything this is not
a dump of data in memory there are
different cashews can be configured to
use multiple algorithms you know any of
you the least recently used for is to be
cached of the most recently used it
depends of your specific use case so use
caching aggressively if you want to
scale as much as you can flat out so but
what you cash mean again frequently
accessed data like your session data for
example if you have a feed in your
application you might want to consider
like caching that one way or another or
cash things that take a long time to do
to compute so where to cash you could
cash on disk the file system of course
it's a terrible idea it's very slow on
sequential or you can cash in it there
in a database which is a little bit
better because data is actually around
arranging data structures that are
designed for efficient access they have
you can add have indices and all that
kind of stuff but it's generally a
terrible idea even if you're running on
like SSDs but the reason why I mentioned
it is that we can talk about these
operations that tell that take a long
time to compute and this is this it
might take two hours or three hours to
compute you just take the resultant and
you put it like as a file that's like
much much better than actually going
through through the whole thing or you
can actually cash in memory fast you get
random access but mean it's a volatile
or you can have something in between
like using a no sequel database like
Venus as a persistent cash so there are
multiple types of caches there are like
local caches that are local to the
particular instance or the particular
server that you're dealing with there
are like cash replicated caches which is
the same way that that that we mentioned
before managing a replicated session in
the cluster that you're right a bit of
data to the cash in node a and it
eventually get replicated throughout the
cluster there are distributed caches
when actually the system starts viewing
or the programmer
it starts viewing the cash that is
distributed across many many nodes such
just like one unit of storage and there
are like clustered caches which is
completely different caching servers
that are like clustered like memcache
for example servers so what is this
caching you know how to cash so most
caches implement a very simple interface
it's extremely simple but they're the
rule of thumb is always attempt to get
from the cash first using a key so since
it's just like a map in memory you just
say get me this key and you get
something in return you get your data in
return if it's a hit that means if it's
in the cache then you save yourself the
overhead who actually having to look
this stuff up in the database and if
it's a Miss then it's okay I mean you
just actually have to go back to the
database and then do your retrieval and
actually incurred an overhead and you
might be good for you to actually stick
it back in the cache so next time you
look up you get you get ahead instead of
instead of a mess when you update a
particularly source you can actually
evict it from the cash you can assign a
time to live when you actually put say
just keep this in the cash for five
seconds or two seconds and there are
different scenarios where that actually
becomes useful and there are many common
operations it's a very simple interface
so in cash in there are certain patterns
that you might want to be my that you
want to be aware of the first one is
actually caching query results when you
have this frequent query that you hit
the database words to select everything
from table a for example a new limit of
10 rolls the way you would cash this is
you check the actual query itself you
hash it and then use that as the key so
the next time somebody comes with a
query you take that hash and then you
look it up in the cache and then you
return the result it's very simple but
it becomes quite interesting we're
actually dealing with permit vines
queries if you have select star
employees where ID is 125 the hash or
like it's not the same as select
employee where hash is 13 whatever that
is right so the way you would do that
is actually to hash the actual query and
then you have the parameter values so
and you'll end up with eventually
multiple instances depending of those of
those parameters but that would be like
probably the best way to do it you can
actually use hash to do a method or
function memorization when we're not
dealing with data access that means a
method that computes resources me has
like parameters you take the method name
instead of the actual query and you hash
it and you stick it into cash so next
time that method is called you actually
go and you look up and see if you've
already done that before if it's ahead
and then you return the result without
actually executing the code just like a
big if statement that that is that
encloses or encloses to memorize the
method or you can actually cache object
and this is a different approach when
you actually take an entire object graph
you serialize it one way or another and
you take the identity of that particular
object you just stick it in the cache
the ID of an employee for example and
every time you look an employee you look
up employee number one two three four
you just go to the cash and give me
whatever is key that's one two three
four and you can get the employee
employer looking for so another caching
pattern is when you actually dealing
with time series data sets that means
data sets that actually frequently
updated with time like and actually the
top of a feed or a Twitter feed or a
stream of messages wherever it is so a
lot of people are pretty much satisfied
with data is not real time data is
pseudo real time we're not near
real-time mean people are fine with I
probably tie a tweet and nobody would be
able to see it for like a second or like
two seconds so you would use caching in
this fashion to actually file access to
this particularly solid source in the
sense that you would cast a cache the
latest page of the feed and you say Yuki
it as feed for example or with like a
particular key and say leaving in the
cash for like five seconds so everybody
who hates the cash
within the five seconds it's going to
get you know that copy and you never
make when you never make a query to the
database but then that thing will just
eventually expire and go away after five
seconds then you make the second query
right so you'll be making a query purty
time and that's and that's and that's up
to you up to the the TTL time that you
actually decided to sit so it's not
trouble three times you really gotta be
very careful and actually most
importantly actually profile your code
to actually assess whether you actually
need cash to begin with you don't want a
little your code with cash here cashier
cashier and go all crazy if you actually
don't need it or you don't have any
significant performance gain so you
really want to be careful also stale
data might actually bite you hard when
there is an incoherent incoherent so
there is an inconsistency between the
actual data in the database which is the
truth and the actual cash in front of it
that you use this might easily easily
happen in two cases it's assumed that
you actually would like to look up you
cash these objects you would like to
look up these employees by their last
name and you would like to look em up by
their idea as well so when you are you
and you actually affect the ID 123 but
you forget the object with the last name
in the cash so you have like two
inconsistent copies that are like just
like sitting there so and so you really
want to be careful they're also stale
and nested aggregates employee has a
bunch of children and then you take this
employee in you one two three you stick
him in the cash and he has three kids
whoever it is and that data actually
ended up would end up being changed in
the database he is them dependence of
his children's information but it's
still on the cash also the network
overhead of mrs. might actually over
wear away the performance gain of that
you get from that you get from the hips
if you're missing missing missing every
time you go and you look up in the
you miss you miss you miss that's
actually time that you actually spend
going to the cash and trying to get
between that information so you really
be mindful of that if you're missing
more than your head in you're just
encouraging that that latency and you
wanna that's what it would actually
would make you question what what type
of cash algorithms you need whether you
need like caching at all also consider
writing and updating the cash when you
actually write to the data store so
consider the having some up in around
some coyote some kind of transaction if
that fits your need to actually every
time you update something will you put
it on the data store you stick it in the
cash if you actually know that you will
retrieve it you to retrieve it later
featured solutions wide everyone explore
is eh cash mem cache coherence radius
which is actually a not considered to be
a cache because its persistence a
persistent no sequel data store which is
just like a large map support a lot of
nice things like built-in data
structures like sets and listen as like
very intelligent key is in namespaces if
you're interested in come and talk to me
after the talk so the last ball neck
that we would like to talk about is the
network we talked about the CPU and how
you actually need to write your
application to maximize inheritance
around us the full power of the one node
how you actually can write your code and
use technology is to actually harness
the power of the entire cluster we
talked about the data store and we
talked about the different options
different data stores that you can you
can use and can even like use multiple
then wanna be polyglot and we talked
about using cash in how it's a good
thing to use aggressively now the last
thing is actually dealing with the
network bottleneck asynchronous
processing so some often we would have
these resource intensive tasks that are
not practical to handle during an HTTP
request window you have this piece of
code that takes 10 seconds to run you
couldn't do anything about it
you you know parallel process it you
just it just takes that much time are
you dependent on some third party or
whatever it isn't takes that much time
so so this is where sync asynchronous
process in would be a good use case for
symbian synchronous is overused and it's
not necessary most of the time sometimes
you look at your request will you catch
your API and you really say well this
one does not really need to be
synchronous so you really give it like I
gave it give it give it give it a
thought when we're talking about a
synchronous pasta say we're talking
about different patterns the first one
is actually pseudo synchronous
processing we're talking about a flow as
follows when you actually pre process
data do operations in advance like a
nightly job wherever it is so that way
the next time somebody hits you with a
request asking for the result of that
particular application or requesting
that data just respond synchronously and
you're giving this pre-processed data so
this is cool but sometimes it's not
possible when you actually dealing with
dynamic dynamic content and you allowing
people to pass through different
different different parameters etc the
second pattern is actually true
asynchronous processing and the flow is
that follows the request comes in or for
the data for like to perform a
particular application and your server
immediately acknowledges it and by
acknowledging we mean that you actually
return and HTTP 201 accepted status code
the right away a I acknowledge it I got
your request and you actually allow do
the processing at your own convenience
you just like throw in some work is a
worker thread to take care of that or
like push it to some kind of job queue
and you do that processing and you can
give the curl the user the courtesy and
allow them to actually go and check for
progress this would be like you placing
an order in amazon com and they tell you
all right your order has been placed and
you can go and say our processing
shipping shipped you can go every once
in a while and then check on that
on that protein de on that proton there
on the status of you are a of your order
versus actually clicking and waiting for
two days until they actually ship you or
like two days until you actually get the
product delivered if that's even
possible and which is not your timeout
so this is what a true asynchronous
processing is there are a bunch of
techniques and technologies that would
like that you could leverage some of
these are jobs were task you there are
different names for them JMS same QPS
have different implementations and
service out there AWS SQ and such and
which simply takes you take a particular
you know york knowledge of requests then
you take that job and you push it into a
queue and you have a bunch of workers
you know they actually pull from that
queue and into the processing later so
you immediately just like push it over
there so the second one is actually task
scheduling you could leverage a bunch of
libraries when you're actually
periodically you'll have all these jobs
that periodically would go and pull from
these queues every hour or every two
hours whoever it is and you'd have some
kind of coordination over there and it
was also like batch processing if you
look at your application and you're all
asynchronous and you think that it's
just better good then more efficient to
actually ship orders all at once so you
have a ship in queue and then you just
like batch like 10 at once and then
you're right you you you move on so the
speaking of network the network bottle
addressing the network bottleneck the
next thing after a synchronous
processing now after being a synchronous
as much as you can because that makes
you more available we want to talk about
this CDN and what syrians are for like
everybody else so everybody is on the
web we have this static content that our
app that is that our application would
serve to these users we're talking about
videos on audio files wherever it is a
lot about who actually go and put in
their webbing from your actuary
if you're running like a java
application or would actually just make
it available anywhere else other than
they you know outside of the web em
forever you want to make it available
and we have these web objects that are
static we have this HTML where these
javascript files we have CSS and we have
all we have all these things do not
serve this through your application
server just simply don't because
somebody requests your one page your
index page and your index page includes
ten to CSS files and ten JavaScript's
and every single and that just like
swamps your server and keeps it busy
every time somebody goes and hits your
index page it's just unnecessary your
server your application server there is
there to execute and be able to be able
to respond to users at all time and
execute business logic not actually be
worried a lot pulling pictures or
serving binary to play your intro video
wherever it is and this is why we would
use a CDN for and what these syrians are
it's just a large distributed system of
servants deployed in multiple data
centers across the internet they're like
all over the place that you actually pay
for I can I AWS cloud farm its front
it's just like a piece of storage
instead of storing your pictures on your
server you put them in a in a in a CDN
and you can just like start pulling that
data from there not trouble free of
course the gunshots when you first deal
with this CD ends its versioning and
Kashi right so assume that you have like
a script filename the script digest and
you deploy energy in a CDN and it's
perfect because you know that this file
is available everywhere in the world in
these edge notes the Japanese people
requesting that script that jazz they're
going to get it from the closest server
to them which is the server in Japan the
mayor the American ones are going to get
it in there from the American node and
it's perfect it's available everywhere
great so what C and does it just takes
scripta jess and then puts all these
copies on all these like edge nodes for
for them to be served and be made them
available also every time your japanese
client is going to go to the japanese
edge node and it's going to go in cash
in his local browser cache script that
jazz so browser most browsers do that so
you end up with another layer of
redundancy so the problem is that when
you actually go and fix a bug in script
that yes and you actually update it that
new content will never be able to be
replicated or propagated propagate
throughout the nose or the edge knows of
your CDN because the file is named the
same yupo you deploy a new script that
jess and all of these notes look at it I
all over the app script digest not
realizing that they have the older older
version right so what you can do and
what a lot of Syrians will allow you is
you actually allow you to invalidate
script that jazz and all of these nodes
and it takes a while so they can get the
new copy automatically now you still
have another problem which is all of
these clients that refuse to go and get
the newer version because they think
they have it they think they have
scripted scripted jazz not realizing
this still have the overall diversion it
sucks so how do you deal with this with
all these dirty state going out of
control I mean how could you solve the
problem you could simply append Oh
actually version eyes these files are
these static script files by actually
changing their name instead of script
dash V nov.1 that jazz when you update
or fix the bug your wave up the version
you have script dash V true that jazz
and all the sudden you have a complaint
completely different URI that the edge
nodes will actually realize all this is
a new file let me pull it and all of
these clients actually say oh this is a
new file it's not the old script v1 that
jazz that I have as simple as that you
could also
vidual you set up the HTTP caching
headers properly or force the
invalidation of the particular file on
all edge nodes and you could do ever you
want I mean depends on what city and you
use and they have like different ways of
configuring things the last thing I want
to talk about to address the network
bottleneck is the CDN it's not the CDN
is the DNS so DNS says everybody knows
it the domain naming service what we
don't realize is that when you register
a new name in the registrar they give
you your own WWE no Superman com it's
yours they give you this DNS service for
a server for a service for free right
the problem with that is that if you
want to write systems that actually
scale you're going to have billions of
requests actually hitting that DNS and
asking a give me the IP of ww Superman
calm and they're going to get
overwhelmed you're going to get
throttles and you get a slow down you
know a great deal so you won't be able
to scale so what you want to do is
actually consider starting pain for a
better service consider using solutions
like AWS route 53 or ultra DNS that
actually will charge you a price for
like a fee but they have the capability
of handling their large amount of
requests that would allow you to
actually scale because you sit there
you're like my why is my server so not
responsive why is it slow what's
happening and it's actually not you it's
actually the DNS that you are using
which is most likely the one the free
one that comes with the registrar so the
next section is about qualifying
scalability you know actually measuring
knowing what scalability is now she's
like just a few slide which is like very
short because we went through address in
it how we can write scalable
applications and address the three
bottlenecks you know differently by
leveraging technologies all right in our
code differently and all that kind of
stuff now let's qualify this scalability
so talking about instrumentation make
sure that your bacon instrumentation in
the co into the code early that means a
bit all right piece of code that
actually simply sends you know data how
long a particular request takes you know
llamame trio camera metrics for example
is an example of that you could
literally hit that servlet on just like
a filter server that will tell you how
long particular requests our requests
like a request steak make sure you have
monitoring will monitor your application
health or cluster health the individual
nodes down to the jvm matrix or the
system resources and you track keep
kpi's or performance KP is the number of
requests handled stands out the
throughput and the latency and also
something called the app Dex index which
is a standard that could give you a
number that tells you whether your users
are satisfied or not literally how many
of your users got pissed off because
they waited too long and stuff you can
Wikipedia the ethnics index logs as well
really one all this throughout the
clusters want to make sure that all your
logs get channel to some kind of data
store so you can actually go and analyze
them later testing make sure you load
tests these are a bunch of products that
would actually help you out with that
you know ganglia for monitoring na gios
New Relic gomez for performance
monitoring jmeter and grinder for a low
testing and such disaster recovery you
know it happens sometimes you know you
wake up in the morning and nothing works
so you want to be ready for this when
disaster hits your goal is actually to
build to begin with a full tolerant
system that's why you're running a con
on a cone on a cluster that's why you
use so you depend on certain
technologies make those choices but in
the case of reach of a disaster you want
to be covering every store as your
service as soon as possible you want to
have what is called at the our PR
disaster recovery plan and you want to
actually do drills literally sit down
and say and literally kill
nose and see what happens or how your
system respond do all go through all
these like simulations that is chaos
monkey from from netflix check it out
it's pretty interesting if you're
running on them AWS scalability also is
the ability to scale team in your hiring
make sure that you hire the best people
this is all stuff that everybody knows
but you are as strong as your weakest
link you want to make sure that when you
hire these people they from the get-go
they walk in and they have a machine all
of it said the hardware is set up
already for him to work you can use turn
tools like vague vague rant and stuff
that they have proper access make sure
that you have a knowledge base just tell
them what's the password for get or
where how to get like an account you're
developing process make sure that it's
something that can scale and it's age
are your teams if you want to scale if
you are startup you want to become a all
getting a big organization you guys
start thinking of that stuff early if
you happen to be the vp of engineering
you happen to be in a position when of
that you actually can can impact your
organization in that level but a a good
way to do it is actually organize people
in these pools you have like a pool of
the vein engineers and paul of QA and a
pool of product owner and even a pool of
architecture owners and then you
assemble these teams to work on
particular like projects in a pulling
for developers moines qa1 architect and
one product manager and then the
throughout like a one sprint and then
you do something with them you keep your
team small and also you want to give
them ownership most importantly to their
devops your each one of those teams is
responsible from not actually writing
the code testing the code actually
deploying it to production and you want
to enable them giving data database
access whoever it is and they're the
ones that would babysit that particular
piece of code until he graduates and
becomes a part of the final production
code so to take home message i'm
probably running out of time right now
I've actually all right three minutes
but the take-home message of this piece
tation is that make sure that you
address these concerns as early as
possible and plan for capacity early and
designed to scale from day one me just
tell me just say all right instead of
doing things this way I'm going to spend
an extra hour and do things like
differently make sure that you actually
sit down and in determine how scalable
is scalable for your own organization
you know you can't just sit there an
over engineer a system that is capable
of using of being used by 300 million
people and you're you have been your
product is one that is in language X
that is only spoken by 200 people in the
bush somewhere so that doesn't make any
sense that will be just like overdoing
it your scalability there is your
ability right a system that is able able
to be evolved to be used by 300 people
do not bite more than you can chew
extremely important building scalable
systems is a process you will incur
technical debt in fact you should incur
technical debt you don't walk in there
and do a dupe here and do this and do
that you just do things just little by
little you know as long as it's done in
a structured and organized manner you
want to also commit to a roadmap around
these bottlenecks and most importantly
guided by planned fear the plan business
feature so you go and say my cpu my
network and my storage which one of
these do i improve and make more
scalable that is to be guided by the
next feature if your next feature is
some kind of weird query that your
business needs so you might want to
start thinking of actually doing some
work and improving the data store
bottleneck and strategically right so
the more importantly I'm learn from
other people's experiences read the tech
blog of Netflix and Twitter and all of
these people and learn from your own own
misery mistakes more
importantly so take it slow and you'd
get there this is the scream the smiling
scream and she found it online but work
smarter not harder you can't just do
everything at once but as long as you do
it in a way that is gradual and as long
as you acknowledge your technical debt
that you'll be just fine and thank you
guys for being here enjoy the rest of
the conference
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>