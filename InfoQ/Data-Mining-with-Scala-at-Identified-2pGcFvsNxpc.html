<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Mining with Scala at Identified | Coder Coacher - Coaching Coders</title><meta content="Data Mining with Scala at Identified - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Mining with Scala at Identified</b></h2><h5 class="post__date">2013-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2pGcFvsNxpc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Oh everybody I would like to welcome you
here at identified and I would like to
tell you a bit more about what we do
with Scala with data and that sort of
stuff so my name is an i will hear a
data team doing a lot of the Scala and
data processing and data and machine
learning and kind of technological level
stuff and we use color and that's why we
were here to talk about it so I would
like to introduce you Simon this is a
marketing logo for basically what we do
for our you know for the stuff we do
with data and so if you think about it
that's actually scallop powered guy
that's the guy there's actually whole
video with it you can go to our website
and there's a animation which is very
cool we have very good ego our graphic
designer is very good enjoy it all
himself that's cool okay so Scylla what
are we going to talk about this will be
mostly about like a higher level
overview about what we do here I'd
identified so what kind of problems we
solve how we solve it what is it good
for what's like what we use and why
skies are so important and why is it so
good and where it brings us okay so the
product basically what we do right so
what we do is that well let me show you
this is what we do kind of it's mostly a
search it's about surfacing the right
people to the right to the right
audience so it's it's an HR product so
well there's actually better screenshot
here for that so you can go and and you
can search for four candidates it's so
you know in a way it's kind of it's one
part of what what
linkedin offers although linkedin is so
showing only the linkedin data whereas
we have access to much more data
especially a lot of facebook profiles
and we're also getting data from other
sources and we're all merging that and
showing that combined so the main
product is basically it's basically a
search type of thing and so as you can
imagine if you get the profiles from
facebook people right there different
different jobs and and you know
different noise so what we need to do
with all that is so in order to you know
find people and find really everybody
and and everywhere which is you know
it's kind of the mission what do we do
we need to get all the data we need to
clean them you know to normalize them to
say that you know different things
really mean the same thing you know we
are given the location see them you know
somebody says i live in SF you know
somebody else as you know san francisco
and and whatnot so we need to you know
put them or put give it all you know
like right meaning put it into right
boxes although it's not really boxes and
we don't really know how many of them
they are and whatnot so we need to merge
it into such entities and and then of
course hopefully fill in fill in the
blanks because if you have more
resources or if you have social graft
and there's a lot of things which we can
just figure out even though people don't
say that immediately and of course turn
it into a product so it's actually very
door starts you know you go from from
the last line back but this is pretty
much the same for all the day that
companies so it kind of sounds like etl
the only except it's not because you
need to do a lot of stuff it's not just
you know simply throwing hooks selected
or something not now it's quite a bit
more involved
hmm that's a not so nice picture of our
architecture it's kind of so the
important part is that basically so I
will focus only on some parts of the
architecture for you know obvious
reasons so there are applications and
mobile applications and whatnot which
are and third-party data which we are
some of that we're scraping some of that
we're getting you know somehow we
integrate all that in a Hadoop obviously
because that's the that's the best thing
for that because you can store our
anything you you have and you can
process it so you can use our archive as
you know processing everything that's
where we run all our data processing
machine learning and what not and we
output our primary output from Hadoop
goes to solar because as you saw it's a
primarily a search product basically
it's a search engine very dedicated and
very specialized and so most of the
product goes to most of the output goes
to solar in order to be served in web
application and we also use solar
they're basically as a you know like key
value store so basically kind of most of
the data we need goes to solar there's
also some postgraduate days and few
other things so yeah also the web
application that's you know again
another thing which I won't cover it's
very cool it's very well done we have
great people working on it but it's not
Skala so you're probably not the right
audience however if you're interested in
you know Ruby and javascript /
CoffeeScript type of things we're doing
here I would like to invite you to some
of our other meetups so you can learn
more about that they're so whatever like
to talk about basically and kind of you
know put emphasis on is one of the
things is actually a data pipeline and
how do you treat the data because I it
actually
you know when you do machine learning I
don't know what's your experience of
whom of using actively machine learning
or really data data science data really
something heavy defeat data yeah I think
you all will agree with me that
basically getting data into the right
shape is really important and that
having spreadsheets around and and you
know text lines of various different
format on every single line and not
utf-8 encoding and whatnot that's
probably not the way to go right because
you'll never ever do anything sexy
because you're just waste your time on
stupid things so Fred reason I think
even with beta our colleagues and you
know our next speakers will agree that
everyone is actually the best solution
out there and I think you should all
really consider that for your
applications because um whatever oh is
it so it's a serialization format it's a
way it gives you schemas and it gives
you principles on which to evolve
schemas which are ridiculously simple
it's just if you create new field just
give it a default value and therefore
you don't break the readers which were
written before and you know it so it
works so you know and don't rename
fields and you know like basically few
things you can you can say them yourself
without actually reading that it's just
you know just be saying and that's it
but it helps you to even even check that
and it integrates very well with a
Hadoop infrastructure and quite a few
other things so that's that's great
about it and if you do like what we do
with that is we create a one repository
which basically so even even the teams
working in Ruby they are free to you
know create new schematic easily can
create new schema they just put the
schema into repository it builds for the
you know javis like Scala well I like
sure it's not sure why we're not doing
Java anymore right it's just color okay
so so it will it builds for us the
the sources so then we have phenotype
safe we know we get to we get a feed so
let's say you know log from that search
engine you know like log and I just get
it and I just type dot in my IDE and you
know it tells me okay hey you have here
a time stamp which is in milliseconds
and it's long and and whatnot and it
helps you a lot it's tremendously speed
up your of your development and so I
would highly suggest you do either dead
or something very similar then the other
part important for that is s Kafka so
basically when you generate the streams
and I mean everything so even even our
crawlers when we crawl some some sources
you know like really crowing some
websites we also generate that as the
raw streams of data but it's again you
don't typed we know that it was crawled
by by which node at what time what was
the response the raw body what was the
headers and everything so we can go back
and figure out there was maybe something
wrong with our crawler or maybe with our
parser we need to repairs the data and
we don't have to put the load again on
our partners that we were scraping them
yet again so that's a that's very that
was very useful for us and so what Kafka
gives you I guess you may have heard
about it because it's written in skala
so what gives you is basically simple
publishing a simple you know publicist
subscribe a type of thing it holds with
very large volumes and the important
thing is that it purses the data in
between to the desk and it's writing
that sequentially so basically have no
problem but excess of it I oh and
whatnot and and you can subscribe to
those feeds and you can write one job
which you subscribe all the topics which
are running and you don't actually have
to write it because linkedin guys are
already written that so you can just use
it and you can subscribe automatically
all the topics and throw them into
Hadoop so you can emphasize that in your
organization that every possible data
stream you ever may want to use for
whatever kind of fancy stuff you want to
do it gets created easily it has nice
schema and it gets persisted for you it
gets stored so you can take it you can
grab it you can work with it so I think
that's really really useful and then we
get to Hadoop right so we generated data
we have them in a nice form there coming
to Hadoop so what we can do in other
with them we can write hive of course
for simple queries it's great i think
beating sequel in sickle in simple
queries is i believe not really easy i
love scholar and a lot of things around
it but i still write some sequel queries
because it's just easy and for the rest
are you scowling whom of you here have
used sculling 123 yeah okay guys of
course scouting know okay whom of you
have heard about skylink okay that's
actually a lot that's good okay so let's
make it quicker so so scalding well I
that will be anyway covered by by the
next speakers so sculling scouting is a
devout dsl it's just a thing where you
express the whole computation it's not
really my participative specific
although I'm not sure if it's compelling
currently into anything else but it's a
it's a way to capture your your
computation into basically graph and so
eventually you can compile it down into
other things hopefully maybe one day in
future I don't know anyway the important
thing is that it's um it's pretty pretty
well it'sit's and even more important is
that it's growing very well so there's
really good community around it so for
cascading there scowling which is the
skeletal yersel which is probably well I
believe really the best way to write
Hadoop jobs these days it's written by
Twitter it's actually really good
actually one of the better things from
Twitter it's it's open source it's it's
working very well by the way cascading
is the same thing that power is quite a
bit of other things like for example the
closure guys what they're using that's
again some thin wrapper dsl kind of
think I'm on top of cascading and so the
good thing is that there are actually
multiple communities not just the
scouting community but there's a lot of
users in in Java there's a lot of users
in a closure and basically all the cross
you know JVM and all across the Hadoop
ecosystem there are people who are
writing jobs and writing libraries which
works well integrates well with
cascading and there is a power in that
so it's good dr options from in scala i
don't know if you've heard them for them
scrunch and Scooby the both have their
advantages and as well as these
advantages so scrunches the main
advantage is that it's it's done by
Cloudera which is generally very good
engineering and it's a patchy project so
those are the two advantages but it's
basically the same thing as cascading
and scrunch is just a thin wrapper on
top of crunch which is a java library
and it's it's way less advanced than
then scalding so nice to try but not
really not really that good at least not
at the time being and Scooby Scooby is
really nice it's pure scala you know
unlike Scouting's colleague is this you
know java think so some scallop people
don't like it because it's not scala i
myself don't really care because i use
the tools which worries you know so
which which helps me to get where I need
to get and so that's why scowling so
Scully Inc I don't know if you've seen
these kind of things
I let that to move to them to the to our
next speakers to talk more about to talk
more about scalding because they
probably even know it better than me
anyway I showed you an example because I
believe they will not show it it's let
me show you it's two slides and it's
actually pretty simple thing because I
don't want to lie to you and so I am
showing you two things which you
normally don't see you typically see
these screenshots of you know like huge
MapReduce job in Java when where there
is if you actually look carefully at it
how far it is just imports so unlike the
other people I included the imports so
this is like the actual whole file I'm
sorry the package is missing there by
the way this thing is from the from a
project which I should open so I should
have one knows it already it's my like a
test bed for you know various Hadoop
stuff and I've implemented their kind of
the same workflow in Pig hive the
Scoobies grunge scalding a plane java
plane Scala and so that's why I can tell
you that scalding is really the best and
also Casca look you know the closure
thing so you know so i will actually
open source it sooner or later elevator
in a week i would like to do it at the
beginning of next week so you can then
actually look at that and really run it
because there are even integration tests
for all that stuff and it just runs in
11 package and that's it so I'll then
post it on the on the meetup group
anyway what what are the other things to
see here which may and may not be card
and the next all is this is showing the
typed pipe which may not look that sexy
on the slides because it's a it's little
bit more verbose but it gives you the it
gives you better code completion and
everything because everything's checked
so there are two things fields API and
sculling and types a typed pipe and
fields API is kind of thing where you
bind based on names
so so it's a that's making it a little
bit more annoying although it's not that
much annoying you can actually to lift
really use it very easily because if you
run it at the beginning it has to
compile your job and it figures out that
those names don't match so it's not in
the middle of running your Hadoop job
it's actually before you start it so
it's actually pretty okay to use that
but this is this is better once you read
more jobs this is better that it will
tell you a little basically tell you if
you have more jobs you change something
your ID goes red wherever it needs to
and you don't even have to do anything
so you already you immediately within a
split of a second you know what you're
making wrong okay bacterias job what is
it doing basically to two streams in
Avro one is request one is actions and
it's outputting so basically it needs to
join them based on some ID which is in
both the requests and and store it so
basically those those three lines here
are defining to input files one output
file the last line down there lazy val
the laser is important that's basically
a service which does you know some jello
location based on IP address to make it
country just to make it interesting
because then when you're comparing
different frameworks well not discolor
frameworks but i'm in the pig and hive
then you know as you know you gf's are
kind of pain in the ass this is a way to
make in type pipe to make a join so
basically first you need to you need a
little bit by something so basically
have to we have to somehow tell it
what's the what's the key based on which
to group or based on which to join then
you can join it and then you know usual
stuff like count map and whatnot anyway
I would encourage you to give it a try
if you want to give it a try like next
week i'll try to monday
push this out with all the integration
tests and you can compare it with big
I've you know whatever you want to other
than that I won't go much deeper because
I'm pretty sure it will be colored and
the next talk just a few notes for you
who are not using it because it's so
cool that you should definitely start
using it so let me help you a bit a bit
of a few notes I kind of see our
problems for four people which were
problem big problem a room for me to get
started with that so yeah first I would
like to thank Chris here or there too
for helping with this basically hated a
little piece of work there compared to
the rest of the things but also you very
very useful for us because we love avro
and heated the offer of integration the
big thing i would like to warn you about
and try to explain you a bit because i
haven't heard it myself and I've heard
about scaling first my cell phone on a
meet-up right and I haven't got that and
it cost me quite a lot of time is the
sterilization stuff so let me show you
in the code so we can see here you know
what put to Beaver if you if you will
look even let some examples running take
a note of this you know the meat locker
thing here which was I told to Chris
it's for this avro thing in particular
once newer version is released you don't
really need to do that but that's for
the other thing only so if you actually
look at that okay it's now a little pity
that it's not on one slide but if you
look at it okay so it's one class so if
you forget the imports there's one class
okay and there's this so what scouting
does for you is that it's somehow
package doesn't give it to cascading and
what cascading does is so if you look at
it it looks it looks like it's run
somewhere on one note but that's not
exactly a true right it has to it has to
take your job and execute something as a
map tasks something is reduced tasks
across whilst
number of machines right across a lot of
machines so that means that some of this
code get executed in map tasks some of
them in reduce tasks and something get
executed also where you start the job
right so think about it as a client
somewhere where you submitting the job
we were submitting the job it creates
the pipeline it creates all these things
well it kind of creates kind of
everything it has to create a pipeline
okay let me go back to the other side it
has to create this pipeline and give it
to cascading so it knows how to plan it
that ok it will compile into you know
maybe two MapReduce jobs and and and
what will be the input what will be the
output do you know and all the details
but in order for that to happen it has
to so you give it something at the
beginning you give it these these things
at the input you take the arguments
maybe you do something with it maybe you
compute something maybe you just open
the path or whatever now the important
thing is that you have to sterilize all
this thing you have to serialize that to
put it somewhere to deliver it to those
members or reducers and that's by far at
least of was for me and for other people
I've spoken with this is the biggest
part if you actually try to start using
it yourself that's that's pain because
serialization there are methods how to
put in sterilization and there are even
some problems with that we didn't
cascading in particular for me it just
didn't work I spent a lot of hours more
than a normal day on it and then I give
up so the way to do that is what I would
suggest you to do our two things first
started working example second lazy vol
everything even though here at the
beginning of the three if it's probably
not that it should not better there but
lazy well definitely saves saves the
third guy kind of my thing that I need
like this like etl right at some job
which somehow loads the database of IP
addresses
you know translated using some libraries
and whatnot point is if i make it lazy
well it's not actually a value it's not
a field within that class it doesn't
have to be sterilized it's just a method
and those guys who need it so in
particular in this case it's called in
what will be compiled into reducer dead
guy can just call this method you know
first time lazy val get initialized okay
and then it's it's it's returned so
basically all the serialization problem
is is solved because you know how to
initialize it so that's good and but for
some parts namely here the other source
because avro has is schemas and whatnot
and it's not defined as Sarah visible in
like a serializable like Java was the
jihad I Odets realisable or you know the
interface then it doesn't work with the
default luckily for us Chris have fixed
that but unfortunately it's not released
so kind of thing to work around that and
work around other possible issues if you
see that something can gets realized
meat locker is pretty neat thing another
library from Twitter it's using its
using creo fertilization it doesn't
really matter it just sterilize the
things anyway you should not need that
all that much basically make sure the
easiest way is to make the lazy wells
whatever you need inside your met
producers and kind of everywhere just
make it lazy val and and you'll be fine
and if you have all the data in a jar
file or if you need some some files in
order to process that just put that to
distributed cache and initialize it as a
lazy well and you know now read from the
distributed cache that's well-documented
is calling you'll find that so no
problem with that and with that it works
like a charm it's very nice very easy to
write we're easy to work with
yeah and plus there's quite a bit more
things coming in the Indus calling
ecosystem which I haven't seen not even
a tiny fraction of that activity within
scuba or our horse ranch so so at least
for the foreseeable future go it's
cutting okay okay so we we have covered
so far well you know how we get data
we've formed them in a row and what not
and why you know that's the important
thing that helps you do all the stuff
than easier gets to how to do and how to
primary you know really heavy processing
is essence colleague any questions
thread I'm not it good so next big thing
before I dive into kind of some problems
and how we solve them in terms of
machine learning is about solar and
losing that's actually you know well I
mean particle you know like most of the
part if your data scientist like you
spend most of the time at least at least
the guys I've spoken to you spend most
of the time either preparing
presentation if you're that kind of data
scientist or you spend most of our time
like the low level non machine learning
kind of like cleaning and then filtering
preparing data basically or you are
doing I don't know what you claim to be
machine to be data scientist just
because everybody does these days anyway
loosing solar / elasticsearch / few
other technologies is the leading is
deleting a search full-text search
engine right as you saw our primary
product is the search basically it's
specialized search so we're using
heavily our solar and losing in
particular solar cloud for for the first
serving the end stuff
and so if you don't know the relation
basically solar is its web service
whereas leucine is the actual library
which is actually doing the heavy
lifting so but you can do a lot a lot
more with that than than just pure pure
search engine because among other things
you can do with that so one thing is the
search engine the other thing is that
it's it's actually pretty good key value
store well it's actually a really good
key value store it's actually pretty
much like you know HBase or Cassandra or
whatever so for us currently although we
need to generate a lot of stuff and we
need to serve them somehow we still
haven't installed any we know Cassandra
or reg base or all these kind of guys
just because since we already have solar
it's just easier to administer one
system and it just works even if you
throw you know more later to it and you
know you can create based on you know
primary ID so basically it gives you you
know what do you have with each based on
Cassandra as well so but it's great for
other stuff as well so because we're
working a lot with the text you know
even so when we when we get a user
profiles and when we look at their
locations I mean a text form as well as
if we look at their education and job
history and whatnot it's all text it's
it's mostly text but there's a lot of
you know subtle details people type
things like they do some minarik even so
like you can't compare two strings at
all because because you know there's
like kids sensitivity and would know
then so a shuriken you can lower case
and and and and strip and trim the white
spaces that's the easiest thing but then
there's a lot of different ways how you
can express one document 11 the name of
even named oven and
and today like you know like think of
like universities how many how many
different names they have this berkeley
there's University of Berkeley
University California Berkeley and like
and call and you know like gazillion of
things so this is actually one of the
ways where if you try to actually put
that into one thing that's actually
where a search engine already came very
handy and other things what is it good
for is that you need to do more things
so for example you want to do some some
spell checking right and so now you need
some spot checking library and whatnot
kind of the one in lucene is just you
know perfect it's just better than
anything else or you want to finish a
state automaton for things like you know
Levenstein distance or or or whatnot
that's also very useful because you can
correct spelling mistakes and whatnot
and like shrink down the number of your
unique unique names of you know other
people or companies or whatever you can
consider but shrink that down which is
very useful for later stages of some
processing you know and you can go along
the long way if you do some custom
custom a few stores on custom fields and
and and you do customs scoring I don't
know if you've done a lot of it I've
have anybody worked a lot on search
problems no one to two three four okay
so basically you can you can store a lot
of fields so for example things which
are useful for us is things like you
store a size of a company even though
you don't display it right but do you
search for something you know like let's
say you search for Google with you know
not to OS but you know 3 o's for example
and you know what let's say there are
such company exists involved we actually
saw it in our data set
but it's a different story the point is
that you know if if you see that you
have a lot of users with Google exactly
Google if you store you know this is
filled about the size of the company
it's a very good indicator for you that
that's the right company people are
looking for that they are probably not
looking for that exact match but they're
probably looking for this pretty close
match because that's you know the very
one known and whatnot so you're actually
getting better resolved better product
if you if you do that and you know
that's that itself is like huge topic
out how to do you know searching and
scoring scoring bestest butts I don't
know how many how many thousand people
of very smart people work on that in
google but now we're gonna know that
smart but you know anyway this talk is
not about art so not that but what I'm
trying to say is that it's very helpful
when you work for text it's it's more
than just search one of the things which
is really useful for us art is a new the
ultimate us which were vastly improved
in in solar for and actually if you're
using the search of any kind use solar
fall or loosing for or if you're using
elastic search upgrade to store for you
know listen for it's pretty much better
and i really keep with the light is
wearing what you can do with those
automatons there is so for example if
you produce something so you know so
things like so you need to work with so
we have we have something like 700 800
oh it's I believe 800 now right 800
million million users and even more you
know with you know history of jobs and
education and whatnot so you were for
example like to do something with it
like you know like typically no machine
learning you know people were like okay
let's you know so typical algorithms you
know used with working with data are
things like you know clustering and like
and then and well regression of course
right linear regression logistic
regression and then you
some trees and you know this and that
the biggest problem with that it's like
we would love to really love to cluster
those things for example but most of the
times just the clustering is a problem I
mean how do you cluster on the text
there's this thing about Custer you need
to have some distance function and it
seems easy for people to do that but
it's pretty difficult to do for four
machines and one of great ways to help
with that are finished eight of them at
ultimate us where you basically you can
match to the library and moose in works
very well that it creates very compact
very fast automatic by we just give it
is just whether you give it a set of set
of words and it can it just creates
automotive which very quickly mention
tells you okay it match already does not
match and you can very quickly say hey
you know build media's with you know
Levenstein distance this and that or you
can say that the instead of automatic
and do the transducer which you can say
that basically assign some values to the
edges and so in the end of the in the
end of the end of the matching you can
say that you have something that some
value which was accumulated when
traversing around the path written
within the outer mountain and it helps
us quite a bit of it very few things so
that's what that's what we use a solar
losing for we use it a lot actually we
embedded for quite a few things we've
embedded in hive jobs like we create
small indices very specialized indices
which we embed into into map tasks so
you know in scalding it's just the same
way of service initialized this this
little thing there's another thing which
initialize you know solar not solar
about losing the library itself and then
we can you know basically run a map task
and we can look at every single user and
we can through you know a couple of
steps which basically is decision tree
kind of figuring out okay you know is it
helpful is it not helpful
be confident enough that we know really
something about it maybe not and that's
that's very powerful the Arabic thing
which is very useful for us is its
clustering it's very useful to cluster
the users based on anything you want to
anything you can i would say for good or
bad for us we don't have that many
numbers about people which is a pity
because most in the cluster angus is
really easy if you have numbers it's
pretty easy to define some distance
based on some some number of vectors if
you have mostly text it's pretty big
problem it's kind of not not really you
know things like k-means like Charles
drink or whatever from any kind of cast
rank I've ever heard about you have to
define of some similarity okay not
really from min has you define some
hashes and so it's not exactly truth but
you know even for the other other ways
of clustering anyway it's kind of hard
and in the end of the day we would
basically love to cluster kind of
everybody I mean after all search is if
you think about search you can think
about it as a problem where you would
would cluster everybody with the right
set of words right whatever you enter on
google is just a cluster with some pages
anyway it's great for locations so we
can Custer users based on location and
then we can look at specific groups you
know the other stuff we know about it so
clustering by itself for us don't solve
that much but it helps us a lot to say
that something those people have in
common but we need to integrate it with
the other stuff so the rarer stuff is
for example social graph right because
it's very useful to know about those
people that they are geographically
about the same area okay well it's
definitely useful to cluster those
people based on few of these things like
you know age and gender and whatnot I'm
not even talking about it in terms of
clustering because that's kind of
obvious so you don't even you probably
you're better off just by getting people
into that then just you don't need to
discover clusters in that I mean you you
know already that that does something
reasonable and you can put it into few
buckets without any algorithm actually
but the social graph is is another very
very useful thing what is it mostly
useful is that a lot of people a lot of
the little bit of profiles a lot of the
data we have for example from facebook
especially people put their a lot of
noise or they don't put their things and
whatnot so what we were able to do is we
can look at the friends right and start
getting information from friends so we
can start making assumptions of course
with not one hundred percent that we can
start making some assumptions about
especially a location that's relatively
the one where you can get quite a quite
a bit of confidence easily on the other
hand there are problems other with that
some things some things are not working
all that well with the video social
graph especially the biggest problem is
that we don't have all that dense graph
as Facebook itself has so we have to
really watch carefully about how many
friends we have for you because you know
if you have very little friends and it's
quite a fun pretty random call a
connection you go somewhere for vacation
to add a part of world you make a
connection there with somebody which is
quiz lay unrelated to you at all so but
it helps it helps a lot and it helps
them when especially once you can can
once you can join it with with the data
you've clustered and that's the hard
part
and it's the biggest hard part and it's
the core of what we are working now is
joining different insights even from
within one data set and even more
challenging it is to join from the other
data sets and so that's what we're
working on primarily now or we're trying
to really improve and do better and
there's a lot we can do there's not that
much we've done already that's the
that's the exciting part that's what I
can work on what I can but I will work
again tomorrow so that's the good part
so there are problems you know I think
you've heard or definitely think about
the problems like you have different
user profiles from you know different
different sides how do you merge them
right so of course you start looking at
it and you have a lot of a lot of simple
things like okay well if the name is
totally different than you know it's
probably not that not that guy but you
know if the name is the same that way
doesn't mean it's it's and related to
that and so it's that's pretty hard we
have quite a few ideas what to do with
that like we're doing some stuff but
currently we don't have all that all
that great all the great results but
that's improving and among other things
for example you know we'll bring in more
machine learning type of things like
like topic modeling from posts we
believe should help a lot that can be
easily clustered and you know tell that
you have something in similar with
people then if you look into social
graph you know to see so like you know
Joe location and and and topic modeling
about what are your interests what
you're talking about what do you like
them on facebook what do you tweet on on
Twitter and other things so those those
are the things that are coming would
we're trying to do and
prop really confident that it will help
us a lot so what's the actual machine
learning where is it so quite frankly
it's not that much custom written and
it's a mostly able to use current I it's
mahout and some are and so the custom
code in this part of stuff is mostly you
know what's what's your similarity
what's your scoring function and how do
you merge maybe a bit more data but it's
you know mostly about mahout as a
library and that's what we mostly do
lately over the last couple of weeks
we've finally started focusing on spark
especially for the iterative algorithms
so we have some ideas like to kind of
put a lot of stuff of stuff into into
spark and like run it for even analytics
type of stuff like you know half queries
problem with that is that it needs a lot
of memory so it's not that good and so
we're not doing that but for the
iterative algorithms it's definitely
great and it's it's it's very easy it's
too little for us to get started with
that you know to do that so we're
flipping some jobs to that and it's
great for these relative stuff
especially you know so we use scalding
so actually all of our computations if
you think about that from from the very
beginning from evil even the large
datasets which came from scraping the
data there's a there's a Hadoop job
which transforms this large collection
into something reasonable which have you
know higher value it's smaller it has
higher value but we keep the steps we
can always recompute that you can think
of it in terms of scale I can think of
that as a as a function transforming one
value of an immutable collection into
another immutable collection but gets
persisted and there's actually another
important thing for us because a a lot
of those things are relatively small so
we want to rerun just some parts of the
of the computation more often and some
parts only once in a while I mean
especially the things where we run for
example the lucene queries for you know
every single user and we run a couple of
them and how many of them because it
goes through the decision tree and like
do we trust it enough and no not so you
know we do another query and whatnot
that's very expensive so that's really
not kind of a thing which we could run
quickly I mean we can run quickly we run
is quickly if we add the new user we can
rerun all that very easily but as we
learn more insight from our data and we
change our models or we change you know
this index and whatnot and basically we
don't know the way back we don't know to
say if we modify you know our model how
we treat the data we don't know which
users influence unfortunately so we have
to rerun on all of them and that's
pretty expensive and so that's why it's
great that we can persist different
steps of the computation and then just
put it together which is nothing else
than just another job which you know in
the end of the day there's another job
which just you know send it to mostly
solar sometimes sometimes postgres but
spark further machine learning
especially for the iterative stuff when
you know you shrink it down into some
reasonable size and you know that now
basically you burn your course because
you're going to you know couple hours
run or you know like hopefully less run
your algorithm on something that's
perfect and so that's our next big thing
we're doing in like big way also I don't
know if it's a couple of you that you're
using sculling cuts getting there was a
pattern library from them I don't know I
haven't tried it yet but it would be
cool sounds cool as some machine
learning stuff seems like pretty basic
stuff but it'll be great you know if it
integrates with with the rest of the
stuff that would
that would be all so great and so where
is this gala I haven't shown you all
that much code have I well the reason
for that is fairly simple because so
Scylla is kind of everywhere yes skies
kind of everywhere except the web
application basically and will actually
accept the publication and scripts for
our infrastructure it's also written in
Ruby it's actually very nice we have
very nicely scripted the entire and
infrastructure but that's for some
different talk what's Kelly is doing is
it's used holler at as a data team it's
a basically take the biggest part of the
code which I haven't spoken about at all
which is doing actually a lot of stuff
it's actually the real time part of
Sylum of 13 of that okay guy all the
real-time stuff older so you know the
important stuff is what we what we do
kind of in and what we do in Hadoop and
wouldn't that's the thing would be
recomputed with all the users that's
where we learn new models that's where
we apply new models to all the data we
have but then there's a real time aspect
of it even in order to search for that
we need to whatever is coming from user
what they're searching for and what's
their context what we know about those
users what they can be searching for you
know like their location and whatnot we
first need to pre-process that in order
to know what we're actually already
actually want to query and also as new
users are joining we also need to you
know we don't want to wait as I said you
know it takes us quite some time to
apply all the machinery so we of course
don't want to wait if you joined today
we don't want to tell you hey come next
week that doesn't make sense right so
there's a lot of a lot of code which
witch is which is a
which is a web service of the data team
which is exposed to the application
teams that's I believe there's like 50
lines of Java and everything else is
skele and these 50 lines are runtime
annotations do you know guys how to
write random annotations so its seen by
java reflection because if so I would I
could climb in its hundred percent
scalable ok anyway as a detail and then
you know all the all the jobs I mean it
would be interesting to do and some talk
i show that but i haven't shown the the
way how we compute you know different
metrics and whatnot because it's kind of
pretty bound to our date heart you are
like take our structures we use and it's
actually first put it two slides but its
fake rid of out of the context and it
doesn't make much much sense you know
things how you because it depends on how
are you computer similarity for example
depends heavily on you know I ok what's
your input like what's what's what's
what's your input vector and what's
inside and what does it mean and what
not anyway so it's not it easy to
present that look it's all fake I'll
talk but maybe next time if you wish ok
thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>