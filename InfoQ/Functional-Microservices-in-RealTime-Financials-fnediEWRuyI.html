<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Functional / Microservices in Real-Time Financials | Coder Coacher - Coaching Coders</title><meta content="Functional / Microservices in Real-Time Financials - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Functional / Microservices in Real-Time Financials</b></h2><h5 class="post__date">2017-11-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fnediEWRuyI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">first of all thank you thank you
everyone for coming here today it's a
pleasure to be speaking to you today my
name is Victor I'm a software engineer
and new bank and where I work mostly
with back-end services dealing with
financial logic today we're going to
talk about real-time financials with
micro services and functional
programming so first of all for those of
you who don't know what new bank does
we're a financial services Brazilian
financial services company we're
essentially building a digital bank from
scratch in Brazil and today our main
product is a credit card this 100%
controlled by a mobile app and we've had
over nine million people request this
credit card since our launch in late
2014 and even though we're in a
financial services company we really
think of ourselves as a tech company and
it's part of our DNA it's essentially
our core competences technology software
engineering and the big question in the
early days was how can we leverage
software engineering to gain a
competitive edge all over these
well-established big experienced banks
that were already in this market and one
thing that we figured was the best way
first to accomplish this was to keep on
moving forward to be able to adapt
quickly to be able to all not never be
afraid of adding new functionalities at
a new code to prod so from the early
days we really built our systems around
continuous delivery
we thought that that was one of the key
components of what would make us a
strong company and be able to compete in
the market so naturally with continuous
delivery we came to the conclusion that
we should build from scratch from the
very beginning in a microservices
architecture because you know as we you
know we've taught over today and the
microservices talks we know that we can
deploy new services fairly easy there
you know you can have a continuous
integration system that I deploy new
features new services or that really
couldn't been concerned about breaking
the rest of my system if you if you do
it right you should get some decoupling
so you're never really afraid of
replacing a new service killing
something off you're never really too
afraid of making these changes and you
also get this hole bounded by context
concepts that allows for separate teams
to be developing separate features at
the same time and be able to get that
surprise without really depending on
anyone else so this is these are really
features key features for us to be able
to compete adapt and scale our business
to the point that we really wanted so
since the beginning this is how we built
our architecture and it turned out that
the key question that arose was what
happens when we need to combine data
from across all these separate services
and especially what happens if you
needed that to do that in real time so
this is the problem that I want to talk
about today so before we do that let's
take a quick step back so you guys have
a little more context of what a service
and new bank looks like especially a
service that deals with financial logic
so the first thing is that it's written
closure which is a functional
programming language built on the JVM
we're gonna have producers and consumers
Turkey Africa and Kefka is essentially
the way that we do most of our
asynchronous event-driven integrations
between services most rights actually
come through for Kafka we get
persistence with DES Tomic what's the
bit which is the database we use for
each of these services and I'll get I'll
explain a little more how the atomic
works in the next slide and we have REST
API s that are mostly used for other
services to access data and for actually
the mobile clients also accident data
and business logic from each of these
services and all of these you know every
service you know everything is running
on AWS on to a ZZZ with configure skill
code immutable infra horizontally
scalable Charlotte by customers each of
these topics could really merit I'll
talk on their own so won't really dive
deep into that
so out of all the technologies that we
use I guess state onyx is the one in
that's the least familiar to to most
people so just briefly explain how it
works so you can think of des tonic as
immutable
append only database which means that
I'm never gonna update a value in my
database
I'm never gonna overwrite anything I'm
only gonna be asserting new facts so you
can one simple way to think about it
it's that essentially it's a database
that works like get and get I'm never
gonna actually erase a line I'm only
gonna say do not consider this line as
of this commit from down now on consider
this new line so the idea is that you
can always you know I will not never
change my data I can always look back
and had snapshots that what the data
looked like looked like at each point in
time so you have this very strong from
you know from you have this really
strong audit trail by default which is
very powerful in financial services in
general if you need to be a system of
record in any way and the atomic is also
acid on right so atomic consistent
isolating durable so it's a really a
database that that's focused on this
immutable data model as well as very
high consistency so if you are in more
interested in to you know learn more
about the atomic I highly recommend talk
for from other new bank engineers that's
exploring for hidden super powers of the
atomic is available online I should
check it out so going back to our
problem today we have about 90 services
a new bank every couple of weeks we have
new services we kill off new other
services and the key problem was that in
a credit card business a lot of the
business logic and a lot of the hard
questions that we need to answer depend
on data from across all these separate
services so let's just think about some
questions that a credit card business
needs to answer should i authorize a
purchase should i block a card should I
charge interest what's the late balance
what does a customer owe me and these
clearly
and on the purchases the customers made
the payments you know any chargebacks
that is had interest currencies just to
name a few and each of these entities
reside on separate services and their
own databases so I need to pull data
from all these places together to be
able to answer these questions and you
know to make matters worse we actually
show these hard to calculate numbers in
real time to our customers that's
actually a big part of our value
proposition is the idea that we can show
in real time what your finances with new
bank looks like and you know new bank
means new in Portuguese means naked or
transparent so it's really idea that
want to be transparent and show the data
to the customer these aggregates as best
as possible and naturally because these
numbers are were spread out on separate
databases separate services and they're
very hard to get to on the first thing
that happened was that we ended up not
having long canonical definition of what
these numbers look like and naturally
what ends up happening is that you
create ad hoc definitions of what these
numbers actually are either by analysts
doing analysis or their models or even
engineers in different services trying
to replicate the same data in different
ways with slightly different definitions
and this is very very dangerous
and because you end up with this
analysis versus operational definition
gap so if can you can imagine that if
they have an analyst that's gonna be
evaluating the the the called corrects a
collections strategy how well a
collection strategy is working
how can he determine that it's going
well or not if he can't guarantee that
the definition of his definition of what
the customer owes us is the same
definition that the collections service
is going to be looking at and because
these were the most important numbers
that we had and these were the numbers
that you know all of our stakeholders
cared about our investors our customers
our analysts and especially our
regulators the central bank we needed to
do something better for them
and the first thing we thought about was
well how do how do we now we need to
find a way to create this canonical
definition for these numbers and what's
the best way to represent this data the
first thing we thought about it was the
canonical way of representing financial
information is essentially a balance
sheet or accounting in general and there
are some benefits to that and the first
one and I think the most important one
is that you can now using accounting you
can apply this generally accepted
accounting principles so no longer are
these balances are matter of an analyst
opinion or the CFOs opinion or software
engineers opinion of what that number
should look like now I can apply these
briefs of principles and I have this
verifiable unbiased version of what the
number should look like
you get conservation of money by default
so every credit should have a debit so
I'm never gonna be just creating money
out of thin air I'm not gonna be
destroying value of nowhere I always
have the traceability of how the money
is moving around and lastly which was
really nice for our architecture at the
time was that accounting is essentially
one of the original events or systems
you only really care about a log of
credits and debits and the balance which
is the number you're going to be looking
at just a calculated value on top all
day and this event source property
really fit really well with the way we
are event driven through Kafka
integrations between services so let's
briefly look at what this accounting
model looks like we have a book account
a book accounts there's nothing more
than an entity there's going to be
aggregating credits and debits and the
book accounts gonna have a type and
these types are essentially the names
that we give to the balances the numbers
that these metrics that we're looking at
so a cash balance a prepaid balance the
late balance currently omit receivables
payables and so on you can have an entry
an entry is nothing more than just one
representation of an amount a credit
account a debit account in a post date
and the post date we say that it's the
business relevant date so it's not when
I
doubt about a payment is not when I
transacted that payments of the database
it's when the payment was actually made
by the customer
so it's this business timeline concept
update and then you get the balance the
balance is just the cumulative sum of
all the entries for a book account and
these three entities are essentially the
the you know the core of accounting
revolve around these three entities but
a new bank we thought that we could add
a few more to add
essentially metadata for analysis and
traceability to how we you know we're
gonna deal with with this model so we
also included a movement the concept of
a movement so if you think of an event a
payment arriving or a purchase arriving
you're actually not gonna just create
one credit and one debit into accounts
are actually gonna be moving money
around for several several different
accounts at the same time and a movement
is just a way for you to group together
the same events that happen because of
the same cathica event so it's just a
grouping of a collection of entries and
carrying the metadata related to the
original event so you can think of a
movement as a mapping one two one one
kafka event two one database transaction
with a lot of metadata related to both
of these actions and lastly we have this
concept of a meta entity the meta
interest is just a reference for a for
the original original entity that it
lives in some other database that were
originated this this movement so just a
way for us to have the audit trail to
trace back all these entries where do
they come from what what's you know
everything's related to it in the
original service what do they look like
so this is essentially the model that we
want to create and I recommend to anyone
that's gonna be building anything with
accounting to read algebraic models for
accounting system those systems because
it gives you a very rigorous view of
accounting it's very helpful when you're
building software around that so you
know our conclusion was we're gonna
create this double entry accounting
service we know our model we you know
make sense what
what we'll actually the dataflow how
would that look like
so the the goals that we had was that
the first thing is that it should be
event-driven via caca
because the great thing about this is
that we can just plug in so they already
existing topics that we had we wouldn't
need to change anything on the other
services and I can just use those topics
to create my new stage and start
collecting data for my new balances that
I wanted to use in my operation as an
analysis and so on
I needed high availability so these
numbers for them to be used both in
operations both and analysis so showed
it to my customer shows to other
services and use it for my models I
needed this to be highly available
otherwise I would never get this - we
know my customers only analysis could be
looking at this number and I would still
have this analytical operational gap and
since you is it needs to be highly
available naturally you're going to lose
some consistency and we accepted that
trade-off is like we're consciously
gonna give away some consistency here
for this availability but we need to
make sure that we have traceability we
only can only be inconsistent if we have
enough information to understand why
when and how we were inconsistent and
the idea that we also needed to have a
very strong audit trail for this because
if I can measure my inconsistencies I
can take action to fix what I calculated
because of stale data incorrect data or
at least at the very least it can
evaluate the costs of these
inconsistencies what decisions that
actually make wrong because of these
inconsistencies and it needs to be
resilient to distributed system
craziness so a system going down
ordering concurrency bugs and other
services and so we need to have some
sort of resilience built into it so the
actual flow and we call it the ideal
flow because it's the one that we
thought if it held the world would look
like and it's like oh this is what
perfectly this is someone look like if
we are
had these services publishing messages -
Kafka in separate topics new purchase
new new payment so on would grab these
events and we you know in our new
service would just apply a function of
the payload of the event and transform
that to this new model and the key thing
is that if you if I can transform my
original event to this entries movement
model accounting model just with the
with the payload of that event then I
have a great set of properties and well
if everything is okay I'm gonna transact
cut to the very base but the biggest
thing is that if I'm just looking at the
event I no longer care about mutable
state so I'm not accessing the database
I'm not doing gets on other services
like I only care about the event payload
and because of that event ordering
doesn't matter so if an event happens
before the other and didn't expect that
it doesn't matter also its threat safe
if I have concurrency you know I'm not
gonna I'm it's not gonna be a problem I
can just have many many threads running
at the same time because it's not
modifying the same state that it's
reading because well it's not reading
any state so in the end with this all
you really needed to worry about to get
consistency is that you needed to
guarantee that all events are consumed
and naturally you want to want them to
be consumed at a fast enough quickly
enough so that the the time between the
event happening at us knowing about this
you know escapes to a minimum so let's
look at an actual example of this flow
so let's say we have a customer with a
current limit of a thousand he eyes he
makes the purchase of a hundred eyes a
eyes by the ways of Brazilian currency
and you have an a so the amount of the
purchase one hundred has interchange is
one hell and one interchange is
essentially one of the ways that we make
money we get for every transaction we
get a percentage of that of that
purchase and this is how you know most
of credit cards in the world work we
charge that in that amounts to the
merchant this arrives that our
servus the wintery service and then we
applying just a function of the purchase
we're gonna first recognize a receivable
when a payable of a hundred eyes and
then we're gonna reduce the limits under
her eyes we're gonna recognize but our
revenue and then all of these the list
of the entries are gonna it's a movement
and all of that is going to be
transacted
at once in the database so you're never
going to have just one of the entries
existing at any point time so it's one
atomic transaction and the final balance
is end up with you have a current limit
of nine hundred its receivables of 100
pay payable of ninety nine and some
revenue of one hell and well great we
finally arrived at the balances that we
wanted so if I just listen to all the
events that I care about they all have
these properties that uh that I
considered so it's all I can only I can
always translate they events to these
movements just as a function of that
event I'll eventually get to the
balances that I need and can just get my
information from this service from now
on and I'm sure you're thinking well
yeah that's great and all but there are
so many ways this can go wrong in a
distributed system and that's very much
true we can't guarantee consistencies
and there are so many ways that this can
go wrong in our name a few but the key
thing is that we can measure it and we
would model out to measure these
inconsistencies so the first thing that
can happen is well a service any other
services publishing to cathica might
have some downtime
I go down for any type of reason and
obviously we need the tooling and the
monitoring around you know figuring out
when the service is up or not any having
the tooling around you know making sure
that it goes back up as soon as possible
this this is just the tooling that
you're gonna need for any microt service
architecture but in addition to that
what we have is we have the concept of
the post h versus produced at timestamps
so i know when something happened in the
real world you know no one actually left
my original service so knowing that if I
have for example post date that was
yesterday but the produced that it was
just today I know that for some reason
that purchase
took one day to be processed by may
purchase system and publish so I know
that there's a gap between my post date
and my produce that that created some
level of inconsistency in because of the
service downtime we can have cat collab
so also just like over the service
downtime you need to monitor this or you
need to have the tooling around knowing
when you not being able to consume all
the all the messages that are produced I
mean you'd have the tooling around you
know increasing partitions spinning new
instances to add parallelism for you to
consume those messages but you were
eventually going to have some lag and
you can measure that just by consuming
the just by comparing the produced at
versus consumed at timestamps of this
flow and each of these dates are
actually stored within that movement
entity these are the metadata part of
the metadata that was further and then
lastly you have processing time it's not
enough for the our service to learn
about the this event it also needs two
transactions actually the database and
because any request that happens between
you know me finding that out and
actually transaction the database is
gonna have some stale data and I can
evaluate this just by comparing to
consume that with the the transaction
time to beat a transaction instance that
you get by default with the atomic the
atomic has a time stamp for each fact
that's asserted to the database so now
well we got our numbers we've found a
way to at least know when we're in
consistence and we can take action on
top of that so I can if I find out that
I if I consume a message that there's a
very big gap between produced at or
consumed at I can let the world know I
can you know for a specific customer
they had stale data for a while I can
react to that
or at the very least I can feed the this
data into my models and understand the
reasons why I'm getting consistent data
and the costs of these decisions and
this flow would be very nice but the key
problem is that this approach of only
having a pure function of the event
payload to create this model won't
always work
and as this and as soon as you start
having to look at states to create these
entries then you have you know whole
nother level of complexities so let me
just give you a very quick example of
this stateful flow that we call so a
flow that requires us to look at mutable
state let's look at that same customer
that made a hundred eyes purchase some
time has passed and now he's late he
didn't pay his bill on time so that a
hundred has became a hundred has late
balance now Q thankfully faint makes a
payment of 150 hats which which the
first thing we do is we need to amortize
that debt when you reduce that late
balance and immediately we see that well
I can't just reduce my late balance by
150 I can't be negative late I can't
have someone owing me a negative amount
it's actually me owing them that I'm out
so naturally because of the accounting
principles that we have that's not the
book account that there should be you
know have that balance something else
has different properties so we can't we
need to look at the state for that then
we increase back the limit to one
hundred eyes and then we finally assign
those fifty eyes to the prepaid balance
which right should work which is where
it should reside and the final balance
is that the current limits back to a
thousand we have cash 150 and now the
prepaid balance of fifty and everything
else didn't change so if you look at the
prepaid balance final prepaid balance is
a direct function of the initial weight
balance so yeah so now these are the
adapters that I'm that it's gonna go
convert the events to my model they're
not just a function of the payload
they're also a function of the current
balances and because of that the first
thing I need to do is I need to make
sure that these balances don't change
during the calculation right I need to
apply some sort of lock I can do that
you know in the database I can do that
via cat can partition keys partition by
customer ID there's several ways
can approach this the key thing is that
these balances can change while you're
calculating your entries and then
because of the properties of accounting
and these balances any movement in the
past will affect the balances from every
day from that point forward so it's not
enough for me to make sure that I didn't
break anything on the dates of that
movement the business post date of that
movement I need to check if I didn't
break anything from that point forward
so it makes things a lot more complex
and one thing that I mean we needed to
make sure is that when you can't allow
for our data to be corrupted because of
ordering of events essentially we need
to be able to guarantee all the
properties that the pure function of the
payload flow the ideal flow had we need
to guarantee those same properties when
we look at state so the first four the
first two problems I mean they're
annoying but there are ways to work
around them for the second two I think
it's more interesting to talk about we
started using invariants so what are
invariants in various are essentially
properties that should hold true at all
times
and when we talk about accounting
essentially these properties have to do
with the balances on each book account
so some balances balances can't coexist
I can't be both late and prepaid someone
can't owe me money while I at the same
time old them money some balances can't
be negative so I can't have a negative
cash balance it makes the does it make
sense
also some balances can't be positive so
I can't have a revenue of credit loss
and because we're using generally
accepted accounting principles these
aren't a matter of opinion either I can
just look up what a balance and what the
properties of what a book account should
look like and just mimic that and
establish those properties explicitly so
let's look at this state fulfill what it
looks like and how did we manage the
stateful flow to be very very similar to
the original flow the same properties as
the ideal flow
so let's get that initial balance of 100
pies let's not worry about the the
current limits for this example just to
keep things simple we still have the cat
cat topics still have the event but now
the way I'm going to convert that it's
not just a function of the payload it's
also a function of the database and that
when I say it's a function of the
database at new bank we explicitly pass
the database as arguments through every
function called that it's going to
access it and we essentially pass in
this time stamped immutable version of
the database and with a Tomic it's
really cool because if I know my
function call if I know the arguments
and my database is immutable you know I
know when I call that function I get
this very nice time series debugging
property that I can always recreate that
states and understand why something
broke so before we actually pass on our
database as an argument to that function
I'm gonna first check the databases
invalid state because if it's no already
in an invalid state it should stop right
there Vedas corrupted need a migration I
need to fix this somehow but if it's
okay I pass that on then I'm gonna
create my movement and with one entry
and the entry here I'm gonna create it's
gonna be removing taking money 150 guys
from late and putting it to cash and
then I check and I essentially with the
atomic one one of the cool things we can
also do is we can have this in memory a
fake version of database that pretends
to transact things so I can just do a
pass on this database with pretended
that I actually transacted this movement
what does this database look like and I
can fetch that new database this in
memory No
exploratory database and use that put
that in as an argument to my functions
and check is this database valid now and
we know that it's not there are
invariant violations on the violation
here is that I'm gonna have a negative
balance I'm taking 150 eyes from this
late balance so I'm gonna get a negative
balance version
I get these violations and I applied
them to a function that's gonna try to
fix it and the way we fix it is
essentially by adding more entries to
that movement and the way so we add this
new entry of moving money from
increasing prepaid and increasing late
and we call these new entries
Corrections and then we use this
movement and again plug that into the
the check which you plug that into this
like fake database in memory exploratory
database and we check again are there
any other violations if there are other
violations I'll keep that loop for I
don't know 20 tries and if I can't fix
it I'll throw an exception but if I
can't fix it I'm gonna transact that to
the database and then I end up you know
after the correction I end up with the
final correct state so by doing all of
this it's a way that I can guarantee the
same properties the same original
properties that that ideal flow of the
pure function had obviously there was
some a lot of challenges getting this to
prod and actually using this so the
first one the most important one is that
this fixing variant logic is extremely
complex so figuring out all the
invariants that we needed to define and
especially the find ways were fixing
these violations in ways that it
wouldn't introduce new violations was
very hard so I took a long time for us
to figure this out we quickly realized
that bugs and other services would
generate incorrect entries or it could
you know could affect our database and
we need to find ways to to monitor that
and the tooling around fixing any
problems that we had they Tomic is great
we love the atomic we love the way deals
with this audit trail we love how we use
it to explore data to pretend that we
had transactions in check the database
is valid we love everything about it
saved us over and over again but the
Atomics indexing is really only tested
until 10 billion fats and after that
amount of data you're gonna get you know
indexing slows down you're gonna get
some back pressure your transaction time
you know transacting database
transactions gonna take more time you're
gonna build lag and not gonna have the
throughput needed to really continue
using the database so we need to keep it
small and atomic isn't the best option
for analytical workload so this model
this transactional model immutable model
is not the best for analytical work
workload especially when you're dealing
with aggregates but we you know
thankfully if we found ways around each
of these challenges so the first one to
fix to figure out this this complexity
of all these invariant violations and
how to fix them we started using
generative testing for those of you who
don't know genetic testing is instead of
in you might know as property based
testing as well is instead of me
explicitly saying what the input to a
function what the expected output of
that function would look like I'll just
write a function that describes a
property that should always hold true
and as you can imagine these properties
that should always hold true are the
same properties that we want to
guarantee in prod there's a very nice
way for mapping the the way you actually
create these generative tests and since
you don't care about is since this
property should hold true at all at all
times you don't really care about the
input so you can actually just generate
random input apply that and just make
sure that after you just throw a bunch
of randomly generated input the
property's still true so we had these
random we have these generators that
generate from our schemas these events
with random numbers random orderings and
so on random dates and the one thing
that we we took away from this process
is that we should embed the least amount
of domain logic assumptions into the
generative test because with a
distributed system all your assumptions
of what your business look looks like
just go out the door one example that we
always used to say and if we were very
wrong when we said it was you're never
gonna have a purchase cancellation
before you actually have the purchase
well if you have a distributed system
and your purchase cancellation service
is a separate service from your your
purchase recognition service you might
actually find out about the cancellation
before you find out about the purchase
and if you embed that assumption into
your testing you're not going to be
exploring all the chaos that can arise
with all these things that can happen in
ways that you've never expected so yeah
we found this the hard way so this is
enclosure this is what the generative
testing looks like yeah a lot of
parenthesis closure is like that but you
can just see that essentially for all
inputs and in this case the inputs are
one account so the customer accounts in
a list of events and these events are
just randomly generated in this case
just purchases and payments so a list of
purchases and payments I'm gonna make
sure that the function and the function
is I'm gonna get my database they Tomic
I'm gonna save all consume all these
these events and the accounts save them
to the database get what the database
look like after I transact those
entities and then check if the property
that I care about so with invariant I'm
looking at is still true in this case
the properties the balances are positive
so you can run you know a quick check of
five hundred trials so I can you know
even add more than that and make sure
that that very never broke with all that
randomly generated an input if it didn't
break great result is true but if it did
break it's gonna return to me a minimal
case that when that property broke and I
can look at it and understand how my
fixing violations logic went wrong and
iterate on that to make sure that that
property will always will true so for
monitoring and you know figuring out how
other services will interact with with
this double-entry service we needed to
create this monitoring and replay
history tooling so the first thing that
we need to do to we cared about is that
we need to make sure that we
Pat didn't have any events that were
missing and this is you know essentially
a batch job that were a big query that
would make sure that every event before
a certain point in time in all the other
databases were recognized in my double
entry service so yeah we just run
essentially a query for that if all the
events are there great if they're not I
need to find a way to republish these
events so essentially all the services
have endpoints to replay all the
messages that it did produce or it
should have produced the nice thing
about this is that with the atomic I can
actually reproduce that message with the
exact payload and metadata as it went
out the first time because it can always
Traverse back in time and look at what
that entity looked like at that point in
time and just recreate that event with
the same metadata and payload so I'm
never actually losing information and
then we you know the other problem that
we had is that if the database is
actually ever corrupted and you'll find
a way to erase it and start over and the
nice thing about this is that we have
this endpoint that retract to all
entries so essentially reset the
business timeline but we don't reset the
database timeline which means that if I
have a corrupted database and I want to
you know make sure that it stopped that
I'll retract all the entries from this
point back republish all the events
recreate the new new new database but
the fact that at one point in time the
state of bases had corrupted incorrect
entries is not lost on me I can always
Traverse back in history and look at
that database before that retraction and
see those entries are still there so the
nice thing about this is that I don't no
longer need to choose between having
having an audit trail and having data
that's easy to work with easy to to get
your head around well the problem with
the atomic we need to keep it small so
we started shouting by customer and we
need to make sure that we could shard
the database by customer and the easy
thing about this is still to make sure
that we no had no entries that was we're
moving money around book accounts owned
by different customers
because it was just very hard to shard
if we did if we did it so any peer to
peer interaction is just going to be to
other events instead instead of just one
moving money between the two so we did
that from the beginning very easy to
shop by customer you can chart you could
even have one database per customer if
you wanted but even if you had this one
database per customer as time goes by is
still the database is going to grow to
infinity because well he's always adding
more data to that hopefully transacting
you a lot of money and paying back to us
so we also need to shard by time and
with accounting we have a very nice way
of representing the previous database
and the end state of the previous
database just the final balances for
every book account so the final balances
for the previous time shard is the
initial balances for your time shard
from there on out and because it's it's
easy and necessary
we were sharding our databases by time
fairly often so every couple of months
we're always charting it and to keep it
small keep a very very small working set
for performance purposes for analytical
workload we actually had we came up with
an ETL solution so we essentially we get
the immutable data immutable logs from
date from the atomic we extract those
logs save them on s3 pivot them to
tables because tables are easier and
better for analytical workload with
spark save those tables on us three then
we apply functions and generate the
balances and not just that we can
actually retrieve metadata from all the
other services because we have them
stored in the service so retrieve that
metadata create new tables first to map
out all the entries how they relate to
to the original services metadata also
with spark and then we load them up to
redshift and let redshift do its magic
we also make it accessible through meta
base which is a bi tool for easier
access for the data so the result is
that we have this real-time balance
sheet per customer with a lot of
metadata that you can essentially you
know have a very clear view of how each
customer or each group across
is affecting your business if you look
up there you're gonna see balance sheets
per user in the middle are gonna see
aggregates Purdue day and download by
creation month so when we started we
open the account for the customer
understand how each of these groups
interact in different way and when we
end up with two timelines we have the
first timelines the actual database
timeline so what did we know at which
point in time and if you look at a
specific point in time we also have the
business timeline which is the you know
business relevant official version of
what what the world looked like in our
opinion at that point in time and it's
it's also feeds into our modeling
because it doesn't really matter what we
know or what we eventually found out
knowing the gap between you know what we
think now and what we thought when we
actually make the decision this is very
important very important in key to map
out in your in our machine learning
models so this really as a whole layer
of power and audit trail capacities to
this service so what we like about this
is that well we finally were capable
creating these canonical definitions of
our most important numbers we now have
financial analysis applied at the
customer level and in real time we have
this consistency traceability so we can
take action on the inconsistencies if
you want to or at the very least you can
price how that's affecting our business
these invariants that we created provide
a whole set of safety nets for our
business make sure that everything's
consistent everything makes sense
incorrect invalid generative tests finds
real bugs in ways that no other tool
that I know of can find which is so it's
super powerful I highly recommend anyone
working in distributed systems to start
doing some kind of generative tests and
you fall in love with it I'm sure when
we have this ability to replay history
without losing data so you don't need to
choose between audit trails and data
that you can get your head around
so that's very very nice Sharda buh-bye
customer and buy time so it's scalable
and it's also extensible to other
financial products and the good thing
about this is that not all financial
products actually required the staple
flow you can get away with the pure
function of the event flow which makes
think makes things a lot easier and this
is all I have for you guys today thank
you very much and any questions sorry in
which way so so the ETL part of our
system that I'm probably not the best
person to talk about it's our data
infrared works that deals with that so
maybe I can I can give you the
information for that and we can talk to
them so yes I'm sure you can do this
with other databases we like the atomic
I we like immutability in general we
think it's very easy to get your head
around it's a lot easier to handle the
complexities of domains and how things
can interact have it also adds this
embedded audit trail by default so you
don't need to worry about you know
making mistakes we think it's very hard
for you to have this replay history
property have this without some sort of
immutable database but I'm sure there
are ways we can still get that with
other databases as well we just just
like the atomic did that from you know
by default
I'm sorry to hear that
yeah
so yeah we're going for the asset
transaction in the situation we don't
want the database to ever be in a state
that creates an invalid somehow is I
don't actually ever want the database to
be in breaking any violations at any
point in time so if I just prevent the
transaction to happen before and make
sure that I fix before actually
transacting I can't see that always my
database will not be breaking those
those violations yeah right so fight I
ever only persist valid events then you
have an exception and it's gonna be a
dead letter that on Kafka we need to
mainly understand why that there was a
you know inconsistency there and why
we're creating this invalid state and
usually I mean after you come we get to
stable in prod and we figure out you
know with generative tests all the
violations that we can have and how to
fix them you know we're not breaking
anything else we shouldn't have that
but until then essentially we get manual
input first to analyze and create new
rules so that I can just replay the same
message and then finally convert to a
valid state
well there's a finite yeah it's I mean
it there are dangers to that yes but not
having played out a big problem yet
of generative test runs that we do so
when we introduce new invariants and in
violation logic we we run thousands and
thousands of thousands of runs for four
that says to actually get through the
pipeline we're gonna do hundred runs
usually hundreds in the of runs to make
sure that it's okay the key thing is
that once you're actually introducing
new code and new logic we want to stress
that and we actually run I don't like
five thousand ten thousand times
we not yet we didn't really I mean there
are concerns and I guess the the thing
is like we we don't have too many events
per second for the same customer the the
level of number of events that we have
the the frequency of these events are
not huge as in for example algo trading
or anything like that
so we you know it's actually we gonna
have purchases that aren't going to
happen you know once or twice three
times a day for the same customer so
that level of possible inconsistencies
will not be super significant for the
business logic you can just you know
okay I really only care about you know
every second it should be correct you
know every that that that's the level of
correctness that we really care about
today if we apply this to probably
anything that has more events more
frequent events then definitely
something that we need be concerned
yeah well for that I think we the way
we'd handle that is just making sure
that we have this immutable
infrastructure that we're just spinning
with configures code we're spinning new
services always with the same
configuration and we're never gonna be
updating reconfiguring our system and it
can essentially through that gap
guarantee that I'm gonna have this same
version of all my services up and
running with the same you know
configurations and since we're deploying
a lot usually the the same instance
won't really be there for more than I
don't know one day or two it's very hard
to get in a weird state like that
usually when we actually have these
random way in the past numbers is
actually customer service agents making
incorrect input for the post dates and
we figured that out with also looking at
the data and saying well this broke some
invariance because now we need to
recalculate all these numbers or have
all these entries that essentially
rewrote history while that's strange we
can look at that
it's so we get it's a it's a unique
database per shard it's and it's going
to be available to essentially yeah
you're gonna have one database
one service that's gonna be accessing
many databases one per charter customer
but each database is essentially just
one transact or one instance of that
database yeah so we do end up because
I'm I'm now getting some data from the
database and modifying those balances we
have this optimistic lock strategy with
the atomic I can the atomic is actually
single threaded but the way that's one
of the ways that he's doing the acid
part of the transaction we so we
essentially should say these balances
these this accounts that I'm consuming
today at this point in time he can't
have changed can't have a different
balance from the point I started to the
point I'm actually transacting and when
I'm actually checking that I'm checking
that within the trans actor which is a
single threaded process they're carrying
can guarantee that you know if if
nothing changed I can just can con
transact and if changed I need to
recalculate everything again and start
from from scratch another thing that
helps is because are we're we don't have
any cross customer entries I can
actually use cafe partition keys to make
sure that for the same customer the same
thread is gonna be picked up because
it's gonna be partition by customer ID
so that's also another layer of a safe
see that we get with with that
mm-hmm
so the out-of-order events well how do
they happen so they happen because other
services are publishing messages in so
one of the ways it happens is because
we're plugged into other financial
systems of record so for example one of
the ways that we recognize payments is
that I'm gonna communicate with a bank
that's gonna recognize the payments in
the you know in our in our system and
this Bank is gonna let us know like oh
this payment happened and since I
actually pay Minh didn't come from me
the dependent came from the outside
world and maybe he won't just won't send
me a batch of these payments for some
bug on their system and on the purchase
side I deal with MasterCard and that
they can also do the same kind of kinds
of things so if all the events were
originated actually in our system then
we could have stronger ordering
guarantees which can be more consistent
with that but we don't and actually what
the terrible thing is that all payments
in Brazil the way your work actually
happened in the past I always find out
about them yeah one business day after
they actually happened and sometimes
more depending on how the guy actually
paid made the payment so ordering is not
there's no way for me to guarantee
ordering the way we operate today now
fortunately anything else thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>