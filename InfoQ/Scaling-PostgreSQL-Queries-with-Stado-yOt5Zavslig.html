<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling PostgreSQL Queries with Stado | Coder Coacher - Coaching Coders</title><meta content="Scaling PostgreSQL Queries with Stado - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling PostgreSQL Queries with Stado</b></h2><h5 class="post__date">2011-11-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yOt5Zavslig" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is a pretty cool first time that
we've had something like this out in
Denver been out the Denver quite a few
times is actually out here last month
for phosphor G the big geospatial open
source conference and I actually gave
this presentation there so if any of you
guys were at foss4g out here in Denver
last month it's the same presentation I
added two more slides but other than
that same I was pleasantly surprised at
phosphor G the number of post Christ
users that were here so there were 700
post quest users here in Denver last
month it was pretty pretty amazing being
here what are going to talk about is
scaling postgres using a product called
startup it's an open source product that
lets you scale postgres beyond a single
server starting up Who am I i'm the
founder of a company called cirrus
technologies focusing on consulting in
the cloud space mainly around databases
in the cloud moving people who have been
in an enterprise data center into a
cloud environment before that I was the
chief architect over at enterprisedb
spent a number of years adding Oracle
compatibility to postgres so we've got a
very deep understanding of the internals
of postgres now most of the co-organizer
of the New York City postgres users
group so we have about 300 members there
now regular monthly meetings it's I
encourage you to do that doing that here
too for you folks who are local here in
Denver there is a Denver users group
start attending those meetings as are
there it's it's a great community as you
get in order to get to talk to other
postgres users month after month it's a
great way of doing it and we've been
doing in New York for a couple years now
so what we'll talk about what stato is
the architecture of it kind of how it
handles different queries how it's
scaled and then most importantly the
limitations I'm going to tell you all
the really cool things about it about
how it will scale but I also tell you
where it's going to fall down so this
way if you have an application that
doesn't quite work with it you'll know
it had any time instead of wasting all
your time
what is it really a continuation of the
grid simple project rich sequel and
really the prior version of an extend EV
has been around since early 2003
enterprise DB acquired it back in 2007 i
believe and then open sourced it as
great sequel so it's been around for a
number of years a little over a year ago
enterprisedb stop supporting it and just
donated it out to the open source
community and when i left enterprisedb i
picked up and i renamed it stato because
of different trademark issues and things
like that's what saddle means is just
heard in polish kind of like slow knee
means a group of elephants in Russian I
figured it was a nice way to name it in
the in the postgres world so it's not a
new project it's been around for years
and years and years it just really has a
new name essentially what it is is it
shared nothing distributed data
architecture it'll lets you take your
Postgres database that runs on a single
server and span that same data race
across many servers or a single server
and paralyze it across all the different
chords you have on that single machine
it's essentially a layer that tips on
top of postgres that handles all the
coordination for you it's essentially an
open source version of greenplum netezza
or teradata it's really designed for
data warehousing and reporting what is
it from the beginning it was designed
for parallel query where postgres is you
have this great big query that's Hannah
lately going going to hit a terabyte
worth of data you throw at it with a box
with 16 cores and you look at it and one
chorus pegging the rest of them sitting
there knew nothing this from the
beginning was designed to paralyze that
across all your course across multiple
different machines from the beginning
it's not just to read only scenario but
it is geared towards data warehousing so
it's not going to be able to keep up
with your massive transaction load but
you can handle updates lita it is
transactional but if the ideal way of
doing it is through parallel loading and
data loading so if you have bulk loads
it does that extremely quickly and
that's the way to really get data in
there you can
do one insert at a time it's going to be
about 10 to speed as a regular postgres
instance but if you bulk load it it's
going to be about 10 times as fast and
it's on the wire it's going to look like
a regular Postgres database it sits on
top of postgres but you could connect to
it from P sequel and it's going to look
like a Postgres database to even though
what you're connecting to is stato
sitting in front of 10 postgres
databases what isn't it it's not a
replication solution like slow knee or
baccardo it's not designed for a
replication of any sort it does do some
replication in there but that just
really a performance enhancement it's
not a high availability solution it's
not designed to give you 5 92 up time
it's not a scalable transactional thing
like post car sexy postgres sexy doesn't
have query paralyzation it's designed
for massive number of concurrent rights
and skilling that across a bunch of
different servers and it's not one of
those elastic no sequel eventually
consistent databases it is a essentially
an open source data warehousing solution
what does it look like again it's
loosely coupled chair nothing
architecture so all these data nodes are
postgres databases they're all sitting
there independent post-press databases
and they don't talk to one another
everything is coordinated through the
central coordinator or sitting on top of
it and there's agents running on each
one of those postgrads databases if you
have to do some sort of row shipping
across no joints when you have data in
onenote desa joined the data in another
note and it does that the agents speed
that up and it maintains everything
through a couple different repositories
you're going to have the metadata
repository sitting on the coordinator
and each one of those data nodes you
could connect to their Postgres database
with piece equal in each one of them is
going to have its own database and in
that database is going to have a subset
of data so in this scenario where
there's four data notes each one of
those nodes is going to have a quarter
of the overall data you got a terabyte
worth of data each one of them is going
to have 250 gig alright
the way you set it up is if you're going
to have multiple cores and you want to
be able to leverage multiple cores the
best way of doing that is to put
multiple postgres instances on each
physical server because it's going to
paralyze at that level you can do it at
the database level it's much better to
do it at the instance of the cluster
level run multiple clusters this will
let you when we get to the end ended a
presentation of being able to scale out
better as you want to add more nodes
your tables are going to be either be
replicated or partition and we'll get
into a little bit more detail of what
that is I think that's why there's been
a lot of confusion over the years of
grid sequel or start out being a
replication solution because it says
tables can be replicated but it's really
just a performance enhancement you want
your replicated tables to be your
relatively static look-up tables state
code country code things that you're
always joining to but never updating and
your partition tables are going to be a
large fact table so if you think of it
as a star schema you're going to have
one large fact table and a bunch of
dimensions around it so all those
dimensions should be partitions
replicated and your fact table should be
part of partition so this way you'll be
able to paralyze across everything and
get linear performance you could also
leverage post presses internal
partitioning mechanisms so say you have
a really big sales table that you want
to be able to partition across multiple
servers but you also want to partition
it by month so sales month or year
you're able to leverage the constraint
exclusion partitioning inside a postgres
as well because whenever you could make
a data file and more importantly an
index smaller do it your queries are
going to go faster so if you could split
it up yet might have a billion rows in a
table but you could essentially through
this if you're running 10 nodes on it
each one of them is going to be a 100
gig and then you run 10 partitions each
one of them is going to be 10 gig that's
a more manageable table size so again
making things smaller makes things
faster which is what I just said and
because of the way stato works you're
able to scan those partitions in
parallel which can't do inside of native
postgres
so the way we do this is in the ddl in
order to create your tables we've added
a little bit of syntax at the end of
saying state codes for instance you put
the replicated syntax at the end of it
and that'll tell that would be stored in
the metadata repository of stato and
what that means is every one of the
nodes in data nodes inside the stato is
going to have exactly the same copy of
it so if you're going to update this and
your 10 nodes you're going to make ten
copies of it but what you're going to do
is all your queries are going to join to
that so you want that local to
everything and you're never never going
to change it how often do we add states
to the country it's relatively rare
things you don't want it's not a big
deal replicating that for your your
tables that have all their data so this
is the tiger data set that you get from
the US government that has all the roads
in the country parts you could put you
have the additional centex of a
partitioning key and here I'm just
partitioning it on the surrogate key GID
which will do a hash partitioning across
all the different notes and pretty well
equally distributed across all the
different notes you can partition it on
any of your columns I wouldn't advise
you to do it on a geometry column
because you have to calculate a hash and
a geometry column that's going to be
extremely slow but doing it on a
surrogate key works really really well
you could do it on state code and then
you know all the New Jersey is going to
be here all the Colorado is going to be
here if you're going to join regularly
on state code to other tables that you
might also partition by state code so
the way stato gets all the performance
enhanced with gains it really is through
its own optimizer so it's sitting on top
of postgres and it has its own parser it
has its own optimizer so when you send
in a query select star from foo it's
going to parse that and then also
optimize it so select start from ponos
it's going to have to get all the data
from all the notes when it's going
through his optimization it does a
cost-based optimizer the same way that
postgres does but what what it does is
it calculates the most expensive thing
inside of stud oh is Rho shipping so we
have to join two tables that are on
different physical nodes
you have to ship all the you know
million rows from one server to another
that's a really expensive operation so
it tries to avoid that wherever possible
and it's going to look for joints to
replicated tables because if it's a
replicated table it knows it's on all
the different nodes you could join their
you don't it knows for sure it never has
to do any row shipping and it's also
going to look for joins on foreign keys
that it knows they're on the same tape
so say you have a sales order header in
the sales order detail where you're
going to if you partition by you know
sales ID and sales line ID that wouldn't
necessarily mean that all the sales
order numbers are all on the same
physical server so it has 20 shipping if
you partition by sales / order number on
everything you could guarantee that
everything's on the same server with a
foreign key and it's going to choose
that plan over something that will do
Rose shipping the other really big
performance enhancement it does is
through aggregation doing aggregations
in parallel and it does that by
splitting up the the aggregates really
into two phases so if you're have all
your data out on all the different nodes
and you want to do an average what stato
doesn't send down average to all the
different nodes what it does is says
give me the sum and give me the counts
of that particular column and then on
the coordinator before it goes back to a
user it's going to some the sums some
accounts and do the division in order to
give you back the average so this way
you're able to calculate all that stuff
in parallel all down on the data nodes
and you're not bringing back all the
data to the coordinator and letting it
do all the calculations at the end
essentially becoming a bottleneck so
this way all that stuff happens in
parallel so this way you're able to
scale literally so now let's lecture you
start looking at a little bit of queries
again this is using the tiger data set
you gave me a nice 250 gig worth of data
and what this simple query does is it
just sums up the lengths of all the
roads in the u.s. that our interstates
you know if you're familiar with how
long it should be this is really
probably about twice as long as it's
supposed to be because the tiger data
set is that the county level some of
those roads go right along the county
borders but this is a perfect example of
a nice simple query that
sequential scan across all the rows in
the table and sum up all the lengths
what do we get so if I were on this on a
single server I ran this on an Amazon
ec2 instance one of the small instances
which had about 1.7 gig of memory in it
on one instance it ran in 101 seconds on
four instances four nodes at ran on 25
in 25 seconds and when i scaled it out
to actually running it on 16 ec2
instances and paralyzed it across all 16
16 instances and ran in 4.8 seconds so
it is fairly linear and if you notice
right around the 12 nodes there it
actually is a little bit faster than
linear scalability and the difference
there is that at that point in time
every all the data that it actually
needs is in cash for the number at say a
disk so because you're adding 12 nodes I
had 12 times 1.7 gig worth of memory in
order to be able to do that so I never
had a hit disk so you're a not only are
able to scale CPUs you're also able to
scale how much cash you have in your
overall system so that's pretty simple
scenario you be able to take that and do
that you know just script it and be able
to paralyze it across a bunch of
different nodes and be able to return
back to results but we had a more
complicated query of this one here of
taking that same road table and joining
it with some census data and also join
you with the state codes table in the
country code table and what hear what
I'm looking for is give me the top 20
counties in the country that has the
greatest density of population for a
kilometer of Road which one are the most
densely populated which would kind of
translate into which counties have the
biggest traffic problems if you had a
more real-world scenario work this thing
we have some sub selects doing some
joints we got an order by and we got a
limit limit of 20 on there so it's a
more complicated query right so it's not
just a simple some so as you'd expect
top of the list New York got a thousand
49 people per kilometer a road and
everything's kind of concentrated in
northeast you get some some interesting
things in there
that Hawaii and Puerto Rico pop up in
there right there Islands I guess things
are a little more densely populated
around there but it's more detailed
analysis but when you're joining
together two tables that have millions
upon millions of rows right and you if
you're running it on that one instance
when I ran it on one instance it took
3900 seconds which is about 66 minutes
as i scaled that out he added more and
more nodes when I got it out to 16 notes
it ran in 282 seconds which is under
five minutes so by scaling from a single
instance of postgres on a single node
against that giant dataset using a
complicated query I was in scaling it
out to actually 16 cloud instances this
were other physical hardware or clouds I
was able to save a now over an hour's
worth of time running that single query
and we see this time and time again for
using stata if you want to go but I
wanted to get that cut in half down to
two minutes I'll just go to 32 notes you
know based on you know that query it is
linear scale will you add more notes you
want to go faster add more notes and
it's really there for detailed analytics
like that query that I just showed you
and it does work across the entire set
of different analytical queries this is
this was done against the DBT three
several years ago and it ran across the
entire 22 query set and as you add more
notes it goes faster that's the the idea
of if if you want to be able to
guarantee to your users that you could
run a particular query in a minute
you're running on one you run it on for
you can see what the scalability is and
you can easily calculate how how many
nodes you need in order to get that
query done in a minute so now you're
told you all the cool stuff about how
you can throw out things from 66 minutes
down to five minutes tell you what it
doesn't do because it has its own
optimizer and parser it doesn't have
everything that native postgres has so
it does postgres adds more features to
it we have to also add at the stat out
into its own parser an optimizer to take
advantage of it and we
and added things like windowing
functions yet you can't run stored
procedures if you do a crate function on
there it's not going to work there's
kind of workarounds for that but
essentially everything comes in as a a
sequel statement there's no full text
search before the latest releases stato
there was no geospatial support it's we
saw a big need for geospatial support
because geospatial queries are very CPU
intensive and would take advantage of
the parallelization extremely well so
we'll be adding more and more features
over the course of time and if they're
specific things you need let us know but
it's not going to be full native
postgres if not everything is going to
work because it as its own parser
transaction performance if you're
sending things in as a single insert or
update at a time especially if you're
going to update something that it's
partitioned by where esta delete it from
one no one node and insert it into
another it's going to be slow it's going
to what you're always gonna have to do
is do an additional network round trip
because everything's going to come in
through the coordinator and then send
you out to a data node and then all the
way back to the users the partition
stuff also has to calculate the hash on
whatever your your partitioning by so
it's going to do an extra extra amount
of CPU cycles on a coordinator or your
partition rose and the replication it's
going to have to commit to all the
different data nodes before it's going
to return back a success which again is
if you have to get data in bulk load it
you could use the copy command you could
use GS loader which will paralyze things
really well for you from the client it's
no actually a pretty cool way if you
have some bad records in there does some
some handling of bad record see this way
you import a million rows and one of
them is bad it's not going to roll them
all back high-availability there is none
all right one of your data nose goes
down you're down there's nothing in the
coordinator saying that if one of your
notes down fell over somewhere else or
just ignore that part and return back an
error of something that wouldn't
touch that particular day to known
anyway so what you have to do is
configure high availability on each of
the individual notes so if you have 10
nodes inside you're inside your shadow
cluster you have to set up replication
or something on each one of those what
you do is use each one of those data
nodes as they fail over of another one
so essentially you're setting up raid at
the server level so this way they all
fell over to one another if one of them
goes down you just one of those nose
might be double doing double duty also
getting a consistent back up across all
of them is a little difficult so if
you're actively putting in transactions
there's nothing inside estado saying
pause there's no PG start back up at
this whole state'll level which will
send it down to everybody essentially
what you want to do is pause the
coordinator for a second and then do a
PG start back up across all your data
nodes and start it back up again and
then you could get a consistent back up
you could use a bunch of different tools
whether it be Omni pitter or whatever in
order to get those backups at the bottom
level you have to handle that yourself
you're essentially going to be managing
10 post Christ databases you have to
coordinate the backups of it so there
again there's no way of ensuring that
there's no transactions happening adding
notes requires downtime if you want to
go from 10 nodes the 20 notes you have
to bring everything down essentially
what you have to do is repartition all
your data so you have to export all your
data out and then import it all back in
so this way it's repartition to cross
all 20 notes because they're your
partitioning algorithm already spread
out the data for the 10 notes what we do
really is some advance planning because
most of the times when we're deploying
stato it's usually on a very big
database no it's on you know something
that's in the multi terabyte range that
exporting all your data out and
importing it back in might take a week
it's not something that you really want
to do all the time so if you set it up
in the beginning thinking that all right
I'm going to start with four notes today
but I'm going to go to 20 nodes in the
future what you start doing what you do
in the beginning is set up five postgres
instances on each one
those four physical servers so this way
everything in the beginning is a ready
partition for 20 notes when you want to
go from four to eight what you do is you
bring everything down and just our sink
your your different databases over to
the other one so this way everything's
we partitioned repoint the IP addresses
start them back up again so this way you
don't have to repartition everything
you're essentially just moving the
database themselves you can do that
through streaming replication or
something else where you could replicate
it over to the new notes so you're down
times minimized and also by doing that
in the early days you're also being able
to leverage multiple cores on the
physical servers themselves so in
summary you can't get really really good
performance out of doing it as you add
notes it does scale literally long as
long as you're not doing the cross no
joints you're going to see linear
scalability so if you have a classic
star schema you're never going to do
across no joint you're going to get
linear scalability we've tried it up to
like 32 64 I think we've done 64 nodes
and seen linear scalability
theoretically to go faster I we just got
tired of trying more and more note this
is entirely open source if you don't
like it submit a patch that there's
something you don't like we'll take
submissions more than happy to it's all
entirely written in Java if you're a
Java developer know anybody that's java
developer you want to add a feature more
than happy to take it so where do you
get it go to stato us that's where you
could download it and if you have any
questions feel free to send me off an
email or hit me on Twitter and again I'm
pimping out the postgres users groups
get active in them here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>