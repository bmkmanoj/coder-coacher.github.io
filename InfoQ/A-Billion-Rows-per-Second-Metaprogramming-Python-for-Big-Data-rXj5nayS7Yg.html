<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Billion Rows per Second: Metaprogramming Python for Big Data | Coder Coacher - Coaching Coders</title><meta content="A Billion Rows per Second: Metaprogramming Python for Big Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Billion Rows per Second: Metaprogramming Python for Big Data</b></h2><h5 class="post__date">2013-09-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rXj5nayS7Yg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay well I mean I'm Billy thanks for
having me here today
yeah so I'm currently working as a
principal engineer at a drawl so and
like like Rajesh mentioned we are
producing tons of data which makes it
very interesting I've been actually
doing big data with Python for like two
long time probably at least since 2000
and maybe something that should be
interesting to you is that if you ever
find yourself in the need of MapReduce
and you like Python as obviously you do
since you are here you should go to
disco project a torque which is disco
MapReduce a an implementation of
MapReduce that uses Erlang and python
that has been around since 2008 I
studied the project and it's the
community is really really active and
it's really great if you like Python
well but this talk is not about that
this talk is about something else
this talk is about the stuff that I an
add role have been working on like for
the past couple of months especially
related to the question that I'm sure
that many of you and actually like the
previous speakers also have clearly had
which is that that one when a kind of a
modern company like a drawl or pretty
much like Eventbrite I'm sure or any
other company producing tons of data has
but when you want to understand what's
going on either in your business you
want to do business intelligence and
today involves data and lots of data so
really the first question is that how do
you actually slice and dice how do you
worry tens of billions of data points or
hundreds of billions of data points and
now if you have been following the field
you know that they have been like the
pumps of interesting projects that have
come out like for the past during the
past five years say so there are like
interesting projects that work on top of
Hadoop so there is a first hive and
they're speak and well I mean you might
use something like HBase even or a of
course they're local of the new aircraft
you have things like Impala but I mean
it's very much like an ongoing thing
so I mean it there isn't really a clear
winner in that space and in a way I do
believe that it's very hard to have a
one project that would fit in all
problems but still it's a very valid
question and actually the reason why I'm
here today and why I think that like
this stuff should be interesting to you
is that actually we ended up building a
custom in-house solution instead of
using well maybe a proprietary solution
like vertical or a redshift or or one of
the well-known open-source projects like
well let's say hi and well the thing is
the power solution that we have built
over actually the last couple of months
of course based on the much of the
previous work that we have done is
implemented in Python and now if you
prefer like the really like if you think
about Python is that everybody knows
that poison is a slow language slow
meaning that it's nowhere near the speed
or performance or of C or C++ or Java
even so in a way I mean you might think
that especially when you are interested
in doing low latency processing in
Python with huge amounts of data it's
it's totally not a match it's totally an
oxymoron statement that you are saying
that you are doing a big data processing
in Python and it's low late and so first
the thing is that if you don't care
about latency you can do MapReduce and
then well I mean then it's totally
doable as proven by this go for instance
but the interesting thing here is that
we are definitely interested in kind of
keeping things at low latency and the
reason for that is that we want to
support first ad-hoc query so I mean you
don't have to define the queries that
you are interested in beforehand and
secondly data exploration as well as
data science really kind of doing the
iterations really fast and you shouldn't
have to wait for hours for a MapReduce
job to finish so the question is that
how do we actually do this with billions
and tens of billions of events so that
you can get results in seconds and well
another thing is that of course if you
had like tons of machines that your
disposal which is very easy nowadays in
a way I mean like doing this debuted
computing is very easy I mean especially
you have immutable data per se if you do
have mutable data then it actually gets
replay horribly complicated but if you
have immutable data you can just add
like sure nothing architecture you
should just shark your data have as many
machines as you need and and things are
kind of they do work but I mean it adds
an extra layer of complexity and it adds
also first like all kinds of issues with
cost and complexity and so forth and
especially when you are starting to do
more complicated advanced things like
data science you really want to keep
things simple so as a kind of
interesting kind of a starting point
motivation for the whole thing about
Python and Python being slow
here's a small benchmark so I mean this
is a very small of course like a subset
of data I mean it's not small in that so
it's and still I mean 1.5 billion rows
in your database is a pretty sizable
database but I mean like the I mean
that's the kind of amount of data we are
probably producing in an hour or so so
the size of the data data set is about a
little bit more than half a terabyte so
not that much obviously in our benchmark
we really care about ladies so smaller
execution time is better
we chose the fastest result from ten
consecutive runs meaning that well I
mean like if there happens to be just
one path that or if for some reason we
don't count that and we don't of course
- the result so this is really about
like testing the query performance well
another thing is that we are comparing
our system in this motivational example
the Amazon redshift and I guess will
actually mention about redshift in the
in the last talk so unless you have
heard about redshift it's a data
warehouse offering by Amazon meaning
that it's basically data warehouses
service so if you have ever used Amazon
services so you know that it's very easy
to get started with these things so you
just click the button and you have your
own data warehouse up and running well
in this case like I mentioned I mean
like this is not about distributed
computing that would be a separate Bowl
and in this kind of you know in many
ways I mean that's kind of it I mean
there are really many good solutions for
that this is about asking how much we
can actually deal with a single machine
so look I mean we are taking a single
server that's actually the most powerful
server that you can find in Amazon with
244 244 gigabytes of memory 16 cores and
so it's pretty much a monster machine so
it's it's really pretty decent
this isn't iron and we kick
purple cluster in red six so we take
three notes unfortunately since kind of
otherwise it wouldn't work and actually
in this case red city is getting twice
the number of cores but about the same
amount of RAM and well so it happens
that actually Red's if if you've run it
in this configuration it would end up
being a little bit costlier but I mean
maybe maybe that's not the consideration
and now of course if you if you wonder
if actually red suit is a bad apple so
it's just a crappy system that we decide
to compare against at least some people
playing that is faster than hive I don't
have personal experience but like you
can actually read the blog articles and
believe them if you like another thing
is that looking at the actual numbers
since this is all about analytics so I
should have made it maybe more clear
that in contrast the maybe let's say
elastic search which is really great for
a heterogeneous data and structured data
and so forth this is really about like
having numerical data having tons of
numerical data and having large
aggregations over numerical data and not
so much about looking up single rows in
a database like you what you would do
with the no sequel database or key value
database say so in this case we are
taking probably the simplest aggregation
that you can imagine you are summing
over a column in a table and remember
that again we have about 1.5 billion
rows so you are basically summing over
1.5 billion values so it's a non-trivial
thing but as you can see both red shift
and our system kind of a manage that
pretty well and yes our system is faster
but I mean the difference isn't that
huge
one thing to note here is that our first
both the system scale pretty much
linearly so imagine that if you had 10
times more data
15 billion data points red zipped would
most likely take something like 19
seconds and our system would take six
seconds so it's not an order of
magnitude difference but it actually
might make a difference when you're
actually doing your interactive analysis
another example a typical thing that you
might want to do is that you take an
aggregation and you want to group that
by a dimension dimension might be a
country it might be a customer ID or
something of that kind again I mean the
systems are pretty much comparable and
again you might consider that it's
pretty amazing that something written in
Python is as fast as
as pretty much state-of-the-art
distributed database that's really kind
of written by people who should know
what they're doing and I'm sure that
they do well actually things start
getting more interesting when you start
digging aggregations over a number of
columns so here we are taking an
aggregation the same summing over seven
columns and now suddenly our system is
five times faster than redshift so you
can kind of a guess the pattern actually
if you pass forward this 250 columns you
can see that MSN starts taking 8 minutes
whereas our system is pretty much the
same as before and now of course you
might say that well it's a pretty kind
of a bad example I mean no one ever
wants to do an aggregation over 250
columns
well except if you do data mining or
data science or anything of that kind
that actually let's say it requires you
to kind of take all the features of a
certain point and then you want to see
the results so it's it's actually a
pretty realistic example and this was
one of the motivating reasons why we
started considering alternatives to
redshift so in stuff courses you can
imagine this only get worse when you get
more data so again this is a Python
based system I mean it should be totally
impossible to have any anything like
this so I mean it's a single server
it's 1.5 billion data points and it's
outperforming the the best of the breed
system running on three servers having
twice the number of cores well actually
it's a real system it does exist and the
stack looks pretty much like this so I
know that I don't have too much time so
unfortunately I don't go into detail
here so this is pretty much an overview
talk I'm pretty happy to kind of give
you any details you might be interested
in after the talk but I just want to
highlight these three items here and so
basically just to give you an idea how
it works I mean typically analysts or
anyone actually wanting to do the
interactive analysis or data exploration
actually would use a tool like Excel or
tableau or which is a visual business
intelligence tool to actually generate
sequel queries that then hit Postgres
that then actually hit this thing called
multi court that I'll be talking about
soon then it hits our servers then it
goes to these worker processes that we
have
one per core and then goes to this
tingled gnome but more about that later
and then to LLVM and then to our
specially encoded data and that's pretty
much the process how it works and I want
to highlight these things there I mean
starting from bottom up since I think
that's the interesting part well really
the thing and the secret sauce here and
and the reason why it is actually
possible to do this with python is that
we do care about data structures so the
thing is that actually kind of caring
about the language that you are using in
a way yes you can get like really order
of magnitude improvements let's say 10
times faster if you you see instead of
python but the thing is that you can get
infinite improvements if you actually
rethink the algorithms and if you
rethink the data structures that you are
using so let me just give you a good
idea what I'm talking about so here
that's really the data that we are
talking about so it's events I mean I'm
sure that like all of you have seen
something like this so it's basic like
JSON messages that the data is produced
by our system so it's very homogeneous
we know the fields that it contains and
they all have the same form basically
and like you see I mean like we have
some boolean field some strings some
integers all nice and simple now of
course well again I mean we have 1.5
billion of these probes in the benchmark
well I mean since it's very homogeneous
data what you can do is that you can of
course property des you can drop the
field names and again it's the same data
nothing changed is it it's a very
trivial transformation well I mean what
you can do as well is that a person if
this is the stress and it's most likely
strings may be sitting somewhere on your
disk waiting to be decoded as JSON so
that's not very nice what you can do
instead is that you can actually then go
encode everything as integers so again I
mean like here so true and false could
be zeros and ones obviously countries
can be encoded with integers between
let's say 0 and 300 like you give an
integer to each country the cost could
be again another integer obviously so
you get this like a nice matrix
integers so it's a three by three matrix
and now well the thing is that computers
actually do like matrices so matrices
are really really nice in computers
point of view and well the thing is that
that's of course is very small matrix
actually if you now think that we have
1.5 billion of those rows it looks more
something like this and this is not 1.5
billion this is something I do hundred
but you get the idea it's this kind of
the same idea we have lots of integers
but instead of having a one big solid
red matrix it's sparse it's it's body
red meaning that actually we don't have
values for all possible rows so it
becomes a sparse matrix and that's a
very good starting point and there I
have that that's pretty much kind of did
100 percent of the original size so the
question is that can we actually make
this more efficient since still this
matrix is actually pretty huge so if you
just took all your data and encoded it
with integer binary data and had it as
far as huge matrix still I mean
processing that would take quite a while
well the first thing that you can do is
that you can recognize the fact the
different columns have different types
so this is the first basic observation
that all columnar database is probably
did already in 1980 is that well I mean
some of the columns might be strings
some of them might be like we had wool
and flags so obviously you can encode a
boolean flag as a bit instead of well
it's a integer and let's say that like
you have cost value that I had in the
example and the cost price between 0 &amp;amp; 1
million so well then you can just use 3
bytes or a 24-bit integer to encode that
value so again I mean you get huge huge
savings like already just by doing this
you get like 50% smaller matrix that
what do you started with so it's a it's
a big improvement and this is a for
something that all holnar databases do
including redshift so that's really the
basic basic trick well still I mean the
question is can you do something more
well the the matrix looks pretty random
so I mean like kind of a dozen it's not
obvious that what you can do with the
random matrix but the thing is that
actually when you sort the matrix let me
patterns start to emerge so it's not
like a random matrix after all but when
you actually store it the rows do start
seeing that well I mean for one I mean
girl like all kinds of things that you
see happening and always when you see
things like that you might start
thinking that well lemme maybe there is
a way how we can actually leverage this
information and that's really the case
you can actually a note by the way that
this is exactly the same matrix we just
saw read it the rows so it's the same
size and the same information and and
all these transformations are a lossless
so I mean we are not losing any data but
I mean one obvious thing that we can do
is that we can get rid of dublicate rows
so we can just kind of squeeze the
matrix and then get rid of like
redundant data so obviously if you have
three rows that have exactly the same
values and then they have count 1 you
can just represent them as one row and
say that the count is 3 and now when you
are summing over these things you are
summing over one row instead of three
rows but the result is exactly the same
and again I mean the matrix is much much
smaller so that's a really a nice nice
like way to kind of a compress the stuff
that we are working on well there's at
least one more thing that we can do
actually they're like tons of things
that we can do but I mean this is just a
kind of a give you an idea of the
process well I mean one is that what I
mentioned about different types yes I
mean it's a good idea what like all
these systems do that you use different
types for different columns but still
there's the question that if you use
let's say the pre byte column that I
mentioned and you have a value of one
and you are still using three bytes then
odd value of one which is like really
really really really redundant since I
mean you really don't need three bytes
for that so actually what you can do is
that you can devise some encoding
variable length encoding so that you use
the minimal number of bits to encode
these values and now so it happens that
if you look at the result the result is
rapes kind of a way smaller than what we
started with actually it's just a 15% of
the original size and it's all very
compactly encoded and like you have
minimize really many minimize the
redundancy matrix and why this is
important well I mean there are number
of reasons one is that well overall
there's less redundancy
there is just like a fewer computations
that you need to perform so I mean like
I mentioned I mean you want to get rid
of duplicate results that's us with
result to unnecessary work being done
but maybe the most important point is
that like these matrices or any dense
data is free cash efficient so you know
how CPUs work so when you do something
let's say the CPU knows that I need to
sum over some column it goes to Ram it
fetches the item from RAM it maybe goes
to l lq - that's near the cpu it goes to
l1 cache and then just the computation
it fetches the next number and if it so
happens that the number is already in
the cache you don't have to go to the
RAM and actually you can do probably
like thousand times more work just by
using the data that's in the cache then
what it would take if you actually
needed to go to grab so you really
really want to pay close attention to
cache efficiency of your data if if you
are operating at this level and well I
mean one interesting feature of this is
that also when you are kind of getting
rid of redundancy you are kind of
implicitly building indices but that's
maybe topic for another talk but let's
let me focus on the cache efficiency
since that's actually pre-assess to the
next topic which is known by well so it
happens that if you care about something
as low-level as CPU cache efficiency of
course using a language like high-level
language like Python is is a bit
problematic since there's a lot of lots
of many things happening behind behind
the scenes and behind your back so
there's garbage collection and and of
course pies in the C Python virtual
machine of course does all kinds of
things so it's it's very hard to first
make sure that you are not doing
unnecessary work well so it happens in
the Python world there are like really
lots of and many interesting projects
going on today that help you to produce
really efficient code in Python noon
bias is one of those projects so you
should definitely check it out so what
it actually does is that he said as it
says it's a just-in-time specializing or
boiler which composed another invite in
a non pipe out to LLVM and LLVM being
the compiler it will get developed by
apple and others
and how it works is that like it can
take any Python function like an example
here you just add a decorator like which
is in this case called Auto cheat and it
magically compiles your Python function
to machine code and it's super super
magic I'll think about it so you have a
Python function and you just add a
decorator and suddenly you have this
super high efficient version of the
function so it's it's great well the
downside of that is that unfortunately
it's not able to take any arbitrary
Python code and make it fast so it
mainly works with numerical code so if
you ever by using numpy or things like
that it can all actually automatically
detect the types of matrices that you
are using and using that information can
actually produce efficient code whereas
if you are doing like what we are doing
80% 90% of the time with Python doing
stream manipulation and stuff it's
actually not that good but I mean so it
happens that this is actually exactly
what we need we need a way to produce
low-level code that manipulates
numerical data so noone buys a good
match for the project how gnome by
actually works in our case and this is
the reason why the talk is titled meta
programming is that we're actually
generating code python code on the fly
so again going back to the stack you
remember that at the top at the very top
of the stack we have a tool like Excel
or tableau or maybe analyst typing C
well so what happens is the sequel where
you actually post a post Chris and the
post press through number of steps
actually sends the query to our system
and eventually hits the part of our
system that handles the compilation the
compiler actually decides the query
takes the columns that you are
interested in and takes the where
clauses that you are interested in and
basically knowing the matrices and the
types of matrices we're analyzing and
knowing the query it can actually
generate a piece of Python function that
answers exactly the question that you
are asking so this is actually the kind
of you get the idea what what do the
kind of the Python the automatically
generated plate and look like and it's
very low-level and the reason that it's
low-level is that that's exactly the
type of Python stuff that nobody likes
so now we can actually feed this
automatically generated Python function
the
nova which compiles it to LLVM so this
is the LLVM intermediate representation
so it looks like assembly so if you have
ever done any low-level coding it's it's
very low-level but the thing about this
is that this when you are operating at
this level it really gives you an
opportunity to kind of optimize things
like that - efficiency and of course the
thing is that now I mean we are actually
producing a piece of code that's
especially it's basically specialized
for it for the type of the query and for
the kind of the data that we are
operating on which is another key
deficiency that we are seeing so that's
that's how it works
well the last episode I don't know if
you can see that but it says that like
the last step is to compile this to
optimize machine code on the fly so
another interesting kind of side effect
of these two is that we can leverage
LLVM optimization framework to actually
optimize the query that we are executing
so if it so happens that there are any
kind of redundancies in the code itself
LLVM hopefully can take care of that so
that's that's really super awesome and
that's also a key why we can both
combine the densely encoded matrices
with densely optimized code and get the
result that you saw earlier well now let
me actually check the time so okay
anyway I'm almost running well
practically running out of time let me
just pass forward so the thing is that
like like I mentioned no system is in
Ireland the analysts actually want to
use this analysts like sequel as we
heard in the previous presentation so we
want to provide a sequel interface now
of course we are not as insane as tests
that we would be thinking that we can
actually implement the sequel standard
by ourselves and knowing that there are
really excellent projects around like
post press I mean of course we do want
to leverage the whatever is available
and now of course there's a question
that how do you actually use something
like post cursor or any relational
database with like no sequel custom
system that we built well now let me
introduce you the multi quorum so again
if you have never heard about multi core
actually I hadn't so I mean you should
definitely check it out yet another
thing that like you should be using it's
a way how you can actually map your
- datasource yeah I know I should be
well anyone let me just passport I know
we are running out of time anyway the
point is that you can write things in
Python and they will show up as normal
tables in your post press database so
you can actually do all kinds of joins
all the magic you see well while
actually offloading the Big Data stuff
do whatever again you might be using at
the deliver on our case well let me just
quickly conclude my talk yes this is
cheating yes and that's exactly the
point
what do you want to do with the language
like Python is that you want to embed
all the domain knowledge you have and
all the tricks or all the kind of the
information about the constraints that
you know that well we will never need
worries like this and you want to
implement the system that really takes
that into account and the key thing here
is that you can actually outperform any
system built in any language and you
just design your data structures
correctly even if you are using Python
and actually it might be that especially
when you are using Python since many of
these things that I showed here would
have been such that yes they are
possible to implement in is C but I mean
it would have been extremely painful to
do all those transformations and stuff
in C which is very I mean which is quite
not as expressive as Python so you can
actually benefit from the fact that
Python is really expressive and do
things that would be way too complicated
to implement in low-level language and
actually you can beat a low-level
language in their own game in this way
so thank you so actually we are using
her like by the way
okay I think I'm confused to speak to
the software I can use now this
attempted vengeance is a number oh oh
yeah well unfortunately so it happens
that I actually haven't asked my boss
yet if we can open source this so I know
it's not yet available but I hope that
what's the kind of the take-home message
here is that you should definitely check
out number you should check out multi
corn and you should also always have the
feeling that anything is possible with
Python so I mean if anyone tries to tell
you that it's not fast enough I mean
they are wrong I mean like it's it's
really not the case but it yes it does
require some work do you see a possible
implementation of place for multiple
data type of demonstration well if I
remember correctly place is actually
another project by continuum IO which is
behind a number and they have like a
very similar world view how you can
actually efficiently encode matrices and
personal vast computation on matrices
and I'm definitely looking forward to
whatever they end up doing so it so
happens that this was something that we
needed know and and well I mean in place
it's not yet available at least not the
features that we would need oh that's it
that's an excellent question yeah so the
question was that like how much of the
performance difference between read
saved and delivered can be attributed to
the fact that actually our system is
running on a single machine whereas
redshift is running on multiple machines
so obviously there is some like a
network overhead so well I mean one
thing is that especially when it comes
to kind of a basic summing over columns
of course actually how great shift work
is that it offloads like computation to
these individual charts and the chart
just like return whatever like
intermediate results so there should be
very little like a network communication
happening between the nodes and actually
there's a dashboard in website that
shows you the network communication it's
really pretty minimal so I don't believe
that that's the reason it's really well
ok maybe I shouldn't they operate okay
go ahead yes so multi boring that's a
very good question of course there is
the whole point behind this thing is
that well well I mean we definitely want
to be doing and like utilizing all the
course that we have on the machine it's
it's ray CPU intensive things so I mean
yes you do want
use all the CPU power well I mean like
really the obvious thing with with
multi-core is that you have so you have
K nodes K pores on your system so you
have K processes so I mean you have one
process one worker process per core well
I mean the things are practically not
quite as simple what you do want to do
as well is that there's this thing in
Linux that you can actually set the task
affinity so that played the kernel won't
be like a moving process it round
especially with the machine like ours
which is actually a non-uniform memory
access machine so you want to kind of
pay attention things like that but yes
the basic approach is to have a well I
mean like in this case 16 Python
processes running on the machine right
so that's another good question about
all about the everything that I
presented here today is about reading
it's kind of everything is about reading
and there's nothing about writing and
well I mean I like Erlang I'm a big fan
of immutability so I mean I don't
typically change anything and it
specific historical data doesn't change
so unfortunately well I mean maybe
fortunately I'm like working in the
field of analytics which is like mainly
the part that actually gets all the crap
that the kind of the guys end up
producing on the operational side and of
course like kind of people you use the
systems so my worldview is that like I
mean like we are actually using earth as
well so what happens is that Kafka like
basically pushes and produces like a
tons of data but I mean the first like
after we have collected those log files
they don't change so it's only immutable
I don't have to care about rights at all
so that's the three easy and simple okay
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>