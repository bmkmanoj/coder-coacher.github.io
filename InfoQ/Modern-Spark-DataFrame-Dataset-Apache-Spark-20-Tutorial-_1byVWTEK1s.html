<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modern Spark DataFrame &amp; Dataset | Apache Spark 2.0 Tutorial | Coder Coacher - Coaching Coders</title><meta content="Modern Spark DataFrame &amp; Dataset | Apache Spark 2.0 Tutorial - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modern Spark DataFrame &amp; Dataset | Apache Spark 2.0 Tutorial</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_1byVWTEK1s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Adam brine Dale I do a lot of
work with Apache spark and I teach a lot
of classes on spark with data bricks and
new circle and in a lot of classes there
are questions that come up about certain
api's data frames and data sets how they
work and how to use them so I wanted to
talk a little bit about how those work
when they make sense and go a little bit
deeper technically into the advantages
and mechanisms here everything we're
doing today is on the Internet and
you'll be able to play around with spark
2.0 through data bricks Community
Edition so this is called modern spark
data frame and data set frequent
questions answers and a look behind the
scenes so what do we mean by modern
spark well about a year ago we're really
excited about the data frame API and
until relatively recently it was marked
experimental because of the way the
documentation is generated but really by
this point data frames are the
established way to do things in spark
what I'm going to argue is that not only
they perform better and they're easier
to use but at this point there isn't
really any reason not to be using this
API so that's kind of the TLDR but what
we want to do is say ok how does this
actually work not just how do you use it
but when I say it works better what do I
actually mean so let's go and take a
look at some code what we're going to
play with here is a small data set
that's actually hosted by MongoDB it
just has zip code data for the United
States so this is not big data but it
makes it really it's got a little bit of
structure to it makes it easy to take a
look and explore with data bricks
Community Edition so the very first
thing I'm going to do here is just
retrieve the data set using W get for
big data is probably not the best way to
do it you'd probably load data into
spark from HDFS or s3 or something like
that but we're going to retrieve it here
since it's a small data set over the web
and we'll just wait for this to download
and it looks like here it is and I'm
going to scroll down so where did this
file go zips JSON we'll do a PWD here
looks like we're in slash data brick
slash driver so how can I get started
with this and data frames
I'm going to go sequel context read JSON
and pointed at the
file and I'm going to say register temp
table which creates a view that I can
use to talk about that table in sequel
so I'm going to go ahead and run that
and notice the deprecation warning that
came up that's new because of spark 2.0
in spark 2.0 we call this create a
replace temp view because register temp
table caused a little bit of confusion
so I can use that call but it generates
this deprecation warning so we'll come
down here and try the spark 2.0 version
notice also that I use the Sparks
session object at the beginning of the
line so I say spark instead of sequel
context in spark 2.0 because of the
proliferation of context objects spark
context sequel context streaming context
there's a new spark session object that
provides an easy entry point for our
code so I'm going to try this spark dot
read JSON and the Creator replace temp
view so we'll run this and we get the
same thing but this shows that we're
using the latest API and then I can hop
over here into SQL and just take a look
at this table not the data this is the
schema of the table I do a describe on
it and we can see that there's a city
location population state the underscore
ID field is actually where the zip code
lives this is sort of a Mongo ism it'll
be familiar to you if you've used Mongo
but we're going to go ahead and change
that to be called zip so in this next
line I'm just going to grab the table
and say with column renamed and then
I'll create or replace the temp for you
so I'm going to throw away the old one
and bring a newer one in here so that's
all we had to do to get our data into a
situation where we can work with it
using SQL so let's do a very simple
analytic query on this thing what are
the most populous cities in Illinois and
how many postcodes do they have right
this is a pretty reasonable thing to do
with sequel we've got a select count of
zip here sum of population and city from
the zip table we made where the state is
Illinois we're going to group by city
we're going to order by the population
total descending and we're
a limit to the top 10 and if we go ahead
and run this just straightforward SQL
against this data set you know we see
Chicago at the top which isn't too
surprising on a handful of other cities
that have a hundred thousand or fifty
thousand or more people here in Illinois
over on the left you can see the number
of zip codes so we're not going to do a
whole lot of analytics work here but
this just is a straightforward query and
what I want to do is talk about you know
why would we do do it this way and not
with the RDD API so the couple of
reasons first I prepared a little
example so we can see what it looks like
to code this with the RDD API so to
rewind a second remember this is a
simple query top most populous cities in
Illinois ordered group together and when
we go to do this using the RTD API first
I'm going to run this cell that prepares
the the file we're going to look at and
this is how we're going to do it we're
going to read the data in we're going to
parse it on the delimiter x' here we're
going to create these scala tuples that
represent rows and then to do the query
we're going to filter for illinois over
here we're going to project out the city
and the population we're going to do a
reduce by key so for each city we roll
up all the population data then we're
going to sort this thing in a descending
order and we'll do a take ten so that we
can get the results back more
efficiently and we'll print them all out
so I'm going to run this and I just want
you to see that we get the same data but
unless you are really kind of enjoy
doing things the hard way this is not
easy to read or to write right once we
know what we're doing if I already
explained what the goal is this kind of
makes sense but it takes it takes a
couple minutes to think about how to
structure this you can imagine it's
pretty easy to get it wrong and in six
months when I come back to it or my
coworker needs to look at it it's really
not obvious that this is a simple
analytics query so just from a human
usability point of view
thus equals a whole lot a whole lot
nicer and what I'm going to be showing
is that not only is that the case but
the sequel version will run faster and
it will execute a better version of the
query than the one that we're trying to
do by hand anyway so now that we've got
that comparison down let's come down
here and ask a simple question is this
the best way to write this query and I
just want to demonstrate something here
so what I wrote is reasonable uh but
would spark fix it if we wrote a worse
version so in this next cell here's a
slightly worse version of that same
query it produces the same answer but
why is it worse well I'm doing all of my
processing before I filter for Illinois
right so I'm doing a lot more work than
throwing most of it away if I filtered
first I'd have less data this is obvious
and most traditional database systems
will do these optimizations for you but
the RTD API isn't going to and to prove
that let's go and take a look at the job
UI and see what just happened so I'm
going to switch over to this tab this
shows my spark 2.0 cluster that's
running I'm going to click on this link
that says spark UI I'm going to come
over to jobs and what I'm going to do
first is look at the job I just ran
that's this one over here job number
seven and what I want to do is take a
look down where it says shuffle read and
shuffle right so in the efficient
version of the query or at least the one
that I wrote pretty well we're moving
about 15k while 14k through this shuffle
between the stages now if I flip back
and run this less efficient version of
the same query right we get you know
nice answer here but if we come back to
the jobs UI and if I click on job 9 and
scroll down notice that we're moving
several hundred K through the shuffle so
we're up by you know somewhere between
one and two orders of magnitude in terms
of the quantity of data that we're
making spark process and that means that
as our job scales up things are going to
take a lot more resources and a lot
longer to do
the exact same job so in other words
with the RDD API the burden is on you as
a developer to understand this and to
get it right
and if you get it wrong even if things
work they may work a lot slower okay you
can see I just keyed in a little label
here that says on two data frame and
data set details so we'll switch over
first let's pick up something small
let's cache this table we don't really
need to cache it but it's always handy
to have it in memory since it doesn't
take up much space so notice that when I
run a sequel cache table command that it
actually cached right away it actually
did some work there this is interesting
because typically caching inspark when
you go through the programmatic API is
as lazy your data doesn't actually get
cached until it gets touched by some
subsequent action but when I use sequel
and I say cache tables if it's cached
right away how can we tell let's flip
over to the storage UI and see if it's
there and here we can see it in memory
table zip and we can see that it's
actually pretty small it fits in memory
and it's only about 2 2 megabytes here
so nothing too exciting but it's all
cached in memory from doing that cache
table directive so once we're agreed
that we don't want to be using rdd's
which are really kind of an internal API
going forward what about data sets and
data frames well what I want to explain
here is they're really the same thing
they've always been intended to be the
same thing and now that we have SPARC
2.0 they really are so how can we tell
well right here we can do a little check
in Scala is class of data frame the same
as the class of data set so if I run
this you can see that it comes back true
and if you want to see where that lives
in the code I'm going to flip over to
this tab and I'm going to scroll down
this is from the sequel package and the
SPARC code base on github as of today
right down here on line 45 we define
data frame to be data set of row and
that allows backward compatibility we
can talk about data frames but they
really are the same thing as data set
and the reason why that's important is
that we want to
data sets and data frames being very
similar so the differences are much less
important than the similarities we want
to program them the same way as much as
we can so data sets offer this strong
typing instead of just having rows we
can have strongly typed Scala objects in
there but the first thing to realize is
this typing is a virtual it's like a
view it's applied when we ask for it but
the underlying data is always stored in
a binary language neutral encoding will
demonstrate that later
so when we talk about having a data set
and making it be one type or another
we're not actually doing any processing
and we're not actually converting
anything we're just treating it as a
certain type when we want to use that in
our code so here's a really quick demo
the traditional sequel context out range
call that used to produce a data frame
if I run it now it produces a data set
notice that I get a data set of long the
schema has a column called ID which is a
long just like we used to have with data
frame and notice that over here it says
ID big int all right so that's the
actual data type underneath I can take
that data set and I can say I'd like to
treat this as a data set of strings so
if I run this cell I get a data set of
string and again on the right it's still
big int so note the Scala type changes
but the underlying schema stays the same
what is this big int well the underlying
types here are language neutral database
style types so think about the kind of
types that you have in a sequel database
or in hive things like big int and
string those are the universal
underlying types and they're encoded in
a binary format if we wanted to go back
to a data frame we can call 2d F on a
data set and that just gets us rows so
instead of having a data for a data set
of the original type we'll get a data
set of row and you can see that here so
here I've got when I do a collect I get
an array of row objects that have these
numbers in them so language neutral what
does that mean well I'm not going to
demonstrate this but with an RDD if I
cash it in Scala and then come in say
through Python and use the spark context
of
find the persistent rdd's pull back some
data that will actually work but the
data I'm looking at is going to be
encoded objects from whatever language
created the RTD so that would mean it's
either going to be very difficult to use
or I'll need to somehow decode this
myself I'm sure it's possible it is
certainly not easy in this case we don't
have to do any of that if I take this
little list of integers here and I say
create or replace temp you numbers so
I'm making a little table called numbers
and now I'm going to come into Python
so here's Python code I'm going to look
up this table by name and I'm going to
do a print schema on it so notice that
it does show up the same way over here
in Python and I can actually read the
contents out in Python so I'm going to
go sequel context table numbers I'm
going to collect that data back to the
driver and python and we're going to
print it out using regular Python code
and here are my numbers right so Scala
Python our sequel it doesn't matter
the data is all language neutral and we
can access it in whatever way is most
convenient for whichever business group
needs to do that so okay so how would
this work with a more complicated type
so there's you know we can have a data
set of strings or Long's or something
but how does this work with a more
complex class so here's an example
remember the class that we're using is
purely on the view side right it gets
mapped nicely against the data but
neither one depends on the other so
recall that we loaded this data from
Mongo it was just some JSON documents as
relatively flat schema it's got a little
bit of nesting where the latitude and
longitude are if I define a case class
so here's a scala case class called zip
it's got the fields with the same names
zip city location which is an array of
doubles pop and state and notice that
inside this case class I've gone ahead
and defined some pretend complex
business logic I've specified a latitude
and longitude for Chicago and I've
defined a business method called near
Chicago which uses some math to
basically find things that are in kind
of an ellipsoid near the center of
Chicago
so this isn't terribly complicated but
in a lot of business domains you may
have methods that you want to use that
are either very complicated they've been
you know Qaid and you know built and
approved for a long time you want to
reuse those in some businesses there's
regulatory restrictions you may have
code that you know needs to be approved
by you know the FAA or the FDA or
something like that and you can't just
go and write a similar operation in
spark and you know put it into
production so if you have code that you
need to use as is or you'd like to use
as is that's one of the places where
datasets really shine because if I have
a data set of zip objects I can use the
methods on zip just like any other
method so let's take a look at an
example first I'm going to evaluate the
cell that defines the zip case class and
then I'm going to take my table called
zip and I'm going to go dot as and then
the type zip and I'll pull back the
first record so we can just see what
that looks like and you can see that
when we get back is a Scala zip class
instance with the data inside so here's
the zip code here's the name of the city
here's the population here is the state
what is this mess in the middle that's
actually a stringify durjan of the array
of double precision floats we can look
at that if we want in this next cell so
I'm going to do the same thing I'm going
to pull out the first record and I'm
going to pull out the dot lok member and
here you can see it's an array of double
and they're the actual coordinates in
there so okay so how can we use this
data set object to do some analytics
well remember I said the data sets are
really the same as data frames so don't
think of them as being object
collections like an RDD think of them as
being just another way of dealing with a
table although you know maybe it will
accommodate more complex tables or
documents so here I'm taking the data
set I'm saying group by state count and
then order by state so I'm just looking
to see which states have which states
have how many records for different ZIP
codes in there and I'm calling dot Show
so what we can see is that
Alaska has 195 Alabama has 567 and so on
might be more interesting if we ordered
by count we can see which states have
the most zip codes with the fewest but
this is showing how we can use a data
set so we're going to use this just like
a data frame now if I do want to use my
complex business logic and I put complex
and quotes here but you may have more
sophisticated code in your company you
need to use here I'm going to read in
that same file and I'm going to say dot
filter and I can filter on a column
expression just like I can with a data
frame but with data set I can also
filter on a function Oh lambda and I can
just use the method that's defined in
this zip object so here is my data set
filtered for ZIP codes near Chicago and
here's kind of a just a first 20 that
come up in the list so using existing
domain logic that you may have invested
years in building is as simple as
plugging them right into certain API is
in the data set just like this
highlighted section right here
so we just looked at how we can use data
sets together with some custom business
logic and I want to talk for a second
about the query optimization if you want
to know a lot of details about the query
optimization pipeline and spark which is
called catalyst there's some fantastic
talks by some of the folks who have
built it including Michael Armbrust from
spark summit so and in our three day
spark classes we actually go into detail
about how this optimization pipeline
works what I'm going to do today though
is just give you a quick demo if you
wanted to actually see with your own
hands and eyes how this thing that it
actually does optimization I've got a
query or two that we can take a look at
so down here we're going to take the
same data set we're going to find a
population greater than 10,000 we're
going to lowercase the city just as an
example of some operation you might be
doing in your logic so this lowercase is
a built-in function you might be using
some other function here we're going to
project that out and then we're going to
filter for state
equals Rhode Island so recall that when
we did the RDD version we said that
pretty much what you write is what spark
is going to execute so the pipelined
operations ran in the same order that we
coded them in and that turned out to be
a bad order in one of the queries and a
good order in the other and we could see
that spark ran what I wrote here we want
to see whether spark will do something a
little bit different so if I do a
collect here we can see that the bigger
cities in Rhode Island we've got maybe
you know twenty or thirty of these this
is what the data looks like let's go and
see how spark is going to execute this
and we can do that by calling explain so
this is kind of like asking for a plan
on a sequel database we can say explain
true and that will get us the detailed
execution plan so I'm going to scroll
down here and we'll take a look see if I
can expand this so that we can see the
whole thing so here are the four stages
of plan we've got the parsed logical
plan that's pretty much what I typed in
it's just Sparks saying okay I
understand what you keyed in here then
we have analyzed logical plan that's the
parsed logical plan combined with
catalog information so combined with the
names of the data sets and the types and
the columns then we have optimized
logical plan so notice the difference
between the analyzed logical plan and
the optimized logical plan so in the
analyzed logical plan we start out with
this project and we alias it and then
we're filtering on the population and
then we're projecting the city and the
lowercase version of the city and then
we're filtering on the state and then
we're pulling out the lowercase version
of the city because that's the only
field that we asked for at the end so we
have a lot of operators here and they're
just in the order that I keyed them in
they're not in the optimal order down
here under optimized logical plan notice
that spark has applied a number of rules
to simplify this query
so first thing that we're doing is we've
got a filter operation here and we've
moved the filter before any of the
projections and we've also combined the
two filters so take a look in here we
have state Rhode Island and population
greater than 10,000 so we've moved the
filters earlier in the query and we've
combined them together and then after
that all we have to do is pull out the
lowercase version of the city so you can
see here that it's easy to ask SPARC
what the plan is actually going to run
like and you can see the difference
between the query that you punched in
and the final one that SPARC is
optimized and so unlike with rdd's I
don't have to write the perfect query
every time and in fact a lot of
different queries I can write in
whatever order is most natural or most
readable for my team and it'll end up
being produced in an optimal way by the
SPARC catalyst engine so notice these
stars over here in the physical plan so
these are connected to a brand new
feature in SPARC 2.0 called whole stage
code gen so you'll be hearing a lot
about whole stage code gen over the next
few weeks and the idea is to take all of
the pipelined operations between
shuffles that's what makes up a stage
and SPARC and actually instead of
composing those operators and running
data through using an iterator it's like
a cursor type approach instead to
generate a block of code that represents
that computation as if that was the
entire thing that you were trying to do
and then compile that code and execute
that code against the source data so to
take a look at whole stage code Jen
first thing I'm going to do is take our
query where we converted the data set to
zip codes and filtered for near Chicago
and I'm going to ask SPARC to do and
explain on that if we scroll down and
take a look at this plan so down here
near the bottom we can see that whole
stage code Jen
is turned on and the function that we
wrote our near Chicago function that
shows up right here where it says filter
function 1 dot apply so our custom code
has actually been integrated right into
the pipe line of code that's going to
get whole stage generated and then
compiled and this helps a lot because it
means that spark isn't doing one set of
work from compiled code then separating
out our work to process near Chicago and
then going back to some other optimized
code it's all going to be in line and if
you want to take a peek behind the
curtain and really see what this code
looks like you can do that by importing
this particular code right here so if we
bring in orga pachi spark sequel
execution debug and then we take the
exact same query and at the end where we
had explained we say debug code gen if
we run this cell here this shows the
actual code that's been generated for
this query so it's Java code and it's
machine generated so I don't expect this
to make a lot of sense the first time
you see it you know like all machine
generated code it has two interesting
properties one is it's very consistent
and two is it can be pretty hard to read
for a human the first time you look at
it but if you aren't interested and you
spend a little bit of time with this
code you can actually see how our
function application is integrated
directly into the rest of the code that
represents this query if we come all the
way down to line 83 we can see that
that's where we're getting our filter so
filter value 1 dot apply and if we
scroll back up we can see what that is
here we can see on line 55 the filter
value 1 is a Scala function and it's a
caste applied to this filter object that
we fed in
so our code that was originally our
business domain code about interesting
customers or conditions near Chicago is
actually getting compiled directly into
the code that SPARC is executing for
this stage so that's all that we're
going to talk about as far as whole
stage code gen there'll be a lot of
other materials online from SPARC summit
that talked about whole stage code gen
in terms of performance and other
details but I wanted to give you a
little flavor of how you can see that
that is actually happening and how you
can even take a look at the code that
SPARC is emitting a couple more things
that we want to make sure that we talk
about why you would want to be using
data frame and data set so you get hive
integration for free a lot of companies
have a big investment in hive and
related tools and SPARC will
automatically work with your hive meta
store so your hive tables are available
in SPARC and you can take data and spark
and make them available as hive tables
really easily so just to give a quick
example we can take our zip table right
here and say dot write dot save as table
hive underscore zip and that succeeds
and then the question is well where did
it go and how do I know that it's
available to hive well if I look under
the tables tab here in data bricks I can
see the hive zip table that's showing me
the hive meta store but what if you're
not on data bricks or you don't believe
me let's take a look a little bit
further down here's the access to the
zip table using the sparks session that
we mentioned earlier
so in spark 2.0 we can say spark table
so given that sparks session we can ask
for the catalogue and the databases that
spark knows about and we can see where
the hive data actually lives so I'm
going to do a spark catalog list
databases and you can see we get back a
data set that says ok here's the default
hive database and here is the hive
warehouse so the default warehouse
location in data bricks in this version
of data bricks is under user hive
warehouse in your installation of spark
you just configure this to match where
every year hive installation is and if
we want to see that our data actually
lives under there we can do an LS and
look under user hive warehouse hive
underscore zip which is the name I gave
to that table that we just wrote out and
here you can see there's parque data
underneath that folder so the data
actually has been moved into the hive
warehouse so that almost concludes our
tour of data frame and data set so kind
of wrapping up and looking to the future
all of the new development or nearly all
the new development in terms of new
api's and optimizations on spark are
focused on data frame and data set so
you get the benefit of all of that
research and implementation when you're
using these api's if you're using if you
doing machine learning there's spark ml
or the ml pipeline's API
you probably heard quite a lot about
over the past year here's a link to the
docs
more recently graph frames so there used
to be an API called graph X there still
is that's the traditional API for graph
processing over rdd's well if we want
the performance and storage advantages
of data frames and data set what we're
going to do is use graph frames which is
an implementation of graph algorithms
over data frames here is a link to a
really cool demo that features a
analysis of passing among the Golden
State Warriors where they use something
like Google page ranks algorithm to look
at who are the key players on the
Warriors you can probably guess who's
getting the ball the most and the last
thing I want to show so we've said we've
we're going to use data frames as the
underlying infrastructure for moving
around our data we're going to do spark
ml to do machine language machine
learning on top of that we're going to
use graph frames to do graph analysis on
top of data frames and the newest thing
is structured streaming so you may have
seen some talks about building streaming
directly on top of data frames and the
early iteration of that is out in spark
2.0 and I've put together a really quick
demo of just kind of a hello world of
structured streaming so you can actually
see what this looks like
so instead of using the traditional
streaming API that's focused around our
DDS and D streams what we're going to do
is we're going to create something
called a continuous query that reads
data from some streaming source does
some processing on it and that data is
going to flow out somewhere else and we
can do whatever queries we'd like on the
live stream so let's just take a look at
a fun demo that we can use to start with
what I'm going to do here is prepare
some folders I'm going to use file
sources and syncs for this demonstration
so I've got a folder stream in where I'm
going to put some data into my stream
there might be transactions or something
like that coming in then I've got stream
out this is where I'm going to write my
data to and in order to support fault
recovery we're going to set up
checkpointing under a folder called
stream /ck I'm also going to define a
schema so that spark knows the shape of
the data before the records actually
start coming in and each one's going to
have a last name which is a string and
email which is a string and a number of
hits like in a web analytic scenario for
example number of hits which is an
integer so I'll go ahead and run this
okay so now we get down to the fun part
let's start our stream up first we're
going to set up the source spark dot
read format JSON schema and we pass the
schema we created and we're going to say
dot stream and I'm going to point at
this input stream so this particular
path is dbfs that's data that actually
lives on s3 that's attached here through
data bricks so I'm going to run that and
notice that I get back a data frame
object and now I'm going to define the
processing we're going to do on this
stream and where we're going to write it
to so the modified version of this data
is defined right here I'm going to take
the data that comes through I'm going to
pull out the last name I'm going to
lowercase the email it's kind of a
normalization maybe and assign that to
have the name email I'm going to get the
current timestamp so the time that we're
processing the event I'm going to call
that now and I'm going to grab the hits
number of hits that came in then I'm
going to take this modify data and I'm
going to say right and I pass the
checkpoint location and then I say dot
start stream and I point to the output
folder so let's go ahead and run that
and you can see right here in data
bricks we can visually see that
something is running it's a stream query
0 status active if I open this
disclosure triangle here it says
awaiting data so we'll see if we can
feed some data in here and we'll see
what happens so in this next cell I'm
using echo in a shell command to
actually take two jason records and drop
them into a file in the temp directory
so we'll do that that's the temp
directory not the actual streaming
directory and then i'm going to move
this file into the streaming input
directory think about this as being like
when you do log rotation right you've
finished creating a file that contains
some records or events and then you
rotate over to a new file name or maybe
move the existing log file to another
location so I'm going to toss this file
right into the stream and we should see
some data showing up in fact you can see
right here it just changed the UI under
stream says sources one it shows some
details about the source and the last
offset and it shows the sink and we can
actually query the output which is a
park a file here so here are the records
that came through so so far we just have
one set of records so what happens if we
wanted to do some analysis on here or
send in some more data so if I wanted to
say roll-up the total number of hits by
email so far we just have three for
Jones and five for Smith so what happens
when this next event happens where Smith
sends in another 12 hits so we'll do the
same thing where you echo to a file
we're going to rotate that file here
into dbfs stream in
and now we're going to run the query the
same one we did before and you can see
that Smith has 17 hits right so we've
gotten another record we did the same
query we did before and now we're seeing
live updating data if we scroll back up
we can see that the offset has moved in
the stream so now it says last offset
number 1 and if I want to shut the
stream down I'm going to come down here
and go stream dot stop so this is just a
very simple hello world of structured
streaming but this is the new streaming
API that you can use directly with data
frames its shipping in spark 2.0 right
now probably the easiest way to try out
spark 2.0 is through data bricks
Community Edition so you can sign up for
a free account and run this exact code
that we've been playing with
we'll go publish this online so you can
import the same notebook and try it or
you can download the open-source
spark project code and build it yourself
from spark apache org hopefully this
helps clarify why data frame and data
set are the future of programming with
apache spark you get a little look under
the covers here if you want to learn
more there's great material online from
spark summit and if you want to learn to
do spark programming yourself new circle
has public classes and for teams of five
or more we can come on-site and do an
in-depth spark programming class thanks
for watching
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>