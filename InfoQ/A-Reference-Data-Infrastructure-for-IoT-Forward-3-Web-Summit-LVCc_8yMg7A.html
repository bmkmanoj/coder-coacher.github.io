<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Reference Data Infrastructure for IoT - Forward 3 Web Summit | Coder Coacher - Coaching Coders</title><meta content="A Reference Data Infrastructure for IoT - Forward 3 Web Summit - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Reference Data Infrastructure for IoT - Forward 3 Web Summit</b></h2><h5 class="post__date">2015-08-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LVCc_8yMg7A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right guys good afternoon hopefully
you guys can hear me okay my name is sue
G Money am sorry took us a while to get
the females to get started but we are
here so this talk is a little different
for the pointer for the four J's
conference this is about sort of
building a platform for internal things
so my background is more from a big data
background in my previous life I was a
web developer doing ruby-on-rails
anybody anymore ruby-on-rails okay so
still alive I guess okay yeah so and
then I then I saw this the just a little
tingle I do and I kind of you know just
you know it's kind of salad using it and
then since then you know we have a
company called elephant scale which we
specialize in big data stuff can owe me
now so the first thing we did is like me
wrote a little book on Hadoop called
Hadoop eliminated it's open source which
is kind of like a novel idea at the time
so you can just search for the term how
to eliminate it and download the book
anyway so that's what the marketing
message there is let's move into the
presentation so internal things right so
when you say this word Internet of
Things I usually see people's eyes roll
over and say our God you know not
another power point about IOT right but
the reality is we are pretty much living
in the age of IOT you know I have one of
those Fitbit devices I was just in
Washington you know just yesterday
walking around and when i hit my my
daily limit within like 10 seconds i got
a text message on my phone that's saying
hey congrats you know you did your daily
thing and you know you're on the badge
and all within within like I you know a
few seconds so to me that is IOT right
so this little device sing to my phone
the phone sensor data to you know
somewhere in the cloud and there they
did the the convention numbers and you
know and decided you know you know I you
know I married a badge I had always in
like in a tenth of seconds so yeah so we
are pretty much in the age of IOT so
let's see you are developing the next
Fitbit or the next devicenet sent next
sensor so that's your hardware platform
you know your little device could be a
could it be a sensor and then you really
want to empower the users with all this
cool stuff right nice dashboards alerts
sometimes my feed rule if it will come
and say hey just you know another 2,000
extra steps and you know you meet your
daily goal right stuff like that so what
kind of infrastructure do you need in
the middle to go from you know a plane
wearable to you know a pretty nice user
experience that's what this talk is all
about kind of mean I wanna give you some
ideas off if you are planning on
building this how would you go about
this again like I said this is a little
different team than the sort of four
days but you know it's hopefully it's
interesting to you guys alright so
before we cannot build this let's go and
figure out how much data we are talking
about so I did this and I said hey you
know what let's say I have million
devices whatever and each sensor data
like every minute which is not not
uncommon and you know the data packet
eyes 1k sir now if i have this i did the
calculation and i came out to like you
know 1 1,000,000,000 events per day and
i used to like a little calculator on my
on my mac to use this and also like oh
no this can't be right right so i kind
of fired up Excel function the numbers
again and I said oh yeah I still you
know yeah I'm going to get 1 billion
events a day then I called my co-founder
and then said hey can you check this
math because you know you know I'm
plugging these numbers and I'm getting 1
billion events is it the right and it is
right so this is the amount of data you
know us better deal with like billions
of events per day terabytes of data per
day so this is clearly in Big Data
territory right this is a similar
project we are working for a client of
ours in Texas they have smart meters so
if you look at the data size next one
yeah right so you can see they have
about you know 10 million meters right
appear and each smart meter reports
about you know 15 minutes and yeah so
they pretty much have about a billion
events a day so yeah so you know we are
pretty much in the same ballpark so this
clearly takes us to a big data territory
meaning you know imagine terabytes of
data billions of events
right this is not something like you
know we can be with a little my sequel
right we need something larger so this
is where we know it starting about Big
Data technologies so and also it kind of
velocity also indicates like you know
events comes at you at very high speed
like thousands of events per second so I
imagine like you're doing a web
application and being able to like save
thousands of records you know per second
right and so that's the kind of volume
we are talking about alright so let's
going to take some of the challenges the
challenge in the IOT world is you need
to process events in real time or near
real time all right so people ask me so
what is the real time but in the adult
im so real time for me is milliseconds
right so you know you make a call to a
database comes back in milliseconds
right maybe like a single-digit
millisecond of a fraction of a
millisecond milliseconds nearly all time
is seconds something happens i click on
the web in a link five seconds later i
see a page yeah acceptable right the
reason is imagine i'm doing a sensor
data let you I'm wondering temperature
sensors and suddenly the temperatures in
temperature spikes I better send an
alert to somebody to say hey you know
I'm noticing any increase of temperature
in the sensor so our response time
should be in seconds I can't do this in
30 minutes and you know and then say you
know yeah that's good enough right yet
dress code in seconds so seconds to me
is somewhere between real-time and near
real-time the the challenge is I cannot
lose any events right because let's say
is it example of sensors right most of
the time I sensor data is you know then
within the normal you know let's say
between 60 and 70 degrees which is good
sometimes you know we get this you know
extreme extreme fluctuation and we need
to report those so the idea is the so
the so the theme is most even someone
Dane right they're like yeah just within
a normal parameter but any event could
be important so that's why you cannot
lose any event right so you're the
process all the events no events can
fall through the crack so if you're sort
of sketching on a back of a nap in so
okay hey i want to sort in a build this
infrastructure what does it look like so
this is kind of what i came up with so i
can i have four different parts I'm
start with capture capturing the data
processing the data storing it and then
querying it writes a very very high
level these are kind of my four
components and we are going to sort of
see what we are going to use to below
these four components so let's talk
about capture so camp sure is nice we
saw my events are coming at high speed
you know thousands of ins per second as
possible and I cannot lose a single
event because you know any event could
be important and so the whatever I want
to capture I want I want some sort of
like a queue system or like a buffer
system and I want to make sure that
scales easily right so these are some of
the choices I mean some of them you know
we'll look family like you know in my
you know all days you know visually use
mq pretty heavily it's still pretty
popular platform in the Big Data
territory there are like you know this
new tools came about fluent flume and
kinases is a queue on amazon amazon AWS
what I want to focus on something called
Kafka not kind of quick good show hands
anybody kind of using Kafka can evaluate
in Kafka oh yeah few hands right yep so
a cough guys are kind of in a pretty
pretty good pairing with streaming
streaming stream processing so the wake
of course cover came out of LinkedIn you
can see the numbers here and you know
and I'm sure stable process light so i'm
not gonna spend too much time going
through each bullet point it's done you
know it scares pretty well right you can
see the numbers like billions of a
minute of the day so this is perfect so
the kind of way how Kafka work is you
think about like a distributed queue or
like a in a simplistic producer-consumer
thing right so producer is basically
producers produce logs they get into a
cluster and on the other hand consumers
pull data so multiple consumers can look
at in a multiple event so you know it's
kind of a many-to-many right so you can
subscribe to the events you want the
cool thing is within Kafka is distribute
as a cluster so the entire cluster is
dispute across multiple machines so you
guys can see here right I have multiple
brokers or between nodes have multiple
nodes so when even comes in its
replicated in two nodes by default yeah
I me know and we can change this so this
way you know even if I lose a node my
events are safe right so this kind of
satisfy the first requirement I cannot
lose any events my events are safe
because they are replicated across
multiple nodes
consumers can get to it and you know
cough class you know out of LinkedIn you
know it's pretty popular all around so
yeah the kafka you know we propose Kafka
as a capture mechanism for capturing
events in real time very good track
we're processing so now the night my
events I in sort of this sort of a queue
or a buffer right I can't just leave
them there right it's not a database
it's just a temporary caption mechanism
I need to grab them and process them and
I need to get to them you know in real
time or nearly all time at least so what
I mean stream processing you will hear
is each event has to be processed I mean
I cannot miss an event right we
established that so how do you do that
the way we can do that is one of the
guarantee is like each event will be
processed at least once or the other
guarantee is each event will be
processed exactly once so there's a
subtle difference but at least once
basically means yes you know even still
get even events would be handle but an
event could be handled multiple times
right even could be processed multiple
times and and most of them that's okay
exactly once is like you know meaning
like an event will be a process exactly
one time so no duplicate processing so
the at least one semantics is a little
easier to achieve then exactly once at
least systems we are building right now
the at least one semantics is you know
pretty readily available meaning even if
a process any when you know multiple
times we assume that's okay meaning you
know when for our purpose we assume that
that's fine exactly once takes a little
bit more work right to make sure each
event we only process even once at once
alright sir so there are there are
several choices right strong was the
original stream processing engine again
came out of Twitter and I was an open
source project Sam's you know so these
are like a knife I and there's another
one called fling so these are like names
you know I'm sure you'll hear in
readings and blocks however the project
i want to focus on is called spark
streaming anybody can I using Spock here
join good spot co a few hands yeah so
spark is sort of the you know so they
know that the red hot thing in big data
I came you know when you into big data
we use our
do so now Spock is the heart of thing so
people ask me all the time all right
what's the different between spark and
Hadoop right and if it seemed is that
I'm sure most of us hair right i'm a pc
i'm a mac a dry time most of us f right
and this is my interpretation of Hadoop
and spa right so yeah how does this
match over it works we all know it but
you know spark is sort of the next cool
thing right so the way a spark a
structure it is you can sort of see the
spark as a base in a generic engine with
multiple components assembled on top and
the cool thing is they're all very
tightly integrated so sparks swimming is
the one that's in you know in terms of
how everything is streaming I can do
some graft Ross Lane I can apply some
machine learning library right here
right so everything is very tightly
integrated so i don't have to sort of do
something here and then go you know save
the results and then pick up the result
and save something it right because i
can do the whole thing as part of the
flow and also you know so we can sort of
see you can run spark as a cluster and
these are basically the cluster managers
for example yarn is a cluster manager of
a hadoop so if you have Hadoop cluster
install you can just deploy squawk on
top of it and and may sources another
cluster manager that came out of
Berkeley so kind of a you know kind of
describes good I know what it is
components are the way spark also is
pretty good for us is it sort of
abstract the storage so here you see
spark as a little thing all our DD which
is sort of a sports way of specifying
data sets and the rd DS you can read
from any of these different oddities so
for example i can read my data from HDFS
is a hydrofoil system y can be from say
amazon s3 I can't read from my Cassandra
doesn't matter one fin spark they all
look the same so this is nice data
abstraction to me it's pretty great
because you know I don't really care
where my data it's going to store right
again so you know my streaming data I
could come in here I can you know save
it to yesterday i can say with the
Cassandra and you know if I music in
other database I can you save it there
right as long as have an adapter for it
from spark I can read and write to it so
this is kind of the reason why a lot of
people like Spock is sort of a unified
stack you can do batch processing you
can do streaming you can do the machine
learning you can do crafts graph
processing everything in like in a one
my system and the cool thing is parked
out of the box supports three languages
java python and scala so with these
three languages you know that pretty
much covers it cuz of pretty wide
spectrum right because one of the
challenges with Hadoop is since Hadoop
is in Java you can let's say your data
scientist who know python now you come
to Hadoop well you can how to either
like learn Java libraries or you know
come up with something else it's park
you're right at home because you know
it's but supports that I'm Python out of
the box so let's look at sports
streaming a little more detail so you
can see my sources come from you know
multiple sources or the one we are
talking about scoff car but spur can
support lots of other other sources you
do some processing and then we save the
result to a database or a file system
whatever in Howard heavy right streaming
is pretty popular is very well
established it's a lot of people go to
spot for streaming and you can see you
know although you know all the big big
players are using streaming cool so that
kind of settles it so we say you know
what for processing we are going to go
with spark streaming next thing is
storage so storage so when evan is
choose a storage you know i mean i know
so we asked me so earlier we are
definitely in a big data territory
meaning we i'm getting back terabytes of
data a day as possible of billions of
events so this is basically beyond the
realm of you know our typical databases
right so what do we need is we need
something we need two kinds of storage
one is real time lookup meaning i want
to go look up what's the latest
temperature for a sensor right and
that's a millisecond query i want and
the result i want to come back in
milliseconds oh I want to sort of you
know maybe I want sort of do some
analytics off in this zip code what's
the average temperature in May so I'm
going to look at like you know thousands
of sensors you know in some sort of a
certain time window that doesn't need to
be a real temporary that can be a batch
query so whatever the storage and I'm
choosing has a support bulb so looking
at this total storage tiers so I mean
also served my sequel so we look at the
chart is like data slide stuff from say
gigabytes here going all the way to say
petabytes and the vertical axis I have
what we have a real time sorry access
time so can equate them a real time and
all the way the badge alright so kind of
my sequel to see night this is the
territory you know I know we all grow up
in right you know
runnable till the web application i have
a few gigs of data great you know go to
my sequel it's great you know I can
clear the data install the data and
create them in milliseconds if we won't
slide the most scale we will probably in
the Mongo or couch and if you really
want in a scale a terabyte scale we
probably went to someone no sequels
Cassandra HBase right so the reason I
had a high loop to two things is I do
MapReduce if you know if you use Hadoop
it can scale to petabytes of data in
size but it's a batch processing system
right so it's great because in a minion
board we need badge and real time
alright so for our for our storage
building our durable storage meaning
this is going to store every event I've
seen the whole universe is basically
what we want to go is basically I do
Vanessa Hadoop I'm in HDFS meaning
Hadoop file system HDFS can we is proven
to scale to petabyte scale it's you know
it's built to handle machine failures so
I go so quick quitting right here and it
runs on commodity hardware meaning it
runs on a cluster but you don't need to
go buy special hardware you can just
just tip your Linux machines it's fine
the way it sort of makes your data is
not lost is say for example let's say I
copy a file a it makes multiple copies
we call them replicas so you can see a
is replicated like three times which is
the default so every time you copy a
file the file is actually replicate a
three times so even if you let's say I
lose node number three that's fine
because I still have three you know two
more copies of the data so that's how
HDFS make sures even when you're running
on a cluster if you lose machines you
don't lose data right so that's HDFS
very quickly and for anyway another
thing we want to choose HDFS is is very
cost-effective so this is this is a
slide I could from houghton works which
is a hard hard winter so you can sort of
see a cost of storing one terabyte of
data right so you can sort of start with
like expenses like sands Nass right all
these guys about in a 20,000 plus per
terabyte how do you can see right here
is about in a couple of thousand so it's
very cost effective to store large
amount of data all right so another
thing on Hadoop is since it stores
everything and once you write a file in
hydro Buchan
modify it so it's very ordered friendly
so once you want you once you dump data
into HDFS that's it is sort of the sort
of the sort of a source of truth right
I'll sort of skip this because you know
come in slides later so then so that we
settle the long-term storage we just had
your HDFS that's great we got it is a
batch analytic system we can throw dat
ax in it very very cost efficient how
about real-time access you know so i can
do like I cannot start with like a are
dbms because it's just the volume of
data is just too great so set up for
this purpose we are going to go to a no
sequel engine and there are tons of no
sequel engines so the joke I tell
everybody is like every month there's a
new no sequel database comes on the
market and when I asked them where did
you get to do this tell ya we evaluate
everything there is nothing kind of work
the way we want to do so we can roll
along right so no seagull is like a you
know very hot hot space right now so the
three popular ones I HP is Cassandra and
accumulo pretty much the same same deal
right cassandra is probably a lil more
popular with spark so okay so for
storage we we sorta we know we realize
yep we are going to use a combination of
storage 14 HDFS for long-term storage
that is throws everything there is and
for real time we are going to show like
a no sequel so maybe the not mind you we
all time storm maybe only stored it I
like in our last one week of data last
one month of data right just tiny
fraction of the data the reason is I
want to query 11 real time all right
finally they come to query so queer is
right there are two kinds of queries
like you know this is kind of why I hide
it's a couple of those so I have a
mobile lab I want to know see what's the
current Cambridge in my home right now
we all turn put right you know you fire
up the app within two seconds you want
you won't see the latest temperature and
this queries has to go to real-time
store and then then you know that more
like data science Aquarius they say hey
you know let's go and sort of analyze
you know what was the medium temperature
sensor intermitted thermostat setting in
san francisco in june right so these are
like you go through lots of data no no
pencil real time so you know even if
they take 30 minutes one hour to compute
that's fine right we don't
so there may be instructions query is my
real time queries are going to go to the
no sequel engine because no c curves can
come back within like either you know
milliseconds or like you know a fraction
of a millisecond and that's what we want
for our like a you know dashboards and
apps like this and for batch queries
meaning like you know what's the average
temperature of San Francisco in June
right I kangaroo HDFS I don't have to go
to know seeker because this query
doesn't need to be real time so cool so
we can kind of highlight you know
satisfied both query mechanisms real
time and batch queries so this is what
we have so kind of in a starting at the
very high level we start over the
capture right we are we using Kafka to
capture the data immediately and then
from Kafka we are pulling events using
spark and then the data is stored in
HDFS and no sequel sort of a hybrid
storage and then final level queries
depending on the query we go to the
right right storage so this kind of
brings me to what we call lambda
architecture so lambda architecture
again came out of Twitter because they
had this problem of the real lots of
real-time events and also they had to
store them for a long period of time to
do batch queries so the veil am dr. kind
of works is when it even comes in new
data comes in it's sent to two layers so
this is my master master date as it
meaning like this contains everything
right and the other one is a real-time
store this may only contain like the
recent data maybe last one week last one
month in a way now but whatever that
works for us sir so the data data is
sent to both places at the same time
bachelor and the real time player and
then the queries depending on the query
for example show me the last into eats
my queries go here right because that's
a real time query doing a query like
what are the trending hashtags for the
last month last week those are batch
queries you know they can go here so you
just assume sort of see that way and
with data kind of goes in both places
and you know they can serve different
queries 11 more like a batch queries one
real time varies and and some of the
queries like for example this one you
can see some of its going to like real
time and badge some queries will come in
you had a sort of
some batch query the real-time query and
probably moister results and that's fine
too right but at least now we have a
mechanism to handle the queries so
taking a model into sort of a lambda
architecture of Magnum turning into a
lambda architecture so this is going to
have envisioned this so data comes in
data gets captured by the kafka cluster
and my sports streaming pools events out
of kafka process them right and then and
then stores the data in both places hdfs
is my my master data set and then also
no sequel for real-time data set right
and my queries my dashboard so mobile
apps but we know what have you they will
either come here for real temporaries go
here for batch queries so this kind of
design without a turnover design but we
came up with into a lambda architecture
so the cool thing about the design is
each component right i mean if you look
at each of these components like silikov
car park how do you know sequel each of
these scalable independently so for
example you can have like a three node
car cluster right and a five notes park
and the ten note I do like each
component is individually scalable
that's a beauty of the whole the design
is you don't have to kind of go and you
know bulk purchase you know like hundred
machines to make this work you can scale
each component individually and they
will open source right meaning you know
you know so they cause nothing and
they've been they've been around for a
long time so they are very well field
tested so having said this was another
thing of you are done right um it's like
yeah cool right VV sort of designed this
you know this pretty cool architecture
for real-time events right another thing
we haven't talked about insecurity and
and this is a huge topic by its own so I
know I'm not going to talk about this in
this one but i just want to mention this
you know in the oldest design make sure
security should be paramore right make
sure each event has you know access
restrictions depending on your use case
right so again we not gonna talk about
this here but um it's an important
factor things so another things i like
to leave you guys where it is and we see
this all the time so I throw in big data
where we see this all the time somebody
will go and you know grab a new piece of
software running on the laptop and
hey I go to working in two hours right
so let's go production the cautionary
tale vc is you know we're learning to
fly right flank like a single-engine
plane is one thing but playing like a
747 is completely another right so at
scale nothing works as advertised so
that's a joke we say no that's like a
foul point version that I'm showing you
guys and this reality which is you know
very different right so so the way I we
sort of say is like you know even many a
product prototyping in our small-scale
make sure you test all the components so
you know you when you when you go to
production you're not getting any
bottlenecks one of the things i will
mention and quickly close it up here
almost I'm you know coming up to the
break is so what you want to do is
rather than you know EJ was building his
own thing on our own we are launching a
new project called io TX so you can I
want to build this opens all right the
whole stack as open source and then so
this you know people can just take it
and you know just start using it so our
background is big data you know we've
been working with all these components
you know for a number of years so we
think you know we can we can do a pretty
good job coming up with the an open flow
solution just you know just assemble the
basic basic you know basic
infrastructure so no you don't have to
start from zero right it may not do like
hundred percent of what you want but at
least like it'll do like forty percent
of what you want so Bridget which is
which is a good thing all right so
that's a URL elephants comes last I otx
then if you go there you can you can
sign up and you know once a project you
know project is going in our you'll get
updates and so on and so forth so
alright guys so that's it again thanks
so much i know this is kind of a
different PMO topic so thanks for your
interest if you have any questions you
can ask me now I'll be around or you can
shoot me an email as well and if you
entered in the project I odx right so
okay thanks guys enjoy your life
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>