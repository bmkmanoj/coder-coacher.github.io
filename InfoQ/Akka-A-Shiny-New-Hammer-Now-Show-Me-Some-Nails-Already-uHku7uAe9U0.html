<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Akka: A Shiny New Hammer. Now Show Me Some Nails Already! | Coder Coacher - Coaching Coders</title><meta content="Akka: A Shiny New Hammer. Now Show Me Some Nails Already! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Akka: A Shiny New Hammer. Now Show Me Some Nails Already!</b></h2><h5 class="post__date">2012-04-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uHku7uAe9U0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so ello everyone i am daga from tap Add
and the vp of engineering there and I'm
about to talk about acha there are a few
reasons why I wanted to to have this
talk and first of all I think that Olga
is a great framework but I also think
that there is quite a bit of confusion
to what is actually is and what it can
be applied to so we've seen a lot of
people talk about acha blogging about it
all over the web and on different
conferences but very few people actually
talk about what you can do with it so
I'm going to talk about a little bit
very short about what it is and then I'm
going to dive into some real world
examples of them how we use acha at tap
at first of all what is acha I'm going
to make this very quick but I'm going to
just take the the definition or the
description from the aqua blog website
and it says that our kai is a toolkit
and runtime for building highly
concurrent distributed and
fault-tolerant event driven applications
on the JVM and that sounds pretty good
is this like no Jas for the JVM a gives
you automatic web scale and it's
probably not so but the amount of praise
that you see about acha sometimes might
lead you to think so the district
definition from the aqha website is very
very general it's a it doesn't really
say much so I took a risk and I tried to
make another definition or description
of it and I say that Locker is acha is
the library and runtime for two
different things so one is parallel ism
which is about taking a task that might
be hard to compute but is easily split
into some multiple pieces and then
execute them on several course at the
same time to improve performance of
programs and then the other thing is
about concurrency which is about
coordinating resources between multiple
threads to control sothink mutable state
and this is where actors come into play
but what does this mean well in my
opinion means that acha is not a general
purpose
framework that should be the default go
to whenever you're trying to build an
application acha has a fairly focused
set of features and has an excellent
implementation of those features which
makes it surprisingly broad in terms of
liq ability but it is not a golden
hammer you should not build all your
systems with message passing between
actors this does make sense anyway
that's my little rant about what acha is
not I'm going to focus on what it
actually is and what it can be applied
for so just a disclaimer we are using a
pagar for some users cases there of
course a lot of them and I'm only
focusing on a few other ways where you
see naka let's get started first of all
I have to give you a very quick real
time at technology one-on-one because
tap at is a company in the ad tech space
and what we're doing is that we're doing
what's called real-time audience buying
and what does that mean well back in the
day 34 years ago when an advertiser went
to an advertising network they might say
something like i want to buy ads place
my ads on sites about travel and i want
to pay a price of one dollar CPM which
means one dollar for a thousand
impressions that worked out well however
over the recent few years we move to
something called real time buying where
we can also apply targeting constraints
which are not only focused around things
like the site or the platform that the
the ad is being shown on we're actually
looking at individual characteristics of
each individual user and we try to do
value prediction for the advertisers
based on the features of an individual
user so how does that work in a very
very general view your browser on the
left side here it connects to a website
that website has some ad units on it and
your browser will call out to the ad
server through an iframe and hit the ad
exchange the ad exchange will then
broadcast information about the request
the little ad unit that's available for
sale right now and will broadcast them
to several buyers each of these buyers
will review the information to get about
this ad impression so it will be the
site
will be the user agent of the device and
so on and so forth but it will also be a
lot of information about the users to
past past history from the user for
instance might have seen this user on
other travel sites or at your own
website that maybe they have hit the
sign up link on your page but didn't
actually sign up then you want to show
ads to that user afterwards I'm not
going to focus on an actual decisions
going on in here but it's what's
important and it recognized is that
there is a lot of traffic and there is a
lot of decisions being made so in our
system all of the buying decisions are
piped through an a-cup actor pipeline
and they are processing about 50,000
requests per second okay so what are our
actual use cases for for acha we have a
lot of them it's kind of it's a
framework that I says that it has a
surprisingly broadside of applicability
because it's very simple to use but the
number one use case is probably for
concurrent access to mutable state so we
have a lot of states changing internally
and then we need to synchronize access
to that state I'll talk about that in a
bit we also used it for graceful
degradation of our service so we use the
circuit breaker pattern which allows our
system to cope with with the load spikes
or system failures in a graceful way we
use it for batching and buffering so you
can have actors aggregating a lot of
small messages and turning them into
something bigger or for a batch right to
some database or something of that we
use them for resource balancing and
throttling so we have a key value store
which we don't have a connection pool
for just build one with actors of a
bunch of workers you can very finely
tuned how those full access to the store
we also use it for scheduling a
background task but that's just because
we can I'll focus on the three first
ones I might not have time for the left
for the third one there so let's just
get into it so actors for concurrent
mutable state that
is the yes one of the most well-known
things about actress is that only one
thread connect X as the state at one
time which makes them very efficient for
for accessing state more going to give
the several threats so this ad exchange
is about taking an offer for an ad unit
that you might buy and then you can
decide if you want to bid or if you want
to buy it at all and then you decide how
much you actually are willing to pay for
it so the actual interface of a bitter
in our system is a simple trait it has
two methods one which says here's a
Harrison offer and then the bitter can
respond by saying I don't want to place
a bid for this at all where you can
respond with a with a bid which has a
dollar price basically in it and the
other one is the on wind method which is
called asynchronously after the bid is
placed if the bid was a winning bid and
as you can see the return type of that
is unit so this definitely has side
effects now in our system we have bitter
topologies this is a simplified view of
that we have several incoming requests
for 44 offers and we have several
auction runners running these auctions
internally our system here we have two
campaigns and we have to remote buyers
and all of these need to get the offer
sent to them they all have to process
the offer in parallel decide how much
they're willing to pay for it and then
return a response now why does it make
sense to model these bidders as actors
well the local actors they have a lot of
state I'll discuss the state and what
with what we store there in a few
seconds but also these remote actors
they have very unpredictable response
times so we have an essay we have to the
hair to we have to respond with our bids
to the ad exchange within the time frame
of a few milliseconds typically we have
many milliseconds of processing on the
server / request and having an
asynchronous model to place all to
broadcast all the offers and then get
the responses back makes a lot of sense
and I'll show you how that works so if
you look at the local bidders first the
first question they will need to be
answer is are we going to bid on this at
all and the first part of this is about
targeting constraints so a campaign may
have a restriction or constraints saying
that I'm only going to run these ads on
iPads that are visiting sites about
travel this is of course a decision that
you made without be made without any
state so no matter how many androids
visiting travels you travel sites you
throw at this bitter it will always say
no I'm not going to bid but then the
second point is about budgeting
constraints so a campaign will have a
budget let's say ten thousand dollars
and it's going to be spent evenly over
the course of 30 days that means that at
any point in time you have to try to
spread spread the money spend out so
when a bitter makes a decision if it's
going to pay the place of it on
something it's needs to a take into
account all the spend that has been
happening so far and run on the campaign
it has to see is it now about time to
spend to buy another impression and then
if it does it actually has to try to
predict how much it might have spent
because there's a disconnect in time
between placing a bid and winning so if
you have a bitter and it now has the
ability to buy one impression has a very
small budget and then it starts bidding
then it sees 50,000 bidder offers in a
second and if all those match then you
might have bid 150 thousand impression
in one second so the bidder is they
don't have to don't only have to try to
pace and keep up with what they're
actually winning they also have to try
to predict how much they might have
spent in order to not overspend so when
we're actually each bidder sees thirty
thousand requests per second simplified
they might actually change the internal
State 50,000 times per second as well
and this makes a synchronization of
state an interesting problem anyway
actors saw that very neatly this is a
somewhat simplified local beating actor
this basically just wraps another bidder
and proxies requests to them so we have
the the offer being sent then you reply
with with a bid and only when you invoke
the win method so the remote bidder is
of course more complicated also has
throttling and stuff like that but from
the auction runners perspective so this
is the piece of code that actually takes
in an offer and broadcast it to all of
the potential bidders and as you can see
here we take in we have a list of
bitters and then we map this is Scala
1.3 so we have still the triple bang
which is a method that sends a message
to an actor and returns the future of a
certain type so in this case we are
expecting to get futures that are the
type option bid we also give these
options the max response time so that
when we wait for them underneath their
future so with all we block victories
here you probably cringe because he
hates blocking but we have a book we're
using stirlitz we have to block at some
point and then we can just flat map over
those options and return all the results
that were actually returned in a timely
fashion so if there are any of these
actors that are having some sort of
issues network latency or partner is
slow as something like that they will
just timeout and we will ignore their
answers and that's pretty much it and
that's actually also what makes this
this model better than just doing
straight method in vacations in this
case is that we can use the same access
model for both local and remote bidders
and we can also if we need
too if we had too many campaigns running
so we can run them in one vm then we can
scale that out and run some of them on
other VMS the second interesting way
that we are applying actors and
dispatchers is for graceful degradation
and I do that through circuit breaking
so we're dealing with fairly relentless
volume of traffic and there is no back
off we start responding slowly we'll
just have more and more connections
coming in we have had several pieces of
networking equipment fall over because
we had a hiccup in our system and stuff
like routers and firewalls they just
can't deal with the traffic so we have a
few standard incidents that can happen
so when we restart a vm it will require
some warm-up time and at that point it
will usually be a little slower than
then it will be later on we do have
occasional key value store hiccups every
single bid request V process or operate
process actually needs to do several
lookups in our key value stores so if we
have a little hiccup there it might
cause response time issues and of course
we also do have some occasional traffic
spikes if we're on a big we have some
some tags on a big site and they have
some campaign running and then all of a
sudden we might see an influx of traffic
that we weren't expecting so question is
how do you cope with with these kinds of
performance issues and one way to do it
is to degrade the service while you're
having issues to try to catch up and
then question would be how do you
actually degrade your service of course
the default would be you could accept
increased latencies so if you're having
something that's facing consumers or
users if each page load takes some time
then they will probably do fewer
requests so that's one way to do it if
it gets too bad then you end up perhaps
refusing connections all together so
it's also a way of degrading your
service until you actually can catch up
now there is another way of doing that
and that is by leveraging
main or service specific options so in
our case we're trying to do stuff makes
them take some shortcuts so do things in
a simple way that we would do if we
running at full performance so three
things we can do here for instance we
can stop loading use of data so when we
get an offer in and we're seeing that
we're having performance issues we can
stop loading or query in our key value
stores for a little while so we will
still might make bids on these offers
but we will might bid with less
information another thing we can do is
you can stop running the predictive
optimization which will save us some cpu
cycles just place a conservative bid
instead of trying to to predict for
every single request what the value of
that would be and then the last option
is the one we use a lot actually we can
start passing on the offer all together
so for us to tell the ad exchange that
we're not going to bid it's a very very
inexpensive operation but it's a
perfectly viable response to that
exchange and the people running that
exchange they won't get mad at us for
slowing down their system so it's much
better for us to respond with no we're
not going to buy this instead of having
a lot of very late yes we want this
because it's too late anyway so next
question then would be when can you
degrade the service and we use two
separate ways of doing that one is if we
predict that the response time will be
above some certain threshold then we
degrade our service and that can be
implemented by having actors with
timestamp units of work i'll show some
code and how that works but I means that
the actual actor considers the time that
the job was submitted and then does some
shortcuts if it's if it thinks that it's
not going to be finished in time and
then the second one is actually by
checking the size of the mailbox so if
you're if the mailbox is over a certain
limit then you will stop or you will pop
things off the queue this is a little
controversial because it's been a lot of
discussion about this well I'll get to
that later anyway if we want to look at
a circuit breaking actor basically looks
like
you have a you receive method and you
have a little case class called work
unit that as a time step in it and then
the actual value that's going to be
evaluated and as you can see we just
check if we currently have passed the
time where it's likely that or actually
if this thing has been on the queue for
too long so we think they were not going
to be able to respond in time then we
just return none of course the nun here
could be whatever it could be another
method invocation this takes less time
than what the default execution flow
would be the drawbacks with this
approach is that if you have long pauses
in your pipeline you will have long
queues and you might actually end up
running out of memory and that sounds
like yeah you have to have a lot of
messages in queue before that happens
but it's actually not that much and
we've seen a fair amount of OMS based on
on just the hiccups in encode sorry in
the processing queue so if we had
something blocking in there which we
have to do in some case because we have
locking io ap is for our key value
stores for instance then our cues might
come become very long and then at some
point garbage collection overhead will
just kill the server you might think
that just bounding the mailbox might be
a way around this saying that if we're
going to we're going to say that maximum
mailbox size is a thousand elements the
problem with that though is that then
you start passing on the most recent
offers so you can't put things on to the
mailbox anymore which means that you're
basically discarding the most valuable
requests which are the newest ones you
want to discard all the old ones that
are already expired other thing with
actually having a bounded mailboxes so
you have a lot of exceptions being
thrown and the overhead of instantiating
a full exception with the stack trace is
actually fairly high so the alternative
way of doing this is that you can use
what we call a circuit breaking
dispatcher and I can show all the code
but I actually I posted it to the
mailing list three weeks ago or so
because you tried it might get this to a
CO 2 point 0 but it's basically a
dispatcher very mixed in this circuit
breaking dispatcher semantics which
takes a circuit breaker policy which
where you determine you specify the
maximum amount the max size of the queue
here I didn't write these out the name
parameters because it's all read a
little long in that line but this is
says it has the maximum size of two
thousand elements in the mailbox and
then we have this method you can
optionally / I the right which is
replied to overflow where you can decide
if you want to do some sort of fall back
function and respond with that so when
the if if the queue is too long you will
still get the opportunity to to take a
look at whatever is being passed in and
you can respond something simple back
but the difference here is that this
will actually pop things off of the
front of the queue and discard that
first whereas things coming in from the
back will be saved so it's a way of of
keeping the most valuable information
also this does not have the same
problems with q's being long because
this happens synchronously so what are
the issues with circuit breakers in
dispatchers there are a few first of all
we've actually runs into this you might
mask real performance issues so you want
this to only happen in exceptional cases
when you're having some sort of
performance issue or outage but the
makes it makes the system so flexible
because all the results that are
returned when you're actually in a
performance issue situation the
perfectly valid so none of your partners
were level the clients will never notice
but you might actually be dropping a lot
of data that you shouldn't so it's
important to to actually instrument your
application as you saw here we have a
little metric his stats increased
incremented here saying that the auction
runner has discarded
a bid request all the problem with this
is of course that it's hard to tell you
have you have you don't have a lot of
levers to push it's just a very crude
number the size of the message box what
would be the right number for that maybe
you should have something you can you
can't really say anything about trends
and through that it's just a hard copy
cut off so might be some room for
improvement there but most notably also
and it's very hard to do in acha tu PO
know if you follow the mailing list raka
there has been some fair amount of
discussion about that a little while
back in there was of a couple of blog
posts about it so they decided to make
the size of the mailbox non visible to
the to the clients the reasoning behind
that well there are a lot of arguments
for which all makes sense I guess but I
think that this might be one of the
cases where it would be very useful to
have the have the size of the mailbox
anyway we'll make it work with with with
like a 2 point 0 0 so actually I'm going
to briefly brush upon one more area that
we use acha it's a very very simple use
case but it's surprisingly convenient so
we use it for batching and buffering so
part of our part of our system is
receiving a lot of data at a very high
rate and then we need to store that
somewhere so we have two different types
we have all these offers we want to
store so we can now use them for for
statistics and we also have all our
impression and click information coming
in and we need to write that somewhere
and we are actually streaming our data
directly into a columnar data store so
not stringing it to disk which means
that there is a little bit of overhead
when we're riding so we want to batch
those those rights so we have a very
simple batching storage actor which is
just an abstract class you define a
batch size in
year right here is 25,000 elements and a
maximum perch involve which in this case
is two thousand milliseconds and then
this thing will take offers and auction
results and it will basically just
buffer them up to 25,000 elements or 22
million two seconds and then it will
invoke the same method which makes it
very simple for us to just bulk right
stuff and of course this is a very very
simple standard queuing mechanism but
it's incredibly simple to get it wrong
if you're doing it with manual thread
synchronization and then it's incredible
simple to implement it with actors so
I'm not showing the actual
implementation has a little bit of
logging and stuffs I can rip that out
right away but it is very very
straightforward and once you start using
akka you see a lot of these asynchronous
patterns that you could write using the
standard concurrent data structures in
the in the JDK but there's just so much
simpler to do with acha lastly what have
been our main pain points with using
akka and the number one is well first of
all we've been very happy with it it's
been working well for us I think we
might have over plied in a few cases but
I think for the most part has been there
has been a good experience but our
number one pain point is probably that
if you start using untyped actors which
is the default you're basically removing
all of that typesafe goodness that
usually have and you're have method
signatures you start passing message
objects to actor reps you have no idea
if they are able to respond to it if
they're able to process it it's not a
huge deal but if you over applied if you
if you were to do all your programming
this way there will be a untangle mess
so it's more of a configuration problem
you might sometimes switch out to actor
eps and then you get a lot of
match errors basically and those can
actually be a little bit difficult to
track down some loud sometimes because
when you have so the actual sender or
message and the actual processing my
message will happen two different their
threads so if it blows up if you send
the wrong message to the wrong actor
then you won't be able to figure out
where it came from because there's no
stack trace tracing that so it's a
little but it's not a big deal but it
can be a little annoying more
importantly as though that testing of
asynchronous systems is harder than
testing synchronous systems there has
been done a lot of work with the Aqua
test kit since 1.2 we haven't really we
try to implement the test kit for
retrofitted into our existing tests when
I think they arrived in 1.3 but that
wasn't very successful so we still have
some tests that that fail intermittently
because of the synchronous nature I
haven't looked at the the testing
mechanic and mechanisms in two point oh
but I still think that it's probably
harder to test than just a straight
method invocation that's all I have for
now questions yeah in the back so when
you're doing this adaptive so the
circuit breaker padding basically if
you're if you're in a state of having if
you're a canoe continues overloaded
state then you might have a system which
will switch on so start doing stuff and
then it will get overloaded and will
snap back into to doing things in a
degraded mode again and will start
oscillating back and forth and due to
the very nature of things when you turn
it switch on you will have a massive
influx which will cause another
performance regression and make the swap
back that's your question right it
oscillates so I think probably if you if
you are running your system evenly on
the brink of being overloaded then that
might be a problem we try to have a fair
amount of spare capacity so it hasn't it
hasn't been a problem for us in practice
but I can totally psych and see that
that might
happen yes definitely so the first
question is what kind of web framework
overusing and we are using straight up
servlets and jetty for this these are
server to server calls for the most part
protocols are everything from well the
different ad exchanges have different
protocols summer JSON some our protocol
buffers some are just URL parameters and
so we just use servlets for that and
then that's all we don't have a web
framework because it's just server to
server communication basically the other
question was about scala binary
compatibility I think right and I don't
think that's very much related to all
kinds particular but it's a problem for
the whole community regardless of the
framework whenever there is a new big
point release of Scala and you want to
start upgrading and then you have to
basically wait until every library you
have as been cross compiled you can't do
it you have to do everything at once
which is a problem especially if older
versions don't get cross compile back to
your versions you actually have to
upgrade the library versions as well as
the Scala version then starts getting
really things starts getting really
tricky so question was have used remote
actors and answer is no so victor says
said now that in two point oh they
default everything might be remote and
then local is an optimization to us we
have I don't know how many actors you
might have but we have a few thousand
actors running in our system at any
point in time and each individual actor
in the system has to see every single
bid request so on one server there might
be five thousand a second and you have a
thousand actors so that's five million a
second and if you were to have another
millisec you know overhead because
jumping over the cable we could
definitely not doing on that fine grade
level we would have to do that two
chunks of things so it's actually a way
we're thinking of partitioning or
scaling out when we have a when our
number of campaigns reach a certain
critical mass than we can really respond
in onenote anymore and then we have to
start petitioning which means that we
might have to have some sort of oil
proxying of actors internal
so some might run here and someone run
somewhere else the partitioning at that
point we might have used remote actors
as proxies basically but we don't want
to do remoting until we absolutely have
to because it's going to incur a lot of
performance already yes we're not using
remote actors so how many course are we
using so we have most of our service are
eight course some of the newer ones that
archive come in our 12 course but yeah
we're using some statistical or more
heuristics of our system which because
you would think maybe that we have one
campaign only exists in one place but we
actually have them existing all across
the cluster because we have all our
volumes are so high we can actually use
the fact that that load is randomly
distributed across the cluster that they
will randomly get the same amount of
impressions that will match and and they
will randomly also well at least
normalized get spend amount of the same
amount of money over a given period of
time they check in with each other on
regular intervals to coordinate for any
skew that that might be in there but
that's how we how we manage that
otherwise we would have if we only if we
could have only one instance of each
campaign for instance the whole system
then we would have had to do something
with remote actors but that would have
performed terrible compared to the
current approach so we started using
akka from the very beginning as I was
January last year again was we started
using it because it was cool and that's
kind of one of the things that I wanted
to address with his talk I don't know
how I forgot it through but I think that
it's very easy to over apply these new
cool technologies and put them to work
everywhere I think we might have done
something that in some cases ourselves
at least we should perhaps a tried just
you manual synchronization first in some
case and see how that performed then
what worth of this afterwards but yeah
we didn't but we've been using it for a
while okay
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>