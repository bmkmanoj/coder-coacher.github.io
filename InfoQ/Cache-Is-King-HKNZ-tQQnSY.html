<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cache Is King! | Coder Coacher - Coaching Coders</title><meta content="Cache Is King! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Cache Is King!</b></h2><h5 class="post__date">2013-01-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HKNZ-tQQnSY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">very excited that everyone can make it
out this morning and it's gonna start
off pretty intense so this is going to
be I had physics for physics majors four
semesters in a row at 8 a.m. and it's
gonna be a little bit like that as we
start so buckle up and I'll try to keep
it light but it's gonna be kind of heavy
so we're going to talk about cash is
king
and of course it's a play on words right
this is you know the actual phrase it's
unclear where this came from they've
sighted from the late 80s in the world
start stock market downturn but actually
today we're not going to be talking
about cash and this kind of cash anyway
and we're not going to be talking about
this kind of cash or this kind of King
we're going to be talking about this
kind of cash CH e we're going to be
talking about this kind of cash and I
think the cash and how we take advantage
of the cash and what browsers do with
the cash is really important so I've
written about this extensively and in
fact a couple months ago I announced
that browser cache and what we can do
with it and how we can control it and
use it to our advantage is the focus of
my research for the next few months and
I want to start by going over this study
that I just released last week where I
want to look at some of the long poles
in the tent when it comes to trading
fast websites and I think those are
connection speed how fast your network
connection is how fast you can get those
bytes down over the wire front end which
is primarily JavaScript CSS also plays a
role but I think javascript is a much
bigger impact on performance and how we
use the cash
so I set up this experiment and let me
describe it really quick I'm going to
use webpagetest.org how my people have
used webpagetest.org so this is the most
important thing to take away I should
have put this on my take away slide this
is the most important thing to take away
from today is go to webpagetest.org and
they you don't have to install anything
anywhere it just gives you a tool that
you can use to load any URL and get
performance information about it it's
done by Pat Meenan who started it when
he was at AOL but now he's at Google has
a couple resources helping them with it
so I actually have access to a private
instance of webpagetest.org and it
supports an API so I'm able to do large
runs on it in fact at 2:00 a.m. this
morning I started a run with 300,000
URLs but I did this last week where I
looked at the alexa top 100 and i ran it
in IE 9 using a simulated dsl connection
so 1.5 mag down 50 millisecond
round-trip latency and i'm only looking
at the empty cache or first few
experience so this would be the first
time for example that a user ever came
to your website or the first time they
came to your website after their cache
had been cleared and we're taking the
median of 3 runs and the median is based
on the window.onload so there's other
metrics we should look at for
performance besides onload like start
render or above-the-fold rendering time
but one that's predominantly looked at
is window onload so that's what we're
going to be using when we analyze these
thousand sites so this is the baseline
and then to look at those long pulls in
the tenth I did three variations so the
first one was replacing that dsl
connection at one point five megabits
with a FiOS connection that's 20
megabits down megabits per second
disabling javascript which would take
that long pole
out of the tent and instead of looking
at empty cash view looking at the prime
cash for you or the repeat view so let's
pause for just a second I always do this
before I run an experiment I don't know
about you guys but it usually takes me
about three to five tries of designing
an experiment running it looking at the
data and realizing some flaw I had in my
design
so before I start an experiment
especially one that's going to take
quite a few resources to run I stopped
and I envision what the results might
say and I usually have something that
I'm expecting it to say I always ask
myself what if the results come out the
opposite would I explain that away by
some flaw in the experiment design and
if so let me address that now so I
stopped before I ran this and I thought
what the results were going to come out
to be so at least three how many people
which of these fixes or improvements do
you think it's going to produce the
fastest result what about going to a
fast Network disabling JavaScript and
using prime cache so you guys read the
title of the talk today did you
I even though I ran this when I'm on my
focus on caching like I want to focus on
caching but I kind of want to see how
big is the thing that I'm focusing on I
actually thought it was gonna be
JavaScript so let's just jump to the
results this is what the results came
out to be so I'm gonna kind of focus on
the median time but the 95th percentile
time is there as well
so the baseline was about seven and a
half seconds so that's the median of
three runs for each website so we took
the median time so a thousand medians we
took the average of that was seven point
six five if we disable JavaScript we're
at four point seven seconds going to a
fast Network was much bigger than I
expected four point one three and using
the prime cache was three point four six
so doing a prime cache a repeat view was
less than half it was twice as fast as
the empty cache page view and I don't
know maybe that's surprising maybe not
but if we look at this next chart which
shows the total transfer size so this is
if any of the responses are JavaScript
or CSS or HTML they're compressed so
this is just a number of bytes
transferred over the wire if we look at
the total number of requests and the
total transfer size we can kind of see
how these numbers make sense so here's
the baseline oh and in blue I wrote the
median on load time just to remind us so
here's the baseline the average transfer
size was 900 K and the number of
requests was 90 if we disable JavaScript
we go down to 59 requests because of
course javascript is doing more and more
work in these websites especially in the
top 1000 web sites they're downloading
other dynamic resources so if we disable
JavaScript the number of requests is
going to drop dramatically even though
there might be some images or whatever
in no script tags is going to be reduced
from what would come down
JavaScript enabled and so with that drop
in requests we also get a drop in total
transfer size but more importantly to me
something that's not reflected here is
the time it takes to parse and execute
JavaScript so we're also skipping all of
that pain here as well I would have
thought this would have been much faster
and then this is really interesting and
speaks to the pain that we have with
building fast mobile sites where with a
FiOS connection we're actually
downloading as many resources and bytes
if we were in the baseline but because
the speed is so much faster we're
getting a much faster time again almost
half not quite but almost and so there's
still a lot this was surprising to me
because of fast networkers gonna make a
huge difference but there's still a lot
of contention in the browser
there's things have to happen in a
certain order in this fast network test
we're still executing JavaScript CSS and
reflows are still happening and yet just
a faster connection is getting us down
to four seconds so that's very
impressive and and I don't know I didn't
mention that the latency here was very
low I think it was four or five
milliseconds in the simulaid connection
so we're not gonna get we might get
speeds that are like dsl on mobile
typically less but the they're very high
latency and so that's going to be a
problem on mobile networks but if you
have a fast home network you might be
getting faster page load times
benefiting from that but this is the big
change is with a prime cache we're
dropping down to about a hundred and
sixty K and about thirty from ninety
requests only thirty requests so now
this is a best-case scenario so
typically the way so what happens in
webpagetest is you say load the page
with an empty cache and then load it
again and record the results for the
prime cache for the repeat
you and that repeat view happens 30
seconds after the first one so whatever
was stuffed into the cash is still there
right there's the user didn't wander off
for a day or two
downloading other images and videos and
websites so this is a best-case scenario
anything that the website owner said
could be cached is going to be there in
the cache whereas typically you would
want to think about a repeat experience
happening the next day or three days
later whatever your typical user
demographics user sessions stats
indicate so this is the best-case
scenario but it produces very
significant savings so you know this
tells me the cash is pretty important so
before I go too much farther we're going
to get into the physics for physics
major as part of the talk we're going to
do a little primer a little review on
how caching works with HTTP how many
people feel comfortable with their
knowledge in caching it's only four
slides
so bear with me okay so suppose we're
requesting Maine Jas we're gonna send up
a simple get request like this the nice
thing about HTTP is it's a text-based
protocol so we can see everything and we
very get back this response that says
it's a 200 ok we're returning some text
JavaScript and it's about 200k of
JavaScript and there's the actual
beginning of the file so that's pretty
simple that's how a simple get works now
the next time the user goes to this page
in a minute in a day this is typically
what's going to happen they're gonna
make another get request and they're
gonna get the same response right there
might be some heuristic caching going on
and we'll talk about that in a few
slides but basically this is the way
it's going to work and what we notice is
we just downloaded 200k twice and it's
probably the exact same 200k it's the
exact same size and that's not really
efficient right so the guys who created
HTTP said well what can we do to make it
so that if a file hasn't changed you
don't have to download it again and the
way that's achieved as with a
conditional get request so the first
time you make the request the thing that
changes is in the response the website
owner has added these two response HTTP
headers and you don't have to add both
you could add one or the other last
modified gives the date that this
response was last modified so the last
time the developer edited it and pushed
it to production was September 24th and
there might also be an e-tag this could
be anything you want by default Apache
puts in the file timestamp which is just
kind of repetitive with last modified
you might put a checksum or something
like that it could be something that is
generated based on something about this
user and their location and their
browser but whatever it is it can be any
string that you want according to the
spec now and then we get 200k of
JavaScript now the next time the next
day the user asked for this resource the
browser knows that it got these headers
the e-tag
modified before and so the browser can
send up these two new request headers if
modified since I want this file main
dodge is I have one in my cache that is
dated September 24th do you have
anything that's been modified since then
and also the e-tag it also has this etag
is the new one got a matching does the
new one have a matching etag and if
nothing has changed the server can just
return this tiny little 304 response
code so we can save 200k so that's
awesome but if you're on a high latency
network like mobile making this check
doing this conditional get requests for
all your resources is going to be very
painful you'll have tremendous savings
but the typical website has about eighty
or ninety requests in it so if you had
to make 80 of these conditional get
requests over a mobile network or even
desktop it's going to slow things down
significantly so the guys thought what
can we do is there a way to avoid doing
this conditional get request every time
and that's where we get into the caching
headers that indicate how long something
can be held in the cache and reuse
before it expires so the way that that's
enacted is the website owner returns
this max-age cache control max age value
and this says this is the maximum number
of seconds that you can continue to
repeatedly use this response this
javascript file without even checking
with me you don't even have to bother to
check and so in this case this is one
year of seconds so within a year if the
user loads this page again and needs
this file there's no request no HTTP
traffic at all that just reads it from
the cache off local storage off disk so
that's really really fast it eliminates
the HTTP traffic altogether now this is
what is really really good and if your
resources aren't changing or you know
when they change you can just change the
file name you want to use a maxi
a year is good a week is about the
minimum you want to use you can use ten
years really once you push something out
live to the world you never want to
change it without changing the name
because there are so many misconfigured
proxies out there about one to ten
percent somewhere in there of your users
will never get an update unless you
change the name so really you can make a
cash flow for ten years it doesn't
matter you're never going to push a
change without changing the name of the
file so a year is kind of good but in
some cases you have something dynamic
like the inbox the address book the
number of mentions things that you don't
ever want the browser to cache or any
intermediate proxies of cache so how do
you avoid caching even though that's the
opposite of what I want to espouse I'll
just mention it for clarity suppose you
had something like inbox Jes maybe it's
Jason or something you can return this
cache control header and set a max age
of 0 and specified no cache and must we
validate that tells the browser anytime
a web page once this resource you have
to check back with me and if it's
something that's confidential you might
also want to add no store like banking
information you could add no store so
the next time that the user loads a page
that needs inbox J s it makes the
request again so that's how caching
works that wasn't too bad but the
takeaway from this is it's really in
sport important to be explicit you
really should never kind of be in the
waffle sale I'm not sure if I don't want
it cached or I do want to cache I just
won't do it anything and I'll leave it
up to the whims of intermediate caches
and proxies out there what to do with
this resource when the user requests it
next time be decisive if if something
isn't changing very much or if you can
change the file name whenever it does
change then make it cashable for a year
if something shouldn't be cache if they
should check back with you your server
every time then say you got to say no
cache otherwise if they might not check
back so be explicit specify cache
control no cache
max-age so that's a good takeaway what
do we actually see in practice this is
where it gets a little more fun so I
think the the physics for physics majors
part is more or less over because
there's always fun to look at what real
web sites are doing and that's what
we're going to go into now so here's the
Alexa top 1000 worldwide and what we see
is I'm just looking at max age here so
we see that I can't even read this it's
too small for me five percent have a max
age that's more than a year that's
really good not as good as it should be
on the other end 37 percent of responses
don't specify any max age right so some
of these might be no caches right
there's no max age you should say max
age the URL you don't have to but some
of these might be a no cache but 37
percent is pretty high and the top 1,000
sites are better configured than the
long tail of web sites out there so we
can look and I looked at this data and I
saw that 14 percent of the responses had
no cash or must revalidate and about 10
percent of that 14 percent were in this
bucket with no max age so that means and
also in here 24 percent of them they had
no cache control header at all so that's
where people have just said I don't know
about this or I'm just ambiguous I don't
know what the answer is and 27 percent
37 - 10 have no max age and they don't
say no cache or must be validate so
again the website owner is just being
ambiguous and ambivalent about what the
caching behavior should be and what that
means is these responses are going to be
subject to heuristic caching which we'll
talk about in a minute so that's not
great the top 1,000 sites in the world
25 percent of the responses really have
no indication of what caches should do
with them
should I reuse it should I not reuse it
I don't know you guys just decide and
really as top websites people shouldn't
do that now if we look at the top
300,000 all this data I'll mention at
the end is from this project I run HTTP
archive org and currently as I mentioned
right now we kicked off the crawl we're
doing 300 analyzing 300,000 the top
300,000 in worldwide URLs every two
weeks so if we look at the top 300,000
again as we get out of that top 1,000
well-configured sites things get a lot
worse so instead of 32,000 percent
having no max age now it's 57% more than
half of the resources have no max age
indicator again there might be some they
don't have a max age indicator because
they're not cacheable but only nine
percent of this 57 percent are no cache
so that means 44 percent have no cache
control header at all and 57 minus 9 is
48 percent have no max age or no cache
or must we validate and so they're
subject half of their responses are
subject to heuristic caching whatever
behavior the cache is out there in the
world want to do and there's no spec
that dictates how heuristic caching
should work so 50 percent of the
responses are going to be cached up to
the whims of whatever cache the user is
behind their local cache or their
intermediate caches and the other sad
thing is as much as I love to go out and
talk to people and beat the drum around
high performance we're really not seeing
a pickup and the use of caching headers
this is for the last two years looking
at the top two years ago we were only
doing 50,000 at the top 50,000 URLs so
during the course of this that's what
some of the drops and and increases are
from is chain as we expand the number of
URLs we're looking at the stats drop a
little bit because generally as you get
closer to the tail performance is worse
so we'll drop a little bit and we'll
start climbing and drop a little bit but
generally there's not a great uptake
in this very important performance
optimization of getting resources to be
cached so we have slow adoption so a lot
of these resources in the case of the
top 300,000 URLs will be subject to
heuristic caching and there's nothing
that dictates what an intermediate cache
or local cache should do for your estate
caching but there are guidelines so this
is what the spec says in the absence of
max-age so that's what we're talking
about here we're talking about resources
that don't have a max age and they also
don't have no cash or must revalidate so
really the website owner hasn't said
anything about how caching should be
dictated for these resources in the
absence of max age the cache may compute
it's up to them a freshness lifetime
using a heuristic if the response has a
last modified time so heuristic
expiration value should you don't have
to but we recommend you should be no
more than some fraction of the interval
since that time so it's actually very
simple but the wording makes it a little
hard for me
so suppose today October 15th we
downloaded Maine Maine dot Jes and it
had its last modified time of September
24th how long ago was that 21 days the
guideline is you should have a freshness
window of two days 10% of that two days
so if the user came back tomorrow if
they download it today and we saw that
it last modified the 24th user came
tomorrow and there was no caching
headers the browser would say well it's
been less than two days so I'll let you
reuse it I won't generate another
request they came back two days later or
three or more then the browser would re
request it even though it's possible and
likely that this resource hasn't changed
so that's how your ristic caching works
now remember we've got browsers and
intermediate caches don't have to do
this if they don't want to and we've got
50% for the top
thousand sites are subject to this you
know fairly vague guideline about what
caches should do so let's assume it
turns out ie9 actually adopted this
exact behavior and that's what we bran
our experiment in so what's a typical
interval between rant when we ran the
experiment I ran it the first week of
October so when I ran the experiment and
the last modified date of the resources
what's a typical interval of that time
the date the experiment ran in the last
modified date so if we look at these top
three hundred thousand responses
websites if we look at all the responses
there's about three million of them no
thirty million of them this is the
distribution about three percent have an
interval so that's the Delta of the time
of the experiment and the last modified
date three percent have an interval
under a day another about three percent
under a week so we have about six
percent of them have a heuristic max-age
that's the interval that's less than a
day which if you take ten percent of it
is like two hours so if the user if
there's if you don't specify anything
the user comes back within two gap two
hours and this is out of the total
number of requests so we had about 40
what was it 48 percent of the responses
didn't have any max age headers six
percent of those will have be able to be
reused in two hours or less but so
that's not very good and we have about
of that 48 percent thirty percent of
them so about a third of overall
requests will be able to be reused
without a response if it's done in more
than three days so that's not bad if all
the browsers out there adopted this
heuristic but it could be a lot better
if they just specified a max age right
it's not that hard and you're going to
get some caching behavior
anyway it's just that you don't know
what it's gonna be it's up to the whim
of the caches whatever they've
implemented so it would be much better
since you're going to get some caching
behavior anyway to be very explicit
about it if you want your stuff cache
specify how long it should be cached and
when the browser should check back for
an update if you don't want to cache
specify no cache and then we also have
this eight percent of the overall
responses are unknown in the case of IE
9 I don't know about other browsers what
it does what the unknown ones is it will
check once per session so the you close
your browser you open it the next day
you visit that website that has
something that was cached without any
max-age and without less modified then
for those the browser's guard checked
the first time and never check it or ie9
will check the first time and never
check again for the life of that session
so again you're getting behavior that
you don't really know about so a lot of
stats a lot of charts let's do something
a little more fun let's bring it home a
little bit so people who live in glass
houses shouldn't throw stones but it's
kind of fun
nobody's perfect so please everyone take
this with a grain of salt if I talk
about a website that you work on so for
example if you're a speaker at this
conference you're sly a slide for your
website is probably in this deck so I
sat down and do this last night and I
was excited I analyzed a lot of websites
and I looked at the speakers and the
websites they worked on and a lot of
these like Airbnb were interesting sites
that I had never analyzed from a
performance perspective before and I
might I was you know excited this is
going to be fun
I'll put people under you know hot seat
a little bit
I'll tell tell attendees to go and why
are you making your site better
configured
and it was hugely disappointing because
most of the sites were really good I all
those speakers who run these websites
thank you very much but they kind of
took the wind out of the sails of my
presentation so here we have Airbnb 81%
so I color code the stats Green is good
red is bad
an ugly harvest color is in the middle
so all green 81% of the responses have
cache control headers sure it could be
higher but that's pretty good a very
small number of them have very short so
you might specify a cache control header
but it might be really short like Oh
only cash it for six minutes that's
bogus well only 10% of the resources
have an expiration time less than a day
so that's pretty good that's really good
in fact
and they've specified last modified you
want to specify last modified otherwise
heuristic caching is really out the
window as well and browsers are getting
smarter and smarter and smarter I didn't
have time to reverse engineer the
heuristic caching for all the browser's
but I'm confident that they're good and
they're going to be getting better so
but you have to specify a last modified
to give those those browsers those
caches some clue about what they should
be doing turistica Lee and 70% is pretty
high and the last modified interval is
about 40 days so in cases where a your
state cache has to kick in
it's going to be about a four day
interval at least in ie9 ten percent of
forty days so that's pretty good Airbnb
is in pretty good shape very good shape
I would say another site I hadn't looked
at Pinterest really hoping for a lot of
red here not get in it so here we see
here we see 131 requests 87% of them
have a cache control header only 2%
expire in less than a day 94% have last
modified and the median last modified
interval is a hundred fifty one day so
10 percent of that would be a heuristic
window of 15 days that something could
be reused in the cache if it didn't have
a max age so this is again really good
so I've got about 10 or 15 other speaker
websites that all look good and I'm not
going to go through those because that's
not fun so let's look at some that
aren't so good so I think this this is
Sydney is speaking today and he's from
stack mob only 25 percent of the
responses have a cache control header
Sydney do you want to raise your hand no
I haven't met her before I hope you stop
bigger than me
so only 25% of the responses have a
cache control header and again
like this is like not being decisive
it's like making a decision by doing
nothing and that's not good like be
explicit should these things be cached
or not because you're going to get in
certain browsers regret get certain
cache in any way do you really want to
have no idea no control over what's
happening or do you want to control what
your resources are going to get updated
so 25% that's really bad the ones that
do have caching headers only 1% expire
less than a day so that's good and 81
percent of the responses have a last
modified that's really good and it's a
really long last modified interval the
median last modified time between when I
ran the experiment and the median last
modified date is 240 days but that's a
red because that means their resources
aren't changing very much and they could
have made them cashable for a really
long time but they didn't specify any
cache control header so they could have
made these things cashable for months
maybe even a year and they're not
achieving that so it's a really lost
opportunity mozilla generally really
good and this is a small page only 32
requests but it's their front page only
31% have a cache control header again
like be explicit not too many have a
short expiration time most of them have
a last modified the median last modified
time is 24 days 10% of that is only 2.4
so that's not a great heuristic window
if there are if the median time between
when they change is 24 days you could
specify a last max age of a week or two
and get better caching Zendesk is
speaking today maketo 70 requests so you
know it's kind of in there it's a median
you know average page 94% of captains
rule that's really good but 59% of them
expire in less than a day like I find it
hard to believe that they're changing so
much and in fact if we skip down
well unfortunately only 69% of them have
a last modified but they have a pretty
long last modified interval and so it
really seems again that this is a last
lost opportunity things aren't changing
that much in reality when we look at the
last modified of the resources on this
page specify a max age header for more
of them catch comm 52 requests on a very
heavy page only 19% have a cache control
header and a lot of them expire in less
than a day only 69% have a last modified
so we really don't even know what the
we're not giving a lot of clues to the
her astok Sharath heuristic algorithms
and again there's a lot of opportunity
resources aren't changing that much we
could have specified a max age here and
Intel I think is the last speaker sign
I'm looking at 90 resources only 66%
have a cache control header 84% of them
that specify a max age expire in less
than a day a large percentage of the
resources have last modified but there's
a it seems like there is a lot of churn
in the resources in this website so they
should be more explicit about what's
cache full and what's not certainly
you're not going to get a large
heuristic interval here 10% of 12 is
only one day but if they're changing on
average every 12 days you could have
specified a max age of about a week so
these speaker sites thankfully weren't
that bad
and that's what we see we saw that
before in the charts I showed the
histogram with the use of max age we saw
that we saw that when we looked at the
top 1000 they had better use of max age
than the top 300,000 so that's typically
what we see so I'm not surprised that
speakers from companies you know that
are you know speaking here at a deaf
conference that their sites are better
configured but we still saw these pretty
bad
statistics out there for the long tail
of sights so where do these bad apples
come from if you do any consulting work
in the area of performance I recommend
that you look at media companies so
we're gonna do that really quick and
we'll see that things are worse out
there then is reflected in the speaker
websites
so here's Boston comm 69% of
cache-control a lot of the resources
could aspire quickly there's a fairly
decent last modified interval so we
could have had a higher less modified
rate and more max-age time calm hardly
anything as a cache control header
things that do expire very quickly
there's not a lot of churn in the
resources so specifying a max age would
have been an easy win USA Today a large
sigh 127 plus almost nothing has a max
age a lot of quick expiring and a fairly
decent last modified interval so they
could have taken advantage of max age
Telegraph a large site 179 requests
almost nothing has a max age the ones
that do expire very quickly and again a
huge opportunity to make things cashable
tmz.com huge number of requests that's
got to be wrong there must be well maybe
not and it's all read-only half the
things have a max age the ones that do
expire very quickly only sixty percent
have a last modified and the last
modified time is pretty short so the
heuristics aren't going to be that good
so the takeaway here is be explicit so I
spent a lot of time last night and this
morning looking at these background
searching Flickr for these background
photos and searching for the word
explicit didn't return what I expected
so I had to change my search terms but
you want to be explicit always specify a
cache control header if things can be
cacheable specify a max age if you want
to be a little conservative be a little
conservative but specify something
because most caches are going to do some
level of heuristic caching if you don't
specify anything it's taken out of your
hands so you're still going to be stung
by stuff being stuck in the cache you
just won't have any idea how long that's
going to happen and when the user is
going to get an update so if things can
be cacheable specify in max age if you
don't want them to be cacheable specify
no cache does that all make sense okay
so that's a very easy takeaway for today
I wanted to touch on some other things
that are kind of outside our control or
outside at least of this cache control
header this is a study that Tenny Toya
and I ran at Yahoo in 2007 so it's been
a while but it was a seminal study that
still gets a lot of links to it where we
found that we ran this on various Yahoo
sites and we found that about 20% of I
heard crickets
about 20% of sites of pageviews the user
has an empty cache we measured the use
of my empty cache what we mean is the
Yahoo resources that had a ten-year
expiration window were not in the users
cache and we were able to measure that
by putting a transparent image in the
page that had long caching headers and
detecting whether that was requested or
not in correlating it back to pageviews
so we found that about 20% of pageviews
were done where the user did not have
yahoo resources in their cache and
that's not too bad that's maybe what we
would expected but when we translate
that into unique users it turns out that
of our daily users about half of them
came in at least once a day without the
Yahoo's resources in their cache how
could that be and why are these so
different well the answer is the usage
patterns of a site if the typical number
of pageviews per session is five the
user will come in the first time and
have an empty cache within for their
next four pageviews they'll have a prime
cache so they would only register once
when we look at it from the unique user
perspective so even though the number of
pageviews the percentage is fairly low
for an empty cache users really anchor
on a negative experience and so when
that first time they come in if they
have an empty cache or at least if your
resources aren't in their cache they're
going to have a slow page load time and
that's going to give them a negative
perception of your brand and your
website so it's really unusual when we
when we have our the repeat patterns
were very high we had a ten year
expiration window why is it that so many
of users coming back still did not have
our resources in their cache we don't
know the exact answer there are some
things certainly that we can think of
this was a great will jam works at
Google on Chrome and he wrote this about
six months ago about caching he started
looking at some of the stats and he
found out that approximately 70% of
users do not have a full cache so take
the inverse of that it means about 30
users using Chrome have a full cache the
cache size varies between about 150 and
300 20 megabytes depending on your free
disk space for those users who filled up
their cache it only took four hours for
them for the median user to fill their
cache only four hours of active browsing
that's pretty fast and then this thing
about where the cache gets cleared about
7% he found of users explicitly clear
the cache through Chrome settings which
was maybe higher than I expected but
this was a real surprise I think to him
as well in the chrome team 19 percent of
users once a week experienced some cache
corruption that required clearing their
cache so once a week over 25 percent of
users were having their cache cleared at
least once a week
so I think that's one thing that's
attributing to people visiting our sites
and not having things in the cache that
we would have expected to be there
here's another thing as cache sizes this
is a study from Blaise dot IO which is
now part of Akamai talking looking at
mobile caches which are certainly much
smaller than desktop browsers but even
desktop browsers with the amount of
browsing that you do in them will fill
up fairly quickly so caches are small so
website developers web developers are
looking at alternatives one is app cache
it helps with offline as well as having
a longer cache the nice thing about app
cache and I'll talk about local storage
is you get cache that's dedicated to
your website a certain amount of space
five Meg might be a typical amount to
think of whereas the browser cache is
shared with all the other websites so
it's possible so your resources are in
contention for that space with all the
other websites that the user visits so
here's how it works you specify a
manifest attribute which gives the file
name for your manifest file that file
has to start with a cache manifest
header at the top we'll talk about the
comment in a minute and you have these
various sections cache is what can be
saved in app cache network are things
that should never be saved in app cache
you should always pull them from the
network
fall back helps you in that offline mode
if your online
hit index out HTML if your offline if
you don't have network access fall back
to this other website Oh something I see
people taking notes something I should
have mentioned the slides are already up
on my website Steve Souders comm and
also on SlideShare under my account
which i think is Souders and you have to
at least for now return a specific
content type for all this to work so
it's not too difficult but it turns out
when you actually try to get it working
it is really difficult
there are a lot of gotchas even if you
don't list the HTML document in that
network section any HTML document that
uses the manifest attribute is
automatically safe than to app cache so
if you have something like a login form
on the page but the user has logged in
even though they have a valid cookie
they'll be reading that out of the cache
and it will still have the login for a
minute so you have to rethink the flow
of your website which can be very
challenging if anything in the cache 404
is nothing is cached you have about 5
Meg that comment in there is that I
talked about in the manifest file if you
change your resource so suppose you have
an image that's referenced in the
manifest file and you update that image
on that your website the app cache will
never be updated unless you actually
modify the manifest file because the
first thing the browser does is it
checks to see if the manifest file has
changed if it hasn't changed
it doesn't check any of the resources
just to be efficient if you're on a
phone for example you don't want it to
in a conditional get request on all of
the manifest file resources so the way
that it controls whether does that check
is by first checking the manifest file
itself and what this means is that it's
going to take two reloads for a user to
get an updated resource now that's kind
of confusing it tripped me up when I
built my first app cache app so let's
walk through an example here so suppose
you go home you this afternoon you build
a app cache app you push it and it has
this green logo jiff right and some user
loads your app
and when they load it the app cache is
empty so the browser reads everything
off the network it gets the manifest
file it goes to the manifest file it
sees logo gif listed it downloads the
screen logo and saves it to app cache
and then renders the app to the user so
then you go home tonight the next night
tomorrow night and you decide to change
the logo to be orange and you push that
to your server so now you've got a new
logo up there and you even remember to
modify the manifest file somehow so now
the next day the user loads the app
again but what's the browser do the
whole point of app cache is to not touch
the network so touch the network as
little as possible so what the browser
does is it loads that HTML document out
of app cache that HTML document
references logo jiff the browser looks
and says oh I have local gif in the app
cache so I'm good and the one that's in
there is the one that I last stored
there is green so I'm going to use that
one and that's what the user sees it
sees this green one that's pulled from
the app cache even though the night
before you updated that logo the user is
still going to get the one out of app
cache but now what happens is the
browser checks to see if the manifest
file has changed it has so it checks all
of the images and it sees that there's a
new logo and so it gets the orange logo
and it saves it to cache but the user is
already looking at the green logo and so
it's not going to be until the next time
the user loads the app that this is all
repeated and the browser says oh here's
the HTML document it references local
jiff I have that in my app cache let me
pull it out of app cache and render it
for the user and the user sees your
update so that's why it takes two
reloads for a user to see any updated
resources so you can get around that by
tracking this update ready event but
it's more work that you have to do and
you have to figure out what you're going
to do in that case if it's something
like the logo change and they're the
exact same size and won't cause the page
to flash you could just swap it out if
it's like a JavaScript update or CSFs
update that's not that critical or an
image update maybe you do nothing if
it's like a security job
fix you might tell the user we've
updated your vert your local version of
our JavaScript and we're going to reload
the app so that you get this security
fixed so you can decide what to do here
but you have to do the heavy lifting
local storage is really easy it's got a
simple API again it's about five Meg
one warning is that we've gotten from
some browser vendors is it can be bad
for performance because it's synchronous
but recently we'll Cham the same guy ran
this study where he found that even for
the 95th percentile it's only like 50 to
150 millisecond impact on load time so I
think local storage depending on how
you're using it could be fine and not
have a large impact on performance and
we're seeing folks like Google search on
mobile and being on mobile use it to
store JavaScript and CSS I'm running out
of time so we're going to go over this
pretty quickly what they do the first
time you load it is they download a
bunch of inline JavaScript and CSS they
save that to local storage and they set
a cookie for each string or module
that's saved the next time you get a
search result the server looks at the
cookies and doesn't have to download
that inline code again and this reduces
the size of the page from 110 K to about
10 K the one tricky thing is what
they're doing is they're reading these
strings out of local storage and eval in
it and so if you have other third-party
JavaScript on the page they also have
access to local storage equal access to
local storage so if you're going to use
this technique you'll want to wrap it
around some sort of cryptography
checksum algorithm and there's a URL for
a discussion on that topic and the other
thing so that's something else that's
that people can work on or other forms
of web storage but the other thing that
I think we're going to see happen is
smarter browsers we're seeing tremendous
improvement in browser performance
bigger caches smarter purging I think is
coming ie10 I think was the first
browser to take mime type in
consideration
rather have you purged images that purge
JavaScript because JavaScript is more
important to download quickly and
prioritizing websites here's an example
of what I mean by that in IE 9 or 10 I
forget where they added this feature for
preserve favourites website data so
anything that's in your favorites or
bookmarks when you clear your cache you
can say yeah but those are my favorites
don't clear it now I don't necessarily
like the UI here I have thousands of
bookmarked websites they're not all my
favorites I'd rather have it the browser
learn my favorites it already knows the
top 10 sites I visit all the time those
are in the start page right so
automatically give those preferred
caching and I think we're going to see
this oh here's another example in chrome
where there's a lot of DNS resolution
and TCP connection pre connects that are
done depending on what you've done in
the past for example that knows your top
10 most visited sites when you start
chrome it resolves those 10 domain names
automatically so that they're ready to
go and other preconnect behavior like
that and I think that's where we're
going no one knows how you use the web
better than your browser it knows what
you've done in the past it knows the
context that you're in now and I think
we're going to see things like we saw in
chrome with DNS and TCP preconnect we're
gonna see that move into the world of
prefetching resources as well I know for
me every morning I sit down and I load I
have a script that loads 30 websites and
then I go get my coffee and I come back
to because I hate watching web sites
take a long time to load and the browser
will know in the future the browser will
know that behavior and it will know at
that time in the morning Monday through
Friday it should start pre connecting
and prefetching resources that were on
those web sites yesterday that had a
long cache time make sure that they're
in my cache and maybe even start reading
them into memory doing some JIT on them
so I think we still have a long a large
number of optimizations for performance
that we're going to get from
browsers so the takeaways that was
actually pretty good this is a brand new
talk I timed it pretty well get the need
cash
so takeaways gather some stats like if
you read that blog post on the study
tenían I did at Yahoo and explains how
we did it with the transparent pixel run
it on your site how many of you your
users are benefiting from cash or aren't
be explicit about how what you're
caching policy is for every researchers
consider augmenting what you're cashing
you're caching performance by looking at
local storage and app cache and lobby
browser vendors to implement smarter
algorithms bigger caches and more
personalized caching for the user I just
wanted to mention I got almost all the
data for those charts and stuff art off
the HTTP archive which is a part of the
Internet Archive project it's a project
I'm running so check that out
and that's it thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>