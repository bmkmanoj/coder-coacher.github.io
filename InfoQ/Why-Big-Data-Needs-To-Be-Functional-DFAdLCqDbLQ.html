<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Why Big Data Needs To Be Functional | Coder Coacher - Coaching Coders</title><meta content="Why Big Data Needs To Be Functional - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Why Big Data Needs To Be Functional</b></h2><h5 class="post__date">2012-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DFAdLCqDbLQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi there I'm Dean wampler I been doing
big data lately i work for think big
analytics which is sort of a hadoop
oriented consultancy and this is mostly
a rant about what's wrong in that
industry so the first thing i should
probably define is what i mean by big
data and there's it's one of those
buzzwords that all the suits are
throwing around and the throwing money
at it but for me what it really means is
that's data that's so big for some
definition of big that traditional
solutions you know relational databases
whatever either they're too slow they're
too expensive or maybe even too small to
handle the computation of course great
examples are like you know all of
facebook's privacy data that they know
about you and you know Google's indexing
of the web and so forth so those are the
kind of the things that's it apart big
date and have kind of forced us to go to
to new solutions and you know the
biggest one right now that everyone's
talking about is this open source
project called Hadoop and just briefly
to explain what that's about it's it's
really two major pieces one is this so
called MapReduce engine for distributing
computation across the cluster where you
break up a job in two tasks and those
tasks or oriented around a map or phase
and a reducer face inspired by the map
and reduce concepts we're used to from
functional programming but somewhat
bending the rules a little bit and then
underneath is like a distributed file
system so you get replication and also
distribution of computation and that
sort of thing basically I see three
trends kind of happening in this
industry that are relevant to this
discussion and the areas where I think
we're letting too many old ideas
influence the thinking rather than
embracing mostly functional programming
and certainly languages like Scala to
make these industries work better you
know for us as developers and the first
one is the one I already mentioned
obviously data sizes are going up
exponentially they say although of
course nothing can grow exponentially
forever certainly the big guns like the
Twitter's your twitter is your facebook
so those kind of people
obviously got a lot of data but even
smaller companies now are recognizing
they have some useful data they might be
drinking the kool-aid a little bit about
what they can get out of that data in
terms of useful information but
nevertheless they're concerned about
making better use of what they have
because sometimes it is a really useful
reusable asset and if it's essentially
free for some definition of free to keep
all that data and to play with it then
you never know what will come out of it
I'll give you an example of a project I
worked on recently a major networking
company in in Virginia wanted to keep
the traffic that they accumulated over
the past year so they could go back and
do retrospectives when customers
reported problems they could replay what
happened and discuss the problem they
could do long-range reports and trending
stuff that was much harder with their
current infrastructure the other big
trend though is that the notion of
formal schemas you know really kind of
nailing down the model of your data is
kind of becoming much more relaxed
because we realized that in this chaotic
world wherein we have to be flexible
about data structures changing from day
to day trying to integrate despaired
data types and so forth so people need
more flexibility and less rigor in the
usual way that we used to do things of
modeling it all up front and knowing
exactly what we have and then having to
shoot a DBA if you want it to make a
schema change which reminds me of the
joke you know what's the difference
between a DBA and a terrorist you can
negotiate with a terrorist but I think
another cool trend that you've probably
heard a lot about is the whole sort of
rebirth if you will of artificial
intelligence and machine learning
obviously a lot of the stuff that's
happening in particular the idea that
you could get gold out of your data is
kind of driven by machine learning this
is a picture of Stanley the the car that
was built by Google and some researchers
at Stanford that was the first
self-driving car that negotiated a
desert maze as part of the DARPA Grand
Challenge a few years back and I think
this a car is now in one of the museums
in Washington DC
but I mentioned this in particular
because there's some sort of traditional
machine learning algorithms that people
are using for doing things like how do I
recommend the best movie if I man you
know if I work for Netflix how does
Amazon decide that now you bought a book
on on Scala you might like a book on
closure maybe you know that sort of
thing but I think there's also some
interesting stuff that's happening with
artificial intelligence like for example
these cars they might have maps of the
world that they think they're in but
they also have to do a lot of highly
iterative highly data-driven algorithms
to actually figure out where they are on
that map and there's some really cool
techniques that they're using to do this
and I can see that some of these
techniques will bleed over into some of
these more traditional machine learning
problems where we sort of more
dynamically figure out the information
that we want you decide what to
recommend to customers or or how to
react to denial of service attacks or
potential fraud events and stuff like
that and it's also kind of driving us
more towards a model of less structure
in our models and more flexibility in
being data-driven and using some core
structures and algorithms that can just
react to the data to create the kind of
emergent behavior that we want I did the
online Stanford AI course this past year
that was taught by a Sebastian Thrun and
I forgot the other guy's name anyway but
in one of the lectures they made the
point that data driven programming like
this is sort of the ultimate kind of
agile programming where you don't really
hard code a lot of business knowledge
into your code you let the data kind of
drive the behavior and I think this is
also pushing us away from things like
object orientation and modeling and more
towards more fundamental data structures
fundamental algorithms and and building
up immersion behavior that way so a lot
of us have built applications that sort
of look like this in some schematics
sense where we might actually be
querying a database you know with the
sequel through some method but then we
turn around and pipe it through some
object relational map and ORM
turned into this grandiose object model
in memory and then do other stuff with
it and I argue that this is totally
unacceptable in a big data world because
there's way too much overhead here as
well as this lack of flexibility I was
discussing that really we want something
more like this where we have this grand
and glorious results that that's you
know a nice set oriented data structure
with relations and all this stuff why
are we kind of hiding it in some ad hoc
model of the world why don't we just
work with that data as God intended I
guess you might say so we might have to
do something to convert it into Scala
data structures for example if your
JavaScript programmer and you're using
MongoDB you don't even do that much you
just take what comes right out of the
database and it's already in Jason but
nevertheless you might have to convert
it to our our core data structures
anyway and then we can start doing
domaine logic stuff with those kind of
data structures instead of our ad hoc
models and kind of another view of this
would of applications we probably all
built is that when you have this rich
domain model and objects is there's
really kind of a strong tendency to put
it all into one kind of big application
and then have all of these feeders going
into it and out of it you know various
web clients and various data services
and so forth and the problem with that
model in my view is that it's it's
harder to scale horizontally it's less
reusable it just becomes this big wad of
code that unless we're very very
diligent about keeping it well factored
and all that stuff it tends to
agglomerate and become harder and harder
to maintain and harder and harder to
evolve and instead we need to focus more
on this you know small highly focused
processes that can be glued together and
I think a lot of the technology that
we've learned and developed in
functional programming and in skul in
particular like actor models and the
reliance on fundamental data structures
like lists and sets and maps and so
forth really makes it much easier to
implement stuff like this and that gives
us the kind of flexibility we need not
just for big data but for just
general data problems I often think back
to a project I did this would have been
the mid-90s when I was working for a
company that was built tools for
embedded systems developers and I spent
like two years working on a new UI and
C++ for this embedded system tool pretty
much disconnected from reality and
eventually the market left us by and
that there was a failed project but I
can't imagine spending two years on
anything anymore I can't imagine staying
in the same job for two years anymore
things move so fast and for me having
the flexibility to just get it done and
but to do it in a clean elegant way with
some fundamental tools is very
compelling so let me talk briefly then
back to those three points just to be
clear about how I think this affects
these trends well if data size is going
up and then despite the fact that we
used to have Moore's law and we sit
still sort of do at least if we scale
horizontally and we have data disk size
or you know just drive sizes and keep
increasing the data just seems to be
accelerating faster than that and we
just really need to keep our processes
as lean and mean as possible and I think
if if you come from like the agile
community there's this great catch
phrase what's the simplest thing that
could possibly work and you know these
days I don't think it's passing it
through an ORM and converting that
relation that results set into an object
model I think it's working with the data
as God intended and getting it through
the system as fast as possible and it
also gives you a lot more flexibility if
i'm passing maps around that's a lot
more flexible than a class design that i
have to tweak every time I have a new
concept enter my domain and also this
idea of data-driven programs it's a lot
easier to connect algorithms and core
data structures things like you know
just the simplest examples are like our
flat maps and our maps and are you know
for each's and all that stuff they just
work across all of these core data
structures that have these fundamental
concepts rather than having to
re-implement those wheels every time we
build an object model so let's get back
to kind of the key
the point of this talk which is
specifics of big data and I want to talk
a little bit about using Scala in to do
MapReduce programming in the Hadoop
environment and what I thought it would
do is very quickly walk you through the
hello world of the MapReduce ecosystem
which is the so called word count
algorithm and the idea is very simple i
have this corpus of documents on the
left and i want to count I want to first
find all the words in that documents and
then I want to count all the occurrences
of those words it's a very simple
algorithm not actually useless that
there is some use for this kind of thing
for example if you're building like an
automated translator language translator
if you know word frequencies that's a
clue as to what language you're looking
at when I throw a document at you and
you don't know what language it is
obviously the word does going to show up
a lot more in English than it will in
Chinese for example not to mention the
character encoding is a little different
there but so let's just hope that went a
little faster than I expected let me
back up so i have like say for documents
one of which happens to be empty for
reason of i'll explain in a minute and
each of those will get in a mapper task
and hadoop terminology and this is all
key value based stuff so what will
happen is the mappers will receive
because this is where the infrastructure
works they'll receive each document
maybe with the document name and we
actually don't care what the key is in
this case and then the contents of the
document the whole contents and notice
it for document three the contents are
empty and then the next phase is the
mapper will will output well first it's
going to tokenize the documents in some
useful way in this case they have it
convert to lowercase and then the most
naive form this algorithm is that every
time it sees a word it will output a key
value pair which is the word is the key
and the count of one is the value so you
can already see there's some
optimizations you can do like not
spewing the network with you know the
same word over and over again but we
won't get into those details that's not
important and then there's this sort and
shuffle
that happens where the keys are sorted
and then some algorithm is used to
distribute these key value pairs to the
so called reducers and the main
requirement here is that for any given
key all values for that key will always
go to the same reducer you know I don't
want the word Hadoop showing up on three
different reducers so I have to either
let the default algorithm figure that
out or in this case I hacked up a custom
algorithm where all the keys that start
with the numbers 0 through and 9 and the
letters a through L will go to the first
reducer and through Q to the second and
so on and so you can see that just to
show the first example out of the first
mapper you'll get the key and value pair
Hadoop one MapReduce one uses one and so
forth down the line and of course now
you see maybe why I had the third
document in there the word map is
generalized here it's not a one-to-one
map and it's really a flat map it's a
zero too many mapping but they but I
guess flat MapReduce would not be as
elegant a name is MapReduce or something
like that so anyway it's not our usual
mapper but close and actually what you
get inside the reducers then is the key
and then some sort of collection of all
the values arraylist i think is actually
the default implementation and in this
case again we're counting words so all i
need to write out at the very end then
is each word and then the total count
and you can see all i have to do is some
the elements in the array and usually
this is tab delimited output by default
so that's word count that's the that's
the hello world program you always do
first so let's actually look at what
this is like to do in the Java API which
is essentially the assembly language of
the MapReduce world and then we'll talk
about doing it in Scala so I actually
use Scala syntax and I'm sure all of you
guys in the nosebleed seats can read
this very well but and it's obviously
deliberately too small to read and the
tragedy is that yeah I didn't want to
offend your sensibilities but know that
the tragedy is that this algorithm is
actually not so bad in MapReduce there's
still a lot of noise here that I'll talk
a little bit about
but as soon as you get to anything more
complicated like doing joins of data the
other typical database join kind of idea
it gets heinous ly ugly very fast and
one of the things that's ugly about this
is and getting back sort of to the
general point no doubt you can read
where it says like long writable text
extant writable those are like the input
key values and so forth and yet why
isn't it what's this long writable
what's this text thing wife and it just
Long's and strings and stuff like that
that we're kind of used to well this is
one of those examples where the API
bleeds heavily into the code and you
have to use their own custom
serializable objects and you can't use
your native types because it's not
abstract 'add and so when i started
reading this code i had flashbacks to
the early 2000s and ejb 1 and 2
programming and i'm sorry to bring that
up i know that's a bad memory for many
of you but unfortunately this is a very
invasive api and it kind of breaks a lot
of what we've learned about good API
design in the last few years and as much
as many of us like to criticize spring
for example because we think it's kind
of gone a little overboard at least some
of us might think that nevertheless
spring did us a great service around you
know two thousand five or so by showing
us a much more elegant way to decouple
the framework from the application logic
and I think that's what needs to happen
here so let's actually do that now I'll
start with a higher level API that
abstracts over some of that gory detail
called crunch and it's actually based on
a Google paper that came out a few years
ago about a google project called flume
java that it exposes an abstraction of
like parallel collections and workflows
and you know pipelines of data and the
sort of way that you would typically
want to think about the kind of problem
that you're doing you know not this
low-level MapReduce stuff which is not
really the core essence of what we're
trying to accomplish and just to briefly
give you a sense of what some of the
concepts are in crunch and these are the
actual Java types we've got we've got
your pipelines you got your basic
parallel collections and then you've got
these parallel do
Oh things which is don't we already have
this in Scala why are we re implementing
the wheel I don't want to criticize
scrunch actually crunch because this is
actually a really nice tool kit and the
guys that wrote it did a great job but
it's really a criticism of languages
that don't really understand data that
they have to in the developers are
forced to invent concepts and and obey
and always do it in their own unique way
that should just be baked into the
thinking of the language and the
libraries as well but anyway we've got
these parallel collections we've also
got parallel tables distributed key
value stores and then we've got more
operations and custom types like
grouping tools that we already should
have in our language and then finally
there's even yet more types that
represent the output of groupings and
stuff like that and by grouping I mean
like I want all the words the just to
show up together and all the Hadoop's to
show up together and that kind of thing
so there's all this custom stuff that
the crunch team had to invent but it
does give us a slightly cleaner API and
this doesn't look that much smaller than
the last example but in fact it is
conceptually easier to see what's going
on there's less noise in this API and
when you scale up to bigger more complex
job flows data flows that actually kind
of adds up to be much cleaner than Java
but again for those of you that can't
read what's on here you'll see things
like their own custom you know f2 or
rather f1 that takes a string returns
the string and as usual it's an
interface you have to instantiate it
define all the functions in line the
word process would be the word apply in
our case and so forth so there's a lot
of boilerplate still because it's a
language that doesn't understand data
and the kind of things you want to do
with data but fortunately the crunch
guys wrote a scala api a DSL on top of
it called scrunch because you know ever
because all scala api's have to start
with SC or s at least it's it's a law of
the land oh and actually I wanted to
point out something I forgot to mention
so bear with me as a backup for a second
so notice all the green on here all the
grain are types
and all the yellows are function calls
so even if you can't read the words and
I deliberately did that not to be nasty
but notice how much green / yellow there
was and now look at the Scala version
where they're very few types not just
because it's an inferred language there
there really wasn't that much in the
Java case either if I well sorry back up
if I had rewritten that previous example
which was Java syntax if I had written
it Scala would still have a lot of type
information but now we've almost
completely eliminated the types down to
a few things that are inferred and
instead we have this sequence of
transformations this builder pattern
where we're we're flat mapping that
stuff you know here and then we're
filtering is that empty will get rid of
those we're counting things this is
basically our word count again and we
have a not only a much cleaner API with
less ceremony and boilerplate in our
face but something that actually fits
the computation we're doing much more
cleanly and actually this code the
number of lines here looks about the
same as a previous one but I cheated a
little bit I actually added stuff that I
left off the previous slide like the
main routine so this is in fact the
entire program except for a few of the
imports that I omitted all right and it
turns out that there's other toolkits
cascading is actually a little better
known and if you use Casca log or
scalding which is a new twitter api you
get the same transformation from lots of
boilerplate down to here's the essence
of the problem so this is not anything
new to the rep to all of us in this room
but basically skull and FP are just
natural fits for for data computation so
i want to leave then with a manifesto
which is that we as a community really
need to maybe be more active in in
bringing our brethren over let's say to
the bright side so I believe that Hadoop
is the enterprise javabeans of our time
and as such there's something out there
that's coming down the pike that's going
to be the spring framework of the Big
Data world it's going to
make people rethink the way they write
programs and do so in a much more
elegant way but we have the tools at our
disposal to make this happen so we
already have an extremely powerful
collection API and we already have some
support for distribution you can
paralyze the collections now and I think
the next logical step is to make them
distribute it in some way and maybe the
best way to do that is actually with
acha because it already handles a lot of
the infrastructure requirements for
building distributed computation and
managing resources and managing tasks so
this is sort of a manifesto in an
invitation for us as an organization or
of people I guess to embrace that and I
hope that we will and then the last
slide is a shameless plug it turns out
my book is half off today the e-book if
you go to the O'Reilly site and you want
it's like eight and a half bucks so what
a deal come on come on do it help a
brother out all right all right anyway
that's it for me questions yes the
question is just discussed briefly the
differences between cascading and
scalding and those api's versus crunch
as a matter of fact I was thinking I
would use examples from cascading and
scalding instead but because of times
sake I wasn't able to prepare those but
the the programming model namely things
like pipelines of data and forking
pipelines and grouping they're very
similar I actually like the the scalding
API a lot better I think it's a lot more
elegant and more concise in the long run
but the core concepts are very similar
and another advantage of scalding and
cascading would be that because it has a
larger user base in a larger community
it's probably going to be better
supported and better optimized one of
the benefits of all of these high-level
toolkits and even higher level languages
like hive and pig that some of you may
have heard of is that the developers can
spend a lot of time optimizing
computationally difficult stuff like
joins or classic case where there's a
lot of tricks you can do with joins
depending on the
of the data to make them much faster
than they would be naively yes sir yes
the question was the night when they
make this scurrilous claim that a dupe
is the ejbs of our time am I talking
about like if I can paraphrase like the
api's or the core concepts or the
ecosystem it's I think it's a little bit
of all of them certainly there's already
people are raising issues with the
general MapReduce model it isn't
flexible for some things it's not
optimal for graph traversal google has
already started doing other work there's
a paper they came out recently talking
about is a new system they have called
prego and there's actually an Apache
project that does something similar
called hama h ama which is the model
they're both using is something called a
bulk synchronous parallel processing BSP
which is essentially a model of
traversing through a graph like one
clock tick at a time and updating the
graph globally one step at a time it's
not unlike the way that a lot of older
multiprocessing systems would work were
you if you think about like if I have a
and in fact like another good example of
this effect or like climate simulations
and other numerical simulations where
you put things on a grid you might
update every element of the grid at once
with all the messages that just received
and then it sends out messages and so
everything synchronous and updated at
once so I think we'll see both
fundamental models emerge that are
better choices I think the API itself if
you've ever looked through the code I
mean these are all very talented
developers and they do very good work
but it's just full of incredible
boilerplate that would probably if they
had done it in Scala or even closure any
functional language it would be
dramatically smaller and dramatically
more efficient probably because they
could leverage some of the things that
they just don't know about I think
that's essential problem so I really
think it's sort of at all levels yeah
but in the back yeah so basically the
question was pointing out a disparity
with sort of the model that collections
and and some other distributed systems
use where if there's a failure at some
point it just throws away everything and
you start over whereas Hadoop when it
breaks a job into tab
if a single test fails it will try to
keep rerunning it up to a certain point
but keep the rest of the work so you
don't have to start from scratch I think
that's that's the right model that you
you try to recover as best you can and
that would have to be addressed in real
model or anything that replaces it let's
say so I don't see a direct one-to-one
mapping necessarily between you know
like say distributes Colin collections
and MapReduce or he do / say I really
think it's more long going to be along
the lines of a replacement system that
has to grow to address those problems
yessum vector yeah so the question was
that an attraction of Hadoop is that
lets Java people do distributed data
programming that normally maybe not
would have been able to use other
frameworks and that's certainly
legitimate out just a little backstory
on me I'm actually running our training
program at think big right now and so I
train Java developers and I train people
in these higher-level languages like pig
and hive and I do try to teach the Java
developers look I know you've always
hated sequel and you've always hated
dba's but you really ought to embrace
you know hive which is the sequel
dialect and pig which is a dataflow
language because they get rid of a lot
of the boilerplate and then only jump
down to Java when you need to but that
is definitely though a general appeal of
the seco system and one of the benefits
of it like one of the legitimate
benefits of VJ bees was that they hide
it a lot of the complexity of
distributed programming from you and
took care of a lot of stuff like task
coordination like avoiding concurrency
or multithreading I mean in particular
so yeah that's definitely an attraction
I just think we could do it in a lot
better way and I think I'm at a time so
I'll be happy here all week don't forget
to tip your waitress and thanks for
coming
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>