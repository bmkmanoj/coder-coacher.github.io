<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From Concurrent to Parallel | Coder Coacher - Coaching Coders</title><meta content="From Concurrent to Parallel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From Concurrent to Parallel</b></h2><h5 class="post__date">2017-07-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tjnGtDbU8l8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's great to see so many people in the
room today I want to talk about today
about parallelism and particularly about
how to think about parallel performance
so the you know the Java class libraries
have supported the ability to do
parallel decomposition for the fork/join
library for quite a few years now but in
Java eight we added something that makes
it a lot easier to use parallelism which
is the streams library and I think most
of you have used streams before right
great so what I'm going to talk about
today is not what streams is but how to
think about parallel performance and
whether parallelism is going to help you
or hurt you
streams makes it easy to go back and
forth between sequential and parallel
execution but we still have to think
about which we want and so that's
basically the topic of today's talk
that's right so this is how you know I
work at Oracle I have to give this
disclaimer slide not an imposter real
Oracle employee C legal slide ok great
so everyone's probably seen this graph
right so this is a graph that herbes
that are put together for almost 10
years ago at this point and it shows a
number of trends in in processor
hardware so the green line this is
plotted on a log scale so the green dots
show number of transistors on a chip
over time right so you know what since
this is a log scale a straight line
means something that's exponentially
increasing and this essentially says yep
Moore's Law works it's continued to work
over a relatively long period of time
transistor counts are consistently
doubling every 18 months but what you
know what changed and you can sort of
see that from the the lower graphs is
what we got out of those additional
transistors so around 2002 2003
something very unfortunate happened
which was hardware engineers were unable
to plow that transistor bounty into
giving us just faster and faster chips
so for the longest time we were able to
make our programs faster just by
dragging our feet and waiting to run it
on the next generation of hardware that
was a great trick for like 25 years
unfortunately that trick doesn't work
anymore and so we have to like actually
do our jobs now and this is demonstrated
by these lower three lines the you know
the the blue line represents clock speed
so we saw a clock speed increasing
exponentially for
years and years and years and then it
leveled off and it's pretty much stayed
where it is
the the lower two measures are other
measures of performance the light blue
line is power consumption so as chips
got faster they were consuming more
power and we basically hit the limit of
how much power we're willing to feed
into chips for you know for cooling
cooling reasons mostly and the bottom
the bottom line the you know that the
purple line is an interesting one it
shows a measure of parallelism inside
the chip how much work were getting out
of a chip for a clock cycle and again
hardware engineers were able to do
really cool stuff in terms of using
parallelism within the chip to give us
faster performance and that game played
out you know in 2002-2003 timeframe as
well so all of these all of these forces
basically conspired to say chips aren't
getting squiggly faster it took from
2003 to maybe two thousand seven or
eight to get over the period of denial
that followed we were kind of all hoping
the good times are coming back but they
weren't coming back
but it doesn't mean that Moore's law has
been repealed we're still getting more
and more transistors every year so what
are we doing with them we can plow them
into more cores on a chip
rather than faster cores so we're still
getting better chips but we we have to
work harder to use them because we have
to figure out how to paralyze our
software to run on many core chips so if
you look at the chips that the that the
electronics industry is actually
delivered if if we went by what Moore's
law said the number of cores we would
have would follow this red line but in
reality you know the blue dots indicate
high-end ship core counts you know these
are the Intel e^x you know Xeon ships
they are increasing but not nearly as
fast as Moore's law says they can
increase and the basic reason for that
is economic no one wants to buy million
core chips because we don't know what to
do with them
so or the set of applications that know
what to do with them is small enough
that there isn't really a market for it
so instead of seeing thousand core chips
today we're seeing you know high-end
chips with 20 40 60
you know cores but now
not thousand two thousand four thousand
quarters and of course you know consumer
chips have even lower core counts than
this so we you know the multi-core age
is here but not as dramatically here as
it was predicted it would be you know
ten ten years ago and I think that's
largely because of us there isn't demand
for these chips we don't know how to use
them effectively
okay so concurrency isn't a end in
itself it's a means to an end right as a
means to better performance and in
particular it's a means to better
processor utilization now what that's
what that means has changed over time
but for the most part we tend to use
concurrency as a way of getting more
performance out of the hardware there
are some programming models that take
advances concurrency actively for
structuring a program like actors but
that's still sort of in the fringes for
the most part we view concurrency is a
tool for squeezing more performance out
of the hardware and the strategy you
know that we use changes over time as
the hardware changes so back in the
single-core era we use concurrency to
get asynchrony to do non-blocking
operations do do useful work while we
were waiting for an IO to complete
you know when ships had a few cores we
tended to focus on sort of coarse grain
task based concurrency so we had thread
pools where you'd be running you know a
handful or a few dozen tasks
simultaneously and this was largely
about improving throughput you know
processing more requests per second out
of your you know fork or server box as
core counts increase the another target
becomes available which is using
parallelism not to do more work on the
same box but to get to the answer faster
so throw more cores of the problem in
order to get to the answer in in a
smaller amount of wall clock time
because maybe having the answer faster
has business value right so the
techniques we use sort of follow the you
know the hardware trends and and and
this makes sense right that you know the
hardware guys cook up some cool new
hardware stuff and then the software
guys have to figure out how to use it
effectively and that starts with the
operating system and that works its way
all the way up the stack you know
through the JVM through the language
through the libraries through the
programming model
and so you know the the hardware is out
there on the leading edge and languages
libraries and frameworks are kind of
trailing and so you know in 1995 you
know it was a fairly you know fairly
cool that Java had built in support for
threads and locks and conditions queues
built into the JVM built into the
language that was kind of a new thing at
the time and that was relevant to the
concurrency that was available from the
hardware of the day when we started to
see multi-core systems that's when the
Java libraries acquired tools like
thread pools and blocking queues and
concurrent collections and when we
started to see more cores that's when
the library is acquired
you know tools for prefer test based
parallelism and you know as the as the
hardware is continued to improve we try
to make it easier to access the power of
the hardware okay so I want to spend a
second on terminology people often use
the words concurrency and parallelism
interchangeably they're not the same
thing and unfortunately the historical
meanings of concurrency and parallelism
you know they're grown up in the
literature I don't think are actually as
useful today as they were when they are
originally coined and so a lot of modern
curricula are sort of redefining these
terms which makes even more confusing so
the historical meaning was that
concurrency is a property of a program
structure whether things could be
happening at the same time whether your
program is structured as cooperating
activities whereas parallelism was
historically framed as a property of the
program's execution do things actually
happen at the same time or are you just
doing fancy scheduling tricks so in the
old model concurrency was the potential
for parallelism and this was a useful
distinction in the older days when true
concurrent execution was mostly a
theoretical construct but I think it's
less of a useful distinction today so
the distinction that people are you know
and that educators are using today is to
say that concurrency is about correctly
and efficiently controlling access to
shared resources so this is things like
constructing thread safe data structures
like concurrent hash map and the
primitives of concurrency are things
like locks
and semaphores cool routine software
transactional memory etc it's about
coordinating access to shared shared
data whereas parallelism is more about
using additional resources to get the
answer faster I could do this with one
core but if I had ten cores to throw at
it maybe I can get it faster I've had a
hundred course to throw at it maybe I
could get the answer even faster so all
right this may sound like an academic
digression why should we care about this
distinction well the reason the
distinction is relevant is it turns out
that concurrency is usually pretty hard
that the tools that we have for
concurrency like you know locks and
threads requires complex reasoning
that's like easy to get wrong you know
and even if you have like you know the
secret wizard spell book it's still hard
you know to get it right whereas
parallelism tends to be really easy at
least in terms of the way you think
about it because parallelism that the
standard trick and parallelism is
partition the problem into two smaller
problems and then work on the problems
independently and then when you're done
combine the results and this is how you
know we effectively you know share work
in the real world right if you've got a
stack of papers to grade well you call
in your teaching assistants you know you
give them each you know one ends of the
other stack you go you know you go to
lunch and let them do the work right and
they don't need to coordinate with each
other while they're grading the papers
and then you know the end they just make
a big pile and and you've got your work
done very efficiently so it doesn't
require a lot of reasoning as long as
you partition the data and partition the
work into independent tasks so
parallelism tends to be much easier than
concurrency people are scared by
concurrency rightly so cause it's hard
parallelism doesn't have to be hard so
it's worth noting that parallelism is
strictly an optimization it's about
using more resources to get the answer
faster if you don't have additional
resources you just have one core to work
on no problem you can still compute the
answer sequentially it just might be
slower now the flip side of parallelism
being an optimization is that it's only
useful if it is actually an optimization
which is to say it actually gets you the
answer faster and it isn't necessarily
the case that using more resources gets
you the answer faster
or even as fast so there are some
parallel computations that are slower
than the the obvious sequential
alternative so that means we have to
think about it unfortunately but it's
not hard you know we have tools we can
you know we have analysis we can look
the problem think about the problem we
can implement measure test repeat etc a
general rule of thumb this is one of the
questions that like oh that comes up on
Stack Overflow all the time is like ok
Java has parallel streams should I use
parallelism everywhere know start with
sequential streams and then if you need
to optimize your code because you're not
getting to the answer fast enough where
fast enough decided defined by actual
performance targets and business
requirements then you can think about
whether making it parallel will actually
get you a speed-up and the way we
measure the effectiveness of parallelism
is by speed-up how much faster is the
parallel execution or slower than the
sequential execution okay now it's worth
keeping in mind that a parallel
computation is always going to do more
work than the best sequential
alternative because it still has to
solve the problem and it has to do extra
stuff to manage the parallelism split up
the work hand it out to workers wait for
the workers to finish combine the
results you know that's extra overhead
on top of actually solving the problem
so the parallel version always kind of
starts out at a deficit against the
sequential version and you're hoping to
make up for it in volume right so if you
have you know 20 papers to grade going
out and finding 20 people to do the work
talking them each into doing you know
doing one paper probably takes longer
than grading them yourself you know with
so you have to have enough enough volume
and you have to have a parallelizable
problem and you have to have a good
parallel implementation in order for
this to win right so what most of this
talk is about is about looking at a
problem and developing an intuition
about whether parallelism is likely to
help you or not of course you still want
to measure on top of that but a lot of
problems you can look at and say that's
not going to paralyse because
not enough data there's you know an
inherent bottleneck here etc also I'll
go through all the examples of things
that uh you know that can deprive you of
parallelism so as an example uh you know
to start with you have to have a problem
that's parallelizable to begin with some
problems are inherently sequential by
their definition so here's a pair of
functions G of 0 you have some operation
F okay and then you're going to find G
where you're saying G of 0 is just F of
0 and G of n is f of G of n minus 1 ok
so and similarly I have another you know
another function whose definition looks
almost the same but actually has
different parallelism characteristics
and I'll go through it but it's worth
pointing out that even though they're
both defined recursively they have very
similar structure the left one is
inherently sequential and the right one
is not and and and I'll show how that
works ok so let's take the first one
parallelism is going to be a lost cause
on this because if I rewrite my function
you know G as an iterated function what
I realize is that G of n is basically F
of F of F of F of F of 0 with n
applications of F and I can't start
computing F of F of 0 until I've
computed F of 0 right the problem is
fundamentally sequential and if I draw a
dataflow dependency diagram well G of 0
depends on F of 0 G of 1 depends on G of
0 you look at that and you say that's
just not going to paralyse right so some
problems just don't paralyze and so it's
worth recognizing that if you're lucky
you try to implement this in parallel
will be no slower than the sequential
implementation that's the best case for
a non paralyzed bull problem all right
so let's look at our other example which
looks almost the same especially if you
don't like math and all you see your FS
and G's and recursive definitions and
all that this problem turns out to be
very parallelizable because if we draw
the dataflow dependency diagram we see
ok H of 0 depends on F of 0 H of 1 will
have depends on H of 0 and it also
depends on F of 1 but
can calculate F of 1 and F of 0
separately because they have no you know
they don't dependent anything and at
each level of the each level of the tree
there's I can get some parallelism
because I can be calculating all of
these s simultaneously right so if you
if you draw out the data flow diagram it
gives you a very quick hint as to
whether the problem that you have is
going to be amenable to parallelization
or not so if you rewrite H you realize
that H of n is just adding up a bunch of
independent things and addition
paralyzes nicely you know and and
computing those F of 1 F of 2 also
paralyzes nicely right so there should
be some ability to extract parallelism
from this problem in fact this is what
we call an embarrassingly parallel
problem but it is possible to write the
solution in such a way that we get in
the way of extracting the parallelism so
step one is you have to have a problem
it's parallelizable step two is that you
uh you also have to have an approach for
extracting the parallelism ok so like I
said these two functions look similar
one was parallelizable and one was not
so let's look at how we might actually
Express the computation of this two you
know to exploit the power possible
parallelism and the key the key takeaway
in this section is we all have some bad
habits we have some bad habits from
having written sequential code for 10 20
30 40 years depending on how long even
programming and we have to unlearn those
bad habits in order to effectively write
parallel code ok so again let's take a
simple problem this is basically what
the H function is add up the numbers
from 1 to n so you know what kind of
data flow graph do you know what do we
get out of that well let's let's write
up an implementation here's a sequential
implementation where we have a you know
some counter and a for loop where we you
know where we increment a you know our
counter well if we if we write it out
this way
we're not calculating the sum you know
we're essentially calculating the sum
sequentially and so we end up with a
computation tree that looks looks like
this you know something like this now
that's not the computation tree we want
but that's the computation tree we get
out of a typical typical sequential
implementation the computation tree we
want is something that looks more like
this where you do a bunch of additions
in parallel and then the next level up
and the tree you do more additions in
parallel and it's only when you get to
that last operation where you have to
two pieces to add that you end up being
sequential right so one of the patterns
you know that we have to unlearn that
getsome gets in the way of writing
parallel friendly code is this
accumulator pattern right as soon as you
start with in sum equals zero you're
already dead right because you've
already created a bottleneck that every
task is going to have to contend for
access to that sum so whenever you see
code like this that should be a warning
sign this is the we call this the
simulator anti-pattern you know this is
going to get in the way of parallelism
so all right well how would we solve
this problem you know not sequentially
well we could try it with concurrency
you know here's a a broken example here
but it illustrates you know part of the
challenge of having this accumulator
variable where we say okay I'm going to
whack the array in two pieces so I have
a left half in a right half and I have
my sum accumulator and then somehow
concurrently magically wave you know
sprinkle the magic concurrency dust I'm
going to you know sum up the first half
and the second half of the array
concurrently
and they're both trying to update the
same accumulator variable what's going
to happen here right this is broken
right because we have a data race both
both of these concurrent activities are
trying to both read and write this
shared shared accumulator and you're
going to get the wrong answer okay well
we know how to fix that right we can you
know make sure that we do this
atomically and how do we do this
atomically well we take a lock well that
kind of makes the code correct but our
performance is going to be even worse
than the sequential version because now
every time someone goes to add a number
to a counter they have to take a lock
and well it's almost guaranteed that the
other guy has the lock so they're going
to spend a lot of time waiting for the
other to give up the lock and it
actually takes a fair amount of time you
know to to release a lock to another
thread so you have this tiny tiny amount
of work to do you want to add two
numbers together you take a lock which
like Dwarfs the amount of work the
amount of time it takes to do the
addition so we have a solution that's
correct but is like actually worse than
the sequential version so all right so
where are we well we had a problem that
was embarrassingly parallel but we tried
a couple of ways to exploit the
parallelism and we failed all right well
let's keep trying you know if at first
you don't succeed right so but I'll step
back
the fundamental problem you know with
the the broken examples is shared state
the first version was broken because it
didn't coordinate access to the shared
state the second version sucks because
it coordinated access to the shared
state but we lost all our performance so
there's you know three ways to safely
handle shared state either coordinate
access using walking don't share or
don't mutate generally the first two
don't share or don't mutate are a lot
easier to get right then than the third
so let's try the don't share approach
let's do the same thing we did before we
partition the array into two chunks will
operate on them separately but we won't
have any shared state right so here's a
first cut at a parallel solution it
looks almost like the other one but this
one actually works because the first
task is busy incrementing the left some
the second task is busy incrementing the
right some they're not they're not
accessing a shared counter so there's
there's no need for coordination there's
no contention we get the right answer
and then when it's all done we add the
two together and everything's good
so this is you know this is basically
the game with parallelism where you want
to divide the problem into generally two
pieces you want to have each piece work
on their part of the problem
independently without having to access
either shared inputs or shared outputs
and that way they can each work
efficiently on their part and you
get and get to the right answer so
decompose the problem into subproblems
solve the subproblems combine the result
hopefully with no sharing and no
contention so alright how's this going
to perform well if we have enough data
then we'll overcome the overhead of like
forking off these concurrent tasks
waiting for them to finish and joining
them and we'll probably get a pretty
good speed up you know if we have if we
have two cores we probably won't lose
very big if we have one core it'll just
run the tasks you know what you know one
ahead of the other but the only thing we
lose is the fixed overhead of dividing
it but given the structure of this
program it's pretty obvious that if we
had 100 cores we're still only going to
be able to use two of them effectively
so we've made progress potentially we
could double our speed with enough data
but we'd like this to be able to scale
to mini course right so let's see how we
would you know how we would extend that
so the basic game for parallel execution
is you know the old track divide and
conquer you recursively divide your
problem into smaller problems so you
take a big problem divided into smaller
problems if those smaller problems are
still kind of big you divide them again
into smaller problems and you keep doing
them until they're small enough that the
logical thing to do is to solve the
sub-problem sequentially right so if I'm
asking you to add up two numbers you're
not going to fork off two tasks you know
you're going to say adding adding these
numbers there's so little work that I'll
just do it sequentially so when you get
down to a small small data size it's
generally more efficient to do it
sequentially so recursively divided the
problem until the chunks are small
enough solve the chunks concurrently
hopefully whether you know they're
independent and then combine the results
so this is the pseudo code is the
problem small if so solve it
sequentially otherwise concurrently
solve the left sub problem and the right
sub problem and then combine the results
this this is basically all parallel
programs have this behavior the nice
thing about this is it's simple right
how many cores I had wasn't an input
into that code oh so it wasn't input
into writing the code and then put into
running the code and a lot of problems a
lot of data structures are already
defined recursively right you know what
trees are defined recursively so you're
already going to be operating on them
recursively anyway so it you know it's a
good match for a lot of data structures
anyway there's no shared mutable state
because even though they're reading data
out of the same array they're just
reading they're not right and we don't
have to coordinate for that the
intermediate results like those
intermediate sums they just live on the
stack and you know so this has the
potential to be very efficient of course
double in the details right you know how
expensive is it to fork off a task how
expensive it is to wait for you know for
a task these all affect you know what
kind of speed up you're going to get
generally because spinning up tasks
especially you know at the early stages
of a program has some cost you want to
be able to start breaking off work early
to start some worker working on some sub
part of your problem while you're still
decomposing the rest of it but the
decomposition should be dynamic and it
can it can incorporate how many cores do
I have what's the load and that's
exactly what the fork/join framework
that we added in Java 7 does it manages
the you know the decomposition of tasks
and the waiting for tasks and all of
that so you only have to write the part
of your program that has what problem I
trying to solve and the mechanics of
creating tasks merging tasks handled by
the library for you so you know to draw
a very simplistic picture you know if I
have an array of numbers I want to add
them up I could add them up sequentially
if I only had eight numbers that would
be the logical thing to do but imagine
it's a you know much bigger array so
what am I going to do I'm going to
divide it into two I'm going to divide
that into two again and I'm not copying
the data I'm just sharing sharing the
reference because my tasks are only
going to work on their little part of
the problem and then you know this turns
into you know a bunch of little add up
one and two add up three and four add up
five and six out of seven and eight and
because addition is associative I can do
that and whatever
I like and then I can you know compute
the partial sums and I work my way up
the tree until I have my answer so it
actually does work like that right it's
fairly straightforward and it's easy to
reason about because you know everyone
works on their little subset of the data
okay so this all sounds great
how does it work in practice well
sometimes it works great in practice
sometimes it doesn't how do we know
whether it's going to work great or not
well we could measure but before we
measure we could think about it so
that's what we talked about what are the
costs in doing that parallel
decomposition
well sometimes has a cost of splitting
the problem into some problems are much
cheaper to split than other problems
sometimes splitting is so expensive it's
easier to just do the things
sequentially you know if you have to
copy a lot of data to just to split the
problem that's you know that becomes an
impediment to parallelism there are the
tasks based cost forking off tasks
joining with tasks you know you can you
know you can do a lot of work in the
time it takes to do a handoff across
threads so you don't want to spend all
your time managing tasks you want to
spend your time keeping those cores busy
doing like actual work on the other side
there's some cost to combine the results
some combination operations are cheaper
than others if you know in the example I
gave where we're summing an array the
combination operator is take the sum of
the left half and the sum of the right
half and add them that's a very cheap
combination operation but what if the
result of the subproblem with a set and
i had to merge the sets that's a much
more expensive operation right so just
like splitting costs combination cost
can also be an impediment to parallelism
and then the elephant of the room is
that pesky hardware a hardware locality
tends to be a significant factor in
determining whether we get parallelism
out of a you know out of a problem or
not
and when I say hardware look memory
locality I mean is the data that a given
task is working on located close to the
other data that it's working on in
memory because if it is it will be hot
in cache already and when the the task
goes to get the next item to work on it
not going to be waiting on the memory
subsystem to cough up data if you have a
data structure that exhibits poor
locality you may divide it up into a
whole bunch of worker threads but then
what's likely to happen is the worker
threads are going to spend all their
time waiting on the cache subsystem to
deliver the next piece of data and
operate on and most systems are memory
bandwidth limited they're not CPU
limited and so if you've got a lot of
cores all cash missing at once you're
not going to get a lot of throughput out
of it so each of these factors yields
away part of the potential for
parallelism and you know you know so if
you analyze your problem and you say are
any of these particularly bad you're
likely to get poor parallelism out of it
and in general you need a fair amount of
data for parallelism to work for you if
you're adding up a thousand numbers do
it sequentially it's much faster but if
you're adding up a billion numbers
parallelism going to give you a big
advantage ok
so like I said Java Java SE 7 adds the
fork/join framework this is a task
management framework designed for
fine-grained mostly CPU intensive tasks
doesn't mean you can't do use it with
i/o but it's tuned for CPU intensive
tasks the nice thing is it scales really
well over the range of cores that you're
likely to encounter on a real system and
so you don't have to worry about the
details of task management and it's
basically optimized for the kind of
divide and conquer algorithms I've been
showing so the two basic operations are
fork off a task and wait for that task
to complete fork and join and that's an
efficient implementation of that magic
concurrent primitive that I was showing
in my earlier examples the overhead
isn't zero but it's pretty good all
right so let's talk about screens you
know we talked about how you know you've
got the stream framework it's pretty
cool people like it it gives you almost
free parallelism in terms of making it
really easy to express that I want this
computation sequential or parallel and
flip back and forth between them so
that's great you know and it encourages
a sort of more declarative style that
you know programming that tends to be
more readable and
they're prone so that's great but it's
not magic parallelism dust and so you
still have to know is this going to
effects acute efficiently in parallel
otherwise you can easily say dot
parallel and maybe it's not any faster
maybe it's even slower okay so you know
you've all seen examples like this right
you know you have imperative code like
this you can turn this into a screen
pipeline you know uh and yeah the code
is smaller it's more compact it's easier
to read but the important thing is it's
it's declarative it's telling you what
answer you want rather than how to get
to the answer and the benefit there is
it's much easier for the library to say
I know how to do this in parallel I'm
going to go decompose it for you there's
basically no compiler that could like
paralyze that for that first example so
you know but parallelism is the bonus it
wasn't the point of doing this the point
of doing this was writing code that's
easier to read less likely to be wrong
but it's a nice bonus that the the
runtime can can optimize it for you as
well okay so before I gave a bunch of
criteria of things that steal away your
your your parallel performance splitting
combination past dispatch and locality
so each of those apply to a parallel
stream the parallel streams work by
splitting their data sources so the
first question is well what's the source
for my stream is it an ArrayList or
linked list is you know and and these
split very differently and ArrayList
splits really nicely you just compute
the midpoint and say you work on that
half you work on that half whereas the
linked list doesn't split so nicely how
do you split a linked list first and
rest well you could do that again
recursively right you have first and
then first of the rest and rest of the
rest and you can keep going and you
actually could get some parallelism out
of this if the task you're doing is
sufficiently expensive right if you're
doing some really expensive you know
cryptographic attack you actually could
get some parallelism out of a linked
list but if you're just adding up you
know numbers or searching you know
searching a data structure for something
you're not going to get it right so
splitting cost is a real concern and so
you need to know where's
data coming from and how well is it
likely to split tasks dispatched while
that hats handled by the fork/join
framework you can't do too much about
that the combination cost though is
again part of your problem am i adding
up numbers or my merging sets and then
locality goes back to the source what is
my source is an array arrays have great
locality is it a linked list linked
lists have almost pessimal locality
right so it you have to know where's my
data and how to laid out in memory so
I'm going to wave my hands and give you
a completely unscientific model for
thinking about parallel performance I
see Martin wincing when I say this so
imagine a really simple model where you
only have two parameters how much data
do I have and how much work do I do per
data element and you know where work
might be you know a basic arithmetic
operation or a bytecode or machine cycle
if it doesn't really matter but you're
asking you know am I just adding up
numbers or am I you know trying to
factor large problems and you multiply
these things together and you this needs
to be larger than some threshold so if
you have a trivial little operation like
adding up numbers you generally need on
the order of 10,000 elements in order to
get a theta at a parallelism with you
know with this framework which you know
isn't unrealistic we all have datasets
that are much much larger than that so
but a hundred numbers not going to
paralyze thousand number is not going to
paralyze if your operation is more
expensive then you don't need as much
data to get over this threshold but if
you have a trivial small operation and a
trivial amount of data you know just
stop sequential sperate just use that
alright so let me work through these uh
you know these items one at a time
source quitting some sources splitter
than others and the cost of splitting a
source is what you have to compute what
the split is and you also have to take
into account how even is this blip so an
ArrayList
you know or an array you can find the
midpoint really cheaply and you're going
to split it into almost people's equal
pieces that's great and as a bonus you
even know exactly how big the pieces are
which helps you avoid copying in some
operations so that's great so arrays are
great link lists have none of these
properties they split terribly you know
you you have to you know take a pointer
traversal to do the split you you come
away and you come away with a terribly
even uneven split one and n minus one if
your data is not already in a data
structure but it's coming from some kind
of generator there's an analog here so
iterative generators like like calling
an iterator to get your next element
that works exactly like a linked list
right splitting an iterator is just like
splitting a linked list on the other
hand if you have a stateless generator
that splits just like an array does so
you know depending on whether your data
is coming from an already coming from a
data structure or it's coming from a
computation you can still at you know
looking at the structure of it say is
this going to split well or not so you
know and as a comparison both of these
streams generate the same you're the
same sequence right in stream got
iterate start with zero each time add
add one to it and then and then stop
after a certain number that generates
the you know the stream zero one two
three four five so didn't scream range
but the first one paralyzes terribly and
the second one paralyzes perfectly right
because the first one is a sequential
sequential iterator and the second one
is a stateless generator okay so it's
real you know they look the same you
look at that and you say well they
compute the same sequence but they
paralyze very differently okay so like I
said before the elephant of the rooms
locality parallelism wins only when we
can keep the CPUs busy doing useful work
waiting for cache misses is not useful
work and so if you if you have your data
an array that in an array that's great
because everybody starts working on
their little chunk of the array but when
you go pull in the first element it's
going to pull like the first 15 elements
into your cache line so fetching the
second third fourth fifth six element
that's basically free and as you
sequentially walk through an array the
hardware prefetcher kicks in and says
you're probably going to need the next
chunk of the array let me go start
fetching
that so you're not going to have to wait
for it you know for as long on the other
hand you know if your data is scattered
all over memory you're going to be
taking cache misses left and right and
you're going to be spending a lot more
time waiting for data and a lot less
time doing computation and you know the
comparison is I have an array of int and
I turn that into a stream and I want to
add them up that's great because I get
great locality you know this one this
one this one this one I'm done whereas
if I have a stream of boxed integers in
order to I get the number that I want to
add up I have to take a pointer
dereference to like go find the payload
of that that integer box and that's not
likely to be in cache so I'm going to
have to wait for that right so the
performance again these computes the
same results but but the performance of
the one that works on primitives is
better and the parallelism is going to
be better as well because you're going
to be able to exploit look the locality
that the hardware gives you and to
illustrate this this is how these things
are laid out in memory if I have an
array of int they're all sequentially in
memory one after the other so you know
the cache is working for me the
prefecture is working for me if I have
an array of boxed integers every time I
go to traverse one of those pointers I
risk taking a cache miss all right so
locality is important the trouble with
locality is it doesn't really appear in
your code right we don't have a locality
key word in our language we just have to
have some knowledge about how our data
structures laid out and how is that
going to affect our performance and if
we care about performance all right a
few other considerations some streams
have what's called an encounter order
that means the the order in which the
elements appear has some significance
arrays have encountered orders there's a
first element of an array there the
second element of array where assets
don't have a meaningful encounter order
they just have elements which means the
iterator is perfectly free to serve up
the elements in whatever order it finds
convenient now some operations their
semantics are tied to encounter order
operations like limit and skip and find
first fine first means find me the first
element in the encounter order that
means you have to
you know you can't just pick any element
you have to pick the first one now
sometimes in our in our problems the
encounter order actually has a meaning
sometimes it doesn't sometimes you just
care about any element that satisfies a
certain property so we can't
automatically guess what the meaning is
right you know to you but you probably
know as the programmer you have an idea
what problem you're supposed to be
solving so if you've got a stream
pipeline and you're using an operation
like limit or skip or fine first think
about do I care that I'm limiting it to
the first n elements or - I just want to
limit it to n elements and I don't care
which and if you don't care which which
is often the case then you can say
there's a stream operation called
unordered which means I don't care if
you thought this stream had a defined
encounter order it doesn't just ignore
it and that will that will enable
optimizations where limit and skips and
fine first will rather than picking the
first end will pick any end so if you're
using operations that whose semantics
are tied to encounter order that limits
parallelism right when you say compute
the first n elements of this sequence
you know I can't bring as much
parallelism as to bear to there as if I
say compute any n elements just you know
start processing on a whole bunch of
cores and as soon as you have n we're
good we're done I can get a lot more
parallelism out of that so beware of
operations that are intrinsically tied
to encounter order ask yourself if it
matters if it doesn't matter tell the
system you don't care about that ok we
talked about splitting we talked about
locality we talked about encounter order
let's talk about a result combination in
some operations like if I'm trying to
find the biggest number in a set or
trying to add up the elements of a set
the merge operation is super cheap
adding two numbers computing the Makah
two numbers for other operations and I'm
doing a group by operation and I'm
creating a hashmap merging two hash maps
is really expensive it involves a lot of
copying it involves sequential iteration
through the key set and not only once
but you're probably going to do this at
every level up the tree right you know
so if you split two
say four levels you'll do merging of the
hash set at the bottom level and then
the next level up and the next level up
and then at the top level so that's a
lot of work so you know you have to look
at this and say is this an expensive
operation is this going to be an
impediment to parallelism you know and
you know if you do an operation like
collecting - the hash set and you try to
do that in parallel you'll often see a
significant slowdown 3 X 5 X 10 X
because all the work you're doing is
just combining you know combining sets
now there's an alternative to that you
can either go sequential or you could
use a concurrent data structure all of
the you know the the operations that
produce sets and maps like group buy and
such they have a concurrent you know
analog where you can say put the results
in a concurrent hash map and just allow
all the threads to you know fire
elements into the into the map this is a
trade-off because you lose or during
it's an intrinsically unordered
operation but you also ditch the
bottleneck of merging your result set so
you have to ask yourself you know what's
my merge operation how expensive it is
it and am I paying for more than I need
am I paying for a an order preserving
merge when I don't need one right so
there's often a lot of ways to tweak
your problem and say well I can get rid
of some of these costs okay so you know
just just to illustrate if I have you
know if I'm trying to you know if I'm
trying to collect the elements 1 through
8 I can create a little set at the
bottom but then I have to merge them
into intermediate sets and then I have
to merge them again at the end right and
that's that's expensive ok so I rattled
off a list of factors a splitting cost
merging cost locality sensitivity to
order each of these could undermine your
speed-up and if any one of these is
really bad unless you're have you know
really expensive operations you're doing
on each element it's likely to be a
significant impediment to parallelism so
you know the good news here is you can
look at a pipeline and say how's that
going to paralyze and you can very often
immediately look at it and say yeah it's
not going to paralyze well because of
this which is great because you just
saved yourself a whole bunch of
measurement time and you know experiment
time you look the problem and you say
it's just not going to paralyze well so
this should always be our you know our
our second step our first step run it
sequentially if it's fast enough you're
done go solve a different problem
second step if you think it needs to be
faster look at it and ask if parallelism
is to help you and usually you can
figure it out pretty quickly by applying
these criteria only then do you want to
move on to well let's try it in a
parallel measure and see how it works so
summing up streams are cool yay
parallelism is cool yay but parallelism
isn't magic performance test parallelism
is hard it's easy to ask for but it's
hard to reason about we have to you know
it so it's only an optimization we
should only apply it if it actually
delivers an optimization and it may
require analysis and measurement on in
order to figure that out now I'll give a
little commercial about performance
you know performance tuning but I always
give which is if you're going to do
performance tuning first make sure that
your code isn't already fast enough what
it's fast enough mean look at your
business requirements if you don't have
business requirements performance your
code is fast enough because if you
haven't invested in defining performance
requirements and you haven't invested in
defining performance metrics which isn't
easy and isn't cheap then your code is
fast enough
done stop optimizing so these techniques
are useful but there you know but apply
them when you already have a good reason
to believe you need to speed up your
code and when you think when you know
the analysis tells you that parallelism
might actually speed up your code and
when your measurement tells you it
actually does alright so I think we have
a few few minutes left oops I think we
have a few minutes left to take
questions so what did we do the thing
with the microphone here
questions anybody don't fire them all
off in parallel hi when you talked about
encounter order I I thought that was I
thought that was interesting because
I've never thought of it that way I
think probably badly in terms of
associativity and commutativity which I
probably mix up quite often but I ask
myself does ordering affect the result
of the computation and then not as
consequence but why why did you talk
about encounter order what why did I
talk to those other concepts as they're
subtle difference or so I mean so I
think the question is correct correct me
if I'm wrong so I would think
parallelism is mostly about is my
combining operation associated and all
of that but you talked about this weird
thing encounter order that I never heard
about why that your question yeah okay
so the answer is most people have heard
about the notion of in order to
decompose a problem in parallel you need
to have some efficient way of combining
the operation most people haven't
thought about
is there something intrinsic to the
operations I'm doing in my program that
are going to that are going to undermine
parallelism and you know parallelism
works best when you can divide the
problem up into chunks that everybody is
operating independently on encounter
order undermines that independence
criteria right if some chunk is somehow
semantically different to a different
you know another chunk because it comes
earlier in the decomposition then that
means they're not really independent and
that limits how much parallelism you
could get and it's some of the worst
examples that we've seen where people
you know say I tried to run this in you
know parallel and I got a terrible
result was when you combine something
like limit and fine first right so
limited fine first are both encounter
sent encounter order sensitive
operations and they actually interact
really terribly but they work great
sequentially so we have all of the all
of these years of you know sequential
training
think all right well limiting really
chief operation finding the first
element really chief operation must
paralyze great actually paralyzes
terribly right and so this is
counterintuitive and surprising and so
I'm kind of here to talk about the
things that are counter to intuitive and
surprising because the other things are
kind of obvious so we have time for one
more question
maybe maybe not oh yes we have time for
one more question question in the back
sir I was just wondering if you could
spend a couple of minutes talking about
the jvm warm-ups get best performance
and streams and parallel streams yeah so
the question is how does JVM warm-up
play into this and you know warm-up
isn't a single thing warm-up is a lot of
things so warm-up involves clasp loading
it involves compilation it involves
initializing JVM data structures and
para and using parallelism also has an
element of warmup
what threads is your TAS going to run in
when you run a parallel stream well the
library spins up a a common fork/join
tasks pool and that involves creating
some threads so the first time you do a
parallel operation it's going to go and
look to see is that is that pool started
and if not it'll go start it and it will
start some threads that's going to be
slower than if that pool is already up
and running so typically the threads in
that pool will stick around for some
amount of time you know a minute a few
minutes etc and if there's no work to do
they'll tear themselves down so that
they don't sit up you sit around waiting
up memory and so just like all the other
aspects of warm-up there's a warming up
the you know the parallel subsystem that
largely involves that largely involves
starting up threads similarly within
each task there's a little bit of micro
warm-up of like getting your data into
the cache right so if you get scheduled
on one of these threads whose data is
going to be in the cache well the
previous tasks hopefully you know you'll
you know your data exhibits good
locality and you'll be able to benefit
from you know you do a little bit of
work to warm up the cache and then you
can you get good performance out of it
but that depends on locality so is
there's warm-up at many levels so I
think that
takes up our time so thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>