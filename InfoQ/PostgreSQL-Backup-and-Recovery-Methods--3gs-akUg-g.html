<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PostgreSQL Backup and Recovery Methods | Coder Coacher - Coaching Coders</title><meta content="PostgreSQL Backup and Recovery Methods - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PostgreSQL Backup and Recovery Methods</b></h2><h5 class="post__date">2011-11-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-3gs-akUg-g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi everybody my name's Kevin Kemper
thanks for coming
today's or this time slot anyway I'll be
doing a talk on post grass backup and
recovery methods just a side note if you
guys have any questions during the talk
feel free to just throw up your hand or
or holler out and we can do questions on
the way and also have some time for
questions at the end so we'll do a
little bit of an overview we'll talk
about the various backup options and
restore options primarily using PG dump
and PG dump all and then we'll look at a
little bit in depth of the different
ways that you can do a recovery and then
also we'll go into a point in time
recovery talk about how how you do the
backups and walk through a step-by-step
in terms of a true point in time
recovery so the the main sort of
snapshot style dump tools that the
Postgres offers are PG dump and PG dump
all PG dump is a tool that is designed
to back up with a variety of options a
single database and PG dump all again
with a number of options will dump the
entire cluster so PG dump is a dump
utility that creates consistent backups
even if the databases in use
it's non-blocking although it's not non
resource intensive if you will so even
though it's not blocking if you've got a
significantly large database and you
kick off a PG dump at you know two
o'clock in the afternoon on your busiest
day you're going to see an impact
however it's not going to be an impact
because tables are block actually it
would be the other way it is possible
that the database would have tables
blocked long enough that PG dump will
actually timeout and say I can't I can't
do the dump at this time again there's a
number of output file up
floormats there's a number of options in
terms of what you want to dump and what
format you want to dump it and so on and
so forth
the syntax for PG dump is obviously PG
dump and we pass it connection options
which are optional and then we also
compasseth a number of flags would tell
it how we want it to operate the
connection options are the standard psql
connection options
- h4 host - p4 port user and you can
tell - I want you to prompt me for a
password with the - W likewise PG dump
and PG dump all will both respect the
standard environment variables PG
database PG host PG port and PG user so
if those are set that is the variables
that will be used for the connection
that PG dump makes to your to your
cluster some of the common options these
aren't all the options but some of the
more common options for PG dump is the -
a flag says only dump data the - capital
C or the - cache clean version of it
effectively puts a drop database command
in the output file and likewise a create
database command I'm sorry the clean
just does the drop the - C and the -
capital C create will give you a create
database command - D will give you
singleton inserts by default PG dump
will give you copy statements you can
exclude schemas we can dump only a
particular schema the excluded schema is
actually pretty useful if you're running
slow nee so slow nee creates its own
schema or schemas depending on how many
databases you're replicating and if you
do a PG dump of a database that has a
slow niece kima and then try to restore
it on another box where slow knees not
installed it'll actually fail because
the functions it tries to restore
require that the Salone libraries exist
so the nice workaround for that is just
don't dump the Salone schema
likewise the the schema only flag says
just dump the DDL give me the create
table statements and create index and so
forth but but I don't want the data you
can specify individual tables or a list
of tables so you could say - tea table
name or you could say - tea like part of
a table name star you can and that works
or you can give it a comma separated
list of tables we can exclude tables in
the same way the verbose flag just gives
you some debugging info the format is
pretty important meaning the default
format is just to dump you a standard
clear text SQL file and of course the
way to restore that is just with P SQL
however you can you can give it formats
like there's a custom format and a tar
format and tar format is obviously a tar
compressed file and a custom format is a
custom compressed binary format and both
the tar in the custom format give you
the option of restoring via utility
called PG restore we'll get into that in
a few slides
but it does open up a world of options
for you in terms of recoverability and
in particular flexibility in terms of
what what objects what data structures
what data sets you might or might not
want to restore it is maintained so all
of the alt all of your referential
integrity are pushed to the end of the
dump pile so it'll restore the tables
and the primary keys and then it will
restore the data and then it restores
all the referential integrity likewise
there's a locked time-out wait there is
a default I think it's 30 seconds don't
quote me on that however that's the
amount of time it's going to wait if
Postgres has a lock on a table due to
what an application is doing given that
PG dump is non-blocking it's it's going
to wait and if it waits the locked
amount amount of time it'll actually
exit and say sorry I can't I can't move
forward if the default isn't working for
you you could set this to
you know something higher like two
minutes or five minutes or whatever to
try to get PG dumped more of an
opportunity to get itself in the queue
if you l to to grab a dump of that
particular table in terms of the file
format options we talked a little bit
about this but the default is do a plane
to do a plane format so if you don't
specify the - a flag at all you'll get
again like I mentioned just a flat clear
text SQL file if you specify the - see
that's the custom it's a custom binary
format and likewise the - T is a tar
format you can also specify a
compression level with a - Z so when you
do either the C or the the T if you
specify compression level you can give
it a 0 to 9 of course the trade-off is
the higher the compression level the
longer it's going to take to to do your
Tom but the more disk disk savings you
get here's a couple of examples so in
this top example we're dumping insert
only statements including a create
database statement so the - capital C of
course gives us the create the - -
inserts gives us insert statements we're
dumping a database called prod one
underscore DB and redirecting it to
straight SQL file in the second example
we're dumping a table called customer
and we're only dumping the data in a
custom format again from the prod 1 DB
database and likewise we're redirecting
that to a file called problem D because
dot FCM the next example we're dumping
only the DDL of the prod 1 DB database
and we're redirecting that to a straight
SQL file and the last example we have
here we're dumping the gold schema and
we're doing a tar format of the prod 1
DB database and redirecting that to to a
dump pile questions so far
that's just something I made up so it's
just whatever schema we happen to want
to dump PG dump all very similar utility
the difference is PG dump all dumps the
entire cluster whereas PG dump you
specify a database there are a couple of
key components to PG dump all that are
worth noting the connection options are
exactly the same a lot of the common
options are the same so the data only
and the clean you can do inserts got
Yatta Yatta a couple of key differences
here is the - G flag the Global's only
and the - are flag the roles only are
pretty pretty useful in scenarios for
example if you're setting up Sloan E and
you need to set up your database on the
slave you can use PG dump and get all
the DDL and push it across for a
particular database and likewise you
could use for example the - R to get
just the roles from the master database
and push it to the slave or you could do
- G and get all the Global's meaning you
get the roles and the tablespaces and
all the global structures however just
be aware that if you dump the
tablespaces all of course those same
paths need to exist on your slave as
well likewise you can tell it I don't
want you to put ownership in the file
and I think everything else is mostly
the same maybe the superuser is
different so you could specify a
different super user that you want to be
specified as the owner when you do a
dump all a couple of dump all examples
this top one we're doing an entire
cluster dump containing only the global
structure so we're doing the - G the
next one we're dumping the only the
tablespaces and likewise the third
example we're dumping a dump of the
cluster without any tablespace
references on the final example we're
dumping the cluster and we're talented
use the superuser gold user and we're
giving it a lock wait timeout of 30
seconds so if we wait more than
thirty seconds on a lock it well abort
any questions yes
jump to a tape so PG dump and PG dump
all inherently the question was is there
any media manager support that allows us
to dump two tape and the short answer is
no PG dump and PG dump all don't support
the media protocols within the tool
itself however we have done some work
where we did go to tape and basically
it's a two-step process so we we either
go to disk and then from disk we have a
command script that pushes that to tape
or in what we did is in one scenario is
we did the PG dump and we'll pipe that
to the tape commands and it sends it
straight on to the tape however it's you
have to go to the UNIX command line and
and write your own if you will so
restore options that obviously when
we're doing PG dump and PG dump all our
restore options are psql
for the flat SQL files or we can use PG
restore and it has a number of options
and it gives us a fair amount of
flexibility in terms of how we want to
restore what we want to restore with
psql of course if we've got just a flat
straight SQL file we can do here's a
couple examples psql - EF of the SQL
file and in this case the first example
we're just redirecting the output to a
log file and in this case we also didn't
give it a database name so the
assumption is that we're we're
connecting to the default database which
is going to be set by the PG database
environment variable likewise were
here's an example where we did a PG dump
and we dumped the prod 1 DB database and
we're simply piping that to P SQL - H
and redirecting that to our QA server in
this case there's another PG dump all
we're dumping only the Global's and
we're passing that to a psql - H of our
dev server
and we're given it a different port and
we're logging it this is pretty similar
to what we would do with Sloane II is
you know we would do the PG dumb Paul -
T on the master and pipe it to psql - H
for the slave that we're setting up and
then we'd come back and do the DDL
likewise the last example we're doing a
PG dump of the prod one database and
piping that to psql
on the test database and then we're
we're logging it so pretty pretty
standard stuff in terms of restoring
from P SQL files or dot SQL files PG
restore is utility that gives us
enormous amount of flexibility the
connection options are the same it
respects the same environment variables
just like PG dump and PG dump all do
some of the common options / PG restore
are number one we've always got to give
it a database name so we give a - D or -
- database or I'm sorry - - DB name and
we give it the database name we can tell
it for example I could do a PG dump and
pipe that or set it to a custom pile or
a tar file and I could take that tar
file even though it's a full dump of the
database so all the DDL indexes data
everything and on with PG restore I can
give it a - a and say I only want you to
restore the data because I've you know
I've already installed the DDL and so on
and so forth like guys we can do the
clean and the create just like I'm on
the way out if you well with PG dump we
can specify individual indexes so if we
just wanted to restore a couple of
indexes from a dump file we could do
that we can restore only the DDL we can
restore individual schemas we can tell
it to disable the triggers when we do
the restore so on and so forth a lot of
the a lot of the the flags are the same
but it provides I think an extra amount
if you will of flexibility when you go
to recover so it doesn't mean that you
have to recover it's not all or nothing
likewise we can specify individual
tables individual triggers and you do
have to specify the - F and tell it is
this a custom file or is it a tar
formatted file no it won't the other
thing if you notice the last couple of
items here the - Al and the - capital L
this gives you a really powerful way to
create an enormous amount of flexibility
in terms of what you might want to
recover and we'll get to that here
momentarily so here's a couple of
restore examples
this first one restores the data only
from a custom formatted file and we're
restoring it into a database called prod
to underscore dB
so we're given it the PG restore the - a
to say only restore the data
we're counted it's a custom formatted
file we're specifying the database name
and then we're given at the dump pile
likewise the second one does it clean so
it removes the data and the structures
first and then it restores only the gold
partner's schema from at our formatted
file and the final example we're
restoring only the DDL ie the schema
only into the QA one underscore DB file
from a custom formatted dump file and
the dash J did we have it on this slide
the jobs that's actually a parallel
restore feature so if I've got a
database and let's say just for
simplicity I've got a thousand tables if
I specify - J and give it ten it'll
actually create ten parallel jobs and it
will split those thousands tables up you
know a hundred per job and it will run
all ten jobs in parallel so what used to
take in previous versions of Postgres
eight hours to restore we could
potentially restore in two hours or
thirty minutes or so depending on
how much parallelism your machine can
handle so the next big feature with the
PG restore is using table of table of
contents files and that's the - L and
the - capital L that I mentioned earlier
so if we have a dump and we've dumped it
to either a tar or a custom formatted
file you can run a PG restore on it and
you can in this case if you look at the
second example here we're doing a first
we do a PG dump and we're doing at our
formatted file and then we run PG
restore and we give it a - L on that
dump file and then we're redirecting
that to an output file what's going to
get redirected into that output file is
a table of contents of what's in that
dump and here's sort of an example of
what you would see in that file so the
semicolons or comments so you get a
header that talks about how many entries
are in this file and when was it created
and so on and so forth and then likewise
you'll get an entry for every object in
that file so if we look at this example
there's a crease effectively a crate
schema there's a comment on the schema
there's a access control setting the
permissions to be owned by Postgres
there's a public DB mon thresh table
that's owned by Postgres and so on and
so forth a little further down there's
some set commands and there's a couple
of data commands they're specifying
these are the actual data restores as
opposed to the table that ETL restore
and there will be index entries in here
and and referential foreign key entries
and so on and so forth so the way to
deal with a table of contents file is
you could effectively go into that file
and comment out anything you don't want
to restore and create a very custom
restore if you out so you could restore
in a set of specific tables and only
restore data on some of those tables and
and restore indexes on all of them but
not the foreign keys on some of
and so on and so forth and then when you
run the PG dump using the table of
contents file you get a very customized
restore scenario what I used to do with
this before we had parallel restore is I
would take the table of contents file
and split it up into multiple files and
create my own parallel restore which we
don't need anymore but now it's kind of
a cool feature as well here's a couple
of examples so we do a PG dump to a tar
formatted file and then we run the PG
restore on that creating the list file
and then we create this queue adb 3
database we added the less file
according to what we want to restore and
then we run the PG restore with a -
capital L and give it the name of the
list file and then give it the rest of
the parameters as normal and it will
restore only the the data structures and
the objects that are referenced in the
list file any questions on the question
was are all these features available and
generic Postgres vs. et B's Advanced
Server the answer is yes this is generic
Postgres any other questions
so the question what are the advantages
of the custom file number one it's
compressed and number two it's a binary
file so it's a more efficient restore
because it's binary and then of course
just the compression so next we'll talk
about point in time recovery so Postgres
doesn't necessarily have a way of saying
by the way just turn point in time
recovery on and let me deal with it
later however it does have all the
building blocks to let us put point in
time recovery in place and leverage a
fair amount of flexibility in terms of
how we want to recover recovery time
so the basic things we need to do for a
point in time recovery is number one we
need to do point in time recovery based
backups which would effectively be the
equivalent of say a level zero back
up and then likewise you need to tell
the the database server or your cluster
to archive the wall segments meaning
every time one of your transaction logs
gets full we need an archived copy of
that transaction log the combination of
those two items that the effectively
level 0 backup and some number of
archived transaction logs allows us to
recover to any point in time we define
likewise for a point in time recovery
basically restore the lathe the latest
base backup or the latest base backup
that we want to leverage for our
recovery you have to do a little bit of
preparation in the system data directory
and one of those things is we create a
file called recovery comp and basically
after we've identified how we want to
recover basically you need to tell the
recovery comp file where do where do I
find the wall segments the archived
transaction logs and to what point in
time in its most simple terms
do I want to recover two and then you
start the postmaster the postmaster will
find the recovery comp file it'll
automatically roll into recovery mode
and once it's finished it'll rename the
recovery file to recovery got done and
he'll come online it's a fairly
straightforward process it's pretty
simple to be honest so in order to do
the backups here's sort of a walkthrough
so in this case we go into our
PostgreSQL comp file and there's a
couple of key parameters we need to
change so one is we need to turn archive
mode to on by default archive mode is
off and changing archive mode requires
that we restart the database server
likewise we need to give it an archive
command the archive command can be any
valid shell script or I'm sorry any
valid OS command so if you're running on
Linux it can be a copy statement like we
have here it could be in our sink it
could be an SCP
it you could reference a shell script
any valid OS command can go in there
effectively what we're trying to tell
the database is I'm going to reference
via the percent P variable which means
that's the full path to the actual
source wall segment meaning the
transaction log and the percent F is the
name of the file that it's going to give
to the wall segment so by using those
two variables we effectively try to tell
the database engine I want you to move
the percent P file to some other
location and that location can be
whatever you want in this case we're
doing a simple copy and we're copying it
to a directory called slash stage slash
wall in a lot of cases I've seen in fact
even with our shop we've done SCPs or
we've done our sinks which effectively
move the wall the wall segments the
archived transaction logs off of the
main database server onto a backup
server somewhere and that way in case of
a disaster if the disaster were
significant enough that the discs were
irrecoverable we haven't lost all of our
wall segments with the crash likewise
there's a there's a variable called
archive timeout in most cases we leave
this to zero however if you tweak it
what it'll do is let's say you set it to
30 seconds what that will do is every 30
seconds it will make sure that if it
hasn't already in the last 30 seconds it
will archive the current transaction log
where this is useful as if you've got a
database server that the traffic is
light enough that you only rotate
transaction logs like once a week and
you don't want you know half a week's
worth of transactions even though it's
not all that many you don't want those
transactions sitting in the transaction
log for that amount of time on archived
in the case of a disaster there is a way
to if you can still get back to your
original directory tree where your
database lived in a crash scenario where
you could recover those but it's not a
guarantee so that's a that's an
nice feature if you have a light traffic
machine the archived command and the
archived timeout values can both be
changed by doing a reload as opposed to
a restart of the database server so for
an example we would do a make der on the
stage wall directory we would make sure
that that directory is owned by Postgres
and assuming we had already made the
these three changes we would go ahead
and restart the server once that's done
if we were in a let's test this scenario
we would want to create some
transactions generate some volume at
that point we should start to see files
show up in the stage wall directory and
they they have a long name but they're
effectively archived version of your
transaction logs the transaction logs
are copied when one of two events takes
place either your I'm sorry the wall
segments are copied when either the
transaction log is full and it gets
ready to rotate to the next log or when
the number of seconds specified in our
archived timeout has passed so the next
step would be
we're archiving the transaction logs or
database servers back online now we need
to do our base backup or our level 0
backup so what you would do is connect
to Postgres and execute the function PG
start back up PG start back up requires
a single parameter which is a tag and it
doesn't matter what you put in that tag
generally when I do this I'll put in the
date as a string however you can put
whatever you want in that tag and there
is some value in terms of having a
formal naming convention for that tag in
extremely rare scenarios you get into a
place where you might not be able to
recover based on a normal time line or
the actual time line that took place and
you have to go back to previous baseline
backups and in those cases you get into
a scenario where it's important to
understand what tag you gave it
in most cases you'll never deal with it
but it is important to stick with a
naming convention in case you ever get
backed into a corner in a recovery
scenario likewise so we run the just
like any function we run select PG start
back up and then once that comes back we
go ahead and we archive all of the
directories that are associated with the
database so if you have your database
for example installed in VAR live PG SQL
data you would effectively backup the
dative directory also if you have
external table spaces let's say you have
a file system called /pg data and in
there you've created several external
table spaces you also need to back that
up
so any any parts of the file system
where you have Postgres data installed
is what you want to backup you can back
it up with tar you could R sync it to
another box you could SCP it it really
doesn't matter just back it up once
you're finished with the backup then you
connect to the database server again and
you execute the function PG stop backup
PG stop backup doesn't take any
parameters and effectively what you're
telling the database is this is the
point in time where I started and when I
finish doing the actual file system copy
for my base backup then of course if we
were testing this we would want to
generate some more transactions make
sure that we had transaction logs that
were archived on both sides of the
backup just to make sure we're fully
testing the scenario and then we roll
into recovery so let's say our database
crashes or we want to create another
instance on take a cue a box or what
have you the first thing you want to do
in the case of a crash is if you're able
to still get to the original data
directory you want to get in there and
copy the PGX log directory and save it
off somewhere if if you can get to it
what that will buy us is any
transactions that were in that last
transaction log that didn't have a
chance to get archived
we'll have an opportunity to restore
those transactions and in that case if
we were doing a full recovery of
everything we weren't specifying a time
stamp we would recover anything that had
been committed and we would only lose
uncommitted transactions if however your
disk is just unusable you can't get to
it at all then you're going to lose not
only uncommitted transactions but any
transactions that were in that last
transaction log number two here ensure
that the postmaster is not running
that's key if you are able to get back
to disk and you had some catastrophe
take place but you're rebuilding the
database server back onto the same box
in most cases you're going to it's
likely that you will have rebooted that
box in between but that's not a
guarantee either so make sure that the
there are no orphaned or in some other
way
non-responsive postmaster processes
running on the box
likewise if if we were able to copy the
original data directory I'm sorry if we
can copy the original data directory we
want to copy that however what we're
doing is oh I'm sorry I covered that
we're just going after the the PGX log
directory so if you look at the
documentation it talks about copying the
data directory the original one but in
reality all you need is the PGX log
directory nothing else is going to be a
benefit to you in that directory at this
point if we were doing in our sync then
we can skip these next two steps however
if we for example say we tarred up when
we did our base backup the directories
and then move that tar file to another
box obviously we want to remove the data
directory in any tablespace directories
and that would be if we're covering back
to the same box and then we want to
restore our last system backup
effectively what we're looking to do
here is take our base backup and lay it
down on disk in the same way that it
looked like on the server that crashed
so whether that's not back onto the same
physical box or not it doesn't matter
but you want to lay things down in the
same fashion so if you backed up
everything from a data directory you say
it's invar live data you it's it's to
your best advantage to keep it simple to
lay that back into a directory VAR live
data it's not required there are some
tweaks you can do to to make it come up
in a different directory path
configuration but in some cases it
requires you to go in and and tweak some
things so anyway you uncompress your
backup or whatever you need to do to
restore your base backup and key things
or you want to make sure that your
permissions are retained and you want to
make sure that if you've created
external tablespaces that the symbolic
links in the PG tablespace directory
which is in the Postgres cluster data
directory make sure that those are still
appropriate so for example if you had
table spaces on the original box and
they were mounted let's say on a sand
and your mount point on that box with
/pg data but when you set up your
recovery box the SAS gave you a mount
that says /pg data - that's ok but you
need to go into your PG tablespace
directory and reset those table
tablespace links they're just soft links
in the directory they're not hard to
figure out but you need to make sure
that those are appropriate next thing
you want to do is in the data directory
that you just recovered you restored
your tarp file or what have you go into
the PGX log directory and we need to get
rid of everything with the exception of
there's a directory within that
directory the only thing you want to
leave is the PGX log directory itself
and the PGX log slash archive status
directory the reason is you took that
base backup let's say two weeks ago that
PGX log directory is in the state of
when you took that backup so to leave
those uncommitted or I'm sorry unarchive
transactions in the same state as the
point
time you took the backup will just
confuse the issue so get rid of
everything in there the next thing you
do is if you are able to save your
original PGX log directory you would
effectively copy that straight into
what's now your new PGX log directory
that's what's going to allow you to
recover any transactions that have been
committed but not yet archived next
thing we do is we create a file in the
data directory called recovery comm and
we'll go over the options for that in a
couple slides as an option and I always
do this I will modify the PG hbo.com to
lock everybody out that I possibly can
so I'll lock everyone out except for the
Postgres user for local connections so
unfortunately if the there's
applications or jobs on the local server
that run as Postgres and connect locally
they're still going to try to connect
the only downside it's not a big deal
but the only downside to that is every
time someone tries to connect when
you're running a recovery the
application gets an error back and you
get some entries in the logs for these
errors so if there's a lot of
connections trying to pain the database
server while you're trying to monitor
progress you start to have to filter
through all these error messages which
just becomes a pain it's not a problem
per se it's just not helpful then you
start the server the server will find
the recovery I'll it'll roll into
recovery mode if you tail the Postgres
log or if you're going to syslog if you
tell the VAR log messages or wherever
you're pushing your Postgres logs you
will see the recovery in action
and once it's finished it'll rename the
recovery comp to recovery not done and
it will come online you'd want to poke
around as DBA and just make sure that
the data looks good that maybe do some
counts make sure that everything looks
kosher and then if you did option 8 here
you'd want to go ahead and restore the
hbo.com file to its original state and
at that point everybody's good to go
one should be able to connect and your
data should be as you specified in the
recovery comp file in the recovery not
file there's a couple of key parameters
we need to specify one of them is the
actual restore command so in our case we
when we archive our wall segments we
were copying from the internal Postgres
data directory to a directory called
slash stage / wall so the assumption is
that we still have a slash stage / wall
on this box or we've mounted it so it's
still that same name so in this case the
the first restore command you see here
is just a simple copy / stage / wall / %
F which is the wall segment file name
and we're copying it to % P which is the
internal transaction log location
inclusive of its full path likewise the
restore command could be a shell script
and you could do some additional
checking before you actually pass that
or move that file to its right location
it could be an SCP command it really
doesn't matter just like with archiving
the wall segments this can be any valid
OS command that well at the end of it
actually moved the wall segment into the
transaction log directory Postgres will
actually call this command for every
file that it knows it needs and it will
pass in the appropriate values for the %
F than the % P each time there's also a
variable you can specify a recovery
target time if you leave this out the
default is to recover everything it can
find however if you put a timestamp in
here it will recover up to inclusive of
any transactions that were committed on
or before that timestamp likewise
there's a couple of other parameters
that you less likely to use one of them
is the recovery target transaction ID
and you can specify
an individual transaction ID and say I
want to recover up through this
transaction ID one thing to note is that
when transaction IDs are generated
they're generated sequentially but
they're not necessarily committed
sequentially so if you it's feasible
that you could generate transaction ID
105 by doing a begin statement and
before you do your commit 87 other
transactions have not only begun but
also committed then you're out of
sequence at that point so just an FYI be
careful when you specify a transaction
ID if you have that kind of visibility
into your system I haven't had any
experience with doing the transaction ID
recoveries or to be honest with the
recovery target timeline which is
another one if you have to specify a
recovery target timeline it means that
you're in a world of hurt as a DBA
it means that something's gone really
really wrong and you physically can't
recover the base back up and the set of
all segments you have based on the
timeline that actually occurred as they
were backed up so you can specify
effectively an alternate timeline and at
least get the database to come online
and then you can go to your boss and say
well we're back but but at least you get
to say we're back so there is a lot of
documentation on this one it's it gives
really complicated really fast and you
need to have some real visibility into
how the transactions occurred in terms
of being able to try to construction an
alternate timeline hopefully you never
have to go there but it's available if
you do there is also a parameter called
log restart points it's a boolean it
doesn't do anything in terms of the
recovery itself all it does is basically
set up verbose flag if you will in terms
of what it logs as it's doing the
recovery so if you have some
particularly long-running transaction
and you turn this on it actually gives
you the ability to see things progress
so you don't have to sit and stare at
the keyboard for an hour and a half
before you see that it's actually doing
something nothing more than logging for
that scenario and at that point you
should have a running server back online
and hopefully all your data is intact
I've done point in time recovery untold
number of times and never had a single
issue I've done it both with full
recovery not specifying a timestamp just
recover everything and I've also done it
by specifying a timestamp and in general
it's been flawless oh and thanks
everybody
we're a little over but don't worry</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>