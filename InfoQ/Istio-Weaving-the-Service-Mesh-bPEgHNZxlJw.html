<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Istio - Weaving the Service Mesh | Coder Coacher - Coaching Coders</title><meta content="Istio - Weaving the Service Mesh - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Istio - Weaving the Service Mesh</b></h2><h5 class="post__date">2018-03-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bPEgHNZxlJw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I have a hopefully a quick slide here
there we go okay so I'm gonna briefly
tell you about my career Google in one
slide this slide spans 10 years so yeah
it's uh it's kind of sad sometimes
there's actually a little bit before
this but uh it's probably best not to
talk about what that was
well otherwise I'll just get sad but I
worked on API infrastructure Google I
worked on the API infrastructure about
eight years now and when I started
working on it this was the general
architecture right you know Google lots
of HTTP traffic comes in there's a big
reverse proxy that sits in front of
everything it routes traffic to a bunch
of services some of those services
implement public restful api is using a
library called g data and i don't know
show of hands anyone here used atom pub
before jason was cool probably best that
you didn't okay
so I know Google is happy building api's
when I say happily not really happily at
all and a couple of things happened but
probably the biggest thing that happened
was smartphones and all of a sudden a
whole bunch of teams at Google needed to
build more api's lots more api's api's
had served a lot more traffic and there
are a whole bunch of API problems that
weren't really being solved for people
in this G data library model now the API
is needed more sophisticated
authentication models they needed quota
abuse protection denial of service
protections they needed Terms of Service
because if you use the API you should
probably see where you're using it for
and there's just a whole bunch of these
cross-cutting concerns that weren't
really being dealt with and so we built
up a tea a team to kind of build all
that stuff out and we put all of that
functionality into this middle tier
proxy which we called the API proxy
inventively enough and that's great and
that worked well for several years and
scaled up and we were serving lots and
lots of traffic but there were a couple
of problems so one there were certain
types of services that weren't strictly
api is right you didn't pay to call it
he paid for the resources that it used
and so we needed some way to measure
that and to track that stuff and there
was the
same basic properties that we wanted to
track but they weren't api's so we
needed a kind of centralized thing that
tracked all those behaviors and allowed
people to funnel usage data into that
and then to emit billing you know an
enforced quota and things like that so
we created a thing called a control
plane and this is working just fine
and then something else horrible
happened and that's the cloud at which
point that was way too slow
right having this middle proxy thing in
the way in the middle of all the data
paths talking to the control plane was
too slow right there were too many
Network hops in the way it was too
expensive we actually built that thing
in Java and then had it was a single
failure domain essentially right there
were many different API is going through
that proxy and that caused spots
I myself have taken out significant
portions of all of Google API traffic by
pushing config configures code FYI don't
just push configure everywhere so we
needed a different architecture and we
moved on to this model where we created
basically we've added a new version of
the API proxy and we wrote it as a
sidecar proxy in C++ and it runs on
every job that actually implements the
service the other thing you'll notice is
that the protocols started to shift
right the protocols look well they start
to look more like our internal protocols
because the truth is that cloud and
Google aren't really all that different
right everything's on the same physical
network everything your workloads run on
our our machines just like our work
clothes run on our machines and they
tend to talk to each other in the same
ways and this was a big trend that was
going on and we had to support orders of
magnitude more scale orders of magnitude
reduction and latency and orders of
magnitude reduction in cost to make all
this work right if you're gonna serve an
API like quite a big table right which
is a very low level storage API that can
absorb millions of writes per second
right you can't cost the same amount
just running a big central managed Java
job it's just not tenable so that's my
Google career for last 10 years doing
all this stuff well reading line because
I spent the last year doing something
slightly different so you know I
mentioned a little bit about this kind
of cloud and intern
convergence right so we had that the
network and the physical distance
convergence talk a little bit about
isolation and reliability and also to
convergence and security concerns so
when you send us your data you really
want the same kind of protections that
we've applied to that data when we serve
consumer facing products and you need
the same kind of intense security and
threat protection models that we provide
and have been using inside of Google for
a long time and so it's these are
important things you'll see a lot of
cloud vendors obviously building out
solutions in this space but critically
important to get this stuff right so you
know there's this big convergence going
on and we're using these side curves for
things but we were really using them for
a variety of patterns that were kind of
designed to aid integration right to
help us insert behaviors into the
network or data path for services that
were massively cross-cutting and that's
a similar problem that most of you have
today when you build services whether
they're micro or not and so how do we
take some of the things that we had done
and start to apply them to that problem
so as I mentioned before this API proxy
thing is a sidecar and you know sidecars
have been probably pretty trendy
recently so we'll talk a little bit
about how that pattern enables you know
in open source and also vendors to
provide solutions that help you kind of
start to abstract away some of the
concerns at the networking level and
also some of the concerns at the
security model that you know you really
don't want living in your code there's
been a lot of you know I went to a
couple of the earlier talks today and
one of the themes in the talks was they
talked about decoupling and velocity
right if you can decouple your operators
from your developers right they can both
do their jobs faster right just like
micro services helps you decouple one
application concern from another right
then those two teams building and
developing those micro services can
iterate faster you have similar coupling
problems at different parts of the stack
or different parts of the process so you
know we know that this track and a
number of other tracks or other
conferences you'll see a fair amount of
conversation about decoupling operators
from developers and it's an important
trend in the industry because if you
have a big portfolio of services you
almost certainly have those two roles
that's not the only coupling that I want
to talk about today so today when you
write services or you write micro
services or you know whatever types of
services you want to call them you very
often are writing networking code
whether it's writing HTP requests or
you're using a library and you're trying
to figure out how to make it work with
TLS or you're trying to configure a load
balancer or anything like that right
you're you're either writing code or
configuration to configure the behavior
of the network and to make the network
do what you want to do so one of things
I want to talk about is how do we step
back and think about what applications
need from the network I also want to
talk a little bit about decoupling the
network topology for security so you
know traditionally when you talk to you
know big IT shops and you talk about
security a lot of the conversation will
revolve around this notion of network
segmentation right how do I package
workloads into networks with very
specific boundaries so that I can
precisely reason about what's talking to
one and what's not allowed to talk to
what and I think there are some problems
with that model and because it's not
fine-grained enough it's not portable
enough and it's not flexible enough to
meet some of them the major use cases we
start to see today in kind of micro
services development and I also want to
talk a bit about modernization and
architecture you go to a lot of
conferences you know one of the words
you'll hear a lot is lift and shift
right how do I take my workload and go
move it to the cloud and often you'll
see that thread conflated with
modernization right do you have to lift
and shift to modernize your application
stack I mean certain you can use lift
and shift to kind of manage your your
your op X and capex distribution when
you're buying compute and storage from a
cloud vendor but is that strongly
coupled to modernization are there other
ways of going about that and you know I
talked a lot of
people who need to maintain certain IT
assets you know on private data centers
while still also working with the cloud
and how do I make those two things work
seamlessly and all of that feeds into a
modernization okay so what is a service
mesh so I like to think of a service
mesh as a network for services not for
bites classically when we deal with
networking right you reason about
sockets you reason about ports packets
maybe you you're using an ala 7 protocol
HTTP I suspect a fair number of you are
but it tends to tail off pretty quickly
after you get to the protocol level at
l7 and generally speaking if you're
building services you need a lot more
out of your network right you'd like the
network to route away from failures
you'd like the network to avoid high
latency routes to a specific service
you'd like the network to avoid hitting
a service that has a cold cash you'd
like the network to tell you when there
are unexpected behaviors latency spikes
packet loss also like the name network
to participate actively in identifying
root causes of failure and you'd like to
make sure that the data flowing over the
network is secure against kind of
trivial Network attacks you'd also like
some observability out of the network
right you'd like to know well sorry let
me come back to this a little bit
there's a variety of features that you
want like we talked about observability
and security but you'd like a lot of
this given to you for free right you'd
like to know that you know as developers
today right I don't have to go and buy
an awful lot of software to get a
variety of properties right that I won't
the long history of software development
is being that the underlying
infrastructure rises up to meet the
needs of the application there over time
but we've all kind of been sitting at
the networking stack I've kind of layer
3 layer 4 level for a while and not
getting an awful lot more value out of
the network than that so how do we raise
that abstraction up and when I say free
I don't mean that you didn't pay
anything for the software because that's
not
the cost that you probably care about
the cost is that you didn't go and have
to rewrite your applications to do it
right should be much much more expensive
proposition for you
I also care about fries and beer is do
is actually an open-source project and
it is free but it's that's the real cost
in this system right if you have to
modernize your applications by rewriting
your applications right it's it's either
hugely expensive and in many cases
completely untenable right it's possible
that whoever wrote the application left
the company five years ago you have no
idea where the source code is no one
knows how to build it but it's still
running in production so that's that's
that's not that uncommon so how do you
help like within that situation so I
talked a little bit about you know a
network for services but the goal here
is to kind of reimagine the network
right like you as application developers
what do you want the network to do for
you or a thing that thinks it's that
work anyway
No do you care actually which IP and
port a packet went to like at a
fundamental level do you actually care
physically where it went
do you just care that went to the right
service that did the right thing on your
behalf and knowing that right so if we
start to think in those terms then you
know there's a variety of features I
think that we all want
you know we'd like the network to be
able to handle service discovery for us
so when I say I want my sales app to
talk to my HR app I shouldn't have to
put into my application code the fact
that HR app lives on these seven IPs at
these 16 different ports in these three
regions and two zones and all these
different concerns right just just don't
make me think about that please stop
also why wouldn't you know when the HR
app talks the sales app or some other
thing no she does it really need to know
how to load balance or should there's
the the receiving app tell it right or
tell the network how it wants the
traffic load bounced again this this is
stuff that should not live in your
application code right because it starts
to pile up and if you think about that
for a second right
we all run in companies or work you know
with lots and lots of different
languages networking code is hard like
you know in my history Google I worked
on a lot of different networking stacks
or kind of high-level application stacks
and all the customers of all those
things were effectively client libraries
writing their own networking stacks that
had unique and special modes of failure
and you know we had some you know many
many interesting examples over the year
of applications or clients that caused
weird behaviors because they had quirk
in their networking code and so if
you're using in a you know some
framework to build a set of services
right that might work fine when both the
clients and the services are written in
the same language because the framework
was designed for that and maybe the
framework is specialized to a couple of
platforms used a razor focused on that
but as your portfolio of services starts
to grow more languages more run times
more frameworks start to creep in and
then things start to fall apart a little
bit it can become extraordinarily
expensive to keep that consistent I have
a fair amount of experience on that I
worked on G RPC for about three years
your PC's goal is to be a highly
consistent RPC framework that spans you
know a range of languages I was a huge
engineering effort to make sure that
they actually all consistently behave
the same and the truth is they don't
write they have little quirks that
occasionally make things go funny so how
do we solve that problem and
particularly solve that problem when
you're dealing with application code
that can't be updated
you can't go put the shiny new framework
into it right you don't really control
it so how do we go about doing this you
know this is I'm sorry to do this all to
you right after lunch but this is gonna
be a heavy meal but we use slide cards
and what sto does is it injects sidecars
into both sides of the network and doing
that allows us to do a number of things
if you look at a set of outbound
features right so service a wants to
talk to service B when service a starts
to make coal is do transparently
captures all the network traffic
routes it through the sidecar and the
sidecar layers in behavior let me wander
so the sidecar on the client side can
inject authentication credentials can
load bounds calls either side
it can inject failures if you want to do
Kaos stuff if you need fine-grained
radley control it can have received a
configuration to split traffic off to go
somewhere else it can do tracing for you
so we can start and initiate tracer and
there's a variety of other features that
you know I haven't listed on here but it
gives you a lot of power in the network
right because now you have a smart
endpoint that belongs in the same trust
domain as your application right it's
really part of your application we'll
talk a little bit about the security
properties of that that is able to take
on a whole bunch of application level
concerns for a networking so you can
kind of consider it like an out of
process library in some ways like
whichever mental model you know works
for you but the truth is that it's part
of the application but it's just not in
the process base of the application
similarly on the server side right the
server side car is in the trust domain
of the receiving application so it's
totally fine for it to do authentication
on that side it can check that it's
received a credential that's appropriate
for the call can do authorization write
checks so if you have an essential Isaac
law management system then it can
enforce policies on that it can
implement rate limits right
a lot of production systems or you know
medium sized production disappointments
it don't often do rate limiting but as
you grow in scale right one of the more
common forms of dos attacks is the
internal data is accidental dos attack
right where somebody runs a load test to
test the service and it has an
unintended consequence and takes out the
system accidental dos are dry by dos or
whatever you want to call it also load
shedding is a big part of that right if
I can prioritize which requests I want
to drop on the floor or if I'm in one of
these modes right if I'm in the DOS mode
and again it can participate in requests
tracing it can provide telemetry which
we'll talk a little bit about and it can
inject faults to trigger behaviors and
see if your system is behaving or
recovering as you would like it to
so that's kind of issed you in a
nutshell right that's that's the
fundamental architecture so how do we
put it all together well we have as I
showed you here you know that the side
cars and there's a control plane that
sits on top of that that configures the
behavior of the system all right so
there's an API that sits above that that
basically operators or developers or
whomever actually wants to affect these
types of things
pushes configuration in that says this
is the behavior I want the network to
have and then we have a component on the
left hole pilot and its job is to
distribute that configuration that
intent down into all the side cars so
that they implement those behaviors so
mostly that's just pushing configuration
down into envoy life
I'm always are really interesting proxy
because it is to receive an update of
its configuration it does not need to
restart right this is actually a fairly
novel thing and networking and proxy
land so it's an entirely API driven
configuration model for the proxy and it
stays off the whole time so you want to
change your networking route has no
impact on like hopping on SLO and
availability right it just changes its
behavior we've another component on the
right-hand side which is called sto auth
and its job is to inject certificates
down into the sidecar and to rotate
those certificates on pretty regular
basis and the reason why we send
certificates it down into the side cars
is because we actually use that to
identify the workloads and to secure the
traffic between both sides I'll get into
some more details about security later
and then the last part that the piece in
the middle which we call the mixer is a
kind of it's the extension model of
Ischia effectively it received telemetry
from the sidecar proxies which then you
know federates downstream to whichever
telemetry collection system you want to
work with but it also implements a
policy check all right so the
server-side sidecar when it receives it
a call from the client will call out to
mixer and say hey should this call out
be allowed to go through and the
information that's available to make
that policy decision is a dependent
dependent on the protocol that's being
used
if the protocol was HCP or geo PC right
we can send a lot of information to
mixer about the kind of layer 7
information about the call to enable
mixer to make that type of policy
decision and obviously you know as I
mentioned down the bottom corner we try
to do this as transparently as possible
so in the case of kubernetes we we do a
little magic in kubernetes land which
effectively kind of rewrites your pods
on the fly to inject this networking
behavior you don't have to know that it
happened and you know segregating
operator roles so that you know
operators can basically enforce that
this happens for all the workloads
deployed into a kubernetes environment
but it's not this model is not tied to
per Nettie's we just did a release of
this year that showed you how to do this
with VMs we're working with meso s-- to
make it work in that environment we
talked to docker folks a lot right is
this pattern just generally applies we'd
actually don't care too much what
Orchestrator you use it's the same basic
model right so I meant a little bit
about envoy I saw the data wire guy
earlier talk about envoy and you know
he's got good taste
so it's it's a C++ proxy you know I've
worked on proxies in a variety of
different languages I probably the first
two iterations of the stuff I worked out
in Google probably should have be done
in C++ it's just you know there's really
solid tooling behind it you can get a
lot of performance you can it's a lot of
really interesting and quirky things
that are just super hard to do in other
languages and it's been used it live for
quite a while they have beaten it to
death and it scales up pretty well with
their service they've been very happy
with it and they've also been a great
community to work with if you ever get a
chance to go see a talk by Matt Klein I
recommend it he's one of my favorite
angry men in tech and his his Twitter
feed is pretty good too but I mention
the API driven updates
it also has features like zone aware a
load balance saying it does a cheapy to
both on the inbound and outbound side
which is actually pretty unique in the
industry today but probably more
important than that it was designed for
observability
right on belay and Matt you know they
have this philosophy of making behavior
of the network a first-class thing and
if you can't really do that unless you
know what it's doing and so envoy
actually produces a lot of information
about its internal behavior that's easy
to scrape and feed into downstream
telemetry systems and that has really
helped them with their production
rollouts with understanding how systems
are behaving you know when they're
having failure modes being able to
diagnose it so how do we model this so I
talked a little bit about kubernetes but
you know we're effectively environment
agnostic but we need to understand the
topology of the network and really that
means we need to understand either
service discovery tools or orchestration
tools because they effectively dictate
the topology of the network so we have
this system called pilot which is the
thing that programs on voice and it
receives basically topology information
from kubernetes or from console or from
Eureka or from whatever you want to
write and plug in to pilot all right so
we suck in all this topology information
and then we merge it with the
configuration that you apply this rules
API that dictates the shape of the
traffic to create this abstraction over
the network topology that we then emit
down into the stack so we've done a
number of integrations I mentioned
console Eureka there are other ones
going on in the sto community right now
and so you know if there's one that
isn't being tracked there that you know
you'd really like to see covered come
and let me know afterwards so if I look
at a bishop that would be great
and then as I mentioned before we push
the configuration down into the network
and we don't restart envoy to do it
partiece we try very hard not to and if
if it is hot restarting it's a bug it's
not a feature and that that's how we
program the network so let's get back to
some of the other kind of high-level
features I talked a bit about
observability and the kind of the
properties of envoy what do I mean by
observability all right so you have this
you know this mesh of applications
they're all
into each other these services if what
we all have names they probably all run
on their a certain Authority they're
probably also performing different sets
of named operations or roughly named
operations on resources within your net
within your kind of application domain
they may live in different zones they
may have different physical
characteristics they have when you make
a call it has latency you know the error
distributions all of that is a set of
information that you want to be able to
easily extract from the system and be
able to put into nice shiny dashboards
into analytics pipelines perhaps into
things that actually feed back into your
deployment management and see ICD
systems so you can reason about
incremental rollouts so that's what we
mean by observability right we need
means he'll extract as much information
from the network as we can and package
it up in a way that's easy for you to
consume and that's what we want to do in
this tier because we can actually see a
lot of what's going on and help
operators fill in the gaps when the
application isn't really helping them
fill in the gaps
good example here would be you know want
to show a graph showing all the
different operations invoked by a
service and the Layton sees of the 90th
percentile latency for each of the
operations but the API is restful it's
hard to actually generate graphs of
restful operations unless you actually
classify the operations right pads or if
they're highly parameterised they make a
terrible classification system so maybe
you want to use something like open API
to classify paths into operational IDs
and then generate your graph that way
you don't want to rewrite the
application code so how would you go and
do that we'll provide some means by
extending mixer to be able to classify
traffic so that you can feed that back
into the telemetry system again without
updating the application so today is few
ships with a kind of out-of-the-box
monitoring experience we use Prometheus
and Grif found to do that and you get
these metrics without instrumenting your
apps we show you metrics that are keyed
by both the source and the destination
of the traffic right so I can have a
graph that says how much traffic is
going between service and service
be not just how much traffic is going
into service be right that's a very
powerful tool I can also trace requests
now obviously tracing requires some
participation on behalf the application
there to propagate context through the
application code itself but SEO can help
you with initiation sampling etc it can
make the trace the the behavior of the
network as it kind of flows around the
application parts visible to you right
so you can see Network latency x' as
well as internal application latency x'
if you've properly instrumented your
code and we want to do this in a kind of
vendor-neutral way so actually there's
an extensible pipeline that goes out the
back of mixer that plugs into a variety
of different instrumentation systems we
use it in today in our kind of the box
experience but I expect we'll probably
ship other out-of-the-box experiences or
you'll see other vendors ship different
types of out-of-the-box variations you
know there's certainly plenty of choice
on the monitoring side maybe you like a
commercial vendor like New Relic
or data dog or Splunk on the facing side
maybe you want to use light stat or open
tracing or you know whatever you want to
use we actually don't care all right we
just want to make sure that's easy to
integrate that and extract that
information from the network and push it
down into these tools in coherent ways
so basically you know this is an example
of the topology you're showing you
Prometheus here which is the one that we
ship but we've also done examples with
stats T and obviously Google stackdriver
I do work for Google it will be
occasional plugs and then we've you know
we have gooeys we show you that I should
kind of showed you the graph and one on
previous screen actually we works which
is a company based in one then they did
a they have a kind of observability demo
that they do using their tools sitting
it on top of the sto metrics and then
showing you a whole bunch of interesting
stuff so you know if you want an actual
product that does those types of things
that they're good people to talk to you
okay so let's talk about resiliency for
a second I think there's some talks
today about chaos engineering now this
is something that Netflix kind of
pioneered an ecosystem for many years
but uh you know what are the features
that you want to the network that help
you make sure your application code is
well behaved in the face of failure you
know people talk about timeouts timeouts
are a very useful tool they're also kind
of a double-edged sword if you're not
careful but they are an important tool
in the kind of operation toolbox to make
sure that your system stays stable you
know maybe you want to you know try a
read from a remote service if that read
fails in a certain unit of time I can
read out of a cache or some more stale
data structure and still serve content
to the user with a reasonably high SLO
so timeouts help you do things like that
retries I mean retries are the the sharp
end of the timeout world you know if you
have cascading retry problems causing
outages it's another great example of
accidental dos attacks but there's a
variety of other things circuit breakers
health checking maybe maybe you want to
be it you know availability zone aware
because you know your application is big
enough to spend multiple geographic
regions and you want to make sure the
system actually writes traffic
appropriately when bad things happen in
zones and things like that and you also
want to do things like systemic fault
injection right at Google we run a a
kind of annual exercise called dirt
where we make certain key systems start
to fail like periodically intermittently
or in very specific tuned ways to elicit
awful behaviors and their dependencies
and you know this is something that I
think the community should probably it's
a it's a thing that people should
definitely be doing as part of their
operational management approach to
rolling out big complex applications
into production anyway on the left hand
side you can see an example of a
configuration object which you send to
the history of control play an API and
basically says here's a bunch of
properties about how I want that network
to behave and is the situation
and in the situation in this case is any
traffic going into that destination
service and we have a fairly
sophisticated language that allows you
to kind of apply these controls to you
know intersections of both source and
destination traffic control okay so you
know that's the resiliency side of
things but you know there's lots of use
cases then really have too much to do it
was at UNC that you know you might be
more looking to do Bluegreen deployments
or I might have to solve the virtual
hosting problem or a variety of other
types of things where I just need to
make sure the right traffic goes to the
right place
so here's an example of traffic
splitting traffic splitting is important
because you want traffic splitting to
work at the right layer in the
networking stack if you look at
kubernetes today kubernetes has this
kind of load balancing system for pods
which is layer 4 based which means if I
one pod wants to talk to another pod
then that traffic is load balanced at
layer 4 not at Larry or so so if you
want to roll out a rule that says I want
one percent of HTTP requests to go to
that other pod you can't do it all right
you need something that gives you that
kind of flexibility and you know this is
a common practice in the industry right
traditionally mostly done using middle
proxies and now we're just pushing that
same set of functionality down into the
sidecar on the client so it just happens
from the client at their origination and
here we're just showing you basically a
weighted percentage traffic so that
between two destinations similar you
might want to do what we call traffic
steering which is not you know just
dividing traffic up arbitrarily based on
percentages but actually dividing
traffic up based on some property at the
traffic now the example we show here is
a user agent filter I want to send
iPhone traffic to one service I'm gonna
send Android traffic to some other
service this is a fairly simplistic
example the routing rule can actually be
quite complicated you know I could
actually write a rule that matches is
based on the source here as well so I
can do some pretty complex combinations
of these things to test out you know
variations of traffic and behavior that
allow me to influence rollouts or
whole bunch of properties in my
operations side of things okay so I'm
going to give you a heavy meal about
security for a second because it's an
important thing when it comes to
securing services what are the things
that you actually want to do right we
talked a little bit about segmentation
right so you want to make sure that only
the services that are supposed to talk
to each other are talking to each other
and only the operations that they're
allowed to use when they talk to each
other are talking to each other well
there's a couple of things you need to
do before you do that right one you need
to make sure that you know who the
service is right who's calling you
there's no point having a policy if you
can't in a strong and verifiable way
know who's calling you otherwise you
might as well be guessing so we have
this notion of verifiable identity you
want to make sure encryption is on by
default so one of the things that we do
at Google is we assume that the our
internal production network is insecure
that there could have been a penetration
attack done to any workload running on
any node in the network and how do we
protect all the other things running on
the network from that event I'm not
saying that that's happening I'm just
saying now that's the mental model right
we want to have defense-in-depth against
a whole variety of different types of
attacks one of those obviously attacks
is you know sniffing the network right
so you want to make sure that payloads
as they go between services are
encrypted and that encryption is
verifiably tied to the identity we have
another kind of reverse problem here
which is the kind of secure naming
problem you know when service a wants to
call service B and it gets an IP address
back how does it really know that that
IP address is part of service B so
that's what we call this for secure
naming you're addressing problem and
then we want to be able to respond to
threats rapidly so what is to does is it
issues certificates to all the workloads
allows them to identify each other and
use mutual TLS to provide verify bio
verifiable identity and encryption by
default so that's how we know who's
talking to
- but if service bees being compromised
I want aid to stop talking to it as fast
as possible so I want to do is revoke
these credential so we have tools to
support rapid revocation so that that
connection from A to B will terminate
now either within a limited time window
right so we can limit the blast radius
of a particular security event to time
right which is a very important property
of your security posture and then we can
often do better than that right we can
shrink it down into very very short
amounts of time and why is this
important so you think about strong
service security at scale now what are
the concerns that you might have well
you should be concerned about insiders
right either accidental or deliberate
right one of the biggest potential
security risks to any company is the
people who work at the company or people
who accidentally walk in the door and
stick a USB thumb drive into a machine
you have to worry about hijacked
services right and this is a problem
with perimeter security today
Equifax is probably the gold standard
nowadays of hygienic services
unfortunately and what hijacking means
is that whatever was hijacked which it
now becomes a Melissa agent gets to use
all the properties in power that the
service had before it was hijacked right
so it's it's part of its custom 8 and so
if everything around it trusts it
implicitly either because it's Network
reachable or for some other reason
that's not good right trust needs to be
managed in the fine-grain way but
fine-grained models are hard to manage
where these fine-grained networking
models are hyped hard to manage right
you want to push out tens of thousands
of firewall rules how do you reason
about fine-grained security like what is
the mental model do you want a half you
know when I talked as I've been talking
I consistently say the word service a he
wants to talk to service B that is the
natural mental model I think in the
micro services world so how do I say
this is service a and this is service B
and how do I do that when service a is
very mobile
alright I'm using something like
container orchestration service a isn't
gonna just sit there on one IPM one port
for the rest of its life it's going to
move around a lot possibly quite quickly
in many geographic regions and it might
even move on to your development laptop
we talked about you know you know test
in prod well that's also you know there
are variations that which are you know
develop on your laptop and have your
laptop the thing you run in your laptop
which is one of the services to be able
to call some of the other production
services which are running in the cloud
alright now your network parameter model
this has to be way more flexible to
accommodate these types of things you
also want to be able to reason about
securing resources I mentioned securing
API is and operations within API is but
resources are kind of the next level
down the granularity alright I have a
database that database is a resource I
have a file in a file system right I may
have one API to read and write files but
one of those files is way more valuable
than the other one right so I have to be
able to reason about those types of
things and at some point in your life
you might have to deal with audit and
compliance either for you know statutory
reasons or because it's good practice
right I'd like to be able to know like
what my security posture is and you know
are there things I can do to make it
better
and then there's a whole bunch of once
that you have right I mean I mentioned
workload mobility it's both a concern
but also a want right you might want to
be able to move from one cloud provider
to another or run some workload on
premise and some workload and cloud I'm
able to move back and forth between
those two things based on what
whether it's cheaper or easier more
performant or whatever your criteria are
I want to be able to administer this
remotely do local development right now
bring your own devices are really common
theme not just in the developer
community but also in the enterprise
world in general I'd also be able to
reason about people and machines in a
somewhat similar way right when I talk
about Authority you know I talked a lot
about service to Service Authority but a
lot of what happens in operations land
is user to Service Authority
and how do i reason about those two
things in a consistent way right if
they're entirely distinct it's it very
hard to think about like what the
security properties of my system are and
I want to keep costs down
a lot of this know if you want all these
properties and you want to deal with all
these concerns you could be looking at a
pretty hefty bill so you know Google's
approach effectively is you know we
don't believe that traditional perimeter
security models are sufficient we think
they're useful and we certainly do a
fair amount with network perimeter but
we have an additional layer on top which
is identity based and strong identities
verifiable identities so here's a quick
overview I showed you a little bit of
this before but we have a CA it provides
certificates they're pushed down into
the sidecar as those certificates are
rated rotated the certificates have an
expiration that's usually quite short
and then envoy initiates mutual TLS
between the two sidecars and that's how
we get verify well identity in crypto
and then those identities are passed up
into the policy layer to make decisions
so I might just kind of glide over the
policy stuff really quickly so we showed
you this earlier but the policy stuff is
you know the mixer is a component which
enables kind of integrating extensions
into the system so in the east geo
community we don't work on say open
policy agent which is an emergence back
to allow people to declare policy
constraints on network or system or
service behavior the kind of
standardized expression language so we
have an integration with that you know
we provide whitelists and blacklists as
features out-of-the-box will probably do
integrations with LDAP systems heck we
might even do an integration with
Kerberos or Active Directory but the
goal really is to kind of funnel all
those things through a common API and an
API that enables us to cache policy
decisions as close to the edge as
possible so that if those downstream
policy systems start to
the network stays off that's the goal
it's not an easy thing to do engineering
ways we're still kind of working through
many bumps with that but that's the goal
and that's as opposed to saying every
one of those policy systems is
individually integrated into the edge
right and the problem with doing that is
then you start to see inconsistent
failure modes between those policy
systems and it's hard to maintain the
aggregate SLA
so there's a reason why we funnel
everything through one standard
integration point with a common API it's
mostly swings dictated dictate caching
behavior for system stability and you
know this stuff is focused at operators
right so we want to say hey operator
what are the properties of your network
that you want integrate them and then
here's a configuration model so you can
manage and roll them out without
requiring application developers to go
and create a new bill so you can do that
okay so here's some examples of you know
some attributes that we produce and we
call this the behavior behavioral
vocabulary and it's a very long list of
things if you go to the site you can see
more of this stuff here's a short road
map and I think this is probably a good
point to switch over to questions
a quick question and I think I saw it in
the roadmap I just want to verify are
you going to plan to do stuff and cross
cluster so like to cluster and yes so
you know when I talk about workload
mobility right obviously issue has to be
able to deal with workloads that span
many physical locations different
orchestrators on-prem cloud you name it
ask me after Christmas hi two-part
questions so you move the library into a
process by itself in the sidecar why not
take it one level further and move the
sidecar out you know cluster which is a
traditional reverse proxy cluster and
the second part the rationale behind
that we do that way is because the only
problem we have is deployment lifecycle
of the proxy like in NY for example we
have a new build for sidecar to push we
are dependent on the app to update it
itself and it's a good couple so one of
the reasons why we run sidecar is
because we want it to be part of the
application trust domain right when we
issue a certificate that certificate
identifies the application if we try to
do the same thing with middle proxies
now middle proxies are super powers
right they're able to act on behalf of n
other things in the network as soon as
you exploit the proxy you have a big
blast radius problem alright so this you
want to scope down the privilege of the
credential but also the workload with
that which that credential is associated
now we do plan to do a variety of things
to help people manage the updates of
sidecars
while Envoy we try not to hot restart it
it does have a lot of hot restore
capability to migrate traffic in place
and so we think we can probably do
binary rollouts in cooperation with
orchestrators without actually impacting
traffic at the sidecar level but it will
require some qualification to make that
work
that is the goal I'm not saying also
saying that middle proxies have no role
here right we actually in is to have
many use cases for middle proxies but we
try and not turn them into superpowers
it's very important
I related to to the proxy discussion and
what you have on your roadmap in terms
of API management can you talk a bit
more about what do you expect to happen
on the API management side and how that
plays with the proxies sure so I kind of
gave a subtle example you know one thing
that API management does today and
that's classify API features right if
you can classify features of your API
you know extract IDs out of you know
restful endpoints maybe using something
like open API then you can start to tie
you know features like quotas Ackles
those types of things to those features
now I mostly talked about this for the
you know the internal to internal use
case but those same set of behaviors are
equally applicable to the external to
internal use case right which is classic
API management right if you go and look
at what those vendors sell there's a
variety of other features you know in
the kind of the long tail of API
management but right so content
transformation and user identity
integration and things like that which
we do intend to plan provide support for
at the platform level so for instance
you'll see sign jot support come out in
sto in the not-too-distant future
there's a variety of things in there
I had a question about any
transformations on the incoming payloads
just so you showed the example of how
you can key on a header value or
something like that for routing is there
extensibility to that for example if I
wanted to integrate with max mind
geolocation database could I make those
kinds of extensions oh yeah so I mean
mixer is designed to be an extensible
platform right so we have a document a
plug-in model we have a guide telling
people how to write extensions for it
right so that's that's the primary
extensibility model there will also be a
kind of API based extensibility model
where all that you know all those
signals that were extracting from the
network you know there'll be a standard
API that you can implement that will
receive those signals and also provide
feedback to envoy to both augment
traffic and control routing behavior
great so let's thank Lois one more time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>