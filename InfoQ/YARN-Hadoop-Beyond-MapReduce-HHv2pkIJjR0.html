<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>YARN: Hadoop Beyond MapReduce | Coder Coacher - Coaching Coders</title><meta content="YARN: Hadoop Beyond MapReduce - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>YARN: Hadoop Beyond MapReduce</b></h2><h5 class="post__date">2013-07-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HHv2pkIJjR0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody thank you for coming to
our talk we're going to talk about
Hadoop and we're going to talk about
yarn what you will learn in this talk if
you don't know already let me do a poll
how many of you know you Hadoop okay how
many of you know yarn two three how many
of you know Java if you don't raise your
hand you're in the wrong conference okay
so most of you don't know yarn and
that's not a surprise because it's a
fairly new thing so in this talk you're
going to learn about Hadoop if you don't
know it yet you're going to learn what
other things you can do in a Hadoop
cluster you're going to learn how you
can use yarn to do that better and then
you're going to learn about an open
source framework called weave which
makes it easier to use yarn so I'm going
to start with something that we all know
simple web application typically I would
separate my presentation and my business
logic layer so I'm just running on two
nodes and they somehow talk to a
database if I want to make that scalable
I can just add as many copies of that as
everyone as I want because they're all
stateless so now this is already running
in a cluster and we're all familiar with
that I guess
now if I'm smart I'm going to make my
web app log and then I can save the log
somewhere and maybe I can do something
with the logs so now I end up having
lots of little log files okay now I have
log files what do I do with my logs my
log files will tell me cookies of users
what they viewed what they clicked maybe
I can find out how long they stayed on a
page what they clicked on most often so
I could actually analyze my logs and
make my web app better I could optimize
the application for the user behavior
that I observe I could actually
personalize the experience for every
user
but a lot of people don't do that and
why is that well now you have these web
blocks they are huge they're gigabytes
sometimes terabytes of data and how can
I analyze them how can I even store them
it's so much data it's really expensive
to store that and do I know how to run
an algorithm to run over a terabyte of
data
most people don't so for all this heavy
lifting
there is Hadoop Hadoop it's symbol is
the elephant really does the heavy
lifting when it comes to to big data
analysis what is Hadoop it's an open
source framework and it's free and by
the way it's written entirely in Java
almost entirely and it runs on commodity
hardware so it really runs on cheap
hardware you don't have to buy an Oracle
RAC or a very expensive storage that is
super reliable you can run it on cheap
hardware and that makes it relatively
cheap to store the data and you can
actually store a long history of your
data months not if not years of your web
blocks there and then the way you
analyze data in Hadoop is using the
MapReduce paradigm the MapReduce
paradigm was introduced by Google in a
paper almost 10 years ago and just a few
years later an open source project
started to implement that in Java and
that's Hadoop what it does it does batch
processing of very large files in a
distributed file system and the paradigm
is very very simple but it's very
powerful too so I'm Google when they
introduced this they they said they were
running for instance their anchor text
algorithm and their PageRank algorithm
in MapReduce and those are the two keys
to their initial success how they made
web search better and a lot of people
found that you can do almost anything
with MapReduce so what do you do with
that now you can turn your web app into
a data-driven app what does a MapReduce
app look like
it consists of two runs in a cluster and
it runs tasks there and there's two
types of tasks one type is the mappers
mappers reads a part of the input data
and each of these parts is called a
split and then each mapper does some
processing on that data and spits out
key value pairs these key value pairs
are sorted and that's called the shuffle
phase and then sent to a reducer in such
a way that all the output for a single
key will go to the same reducer and now
that reducer receives the bag of all
those outputs for this key and can now
aggregate analyze to whatever it wants
to do and write the final output and
that again because the reducers run in
many tasks writes lots of parts so these
are part files and if you want you can
then run another MapReduce job that for
instance reads the parts that this one
wrote as their input and so on and you
can build complex workflows of MapReduce
jobs and you can run run many of them at
the same time in a shared cluster that's
why the MapReduce cluster looks like you
have all these different jobs running
but there's also there's always some
nodes that are not used because I mean
who gets 100% utilization out of this
hardware so when I look at that this
well I've paid for this cluster maybe it
runs on Amazon I'm paying and these
nodes are idle what can I do with them
maybe I want to run something something
more than MapReduce maybe I want to do
some real-time analysis I can't do that
with MapReduce but I might be able to do
that with something else maybe with
storm or some other real-time streaming
engine or maybe I want to run a message
passing algorithm over this data there's
actually a paper that shows that you can
compute PageRank in real-time using
message passing and with that your
PageRank would be available instantly
whenever you see a new page
and you don't have to wait for a long
batch job job - to finish so in general
what can I do with my cluster if I have
capacity there that I don't need right
now can I utilize these idle resources
well let's look at some of the apps that
I want to run so a message passing app
is also a distributed application it
runs in a cluster it runs several agents
every agent has some state altogether
the states of the agents make up the
state of the application and then they
just send messages to each other and
manipulate the state based on that and
eventually either the state converges or
sometimes it just keeps running forever
but this is a pretty common pattern for
a distributed application yeah another
kind of application is a streaming app
in the streaming application we have
actually real-time events they come into
the system and they're processed in a
pipeline of steps and the pipeline can
branch actually but it also runs in a
distributed mode and it can scale each
of these little boxes here could
actually have many instances or I could
do something really simple I've
developed the web app and I just want to
test it whether it holds up to the load
that I'm going to get on Thanksgiving
and so I'm going to run a distributed
load test very simple I just I just
start n nodes and they all do the same
thing they just hammer that web service
that should be really easy to do right
if I have spare compute resources I'd
really like to do that easily so why am
i telling you about this I work at a
company called continuity and what we do
is we build a big data platform that
allows you to run all these different
kinds of things in a single cluster and
it makes it really easy so right now
we're running things like real-time
stream processing ad hoc queries
MapReduce jobs and and several other
things in the Hadoop cluster which means
for us this is really a big problem and
we need to solve this we need to find a
good way to do this and what we really
want is a multi-purpose cluster where
all these applications all these
different
types of applications can run at the
same time coexist with each other and
how do we get that we get that with yarn
so what is yarn for that I have to go a
little bit into the history of Hadoop so
when Hadoop was written it was written
solely as a MapReduce engine and because
it runs on a cluster it had a cluster
cluster management component but that
was tightly coupled with the MapReduce
programming paradigm so the only thing
you could run was really a MapReduce job
and in the latest version of Hadoop 2.0
that has been changed so now the
resource management of the cluster has
been decoupled from the application
management that actually understands the
programming paradigm that I'm running
and the good thing about that is now I
can run different types of applications
in a Hadoop cluster yarn just like
Hadoop is written all in Java it's a
multi-tenant it has security and so you
can actually run if you if you have
clients you can actually run different
clients jobs in the cluster and they
cannot impact each other it scales to
many thousands of nodes most people
don't have a need for thousands of nodes
most people who run Hadoop run it run a
dozen nodes and maybe after a year they
go up to a hundred nodes but few people
go to a thousand nodes there's companies
however like Google Yahoo Facebook
Twitter they run clusters of thousands
of nodes because they really have
extremely big data and yon works on a 12
node cluster equally well as it works on
a 5000 or cluster so if you have a
growing business you can start with a
small cluster and you can you can grow
it and eventually you reach that scale
where you really need something like
yarn here what does yarn mean I believe
it means yet another resource nanny or
manager it's not clear
and the way it works is that it adds one
machine to the cluster which is the yarn
resource manager and the resource
manager knows about all the resources in
the cluster and then every application
needs to add one node to its sum to its
application which is the application
master and the application master
communicates with the resource manager
to obtain resources from the cluster
once it gets these resources and it can
start tasks and it's now able to control
the application so how does it work
every node in the machine of course it
has memory and it has CPUs but yahn
takes a slightly more simplified
approach what it simply says every one
of these compute machines has a number
of slots and a slot is you can say it's
a container in which I can run a task
and on each machine it runs a process
and that process manages and monitors
the slots of that machine the slot is
really a JVM you can run any Java job
there you can actually run any shell
command there if you want and what the
resource manager does is it assigns
slots in the cluster to an application
the application has a master and the
master request slots once it has two
slots it can start tasks there and it's
now in control until it releases those
slots
okay a few more details on how this
works so anything that's pink here
belongs to yarn and there are four
protocols involved the first one is I
have a client
this can be my laptop and from this
client I want to submit an application
and the first thing I have to do is I
have to start the application master for
that I use the client resource manager
protocol once the master is running it
communicates with the resource manager
to get resources that's the resource
manager that's the application manager
resource manager protocol there's also a
node manager running on every node in
order to start a job in a slot the
application master can't do it who that
do that itself it needs to submit a
request to the node manager the node
manager does it on behalf of the
application manager so this is how it
works
looks fairly simple or does it look
complex I don't know simple complex
simple Wow okay
you think this is simple good have you
written a yarn application so I thought
this is simple when I started using it
okay so let's go a little bit more into
the details how does it actually work
and let's just look at the first step
the first step is to deploy or to
actually start the application manager
in one of the slots in the cluster
that's the first thing I have to do so
let's see how that works I have to give
you a little bit of background for that
so why not let's not forget this all
runs in a Hadoop cluster and Hadoop has
a distributed file system which is
called HDFS all the data that jobs in
the cluster can share are in this
distributed file system and the way it
works is that every node has a local
file system and it contributes apart
it's local file system to this
distributed file system now when I run a
job locally on a node it sees only the
local file system through the native
file system protocols Java file java io
all right if I want to see things that
are on the distributed file system then
I need to go through the HDFS protocol I
have to use a different file system
protocol file system client so an
important implication here is that when
I run a Java job this Java job this jvn
can only depend on files that are in the
local file system it cannot depend on
jobs that are in HDFS because the JVM
doesn't understand HDFS and so what does
that mean
for us we want to start an application
master in the cluster and the client has
the jar file with all the classes on his
local file system now if we would just
say ok yarn start this class this is the
main class started in one of the nodes
that slot in the cluster cannot access
the local file system of the client so
saw in some way we have to copy the jar
file into the local file system of that
compute slot
alright of that container and the way we
do this is before we submit the job the
client uses the HDFS protocol to copy it
to the distributed file system it then
sends a request to the resource manager
and points it to the location on HDFS
the resource manager tells the node
manager I want to start this job in the
node manager copies the jar file from
the distributed file system into the
local file system and now we can run a
JVM that loads the classes from there so
it's a little bit more complicated than
one would expect at first
so what does that mean when I'm writing
a yarn client what are the steps I have
to follow so just for this first step to
start the application master I have to
first connect to the resource manager oh
the step zero is copy the job file to
HDFS so here I'm assuming that it's
already there I connect to the resource
manager then I request a new application
ID from now on when I talk to the
resource manager about this application
I have to use that ID then I create a
submission context that identifies the
application and I create a container
launch context that describes the
container the requirements for the
container in which I want to run the
master then I can define in that context
I can define the local resources what
are the jar files and other resources it
needs that have to be copied to the
local file system I can define what
should be the environment any
environment variables I want to send
then I have to define the command and
the command is actually a shell command
and then I give it three sauce limits I
can say this particular container can
only use two gigabytes of memory if it
goes above that kill it
and so once I've done that I can submit
the request to start the application
master okay so I can take you through
the code I have probably eight slides
that explain exactly these eight steps
who wants to see the code oh really
hmm that's interesting okay let's see if
we have enough time to do this so what I
have to do first I have to get a young
configuration object that extends from
Hadoop configuration I have to get the
actual location of the resource manager
its IP address and port from that
configuration and then I create a new
configuration a Hadoop configuration and
in that configuration
I set the well here I set a security
class
which is kind of awkward because I don't
know what it does it's simply something
I have to do I could say I don't want to
run security and then I can use a
different class here and then I create a
protocol in order to create a protocol I
have to go through a factory method and
the RPC here is a Hadoop RPC factory and
I have to provide the class that I want
to get and the configuration so now I
have a protocol with that protocol I can
talk to the resource manager so as I
said the first thing I have to do is I
have to create I have to request an
application ID that's what I'm doing
here in order to do that so yarn uses
protocol buffers for its serialization
of objects and it's its RPC calls and
for that you use factory a factory
method to to instantiate the request so
here I call record new record off the
get new application request class and
whenever I make requests in yarn I do
this in the same way this is the pattern
I go through this factory to get the
request and I can set properties on the
request and then I can send it and then
I construct an application context same
way and hope something is missing here
we can't see the last line so here I
basically create a container launch
context from two things that I just
generated don't know what happened here
why this doesn't fit on the screen
anymore okay so now I have the context
now in the context I have to define the
local resources and basically what I do
is I have to set for every jar file that
I want to have copied to the local
filesystem of the application masters
container I have to describe that where
is it on HDFS what type of file is it is
just a file or maybe it's a zip file
that has to be expanded or is it the
directory that has to be copied I have
to say what is the actual path on FD
HDFS I have to
the modification time and the size of
the file and the reason for that is
after it gets copied this is used to
verify that the file is actually the
same that it hasn't changed in the
meantime whether it hasn't been
corrupted when it got copied and I put
that into the local resources and I can
do that for as many jar files as I need
I can do that for property files I can
do that for whatever resources I want in
the local working directory of that
container then I define the environment
that's simply a map from string to
string important thing here is if I want
to have a class path set I have to
define that the framework doesn't do
that for me so if I copied any jars into
the working directory and I want them to
be available to the JVM I have to add
that working directory to the class path
I guess for security reasons it's not
there by default and then I define the
command so what am I going to run I'm
going to run a Java command a JVM I'm
going to run the main class my app
master and then I run some arguments
this is where you would fill in whatever
your app master does rather than
whatever arguments it takes and so I add
this command to the Container context i
define resource limits in the same way
again new record set some properties and
add it to the context and then finally i
can submit the request so now that it's
submitted I'm pretty much done because
this happens once the request is sent
and it is sent successfully
everything happens asynchronously I
don't need to wait for a response the
response will always be ok I can then I
can go back later to the resource
manager and ask about the status but
here I'm pretty much done in the client
ok do you still think it's simple
sure ok
all right so now I'm in the state I have
started the application master
I still haven't started any tasks right
it's now in the cluster and there is a
resource manager so what do I have to do
I pretty much have to repeat now in the
application master code I have to repeat
all these steps that we just did for
every task that I want to run and I do
that in a loop um so right here so the
first thing the master does it registers
itself with the resource manager it says
hey I'm here I was successfully started
and then it goes into a loop this loop
should probably run maybe every second
and send a request to the resource
manager this product call to the
resource manager is a single API call
and this single API call has as we see
five purposes so the first purpose is
the application master sends its
heartbeat
it says I'm still alive second thing is
it can request containers can say give
me five more containers with this much
memory and so on
it can also release containers that it
has previously received so it submits
that and then now it receives the
response from the resource manager that
response may or may not have some
containers it probably will have some
containers because we requested some and
it'll also have a list of containers
that have previously run but that have
just terminated so when they have
terminated I know I can release that
container so now I have received some
some containers from from the resource
manager now I want to start tasks and in
order to do that I send a request to the
node manager and that request looks very
very similar to what we just did in the
client to start the application master
all right and I sent that to the node
manager and say please run this task for
me and then
does all that for me a couple of caveats
here so the rule is an application
terminates and after all the tasks have
terminated the application master can
terminate if the master terminates first
then the resource manager doesn't know
what to do with the tasks that are
running they are now zombies or orphans
and because the resource manager doesn't
want to waste resources is simply going
to kill them all so you better write
your master in a way that it doesn't
crash there's some plan to support
restarting of the master in case it
crashes the problem with this is that
the master has some states heart it
knows about all the notes that it has
all the slots that it has all the jobs
that are running there it maybe knows
the state of each of them and now if it
crashes it loses all the states right so
when you bring up a new application
master it doesn't have that context so
that's the question what's the point so
this will only work if the application
master had a way to persist its state
and that's not there right now also so
starting a task in the container is very
similar as I said to starting the
application master it's always the node
manager that runs on a particular
computer note it manages a slot that
note manager will start the process for
you and the note manager will then also
monitor it if it crashes if it fails it
will notify you but it can't notify you
because it's not talking to you right
but the note manager does it informs the
resource manner manager and says here's
a here's a failed slot and then the
resource manager next time I call it
it's going to respond to me oh these
slots have have terminated either
crashed or with normal conditions and
then when I find out about that just
because the slots has terminated it's
still mine because I may have data there
I may have logs there whatever that I
want to collect so the resource manager
is not simply going to clean it back
from me
I have to release it explicitly another
tricky thing is how the interaction with
the resource manager works it's
completely asynchronous so when I send a
request and I say please give me five
nodes or give me five slots most likely
the response will contain zero slots
because all that this request does is
registers that request somewhere in the
resource manager and now the resource
manager will fulfill my request
asynchronously next time I make a call
I'm probably going to get back some some
slots that are requested in the previous
call so if I don't get back any slots
that doesn't mean that the resource
manager didn't understand me it just
means it may take a little while if I
make the same request again and I ask
for five again because I didn't get
anything back that adds to the five that
I already have so now I have requested
ten all right so every call to the
resource manager every request is
incremental and the master itself needs
to keep track of what it has requested
what it has received and how much more
it needs another interesting thing here
is when i run a distributed application
i probably want to have application logs
when something goes wrong i want to look
at the logs right the way that Yarn does
this and this again has to do with the
distributed nature of the file system
and every JVM will log to the local file
system when the job has terminated then
those logs get copied into the
distributed file system and then the
client can retrieve them from there now
that's a little bit difficult if you
have a job that never terminates that
runs forever but okay that's on a
different sheet of paper but this is the
way that you can retrieve logs okay I
was going to show you an example until I
realized how many lines of code it is
the simplest example that you can write
would be an application that takes a
shell command and runs it in n
containers nothing but that runs the
shell command into a minute
this is a sample application that comes
with yarn when you download the code and
it's sixteen hundred and eighty-one
lines of code for a simple hello world
example over time the api's have
improved a little and it's got a little
bit simpler but it's still in the
current version it's like fifteen
hundred lines of code and the conclusion
that I draw from this is yarn is not
simple at all and my colleague Terence
is going to step up now and show you how
it can be made easier so as you've seen
ganas parvo but at the same time could
be pretty compressed to write even a
very very simple application so there
are several sock comings of our yarn so
first of all you see the complexity the
protocol itself is very complex the API
itself is pretty low level for variables
so you have to write the write out
everything that you want right through
the API they provide and also the
clients needs to actually interact with
the HDFS they need to set up the EM
promise we need to know about how the
Hadoop M problem is being set up you
know everything so basically it's like
you have to do everything menu by
yourself right and also the other things
that end with mention is that the log
collections won't happen when before the
application actually time lace that
creates a products by especially for us
for example if you're running a
real-time and if it takes programs right
it never terminates right because the
real time strings keep coming in so you
never be able to see the laws and if
there's something happen you never be
able to know right and also the
application master as of the current
motion is a single point of failure in
yarn because it isn't support we're
starting a application master and then
between the application master and the
containers that you start to run the
task there is no defined at
communications so basically you're on
your own if you want your container to
communicate with your application master
for status for messaging for you know
any kind of thing
you have to work out your own protocol
to do that and that all this make it
really really difficult about that's
what we draw from my experience when we
start to use yarn and we think that most
of the typical distributed applications
of relative is simple simple in the way
that you always want a bunch of tasks
they may be the same task they may be
different tasks and then I want to run
it on a Custer with a number of
instances of this you know and above
tasks so there are several examples
right for example I want to run and
procedures and M consumers for a limited
amount of time or actually the tasks
that I write I don't want them to be
aware of too young right because if I
write my tasks in a way that it has the
way of the yarn it makes it really
difficult to write for example unit
tests or small skill tests or
integration tests because you always
need a full-scale Lancaster to run your
task right which is not desirable in
most of the scenario so that's why we
think oh can we actually mix writing
application or yarn as simple as writing
Java threads I believe most of you
actually deal with Java threads right
right so you simply just how to do it
right you simply implement implement a
runnable interface or the cotton balls
or you extend some threads right a
couple of ways and then you start to
thread or you submit your runnable to a
executor service right so and what
beyond that is for a distributed
application it typically needs several
more supports from the framework itself
for example monitoring and we starting
off the containers like for example if
your tasks kind of get killed or die or
you know exceptions an ax itself you
might want to have project to restart a
class so that it can you know keep on
processing incoming data and you want
you have like more close to real-time
log collections and metrics
actions rather than you have to wait for
your application Tammany in order to do
that and all also you want to have a
lastic scaling which means you can say
let's say you are using yarn to run
bunch of application servers okay so you
want to increase or decrease the number
of instances based on the current load
and you want to do it dynamically you
don't want to stop your applications and
then we start we configure and we start
it you want to do it in real time right
and of course for long-running jobs for
real-time jobs in our type of services
as well and that's come the weave
so we've what is weave so weave is a
library that we built that can
compliment yarn to make it simpler much
simpler to use it's an open-source
project is a virtually like apache
license its hosts own geek up to white
provides it provides some things first
of all it provides a generic
application master which means you only
need to focus on the task and energy
would be you only need to focus on
writing your runnable you don't have to
worry about like how to run it how to
negotiate with yarn or you know things
like that and the weave library provide
a DSL for specifying how your
applications should one like how many
tasks it contains how many instances how
many resources you know things like that
in a very simple way and then provide
set of API for you to use to define your
container tasks basically I'll show you
later but basically we call it with
runnable which actually is extend from
runnable itself so that you can simply
write your tasks by implementing a the
run method and also the in need and stop
like kind of life cycle methods and also
it has building support for service
discovery logging metrics and monitoring
as well
so you might not see the details of the
code but this is a showcase if we use
the weave library to write this
distributor shell applications as being
the example application
you can find a yawn distribution if you
just look at the the important part of
the code so if we if you remove all the
broiler plates the young version takes
around like five hundred lies okay and
then the complete program that you write
in we've it takes around 50 lines so
that's the simple simplifications that
we are talking about by using we've to
write distributed applications that can
run on a young custom so this is how the
framework looked like so the yellow part
the weave client is to think that you
use and you interact well basically is a
set of Java API that you can use all you
need to do is to define a bunch of we've
run about right and then you submit the
application through the weave Runner
which is similar to a executor or
executor service and then the whip
runner would do the rest for you
so it talks to the yang because again
resource manager and then it will submit
the general application master to run
and then that one will takes all the
runner boughs and the specifications and
request appropriate amount of resources
from the lancaster to run the brand
opposed to run a task that you specify
at that you submit through the crime and
it also use zookeeper how many of you
know zookeeper okay that's good so it
also used to keep so zookeeper in one
sentence is basically i distributed
custer management tools so state
management tools so we use zookeeper to
manage the states and messaging between
the application master and cry and also
between application master and the task
that is starts on the young custer and
one thing you might notice this in the
application master that we start we also
start a embedded kafka server so kafka
is a very lightweight
pop subsystem that which is the open
source system as well that we use for so
that all the tasks that we start on the
I cluster for their loss and metrics
they actually will keep publishing to
the Kafka system so that it's very easy
to be
consumer to consume the logs and metrics
so that you can have a real-time bills
or other locks and metrics being emitted
by auto tasks in real time so this is
how it looks like so let's look into a
little bit more details so let's say now
you want to write a application using
the route library what you have to do so
right now this this application is
showcasing a echo server which is a very
very simple one what it does is when it
runs it bind to a random pot and then
start reading in incoming message and
then just write it back to the same kind
and cross the connection that's what it
does so I skip some details about the
actual server part but I believe every
one of you will know how to write is
pretty simple if it's solidifying that
you basically have your class implements
with runnable which itself is actually a
runnable and then you can use the
standard sm f4j Locker to write out your
lot message the with framework itself
will pick that up so it will have to
write a panda and then start publishing
the logs to Kafka when these tasks start
and then you can get something called
with contacts from the with runnable so
that you can interact with the runtime
contacts for example are knowing which
holds your in or want to interact with
the discovery service step apart apart
with library and then you simply just
implement your what you want to
implement in run method so it's
basically no different than you
implement a friend right and also there
are some extra support like for example
you can say contacts or lands and you
can say well I have a echo service that
are available on this hose and this
point ok this what the with our library
will provided to you so that when you
write your client you can actually use
the discovery surface to see Oh actually
I have like 10 notes that are running
this particular service and then I can
select one of the are one of them and
then start talking to it or something
like this so it's a pretty standard
way of using discovery service and
that's it so that's as simple I said so
after you write your task right you
write your relabel now you want to run
it right so how to run it so you have
something called a weave runner which is
like an executor okay so the library
will have a way for you to create so you
just say a whip runner and then you
kneel a young with runner okay then you
can say I want to run I want to prepare
to run something so you say run adopt
prepare new echo server so it just Neil
your runnable so right you don't say
echoes of a top-class you just knew it
okay and then you prepare after you
prepare you can do some some more
modifications to this particular run so
for example you can attach a lock
handler so this one simple just collect
all the distributor locks and print it
out to the console screen that you run
your current okay but you can do other
stuff you can have a lot of collectors a
lot handlers that actually collect the
loss and write to some other HDFS for
future immerses or you know you get
hooking up with the monitoring system to
detect any video or any problem with the
containers they're in they're running so
and also you can do a bunch of stuff
like manipulating class populace adding
some more extra resources you know
things like that but let's say you just
want to start it right so now you you
prepare and then you stop after you
start you get some in coid weave
controller the controller simply just
expose a method for you to control the
running applications so you can for
example send some commands which is like
message right is you define your own
message is simply just key various
things that you can send to run the
applications right and then you can
actually send to a particular runnable
as well if your application is like
having a lot of different tasks and then
you can of course stop the application
there are other methods available in the
controller for example you can attach
listener so that you know the changes of
the state are running state of the
application as well those are kind of
for advanced the usage but
you can see it's a simple step so with
these two slides you already be able to
start the applications that runs on a
young cluster which makes your
application I'd disprove the
applications already so I would like to
go a little bit deeper so for example
you have more than one task so that in
that case we call it an application so
application basically is a collections
of relevance or collection of tasks and
the application will provide a
specification we call it with
specification which you basically
specify like one of all the run doubles
and for iterative run doubles you can
say I have a bunch of files and
resources that I wanted to be available
locally and then you can also say for
the pedicle for this particular class
tasks I want to have like five instances
of them which each of them will have two
cores and like a two gig of memory
something like that you can using this
DSL to specify things that constitute
the whole applications and the wif APL
support something like well you can
define bunch of tasks and then you can
define like like the ordering like the
starting sequence of them so for example
if your applications have multiple
things that you want you need them to
start in particular order you can say
that as well and so you construct your
applications and then to start it is
more or less the same way you basically
use the we've runner and s Appa now you
prepare the application not a single
partner but you prepare the application
and then you say stop exactly the same
way like the previous slide and as I
mentioned you can also specify order
like for example if your application
have free things like input reader and
then some processor and output writer
you want to start the reader first and
then the processor and then director so
you can specify it like this as well
so in summary most of time we see people
like having applications that generate a
lot of data hadoop itself the MapReduce
programming program it's kind of define
designed it for processing large amount
data but in terms of like batch
processing and large-mouth data that's
what it is good for that's what it is
designed for and eventually the Hadoop
ecosystem you know kind of evolved and
it'll recognize that the Hadoop cluster
itself has more potential than just
running batch jobs right so that that's
why they build a Hadoop 2 and then try
to separate out the resource manager and
Hadoop and MapReduce programming itself
so that you can starting to use yarn to
you know write different kinds of
application beyond MapReduce and then we
try to use that and leverage their power
and eventually we divide the designer
brief which we believe makes ya much
easy to use and as the previous keynote
some of you notice easy to use is key to
success that's what we believe as well
and we believe now we can turn like any
Java developers to be a developers that
is able to write these super
applications that can run on a cluster
that can scale form like doesn't upload
to thousands of nodes in the same way
thank you
so that's a I think it's question time
right so there are a couple of lengths
here the first one is the github link of
the weave library so anyone interest you
can check it out the second link is the
link to the Hadoop Jana is the official
site of the how to be on distributions
and lastly we found Cambria continuity
and we are hiring ok thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>