<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lead Designer of Scala, Martin Odersky: What's Next for Scala | Coder Coacher - Coaching Coders</title><meta content="Lead Designer of Scala, Martin Odersky: What's Next for Scala - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lead Designer of Scala, Martin Odersky: What's Next for Scala</b></h2><h5 class="post__date">2011-12-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qqQNqIy5LdM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so thank you for the invitation
it's a pleasure to speak here I was
already in at the debates meeting two
days ago and there I found that about
half of the audience hadn't done Scala
before so I said I had to derail my plan
and essentially give an intro to SCADA
and only got back to the some of the
juicy bit towards the end so I promised
all of these guys to say well if you
want to know more about what's really
going going on for SCADA 2.10 and so on
come here so here I won't let myself be
the rails we go right in the middle of
things apology to the newbies who
haven't seen SCADA before there's only a
couple of slides for you afterwards we
sort of talk about what's new what's the
Delta from what we have to what we will
have in a couple of months from now
so today Scala is doing pretty well
it's just a non-exhaustive list of
companies using it - you mentioned the
PHP meeting so one of the guys here
Foursquare actually migrated from PHP to
Scala some years back I don't I hope
they didn't regret it
not really and there are lots and lots
of others so what we see typically where
it's mainly used is web platforms like
LinkedIn Foursquare Twitter then also a
lot in financial services in particular
trading platforms financial modeling
simulation and it's really lots of odd
things lots of other areas as well and
the advantage that most people see in
SCADA is that it's very fast - first
product because it's pretty pretty agile
you can use it like a scripting language
it can be very productive and it's
scalable afterwards so you won't
typically run out of steam as you would
run if you were writing PHP
your Ruby or things like that it's a
language with a fairly good performance
story so some statistics here if you see
where are we in the grand thing of
things so that's just came out from I
think data is that some some analysts so
there here you see on that line that's
the number of Stack Overflow questions
and that one is the number of github
projects and you see Scala
is not yet a mainstream language that
would be the upper triangle but it's
sort of ahead just in front of the pack
of second-tier languages and hopefully
over time we can sort of shift this
thing a little bit more towards the
upper right here's another graph that's
the number of job ads indeed.com so it's
a job ad aggregator for all jobs in the
US so there you see it was basically
flattened noise and then it jerked up
that was when Twitter announced that
they were actually using Scala in their
stack at that point and then there's a
further acceleration that's when we
announced that there was a company as
supporting supporting there the
technology so in all these measures
we're doing pretty well and that's
actually very encouraging because
essentially for me the modern times are
not that that that old yet so SCARA 2.8
came out 17 months ago Paul told me the
other weekend I was totally surprised
because for me everything before 2.8 is
really prehistoric the dark ages so so
that the modern age of Scala is actually
pretty young 17 months and I must say
that anybody who really went deeply into
it before like like Twitter did it was
very courageous and I'm really glad they
did because that was really creatively
the critical mass but for the late
comers who got into in 2.8 or later it's
so much easier so 2.8 remain and the
remain new things were new collections
which then a lot of people like a lot
and a priority from when we start at the
company and I think we're close to
finally close to being there I'm going
to talk a little bit about what happened
in the inner Clips later we have more
recently done something on the dockside
there's a community dock dockside that a
lot of people have contributed into done
really good work and I think that's also
very very important because before the
talks where we're lacking and lots and
lots of bug fixes so that was Kara 2.9
and in the the main addition of 2.9 like
I said we're parallel collections so
parallel collections maybe I give you if
you're newbie then nevertheless a quick
intro of what they are just that you see
what they are
so that's not skaara that's Java right
so somebody can tell me what that thing
does where we are show
what does that thing do well it's okay
you don't need to know Java so that that
sorts an array of people partitions into
an array of miners and an array of
adults I was a bit mean to to specify
arrays so you had to use these in
intermediate data structures array lists
because well arrays you have to know the
size beforehand so it's tough okay so
that's on the bottom here you see the
same thing in SCARA so you say people is
an array of person and then you say -
and adults is people partition according
to whether their age is less than 18 so
I think it's pretty obvious which one is
shorter and also pretty obvious obvious
which one is clearer
so obviously this one is much closer to
what you want to achieve whereas this
one exposes all the details how you go
about doing that okay and that does that
line here I think James IRA you work I
think I got it from you so so you you
James was the first one who Pro product
is wonderful example because in a single
line that already shows some of the key
concepts of functional programming which
of course is a key ingredient in Scala
so what you have on the left is a simple
param match that partition thing
returns a pair of arrays and using
pattern matching we can deconstruct the
pair and name the parts of the pairing
weeks we say well the first one is - the
second one is adults partition is
actually a method call on the people
array in Scala every infix operator is a
method call on its left usually and you
might say well why do arrays have
partition method what's that data
structure and I should say you this
array of person is exactly the same as
the Java array so we do that with what
we call implicit so implicit wrappers
can actually inject new methods and also
new interfaces into all types and that's
very useful for these four
these situations and the final one is we
have a function value that's this
underscore age less than 18 that's the
function value of the criterion that we
passed to partition so that we can do
its job okay you might say good so
that's a quick intro to collections what
about parallel in Java I don't think I
want to try it's obviously too much work
to paralyze this partition thing at
least it wouldn't fit on a slide and we
would lose a lot of time doing that it's
possible but it's work so in Scala what
you can do is the only new thing is dot
par here so that's a parallel collection
people dot parse or dot Park on converts
any collection into a parallel
collection and that means that
afterwards all the operations on that
collections are executed in parallel
where it makes sense so taking the first
element of course you can't do that in
parallel but partition you can do very
well in parallel so that's an operation
that will be done in parallel here so
that's a obvious win last week for
instance growing factly
from The Guardian told Julian who
visited him that garden is a big
newspaper I think second largest after
New York Times in terms of web views and
I wanted to do a quick real-time
statistics how many people clicked on
what pages so I threw together a quick
app in an afternoon
using collections it was all very fine
there was a bit sluggish and then we
said well let's try to make them
parallel throw in a couple pars and lo
and behold it ran but faster it ran fast
enough so it's not always like that I'm
not saying we have invented the silver
bullet for parallelism I thought that
would be you wouldn't believe me anyway
if I told you that but it's there are a
lot of situations where it is like that
so it's a good good thing to try in
particular because collections are just
so good anyway that you want to program
with Corrections even if the parallelism
story it doesn't actually hold anything
and if you do that and well if sometimes
or quite a lot quite often they actually
do paralyze nicely then that's an
obvious way of course
okay so going to the types the types are
actually pretty challenging of this
whole thing and that's sort of the two
phases of Scala so on the one hand it's
a beauty to use so the user sees that
right on the other hand if you look at
how its implemented in what are the
layers and it can be pretty frightening
so the layers some parts of the layers
are that thing here that we say well
this array about something like an array
is a sequence and that is a special case
of an iterable and that's a special case
of a traversable and then we have here
the dot par takes us from here to here
so we have the parallel sequence and the
parallel iterable over here but then we
have to say well there should be cases
where I want to get a collection and it
I don't care whether it's parallel or
sequential I just wanted to do its job
like a map or a filter or or a partition
and in that case I have these other
types which says well it could be a
sequential collection or a parallel
collection they are called Jen sequel
sequence Jen iterable and Jen
traversable you might ask well why
didn't we just make par iterable a
subtype of iterable and the answer is
well that actually would break existing
code because when I interact with an
iterable now and I do a for each let's
say I know that the thing would go from
left to right sequentially so if I throw
a pirate herbal in there then that
assurance doesn't doesn't hold any
longer because the thing will be done in
parallel that means I might observe any
order including interleaved order so we
can't do that so in order not to break
existing code we had to put in this new
hierarchy and logically we should have
named that sequence and iterable and
traversal and then that sequential
sequence and so on but since those names
were taken
that's the names we invented now one
thing we actually think of we I take
that very seriously they say well
they're good reasons we have to do all
that but it's still a lot of types I
mean let's face it a lot of types that
hang together and integrate things so
one thing we are considering four to ten
is actually two
merge the traversal an iterable layers
because they don't differ that much so a
traversable is a thing that has a for
each method so you can essentially go
through it with with a function either
sequentially or in bulk whereas an
iterable gives you an iterator intervals
are somewhat more general because
essentially you can for instance go
through two collections in lockstep like
when you do a zip or a compare with a
four inch you can't do that for each is
only one collection but we might be able
to actually merge that better and that
would remove about 20% of the
superclasses of something like list
which might be worth it okay so the next
step afterwards so if you is to say well
parallel collections they're great what
about going to distribute it so big data
tens of thousands of servers or
something like that can be extend the
same things there and actually that's
been done several times and every time
the result is extremely impressive so
I've seen that scarra days are talked by
just your athon cascades which it has
nothing to do with cascading in this
whole area people we invent the same
names all the time so cascade is
essentially a collection caller
collections front-end over Google
MapReduce or rather flume Java so from
Java sits on top of MapReduce and has
some optimizations by fusing several
MapReduce steps from Java is quite
impressive software but it suffers from
the problem that is it's it's rather
pain to program so you have to
essentially describe every step with a
class and that's really bulky too why
are these things up
whereas with parallel collections what
you do is precise something like that
only now it works over the internet with
the things so that's really really very
nice and Josh Torres and Danny Mara did
that for that for MapReduce the spark
work well that will be presented it's
actually quite similar so that's another
thing that goes in that in that context
and also there's being something done by
Twitter and cloud era called scrunch
which is essentially the same thing now
on top of Hadoop so all this thing is
pretty exciting
it takes a lot of the tedium out of the
problem with the big data so I think
that's the next logical step to do that
okay so I've talked about to nine
parallel collections where we're going
to take them what happened now
essentially in the post to nine era
where I think three big things the
Eclipse IDE the play web framework 2.0
which got just announced in beta akka
2.0 which is close to being being the
first beta release and then finally it's
got a two point ten so I'm gonna talk
about each of them a little bit so the
Scala Eclipse IDE is very very close to
final it's now in our c2 it would be in
it would be in final now if it hadn't
been for spring spring released a new
version last week which turned out to be
incompatible in the weaving with what
our plugin did so it turned out that if
you installed Scala on top of offspring
that worked fine if you install spring
on top of Scala it would fry your
Eclipse and you'd have to essentially go
back and and install a new Eclipse so
bad bad luck so we couldn't release it
that way we had to fix that problem
first and once that's fixed it will be
it will be in final okay so the goals
for that version of the IDE were
primarily it should be reliable no
crashes no freezes it should be
responsive so never wait when you type
and it should work with large projects
and files so our own benchmark is the
Scala compiler it's a bit more than
eighty thousand lines of code some files
have five thousand lines of code so you
should be able to edit those things and
without waiting and getting all the
services like hyperlinking and
completion okay the Scala compiler is
also an excellent example of advanced
use of the type system with lots of the
path dependent types self-types make
sense the whole thing so the feature set
is to attain these goals the feature set
was intentionally rather small so we
wanted to do the highlighting
completions including completions with
implicit so let me
if you have an INT and all the stuff
that you get in an ER richen that's
added is visible or if you have an array
the partition method should be visible
even though in as Java array of course
it isn't hyperlinking and good project
builds based on SBT here and then the
other big feature that we had to do was
good support for mix Java Scala projects
which I believe we have now so all
features should work between Java and
Scala sources then J unit tests running
and then the other things we will get to
now that this is released but so far we
have gotten some contributions from
external libraries so essentially the
first step to get something some limited
refactoring a code for Mara that
actually works pretty well
mark occurences structured selections
and show inferred semicolons the next
big step beyond that once we push that
out will be to work on the debugger so
right now the debugger is a Java
debugger it's good enough but sometimes
it's annoying because it just doesn't
understand Java Escarra stack traces
that work and things like that that
would be the next step afterwards okay
so initial tweets were quite encouraging
so instance the only Freeman the author
of lyft jason said latest Scot Eclipse
plug-in works surprisingly well even
managed it manages our mixed Java Scala
project or hear the latest beta of the
Eclipse plugin is much better I'm
starting to like it
after years of misery the Eclipse that
I've written actually seems to work
quite well so you see that yeah
obviously there's some improvement so
that's all I think we can't we can't
argue about that good so the
architecture of the Eclipse plugin is
quite interesting because I believe it's
actually the only one that uses the
standard compiler as the interface for
for doing all these things all the other
ideas I know have a sort of a special
compiler like the the JIT compiler and
things like that it's not the same as
Java C also for instance in
laj have their own compiler to do
completion and and error error
highlighting not the standard scholar C
so we use the full scholar compiler for
everything for error highlighting
completion hyper linking and things like
that and the other thing we had to do is
wave into the JIT compiler where it'll
needs help because to do the interruptus
java essentially the Scala IDE the
scalar plugin needs to masquerade as a
sub perspective of the Java perspective
so essentially we need to go into the
JTP otherwise we couldn't have joined
projects and that was in times hard
that's also the reason for the holdup
with the final version because the JDT
was not meant to be extended there were
pet requests for years now to the JTP
team to say well please publish this
extension point everybody knew what they
were and they all got denied because i
said we do not want to use the j dt for
other languages except Java so if you
want to do that you're on your own we
won't support that ok
why rely on the Scala compiler well the
main reason was reused so writing a type
checker is hard I think 1 to 2% years is
optimistic it's probably more than that
to be consistent so if you have two
compilers they might disagree and one
might flag an era where the other finds
finds this thing ok that's very annoying
and also because quite a lot of compiler
plugins have already been written for
Scala and if we use the same compiler in
the IDE that means they all work in the
IDE which is of course a big advantage
so why might you not rely on scalar C
well guess the first reason is speed
scalar C is not a million lines per
second compiler it's more like a
thousand lines a second and that's if
you have a 5,000 line file that you want
to recompile on every keystroke that's
simply not fast enough so you have to do
some clever tricks to make it to make
that disappear and also there's a very
tight dependency that way on the Scala
version so 2.8 2.9 2.10 each one of them
has a different compiler and the Eclipse
plugin has to work with all of them
which was quite an
a constant engineering problem to make
that work but I think it's overall it's
it's the right choice to do that so at
the heart of the IDE is then the
presentation compiler which is a very
intricate piece of software the Steiner
compiler is already very implicit piece
of software and now we say well the
standard compiler now has to work
asynchronously has to be interrupted
live at every point you have to be able
to do targeted type checking that means
type check only some part of the thing
not not not the other parts of a file
and we have to be able to have it stop
and give us a partial result after
arriving at a certain point with type
checking so a lot of new demands to the
presentation compiler the way it's done
is that the presentation compiler sits
on its own thread and here the other
Eclipse threads and there's a work queue
so for instance your the Eclipse thread
might ask say might tell the
presentation compiler well I want to
know the type at a certain position for
instance in order to do afterwards a
completion so that that means to find
out all the members of the type and all
the members added by implicit that are
then presented to to the users away when
you do control space you want to see the
types ok so the presentation compiler
then would pick up the asked iPad and
would go to work it would go through
through the tool it a type tree and find
the in find the type at a given node now
it could be that the typed but so what
the presentation compiler does well
nobody asks it anything it will be busy
and we go and compile all the all the
modules that it has loaded all the files
that are currently opening everything
buffers the presentation compiler tries
all the time to re recompile those if
the user types a single keystroke it has
to throw away everything and start from
scratch because that keystroke could
potentially change all the dependencies
so so that's what it does and when
somebody asks then the same thing it
will interrupt what it does so on every
node when it type checks it listens is
somebody wanting something from me and
when it gets interrupted it tries to do
that
so if that's for instance an ask type
apt it will say well do I happen to have
the note where we are at this position
does it already have a type if that's
okay if it has a type it can immediately
return that if it doesn't have a type
then it goes into targeted type check
mode which says well I started the root
of this file at that type check
essentially only on the path of the root
to this node I don't go into left or
right subtrees I go from the root of the
thing to that note where somebody needed
the type and that's possible
unfortunately because the compilers
design essentially embraces laziness
everywhere very systematically so
everything I wear Anita types I first
created a symbol like a symbol for a
field or a methods or things like that
and it's type is a lazy lis computed
value that means only if somebody needs
the type it will start the computation
and the computation would then look at
the subtree and maybe to type inference
local type inference and produce the
type but if nobody needs the type it can
just leave these things alone
so that means going on the path to this
note and finding the type at this node
will just force the minimal set of
recompilation that we need so in that
sense the architecture of the Scala
compiler actually make this job of
incremental type checking much much
easier okay and then once it got the
type type at the result is communicated
through a sink wire and then it goes to
the next thing so that's basically how
it works
so all compiler activity happens on the
PC thread when the queue is empty we
compile the loaded files and the work
queue is checked when the type checker
reaches safe points in the ast so
typically it's done type checking a node
and we'll say that's a safe point I can
pick that up at about a thousand times a
second more than that a million times a
second and it drops everything when a
file is changed okay so the
implementation was
post some interesting challenges so by
that I see tech chure it means that now
because the type checker is restarted
every time what somebody presses a key
so you can literally have hundreds of
type checks tried type check runs per
minute and that means the tiniest memory
leak you have goes up very very quickly
because if you have so many you leave a
couple of bites behind after the
compiler run with that hundreds of types
runs per minute that could can grow very
quickly the the other problems were side
effect state and this targeted type
checking and the next part of it was D
so we needed to improve the compiler to
do that the initial part of the
presentation compiling and he did have
memory leaks that we had to fix forth
for the thing and that means we had to
actually back port things to all the
versions so the Eclipse plugin now works
with two nine one with two ten but also
with two eight and there we had new
versions of to eight so the latin the
latest official version of two eight was
to 81 and now we had a - eight - and now
it's twittering that essentially
contained only these improvements that
are needed for the presentation compiler
okay so that was the Eclipse thing the
other new thing is play so when you
already mention that it's used here it's
a cool web framework it's very much
inspired by Ruby on Rails is the same
convention over configuration rapid
deployment web framework or America
so essentially what it does it it
contains actually a hot compiler in the
play framework so you can say if you
scala file and we will pick it up it
will on-the-fly recompile it and
integrate it into the running thing so
no build step nor recompile step it's
almost like it looks like it's an
interpreted language where you load this
thing directly so originally play was a
Java web framework with a scholar module
and it's now migrating to a Scala base
that will have two API is a SCADA API
and the Java API that will both be
first-class API it's for the framework
it's going to be integrated in the neck
version of the typesafe stack and types
of will contribute to the development
and provide also commercial support and
maintenance so it's a very very very
nice framework and we're very glad to
have it in the stack also runs great on
Heroku the roadmap of what we want to do
then
so the initial typesafe stack that came
out in May that bundled SCARA to 9 and
akka 1:1 we had a point release in
October that bundles the 291 essentially
the yield greatly improved to 9 series
with akka 1/2
the next big step will be come out in
the first quarter next year that will
still be there to nine series of Scala
but then akka 2.0 which will be a big
step ahead for lack of time I can't
really talk about that in the talk but
if you ask me afterwards I can tell you
what it is and also the play framework
second version and then we're looking
about six months later at a version
which will have two it's got a two point
ten and then the the current releases of
akka play and also a database connect
layer that we are are working on SCARA
2.10 will probably come out immediately
after this probably also in the first
quarter of 2012 but we didn't want to
roll it in there into the stack because
we didn't want to force people to
upgrade the language to get this
actually the new libraries so the order
will be new libraries and then
immediately afterwards a new version of
the language which will then be in the
in the next version of the stack say six
months later okay so now we come to
SCADA 2.10 so what's in this Carra 2.10
over what we have now so I think the
biggest step ahead is the new reflection
framework that's sort of on the same
order of SCARA collections to say well
it's well no it's no scholar collections
replace what was there before the
reflection framework replaces avoid so
there was no scholar reflection and now
there is and that that's a big big step
forward there's been it it will be
integrated with something we call
reification the type dynamic
something useful to be able to interact
with the JavaScript in dynamic languages
and reflection also more IDE
improvements so we plan to have find
references debugger worksheet faster
builds we're working on that and then
there are some Scala improvement
proposals for string interpolation and
simply implicit switch look like they
will be accepted
we have Scala improvement proposal we
have restarted a process to equate the
community can discuss and propose things
to improve skaara
I think they're currently four of the
new ones out there so that's a bit
modeled after the Python pep process
Python enhancement process so
essentially open discussion and then a
jury will decide what what what what
goes in and I have the last word okay so
one of the things that will be new in
Scala 210 is the type dynamic so here
you see roughly what that is so dynamic
is just a market market right so nothing
more than that
and you can then have classes or trades
that extend it and those classes are
traits that extended have to implement a
method called apply dynamic which is not
the same as invoke dynamic you can
implement it with involve dynamic if you
choose but you can implement it in other
ways as well so apply dynamic takes a
method name and the arguments of the VAR
args with any and this one here doesn't
do anything except print what what it
gets but here's the usage scenario where
you say ok I create a new object new Jas
and I call now X dot foo of 1 so of
course that thing doesn't have a full
method but it will still type check and
it will just translate and to apply
dynamic of the Foo method and then the
added arguments and with the bar field
it's the same thing so that means that
we have essentially a gradual transition
from static to dynamic and reverse we if
we want to have objects that we don't
want to type check beforehand be it that
they come from JavaScript what you see
here or a lot of things let's say for
from database wrappers we don't
have a schema we don't know what's in a
row just call the type dynamic and you
essentially then you just delegate the
the responsibility to the runtime to do
the right thing with that a pretty
simple addition which i think is will
solve a lot of pretty hard problems the
second proposal for 210 is string
interpolation I was sort of again string
interpolation for a long long time
because I always said well this one is
not that much longer than that one but I
have to admit having after having
written about 10,000 strings with the
pluses in between that if that if it
does get tedious and it is not very
vegetable so finally we have an idea how
we can get string at interpolation but
actually much much more than string
interpolation in one one tiny package
and that was sort of for me the bang for
the buck factor was then high enough to
say well okay let's do it so what we do
here is that we would have a string
where just the dollar thing gets
interpolated but of course doing that
directly we can't do because while
strings have a meaning in Java and Scala
and the dollar is just a printable
character so we can't do that
so what we do instead is with the syntax
we write here and ice so the S means
essentially Scala standard strength but
the S could be something else so there
could be something an arbitrary sequence
of characters here it's an S so what
that gets translated to by the spec is
that the compiler will say well I create
a new string context where I put all the
bits that are not interpolated here as
of arak so Bob is years old and then I
call essentially my processing method
here it's called s so s will be a method
on the string context and that then gets
the argument that fits in here so that
gets the end and what the method then
would do is it would create a string
buffer it would put that in the string
buffer it would put that converted to a
string into the string buffer and
finally that and it would return the
string okay so that's how interpolation
works and it's geared towards the the
fact that this s can be pretty arbitrary
so here I told you what it does but if
you had another method name in here then
it would translate to the same thing
only with
other method name and that the other
method name could do other things and
some of these things are quite
interesting so one thing we could do is
we could have an alternate XML parser so
we write XML and then a bunch of things
in quotes and that thing would start an
XML parser that would actually give us
the tree that corresponds to X and up
that tree needn't return the same
representation as the current exam a
literals for instance it could return an
anti XML representation what 10 years
beaver has done so that means we
decouple actually XML from the actual
library and who knows if that's
successful then maybe at some point in
the future
Scala won't have xml literals anymore
because that will be a very good
replacement for it what would have to be
a deprecation process and things like
that but I I would personally be very
much in favor because I try to tell all
the all people look Scala is actually a
pretty regular and orthogonal and simple
and small language and they say yeah but
what about XML and I said yeah except
for exam I so if we can get rid of that
and I have a strong argument for for
that thing so I'm all in favor the other
thing we could do is actually then even
have a hook up a scanner parser with
that thing and actually have code that
inter inter related Scala code that ask
Alec parser would read that would create
a Scala tree and that would a add as
these these things into the tree so that
would open up the whole world of macros
quasi quotation and things like that
and he could think of many many other
things req expects prescience is a third
one that you could could imagine so the
that the simple change has has a lot of
very powerful potential ok
so that was the one sip the next sip is
so the everybody likes that so the
string control we went through a couple
of versions the final version it's
basically it's pretty clear that they
will be accepted because everybody loves
it the second one is not at all clear
where they will be accepted accept it
because it's very very controversial so
of
things why I put why I put it out here
is to get some feedback of you what you
think of that so the the thing is so
I've written a lot of Scala code of
course and it's gotten easier over the
time so essentially my fingers type it
automatically except for one thing
when I type if I always forget the open
parent always so I my fingers don't want
want to write right the condition not
the open open parentheses why is that
because in Scala actually there are two
other usages of the key word if in a for
expression is a filter and in a guard in
a parent match where I don't need the
parentheses and that's just not very
regular so can we like go and have a
alternate syntax that lets you write the
ifs without the parentheses then of
course you need R then to separate the
condition from the body so something
like that and while we're at it of
course we should do the same thing with
the while where we would do have to do
in the body and for the for expression
it would be the same thing I do know
parents need it here or a yield for the
year we already have so for me that's an
overall cleaner design less notation
less less less noise in the thing on the
other hand some people told me look it's
not standard and you don't change the
language anymore and now there's another
convention people will write with
parents or with that and things like
that and I accept that also of course so
it's not at all a done deal whether
we're this will will will go in or not
maybe after the after the talk I'd be
interested to hear your opinion on on
those sides okay the third one is again
something with that I believe is a clear
win and that's implicit classes so I
said the partition method gets added to
arrays with an implicit here there's
another here you see another class that
adds the min method so minimum two to
the int type so you can write one min to
or X min Y this gives you the minimum of
x and y and the way it's done is
it's a min method is a member of a class
rich int and the rich in takes an INT is
a parameter and it's an order to it
extends and implements an interface and
it implements the min method and that's
the situation right now so you have the
class here and then you have an implicit
wrapper method which we also call rich
int here that will create this class
disk like this this wrapper class when
it's called
and by by the fact that it's implicit it
means it will be inserted by the
compiler whenever somebody calls min on
an INT the problem is that's a bit
cumbersome to write and we said well
that's okay people tend to abuse
implicit anyway where if we make that a
little bit harder no no big deal but on
the other hand it would be much nicer if
we did it that way so let's just add an
implicit to a class and that way we get
the wrapper for free so basically
generated behind the scenes and that
means we get a lot closer to the
simplicity of extension methods because
people always say well yeah but these
implicit are so hard extension methods
in dotnet are much nicer and I have to
say yeah sure they're shorter implicit
can do more of course they can implement
new interfaces like this order you will
never be able to do that with an
extension method but there was a price
to pay so okay with the implicit class
we get the syntactic noise down then you
might have the other objection to say
yeah but you still create these wrapper
objects or we can keep our fingers
crossed and hope that the VM will
optimize them away but it doesn't always
it usually does a ok job but not a not a
perfect job doing that so the next thing
that I you you don't see on the slide is
that we want to give you an possibility
to write in line in front of the
implicit so we will let you have classes
that can be inlined
in line classes so what that means is
that if you then invoke a class with new
class name and then immediately call a
method and the class is declared
implicit the compiler will not generate
an instance of the class it will
immediate
call the method there are some
conditions to make that work Tamiya the
method must be final so the compiler
will know what method to call the class
must not have state the class must what
was the third condition not no
side-effects during creation but these
are all things that one can check that
the compiler can check and it can flag
the in line as an error and if we do
that and implicit wrappers would have
exactly the same efficiency as extension
methods so that that's a big win I think
okay so that's the next thing and the
last part which is actually the biggest
chunk and I'm gonna talk the rest of my
talk about that is reflection so
previously there was no reflection in
Scala so you needed to use Java
reflection and that meant that there was
no runtime info available on scara's
type people were incredibly inventive to
actually recover that runtime info
anyway so they were looking at the at
the Java signatures that we generate to
actually do interrupts with Java and
those can be recovered by Java
reflection you can look at the attribute
we can parse that and then they were
sort of concluding back well if the Java
generated signature is that and probably
the Scala type is that the problem was
that sometimes that was wrong and it was
brittle and it wasn't wasn't it wasn't
perfectly accurate anyway so what we
what you can do now is you can for
instance get a class from Scala class
class object from from from a string
like Java class for name you can also
get the type of an object so what we do
is actually we have a mirror based
design because that's sort of a more
general design with intended that didn't
have been the Java design so there's a
mirror a standard mirror has got to
reflect mirror that we can ask to get
the type of an object and you can then
use the type to do other things for
instance you you can say well the super
type with the with it with a given class
of that object what's the type so for
instance if your type is is is
string well no some car well in place
the list lists interest and we say well
what's the super type of traversable
we'll say well that's traversable of
into something like that you can get all
the members of a type you can get named
members you can get them from for each
member its type using the type sig
method you can ask for instance whether
that whether that type is a subtype of
another type so essentially you have the
full power of Scala types in reflection
looks pretty reasonable no okay so
that's actually pretty hard so to show
you how hard it is let me show you what
Java does so that's the interface type
in Java if we look at the Java doc we
find that it's actually empty there are
no methods on types but nothing you can
do with the type if you look at it and
the particular types subtypes like
generic array type parameterised type
type variable class then you find that
those methods only tell you what they
are like parameterize type type will
tell you well what's it its parameter
but there's still no methods like what
are the members of the type and postings
so are the only essentially full kit
that you have is on the level of classes
so that was pretty near extravagent for
actually has full reflection support you
can for instance ask what are the
members of a class is a class a subclass
of a class of another class but four
types nothing and it's not the same
thing
for instance list of int is not a
subtype of list of string right or
traversable of string so the question of
whether type is a subtype of another
it's a very interesting question and
there is no way to answer that in Java
so want to know whether an arbitrary
type a conforms to be in Java and says
write your own Java compiler that's the
only way you can do that so why why the
oversight why did why wasn't that done
why it actually turns out that in order
to do that you need to write the
essential parts of a compiler that's
what the compiler does decide whether
type a is a subtype of type beat
and if you do that then you will need to
ensure that your reflection compiler and
standard Java C compiler agree and
that's overtime it's almost impossible I
mean you have bug fixes you have things
in the compile it goes forward and
everything is to be back ported to the
reflection friendly imagine the
nightmare so it hasn't been done okay so
how can we do better the problem then is
to manage the dependencies between
compiler and reflection so we said okay
parts of a compiler will have to be
embedded in our reflection framework but
other parts not and the reflection
framework will add new parts so we have
a partial overlap so before I show you
how to do that it's maybe time to
refresh a little bit the state of
dependency injection in Scala
so let's see what we do then so
dependency injection means we want to
avoid hard dependencies to specific
classes so that we can rewire them in
our reflection framework that will have
to be rewired to either the compiler or
the runtime reflection and instead of
calling specific classes with new which
is a hard reference we want to have
somebody else do the wiring so that's
the parents injection frameworks for
dependency injection the most common
ones are juice and spring so here's the
juice example so here's a simple example
of essentially a coffee pot thing so we
have some service interfaces and on on
off device a sensor device warmer and a
client and then we have the
implementations so there's a there's a
heater here which is an on-off device
there's a pot sensor which is a sensor
and then there's a warmer which contains
the pot sensor and the heater and it has
a trigger method which says well if the
sensor tells me coffee is present and
heater on else heater off and there
there's a client object that takes a
warmer and and triggers the warmer and
the problem then is that to actually
wire their up to say well essentially I
need to know
I need to get supplied the actual warm
and the system I don't want to do it
statically with her no I have to do some
configurations so here we have some
binders which says well the on/off
device here is the heater the sensor
devices the part spot sensor the I warm
as a warm and the client is my client
and then we have these service injectors
and lo and behold mine finally we can
just start this thing so that's
dependency injection with juice which of
course works for for Scala as it works
for Java it's pretty standard but it's
also yeah I mean show that if you show
it to a newbie then I think there's a
lot of incantations you have to do it
this way you shouldn't you shouldn't
understand that right you won't need to
understand the basics how it's done you
just say you do it like that that's
that's okay okay in Scala it's actually
but in Scala you can do that but you can
also do another thing namely use the
cake pattern so here we have the same
service interfaces here we have the
implementation heater pod sensor warmer
component and then in the client we have
the the sub components as vowels and we
trigger it and the advantage of having
it with vowels is that we can freely
override that so you can override values
in in the in the base class here you
would do a new client and you would
override the on/off value with a new
mock heater in the sensor with a new
mock sensor for testing and it would
just work so over being able to override
fields gets you already part of the way
here you have the same fields on the
right so that's the sort of intermediate
thing in that in this new model
components are classes or trades
requirements are abstract values so the
warmer component here would say okay I
need an on/off device but I don't want
to specify which one so I just have a
while here which says well when somebody
who will implement me will have to
supply that device but for the moment I
will leave it all open and the sensor is
a sensor device so requirements I
abstract values and wiring is done by
implementing those values and the wiring
can be changed by just overriding these
values so it's our altogether
I guess a pretty simple model but it
doesn't really work just like the
constructor injection doesn't work for
cyclic in dependencies this doesn't work
for cyclic dependencies either so what
do we do about that for instead of
constructing injection induce you would
have field injection SATA injections or
that would work but that's altogether a
different thing I don't want to go too
much into that for Scala what we have is
the so-called cake para so what is the
cake pattern so we start with the same
service interfaces the service
implementations now say we have a heater
component and the heater component
relies on the fact that it needs to be
part of an assembly so that's done with
this self-type in the top where we say
this : assembly and then the heater
component has nested a class heater
which is the same thing as it as we had
before for the sensor component I do the
same thing the sensor component requires
that it's part of an assembly and it
contains a class sensor and finally for
the warmer component we have that here
so components are traits the wiring is
done by mixing composition we say that
our trait assembly is a warmer component
and the sensor component and a heater
component all together and the
requirements then they are the type of
this so what what do I mean by that so
here we see let's see where do we have
some for instance in the sensor
component we have a heater door off
so where did the heater come from it's
not declared in sensor component well
when we write heater that off what it
means it's really this dot heater or
that off right and the type of this in
sensor component is not sensor component
but what it would usually be but it's a
type that we indicate here so it says
the type of this in a sense a component
is an assembly and we go to assembly and
say aha there we have a heater component
and the heater component has a has the
heater so that's how the heater got from
the heater component into a sensor
component it could be used in the sensor
component but defined in the heater
component simply by saying ok these
things are mixed together here and here
we in a sense we require ready that
sensor component must be part of
something that's at least as good as
assembly so that's the idea of
self-types that we have in in SCADA and
it's again it's something that is in
principle is very very simple so what
the classical rule is always that the
type of this is the type of the
enclosing class and that's actually if
you think of it there's no good reason
why it should be the type of this could
be anything and here we just declare
what it is you just have to make sure
that when you have a concrete class or
you create an object that then the
concrete class has the same idea of what
its self type is what the type of this
is and all components did what what the
components want agrees with that so the
the situation is really very analogous
to abstract methods in object-oriented
languages an abstract method doesn't
have an implementation but the compiler
doesn't scream and says well you can't
rotate this class it has a method you
haven't implemented you said that's ok
by the time you create an object of this
class the compiler will check that you
will have implemented this method
self-types it's the same thing the type
of this is something richer than the
component that's ok by the time you
create an object the compiler will check
that the two types agree that's that's
all there is to it really and it's been
something that we started actually when
we modeled or object-oriented program
in theoretical computer science we try
to model that and it sort of fell out as
something that was slightly more elegant
in our model so we said why don't we do
it in the models but in the language we
don't need to do that and then we
somebody told me well no no you should
be honest what you have any model should
be in the language that's it okay put it
in the language as well probably not
much use usage and now we found out well
this thing actually gives you dependency
injection so it's big deal so it's a
very nice story how that how that made
it from sort of saying the model becomes
more elegant but I believe the code is
well okay why is it called the cake
pattern
well it's called the cake pattern
because you see the cake has slices and
layers so these things they are the
layers so there's the outer layer
there's the inner layer the warmer class
in the warmer component traits and the
slices are essentially the warm of the
sense of the heater so you can
essentially layer things and slice
things and then combine it all into a
cake okay the cake pattern is used in a
lot of Scala code I believe for instance
for squares whole system uses the cake
pattern almost everywhere so does the
Scala compiler so here's a very very
abbreviated thing what this got a
compiler does so a big part of the
compiler is dealing with types and
another big part is dealing with symbols
so symbols represent definitions
declarations like fields and methods and
classes and things like that and they're
too large parts so you don't want to
merge them into a single one and they
both need to know about each other so
the types when you have a type you have
for instance a method type is a
particular type that has parameters
which are symbols and a symbol of course
has a type so these we have references
on the type on the type level now from
one to the other not just on the field
level on the type level and then we say
a symbol table has the symbols part and
the types part it extends those two in
reality there are not two slices of the
cake like here but about 20 of depth
there are the different aspects of
things like that
okay so that's the compiler and it uses
the cake pattern so how can we make use
of that for reflection the first thing
is they are very close both need to
decide different the same the same
questions is a type of subtype of
another what are the members of a type
and so on but then again they are also
quite different so for instance if
there's an error the compiler would
probably show that nicely on the console
row in the IDE
whereas if there's an error if
reflection what can it do there is no
console it has to throw an exception a
compiler will access a lot of files when
it when it very compiles in reflection
you don't have a class path so you can't
actually go out and read files or things
like that you have to do it all
essentially with the loaded classes and
their annotations and so on so a couple
of differences but we are close enough
that it makes sense to combine these two
things so what we do is we try to work
with several cakes so we say we have one
cake which is reflect internal universe
which is essentially all the internal
parts of reflections that contain
symbols and types and what the members
are and things like that and then we
have NEC and it sees these the standard
Scala compiler stood at some point for
new Scala compiler now it's no longer
new for a long time already so NEC
global that's the root of the Scala
compiler that would be a subtype of that
it would add more stuff to that and
change some specific things like error
reporting to go to the console and then
there's another thing reflect run time
mirror which is the actual mirror in
reflection that also is a subtype of
reflect internal universe so each of
these is a cake of many traits in
classes and here we have two refinements
on the left and on the right so that
works well but there's a problem with
that that here this thing here simply
exposes way too much detail because
essentially for the compiler to compile
needs a lot of access to symbols and
types a lot of very very sophisticated
very subtle methods no way we're going
to guarantee that these are stable from
them
of the next nobody would have the the
the the stamina to document all these
things in a way that's expected from a
from a public-facing interface so we
can't do that that would be totally
confusing I mean these these things have
were way way too many methods so we want
to abstract things also there's that's
the other small issue that on the for
the public facing API we need to be
thread safe in the compiler we need to
be fast and not thread safe because it's
single threaded so so that obvious
differences here so what we do instead
is we put a cleaned-up facade on top of
this whole thing which is called reflect
api universe and that's essentially
there's an object that implements it
which is called reflective mirror
so here's the here's the the roughly the
thing of how we make this facade so we
have our trade types and it's part of a
universe and then we have a class called
apse type abstract type which gives you
the cleaned up into a interface of a
type so that's essentially what we want
to expose to the reflection framework so
it contains things like type symbol
declaration that gives you the member I
sorry no that gives you the thing that's
declared in the type member gives you
the member of the type all members we
have seen is subtype we have seen base
type so all the implementations of these
methods that we have seen initially are
in the first set and then we say here in
this trade types we say well it will
have a type type and that type will be a
subtype of Apps type but we don't say
what that type is it's just an abstract
type now in the actual implementation of
the reflect internal universe that's the
thing that we hide from you there is a
concrete implementation of that type
that actually does the thing but here
it's just abstract that we just say
there is a type that has this interface
and you can refer to it and you can call
all these members but you can't call the
others because they are hidden from this
type of structure so that's a very
powerful way to actually package up
things to do encapsulation information
hiding which is much more powerful than
standard interfaces the fact that you
can have abstract types is very powerful
so why can't we do that whole the same
thing with interfaces so with interfaces
the problem would be let's say we have
this less than method that takes another
argument he type they are the other type
right let's say we don't want to do that
so if we if we mentioned that this thing
is a type we have to tell you what it is
because it's part of the interface
so type leaks so we can't tell you what
it is we could maybe say that has a type
apps type right but that means that the
less than method then could only use the
cleaned-up interfaces of the apps type
and it probably wants to know much more
internally so I can't do that either
and we can't reimplemented with a pipe
because if we override a method while it
must have must be exactly the same type
it can't be something that's stricter
than the other thing so that shows that
with the standard way of just interfaces
you don't actually get this information
hiding if there are flows from one
method to the next by other types then
you really need to for power of these
abstract types okay so here's the
implementation that's the reflect
internal package so here you have the
trade types and here's the concrete
class type that would slot in so that's
like an abstract definition that's a
concrete definition that tells you what
it is it extends apps type of course in
a container full implementation of the
interface but it would also contain many
more implemented many more methods so
interfaces are not enough so in the
conclusion what I wanted to show you
here a little bit is what you can do
with Scala in terms of composition and
abstraction it's actually a very
powerful and but I believe also very
regular language when it comes to
position composition because it has a
couple of simple principles first as
everything can be nested classes methods
object types you can nest them all
inside each others second one principle
is everything can be abstract a method
can be abstract but so can be a value or
a type and third is that the type of
this can be declared freely and that can
express dependencies and finally that
thing together gives us a great software
architecture that allows us to attack
problems that are previously unsolvable
because I believe really to give a rich
reflection interface to a language was
an unsolved problem I don't know any
other language that has that and here
the the composition principles made that
problem solvable by being able to really
extract the key parts and make them both
reusable and to instances reflection
compiler but also be able to abstract
over that so that we can hide the parts
that are not not important for the user
okay I think I probably need skip this
thing and go right to query
okay so there was a recent blog post by
David Pollock where he claims that the
major obstacle to skal adoption and
industry is the binary compatibility
between different versions of the was
this process called fresh Scala the idea
was to simultaneously build all the
libraries and testing frameworks with
each new release and that was not
implemented yet so I wonder what are
your opinions and what are the plans to
automate this process yeah so it's
absolutely right so what we hear we've
asked a lot of people in industry that
Euskara
and people and also people that do not
use scallop for various reasons
essentially what are your main pain
points and we got number one always IDE
in Mercury tip so that's what we have
worked on intensely and number two
binary compatibility so what are we
gonna do about binary compatibility the
first thing is really what David
proposes he ran into an open door
because that's precisely the thing that
we have started doing the Josh Torres
who essentially started fresh Scala or
but David's tile first color but just
furious said she was volunteer to do it
he didn't have time then now he's part
of typesafe that's precisely what he
will do so so there will be a it's
called community build a set of blessed
libraries that where the idea is the if
somebody wants to contribute that
library it will be built nightly with
the nightly builds that means it will
come out at the same time as any new
scholar release you can then package the
kept get all these libraries in a single
package the other thing that does for us
because I think to the libraries they're
really two problems it's not just binary
compatibility it's also the fact that
some of the libraries get pushed out
with great-aunt it's anticipation and
enthusiasm but then they sort of whittle
away and people lose interest and
they're not maintained and then if you
bet your system on that library then
you've lost so one other part of this
community built will be well those are
libraries that are guaranteed to be
maintained so if somebody wants to use
the
library they must guarantee a level of
commitment to actually maintaining that
library if you want to propose further
libraries to do that and have and and
are committed to maintaining that and
send mail to Josh serious and we I'm
sure we would love to hear from you and
and work with you on that so that's
that's the the the short-term version
there are some other aspects to that one
is that typesafe will guarantee stable
bills so you don't have to change as
often the third part is that we already
have guaranteed binary compatibility of
at least minor versions so two nine one
was binary compatible two two nine and
we actually checked verified that with a
tool we've open-sourced a tool so now
other libraries can do the same thing
that if they actually put out a new
library than that library that can
verify that the library is binary
compatible one thing that's sometimes a
bit overlooked is that it's as much a
library problem than a compiler problem
because library risks being binary
incompatible if let's say you add new
things to at write and then implementers
of that trait become weaker can't can't
can't really work anymore the that
things that you can't do in Java it
would be illegal the compiler would
would would yeah you'd have to rewrite
your implementations Scala is sort of
one step ahead and says well okay you
can actually add new methods to a try it
as long as they have implementations
everything will work as it did before
only you need to recompile but the
problem is that since it's now so
convenient to actually add stuff to a
trait which in Java you can't do people
actually do it and write binary
compatibility the alternative is not
necessarily better so for instance if I
take Eclipse Eclipse has a couple of
interfaces that are numbered through so
interface one interface to interface 3
interface 4 and so on and that was every
time they added something to the old
interface and then for a client of the
interface it's a nightmare because you
don't know what you have you say well if
it's the new version then I can call
this if it's the old version I have to
do something else so you have a lot of
spaghetti code if instant
soft instance of instance of in Java
it's the only way to do it because you
can't actually add two interfaces in the
current Java in Java 8 there looks a
thing like virtual extension methods
that could solve that that would be very
much like the tried methods and I'm
lobbying with the brine guts and it did
the Java team to actually make that
general enough that we can implement
rates that way and that would solve a
lot of the binary compatibility problems
I'm not there yet so if you can help me
I have a good good direct line to to
Brian or something like that Lobby them
that they do that hey I use IntelliJ and
I sound like you're doing a lot of work
on the Eclipse side of things is there
gonna be that work that you're doing
it's like that will IntelliJ be able to
take advantage of some of that work for
the compiler or lobbies and eclipse a
year from now yeah so so we we
concentrated on eclipse because IntelliJ
is in pretty good hands already so we
don't want to compete with those guys
and and Eclipse was entire strikes so we
felt that eclipse was the one more in
need which is not a value judgment at
all that we say you should be using
Eclipse and not IntelliJ you're very
very happy that jetbrains and outside
contributors - such a good job with
IntelliJ can IntelliJ use the Eclipse or
what we do with the presentation
compiler in principle they could yeah so
intelligence model is a bit different
that they use the IntelliJ data
structures the IntelliJ trees that
shared between all languages and that's
why they get the whole they're fantastic
refactorings for free essentially you
can import them immediately and you get
this pasting from Java to Scala and back
so that's really a cute trick trick what
they can do where they suffer a little
bita stand in the supporting compiler
they had to write their own which we I
personally was was founded great that
they did that because it was really
fantastic to get some sanity check
whether our spec and our compiler match
so in the early days of the IntelliJ
plug-in I always got I think mails that
said well the spec says this
- says that explain please and it was
most most of the time it was a bug in
the compiler or more likely the spec
actually that respect just just didn't
didn't do things correctly so this was
immensely immensely helpful to do that
but now can they use the presentation
compiler I believe they could but it
would mean essentially having two
parallel trees so essentially to have
their own trees and then to just
generate the scalar trees on the side I
mean presentation compiler runs in its
own thread anyway and then to just
essentially get the information in and
out index by position to say to
type-check a tree on the IntelliJ side
they would say well that's the position
start point end point and that over to
the to the presentation compiler I get
the type out of it and continue so it's
possible and maybe maybe it will be
there I think it would I mean it would
increase the memory footprint but I
think it would would certainly increase
the the error highlighting in IntelliJ
yeah last year's at least two new Java a
GBM based language appears they sail on
from JBoss cutland from IntelliJ and
both of em seems borrowed a lot of Scour
I mean Steen syntax in organization what
I can
tol about Ram what are your opinion
about whether I think that's very very
good so it shows that I I mean we have
we have borrowed a lot of things from
other languages so if other languages
now borrow things from Scala that's
that's altogether a very good thing and
it shows well first we did something
that obviously is attractive and
worthwhile and generally I think that
having language experimentation and new
languages on the JVM is a very good
thing so I there there's some really
silly language wars that start and
people say well I do design a new
language says Scala and things like that
and I think that's just silly
so to give you one story when when I was
when Java came out in 95 I was very
attracted by Java that was even when it
was in alpha so I set out to write a
Java compiler because that should be the
basis of writing then
our extended language compilers pizza
and things like that and the colleague
of mine said well why do you write a
Java compiler there is already one
well that compiler then became Java C in
the end and and things like that so it
shows that there often people say well
why do you do this new stuff there's
already incumbent but I think well who
knows I mean it might be something right
that happens there so I think absolutely
innovation is good experimentation is
good or my opinion about art is length
so I haven't seen that much yet so
before we can so so I I've seen some of
the initial presentations where the
problem is you can't really judge before
it's out there often you say well we
want to do things this way and then
there's a reality check that you say
well Scala is also evolved so in some of
the things that we pushed out in Scala
we believed well this is very elegant
this is the right way and then we found
no actually there are lots of traps that
program has fallen to so we had to
change it so I think we need to judge
them and they're out it's too early to
do to decide now of the two languages it
seems like Kotlin goes more in this
carrot direction in terms of syntax and
salem but all that is please have some
superficial to judge the language by its
syntax so so we have to see when they
are out and then see see</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>