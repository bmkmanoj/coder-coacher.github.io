<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scala and Machine Learning with Andrew McCallum | Coder Coacher - Coaching Coders</title><meta content="Scala and Machine Learning with Andrew McCallum - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scala and Machine Learning with Andrew McCallum</b></h2><h5 class="post__date">2012-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DkRLTToCw-0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh hello welcome i'm very glad to be
here this is actually a new style
meeting for me i've never been to
something like this before but I'm
really enjoying it I I like new
experiences and maybe by way of
introduction I've had a number of fun
experiences along the way I've been a
chief maintainer for a free software
foundation project I've been a postdoc
I've worked in a startup company and now
I'm a professor at a university where i
do research in machine learning and
today i get to talk about two things
that i love a lot machine learning which
I've loved for many many years and Scala
which I've loved for just a few years so
what is machine learning they oh it's
essentially the study of computer
programs that automatically improve
their behavior with experience or
training data one way to see this is the
machines that program themselves but
don't worry the you know this is not
going to put programmers out of business
but it's actually going to make your
life harder in some ways look at what
this really means is that it's about
writing a program that encapsulates
within itself enough structure to
represent an immense number of different
possible solutions and it's then you
know then we use data to then estimate
the parameters that guide the behavior
through this structure of different
possible solutions and no interest in
machine learning has been exploding in
the last decade or more because so many
of the things that matter in the world
that we care about are really driven by
machine learning like genomics which is
essentially all about finding patterns
in gene data that's really revolutionary
is revolutionising biology we have free
web search and a lot of services from
google because they're machine learning
algorithms that have been very highly
tuned within google to show us relevant
ads that people actually click on there
exists already you know self-driving
cars and maybe we'll all have one
sometime soon and at the heart of this
in order to handle the uncertainty in
the world machine learning algorithms
helped make this possible and machine
learning is a big part of what makes
siri work inside of our iphones and how
do i know that actually well you know
before syria was an apple product it was
a startup company and before i was a
startup company it was a federally
funded research project with a lot of
academics involved including me actually
so I want to talk a bit about machine
learning it a bit more detail I'm sure
great yeah that this helps and
tically about its structures let's start
with a very simple example like spam
filtering in which you observe some
email messages and we call these these
are the observed random variables and i
don't mean variable in the programming
sense this is look in the statistical
sense and it's the job here to then for
each one of these predict whether it's
spam or not spam for each of its
corresponding things and the way and so
which what this is the predicted random
variable and the way that this is often
set up is with a function that I'm
drawing here which gets which takes as
input the to the values of the to hear
random variables that it's observing and
produces as output a real-valued score
that you can think of as a compatibility
score for how compatible do I find the
values in these variables like if I see
Nigerian bank account here and I see
spam and I think all that sounds pretty
compatible and you know for for not spam
I would say that's less compatible and
it's the job then of sorry so these
functions are called factors and it's
the job of probabilistic entrance to
cycle through all possible values in the
predicted random variable to say which
one score best in order to pick what we
think is the best answer and what I'm
drawing here is called a factor graph
it's a particular kind of beige in
network or graphical model that are
really it's really becoming the lingua
franca for so much of probabilistic
modeling that's at the heart of machine
learning so let's take a little bit more
complicated example like Chinese word
segmentation so you know in Chinese
sometimes one character is a word but
sometimes but sometimes a word is
actually two three or four characters
but when the Chinese right things that
there are no spaces between words it's a
trans person beats this they just are
finding the spaces themselves in their
head as they read it and so to do
natural language processing which is
actually my particular area of research
it would be very nice to find these word
segmentation boundaries to do later
kinds of processing so we observe random
variables that are the Chinese
characters and we'd like to determine
which ones are the start of a word in
which ones are not the start of a word
and we can do these with some factors
here but this is a task in which when
I'm determining that this is not the
start of the word it would be very
helpful to know whether I thought that
the previous word was a start or not
it's like the context of my solutions so
far so we want to add factors that that
capture this as well sort of
compatibility of
neighboring decisions and this makes
entrance a bit harder right because I
can't independently decide for each of
these red or green variables which value
to give it because they all interact
with each other and so naively speaking
I need to consider all possible
labelings of this sequence which is that
the size of that set is exponential in
the length of the sequence but
thankfully for graphs that are
tree-shaped there are efficient dynamic
programming solutions that can make this
fast okay so and this kind of model is
actually called a linear chain
conditional random feel it's one of the
most widely used methods for sequence
modeling and natural language processing
okay so then there are other kinds of
tasks that lead to really hairy
structures like the following
information integration so say that
you're trying to do a study of medical
outcomes and you'd like to gather data
from 100 different hospitals spread
across the country and so you want their
database of patients and you want to
integrate those together so that you can
do some data mining on the results but
each hospitals database was built by a
different DBA with a different schema no
different ways of setting up the values
and you would like to unify these all
together into a single database so this
involves doing schema matching
essentially aligning the columns of the
database against each other Co reference
especially aligning the rose against
each other because the same person might
have been a patient at more than one
hospital and then finally doing some
canonicalization to decide well how
should I represent the canonical values
of this merged database and it's in it's
a quite complicated task I'm not going
to get into great detail about the
structure of these models but i can say
just a few things in these models we can
do better if we know each of these are
representing a record from the original
database this is saying i think that
these all go together these are rose and
we actually have factors on partitions
of this model so we have in a way as
many factors as there are partitioning
possible partitioning is in the world of
this data which is there's a super if
that's called the bell number there's a
super exponential number of partitioning
I can't possibly create all these
factors ahead of time have to be
creating them on the fly as I'm would be
sort of considering various hypotheses
and then they're extra factors in the
model as well and this is just for the
row alignment part of its kima alignment
looks a little bit like that as well but
it's more complicated because there
could be hierarchical relationships and
how the schemas organized
and furthermore I want to actually be
doing both of these tasks at the same
time jointly because they interact with
each other and it's just incredibly
hairy and so how to do this as a big
issue and you know it brings up big
issues for me as a machine-learning
researcher because just how to do
entrance and parameter estimation this
model is very tough but today I want to
talk about another aspect which is just
doing the software engineering of this
is being a big problem for us as
researchers because often you know the
progress the essence of the research is
to propose a new model with a different
structure or different factors in
different places and it can take us more
than a month just to code up one of
these things and the nature of research
is that you try something and it doesn't
work and then you have to try something
else and you know it's really slowing
our progress how long it takes us to
engineer some of these things so there's
been some interest already in the scala
community in supporting machine learning
in various ways it's a very nice package
called scala LA where the last la stands
for linear algebra that essentially
provides matlab like functionality in
scala very nice this is out of stanford
up to ml provides data structures for
vectors and matrices and does amazing
things by automatically producing code
for your GPU not just the cpu to make
things incredibly fast spark is a
project out of Berkeley that makes it
very easy to do handle distributed data
to distributed computation has shared
types for accumulators that make things
quite easy there's another project to
Carnegie Mellon it's C++ not Scala but
it's can think of it as an alternative
to MapReduce that has updates not from
map and reduce but for update and sync
operations on graphs which is a very
nice fit for machine learning but so all
of these provide very nice facilities
for us to say take our machine learning
problem and distributor to make it
faster by being parallel but it's not
really solving my problem because it's
getting me kind of the same primitives
that I did my raw coding in before it's
just making them be fast and distributed
I need something that operates at a
higher level that makes it easy for me
to define models and then do inference
on them and a number of people have been
concerned with this issue and there is a
rising in the community this notion of
probabilistic programming
languages these are languages or dsl's
that make it easy to specify a really
rich complicated probabilistic model and
then using the full power the kinds of
things we're used to having
indeterministic programming languages
nice data structures control mechanisms
abstractions that help us with
probability models and then once we
define the model in a lot of these
systems the inference implementation
comes for free as a one-liner age the
guy here's the model now just go do
inference and you know it was it was
coding up the inference method for some
new model structure that was especially
tough for us before so I'm going to do a
whirlwind very fast tour of some other
systems mostly not in Scala and then
talk a bit about the system that that
we've been building at UMass ok so one
system out of Berkeley called blog is a
generative model of objects and
relations by generative I mean the
graphical model you visit the variables
sort of in some order of some directed
graph generate them one at a time
conditioned on their parents and it's a
nice you know small language that lets
you specify things with kind of the way
that statisticians are used to writing
equations you know the number of
researchers as a random variable that's
generated according some distribution
the collected some prior and then you
know this value will then get used in
the generation later and you build these
things up and it does inference by
Markov chain Monte Carlo which is a
sampling method church is a system out
of MIT that also is a generative model
this lets you tell the generative
storyline in scheme actually so you
write a scheme program with extra random
operators that say that represent the
uncertainty in the probabilistic model
and so you I mean the right mental model
is that you can run one of these
programs once and it will generate a
single sample from the distribution of
all possible values and you would do
inference say to get what's the
distribution over a given some
particular value on B or you would run
this program many many times generate
many samples grab just a subset that
have the particular value on B look at
the distribution of a and that subset
that would actually be quite inefficient
so the magical amazing thing that they
do is that they do Markov chain Monte
Carlo over possible execution paths
through these programs
inferred net is a system from Microsoft
it's actually getting deployed in
products that built up a factor graph by
a series of declarations I'll talk too
much about that let's see here Figaro is
in Scala it's just been released
recently it's by avi pfeffer who used to
be at Harvard it does now at Charles
River associates here in Boston it's
object-oriented it has this notion of a
model which is a basic building block
that encompasses both data and factors
and then they compose together and
Markov logic is a system that lets you
define where factors are placed by
expressing clauses and first-order logic
and so it's going to completely
declarative you just say here's the my
logical structure and then you know goes
off and does entrance so I want to talk
a little bit about this declarative
versus alternative consideration here so
in a way in a broad sense thinking about
declarative models has brought great
clarity to artificial intelligence
research over the last two decades but
I've been feeling like it's perhaps gone
a bit too far because a lot of the
domain knowledge that we machine
learning model builders have about what
good solutions would look like is
actually procedural or imperative and
actually I'm afraid that I'm using these
terms in a much more fuzzy way than you
would in this community in which maybe
use the more precisely all I really mean
is not declarative I just mean thinking
about as a sequence of actions and I'm
actually going to use procedural and
imperative interchangeably which maybe
you would hate I don't really know so
just read not declarative when I say
that and there's been increasing
interest in this direction like they're
in our machine learning community there
was a lot of previous interest in
combining logic and probability and
there's sort of rising interest in some
new systems in combining these non
declarative plus probability systems and
infer dotnet and shorts or two examples
so I'm interested in build and trying to
get the best of both worlds by building
imperative tools for creating a
declarative model specification so the
approaching the toolkit that we're
building preserves the declarative
statistical semantics of factor graphs
these lingua franca that for building
probability models
but provides these imperative hooks for
defining their structure their perimeter
ization the way that inference is done
and the way parameter estimation is done
so we've been calling these things
imperative ly defined factor graphs and
the design goals of our system are that
it will first it represents factor
graphs that it's very scalable so it's
not just a toy to be used in the
classroom but you can really deploy it
at large scale you know industry it's
large in terms of input data output
configurations the number of factors the
amount of total ban of data then we can
leverage object-oriented benefits that
variables factors infants methods or all
classes that can be subclassed and
modified and that it makes it easy to
integrate this sort of declarative and
procedural domain knowledge that we have
to bake as much smarts as we can into
the models that we built so our system
is called Factory it's a DSL for machine
learning with factor grass and it's
implemented as a library in Scala so you
know programs in factory our Scala
programs it's worked out very nicely for
us that it's a library and not a new
little language of our own creation
because we can now leverage all of
scholars great facility for just doing
our data pre-processing and and you know
debugging it all just printing out
Diagnostics in the middle of our model
definition and in leveraging Scarlett's
very nice type system and you know oo
design capabilities has made it very
nice to build subclass of all variables
and factors and objects here factory
strives this is different from some of
the previous systems that i described
strives to keep very distinct its notion
of variables in the random variables in
the model in other words the data that
represents a solution the factors in
other words the model or how we how how
the model expresses preferences for some
solutions over others the entrance
methods and the learning methods and
these are all defined quite separately
allowing a lot of mixing and matching in
a way if it's worked out very nicely for
us and it is indeed scalable we've run
it on you know billions of random
variables with super exponential number
of factors and there's a database
can so you can run on more data than you
can fit in memory so the way to think
about writing a prolific model in
factory comes in certain very distinct
stages that correspondence kind of
separation I was just talking about
first you define you think of as
templates for the data or these are
actually just classes to represent
random variables and you can use a lot
of data structures that you would use in
deterministic programming which is it
that unusual for people who are used to
more basic graphical models and only one
special requirement for doing sampling
based inferences that you provide an
undue capability on the setting of these
variables almost think about them almost
like transactions so you can say I
proposed to make this change to my
configuration and then you can undo that
then you define templates for factors
expressing preferences over certain
solutions and this is again distinct
from the data representation so I can
hear define what my problem is in
essence and then keep that constant as i
hypothesize different possible solutions
with different model structures without
having to change the stuff upstairs and
then I select an inference method which
often is actually just a one-liner but
you can also bake some domain knowledge
into that infants method and then you
read in the data create variables and
then call your one-liner inference
method alright so let's get a bit more
detailed so what are some components and
these are actually on the left all class
names in the library so there's a notion
of a variable a random variable which
all have values and can be set and have
domains there are domains which is a
representation of the set of values that
a variable can take on there's a diff
object and a dis list which represents
you know a list of changes to these
variables which can be undone there are
factors and the values of the variables
that are neighbors to factors in the
sufficient statistics of those values
sometimes factors belong to families
that may share common parameters and
some kinds of families are actually
templates which are able to generate
factors on the fly and then models are a
collection of factors or a way to get
ahold of factors and then some inference
method can work on some combination of a
model and some data
and here are some examples of kinds of
random variables we have some very
standard ones that you be used to in
statistics but some other ones that are
very powerful but unusual for poor
people in my world to think about
variables that could be set valued or
variables that are sequences so like a
couple summers ago I wrote a machine
translation system to go between French
and English in this that operated a lot
on sequences of words and actually I
wrote the whole thing in just seven
pages of code okay so let me show you an
example let's go even more concrete of a
relatively simple model it'll be this
linear chain model that we described at
the beginning of the talk it's going to
do segmentation here it's actually gonna
do sentence segmentation is going to
find the boundaries between sentences
it's good so it's going to put a put a
true at the beginning of a sentence and
a false at something that's a in the
middle or end of a sentence so first we
need some way to represent our data
which are labels and words or a token so
I'll define some classes for labels
which is a kind of boolean variable here
and a token which is a kind of
categorical variable it holds a word in
essence and I should say something about
how these things relate to each other so
I can say that they have a trait that
says that they're a link in a chain that
represents this this sequence of words
and this essentially provides those
variables with methods for previous and
next so they can navigate up and down
the sequence and you know these are just
regular Scala classes and so I can also
express how the data is related to each
other by adding member variables to so I
can say well what's the corresponding
token that goes with a label and vice
versa and this is maybe it seems very
simple to you but it's quite a change
from the way that a lot of people have
been building graphical models in which
there wouldn't be the sort of natural
pointers between related pieces of data
they might just build up independent
arrays and just remember that
corresponding indices actually are
related to each other and this has been
very nice for us also just the fact that
since these are regular objects we can
add arbitrary useful methods to our
random variables has been a big win for
us that hasn't been in some other tool
kits okay then we specify our model it's
so this is a model it's going to have
template for a fact for factors so here
we're going to say yes
we say that this kind of factor exists
it's a factor that expresses a prior
probability on certain labels
independence of any evidence here's
another one that expresses this factor
between tokens and labels so for every
label should have some factor that
expresses some compatibility score over
itself and its corresponding Toby what
this says and then here's how we specify
this vector here and then once you
specify the model you just you know lead
in the data somehow create a new
inference method like here's a gibbs
ampler and then ask the gibbs sampler to
do its work and it's these that's like
this one line and this actually having
it to its work that just is mind-blowing
to we machine learning people because
this is what would have taken a month's
worth of effort previously okay so this
is actually a deep bit of Scala magic
that isn't at all what it looks like
actually like this thing is not actually
have typed label it's actually some
quite complicated almost like getter
it's a method that knows how to navigate
among a sort of cross edges of this of
this variable and I wanna of these
variables and I want to try to lift the
cover of some of this magic here and
show what these things actually create
are these things called templates and
I'm realizing that I'm very short on
time I really might not be able to say a
lot of this but so you know we can
create templates for these different
kinds of factors and let's then think
about what work these templates actually
have to do right they're going to have
to create factors on the fly when
they're needed Sookie operation and say
a sampling based kind of inference is
that you propose some change to a
variable this proposal method runs we
automatically build up a list of the
variables that have changed and then I
need to find the factors that touch the
variables that change so if I just
changed the green variables I need to
find the red factors and I actually
don't need the other factors and so I
can be very efficient by never ask I
don't have to ask them for score as it
can ask just the red vectors for scores
and then I also need to find the other
variables that touch the factors that
are needed to calculate the
scores and so it's just sort of
complicated process of how to navigate
through this data structure and you
think about how to do this so there's
this question of how to find factors
from variables in vice versa and it's
Berkeley system they build a highly
index quite complicated data structure
to store this kind of mapping but it's
can be quite painful to maintain and so
what we're going to do instead has
maintained no such map between these
things and solve it in a different way
we can say well finding the factors from
the change variables is easy actually
because the number of templates is often
quite small I can just take each change
variable ask each template do you care
that this variable changed yes or no and
if it does give me a factor and that can
be quite efficient the hard part is
given a factor template and one changed
variable how to find the other variables
and in my experience when you've got
something hard that's a good place to
just declare this to be a primitive
operation and then let it be solved in
various different ways so this is what
we're going to do this within a template
you define these methods called unroll
which given a single changed variable or
responsible for somehow going out to
find the other needed variables that are
the neighbors of this factor and
returning them so let me show you what
this looks like in practice in the code
so here in this template we could be
given a single change a single changed
label and then we just need to find his
corresponding token and well you know
luckily we had code that makes that very
easy so you know these things are often
one-liners and correspondingly for these
tokens here we can just navigate through
label next and previous to find what we
need things can get more interesting
safe or I won't maybe motivate this but
described here's an additional kind of
factor that could be useful that's
expressing that is the notion that well
two words that have the same string form
I should have a mild preference that
their labels or maybe a bit more likely
to be the same and I would like this
factor to express that preference and
sorry so
okay no the default kind of
parameterisation here would look at all
possible values of these two neighboring
variables which for this named entity
recognition problem would be five there
would be 25 parameters here but really i
don't i don't need 25 parameters here
because i'm just i only really care that
they're the same value i don't care what
the value is and so the way that we can
express this is that actually these
templates are an instance of the cake
pattern in which there are multiple
needed pieces of templates that get
pulled together and we can pull that
apart to say that I have a template with
two neighbors but whose sufficient
statistics are not the original values
of my neighboring variables but
something different that i can define
and and i can define methods that make
that mapping between labels and values
in an interesting way okay I planned on
going through this in more detail but
let me for the sake of time actually
accelerate just a bit and show you just
very briefly an example example of what
this looks like we're running let's see
here oh no if I can actually I'm not
really going to fit this so here I'm
going to train a named entity
recognition system on two hundred
thousand words of newswire data finding
names of people organizations locations
and miscellaneous named entities oops
okay yep and so here I just loaded in
two hundred thousand words actually more
than that girls together to test data I
went through and found about 50,000
observable features of these things of
these words this is running on my little
macbook air it runs about three times
faster on the regular machine it's just
made its first pass of training through
all these things it's already getting
about ninety seven percent accuracy and
on third iteration it's done it's got
ninety eight percent accuracy here's
some examples so it's found that EU is
an organization and that Britain is a
location and so on and this kind of
training in an earlier system that we
did took about six hours and we just
finished it in 20 minutes and it's you
know Scala I didn't slow us down
what factory did was it let us use some
fancier new machine learning methods
that were quite a bit more difficult in
previous systems okay let's see here so
about 3 minutes left and so I'm going to
skip a number of things including some
stuff on topic modeling a minor point
that I want to make here just extremely
briefly is that we've also been very
interested in large-scale distributed
parallelism and we have found some great
advantages to thinking about how to do
parallelism and thinking about how to do
the machine learning entrance at the
same time in an intertwined way that
actually makes things significantly
faster it's like the inference method is
thinking not only about how to solve the
problem but thinking about how to most
efficiently distribute the data across
the machine and this is one of the ways
that we're very excited about say
integrating ideas from factory which
knows about the machine learning
modeling and you know with some of these
previous Scala machine learning aids
that I described earlier that are
thinking deeply about distribution and
you know putting them together in a
deeply intertwined way time to show this
but this is a like this is with
parallelism without being smart about
using machine learning to think about
parallelism and when you do them
together they can make a very big
difference okay so I'll just conclude
with just a couple of points here so
what we love about Scala I think it's
not going to be any surprise just the
same things that you love we've made
extensive use of traits and it's been a
big win for us we also use implicit
parameters say to keep you know
automatically pass in diff list
arguments we've made a lot of use of
function arguments say to capture
closures for partially built factors
that still need more information to
finally create them and you get that
information later you know we've used
implicit conversions and the cake
pattern in a number of places but what I
am more interested to talk about are
some things that we'd still like to see
from Scala and I have three things to
say so one is would be nice to have just
a little bit more efficiency and you
know I know that there's a desire not to
do premature optimization but I kind of
feel like we're at the point where it
just a little bit more optimization
wouldn't be premature so there's some
places where as we're scared to use the
Scala
collection library just because we just
get surprised by some strange and
efficiency there we're tremendously
happy that it seems that this you know
for loops over integers seems to be
fixed it's no longer you know wrapping
integers and you know creating a lot of
objects for us this mattered to us a lot
it was quite painful to just do while
loops everywhere but we still get
surprises like just last week we were
using signum you know which is one of
the easier it's a single operation on
the cpu just gives you the sign of the
number and but this is what it turns
into in scala unfortunately it create
boxes and it's just mind-blowing to us
it's just so sad so okay so it's some of
that and that point number two is I
desired for longtime constructor
arguments for traits right this would
let me get manifests with the manifests
for the type arguments of traits as well
as a bunch of other things I know
there's been discussion about this for
years and donna malaria what you know I
think he's been working with EPFL folks
is I think has a way to do this and I
know it's been sort in discussion but
you know not happened yet and lastly
this is my last thing and then I know
I'm just a little bit over time but so
this is something that I've wanted so
often and I don't even know I don't know
whether it's considered or being
discussed or whether it even makes sense
and i thought this audience could tell
me and this is the following issue so uh
you know when you you can stack some
traits that override a method and when
you called super foo here you know as
you all know I'm sure there's
linearization and the meaning of that
superfood depends on the linearization
of the traits it will cause them sort of
the implementation of through the next
step through set of traits I would like
the same kind of thing to be possible
when I say extends super foo in an inner
class all right so right now if I say I
have an inner class foo here and then I
have some some traits that create new
inner classes that extend super foo this
superfood was just statically refers to
this outer one and I would like to be
able to stack a bunch of traits here and
how
foo here incorporate you know all of the
aspects of all of the sub definitions of
through and all of these traits here
it's like a kind of cake pattern that
lets me reach into an already defined
inner object and add some functionality
that previously inter object with that
I'm sorry I'm a little bit over but i
closed out on a happy state questions
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>