<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Applying Failure Testing Research @Netflix | Coder Coacher - Coaching Coders</title><meta content="Applying Failure Testing Research @Netflix - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Applying Failure Testing Research @Netflix</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a3mHQCkr-4I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon I think we're gonna get
started
Peter and I want to thank you for
attending our talk and for the
opportunity to come here and to speak to
share a little bit of our experience
with you we're excited we hope you enjoy
it we'll be talking about monkeys and
lab coats or break you know breaking
things on purpose and production how we
have we do automatic failure testing and
how we made it work at Netflix so as as
the professor on stage it's probably
fitting for me to begin with a quote by
Aristotle a quote or a cliche if you
like that I'm sure you're familiar with
the whole is greater than some of its
parts and we've probably seen this quote
interpreted in various ways in different
context but the way I'd like us to
interpret it in this context because
this is a talk about collaboration and
how it can work is that if a
collaboration is done well and carefully
we can walk away with it from with much
more than sort of the mere totality of
what we've brought to it that is to say
collaboration when done right can be a
multiplier oh one other point I'd like
to make and this might be sort of a
controversial point is that it seems to
me from my experience that the further
apart you begin sort of the further you
walk from two poles to a a middle where
you meet when you do a collaboration the
richer the opportunities for
multiplicative effect I hope will
convince you of this today and so just
to give you an idea of just how far
apart Colton and I began let me give a
little bit of introduction of us so I'm
Peter Alvaro I'm an assistant professor
of computer science at UC Santa Cruz
before that I was a PhD student for many
years at UC Berkeley and before that I
was a software engineer for about eight
or nine years in industry when I left
industry I swore I'd never go back so
never say never something that I have to
come clean about for this crowd even
though it's a little bit embarrassing is
that I'm really a prototype er at heart
I'm so all about the idea that once I
convince myself that the idea is
possible by constructing a prototype I'm
not that great at finishing it I wish
somebody else would take care of that
that mere engineering and so this
collaboration was quite a learning
experience for me being forced to push
stuff all the way through
so hi my name is Colton Andrus I cut my
teeth on failure testing at Amazon we
were on the retail website availability
team and I had the opportunity to build
a failure injection system there to help
us become more resilient after that I
went to Netflix I figure if there's a
place that you can go and learn more
about failure testing Netflix has got to
be the place and indeed it was but when
I got there I found that there was some
opportunity to improve I had an
opportunity to contribute and to build
kind of the next generation of the
monkeys there with fit a failure
injection service at both companies I've
had the privilege to serve as a call
leader or an engineering crisis manager
someone who's nominated to manage
large-scale incidents to be on the phone
to help resolve it to figure out what's
wrong and truth be told I kind of miss
getting paged at 2:00 in the morning
there's a little rush that comes with oh
this is broken for everybody I can fix
it to further emphasize how different we
were at the beginning of this
collaboration I think it's worthwhile
looking at the figures of Merit for our
respective communities how does in an
academic versus a practitioner measure
whether they're on the right track
whether they're achieving success and so
as an academic one of the most important
sort of KPIs is the so called h-index
can I see a show of hands who knows what
an h-index is hey some people know okay
so my H index is K if my K most cited
papers have been cited at least K times
yeah so it's sort of a you know slightly
sophisticated retweet count
we're also measured by our ability to do
research in the future which really just
comes down to dollars cost about a half
a million dollars to mint a PhD so how
many millions have you brought in from
grants from gifts from industry and
things like that and arguably maybe most
important how high is your department
ranked among other departments in the
United States in terms of the quality of
research that you're doing so three
different KPIs that collectively tell me
how I'm doing as an academic measured
very narrowly based on my impact on
other academics so it's just a
popularity contest it is a popular
contest and that's why it's important to
remain popular because popularity
contests work out great for for popular
kids so what do we care about an
industry we have some hard metrics
you're probably familiar with that
measure whether or not we're winning
whether we're successful at what we're
trying to accomplish the measure of
availability however you slice and dice
it is an important one the number of 9s
that you're able to serve to your
customers to me we're winning if we have
four nines or five nines of availability
our customers essentially never
understand that we're down then what
happens next is the number of incidents
so sometimes it's also the severity that
goes into the the overall metric but if
you're on calls all the time if your
things are broken all the time no one's
happy customers aren't happy management
isn't happy engineers aren't happy and
lastly and I think most importantly I'm
lazy so if there's something I can do to
reduce my operational burden if there's
something I can do so that I don't have
to firefight in the middle of the night
then I'm all about that so kind of an
unlikely team in academic and and an
industry collaborator so how did this
come to be so I want to I want to
provide a little backstory here but I
also want to provide a little context so
before we dive into how this came to be
let me tell you a little bit about fit
so I mentioned fit or failure as a
service is what we built at Netflix and
it enabled us to do this collaboration
because it provided a couple of key
things fit is built to be application
layer failure testing and so what it
enabled us to do is decorate individual
requests as they entered the Netflix
ecosystem and follow that request as it
traveled its path and break anything we
want along the lines it integrated with
our tracing infrastructure so we could
see what happened and it led us that
that fine-grained granularity allowed us
to control the blast radius or the
potential for who we were going to
impact so we could start and we often
did start by testing a single user mine
usually or a single device maybe the
Xbox I have maybe my browser and as we
saw that that worked as we expected then
we built upon that we increased the
failure scope so then we might run for a
percentage of customer traffic
then we might run for 10% of customer
traffic and I find that the successful
failure test that we ran our goal was
always to run it at scale at a hundred
percent in production so you may not
have known this but we've done this
several times and probably while you're
watching Netflix maybe it manifest is
the ratings go away or things change but
but we were able to to do this
successfully
the last bit I'll say about this is we
built it as a service we wanted other
people to be able to extend this idea
and to build upon it in ways that we
didn't anticipate and so we provided a
service API and we allow it to be
extended upon I'll leaves giving a talk
tomorrow about chap which I'd recommend
that you attend but chap is built upon
fit so the infrastructure and the the
failure service that fit provided
allowed them to do these more
interesting experiments on top of it so
great fit works great we went out we
were able to use it to help improve the
reliability of both the edge services
that we owned and Netflix as a whole but
it's primarily a manual process we would
work with engineers we would sit down
with teams we would talk about what
could go wrong and we would work through
those exercises now obviously we would
prefer something that would scale
without the number of engineers involved
so surely there's got to be a better way
and the opportunity here was for me to
be able out and to look for people
smarter than I who might have figured
out this problem and lo and behold I'm
at my desk it was holiday break 2014 and
I'm watching this video from this
interesting-looking professor guy and
he's talking about lineage driven fault
injection and I I listened to this talk
and I go read the paper and it's it's
very interesting I thought AHA this is
it this is a way that we could go about
doing failure testing for everyone
automatically so how do you get how do
you get time with a professor well one
you blind email their CS that
berkeley.edu email address and you say
hey I read your talk and I saw your
presentation I'd love to chat more I'll
come to you in the city and by lunch
would you like to get together and truer
words were never spoken
because grad students and I was actually
grad student at the time grad students
and professors are extremely lazy and
extremely hungry so so you know buy me
buy me lunch sometime so Colton came to
the city and he bought me lunch it's a
really nice lunch and we had a great
talk I shared some my ideas I I went a
little deeper into LDF I that then I did
in the paper right so Colton learned
some things and uh Colton told me more
about the Netflix infrastructure that I
had ever known or maybe even wanted to
know and it could have ended there you
know that would have been an example of
like an additive collaboration where we
both come to the table of some things we
know and we we trade a couple things
Colton goes back to to Netflix knowing a
little bit more about the academic take
on how to automate failure testing I go
back to academia with a little bit more
street cred next time I write a paper I
might have a case study that looks a
little bit like Netflix and has a little
bit you know it looks like a realistic
use case right
but I know from experience that that
this kind of additive collaboration
doesn't really move the needle much it
doesn't really advance the state of the
art it would be additive right so fast
forward a couple of weeks Peter calls me
up and he says hey I've got a crazy idea
what if we built this do you like
Netflix is a hip place they're open to
taking risk do you think that that
they'd let us go build it and Netflix is
pretty good one of their core values
freedom and responsibility if you you
have the freedom to make choices and to
take risks and you're responsible for
the outcome so I went and I spoke to my
management and then they grilled me a
little bit on the value and and how it
would translate into to real value for
our customers but in the end we were
able to convince him and so Peter came
and was my professor intern for a summer
as we we went about exploring this idea
and you know just to pop up a level you
know I made a big deal in the very
beginning of this talk about how
different we were this was really a
point of convergence for us at least
within Netflix the responsibilities and
freedom of Engineers look very much like
the story in academia right and I could
even we don't have any bosses we're
completely free but as Milton said
we're free to fall right any chance we
take any gamble that we take all we're
putting on the line is our is the
success of our career and so both of us
a little bit concerned about this
responsibility did some due diligence
before we did our collaboration we both
wrote down individually what we wanted
to get out of this summer how we would
know if we had succeeded and when we
compared notes we were delighted to find
that we had written more or less the
same things so for my part it's really
important to me to take these ideas that
we've developed in the lab these
idealize models of reality and to
provide evidence that that they match
reality right equally important you know
we want to be able to show that our
algorithms actually work at real-world
scale because as many of you know or
maybe don't know in the lab we don't
have large-scale systems right we're
forced to simulate them and we're always
afraid maybe we're quiet about it but
always afraid that because we're
creative people we're inventing and
solving imaginary problems rather than
addressing real-world ones right and
then arguably most importantly when I
returned after that summer to the lab I
wanted to return with a treasure chest
full of real bugs from real systems that
wouldn't have been found without our
work and it turned out that Colton had
more or less written down the same exact
thanks oh right so now it's time to dive
down and talk a little bit about the
idea behind LD fi that so attracted :
what is the main idea and how does it
work
right considering microservices based
company in which a large number of
services that are independently
maintained interact yes well what does
it mean for a user to have a successful
interaction with something like that
that might involve connecting some large
number of services let's say like 100
services at the end of the talk we'll
see an example in which a single
interaction actually involves
cooperation of a hundred services and in
order to have confidence that those
interactions work correctly under fault
to a first approximation we would need
to see what they do under every possible
combination of false right so if we have
a hundred services that are
rating at least back-of-the-envelope we
have to consider the set of all subsets
that's 2 to the 100 different
experiments that we would need to
perform to cover the space exhaustively
exhaustively that's like a one with with
30 zeros after it right so that's
obviously not going to work now we might
not be ambitious we might say well we're
going to consider what happens when one
service falls down during an interaction
and that's tractable that's like a
hundred execution so you imagine that
these experiments are going to run order
of seconds right but if we turn the dial
up to four faults we're looking at three
million executions if we consider up to
seven faults happening we're looking at
16 billion experiments we would need to
run this this isn't going to work right
we need to do something smarter now an
approach like chaos monkey which was
pioneered in Netflix says well it's hard
to do much you know like when you do
random you certainly don't hit the worst
case right so we could flip coins and
that's pretty much what chaos monkey
does you imagine there's this dartboard
characterizing the space of possible
experiments under failure and chaos
monkey randomly picks failures to inject
some of them are fairly deep in the
space some are quite shallow right now
the advantage of random search is that
it's very easy to automate right random
search works equally well on any
infrastructure you don't need to tune it
to an infrastructure unfortunately
random search doesn't work very well on
any infrastructure right and certainly
it doesn't give us any notion of
coverage like let's say there's a point
out here where there's a known bug what
are the odds we're gonna hit it with a
dart and can we ever hope to blacken
this this dart board by stabbing into it
at random right the Sun will be burnt
out before we finish an approach that's
more popular in practice when the colton
already alluded to that that the users
are fit to is to exploit the intuition
of the actual service owners right in a
service oriented architecture different
teams manage different services there's
quite a bit of deep knowledge on a
service by service basis so you can
exploit that knowledge and do a sort of
engineer guided walk through the fault
space achieving much better coverage
exploring some deep faults and some
narrow faults but unfortunately unlike
random search this approach is
fundamentally unscalable right it's only
as good as the intuition of the
programmers and it only scales with
humans right so the i main idea is to
kind of - main idea
behind lineage and fault injection the
first is that you know fault tolerance
is really just redundancy like
definitionally fault ha and redundancy
are like synonyms a system is fault
tolerant precisely if it provides more
ways to obtain some good or expected
outcome than we anticipate there will be
failures in an execution right so if we
could spot the redundancy in a system we
could know what faults it can tolerate
or conversely what faults would make it
fall down but how do we know redundancy
when we see it so another key idea of LB
fi is that instead of starting with this
really open-ended question like this is
how we do verification with model
checkers we say could a bad thing ever
happen let's just check every state in
the state space and say that a bad thing
never happens when the state space is
large this gives us very little
direction about how to carry out the
search right a better idea is to begin
with some good outcome or some
representative class of good outcomes
and say exactly why did the good thing
happen and you know could the system
have made any wrong turns along the way
the answers to those questions are going
to turn out to be the much smaller set
of faults that are going to be
interesting to spend resources injecting
ok now I come from a database background
and in the database world when we think
about how we explain why things happen a
really popular tool is provenance or
lineage a succinct explanation of the
data and computational steps that led to
some outcome so let's take a look at
lineage a little bit more deeply let's
consider a good thing that happened in
some execution you imagine it's a system
like Cassandra the good thing that
happened was that we did it right and it
was acknowledged and it was durably
stored you say well that's good why was
it good so we start to unroll in this
well the rate was stable because it was
stored on some replicas replicas a but
that's not the only reason it was stable
it was stable also because it was stored
on another replicas replicas B its
redundancy right ok but we continue
asking this why question why was it
stored on replicas a well because the
client broadcasted its replicas but
there might have been more than one
broadcast there might be more than one
reason it's on replicas a right so when
we're done recursively asking why
since were left with this graph that
essentially shows us how the good thing
happened and what redundancy there was
in the system to support it right so now
we can start to ask okay could I make
any cuts in this graph that could
prevent flow from the leaves to the root
right so naively the replicas could fail
or the broadcast could fail that's four
things that could go wrong so that's two
to the four experiments that we could
carry out sixteen experiments but it's
easy to see just looking at this graph
that not all those experiments are
interesting right like it wouldn't be it
wouldn't be interesting to fill these
two things because I can see that
there's a path from leaf to roof right
by contrast those two failures look
promising
right because they block the flow okay
well how do we take this kind of human
intuition staring at a graph and turn it
into a problem that a computer can solve
efficiently right well we can just sort
of mine out the individual sort of sub
trees that are themselves sufficient to
produce the outcome like a client could
broadcast saying I could get two
replicas and would be stable we say okay
well what would it take to break that
subtree well we could crash replica a or
block broadcast one we said wait a
minute there's more than one subtree
here's another one and to break the
whole system we'd have to break both of
those sub trees so we'd have to do this
and this yeah and so we just keep
turning the crank mining out these sub
trees until we've exhausted them all and
along the way we've caused up this
boolean formula which you may recognize
as already being in conjunctive normal
form so we could just take this formula
as it is and pass it to an efficient
off-the-shelf Sat solver and say are
there any solutions to this formula the
solutions to the formula are going to
represent faults that are interesting to
inject that could break the flow for
example here's a suggestion interrupt
these two broadcasts okay so what we get
out of this and indeed what we wanted
was every time we cast a dart into this
dart board we don't just cast it
randomly we cast it into a region that
we can prove based on our model is
interesting could break the expected
outcome but more importantly every time
we cast a dart into the dart board it
casts a shadow over the search space
that we never need to explore again we
add constraints to our model and our
solution space diminishes
quickly so at a high level when Colton I
began our collaboration I said Colton
here's the recipe for LD fi it's easy
you have all the pieces it'll be a snap
you start with the successful outcome
and work backwards you ask why did it
happen and this recursive process of
asking and answering why questions gives
us a lineage graph which we can
efficiently encode into a CNF formula
solve the solutions represent hypotheses
that we should go experiment with using
our fault injection infrastructure and
when we run an experiment one of two
things happens right we find a bug in
which case success or you know defeat or
something I'm not sure it depends how
you look at it or we find evidence of
additional redundancy and keep going
around the loop until we run out of
moves so we're either going to end up at
a bug or we're gonna run out of moves
and say we've covered the space
everything looks good great so we've got
an approach now we're gonna talk a
little bit about what what the process
looked like how it evolved over time how
we took this idea seems simple and
straightforward
maybe not simple seem straightforward
and and wanted to apply it to production
and some of the challenges that we
encounter ok so you know our recipe says
step 1 is start with a successful
outcome so okay so like what it you know
in Netflix what a success look like yes
this this was a fun question to ask
Peter what is success well so you know
when I did in the lab I was doing stuff
over sort of distributed databases so
success was like a row that said it
worked there's a zero return coder so
well you have return codes you do like
request response over HTTP right so why
don't we just say if we send the client
a 200 we're good yeah yeah that's an
option we can see HTTP status 200 we can
give that a shot so we go out and we we
run some requests and we look at some
things oh yeah I mean let me tell you
about the dirty secrets of production be
great if we all followed this convention
and it was true all the time but in fact
we have a framework within Netflix that
purposefully always returns a 200 and
wraps an error inside of it so why would
you do a perverse thing like that
it's production man
okay so kind of went back to the drawing
board here and we said okay well what
what can we use to measure success so I
want to draw on an Amazon leadership
principle that I'm fond of start with
the customer and work backwards so when
we're talking about what's des means we
can talk about it in very concrete terms
success means that our customers were
able to do what they were trying to do
that they didn't get an error and that
something didn't prevent them from doing
it in the case of Netflix can they
string so really that's what we want to
measure so nice the nice thing is within
Netflix then we have real user
monitoring built into our client-side
devices that propagate back up
information to the service about
successes and failures and in fact we
were able to tie these in and learn
about whether a specific request had
resulted in a customer facing error
worth considering if you've seen this
screen before I apologize I it might
have been us but I don't think so so we
want we want to catch those errors and
we want to find them and we want to
measure them so that we can have that
feedback so we sort of solved our first
the first problem that presented itself
to us and it's worth popping up a level
and saying like what was it what was the
resounding lesson for free Colton and
myself and for me it really had to do
with when you feel like you're lost
start with something that you know start
with the gold standard and kind of work
backwards from that gold standard start
in the middle of the maze and work your
way out and so for me like in my
academic idealized model of reality it
would never have occurred to me how to
have customers be like part of the model
but thanks to Colton's insight on it
I'll never again look at systems like LD
Fi and fault injection in general
without sort of contextualizing success
in the context of observable results for
end-users and and for me working
backwards from what you know as I read
the LD Fi papers I watched Peter's talk
I had it was an epiphany moment you know
I'm a pessimistic person I like to think
about all the things that could go wrong
sometimes that makes it a little bit
more complicated a little bit more of an
intractable problem but you know once
once Peter convinced me oh let's start
from what went well and look at what it
could have prevented it that narrowed my
my vision in our pro
it allowed us to take a more directed
solution awesome so one down three to go
right so the next thing that we need is
to be able to ask and answer why
questions what I would love is for you
to tell me that netflix has lineage what
have you got so you're gonna be excited
because we do we have tracing its built
in similar to dapper or Zipkin we have
the ability to see where request travels
through our system okay sounds legit
pictures worth a thousand words let's
stare at a picture so Colton I spend a
lot of time in front of the whiteboard
pretty much every solution involved us
staring at a whiteboard for a while and
one of us saying I've got it so we're
looking at the whiteboard okay III buy
it
so it's you know request comes in and
hits API and it calls that fans out to
some services that are required to
satisfy that request and then it fans
back in and in some sense this is an
explanation of how the system provided
its expected outcome and even nicer than
that in addition to being an explanation
it also identifies the inflection points
services at a very coarse grain where
failures could happen so that's great
that's that's exactly what I wanted
except yeah so there's one wrinkle with
this right where's the redundancy
remember that graph I showed you where
like there's replica a and replica B
where's the redundancy yeah there's not
any redundancy in that graph
I mean granted you know it's Netflix we
have redundancy everywhere I believe it
redundancy in regions redundancy and
hardware even redundancy in code well so
that's great but but but uh I need the
redundancy to be in the graph because
because the way that LDF I prunes that
giant space of inputs is is by
recognizing the experiments it doesn't
need to do based on the redundancy so so
you know like we knew this might not
work out like maybe we should just cut
our losses here right game over
ya know it was a good shot yeah but but
it's three o'clock I don't have to go
home for a little bit
it's like stare at this thing for a
little longer what could we do so so
what what could we do well we know
there's redundancy like let's just go
break some stuff and let's see what
happens like we got some time we got
this request we pulled up the graph like
we know it's there let's go break some
stuff and see what occurs that day all
right
okay so we break it we run and it
doesn't result in a user-visible failure
okay what it does result in is another
graph okay so I see what you did there
you didn't need bookmark to satisfy the
user because you have a bookmark
fallback
but I'm still not you know I'm still not
seeing any redundancy well but we know
it's there so what we need is we need to
be able to need to be able to learn
about it over time we know that the
fallback is there we know that give
bookmark works but we need to somehow
put that all in one graph that we can
solve oh of course
so like you know I was coming with this
model where a graph reveals redundancy
and now I'm giving a bunch of graphs
that don't reveal redundancy what
reveals redundancy is the site its
behavior over time reveals its
redundancy what I need to do is think
about these models of the system we're
done of system redundancy as being much
longer lived something that we build
over a longer time scale right so in
fact very trivially we could take both
those graphs and route them under the
same node and now it would be a graph
with ands and ORS now it would
essentially capture the redundancy of
the site and over time we can just
imagine enriching this graph as we learn
about more successful executions despite
faults yeah yeah oh man so I got to pay
homage to Colton on this one I I was so
rigidly attached to my initial model of
what a lineage graph looked like that
even if I had somehow been given VPN
access to Netflix and could actually
look at these traces and look at these
systems it would never have occurred to
me to kind of pop up a level and think
about redundancy is something that's
revealed over time in response to
failures as opposed to an individual
executions so I got to hand it to you
Matt and I like to think that you know
this is the kind of thing that maybe
neither of us would have would have been
able to do on our own this is like an
example of that multiplicative effect
that I was talking about you know we're
kind of starting at these two different
poles and marching towards this middle
point and when we get there we find that
we have more than either of us brought
brought with us and I think really
Peters too kind you know as we're
sitting here as we're white boarding as
we're side-by-side trying to solve
these problems you know we discuss it we
debate it we talk about trade-offs we
find something that'll work and so this
is this is exactly what comes of that so
back and forth some oh this doesn't work
let's try this we get some more
information and together we're able to
find a solution awesome so now we're
really cooking with gas so the next
parts easy right
we just have to take that graph and
extract the subtrees as I showed you
this is kind of my area and I also
wanted to prove to Colton that I could
like write real code
so I said I got this I'll like I'll code
it up I'll code up the solver component
and so you know I gave myself I figured
this would take about two hours I
started going I installed IntelliJ that
actually used up the whole time budget
actually and then I got Gradle working
okay so the next day I mean you know I'm
like okay well how do you convert a
formula to CNF this is undergrad stuff
right
so it's undergrad stuff but it's still a
pain in the ass I had to like write a
lot of notes figuring out how to
distribute the ants over the ORS and so
on and so forth figure out how to do
good commit messages and so suffice it
to say after about you know three three
and a half days of coding I committed
some code to the Netflix production repo
code that I'm proud to say is running in
production at a company right now well
minus the print lines yeah right I'm
sorry about the print lines so okay
that's done and now it's just the
victory lap right because we know how to
inject false we already got the great
infrastructure for that we can just hook
right into the API so it's just a matter
of sort of doing a victory lap around
this thing and replaying the
interactions as many times as we need to
to either find a bug or you know find
more call graphs what are you looking at
me like that for did you say replay now
you know you replay the request I don't
know if that's gonna work why not well
and we're talking about a distributed
system requests that are hitting things
all down the line who knows what's gonna
change underneath this who knows what
kind of state is gonna be mutated or
things are gonna change just replay the
gets oh so idempotence yeah so we can
leverage item phones right hmm that
sounds like a reasonable approach who
thinks that would work
yeah oh so many smart crowd smart crowd
so so oh dude why is reality so ugly
this is so messed up and so and of
course yeah it's not just a matter of
the request might change something it's
like what's the point of replaying
requests because the services are
changing under you all the time you know
like one is one is reminded of another
dusty old philosopher quotes out of
Heraclitus who said you know you can
never step in the same river twice and
that's not just because the river keeps
flowing or because the foot changes the
river but also because the foot changes
right there have never been two requests
in the history of Netflix that are
exactly the same so like we're
right like this is a good time to cut
our losses right we had a lot of fun
over the summer to hit some spots oh no
I think it's all this what we need is we
need to find a way that we can treat
requests like they're replayable without
actually replaying them okay can I
whiteboard for a little bit yeah all
right so what you're saying interpreted
through an academic lens is that we have
a Vince and balls problem right and we
have this like infinite stream of
requests no two of which are alike and
what we want to do though is we want to
group them into some finite and
manageable set of equivalence classes
among the requests in which any two
requests if they map to the same bucket
we can treat the second one as if it was
a replay of the first we could like
simulate replay with some sort of fuzzy
notion of fungibility but that would
require R&amp;amp;R primed it like be the same
according to some definition the
sameness what does it mean to be the
same well let's think about that I mean
we're doing failure testing so what we
really care is do they fail the same and
in some regards you know we want to know
do they behave the same well if we think
about it we have these nice graphs that
we already have that we're using to
answer this question that's brilliant
it's almost like the same solution so
those those call graphs that we used as
as a proxy for lineage we can use a sort
of a proxy label for the classes right
of two requests produce the same call
graph then those two requests are
sensitive by assumption to the same
failures right so so yeah so that's like
a class label but wait a minute hold on
hold on in order to do the injection and
fit
based on what you told me about fit
we're gonna need to sort of annotate the
things right when they come in yeah
which makes any decision as the request
comes in with what data we have
available which means that we need to be
able to predict the call graph that a
request is going to produce when it fans
out through this through the site before
it pans out through the site which means
let's see we need to we need to learn a
function f that that we give it a
request and it predicts a a call graph
all I mean sounds like a supervised
learning problem tomorrow well Netflix
has a lot of smart people that know way
more about machine learning than I do so
let's go chat with them you know and we
go and we sit down and we start talking
to them and it turns out that we can't
just say they solved this problem for me
and get back to me in two weeks we have
to do some work to make it work we have
to do some scrubbing we have to make
sure that the matching works correctly
we have to try different algorithms and
so what we found ourselves with was a
problem a conundrum we could either
solve the machine learning problem which
looks sweet and a lot of fun or we could
solve the failure testing one and since
the failure testing one is the one we
needed we chose that one so what is the
simplest thing that worked so credit to
Peter here because you know I was a
little disillusioned wasn't quite sure
how are you gonna solve this Peter goes
off and dives into the data lake he's
walking through requests he's walking
through all sorts of information and he
finds that for a subset of requests
there's a pretty consistent mapping
between some of the parameters we see in
the request and the services that get
executed so great if we just target
these requests we can find that mapping
and it will work well enough for what
we're trying to accomplish so lesson
three and I'm gonna go quickly because
we're actually running out of time was
was in large part a lesson for me right
I came to the table with these sort of
pristine idealized models of reality and
as much as I didn't want to let go of
the simplicity because simplicity is is
what is what makes a model good it turns
out it's a lot easier to adapt to fit or
retrofit a model to an extremely dynamic
reality than it then it would ever be to
do the reverse especially in a summer so
what I had to kind of give something
right and this is like collaboration
involves compromise not just the kind of
compromise where you just give something
up where you give something up to get
much more back and you know it's a great
opportunity for us as well
we learn about how our models could be
better the data that we should be
capturing additional things that we
could do better that would make it
easier for us to solve these problems
down the road
so you know as Peter talked about a
simple matter of code a little bit of
time goes by two or three months and we
go hack at it and we make it work is the
long and short we go through we fix some
bugs we have some rendezvous problems
with reporting customer data you know
we've got some I'm an engineer I wrote
some bugs
I had to debug them and figure it him
out but and I guess I kind of ruined
this doesn't work I mean obviously we're
here so probably be talking about it if
it doesn't work but it does so let me
tell you a little bit about the request
the the the case study that we studied
so at Netflix there's a request that's
called the app booth that happens at
startup it touches a lot of different
services and it sets up your session it
authenticates it downloads some content
it caches so that it's going to be a
quick and important user experience
it's what Netflix tells a moment of
truth if you go to boot the app and it
falls on its face you're probably not
gonna be happy as a customer and that's
certainly not what we want so in this
case we're talking hundreds of services
that are touched in this request as
Peter mentioned if we were to do a naive
approach we're talking a very large
number of experiments so I'm happy to
say that we were able to explore this
entire search space in 200 experiments
and furthermore we were able to find 11
different bugs some of them single
points of failure
some of them multiple points of failure
but bugs that would have been customer
facing bugs the real bugs probably the
customers were hitting so we both got
what we wanted Colten found bugs before
customers did I got to argue that we
identified bugs that another technique
would never have found because of their
depth and complexity so you know we did
it this is the part where we part ways
and our two column are one column that
we merged into kind of breaks off again
into a duality I'll go quickly through
this but obviously as an academic
there's a bunch of academic questions
that I would like to ask and I have you
know a sort of comfortably
academic timeframe to do them in our sat
formulation of the problem sometimes has
a very large number of solutions and
they're not all equally likely so in an
ideal world what we'd like to do is next
time we use our fault injection
infrastructure we want to test faults
that we think are the most likely ones
to be tripped by customers this is going
to involve turning that decision problem
into an optimization problem that tries
to find the best next solution this hard
work it's going to take a few years and
lots and lots and lots of money
obviously now him I would like I would
like to be able to improve our tracing
infrastructures to collect something
that looks a little bit more like
legitimate lineage and database systems
in particular some of those services and
that graph are stateful and the
explanation about why you got a response
doesn't just have to do with that call
graph but it has to do with call graphs
from the past that changed that state
now Colton is extremely suspicious about
it thinks it's extremely unlikely that
that this is going to work in practice
that the overheads of provenance
collection are going to be too high for
industries to bear but that's what makes
it research um we also kind of looked we
didn't look at time right we said what
set of services fails but in order to
trip certain faults we have to fail
services in a particular order which
needless to say vastly increases the the
complexity of the problem by a factorial
figure thank you oops yeah so on the
other end you know Richard device
metrics we're only capturing them for a
subset of devices we want more inch more
insight into what fails this this
request class creation problem it's not
solved this still needs to be done right
now we can only focus on a subset of the
Netflix API we can't hit all of it and
as Peter mentioned you know we can do a
better job in prioritizing how we search
this space so I have lab now and a team
it's called disorderly labs you should
visit our website I have a bunch of
brilliant students who are gonna be
working on this problem for the next
five years at least and then beyond that
if I'm lucky enough to get tenure so
look me up especially if you want to do
these kinds of collaborations in the
future I have some good ideas still and
so I left Netflix in January to found a
company one of the things that made this
possible at Amazon and Netflix was the
infrastructure that allowed us to cause
these failures in production in a safe
and deterministic way not everybody has
but I think if everyone did that they'd
be able to do this better more
thoughtful fault exploration in
particular gremlin is built to run on
Linux has a command-line interface that
causes bad behavior things like taking
out CPU or RAM making mucking with
network traffic as we talked about an
API I think an API is extremely
important who knows what's going to be
built on on top of it and the ability to
tie into your continuous integration and
continuous deployment is what helps us
prevent the drift into failure lastly I
think that good engineering tools need
to be easy to use you want people to do
the right thing you need to make it easy
for them to do the right thing and so
I'm a big fan of how a simple powerful
web UI that guides users through
understanding how to fail what could go
wrong and helping them to to understand
it things like being able to schedule
attacks being able to configure it to
confine that blast radius and the
failure scope and of course the big red
button this is whole tall because if
things ever go wrong you should just
stop it
so we should probably go fast yeah I
think you know we've talked through
these these lessons you know work
backwards meet-in-the-middle
adapt a theory to the reality and if you
take away one thing from this talk
because you fell asleep and I'm sorry
you know this this was a talk about
putting together two things right and
the metaphor that I like to use is that
you know we start at these two poles
that we've marched to the middle what's
interesting is you know we started with
all this knowledge so we're like
carrying all this stuff in our arms we
had to make all these compromises along
the way so as we're marching we're like
dropping things and yet somehow
miraculously when we get to the middle
we end up with more than we began with
it's like some loaves &amp;amp; Fishes kind of
miracle it's amazing for me so academic
academia plus industry can be academia
times industry right if you're willing
to make compromises embrace risk and
march towards that middle point right we
can provide we can build better
solutions which is what I think we
succeeded in doing so thank you very
much thank you
nice to the press we have time for
questions
obviously the invitations open to come
and find me and Peter online on Twitter
Twitter Pinterest Peters big on Twitter
or anywhere else and come chat with us
go ahead gremlins in closed beta we
launched closed beta on Monday there's a
handful of companies that were we're
working with we're planning to do open
beta come January 1st if you'd like to
hit our website I threw some stickers
around people like stickers you can hit
our website and sign up so how are you
guys simulating things breaking in prod
without actually breaking things in prod
so that's that's where the the fit the
failure injection infrastructure allowed
us do we kind of abstracted over it here
I actually gave a talk in queue con New
York last year about that if you want to
go watch it but the long and short of it
is we're simulating failure at key
injection points along this back so RPC
boundaries hystrix if you're familiar
with persistence and caching tiers are
great places to either inject a delay or
return an exception or throw an error
and we had the people that own those
libraries at Netflix write that failure
code so they did their best to emulate
real production failures as close as
possible
I think this is maybe more on the other
question but like to this logistical II
how do you you know how do you simulate
do you actually have a test
infrastructure that has every service
running or do you just find individually
the points and just test those in
isolation Oh Netflix are Cowboys they do
everything in production and the reason
that they can do it safely is called
vindicated is that they can in simulate
failure at the granularity of an
individual request without affecting the
other requests that may be multi-tenant
Adhan that on that portion of the of the
architecture so we're testing in
production and in the strict sense but
controlling as colton said the blast
radius of what what you could affect
I think you guys are meaning able to
grab those production issue information
like the pelo in order to do that an
analysis right and how does the Netflix
grab those pelo production pelo without
affecting the performance yeah so two
points fit is fit is meant to be you
know short-circuited where it's not
gonna do any work unless it needs to if
we've chosen to decorate a request with
failure we're okay with a small amount
of performance degradation it's one that
we've measured and that was negligible
it didn't really show up in custom I
think it's question was about tracing
though right I think tracing was an
element of it so I think like you know
in general Netflix is using a system
called self which is an inheritor of
Zipkin based on Google staffer so it's
doing this at sample rates that are
completely controllable to trade off
between observability and an overhead
and things like Zipkin open tracing are
becoming an emerging standard that is
you know you're beginning to be able to
expect that large scale micro service
architectures are collecting traces of
this kind obviously at a fairly low
sample rate do you have a sense of the
financial value of the investment in
quality call on me I think that I love
to ask what's the cost of being wrong
what's the cost of downtime for your
company if you work at Amazon and you're
down on Black Friday its measured in
hundreds of thousands of dollars a
minute so I think that that that's
really what you have to that's really
what you have to do and if you want to
go convince your management that failure
testing is an important investment not
just for the training or the operational
burden reduction you make it about the
money what does that nine cost you and
that doesn't even count the cost to your
brand the cost of customer trust
give some examples of a bug that you
found with the system that you could not
have found with the engineer guided
because you obviously doing a lot of
feel like testing already but engineer
guided so like like you have a specific
example of like why you found it with
this that you couldn't have found it
with engineers so we did a lot of our
proactive failure testing at the failing
us failing one thing at a time there's
so much low-hanging fruit that just
failing a single component tended to
keep us busy some of the bugs that we
found in Abu involved three and four
services failing together yeah yeah so I
mean what I would add on to that is that
you know like a lot of the sort of
engineer guided stuff is focused on
whether this sort of solipsistic view of
a service and its immediate dependencies
and as a service robust to failures of
its immediate dependencies or something
like that you can go a very long way
with that but it's also true that
correctness is really an end-to-end
property of a whole users interaction
across many microservices and it's by no
means trivial to sort of stitch together
and compose the expertise of the
individual service owners to reason
about what could go wrong sort of
transitively and so and that's why it
was so important for me to find not just
dot just to count the bugs that we found
but to illustrate that some of them
involve these non-trivial compositions
of multiple types of faults that would
involve the expertise of many service
owners who don't really know the
totality of that graph and I'm sure as
you know being from uber like at scale
you see not just single failures but
combinations of failures and some of the
worst outages are two or three things
that you don't expect to go wrong at the
same time yes thank you all right thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>