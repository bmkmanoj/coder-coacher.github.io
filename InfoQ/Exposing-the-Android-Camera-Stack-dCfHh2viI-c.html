<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Exposing the Android Camera Stack | Coder Coacher - Coaching Coders</title><meta content="Exposing the Android Camera Stack - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Exposing the Android Camera Stack</b></h2><h5 class="post__date">2012-09-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dCfHh2viI-c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm balvinder core and he's my
coworker Joe Rixon both of us were ketab
Tina imaging so we've we've divided our
talk today we will cover right from the
camera api's to the android framework
and the camera service going down to the
camera hal device drivers and even
hardware and the top players i will be
covering and midway through we will
switch all these wires and Joe will take
over to talk about the camera he'll
device drivers and the hardware so
basically there are six classes and
eight callback interfaces the class is
the main class of course is camera and
then the next most interesting and most
complicated is camera parameters we have
camera info camera sighs camera face and
camera area info is really very simple
it just tells you whether your camera is
a front facing or rear facing camera it
also tells you the orientation of the
image that you're going to get back from
that camera sighs like the name says is
quite simple it's just height and width
of the image coming back area is a
little complex more because of the way
it gets used rather than the API itself
it basically defines a rectangular area
and it has a weight and it's you have to
be more of a professional camera person
to understand the metering regions and
auto white balance and things like that
camera duck face is very interesting
personally to me because it is the class
in ice cream sandwich android introduced
face detection api's and this is the
class that helps with giving back face
data and so most of the camera api's are
you take a picture and take a good
picture and how do you do so basically
it's a camera application is one of the
use cases but the more interesting ones
yeah it's behaving differently here the
camera face is it's what other things
you can do and with that we'll get into
more of the computer vision kind of
applications face detection what and
things like that personally that I find
that very interesting and I think
there's going to be a huge slew of
applications around that then we have
callback interfaces face detection error
call back we'll talk about the picture
preview ensure the callbacks when we go
over the API for how to take a picture
the one that is in orange on the screen
is the one new API that was introduced
with jelly bean it's the autofocus move
called egg and I'll talk about that in a
little bit camera class so basically
think of it as all the actions you would
actually do with the physical camera you
have to open the camera you have to shut
it down at some point you should be able
to access all the controls if you what
is the scene mode all those different
controls and I'll talk about that in the
camera your parameters class so what is
one of the first things one does where
the camera is preview basically you see
in your little viewfinder what is the
scene that you're trying to take a
picture of and preview is pretty
complicated so I really never worked
with cameras and all this before I
joined up t9 to meet used to be a tiny
little thing at the back of a cell phone
but after a year working there I am just
amazed at how much goes into that tiny
little thing so preview it's such a big
deal once they bring new hardware up
it's like if you get preview working
it's a major achievement so now I
started treating preview with a lot of
respect more than I did a year back how
do you get live previews in the callback
and then there are other things that I
find quite interesting is how can you
not you suppose you don't want to
display anything let's say you're
writing an app where you're using the
camera for gesture recognition all you
want to see is the gesture you don't
really want to show that on your preview
so can you actually grab the preview
frame but
not displayed so we'll talk about that a
little bit then of course it's capture
at the end of the day for most
applications you do want to take that
picture of your pet your dog your wife
whoever want to unlock and lock the
camera that's when we hand over access
to the camera hardware to another
process and that is basically when you
go from still mode to video mode and
then the media recorder comes into play
I'll show a little bit of the code for
that as well there are three basic
actions that the camera class provides
start auto focus so before you actually
take a picture you want to kick up the
autofocus to make sure that your object
the picture of what you're taking is
focused smooth zoom is used where you
can smoothly zoom in and out of the
picture and the one that was introduced
with ice cream sandwich is start face
detection okay camera parameter so if
you open up the developer page and the
javadoc for this class it runs into
miles but if you look at it there are
three patterns and that's what I've
broken up here of the way the methods
are they there are some that are
mandatory feature sets that any Android
phone or device i would say certified
right i mean you could really do nothing
if you want to but is expected to
provide those start will be get
supported and then there will be
something behind it it could be preview
sizes picture format picture sizes all
kinds of things so you can get a list of
what scenes modes auto white balance you
can get a list of what is it that this
particular device supports and then you
can do your gets and sets on it the
second is a list of optional feature set
because i even Google recognizes that
the hardware the capabilities of the
hardware is going to differ
from device to device and so there's the
optional feature sets of some things
like is video stabilization supported
right now that's a feature of the
hardware and all platforms don't have to
support it so for them the pattern of
the method is is video stabilization
supported is video snapshot supported
and then it has its corresponding get
and set the third is custom feature set
so we have certain features like
obtained as a hardware manufacturer so
we have certain features which are not
part of the regular ones and for that we
use we call the custom feature sets or
extensions and we use the dump pipe that
the architecture provides which it's
just name value pairs with strings and
you can use the gets and sets for that
and then at the bottom I have a whole
list of what are the different things a
list of methods that and features that a
camera can provide I briefly covered
most of this the camera face is
coordinates for the left I right I'm out
outer bounds of the face is what is
covered by camera or face right now
however you will find that a lot of the
newer hardware can support a much bigger
set of coordinates related to face data
okay so what's new in jelly bean the
when Ice Cream Sandwich came out it came
out with a huge set of api's for camera
but jelly bean there was one camera API
which is the autofocus move call back
basically there are different kinds of
focus modes that you can put your camera
into if it is a continuous focus mode
either for still or video you really the
application at times needs to know
whether the focus has started or it has
stopped and so this is what that
callback provides the other thing they
provided was in the media package and
it's a set of sounds that if your
application wants
to use to mimic the shutter or the take
picture callback that the system
provides you would use this it's just
convenience functions to use in your
application the most interesting part
for jellybean was that there was there
is a new system camera application so
now if you look at the jelly bean source
code under packages apps it has the
legacy camera and it has the new camera
app and the major difference is the
support for gestures which even the iOS
5 are talking about is how you can once
you take a picture normally you have to
click on the thumbnail and bring up the
gallery application to see what picture
you take but they've now provided
support for gestures so you can just
flick right and see what the latest
picture you took our others and then the
other gesture they have support for is
the flick up for to delete and undelete
but I think the bigger thing is I
haven't had a chance to dig very deep
into what the differences are but the
other thing is it's more this whole
jelly bean buttery feel so it's more
about providing a smoother interface for
the camera application so one of the
biggest things when developing a camera
application is the time it takes to
start up the camera the time it takes to
display the preview in and it could
become a very jerky app so a lot of
optimizations are around how to create a
smooth user interface because what we're
moving back and forth is a lot of data
and a lot of time consuming tasks this
is just a table that I had put together
when we evaluated the ice cream sandwich
features and at that time a lot of
people got very confused even internally
as to what was an API and what is part
of the system app and people were
searching about why can't I do face
recognition so we put a little table
together to see what what is part of the
API is what's just in the system app and
what is actually closed source and
proprietary
solution so this table just reflects to
that okay so some of the prominent use
cases when dealing with API with the
camera API is of course is the first to
get your preview right second is to take
a picture third is to save your picture
and then the fourth is too smoothly move
from a distill mode of a camera to a
video mode some of the secondary use
cases is to create a is to configure
your camera the way you wanted you want
to certain auto white balance a scene
mode things like that so this is more
for what I would call a power user of
the camera rather than a general user
just click and shoot you would use that
then we talked about it is how do you
process the metadata the face data it
could be frame data histogram all the
metadata around an image we have we do
snapshot which was also introduced in
Ice Cream Sandwich as an API but the
it's the hardware that actually needs to
be able to support it in that case what
happens is you need to you have the
camera in video recording mode you
should also be able to just maybe touch
the screen or click a button and be able
to get a full high resolution jpeg image
so one of the things is as an
application developer if all you really
want to do is you just want to take a
picture or take a recording you can use
system in tents for that you don't
really need the camera AP is for that
the camera api's or if you really want
to develop your own custom experience
with getting a picture or taking a
picture and things like that so i would
highly recommend to that the on the
android page there is there's the link
on the slide which is a very good
tutorial on how to do different things
that you would want to do with the
camera so do check it out and a lot of
your
beginner level questions will definitely
get answered from there okay at this
point I'm going to bring up eclipse so
it's it's a simple activity and I'll
just described the layout to you or
maybe we can take a look at this
so it's just in landscape mode there's a
shutter button on top there's a little
button where we switch between the two
modes and this will display the
thumbnail once we have something to show
so very simple basic eye appt camera app
okay so this is just some of our field
and we can go straight to the oncreate
I'm sure you're pretty familiar with the
inflating items and getting handles to
them so the first one is the shutter
button where once it gets clicked on the
first thing you do is call the autofocus
the next one is just a share button so
that which right now is not is invisible
so you couldn't see it but that all this
does is that it gives you the capability
to share with any of the apps that can
filter the send intent switch button
this one will toggle between the two
modes and I'll go over the code for that
in a minute
so so let's go and see how the preview
works so we need the custom surface
callback which in this case this
implements a surface holder callback
which has three methods to implement the
surface destroyed created and changed
and once we have a surface is when we
will actually open the camera and set
the preview display to the holder so
there are two methods for preview one is
set preview display and the other is set
preview texture and I will talk about
that a little later on this one helps
the live preview stream is not returned
back to the application the preview
frames go straight to the display now
there are three there are two other
methods with preview if you want to copy
off the frame back in the application
yes please do you have a question
they're back there okay one is the one
shot preview callback and another is a
callback where every single frame that
gets sent to the display subsystem a
copy of that is made from the native
memory into the memory space of the
application and is returned back to the
app in the one shot preview call back
only one time whenever the call is set
the next preview next preview frame a
copy of that is sent to the application
so under normal circumstances if you
don't want to do anything with the
preview frame you don't need to set the
preview call back it's only if you're
trying to do something extra and
wilshire I'll talk about an open source
project which is a barcode reader where
they use the one short preview call back
to get a frame a copy of the preview
frame and then the processor to
recognize what kind of bar code it is
so this one goes straight to the display
the set preview texture on the other
hand is able to send the previews it
bypasses the display all together and
you can send it either to the GPU or to
the CPU and we'll talk about that in a
later slide so once you have set the the
surface here you can go ahead and start
preview once your improve you this one
just is just a boolean to say that okay
we're now in preview mode now the other
interesting thing the next use case is
the ability to take a picture so if we
go back to our shutter here we started
an autofocus with a autofocus call back
once the call back returns meaning that
the the image is now focused it's when
we will fire off the take picture call
right so we have take picture there are
two take picture calls that are a little
bit of a label in the API is one of them
takes three parameters the other one
takes for the first one in both cases
the shutter call back so as soon as the
shutter is clicked you can you'll get a
call back so if you want to do anything
with it I don't know maybe display stars
or a sound normally people just do a
sound so if you want to do a custom
sound or something you would put that in
the shutter call back the next one is
the raw call back so if you are looking
forward to get back raw data and now but
the hardware has to support the
capability to provide unencoded data
back and if you want to know what work
is raw you'll have to ask that question
to Joe these are expert on different
formats the third one in this case is
the jpg call back so far I've never
really seen the Rock Hall back being
used anywhere or hardware that supports
it but it could be
limited knowledge jpg call back of
course returns the JPEG image so the
picture which was taken so so the
preview cook is typically at a lower
resolution it could be a Ouija a or
something and then the JPEG you'll get a
full resolution image so if you have a 3
mech camera will be probably three mag
58 mag whatever your camera supports you
can of course use the camera parameters
to explicitly set the sizes of what kind
of a size you want it but by default
it's typically the highest resolution
that the hardware supports so once you
do take picture at this point you get a
callback which has the data and unpick
chur taken and so you have to restart
preview so that yes please ok so the
post view is some of the cameras a
hardware have a post view mode and I
wouldn't know the industry a definition
of post view but I can tell you what
post-war the obtained no sensors do
propose to you it's a high power mode
and there instead of the preview being
at a lower resolution we have things
that go at a full resolution is
constantly streamed at full resolution
so at eight megapixel sensor is
constantly capturing in a circular
buffer it will capture images at eight
meg and then we have a smaller
resolution that gets piped as the
preview and when we come to the hardware
section Joe will go over the circular
buffers and show you how it works yes it
depends it depends on the hardware what
they return in the post view call back
so so this is one of the things I have
as missing things in the Android API is
what exactly does the post view mode
mean and we typically find that the
supported sizes for preview and picture
taken in post view don't always match
the ones in regular right so we don't
have that so that API is also missing
right in the way we worked around it
internally is by using the camera
parameters and having specific mode so
we work around that we actually
explicitly set it in to post view and
then we have strings that return all the
supporters sighs so that's how we worked
around it but I think there is the need
for this camera API which talks about
the post view mode and the functionality
there I mean what are the capabilities
in post view mode no it doesn't yes it's
not defined at all and i believe it is
it's not standardized it's not defined
its hardware dependent so it's subject
to interpretation I think it's probably
mostly used by OEMs because they know
what their system capabilities is versus
a third-party developer being able
actually being able to use the post
recall vectra lively so that's that's my
take on it okay so same picture is just
an icing task and we'll just save it to
the file system and that's pretty much
it so with this I'll just talk about how
do we go from still mode to the video
mode switch video on right there
so one of the first things you have to
do is you have to unlock the camera so
that right now the application has a
handle to the camera but when we need to
go when we get into video recording we
need the media recorder process to also
be able to access the camera and so the
first thing you need to do is unlock
create a new media recorder these are
just callbacks for errors and in phils
you set the camera you have to tell the
camera ok this is the camera instance
that I was using you hand it over to the
media recorder you set your profile set
your video source and set output file
what's the filename duration file max
file size and then you also have to
again provide the set preview display
holder for that the one that you had
already created and you start no I think
you put prepare and then start and the
media recorder state diagram for those
of you are familiar with the API is
pretty well documented and if you miss
even one of the call somewhere in there
it will fail and you will go into one of
those catches their face detection so
face detection you basically you have to
the way to detect there's no is face
detection method to see whether face
detection is supported or not instead
this one is yet max number detect a
sitter than zero I'm not sure why he is
faced detected supported call is not
there but that's how it is and if it is
then you can start face detection so the
way the FBI works is in the way some of
the cameras tax work is that then you
will get a call but you'll have a
listener for the face detection listener
and then you have to do a lot of
complicated math to figure out what the
coordinates are and accordingly put
overlays on your preview screen some of
the newer ISPs will actually just put it
on for you but so we typically
II I really haven't worked that much
with the on face detection call back
because we just get ours overlaid on top
okay other applications so this was
where it's an on camera application so
one of the things is i already talked
about is is that it's an open source
library for 1d 2d pictures and they use
the one short preview callback which i
mentioned briefly and i'll show you that
application also app demo at the end of
the talk live preview without a display
and so this only started as recently as
honeycomb and basically went mainstream
with ice cream sandwich so there is now
support for GPU processing and cpu
processing so you can end the way that
works is which i mentioned earlier too
is the set preview texture so for for
you to work with GPU processing you do
need to know opengl and need provided
context and then there the call is
surface texture that update text image
whenever you make that call the texture
will actually get updated with the
latest preview frame that is available
if you want to do if you don't want to
work with opengl code you can still have
a surface texture and and but use just a
dummy ID instead in this case it gets
piped to the cpu not the GPU and it's
not as efficient of course as sending
things to the GPU but then you don't
need to know opengl and what I saw was
that the opencv sample codes for those
of you who may be new to this opencv is
the open source computer vision library
they in all of there's a sample code for
android they use this option number two
and i have a few demos that i can show
at the end apps just there just sample
apps straight from opencv and that show
but the real interesting ones use cases
for this is the google i/o 2011 talk and
there's a link here i'm sure that slides
will be available right there must be a
way to make them available ok so that is
really interesting and they have a lot
of use cases of how actually
you would use the preview and do a lot
of fun things so I would recommend that
you take a look at that afterwards
Android media framework okay so most of
you have seen this for colored Google
colored diagram and that is the camera
hell the piece that we work on mostly so
it's basically if you're familiar with
the architecture there's the hardware
independent piece and the hardware
dependent piece the hardware independent
piece is the application application
framework which mostly written in Java
and then we have the camera service
which is native code and everything
below that yellow line is all hardware
dependent and the hardware the OEMs or
the hardware manufacturers typically are
the ones who implement those pieces so
how does it work we have an application
and so the camera service lives within
the media server process and once an
application makes a request for the the
M camera open call it will make a call
to camera service over the binder
interface and a camera object is
requested from the hardware it also
creates a connection to the surface
flinger so that the preview can preview
frames can be displayed if there's
another application and it makes a
request to the front facing camera that
one will also get serviced but if there
is already an application has a handle
open to an ex to one of the cameras then
the other applications get refused so as
a good android citizen if you're an app
developer in your on resume in on pause
you need to release and connect your
camera otherwise other people will not
other applications will not be able to
use it so inside the camera app
that's what it looks like there's a jni
layer the camera service which is
hardware independent resides in the lib
camera service esso shared library so
what is the what is the code in the jni
layer the Android hardware camera do it
basically creates a persistent context
for all the callbacks that come from the
native layer it holds references to the
java camera object face object and area
object so one thing to remember is that
a copy of the jpeg or the preview frame
is made here in this layer so it is
expensive if you're thinking of doing
the preview callbacks just remember that
you're actually copies of frames are
happening so it is expensive it's
basically a resource manager for the
camera hardware it this is the one that
checks for permission so you do have to
declare your hardware permissions in
your application for you be able to use
this one it keeps information about how
many cameras are available their
orientation and things like this so this
is the open source folder structure and
it's probably too small to see but we're
different pieces of the code live and
interestingly this got changed in jelly
bean and things moved from frameworks
base two frameworks AV but when I looked
at do it did a diff of all the folders I
didn't find too much of a difference in
the code is more like bug fixes or
support for the new API call the
autofocus move call back so there was
there's I didn't see any major
difference in the code base they just
moved it around and with that it's going
to be all about Joe so hi my name is Joe
Rixon and I'm a software engineer for at
Tina similar to balwinder i work in in
the driver group we do device drivers
and also low level software and so the
next section we're going to talk about
is the hardware dependent
we'll start out with a camera how and
i'll be talking about the three areas
here at camera how followed by a colonel
device driver and then a little bit on
sensor hardware so the camera hardware
or I'm sorry the camera Hal is library
that's specific to the hardware and this
is usually written by the hardware
vendors TI Qualcomm or others camera Hal
interfaces with the camera services
stack that's right above it and it does
so through for froyo used a camera
hardware interface and for ice cream
sandwich it uses camera H and camera hal
interfaces of the lower level driver in
this case I'm talking about video for
Linux but there is also other interfaces
that it could talk to it could be open
max or even others and when it's talking
with video for Linux it uses file i/o
operations this gives a block diagram of
a camera Hal and this is based on tis
OMAP for open source code that that we
use and so in this block diagram camera
how block is sort of the quarterback for
all the other blocks memory manager
display surface manager event
notification and the lowest level one is
a camera manager each of the blocks are
specific to the hardware that's running
and so depending on the hardware
platform Qualcomm might have a little
bit different than ti some of the
functions of the camera how block our
initialization so this will initialize
everything from the camera Hal on down
it'll initialize a device driver and
i'll also initialize a hardware and get
it ready for previewing or whatever
camera services interface everything
that's sent from the top layers is
handled by this block also dispatching
it either internally or to the drivers
and then the camera state machine has
also maintained here too and this is the
camera as a state machine it moves from
standby to preview from preview to
capture
preview video it doesn't a sort of jump
around in two different things you don't
move from standby in to capture for
example and so a state machine has to be
maintained by the camera how memory
manager cameras as you probably are
aware are very memory intensive so so
you want to have a good memory manager
to allocate buffers properly and free
them when when they're not in use any
longer the display surface manager
controls preview activity between camera
Hal and the display device and so it
coordinates back and forth of the
display display process it sends frames
to the display when they're ready for
preview and then also signals to the
lower level driver when buffers can be
cuter on queued event notification there
are several events that get called back
up into application space and I'm sure
you're aware of those are broken down
into three broad events at this level
there's notify which you get called on
when there's an error or a zoom event or
a shutter event or things of that sort
there's a timestamp which is used for
video frame stamping and then data
called back one preview if there's a
preview call back compressed images or
post for your images and the lowest
level is the one called camera manager
or camera adapter and this talks
directly to the underlying device driver
or rendering software for a use case for
this what i did is i chose preview so
we'll take a look at what camera hal
does for preview then what the driver
does for preview and then underneath it
what happens in Hardware one preview
preview is executed preview is
displaying the camera Avenged in
real-time as I'm sure you're aware and
so the camera passes that image straight
over to the display device from the
application view that's a single call
but from everything down below that
that's many calls and many different
things
this sequence diagram here gives an
overview of dull the different calls
that sort of go on so single call from
the application down to camera server
then from camera hal down there was a
number of different calls very what we
call I octal Xin between the camera hal
and device driver so the first one be to
set the format of the camera image it
would be set the resolution which is a
size and the image format maybe y UV or
RGB or whatever the next one is
requesting buffers the video for linux
organizes camera screen shots in buffers
it will take a whole slew of buffers and
do that in a circular list and this is
important for keeping the camera moving
and not dropping frames and things like
that so requesting the buffer is an
important part if buffers are allocated
in kernel space versus user space we
want to query the buffers find out
things about them and then map them into
the memory space for userspace last two
things we queue up the buffers into our
internal queue and then stream on turns
on preview and that starts the camera
pipeline flowing the next slide here
shows what happens during previews at
signals to the driver that a preview is
ready the driver and turn signals to
camera how that a preview image is ready
camera how we'll take that image or an
appointed to that image DQ it out of the
queue of buffers and then pass that off
to the display device which will either
copy it into the display memory or pass
it off as a memory mapped object into
display and if a call preview callback
is enabled then that will be copied for
userspace next section we're going to go
down one more level and we'll talk about
the camera driver at kernel level the
from the previous previously for a
camera how everything above that is in
terms of streams and parameters and
things of this nature things you're
familiar with from the application level
at this level down below everything is
in terms of register writes data being
transferred over parallel and rippy
lines things of that sort so the kernel
driver represents a standard interface
for different types of camera hardware
so what this means is that you may have
various different types of camera
hardware below this but at that level of
the video for linux or openmax or
whatever it's a standardized level and
that's what camera how will connect into
it can support different types of
hardware topologies such as a image
sensor processor which I'll talk about
in a little bit versus a smart sensor
are handled at this level and video for
Linux does a pretty good job of handling
these things this block diagram just
gives an overview of a typical video for
Linux driver the top layer contains all
the file i/o calls that are used between
camera Hal and in the driver also the
driver infrastructure is routed here the
controlling interface controls data flow
and control flow on the lower right-hand
side each camera will have its own file
that will have all its hardware
dependent parts in it how to detect the
camera how to initialize it power
management things of that sort and of
course the driver has buffering memory
management things inside of it the if
you take a look at video for Linux
driver it's just a huge directory is
like you know a hundred different
cameras supported and what happens is is
that those top layers on our previous
diagram there are all the same but
underneath each camera can have its own
file and you can use that with the top
the top layer so you can support
multiple or many different types of
cameras or different types of topologies
usually these are all done at compile
time so you pick out your cameras if
you're going to use one or two front
facing back facing compile them
and you want to sort of keep these to a
minimum you'd want to compile them all
in because it takes a long time to
initialize initialize cameras and video
for linux exposes these cameras has
devices in the dev directory video 0
video 1 etc some of the colonel
resources memory is very important as I
said before he can either allocate
buffers in kernel space or you can do it
user space and have those pass down when
you talk about a camera euro is thinking
that the hardware transfers the image
out of the camera into memory you never
hear of anybody pulling the camera for
data that's very old and also most
modern ships have mmu associated with
the camera hardware what this means is a
rather than having one contiguous block
of memory for each image which is very
difficult to allocate in a real-time
operating system ursday operating system
you can have a scatter gather memory can
be used so blocks can be placed anywhere
in memory and it can DM 8 any of those
interrupts of course are important for
the camera you don't want to pull it
interrupts they have different
capabilities for interrupting when the
frames finished or starts things like
that there's the control and the data
path of a camera control refers to how
you're controlling the camera how those
parameters you send down get set data is
how that image it comes out of the
camera I squared C has been used for
controlling camera register reads and
writes for years I squared C is a little
bit of a slower interface 100 kilohertz
or 400 kilohertz or the standards or
even mega mega bit right now alternative
to that is a spy which is in use by some
camera camera vendors control signals
there's things like reset and stand by
you guys are all software people you
don't care about that stuff and then
power power things power management for
camera is very important you don't want
to number one have a camera on all the
times it tends to use a lot of power
number two you want to turn it off but
you don't want to lose all your data you
want to be able to come up and preview
quickly
if that comes up so you want to be able
to suspend the camera and put it in a
standby state we talked about the
buffering system used for a video for
Linux this is just a drawing of how it
works depending on the size of the
buffers the number of buffers will be
implemented the camera Hal and the video
for Linux have us handshake between them
camera the colonel fills up the buffers
the Hal empties the buffers or choose to
the buffer so it's sort of a back and
forth relationship but for filling
starts one video for Linux stream on
command is executed and then stops one
stream life is executed and it doesn't
matter what size of image you're
capturing it uses the same format the
whole time what this very busy table
does is sort of outline what happens
during preview for the camera so camera
Hal will send down all of the calls in
the on the left column there and these
will be converted either into hardware
calls are into driver calls
and the last entry on the first table
their imaging a starting by the image
enable or preview start my last slide on
the driver video for Linux driver has
been progressing to the point where
there's something called the media
controller architecture and this is
fairly new over the last year or two and
this is allows the application
programmer to control the pipeline the
actual pipelines so for certain types of
camera architecture you can go in and
tell how you want the data flowing
through it through a resizer do you want
it to have y UV conversion things like
this this works really well for an open
source architecture many isps and
cameras are closed architecture it's
actually a very nice where it's used one
example of that as a tis OMAP 3
processor that's an open architecture
you can download documents on the web
and it's and use this our driver
controller for it the other end of that
is proprietary ISPs move their software
out of the driver and this is to avoid
open source issues what they'll do is
move the code up into kernel space
camera hardware overview this is the
lowest level of the stack that will be
presenting basically from App tina's
point of view there are two types of
cameras one is a Justin sensor what we
call a bear or raw sensor it out put
something called a bear or a buyer image
it has no or very limited image
processing and a requires image sensor
process or something to do the image
processing in Hardware either on the
host or on another chip and it has very
simple controls the other type of sensor
is what we call a smart sensor a
system-on-a-chip sensor and this outputs
a processed image so this is sort of an
all-in-one package
no host is B is required and it supports
complex more complicated controls so a
bear sensor discussion there was a bear
pattern on the upper left there this is
repeated throughout the array used with
an internal isp so most of the cell
phones nowadays from qualcomm RTI those
chips have internal ISPs so you'll just
attach the sensor to it for chips that
don't have a host ISP they can use
something called in a companionship
which is just a processing image
processing chip the sensor controls
exposure exposure and gains are the two
types of controls for a raw sensor is P
has much higher level controls such as
auto white balance or exposure things of
this sort and here's a block diagram of
a simple a bear sensor down in the lower
left-hand corner as the pixel array it's
not drawn to scale it takes up the whole
picture usually that is read out one
line at a time that goes through whether
we call an A to D converter converts it
to digital and then the pixels are read
out either a bit 10-bit 12-bit or 14-bit
most pipelines right now support 10-bit
accuracy they're going to 12 bit in
future processes ISPs second type of
sensor is called an SOC type sensor this
one will output 0 y UV RGB or even jpg
and its hooks up much simpler you just
hook it up the host processor with or
without a nice p built in these types of
chips were real commented up in previous
years so earlier days four or five years
ago we would sell quite a few of these
types of chips now that TI Qualcomm
everyone has a built-in ISP we don't
sell very many of these lower profit
margin for us I guess but
here's a block diagram of soc type
sensor at the upper right hand corner
left hand corner as everything that was
in the previous sensor is located in
that block there the in the lower
right-hand corner is where most of the
image processing takes place things like
lens shading a direction changing the
color format things of that sort and the
right hand control is where the auto
functions our measurement engines and
changes these sorts of things also uses
electronic rolling shutter for readout
when you're taking a picture and it's
moving very fast and you see sort of a
tearing that's a function of the
electronic rolling shutter because it's
reading one line out at a time and
you're moving faster than that's able to
do so so you get the sort of tearing
affect global shutter it takes it all at
once we have sensor to do that that are
more common for things like automotive
applications to talk about preview at
the camera level this is my last slide
really a preview is just a outputting
amount of image it's just but there's
several things about preview want to
keep the power consumption low if
possible you want to keep it the
bandwidth low if possible so three maze
three ways of doing this in previous
years I use something called skipping
which is just skipping every other pixel
so you'll take a riad row and odd column
or skip three or four problem with this
is that well the good thing is saves
power it's very good that the problem
with it is you tend to miss things so if
you have fine granularity you might miss
items in your preview a better way of
doing is something called binning that's
where you combine pixels so rather than
skipping those two or three or every
other one you combine them into one
pixel and that's good it's a nice
averaging method it gives you a much
brighter pixel at averages all three or
actually it combines that does an
average so then but then you need to
process that down to its image you also
what newer sensors are doing is they
have actual built in hardware scalar so
they actually have
hardware algorithm that was a shrink
down the picture thank you guys
appreciate
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>