<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Petabytes Scale Analytics Infrastructure @Netflix | Coder Coacher - Coaching Coders</title><meta content="Petabytes Scale Analytics Infrastructure @Netflix - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Petabytes Scale Analytics Infrastructure @Netflix</b></h2><h5 class="post__date">2017-11-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/45Wat2mB9Vk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is dan and I work on the Big
Data Platform and today we're gonna be
talking a little bit about our
cloud-based analytics infrastructure
just to give you a quick overview what
we're gonna talk about first is just
what kind of data we're talking about
then we'll go over some of the metrics
around our scale walk through the
architecture of our platform in the
cloud and then the two main topics we'll
be covering is data warehousing and
orchestration which is Jeannie and
finally we'll have plenty of time
hopefully for Q&amp;amp;A so the first thing I
like to say when I talk about data at
Netflix when we talk about analytics
it's that actually not the data that
most people think of when they think
about Netflix so you think of the
streaming data the actual video or audio
content and that's not what we're
talking about here but we're actually
talking about is all of the micro
services that we have out in the cloud
the client devices marketing campaigns
all sorts of different resources that
are funneling data back through our data
pipeline to our back-end services so
it's not the video streaming data but
it's a lot of other kinds of data and
why do we do this well Netflix is
actually a very data-driven company we
like to make decisions based on evidence
so we don't want to make changes to the
platform that we can't substantiate will
actually improve the experience for
users and this can be somewhat
counterintuitive because a lot of times
you have a feature you think oh
everybody's gonna love this but it can
actually make it more confusing it can
get in the way of people finding content
that they actually want to watch and
degrade the experience so we want to
make sure that we we actually do things
to improve the platform so one of the
things that we do with this data is a
lot of a be testing so we test lots of
new features ways of you know navigating
the UI what we show users different
algorithms and all of this is done with
different cells just like many companies
do right now but the idea is that we can
collect all of this data through our
pipeline through many different tests
and then we have systems like this we
call it our ignite platform but it
services this data from our data
warehouse in ways that data scientists
can actually evaluate the data and
choose the right kind of pushes to make
to our platform another thing we do is
we collect information about how well
the video is actually streaming so we
publish the ISP speed index this gives
you know people around the world a good
understanding of how well their ISPs are
performing and what service you should
be going to in order to get the best
Netflix content another thing that we do
is recommendations so obviously we want
to make sure people can find the content
that they're going to be interested in
and there's a lot of the work that goes
into this and a lot of data that you
know is used across different cultures
different countries at this point to
come up with the best recommendations
for any individual
user so it's not surprising that we
frequently say oh that our biggest
challenge is scale the amount of data
that we collect grow super linearly with
the number of users of our platform so
just because we add some number of
millions of users it doesn't mean that
we scale linearly we actually collect a
lot more data we're doing a lot more a
be testing so we have to be able to
scale beyond just what our user user
base is scaling to so here are a couple
metrics of Netflix right now so we have
more than 86 million users of the
platform we're effectively global we're
in almost every country in the world so
there's a lot of different cultures
there's a lot of different types of
connectivity devices other things that
we have to account for and then we have
over a thousand devices that are
supported just about everybody in this
room probably has a device that they
could pull out and watch netflix on
right now so that's a lot of information
that we collect along with the millions
of hours that we stream a video content
that generates a lot of data that we use
for that we collect in our analytics
platform so what is our platform
actually look like from a metrics
perspective we have more than 500
billion events coming through our kafka
data pipeline every day and this comes
to a centralized data warehouse our data
warehouse is actually in excess of 60
petabytes right now which is a lot of
data but we go through this data very
quickly like the event data that we
bring in we typically delete very
quickly just because it doesn't have a
lot of meaning beyond say three months
in a lot of cases so most of the data
gets deleted very quickly but we ETL
that data we enhance it we enrich it and
then store it in our data warehouse for
analysts to use and it progressively
gets more meaningful so we read about
three petabytes a day and we write about
500 tera
so what does our architecture look like
so we have two pipelines for how we get
data into our warehouse one is the event
data so these are all those you know
little events that I was talking about
log messages things and that goes
through our Kafka data pipeline coming
from all the various services we have in
the cloud and many other sources and
then we have a process called Ursula and
what ERISA does is it just splits these
events and stages it within our
warehouse and based on the event type
and then also pulls together and merges
the files so they're big enough to do
big data processing on top of the other
pipeline is our dimension data so if
you're interacting with a device in
Netflix you're likely talking to a
Cassandra cluster at some point or
another keeping track of you know where
you are in a show or serving new content
so the Cassandra cluster holds the
dimension data and what we do is we use
the backups from Cassandra and agathis
which is an open source project we
created to suck that data into our data
warehouse for the dimension data and
then you have both your dimension and
back data that you can do all sorts of
analytics on top of so the actual
platform itself from the compute
perspective we're going to walk through
this so at the very lowest layer we have
s3 which is where we have our source of
truth data for our warehouse and then we
have Parque which is our predominant
file format which we use for performance
we'll be talking about this more
specifically when we talk about data
warehousing on top of that sits all of
our compute layers so we run EMR
clusters typically everything on top of
yarn with the exception of you know
druid and presto which are their own you
know separate clusters but we have a
wide variety of compute tools on top of
that we have an orchestration layer
services that we use to run all of the
services across our big data platform
and tom is gonna be covering some of
those topics then on top of that we have
a whole slew of tools for doing things
like quality checks and visualizing the
performance of jobs and understanding
what kind of workflows you're actually
executing and finally at the very top
for our data scientists our users of the
platform we have an interface so a web
interface you can go to and launch
queries track job status you know
schedule things as well as an API so
anybody can execute this just with
Python
don't have to have any of the actual
clients installed on their local
machines or in a container or anything
like that they just need a Python API so
what do our clusters look like so right
now we've got two large clusters our
production cluster is about 2,300 d24
XLS so these are big instances in Amazon
they've got a lot of disk storage
there's 24 terabytes per instance on
these disks and this will be important
as well when we talk about the
warehousing the ad hoc cluster has 1,200
this is what people typically will
develop different workflows on or if
they're just doing some sort of one-off
analytic they'll run on this cluster as
well as you know doing some backfill if
they need to rerun some data we also
have a whole bunch of other smaller
clusters that we use for testing or
specific build clusters for certain
types of connectivity or workloads and
then presto for our ad hoc use case so
if people are just you know doing very
quick analytics iterating on a
particular problem they can do that very
very quickly with presto
so this launches us into the the first
big section which is about data
warehousing on top of s3 and s3 is
actually not the most common use for a
data warehouse within the Big Data or at
least the open source Big Data community
most people run on HDFS and you know
this goes back to the old adage of you
know keep your compute and your storage
together but we're actually separating
those two so why do we do this well
there are a lot of reasons why we use s3
so one is they have lots of nines you
know they take care of the the
durability the availability they also
have a bunch of features that don't
exist within HDFS so you know HDFS
doesn't have versioning they have a
trash so it's a little easy to recover a
you know data that was deleted once but
in s3 you actually have full vir
versioning of all objects if you use
version buckets and so that really helps
you know the users of the platform so
they don't have to worry about tiptoeing
around data and making sure they don't
make a mistake and delete a whole data
set we can deal with that we can recover
so their velocity increases they don't
have to worry about are they going to
accidentally do something very bad and
most importantly that and this underlies
our entire deployment is that we deke up
our storage and compute now this is
mostly just for the warehouse data
obviously within the cluster there's a
lot of locality there's a lot of
execution that happens you know between
stages of jobs or in shuffles and spark
locality still exists but our actual
data warehouse is separate and there's a
lot of good reasons for this so one good
reason for decoupling the scaling is our
decoupling is is scaling so each one of
these boxes represents a petabyte of
data so this is our data warehouse size
now if you take a look at all of our
cluster resources all the discs across
all of our clusters and you put that
together and you account for the
three-factor replication that you would
need probably as a minimum with HDFS
especially in the cloud because you know
instances they can come and go you might
have to terminate them and you don't
allow for any buffer at all we don't
have enough capacity in our existing
cluster to house our daya warehouse we
would need something at least two or
three times the size of our current
deployment as large as it is just to
hold our data warehouse and then you
have all the complexity that comes along
with keeping your storage and compute
together and this is a problem that a
lot of companies get into where you have
a big Hadoop deployment and all the data
is in that cluster and all the jobs are
running on that cluster now you need to
do an upgrade and you're trying to
migrate people over to the new cluster
and nobody wants to go because the data
is not there and now you have to figure
out well how do I get the data over to
the new cluster how do I sink this how
do I make sure everybody's running off
there the right data and it really
decreases the ability for the platform
to evolve so we can move very very
rapidly we can have different versions
of the same you know deployments out and
running at the same time we can have
many different versions of the different
compute engines running at the same time
and this allows us to kind of evolve
independently of the data so that's part
of what I was talking about in terms of
decoupling the computing storage each
one of these clusters can still use the
production data so even if we're using a
test cluster testing out new features we
get a used production data for that we
don't have to silo this and keep them
separate now of course if you use s3
it's not that it's always going to be
better than HDFS there are a number of
trade-offs and one of those is is
definitely performance so there's much
more latency in
to s3 than there are in HDFS and this
can affect jobs in their split
calculation specifically so if you have
a lot of directories that you need a
list to figure out how to break up your
jobs and execute them in parallel this
can take a lot of time however this also
runs off cluster so your job is doing
most of this work and it's not impacting
the total throughput of the cluster but
it is impacting possibly stages between
different workflows so if you have a
hive job that's feeding into a pig job
that's feeding into a spark job you're
introducing latency between each one of
those another area is the actual table
scan so when you're doing some sort of
table scan operator within the execution
engine and you're using like park' file
it does a lot of seek operations and
those again are much more expensive
because there's a higher latency for
every request that you have to s3
there's also some overhead in the actual
read request itself just a protocol
that's being used but what we find is
actually that the performance of these
converge as the volume and the
complexity increases so if you're doing
a if you're running a job that has some
sort of heavy stage or parsing that
happens in the initial stage the
complexity of that is forced over onto
the CPU side so you become CPU bound and
your i/o matters less and less to the
point that it's effectively the same as
running on top of HDFS so in cases where
you have really short running jobs and
possibly reading tiny tiny files you can
be impacted by just the difference in
performance from s3 to HDFS but if you
are operating at scale with large jobs
large input sizes that diminishes to the
point where it's almost non-existent so
how do we see this so this is kind of a
simple overview of a job typically you
have a section that happens in the
client this is the the planning stage
the split calculation that can have some
impact and it gets you know exacerbated
by the number of like locations that
you're you're actually listing and how
much data is being split in that in that
phase but then on the cluster side you
really have two areas that are impacted
by any performance difference in s3 one
is the initial table scan
and then the other one is actually the
store operator but inside that you have
many different stages and all of those
are either using HDFS for like big hive
MapReduce with spark it's gonna be using
local disk and shuffling and then in
press so it's actually using in-memory
so there are a lot of stages and what
happens is this cost is also amortized
over the length of a job so if you have
jobs that are many many stages just that
impact at the very ends is not going to
be very significant compared to the
whole operation okay so in addition so
we talked a little bit about the actual
data and how we store it in s3 but it
doesn't make a lot of sense unless you
can make you know you can get to the
data that you actually want to so we
have a metadata system that we call
medic add and it's a federated metadata
system so you can think of it in terms
of like the hive meta store but this is
kind of the hive meta store on steroids
because it reaches across many different
systems it can talk and you know
describe RDS instances Teradata
deployments redshift deployments druid
deployments and it allows us to get
information across all these different
systems but it also exposes a high
thrift interface for anything that we
can run the big data workloads across so
this means that hive presto spark
anybody can talk to it and get a better
understanding or an accurate
understanding of where the data is
located within these this large data
warehouse and the most important thing
is it provides a logical abstraction
over the actual data because nobody
wants to remember s3 bucket some path
into a warehouse and then it's
partitioned by some values this is all
handled by the actual warehouse what
does that look like well if you're
familiar with hive this is a very normal
concept it sounds or is set up very
similar to a relational database so you
have databases so right here we have you
know like a data science and ETL
telemetry data a B test data and then
within each of those you have different
tables and you can select just by
calling out the name in sequel or in you
know using spark or Pig which one you're
looking for and then the added level of
indirection is the partitioning so this
is how you actually structure it within
s3 the nice thing about this is it
actually overlays very easily on top of
s3 in the same way that it works on HDFS
so that partition
location is actually just pointing to a
path an s3 and all the data for that
particular particular partition is
underneath that path so of course the
most important thing is down selecting
how much data you're actually going to
process and by structuring your
warehouse and your tables appropriate
you can really limit how much data
you're going to process now no job is
gonna process all 60 petabytes but many
of them can carve down to just the data
that they're actually interested in in
order to answer a question and that has
a big impact on the total performance of
the job so now we've talked about the
warehouse all the way down to a specific
location but how do we actually store
the data so I mentioned we use Park a
very heavily and there's a lot of
features in Park Hae that add to the
performance especially on top of s3 so
for those who don't know what Parque is
it's a file format it's columnar you
know that means that it's going to take
the data and instead of like a comma
separated file or tab separated file
it's gonna store the column data
together contiguously on disk and this
has some benefits one you can improve
the compression because data in columns
is going to be very similar across the
entire column and so you get really high
compression there you can also do column
projection so if I've got a hundred
columns in a table and all I care about
is maybe five of them I only have to
read the data for those five and that's
really helpful because it cuts down on
the total volume of data that you're
processing in the job so that's nothing
particularly new you know we've got
columnar file formats they've been
around for a long time but one of the
things that parkade does add is it adds
metadata about the the actual data
that's in the file so at the very bottom
this is the structure of a parquet file
so you have the data is actually broken
up into row groups and that's maybe you
know some size of data that you want to
hold contiguously together but within
those row groups you've got column data
and that is what is actually stored
contiguously on disk and what we see
here is a file it has two row groups in
it but at the very bottom we have
metadata and what it has is information
about each specific column what is the
min value what is the max value as well
as the areas highlighted in red which
are dictionary pages so if it can be
Dictionary encoded you have a full set
of the values even though you don't
necessarily know how many times it's
repeated or anything like that
you know what is actually in that column
data and we can use that especially when
you stage your data appropriate
appropriately to take advantage of some
of the features in the processing
engines to either skip or do certain
types of operations like counts very
quickly so one thing you have to do is
you actually have to stage your data at
this point so one of the things that we
do is you partition by low cardinality
fields so these are things like date or
hour or maybe region or something like
that
where you know you're gonna have very
large buckets of data under each one of
those partitions but then what you do is
you actually sort by the very high
cardinality fields and a really good
example of this is like telemetry data
so at Netflix we have a system called a
Atlas and it keeps track of all the
metrics for all the services at Netflix
and they have one column which is the
actual metric name and that's typically
what you're going to be looking for be
it latency or you know throughput or
something like that but there are
millions of these different metrics
across all the different micro services
and you know ways that they slice and
dice this so if you sort by that high
cardinality field you can reduce the
amount of data that you actually read by
a hundred x easily so what does that
actually look like so if we store our
data you can see all if these are all
data files and the data we're actually
looking for is all the little red lines
that you see there if they're all
sporadic throughout the entire data set
but if we sort that you put all of that
data into just one file and then what we
can do is as we go through that you can
you can filter out just the one file
that your actual on a process yes you
have to read the footer for every single
file but that's tiny you can put in
comparison to reading the entire data
set and processing the whole thing so we
take advantage of this with some of our
biggest data sets like I mentioned our
telemetry data set we do it with we also
have like UI events that we have a
common logging so it means that we have
lots of different event types and we use
the same approach there so if you want
more information about how to take
advantage of these there was a recent
release of park' 1.9 that has all of the
the features that we've been working on
and there's a SlideShare here with a
presentation that goes into great detail
about the various properties and
everything else so
we'll be posted I believe after the
conference so if you're interested you
can go take a look at that so with that
I'm gonna hand you over to Tom and he's
gonna take you back up to the sack and
talk a little bit about our service
layer right thanks Dan okay so Dan went
really low I'm gonna come back up a
little bit higher and talk about our
Jeannie service which is an open source
service developed to solve a few
problems and to kind of establish why we
develop this service I'm going to walk
through kind of a story of how a data
platform might evolve and some of you
may be able to relate with this some of
you may not so you start out with a
nascent data platform right you've got a
few users they might log into one
machine and access your one Hadoop
cluster or whatever processing cluster
you have everybody's pretty happy it was
enough resources for everybody there's
not much stuff going on pretty soon you
realize that you need to start testing
some stuff and you need two versions of
things so now you've got a test gateway
you've got a prog eight-way you got two
clusters it's not too bad people know
where to go people may need to replicate
data like dan was mentioning from prod
to test but you can still kind of handle
the situation pretty soon a company's
growing organization is growing we have
more users we need more resources more
client resources more cluster resources
everything's growing doing deployments
becomes much more complicated you don't
want to has much downtime you don't want
to as you have more gateways you need to
deploy different binaries to these
gateways it's becomes very difficult to
start managing that pretty soon you
decide oh I need clusters for specific
purposes so Dan mentioned at Netflix we
have a lot of clusters we have ad hoc
clusters and presto we have lots of
production who do clusters test-tube
clusters and you start scaling
horizontally across all these different
needs then your user base starts
maturing so your data scientists
actually become you know kind of smarter
than people running the data platform
they they want to run the new stuff
SPARC to you know want to use R or
Python or whatever it is they start
complaining about their job being slow
because they're not getting enough
resources or their co-workers starving
them out and you as a system
administrator of their data platform
need to respond to all these things
quickly and dynamically and pretty soon
everybody starts looking kind of like
this
the administrators aren't happy the
users aren't happy everybody starts
crying and you know people are stressed
so a few years ago we started developing
geni to solve these problems we wanted
something that from the administrator
point of view helped us to administer
launching clusters maintaining binaries
all that stuff and abstracting those
details from the users well at the same
time allowing users to have a scalable
set of resources to access these
clusters and not really worry about
where the clusters were how to connect
to them what to run on them all that
kind of stuff or setting up a working
environment so I'm going to walk through
a couple problems that we face at
Netflix from administration and user
point of view for the administrators
we've got a lot of moving parts and Dan
and my team we're kind of the
administrators in this case because we
do devops so we've got about 15 clusters
dan went walk through some of them at
any given time we run about 45 different
variations of the executables you can
run against those clusters whether it be
different versions of SPARC different
versions of you know hi different
versions of pig or presto whatever it
may be and then we see a very heavy load
of jobs we run about 45 to 50 thousand
individual jobs which could compose of
actual a breakdown of those different
processing layers so one job could
actually result in four or five actual
MapReduce jobs or SPARC jobs resulting
on the cluster and we've got hundreds of
users so in the talk before this if any
of you were there they talked about how
you have 80 data scientists may be at
stitch fix we've got well over 300 I
believe in Netflix now so between them
and a lot of other people we're seeing a
lot of users and for users they don't
really want to they don't really care
about the details right they don't care
where your clusters are what's running
they just want to make sure they can
connect and get to their data we've got
a lot of data they want to do it they
want to do their jobs they want to be
easy so how do we kind of merge those
problems together and have a solution
that's kind of where geni comes in in
our platform so I'm going to talk about
how it comes in and helps the platform
administrator which is our team which is
really all I care about so
the administrators we want a tool that
basically simplifies configuration
management deployment so when we launch
a new cluster we don't want to be
copying around site XML files or
configurations about what the cluster
looks like what version is running
anything like that we want to minimize
the impact of changes to users this is
really important
Netflix is we have 300 data scientists
they've got a lot of important work to
do we don't want when we change out a
cluster version or anything like that we
don't want them to have any impact in
terms of being able to do their job we
want to be able to respond to any
problems with the system quickly so we
want to know what clusters are running
where they are how to shut them down how
to do any of that kind of stuff and we
definitely want to be able to scale the
client resources as load increases so we
don't want to be scaling out and sending
out a new gateway every time that we
need to we just want you know the system
to handle that automatically so from an
administrative perspective there's I'm
gonna go into the Jeannie data model so
it's pretty simple we've got something
called a cluster it just basically
represents the metadata about the
cluster which could include things like
where the site XML files are located we
we personally load them to s3 women the
cluster is launched on EMR then each
cluster it can be linked to many
executables and we call these commands
so basically those are what you would if
you were on a command line terminal you
would execute hive you'd have SKU Pig
that's what they execute the command
would be and then each command can be
linked to many applications so the
applications represent the dependencies
so it would be a tarball of all your
binaries for hadoop at our ball of your
spark application whatever it may be and
jeanne will take care of downloading all
the necessary dependencies into the
working directory for a given job and
I'll get into that in a little bit it's
important to note that each of these
resources we have in the data model can
be tagged as specific tags and we use
these tags a lot to uniquely identify a
cluster and allow users to just set
certain tags to say I always want to run
on say the SLA cluster or I always want
to run SPARC submit I don't care what
version is just give me the default or
give me I always want on the SLA cluster
so it allows us to move the
bags around from cluster to cluster as
the role of clusters or resources change
and I'll get into some of that too this
is just a quick screenshot of the Gini
UI that shows how you can search
resources it shows all the clusters that
we currently have registered in Genie
and kind of their state you can filter
on state like that so this is kind of
how we as administrators can go and see
how anybody is launching clusters and
where they live and how to connect to
them and that kind of stuff so let's go
through some quick use cases that
somebody might want to do in a Big Data
Platform if you're an administrator so
let's say you want to update a cluster
the first thing you would do in the
cloud is kind of start up a new cluster
you'd then register that cluster with
geni with a set of kind of default tags
which generally means your ID and your
name but none of those common tags like
SLA or anything that would be common
across a bunch of clusters you then run
whatever smoke test you want to make
sure the cluster is good let's say
you're testing a new version of Hadoop
you want to make sure all your previous
tests still run fine you would then move
your tags over from the old cluster to
the new cluster in geni so like that
schedule SLA tag I showed earlier you
can move that to the new cluster and all
the jobs would automatically load over
to the G to the new cluster so anybody
submitting new jobs that wanted to go to
the SLA cluster would go to the new one
they didn't have to change anything on
their client code they'd have to change
anything in there you see for code
anything like that it just automatically
works and you can let the old cluster
the old jobs finish on the old cluster
behind the scenes users can still access
them because geni still knows where
they're running you can still work
through that and then you could shut
down the old cluster when they're done
and there's no downtime to users in this
C's case next let's say you want to load
balance between clusters let's say you
in the middle of night you have
different loads so for Netflix
specifically you know people watch a lot
of TV late at night but then you know in
the middle at night they're sleeping so
our load goes way down we have a lot of
resources available well we may want to
move some of our ETL jobs over to a
different cluster so we take the tags
that are in our production cluster and
copy them over to our ad hoc cluster
because all data scientists are usually
sleeping at night hopefully so we take
the resources that are on those ad hoc
clusters and we put the same tag on both
and now Jeannie will just start putting
jobs to both of them because they're
really the same cluster it's just how we
reference them then in the morning when
we say okay the data scientists are
starting wake up we need the resources
back we remove the tags from the from
the ad hoc cluster and jobs are stopped
being submitted to than that should be
on the production cluster and this is
transparent to all the clients they
don't really know which cluster their
jobs are going to they just know how to
access them via geni so and then usually
if you're an application administrator
you want to deploy a new version of
SPARC let's say you have to find all the
clients that are running those versions
write all the gateways any users that
are upstream running SPARC or whatever
it may be but you need to update that
without any downtime for geni since all
the jobs run on the Genii nodes we can
load the new binaries to essential
download location in our case that's s3
and we can then and then Ginny will
automatically invalidate that's cache
and it'll download the new ones on the
next run-through so this will be an
instant change across the geni cluster
as jobs are run so you don't have to
actually hunt down all the places SPARC
or whatever it may be are deployed you
just need to update Jeannie's
configuration and Ginny will take care
of the rest okay so those are some use
cases that refer administrators
hopefully most of you can relate with
them in some way
so for users the data scientist they
usually want to discover a cluster for a
job to run on they want to run the job
client which could be hive SPARC
whatever they want they don't care about
what your dependencies are they don't
want to have to install SPARC they want
to install hive they just want to run
they want to be able to check on the
status of their job easily obviously
they want to view a history of their
jobs to see how their performance
changed over time and they obviously
want to get their job results so I'm
gonna walk quickly through how Java
submission to geni works just so that
we're kind of on the same wavelength
here so you're a superstar data
scientist and you have your boss wants
you to create a report or you need some
sort of new report on an a/b test and
you're like I need to run a job so you
submit a job to Jeannie and basically to
the key field in any job submission are
these cluster criteria and come
criteria which leverage those tags I
discussed earlier in this case the user
wants to run on a yarn cluster that's
the SLA roll so kind of a production
cluster they don't care what it is they
just want the production cluster and
they want to run SPARC version 1.6 that
Java submission goes to geni through the
REST API and geni accesses its RDS
instance in this case my sequel to get
the metadata about what's available
currently in the system it finds that
there's a bunch of clusters and a bunch
of commands that are currently available
it'll then compare the tags that are on
those resources to commands in the
clusters to match anything that could
match what the user selected and it
finds that there's a cluster that
matches and a spark command that matches
it'll then download all the
configurations out of s3 this bucket is
supposed to represent s3 my boss told me
they would so that's us three so all the
files are downloaded onto the geni node
at runtime and set up in the working
directory so for that spark command and
for that hadoop cluster there all
downloaded into the working directory
which becomes effectively like if you
were just sitting on the command line if
you executed SPARC that's your working
directory Deenie will then submit the
job to the cluster that was selected
it'll run your user can go into the
genie UI monitor it see what's going on
and wait for it to be done so hopefully
that's clear so the genie data model
looks kind of like this from a user
perspective their job is linked to the
cluster command and applications that
ran with they can go and access this
metadata anytime going forward
a job request it's kind of hard to see
the scale but basically there's some
important fields in here the cluster
criteria and command criteria you can
see and any command arguments they don't
want to add to their executable and tag
you can tag your job so you can search
them later and that kind of stuff from a
higher level we've created Python
clients Java clients that users can use
to submit jobs to genie that abstract a
lot of the details of those JSON
payloads so in this case this is a
simple example using the OSS Python
client from somebody who wanted to run a
presto job so they're just selecting you
just say what script you want to run
any parameters you want to submit in and
you can run the job and wait for it to
be done just via these few lines of
Python so you don't don't worry about
where the presto cluster is what the
binary that's running is this is all
through service based access once your
jobs are done users can go to this job
history UI
and look up jobs and go to the find the
job output directories so those little
file folders if they click that for
their job they're taking us something
looks like this and this is basically
their working directory so all their
scripts are loaded in there the output
is in standard output standard error
obviously holds your error file so it's
just like they have their own custom
working directory on the box so there's
nothing that the user needed to do to
set all this up do you need download it
at all out of the configurations and and
ran the job and executed it on the
cluster so that's kind of a brief
walkthrough through Jeannie's so
wrapping up we've got our data warehouse
which Dan discussed we use s3 to scale
out as a robust set of nines as Dan
mentioned we like to decouple the
compute from the storage for all the
reasons that Dan discussed the
performance and the ability to run
clusters across all the data sets I'm
going to use Parque for speed a take
advantage of the column or format on the
genie side we're running the actual OSS
code in geni so if you want to run geni
you can take the same code and run the
same stuff we are we're running about
45,000 jobs a day on those geni nodes
which are about a cluster of 25 I to for
excels in production and we keep about
three months of jobs in the database and
the job history so that's about three
million or so of history and if you want
to get see what Ginny is all about you
can go to the github page we've actually
created a demo using docker and docker
compose where you can stand up jeanny
along with a couple of clusters on your
localhost and submit jobs to those
clusters and you can see kind of how it
works so with that I think we finished a
little bit early so we've got time for
some questions if anybody has them
just don't piss us off cuz I don't know
it so do we oh we don't all for this
yeah we don't have a mic so we're gonna
yeah and we'll repeat the question okay
how do we handle EMR five master failure
so there's it does happen all the time
well I'm not gonna say it happens all
the time but it does happen and what we
actually do is we actually use Jeannie
for this so one thing that you can do is
you can either have a small standby
cluster that just has a couple nodes in
at the sitting you know idle and using
the Jeannie tags when we detect a master
failure we can just reset the tags to
the new cluster and issue a resize and
it will you know go up to size terminate
the old one very quickly and usually we
can do that with very minimal downtime
we also do this with just our upgrades
and everything else so we do a red-black
deployment and we use the tagging and
Jeannie in order to handle that but yes
it does happen next question way over
there in the corner how do you handle
two systems in different so we actually
use we have all of our data house a data
warehouse in US East one so we bring all
the data to one place because that's
going to work best for the big data
compute if you're reaching across to a
region in some on some other continent
you're gonna see some serious
performance degradation is it is my
expectation I've never actually tested
it as far as the disaster recovery
you know we rely a lot on s3 for the the
warehousing that goes to the eleven
nines of durability or whatever they
they spec out so that's what we do from
that perspective and then the clusters
themselves we run in two different a zs
so we can always make sure that we have
a sizable cluster in one easy or the
other and using the tags if something
happened where we lost our production
cluster we could always just retag our
ad hoc cluster to be the production
cluster and it would start taking all
the production load and while there
would be some interruption because it's
not as large as the production cluster
at least the the critical jobs and
everything else would still be able to
go through so so the question was how do
we handle access control and can we talk
a little bit more about meta cat so meta
cat again is a federated metadata
service and the idea behind this is that
we have lots of different data systems
and we want some central way of talking
to each one of them or getting
information about the data that's stored
in each one of them that doesn't mean
that you can read data through meta cat
it's just describing all of the data
sources so you may be able to say
something like oh we have a druid
cluster and this is the data that we
indexed into it and you know this is the
configuration for it so any system that
needs that information has one central
place to go to so we do that with Terra
data we do that with redshift and it
makes it really easy for us to do things
like you know we have libraries built
around moving data from one place to
another
while s3 is our source of truth we want
to be able to say oh well take this s3
data or maybe this specific table and
these date ranges or some selection of
that and move it over to Terra data or
move it into redshift and so people can
just issue very small commands that just
say
move you know h2r hive to redshift and
it will take care of it for you so
that's the the purpose of meta cat but
one of the things it also does is it
abstracts something so you can actually
run against a hive meta store so it
exposes an interface to get metadata in
the same way that all the big data
compute engines that are out in the open
source typically expect to see it so I'm
not saying that we're not concerned
about our data I mean there's always
disaster cases or potentially malicious
cases where data will be lost what we do
is we leverage the tools that they have
for both you know access protection and
you know control over what users can
actually do to the data and one of the
things that I mentioned is that we have
version data so we let people soft elite
we don't let people hard delete so there
are things that you can do to you know
ensure that you're not going to
accidentally lose data in those cases
but you know there's obviously the case
where there's some giant disaster and
you know there there are things that
we're looking into for better ways of
backing up as much as we can of the data
warehouse but replicating sixty
petabytes is a lot of data to have a
second copy of expensive I saw some
other hands here did anybody else have a
yes so I'll answer a couple of those at
kind of a high level and if you have
more specific questions so the question
was about performance with spark and
park' against s3 how do you do the the
right size of files how do you set the
right options some of the options are
set in in that talk from the that was on
the slide there as far as the actual
file size block location doesn't matter
in s3 because we don't have any
information as to the block size or how
s3 is actually carving up the data in
the back end so you don't necessarily
want to try and align to certain block
sizes there are ways you can kind of
fake it out with the file system for
split calculations that may have some
effect on how those get divided up SPARC
specifically we actually have a heavily
modified internal version of SPARC to
address some of those problems with the
read and write path we are working on
trying to make an open-source version of
our s3 output committer which takes care
of a lot of the problems that you know
you see with the open source especially
you know because it tries to treat s3
exactly like any other file system
they'll try to stage its intermediate
data there and then when it goes to
commit it'll copy it into the final
location but s3 doesn't have a copy I
mean it doesn't have a rename function
it only has copy so you effectively
copied to s3 then re copy it to s3 and
then delete the temporary wants and
sometimes that can happen multiple times
so there are high latencies and we've
worked around that in certain ways so we
use an s3 output committer that actually
writes locally and then uploads to s3 is
the final commit and does that you know
massively parallel so it's actually
quite performant
so any more specific ones we can take
offline
so I doesn't have support for Parque
with vectorize read anyway I mean it's
got a vectorized execution but the read
itself is not vectorized so I wouldn't
worry about it too much with hive as far
as the vectorization I think that we'd
have to look at that more closely
because I'm not sure that it is quite
linked to actual actually where it's
stored because a lot of these systems
like with spark or presto the execution
that they do is not necessarily
vectorize in the sense of like CPU
vectorization it's vectorized in the
sense that they're doing an operation
across a lot of data that's you know in
one particular column or maybe sets of
columns at a time and the way the Presto
does it it just bytecode compiles the
entire stage or execution that it's
doing and like spark judo is doing that
across all the difference you know
against all the different operators
within a stage as much as it can so it's
actually not vectorization in the terms
of cpu right exactly right
using the summary files or the yeah so
we actually don't use summary files and
part of the reason we don't use summary
files is because we have a metadata
service to do this for us so we entirely
do not rely on any of the the footer
data for split calculation or the
summary files for describing the the
schemas or anything like that all of the
schema information is actually in the
meta store and across most of the
engines there is a feature which is
called column index accessed so if
you're typically reading park' files
you're going to be reading by name so
you're gonna be addressing columns by
name but any system that's working with
a hive Metis or hive traditionally will
reference things by their column ordinal
so what we do is there's a flag that you
can flip to do ordinal base access and
then what you're doing is you're
basically saying I don't really care
about the file schema I only care about
what the metadata schema is and that's
the way that we work any other questions
guess who'll let you go for the night
thank you very much thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>