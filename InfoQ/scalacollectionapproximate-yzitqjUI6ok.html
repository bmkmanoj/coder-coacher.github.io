<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>scala.collection.approximate | Coder Coacher - Coaching Coders</title><meta content="scala.collection.approximate - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>scala.collection.approximate</b></h2><h5 class="post__date">2013-03-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yzitqjUI6ok" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm lb Brian and currently at Etsy I'm
going to talk a bit about approximate
collections so so approximate
collections why would we ever want an
approximate collection and what do I
mean by that so there are times when you
have a collection sort of conceptually
of an unbounded number of items are a
very very large number of items right
millions of items billions of items but
you've only got a bounded amount of
memory and so you want to get some of
the advantages some of the summaries or
aggregates or properties of a collection
like a set but you want to do it without
actually storing all of the items and so
necessarily what you're going to have is
an approximation so almost everybody I'm
assuming in this talk that everybody in
this talk knows bloom filters which are
probably the like best known sort of
approximate set the context of this talk
is is the algebra library which which
came out of Twitter and if you were in
Oscars talk earlier you might have seen
a bit about it it it's a it's an
abstract algebra framework but but
specifically focused on data structures
that you might use in streaming or in
distributed computation like Hadoop for
aggregation so in practice a lot of
those end up being approximate
collections this came out of the
scalding project which I started a
Twitter and Oscar extracted a bunch of
stuff from it that that he wrote and and
added and a bunch of us have contributed
to it but most of the credit for a lot
of code you see in this talk should go
to Oscar there's no contents may sell
during shipping what I'm showing you
appropriately is sort of approximate
code a bunch of it is real code from
algebra some of it is sort of api's that
we're thinking about but haven't
completely implemented yet or it's only
sort of Spada ly implemented some of it
is sort of a little bit you know
alighted just for ease of presentation
so the concepts are all their most of
the implementation is there but some of
this is kind of up in the air and part
of the point of this talk is maybe it
have some discussion about what it
should look like so if you
we're going to implement a bloom filter
kind of them the kind of simplest most
naive API that it might look like is
this so you know you you need to be able
to update it with a new item and then
you need to be able to query as to
whether or not the bloom filter contains
that item and in the bloom filter you
know most of you probably know this that
if the item is not there if the item has
never been added to the bloom filter it
will it will answer false excuse me if
it answers false that means that the
item definitely was never added if it
answers true it might mean the item was
added it might be a false positive right
and so you know that's that's maybe how
you might think of this how you might
implement it in Java or something in
Scala we want to make this nicer
probably in a couple of ways one is we
probably want this to be immutable and
the other is that we can make this sort
of start to look like a set because
that's conceptually what this is so we
might have the plus for the element and
have that routine return a new instance
of the bloom filter and we might have
some way to union two of these things
right we might have plus plus for that
so you notice thinked and contains right
which is also part of the set API in
Scala so this thing starts to look a
little bit like a set and it's worth
asking once this starts to look a little
bit like I set what else from set might
we want to add here so you know all of
the stuff that makes set iterable is
just totally out right because the
nature of a bloom filters that you do
not have these elements and so you
cannot iterate through these elements so
we can't do you know for each map Latin
have any of that stuff right can we do
sighs right because size is another
thing that sets tend to have and it
turns out that in a bloom filter you can
do sighs there's a paper that describes
how you can estimate the number of items
that have been added to a bloom filter
basically just looking at how many bits
have been set in the bloom filter right
and as long as it hasn't gotten too
filled up right once it gets filled up a
certain point your estimates going to be
really bad but in general you can
actually get a pretty decent estimate
and so you know when you say things like
pretty decent estimate
you know is natural to ask like how good
is the estimate and in fact the paper
gives bounds right and the bounds tend
to be of this form right if I tell you
the estimate that I have of the size i
can give you a sort of minimum and a
maximum right of what i think the the
range is that the size is in fact going
to be and i can also give you a
probability that the true size is within
these bounds right and that's sort of
like you know if you see stats reported
as like you know plus or minus five
percent 19 times out of 20 right it's
the same pattern right I've got I've got
a bound I've got a confidence bound and
then I've got a probability that the
true value is in fact within that
confidence bound and so you can end up
with an API that looks kind of like this
right and it turns out also that if you
have an estimate of the size of a bloom
filter one of the other things that you
can do is you can estimate the false
prophet or give a probability for the
false positive rate right and so that's
useful because when you ask and you get
back a true you may want to know like
how much can I rely on this true so this
API starting to get a little bit clunky
I think you know with all of these
different things asking different
probabilities asking different sizes min
sighs max sighs so so it would be nice
to clean that up a little bit and one
way that we can clean that up is by
introducing some new types so rather
than returning a boolean and having the
separate method for the probability that
that bullying is wrong we can return an
approximate boolean and similarly rather
than returning a size and then having
all of these separate methods giving us
some bounds on this we can return say an
approximate long so and these types
exist within algebra so what does this
look like well an approximate boolean a
you know basically is just a boolean
value and a probability right so you
know actually in the case of a bloom
filter if it returns false that
probability is going to get set to one
right so it's going to give you back an
approximate boolean that is in fact an
exact boolean but if it returns true
it's going to give you you know some
probability which is the same as that
like probability of false positives that
we saw another you had these types it's
interesting to think about like what
operations can we do on these types
right so in the case of approximate
boolean we can for example negated and
the first time that I saw this coded
actually threw me for a second because I
kind of expected to invert the
probability somehow in my head I was
like oh that should be 1 minus profit
that's not right if we know with some
probability P that we have true then
it's that same probability P that the
negation of that is false right and so
that's all this is doing right is giving
you a new approximate boolean that flips
the bit and gives you keeps the
probability the same right things like
and are also possible it gets a little
bit more involved but if you sort of
read through this code what's going on
is that if we're anding true with true
right the approximate true we get back
has the same probability as multiplying
those two probabilities right so it's
true with the probability of you know
their their combined their joint
probability right otherwise it's false
with the max of their two probabilities
right so you know if I tell you that
this one is like very probably false and
then I and that with something that's
that that is approximately true then it
doesn't matter how approximate that true
value is because I know the false is
really really good right and and so that
excuse me it's it's the it's the maximum
of the false probabilities right the
true probability is don't matter so you
know or you can implement us very
similar it's just everything's kind of
inverted right okay now we also have
this approximate long what does that
look like well again we're just kind of
encapsulating in this type the same
methods that we had before right so
we've got a min we've got an estimate
we've got a max and then we've got a
probability that the true value is
within that range and there's no reason
we have to do this specifically for long
we can do this for any numeric right so
we might as well use the numeric type
class and do that and so what can we do
with with an approximate numeric well
probably the most important thing we
want to do apart from just directly
reading the estimate in the range is
asking whether or not a certain value
that we're interested in is within these
bounds right and kind of nicely
what that returns is an approximate
boolean you know what else might want to
do you know we can add approximate
numbers we can we can subtract
approximate numbers I'm not showing the
implementations because that's not
really the point you can go look at the
source code you can kind of imagine what
the what the manipulations on the
probability would be so okay you know
can we generalize this right so bloom
filters do this but but do we can we
just think of a general approximate set
interface of which bloom filter is
simply one implementation right and and
what would the other implementations be
so another one that we use a lot in
algebra does hyper log-log so hyper log
log is this fancy distinct value
estimation algorithm right Sol Sol bloom
filter was really designed for the
contains operation right bloom filters
optimized have good bounds on contains
hyper log log was optimized have good
bounds on the size method right it can
do contains it does contains kind of
janky which is that if you have a hyper
log log and you want to know whether it
contains something you just add
something to it and see whether the size
changed right which is not you know
going to give you the best bounce right
but on the other hand it has really good
bounds on size so I think Oscar
mentioned in his talk that if you're
willing to give it you know like 15k or
so then you get under one percent error
which is pretty good and I think you
give it 1 K you still get within like
two percent error so how does it work
I'm not going to explain this talk how
hyper log log works but I'm going to
explain how one of the earlier in the
family of this algorithms that that came
from the same people works so if you
have n values in this set and you see
them in a stream or whatever you know
however your computing this every every
value that you see you hash using a you
know good uniform hash function the
let's say for the sake of argument
hashes them to between 0 and 1 and I've
got little marks on the number line
they're supposed to represent like where
those hash values fall right so this is
sort of supposed to represent like a set
of I don't even know what that is 10
items or something right
and it's more or less uniformly and you
know this is it's not perfect but it's
uniformly distributed from zero to one
right so okay that's fine now let's
think about the distance between any two
of these adjacent hash values right so
let's call that W so what do we expect w
to be well if we have the distance
between 0 to 1 and were uniformly
distributing these things across it we
expect that to be 1 over N where n is
the number of items in the set right i
think that's that's fairly
straightforward and that's true you know
wherever we're looking at two adjacent
ones or if we're looking at the distance
between 0 and the first one the minimum
one right that's that's always going to
be true so okay that's cool because that
means that if all that we do instead of
remembering all of these is just as
we're screaming through this we just
keep the minimum one we've seen right
now we have an estimator of the total
number of items right so we just look at
W we do one over w and that's our n and
that's going to be actually a pretty bad
estimator right it's going to be very
high variance and so a better thing to
do is to keep the K minimum that we see
right the K lowest ashes that we see and
now we've got KW and k / end is is going
to you know expected of you KW and so we
can again estimate n and and now it's
just a trade-off of space versus
accuracy right so the more k you know
the larger you know an approximate set
we're deciding to keep the better error
is going to be so that's cool and in
very useful the hyper log log is
basically this but like better optimized
so there's another one that we use in
algebra which is which is min hash and
in min hash is optimized for a different
purpose again which is set similarity so
the assumption is you have a couple of
these approximate sets and you want to
see how similar they are to each other
so how does that work well it actually
starts out very similarly which is to
say that you have some hash function and
every item in the set
you run through this hash function and
you keep the lowest one right so so far
this looks almost exactly the same but
but you do something different with it
and if you have two sets and they have
the same hash function and you kept the
lowest one of each the question you ask
is what's the probability that the two
sets are going to have the same lowest
hash value and it turns out and I don't
think I want to walk through a proof of
this but hopefully people can kind of
intuitively see this that the
probability that these two sets are
going to have the same lowest value is
the same as the size of their
intersection / the size of their union
right so if you sort of you know hand
wavy proof right is that if you sort of
think of like combining these and and
that's going to be you know have as many
elements as the size of the Union and
then ask like what's the chance that
that's going to change the lowest one
that's sort of the probability of you
know how many of them intersect / how
many of them Union right so anyway that
that quantity the size of the
intersection divided by the size of the
union is actually a common similarity
metric which is Jaccard similarity or 1
minus that is is the Jaccard distance
which is the actual metric right so
that's cool but you know you're just
going to get a binary 1 or 0 out of this
right so knowing that like you know
these sets are yes they're similar with
this probability or no they're not
similar with this probability is not
actually that helpful what you want is
to know how similar they are and so what
you do is you do this K times with K
different hash functions right so
instead of just keeping so the
difference between this and the last one
I was describing right is that the last
one I was describing you have one hash
function and you keep the K lowest
values here you have K hash functions
and you keep the one lowest value for
each of them right and then you can do
this pair wise comparison where you look
ok for this hash function does the
lowest value that I've kept match for
this has functioned
the lowest value that I've kept match
for this hash function and you do that k
times and you see how many matches you
get and so the number of matches divided
by K is your estimate of that
probability which is equal to the
Jaccard similarity right and so from
that you can get similarity and so okay
this sort of requires new extensions to
our to our approximate set API right
which is we want Jaccard similarity as
an approximate double and then actually
if you have their similarity if you have
their Jaccard similarity and you have
their size you can compute the size of
their intersection and and I should have
put the math up here right but if you
think about it if you take their if you
take them you have a union operation
here so you can get the size of their
union you have the Jaccard similarity
which is the size of the intersection
over the size of the union so you
multiply those two things and you end up
with the size of the intersection right
so this is this weird kind of asymmetry
between unions and intersections that
that you tend to see in these structures
which is that unions are sort of free in
the sense that they're lossless right
you don't increase the error by and
large by Union and two sets and so you
can kind of Union all day long and take
the size right but intersection is kind
of you get one shot at it basically
right because your air is just going to
keep going up and so usually you you
have an actual Union operation right you
have a size operation and then you have
this intersection size operation and
sometimes and put code up here for it
but sometimes you can do like an N way
intersection and that's a little bit
better and a little bit more useful but
in general this is this is kind of what
you get so this part of the API am not
like super happy about in that it would
be so for example similarity and
intersection size are related quantities
but some of these approximate sets are
better at estimating one and some of
these approximate sets are better
estimating the other and so you don't
really want to get the user to just use
one and do the multiplication themselves
because then you might end up with two
high bounds so so there's that
so that's approximate set stuff and and
it would be nice also if there were a
proximate Mac approximate list kind of
doesn't make sense because really lists
what you want to do is iterate over them
and again you can't iterate over them
but approximate map might make sense we
don't have yet anything that totally
feels like an approximate mat but one
thing that we do have is what you might
call and this name isn't in the code but
I just made it up for the talk an
approximate frequency set so what I mean
by this is it's basically an approximate
set you're still adding elements to it
right but not only can you ask does this
set contain this element you can say how
many times did I add this element to
this set and that can be really valuable
right so you have an operation that's
not quite yet but it's like get
frequency or get count and again that's
going to return an approximate number
and we only have one instance of this
implemented it certainly there are other
structures that would implement
something like that but that's the
countenance catch and so I'll briefly
describe how accountant in sketch works
so you imagine having just a just a
bunch of buckets right a fixed size
number of buckets each of which is a
counter and each time an element comes
in you have a hash function that's going
to choose a bucket for that element and
you're going to increment the counter in
that bucket right and and so whatever i
have k k hashes to that bucket i'm going
to increment you know from for there was
before 25 and now i want to know later
on how many times have I seen k well I
use the same hash function i get to the
same bucket I look it up now is that the
number of times I've seen k I mean no
probably not it is an upper bound on the
number of times I've seen K right
because I have incremented that bucket
at least as many times as I've seen K
but it's probably quite an over estimate
because i have also no doubt seeing
other things unless my number of buckets
here is like on the same order as my
number of items in which case this is
kind of pointless i have no doubt also
seen other things
that hash that same bucket and so that
number is going to be too high and so
how do I solve this so there's a common
pattern here which is sort of how do I
solve this is basically we do this K
times right so again we do this K times
we have K different sets of buckets with
K different hash functions right and
every time the element comes through we
get the K different buckets and
increment each of them and then when we
want to do the query we look at the K
different buckets that are hashes too
and we take the minimum one right
because all of these are upper bounds so
now we have K different upper bounds and
so if we take the lowest of the K
different upper bounds then we get still
an upper bound but we get the best upper
bound right and so we've managed to
reduce our error by having more of these
and and so that's actually all the stuff
that I've prepared and I went through
that a little quickly i'm i'm from etsy
my team is is a distributed team
entirely distributed team we don't
commute to the office so this group
doesn't commute but if there are any
questions I'd like to take them yeah so
the question was can I talk about some
of the use cases for these various
operators right so a you know a bloom
filter so I'm going to be coming at this
very much from the context of kind of
Hadoop distributed computation right hey
bloom filter is is used all over the
place right often when you want to know
so you have a bunch of machines you have
data distributed over a bunch of them
you want to look up some value in each
of them and you have a bloom filter that
cash is in memory sort of what values
are on disk and so you can quickly check
in the bloom filter to see whether or
not it's worth looking and disk for this
value or if you're doing join especially
like a join of a very large table to a
very small table one thing that you
might do
is produce a bloom filter of all of the
keys in the small table right and and
ship that over to all of the various
partitioning zuv the large table that
you've got like spread over your
distributed file system and then you can
use that to filter the large table down
considerably and then you have fewer
keys that actually need to go into your
shuffle or whatever it is a hyper log
log you know any time you I mean unique
visitors is the like classic use case
for this right so you know you want to
know over this very large stream of
events how many unique visitors you have
that day or that month of that year and
the fact that you can Union them is
really nice so you know as a trivial
example right if you store the hyper log
log for each day of the month and then
you want to know the unique visitors
like across the whole month you can just
Union those up which is quite an
efficient operation and then get that
right you can also imagine sort of more
complicated analytic stuff where you
have a whole bunch of like segmentations
like you know segmented by that
segmentation by time but you might also
second by geo or whatever and so you
want to Union up a bunch of these little
sort of shards and see how many people
are in them and then like see how much
intersection there is with something
else min hash which is about set
similarity is very often used for like
personalization algorithms or
recommendation algorithms so you know
Google News uses this where there's one
set which is like the number of articles
that I have liked and or read and
there's another set which is the number
of articles that you have read and they
want to see how similar we are to each
other because if we're similar enough
then maybe if you read an article they
want to recommend it to me right or you
know other similar similar similarity
stuff right count min sketch one of the
main applications of countenance
sketches like a heavy hitters so
streaming heavy hitters so what that
means is that if you so the count min
sketch bound is basically the the
percentage of the stream has a
percentage error right so i can say with
certain error bounds that
this item is you know makes up six
percent of the items I've seen over the
stream or whatever and so what you might
do is say I want to record any time an
item is is estimated to be greater than
like one percent of my stream or two
percent on my stream or something like
that I want to keep track of it and so
along with my count man sketch I'll keep
a list of sort of all of the items that
have been estimated to be above that
count min sketch is also very useful
back to to joins in a distributed
context because estimating the frequency
of various keys tells you a lot about
what kind of joint algorithm you might
want to use and you know how skewed it
is and so one of the things that that
count men sketch gives you a very good
estimate of is what sometimes people
call the f2 or the second frequency
moment which is like the anyway it's the
dot product of the frequencies but
basically it's a measure of skew and so
counting in sketch is very good for that
count min sketches also this gets really
involved but if you build up a tree of
them it's a way that you can estimate
percentiles so you can end up getting
both range queries and you can get like
mediums or nine gifts percentiles or
whatever which tends to be a really hard
thing to get a good estimate of in a
distributed context or in a streaming
context but if you use like you know
enough basically you use sort of log and
countenance catches it given that you
have n distinct values then you can get
really good estimates on on percentile
stuff so you know those are the things
that that I've looked into using these
for I mean I'm sure there's a bunch of
other things that other people use them
for so a cache is another kind of
approximate set yep with the other
inverted right what is this API biased
to the bloom filter style of sets what a
cache mix-up it's not know right so so
an expiring cash right if it tells you
that the thing is there it's definitely
there if you tells you that the thing is
not there it it might have just fallen
out of cash and and you would just use
approximate boolean
but your truths would have the
probability set to one and your falses
would have the probability set to
something right which is the inverse of
what a bloom filter does but but the API
would actually be exactly the same yeah
so there is a lack of intersection
between the intersection and union as
far as I can think that's completely
orthogonal to the what we're talking
about that the contains you know whether
whether it's I don't know the technical
terms for this right but but whether
it's biased towards false positives or
false negatives right yeah in the few
algorithms where you need to use K
different hash functions can you talk
about what if any constraints are on the
different hash functions and what ash
functions you would tend to use or how
you do that yeah so what we use is
murmur hash three and murmur hash three
takes a seed and so the the approach
that we've been using anyway is use a
fixed seed to initialize a random number
generator use that random number
generator to generate k different seeds
use those k different seeds for K
different murmur hash variants I have
like no formal you no justification for
that it seems empirically to work fine
so kirshen mitcham it Kirsh and Mick
mitcham occur did a formal study where
they showed that you can use to hash
functions as long as they're linearly
separable and you can use linear
combinations of those two get n hash
functions with a decay of about one or
two percent worth of overall efficiency
in the algorithm so if you're looking
for a way to cut your hashing cost that
may save you a lot of work yeah thank
you why does approximate boolean have a
boolean value it's just a probability
that's a good question i'm still
thinking about it Oscar is going to
answer that one the real answer is that
you want to push all kinds of proofs
through about like not or and or or and
if you know something about the the
space that you're working in you can
actually do that if you give me like an
approximate a like it's it's it's
probability it's at least with
probability half a and then I want to
combine it with some function like from
A to B to C that that's also like you're
going to give me an approximate be then
I have to resort to relatively loose
bounds about like the probability that i
got see correct you have to only go with
that and correctness and that falls off
exponentially but the wind that obvi
showed where you can we you have certain
facts about the way bullion's combine
that if you see an or if either one of
them are true then you can strengthen
the balance and they don't necessarily
fall off exponentially fast that was the
only reason for it the question is how
we persist these yeah so that's a that's
a great question right now the answer is
badly which is to say that we use the
cryo library for for serialization
that's great for temporary storage when
it's like just going from the map nodes
to the reduced nodes across a new
cluster and often that's all that we
need right often you're sort of going to
pull a final answer out of it and that's
what you actually need to persist in the
long term but you know serializing with
that particular framework on disk does
not seem like a robust long-term
solution so one of the things that we
are adding to algebra is more stable
long-term bye Jek shins to and from byte
arrays but couldn't resist but but yeah
so so that's that's that's a big to-do
item actually for the framework right
now most of these effectively get
represented as bit fields right so it's
not like serializing them is hard but
but there does need to be better code in
there than there is right now thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>