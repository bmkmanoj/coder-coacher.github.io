<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2012 PyData Workshop: Boosting NumPy with Numbexpr and Cython | Coder Coacher - Coaching Coders</title><meta content="2012 PyData Workshop: Boosting NumPy with Numbexpr and Cython - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2012 PyData Workshop: Boosting NumPy with Numbexpr and Cython</b></h2><h5 class="post__date">2012-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J3-oN_TulTg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I am Francis call Det I work for
continuing continuum analytics and
during my tutorial I would like to
introduce you to two new mex and siphon
as a libraries that can help you to
accelerate operations with known by ok
can you everybody can hear me well ok so
for this tutorial you can you can grab
the exercises materials in this URL or
this exercises had has also been put on
the on the USB key that you have ok so
it's just the sitar ball you have to
constrict everything but all the source
code is there ok so first of all I will
intro may I will do some small
introduction about the era of big data
that we are then I will talk a little
bit about numpy and its ecosystem then
we will proceed with some exercises on
new mex which is a library to accelerate
to interpret and accelerate numpy
expressions then I will say a few things
about site on which is a way to compile
which is a language which is very
similar to python but can be compiled
and runs much faster than Python itself
so of course we we are in the age of big
data this is apparently there is
apparently too much hype on it but the
reality is that hahaha yeah it's true
hahaha but the truth is that we have
reality much more sensors computing
devices are pervasive it is really
amazing how many information you can
transmit you by using your smartphone
and it is also true that Internet allows
to grab data anywhere and it is really
easy to access and to cumulate big data
and of course meaning all that big data
has a lot of benefits because you can
extract a lot of value from meaning this
this data but however the problem is
that the beginner that the data is of
course this law where we can process it
it's just a matter of a matter of fact
so processing this data faster give us
us more gift as more chances to get the
insight behind it so our challenge is
that you must we must use our
computational resources optimally in
order to to be able to get the most out
of big data and numpy place a very
central role in this in this aspect
because it has become a de facto
container for big data applications in
the in the Python universe in fact numpy
has created a very comprehensive
ecosystems ecosystem around around it
and there are many many libraries that
uses non pie in order to deal with this
big big data ok there are machine
learning libraries syfy library cycads
image packages for processing images
stats models CH pandas many many things
I couldn t opencl
also relies on Mumbai in order to
accelerate computations and noon Pike
has become the de facto standard for
good reasons okay it provides a very
handy multi-dimensional data container
that allows for efficient data access
and also provides powerful weaponry or
data handling and most important is that
it provides an efficient in memory
storage let me show you an example of
how moon pie is much more efficient that
the containers that normally provides
Python for example if we are retrieving
data from from our relational database
and we are using an native Python driver
the way that Python handle the driver
will will handle the data to you is that
it will return you a list of topics on
every every cell here it's a new element
here it's a cell of the table okay so in
order to build all this structure you
need to build an object for every cell
hmm and that results in a lot of
overhead so the moon pie way is to
create an instructor at array here and
there you can put the results all or
your selections slight in memory so as a
result you have faster kitchen time
because it is much faster to create one
one single object that creates many
objects you don't have memory
fragmentation which is essential when
you want to optimize your directors 22
data and in this way you have only one
data light normally for one little bite
in your data means only one one bite in
memory okay well here we have several
many many objects and normally for the
one data right
we need several bytes in memory okay so
this is really interesting but of course
nothing is perfect and a loan on Pais
were just great for many many use cases
it also comes with its own different
efficiencies the first one is that
unfortunately moon pie is forced it's
important to follow the Python way to
evaluate expressions for example if you
are evaluating I a by B plus C you first
need a temporary okay to to host this
information and then add the sea the sea
array here i will explain that why this
is a problem more in detail you know in
a few minutes and the second thing is
that numpy doesn't have support for
multiprocessors right of all the works
except if you use blast computations and
you linked your non pie with with a
blast that support multiprocessing okay
but in general moonpie dozen dozen have
this multiprocessor support so here is
where a new mex track can help okay
because no max is a Python library which
is geared to deal with complex
expressions so one new mex has its it
comes with a specialized wit or machine
that evaluate expressions okay is it
also where's just-in-time compiler that
can compile these expressions into for
for the for this beautiful machine it
accelerates computations mainly by
making more memory efficient usage as we
will see and it also has support for
extremely easy to use multi-threading as
we will sit which by the way is that
it's active by default so you don't have
to
do anything in order to to access this
capability so for example let's let
suppose just to make an a simple example
that we want to evaluate this polynomial
in the range minus 11 with a step size
of 2 a.m. 10 to minus minus 7 using both
Mumbai and new mex okay so if we do that
and in order to compare appears with
peers and apples with apples which we
should be sure that no max only use one
threat because known by only uses one
thread by default ok so the exercises
must be carried out the exercise must be
carried out with with this ok so let's
enter into i buy some control console
and first of all let's create the x-axis
the points for evaluating
the polynomial now I think sorry I need
to enter in pile up mode
the comma yeah okay and now we just
evaluate the polynomial so this is the
way that can everybody see the text me
Oh
okay so here we can we already done this
computation we use in vision vampire of
the polynomial and we should if we
measure the time that the computational
stakes we can see this one second and 18
milliseconds okay so if we use let's try
with new mex so just import the new mex
library as an E and then
let's do the same computation using new
mex evaluating this expression is as
easy as convert expression into a string
form okay and just let's make sure that
the result is the same than in a moon
pie so let's make use of all clothes
okay so all the results are the same we
have evaluated the same expressions
exactly the same day same thing and let
me try let me show you the time that new
mex takes for doing this oh yeah exactly
we need to to make sure that we are only
using one thread
thank you so here is too because it
returns the number of threats that is
that it was using by default before so
okay so new mex takes only 20 20 seconds
okay and if we remember the time that it
takes to numpy to evaluate the same
thing it is almost six times I'm almost
six times more time that the new mex so
new mex can can evaluate these sort of
things much faster than the moon pie can
okay so that's a nice thing but what
happens if we r s press this polynomial
in a better way this is the factorized
version of the simple polynomial and
they do and we relate their computations
so with that
this is the short version of the
polynomial the factorized version
okay so it takes just half the time than
the original the original expression
mm-hmm can you any kind you can anybody
explain why this is so what do you think
we are using just moon pie here and here
why this expression is it's twice as a
slower and this one less number of
operation that's that's a possibility
but this is not the reason in fact this
operate with with less we did this this
takes legislation operations to to carry
out but this is not the reason another
suggestion oil is most is not the main
reason it's sir it's a reason but it's
not the main one no no these are
possibilities that contributes but it's
not the same envision the main reason is
that in this expression you are using X
power 23 power 23 okay and this is this
is and this is executed calling the sea
Powell a function in in the sea library
okay the poor fog function it is called
transcendental so you cannot wait it
cannot be evaluated in one single clock
as for example a multiplication or Assam
it takes tens of cycles CPU cycles in
order to evaluate that okay so one first
thing that new mex dyess in order to
optimize the evaluation of expressions
well here it is the time we can see this
this expression here the original
version and what one measure that new
mex does
is that it a new mex analyzed in the
flight this expression and can convert I
can expand this whole thing into a
serious or multiplications ok so this
production in this example this
multiplication all it takes only three
o'clock cycles or two to two clock
cycles ok because one multiplication can
be done in one single clock cycle and in
this case moon pie is using is calling
pow that can take tens of more more
clock cycles so this is the first one of
one of the reasons why new mex is
performing much better in this case
however in this case we have we don't
have this expansion and new mex a moon
pie are executing exactly the same thing
ok however new mex can do better much
better than moon pie and in this case
you are right there are some temporaries
things and Tom from temporary objects
that are making this evaluation slower
so let me explain why moon pie is so
slow we're so slow in this case well
noon so new mex is executing this
expression three times faster around
three times faster than moon pie so this
the single that the short answer is that
it can do that because it makes a much
more efficient use of memory resource so
this is the short answer but in order to
splain the things in detail let me tell
you about the CPU starvation problem
that most of our currency peers are
suffering so the thing is that currently
CPUs are typically stay bored ok and
doing nothing most of the time so this
is because they are waiting for data
hmm you may have the fastest cpu in the
world but if your memory subsystem is
not fast enough to fit it they are
staying doing nothing so this is a graph
on how the speed of of memory in red has
been involved we've all been through the
time in nanoseconds okay from 1992 till
2007 okay and in green we see the speed
on how the CPUs the cpu speed has been
evolving so it is evident that the
between speeds are our much are being
drift a lot through the time okay so
this is an with the with the age of
multicolored in blue this this
difference is still growing more the
problem with them is match between video
to memory cache between memory and CPU
is is so apparent noah days that people
it's writing complete books about the
problem okay so this this book appeared
33 years ago and i think the title is is
splitted explicit enough it's the memory
system you can't avoid it you can't
ignore it you can fake it ok so in order
to get the most out of your computers
you need to understand how this memory
system works so this is how things are
now memory latencies are much slower
between 250 times and 500 times is lower
than processors that's a lot ok and
memory bandwidth is improving as a
better rate than memory latency
however it is it is still much slower
than processors and it's between 30
times and 100 times slower so in order
to to cope with this problem what
vendors are the wind that are doing is
to introduce cashes in CPUs ok so the
idea and the cash is the is that the
caches in CPUs has much much less
latency and throughput and are much
better the latency and throughput are
much better than memory main memory
however the faster the cashier on the
smaller they must be this is why this is
because of heat dissipation proud pop a
power dissipation problems ok you cannot
put a lot of memory in sight of a cpu
and make it run very fast because they
dissipate a lot of a lot of heat so this
is the devolution of the CPU cache
through the years ok originally we only
had a CPU memory a mechanical disks ok
this this is this architecture
architecture it's very simple to
understand and it's relatively easy to
to to live to make an efficient use of
memory in the 90s and 2000 we have seen
the vendors we're implementing this kind
of architecture the CPUs did appear a
level one cache and then a second level
cache okay then the main memory and then
the disks and we will be seen in during
this decade is that in CPUs will wear
three levels of cache main memory and
another layer which is solid state disks
that in many many cases will act as a
cache of mechanical risks okay
so we must be aware of this kind of
architectures if we want to get the most
of our architectures of our computers
now and this is not easy so in order to
to actually improve the hour over speed
we need to understand what in which
cases CPU cache caches are effective and
they are effective maybe in a couple of
scenarios for the first one is when
there is time locality so this is when
the basically a data set a small part of
data set is being reused and the second
one is when there is a spatial locality
that means that the data set is access
sequentially okay so for the time
locality realize that you have an
Assyrian memory here and you are
accessing these four elements repeatedly
through the time okay so the first time
the CPU needs these elements the system
has to fetch the elements from the
memory but the second time the elements
has been transmitted to the cash and the
CPU only needs to fetch them from the
cash okay so this is the time locality
and the spatial locality is when a data
set is accessed sequentially ok so in
modern systems when you have for example
an array there are the directions where
you can retrieve data much more faster
than the other direction and the
direction that you can retrieve data
faster is when the data is contiguous
okay this is because of how memory
subsystems are designed so in this case
for example when the cpu needs to fetch
this row here the first time that it
takes it
this element the the continuous elements
are being transferred automatically to
the cash so the second time we will get
the elements from the guys not from the
from memory and also modern modern
systems has a special hardware to detect
patter access patterns so if the CPU is
detect the the text that you are you
will be accessing elements in this
direction he will anticipate and
download or prefetch this these elements
here into the cache so the CPU will
retrieve the elements from the front
deck guys ok know from memory of course
this doesn't work if you are accessing
elements in this direction for example
this is a bad this is not a good way to
travel data or just in random in random
order ok so this is are the two ways
time locality and space locality and
there is a simple technique that ensures
you that you can access you can make an
efficient use of your caches that is
called the blocking technique the
blocking technique is very easy to
understand it's just for example if you
have two data sets in memory and you
want to do an operation with the data
sets the blocking technique says well
just transfer blocks contiguous blocks
okay into your data sets transfer them
to the caches do all the palatians in
cash because it is faster and we when
you have the result copy the result to
the data set in memory after them okay
and while these blocks are in error in
cash they use them as much as you can in
order to make advantage of the time
locality
okay so this is basically this technique
is basically what new mex does ok so
this this splines explains why no max is
still faster in this situation so for
example for numpy we were if we have to
complete this expression here at a by b
plus c new moon pie we'll need to first
compute this multiplication and we'll
need to create this temporary in memory
completely memory ok and then use this
temporary and soon some to this one to
create the final result in memory so the
thing is that numpy needs to put much
more memory this temporaries are going
travelling to memory and this takes a
lot of time in modern systems ok on his
hand what new mex does is to apply the
blocking technique ok it transfers
blocks small blocks of the operands to
the cash it does all the operations in
cash of course with temporaries with
everything whether temporaries also fits
in cache that's important and then put
the result on the final destination ok
so you completely avoid the creation of
big temporaries in memory which is low
in down many numbe operations nolvadex
it's that clear ok so you don't need to
use new mex win order to to use the
blocking technique for example this this
is nip that has been contributed by
Stefan it's using also the blocking
technique for evaluating the the
polynomial
but using pure numpy okay it only uses
it makes a loop and it fetches blocks
blocks from the from the X and they then
do all the operations in place okay so
this is how to do the same thing in
numpy and you can see that if we
represent the block size here in terms
of the execution time we can see here
that we have a minimum time here okay
what do you think this minimum
represents the size of the cache okay so
when the block size fits in the in cash
everything good runs fast whenever the
block size rounds off out of the cash
your time to take the computations
increase because of memory bandwidth not
not because of we are doing more
computations okay the thing is that many
times implementing this this blocking in
in moon pie is quite inconvenient okay a
new mex can do that for you
automatically and the second the second
reason for not using this is that new
mex numpy also have some overhead from
fetching views and doing all this kind
of of things in Python space new mex is
doing the same thing in CA space so it's
running much faster yeah this is because
of the in the beginning there is
overhead so when the when the block size
is very small this loop is much is much
longer okay so you have to do much more
iterations and there is an overhead of
creating views in Mumbai okay so this
this part
is just a construct a consequence of
this overhead here there is a good
balance between the overhead of non pile
and the cat and the block sizes and here
you have block sizes that exceeds the
CPU vcpu size the CPU questions yep okay
well another interesting think of new
mex is that I don't know which time is
it okay is that it can make use of
multiple multi-course automatically
that's really interesting thing and you
can do that automatically and you can
even select dynamically the number of
threads that you want to run in order to
evaluate your expressions so let's make
a an exercise let's put the number of
threads for evaluating this expression
22 which is the number of course that I
have in the my laptop and just redo the
computation by using new mex
okay so by using two threads let's
switch to one just to see the difference
by using one thread the time is 17
cynthy seconds and by using two threads
it's 11 ok so the it's very easy to use
so it's just a matter to select the
number of threads that you want to run
by default this number is the number of
processors that you have in your system
yes yes it detects automatically the
number of course that you have a new
processor and its use all of them but
you can you can select the number you
want automatically ok so let's have a
look at a machine that has 16 cores and
just to see how the facc scales in blue
there is the non optimized version of
the of the polynomial in red we see the
doc demise version of the polynomial and
in yellow this is an interesting thing
there is also there is only the
polynomial is just an X that means that
we are doing just a copy operation we
are not computing anything just we are
doing a copy of vector X to a vector I
why ok so we can see that the the
performance the despot app is quite it's
quite good because it can reduce it can
be around four times faster to run on
all the 16 CPUs ok and for the optimized
version of the polynomial see the times
are better but when we have many many
CPUs the difference of time is not that
much that means that for
max you don't really mind too much if
you use an optimized version of the poly
all of your expressions or not it new
maps is normally able to manage your
complexity for you ok and an interesting
thing is that when when you're doing not
operations at all new max is able to
reach almost the performance then if you
are not doing operations at all that
that is because in this case the
bottleneck is memory access is not CPU
ok so new mex allows you to get almost
perfect or allows you to reach
performance that is limited by by CP by
memory bandwidth operations this is a
comparison of new mex with a very very
well optimized see version of the same
thing by using openmp openmp is a
specification that allows is that allows
you to annotate your C code in order to
produce parallel code automatically and
it is supported by most of the of the
modern compilers ok let's have a look at
how this works
so this is the C version of the level of
the polynomial evaluation this is the
interesting thing this is the loop where
the polynomial is it's been computed and
OpenMP is just as easy using opening
pitches as easy as adding this
annotation here with the pragma OMP
parallel and you declare that the what
the day I they index this loop can be
paralyzed okay and you can also specify
the schedule if they are the the openmp
will do an anesthetic partition of your
job or will do and in a dynamic
partition in this case we have used a
static partition because it is the most
efficient one so this is how the openmp
works and we can see here so this is the
fastest performance that you can get
from the system you can see that new mex
can it's around only twice as a slower
than the best way to compute this this
polynomial okay this is very interesting
thing because you know that new mex is
interpreted and this code is compiled go
so without any effort or very little
effort new mex can't help to you to
accelerate your computations a
comparison with a version of CEO of
OpenMP and see which is doing only a
memory copy okay and we can see that the
performance is very similar than to
evaluate in the polynomial this is
because this problem is
basically memory bandwidth limited
bounded for problems that that are not
memory bandwidth bound it for example
competing transcendental functions like
the sine or cosine the scalability of
new mex each is much better okay because
you don't have this bandwidth limitation
you are using your course much more
effectively mm-hmm but they still yes
and you can see here also an interesting
thing is that this this machine doesn't
have 16 physical course it only has
eight physical course and the rest of
the course are logical where this
machine is using intel hyper threading
ok so these logical cores here are this
high portrayed hyper threaded ones but
we we can see that after da the speed
that hyper threading can give us is
around two thirty percent can give as a
thirty percent better speed so hyper
threading is not only hype it works ok
thirty percent is not is not that bad ok
so but new mex also has limitations and
for example new mex one of the
limitations is that new mex only
implements element-wise operations ok so
we can you can only deal with
expressions that where you can do as
operations element by element but using
always the same index between the
vectors ok all matrices in particular
you cannot do this sort of things by
using New Mexico for example we are
using and index a different index here
than in in the same in the same
iteration of the loop we are using index
I index I but index i minus 1 so this
kind of these sort of things cannot be
done with new mex / okay so this is
where scythe on can't help okay and this
is the second part of the tutorial and
siphon is a language that is very
similar in syntax with to python that we
will see it generates the extensions so
it is compiled it is not interpreted but
and the most important capability of
sight on is that it is very easy to
access both see and Python libraries
from from it and it's very useful for
blue in C C++ libraries with cipher in
addition in it integrates very well with
moon pie as we will see to and also has
support for multi course right on the
end on the on the language this
capability has been implemented in the
past August it's really important and we
are going to exercise that too so you
want to go to the exercises exercises
site from poly directory you will see
an example of performing
you
ok I think I have Roland
can you see the code yeah so this is a
sample an example of sight on code the
syntax is very similar to to python okay
you only have different things for
example this team poor thing doesn't
doesn't exist in in Python but that
means that this moon pie library has to
be imported in order in CS pace in order
to get access to the c kappa to the capi
of of mumbai then another difference is
that this CDF function means that it can
be only called from CS space in that
case from size and space not not from
Python space okay and you can you can
also type the resort com of your of your
functions and also the types of the
parameters so it's a kind of a mix of
oxy and with but with the syntax syntax
of cipher in that case we evaluate the
polynomial here and
another function which is called poly
that takes a couple of parameters here
this is a function it is called it is
def is not see def that means that it
this function can be called tracked from
Python directly ok and this don't do
these two parameters the types of these
these two parameters are pure ND erase
that means that from Python you can pass
the these these arguments as Mumbai
arrays and you can manipulate here the
arrays much more efficiently by
declaring that they are in the arrays ok
so let me show you how this is called
from from Python space from personal
space is just a matter of imparting the
poly which is the extension that has
been generated by the site on sources
that we have seen we generate this
vector X vector and we generate a con
tie an empty container for the for the
for the outcome of the pollen of the
polynomial computation ok and then the
only thing we need to do is to call the
poly function that is this part of the
poly-si extension and we pass the x
which is a normally an umpire ray under
why it is another moon pie all right
that's all ok this is the way you can
you can access to the sea extension that
is created by by this code ok so let's
run it and let's see how you can
accelerate this sort of computations so
let me show you we very very briefly how
to compile how to
creator set up dot x dot py in order to
create the extension so the only thing
is to import the DS two tails and then
declare that we are going to build an
extension okay the source code is here
Polly dot P PI Y X and we you can pass
different compiler comments this is for
the parallel version will be seen soon
and then add in some meta information to
to the to the packets that's very easy
to create this and then to create
extension is just a matter of using the
disappears package in order to create
set up the build extension in place this
flag means that we want the extension to
be in the directory in the working
directory ok citin has created a see
file out of the of the source file and
then it pass it compiles the result with
GCC and it creates a that a library here
I as the extension so if we run the
result using the script we can see the
result here ok so siphon allows to run
this in 40s 45 20 seconds which
curiously is much it's much much more
time than it takes a new mex so
something is has been going wrong
because in general set with seitan you
can get much more
speed with new mex so let's have a look
at how we can optimize our oversized
phone code for this site and provides a
nice tool which is a way to annotate
your code so if you use siphon menos
minus a you have this will produce an
estimate tml file which is the notated
version of this one let's open this
annotated version in our browser okay
it's here let me reload this
okay so in the know today's version the
yellow the the line is the darker the
yellow is the most operations need to
complete okay so we must focus on very
dark yellow lines in order to optimize
our code okay for example here we have a
will click on on this line you can see
the output that site on is is creating
for this line and this output is insane
okay so we have a lot of things here but
we cannot do very much about optimizing
this okay but let's focus on this this
yellow lines here
okay this is a simple assignment here
x.x i equals the I element of the X
vector okay and it is creating a lot of
overhead Python of a Python overhead
call in a pipe flow from double
conversions so this this thing is being
executed mostly in Python space because
it is calling siphon a Python API python
functions so here's the same thing and
here too so how we can optimize that
well let's have a look
another version which is called Ollie
static and let's see how it differs from
our current version
ok so this changes are not important the
important changes are here ok this this
version adds a static typing of several
variables in our loops that's really
critical if you want to get the most
performance in our site on files so
let's copy this file here
and let's have a look
No
okay so by declaring this variable here
i vill n + XY x I and why a we can see
that we are we are specifying to the to
site on that these variables are long
and are doubles okay so sighs on with
this information can create much more
optimized see code let's have a look at
the new code that site on generates we
create the notated version
this is the duel Bastion and with if I
do reload we can see how these two lines
here that has been represented in yellow
now are turned to white okay that means
that did this blinds doesn't expand to
python code it only spends to see code
okay so they are much more efficient as
these lines are in the middle of a loop
the boost can be no can be important so
let's compile again with a new version
and now run it
so for I think it was 45 before just by
by doing this this static static type in
the time has decreased by a factor of
three which is really really really
interesting okay so and in fact this
number is very close to what new mex is
doing in fact it is it is bad it is
better this this time then new mex using
only one cpu okay so let's go with how
to paralyze this disco'd with seitan to
doing this insight on is also quite easy
so this is the the parallel version of
the of the same code and we are importer
a special feature which is called p
range the range is the standard way to
paralyze loops inside zone okay so and
here we have we we have added another
flag to this to this function which is
called no Gil how many of you know knows
about Gil what Gil is okay most of you
for those did you don't know Gil means
stands for global interpreter lock okay
so it's a well some some call it feature
some call it back but it's a it's a lock
that is implemented in in in python that
prevents from several threats to run in
parallel okay so this is a common
limitation of python applications that
they have to fight with the gill in
general so the way thats iphone has to
fight with jill is with the gill is that
he can release the guilt the guilt can
be released when when calculations are
that are being done in CS space not in
python space so siphon can generate sigh
phone call is tickled so this is one of
the reasons site on can't get rid of the
deal so this is a way to specify hey my
function will not need the gill so
please a generate code that doesn't
depend on the gill but here it's just a
declarative thing where the hill is
actually released is here so this works
before for I in range ok and now used by
using P range we have to make sure that
we put this no Gil parameter to true so
we are telling it we're telling to site
on here that we want to release the gill
when we are performing
the loops in the in this computation
okay and also you can select the
scheduled to be dynamic or static
because this implementation of
parallelism insight on is based on open
MP as we have seen before okay so let's
make use of this new version of the
parallel let's copy this
and execute compile okay and then run
this way this this this time we will run
in the parallel version of of the of the
of the polynomial of the computation and
we can see that where my laptop only has
two two CPUs but we were able to see I
would speed up from 12 to nine twenty
seconds which is really nice so here we
have a a plot where we can see the speed
apps that can be achieved by using the
parallel version of our code by using
seven okay curiously enough it is a bump
that when using hyper threading it seems
that that the performance degrades I
don't know exactly this is why this is
but well this is the results that I get
but in general this this performance
seems better well this is a comparison
between i don't know if i have time to
explain it yeah maybe ok so we also have
seen that site on provides a way to
distribute the job differently
through the job among the different CPUs
okay by default it uses I think in fact
the openmp doesn't specify what the
default is but most of the platforms use
the static partition of the jobs but you
can also specify a dynamic partition of
the jobs the nemi partition means that
instead of splitting your your data
domain in even partitions the the
dynamic thing means that it splits only
it only splits only uses a few partition
first send the partitions to the to the
CPUs get the results back and whenever
when a quarter has finished with his job
the Master will send another job to the
CPU okay so it has to wait until the job
has done and the static partition means
that it everything is has the same size
the your range domain and it is sent to
every to every CPU one after the other
okay so this has more more overhead
because it implies some handshake but
and we can see here that for the
polynomial computations the static the
static partition is works much more
efficiently than using dynamic one okay
but it is this is not always this the
the case because for example for doing a
computation of the Mandelbrot set as
many of you will know competing this
black pixels here takes much more time
that computing the the blue ones okay so
if we apply the different schedule
schedulers to this problem we can see
that the dynamic a dynamic partition
works scales much better
than the static one ok this is this is
because the time that it takes to
compute the pixels is not always the
same so a dynamic partition in general
it is bet is it's better you can see the
C code for this for this evaluation also
in ind exercises turbo ok so to conclude
let's see how the parallel
implementation of sight on can compare
with the pure see OpenMP version ok so
the parallel version of sight on is in
blue and see openmp seen is in red ok so
for one processor site on has some
overhead but this overhead reduces us as
we are using more processors as we are
throwing more processors into the into
the computation ok and in fact 48 the
time is more or less the same even 447
the tiny is exactly the same then using
the peel openmp and see version so that
means that we size on you can actually
get all the performance from the order
from your applications ok that's really
interesting and we just refer for
curiosity the opening p version of doing
the copy doing this one the performance
is exactly the same than then using what
we already have seen this this result
okay that evaluating the polynomial is
the same thing it takes the same time
than just copying the legs the extra
vector to why
this is the same exactly the same plot
but instead of representing times in
this in this axis I have represented
memory bandwidth okay so this really
interesting what because we can we see
that when we reach a number of CPUs we
we are mainly limited by by the memory
bandwidth of this machine the memory
bandwidth of this machine is around 16
gigabytes per second okay so we can see
that are adding more cpus is not going
to speed up our our computations because
this this or this particular problem
this machine is already delivering all
the bandwidth is it can provide and yeah
that's interesting oh yeah I anything
exactly you have here in in X there is
10 million elements which are double
precision and here you have also 10
million so I some the both and I get the
work set size and just a divided by the
time
here is it is a fair final comparison
among new mex in blue sites on parallel
in red and see OpenMP in yellow okay new
next is the lowest consistently the
slowest that but the vantage of new mex
far as i said before is that it is
interpreted okay so you don't need the
compiler step in order to to get these
speeds then when you want absolutely
want the highest speed the best thing to
use is second parallel because you can
reach more or less the same the same
speed then p that using piercy opening
pi application especially if you use a
sensible number of cpus for example when
you starting to use more than C chord
the four cores this in this example you
are you are reaching more or less the
same speed then then appear see
computation okay maybe it's a good idea
for example here we can see that with
with 10 threads we are getting worse
worse results that using using for
example seven threads so yes it put in
more course to your application is not
always a good idea so it's always nice
to do some bass marks because maybe
there is an optimal number of course
that can execute your code more
efficiently so it's always a nice thing
to to do some benchmarks and explore
which is your ideal number of course for
your computations yep good question the
reason is when you put you know most of
mother computers are memory bandwidth
limited when you put a lot of course
trying to access memory at the same time
you are pushing a lot of pressure on
your memory system so memory system okay
so in many
in many use cases it is best it is
better to reduce the number of course
that ad accessing are made I doing
pleasure on your memory subsystem okay
and this is why you you could get this
these sort of things another important
thing that you may you may want to see
here is that this is just a copy okay i
I don't know exactly how this is this is
implemented in hardware at hardware
level but one interesting thing is that
a single copy can be more efficient if
you use more cpus this is a striking
result because you don't expect
initially mm cpy going faster if you if
you use more smokers than done one in
principle use pet what you SPECT is to
reach the highest performance by using
one min cpy call okay you spec that but
the truth the reality is that I
consistently get this kind of speed apps
in in copy a plea in copy operations
when you use when I use several CPUs I
don't know this planation of this if
anyone can bring some light here it will
be grateful you understand the problem I
don't know anyway and of course I in my
opinion this dependency of the number of
course in doing the copy is the most
important reason why we see SP tabs when
doing memory bandwidth bounded
operations these operations is basically
memory bandwidth bounded and this this
kind of operations is are not in as well
the usual operations nowadays are all
all memory bank
if okay mobile bandwidth limited okay so
that's all that I wanted to say just
remind that moon pie is the factor
container for for Python in the big d
tire area either and that doom Pike has
paid for my problems when evaluated come
when evaluating complex expressions and
that new mex and siphon can help to
solve this new mex and thigh and siphon
has different different areas of
application but in in some areas they
can they can they can I don't know
overlap yes thank you so it's up to you
which one you want to use they are both
very interesting tools to to use okay
see if you are more interested in about
the CPU starving problem i frightened a
an article about this why modern CPUs
are starving and what can be done about
it and for knowing more about new mex
there is a nice overview on the website
and using parallelism we siphon it's
very well explained also here
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>