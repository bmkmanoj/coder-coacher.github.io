<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalability and Performance - Scaling an App Agile Style | Coder Coacher - Coaching Coders</title><meta content="Scalability and Performance - Scaling an App Agile Style - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalability and Performance - Scaling an App Agile Style</b></h2><h5 class="post__date">2011-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J20Ff3CatLE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">great so I'm David Stevenson who the
heck is that I'm a guy from pivotal labs
I know Jonathan Segal and I used to work
together down in Santa Barbara and so
I'm here I've been given this talk
several times but I'm getting much
better at it each time I'm gonna focus a
little bit on scalability because I'm
understanding is that you've been
getting a lot of talks on performance
and you probably getting a little bit
bored about performance and scalability
although I think we'll probably all
agree they're linked so I'm gonna try to
focus on on the scalability side of
things and also because I'm from pivotal
you know we're pretty extreme about our
agile process and I'm gonna talk to you
about how how does that fit in like how
do you do performance optimization and
how do you do scalability while trying
to be agile at least as agile as
possible so I'll give you my take on
that and and then maybe we can start a
discussion cool so there's a lot of
approaches to optimization and I think
like traditionally engineers computer
scientists capital a architects we enjoy
like a performance optimization it's
really fun so like we like to do it a
lot and as early as possible that's not
where I like to be but I I like to be
kind of here closer and more on the like
let's optimize as late as possible and
maybe like there's an extreme approach
to agile where are you really on the far
end of this spectrum like this is
proactive optimization and then there's
reactive optimization I like to be
mostly reactive because I think I find
that a lot of people spend a huge amount
of time optimizing problems that they
don't have they like to focus on
optimization because it's fun and
because we're computer scientists we
enjoy solving these hard problems but it
is not necessarily good for the for the
customer and it's not good for our
business before I worked at pivotal I
was at a startup that actually went
under and during that time we spent a
lot of time on optimization like I think
we spent two months rewriting the
of our application we ripped out all of
the Ruby logic and replace it with
stored procedures and that was about the
time we started having to lay people off
so I've always seen this correlation in
my mind between like spending massive
amounts of time on optimization and you
know running out of money and I think
those are often related so I'd like to
be as reactive as possible without being
so naive that I'm obviously making a
mistake and I know that's a little fuzzy
but maybe we'll look at some examples
and try to figure out exactly what that
means so the process that I use for
optimizing is similar to the process I
use for writing software and it's like I
build some things and then I measure
them and then I optimize them and maybe
I don't even optimize them
I only optimize them if there's a
problem in the same way I only refactor
my code if it needs refactoring I don't
I don't optimize preemptively and
because I've sort of built some stuff
and then measured it I know whether I
need to optimize it and I'm not doing it
in advance and there are a lot of
downsides to optimizing early and I
don't think I have to sell you on that
but beautiful code is usually not
performance that's the problem so when
you decide to optimize and you know you
end up with this code that is much
harder to write and it's much harder to
read which has obvious downsides to
everybody on your team you know if I
come into an application that has been
heavily optimized it's really hard to
understand the business rules the
business rules have usually been
abstracted or have been you know pushed
aside in exchange for these
optimizations which really is bad for a
couple of reasons like obviously it's
hard to learn code that's that's not
beautiful and not easy to read and it's
hard to change anything at that point
like if you optimize it like when we
converted say our Ruby logic to stored
procedures changing those stored
procedures was not a lot of fun and you
know we ran out of business before we
actually had a chance to change them but
I imagine that changing them was going
to be a lot harder than than anything
else so you can get yourself into
trouble if you optimize early I'd like
to think about it as a plan like for
scalability especially like I have a
plan to scale every component of my
application but I'm not going to execute
that plan right away and really I
oftentimes never execute the plan I have
all these grand plans for scaling and
they're a lot of fun to come up with
but that's as far as I usually go
because most of the time we don't need
to execute that plan and sort of like
the best plan is a plan that you didn't
have to execute because you didn't waste
any time doing it I sort of have a great
job at pivotal though because I get to
do a lot of performance optimization I
kind of go around from client to client
doing these really hard scalability and
performance problems so I kind of get to
like optimize all the time but when I
come in I know it's because people are
needing the optimization because they're
actually having problems as opposed to
speculatively you know I'm a CEO so
there's probably going to be like
millions of people using my site even
though I don't have any users so they're
probably like millions of people so you
guys should worry about performance
that's not the circumstance I find
myself in and I think they're sort of
like people will get really worried that
it's gonna be too late they're like well
if I don't do it now what happens when
some really big event happens and I have
a couple things to say about that like
like and I know this is wordy and my
talks a little wordy but it at pivotal
the project product manager they dictate
the backlog and they know what's gonna
happen most of the time like they're
knowing about big demos and press
coverage and such and they they know
maybe approximately how many people are
going to hit the site so if that's a
requirement obviously don't that you
know don't be naive don't not scale and
don't you know not optimize until it
becomes a problem if you know you're
gonna have X number of people like if
you know that your product is gonna be
announced on Oprah's show you should
probably do some work in advance you
should say well how many people are
gonna be a watching Oprah and how what
percentage of them are going to hit my
site okay well now I can write a test I
can write a load test I can verify that
my site performs and so the product
manager is going to kind of be aware of
that and and I think most of you
probably know where your site is going
to become viral so the other possibility
right is that maybe the site explodes
unintentionally or in a way you don't
you know understand what's gonna happen
it wasn't because of press coverage is
because you know you told him that other
guy and now a bunch of people are using
my site well I think most of you know if
your site's going to become viral like
what part of the site like where your
loops are within the site so all you
have to do is optimize those loops a
little bit and think about
you benchmarking those loops and say
well could we handle like 10 times as
many people as we're here right now well
that seems pretty good even twice as
many people are as here right now is
probably not going to happen overnight
and so you know because we're measuring
these things we know we kind of know
about like what we can handle and what
we can't handle so there's no surprises
and and again like the test suite thing
right so in pivot so we write tests the
test Suites allow us to actually change
implementation we just throw out like an
implementation and rewrite a new
implementation and it's great because we
guarantee that the new implementation
does what the old implementation does so
it kind of removes a lot of the barriers
to optimizing that usually people say
well if I opt amaizing so we're gonna
have to like do extensive QA after I do
optimization and the answer to that is
no you don't you just have to have tests
that verify your application is working
and then you can throw out the whole
application and write another one and
you won't have to spend a lot of time
verifying that it works so talking about
scalability man like I'm a scalability
falls into several categories right your
application every component your
application scales in one way or another
like linear scaling is my favorite
because it means that if I add more
hardware I'm gonna get more throughput
or more performance through my
application there's other scalability
plans that are that are a little bit
more suboptimal
right logarithmic is interesting you
know diminishing returns the more the
more Hardware I add it will give me a
little bit more performance but then I
add even more hardware and maybe a
little bit less and you start
diminishing returns these are these are
kind of dangerous scenarios to follow
yourself into and sort of like the worst
right is the flatline like this is it if
I add more hardware we cannot get any
more performance or any more throughput
out of this application component so you
want to know for every component your
application like what's your scalability
plan like you expect it to scale
linearly do you expect it to scale not
at all and you don't really want to be
in this in this final category we have
no idea like how this component is going
to scale we found ourselves in this
situation with like an e jabber D that
we didn't understand that we were
brought into a project that was running
a jabber D and
none of us really knew how this was
supposed to work and so we got in
trouble because we didn't really
understand the scaling plan for each a
birdie and we didn't know it could we
add more hardware or whatnot and so you
don't want to find yourself in this
scenario you know for every piece of
technology that's in your site you want
to know what your scaling plan is so
great so that's some of the
philosophical like when do we measure
when when and why do we optimize
measuring is really important right if
we're if we're the applications gonna
scale and it's going to be performant
and it's gonna handle load like we've
got to know how much load we can handle
and I don't know when when scaling or
performance whichever one you're most
interested in becomes a problem and I
guess we should disambiguate right okay
so performance is speed and scaling is
capacity and they're related because if
I make a site that's faster it can
obviously handle more traffic with the
same hardware most of it almost always
right if I can decrease my request time
from 100 milliseconds to 10 milliseconds
I can usually handle 10 times as many
requests but most of the time
scalability is is achieved by by linear
methods by adding more Hardware so you
know we add more rails application
servers we add more load balancers stuff
like that so that's I just wanted to
clear up the relationship between those
two things so assuming that we want to
be able to handle load and we kind of
have some idea for the load we'd like to
handle but we don't want to spend time
optimizing unless we need to
then measuring becomes really important
we've got to know when there's a problem
and then we know whether we should spend
time fixing it or not so like the
simplest form of this is just go into
your production application which is
already running and and make sure you're
running some things right New Relic gave
a talk here make sure you're running
things like that we sometimes use really
simple things like cloud watch is
actually not terrible at just monitoring
like CPU utilization for example and
look at your application and say how
loaded down are we right now and how
loaded down you know have we been
recently if all my machines are spinning
at you know 10% CPU I'm probably doing
okay maybe I got a lot of excess
capacity so that's the simplest form of
monitoring that you'll probably want to
have
all the time and so then you say okay
well I've got monitoring and I think my
application can handle some load but I
kind of know that it needs to handle a
certain amount of load because maybe we
have a big demo coming up and or we have
some press release and I expect some
traffic so oftentimes measuring includes
load testing and sometimes you need to
design a load test and other times you
can just use production data as your
load test or use production traffic some
people just want to know that they can
handle the traffic they have right now
or maybe twice as much traffic as they
have right now and feel good other
people know they're gonna get 10,000
people coming in over the course of an
hour because of a huge event so
sometimes you have to design a load test
and sometimes you're lucky enough to
actually just use existing data for load
testing I know that some people like
replay traffic and there's all these
crazy techniques where you're like split
traffic and double it so that to see
what it would look like or split traffic
and replay it however you want to do it
that's that's one way of generating a
load test right you could use existing
production data and like replay it at a
higher rate or something like that I
kind of like if I'm gonna design a load
test I kind of want it to be like an
r-spec test actually I want it to be
like you know given some very fixed
scenario of some very deterministic set
of data and some very deterministic set
of traffic like how would the site
perform would we be able to handle
10,000 of these users or 20,000 of these
users or just a hundred of these users
so in order for that to happen you need
a data set right and you need load and
so you know generating a data set is
basically a huge pain and some people
like to just use production data for
their data set which is awesome because
it's really easy you already have
production data you just use that it's
non-deterministic a little bit because
your production data changes over time
but it's also a lot of work to do the
opposite right it's a lot of work to
generate a load test and actually
generate a data set I've done this
several times before where will like you
know basically define a seed file with
like load 500 users in the database and
load all their data and oftentimes we
use like Factory Girl for this because
we probably already have it in our test
suite anyway so we know how to make any
object in the system we just make a
bunch of objects put them in the
database and then we generate some load
this is acceptable but I kind of like
using production data more just because
it's a less work but this active record
importer gem is awesome for this by the
way because it allows you to do bulk
insert statements right you know the
rails won't let you do bulk insert
statements so basically inserting like
10,000 rows takes a really long time and
this solves that then you got to decide
what we're gonna use to generate your
load there's some great tools out there
these are not them
Apache benchmark is like really crappy
but I think you probably used it maybe
once or twice in the command-line you're
like how many how would my site perform
if I had like 10 threads hitting this
one URL it's a terrible idea because
your users aren't going to sit one
behind the other like ten at a time so
heb perf is sort of the the rail
standard for a really long time it's got
the advantage of you basically set the
traffic rate you say you know how many
people how many people per second are
gonna hit my home page and ACB Perth
will just launch a thread every X number
of milliseconds hitting the home page
and what happens is when you're doing
badly those threads start to pile up and
you have like you run out of file
descriptors and that's a bad sign that
means that you're putting way too much
load on your machine and you can't
handle it so hv perf was sort of the
gold standard it's really hard to use
and it only does one cookie so if your
session cookies longer than 255 bytes it
gets cut off or you set two cookies only
one of them gets carried from page to
page so it's not like easy to use at all
and it's very limited I've had a lot of
people talk to me about this next one
jmeter it's like you know a graphical
there's a Java based benchmarking tool
it's much more powerful than either of
these other tools and its graphical it
can be scripted which is pretty cool and
my understanding is I've only used it
once or twice but I understanding it's
it's a pretty good way of generating
load it doesn't really simulate a real
browser right none of these things
simulate real browsers you're basically
saying you know hit this URL and then
hit this URL but when you're hitting
that URL you're just making a get
request it's not a browser it doesn't
gently download any assets it doesn't
carry cookies although jmeter might
carry cookies in a session so like what
I think is awesome is browser mob which
is this tool that lets you launch
browsers in the cloud
and it also runs basically HGTV perp so
if you just wanted to run an on command
like a non graphical browser it cost ten
times less and you can launch you know
non-graphical browsers or maybe it's 50
times less you can basically launch a
lot of HTV perfs for the price of of one
selenium driven browser but either way
you write your load tests in JavaScript
which is awesome so you're saying like
you know get this page and then you know
click this link and you know follow type
in this form field and hit enter or
whatever you can see from this graph
this is an example of a browser map test
that we ran as we scale up the number of
transactions happening at some point
this brown line is failures and you know
the site's humming along pretty well and
then we start to get a lot of failures
and at some point like so many things
are failing that like it can't actually
add any new traffic before the script
fails and has to be restarted so you can
see some nice like scaling like when
things really started to go downhill and
then when things really really started
to go downhill and then other stuff out
there when the data became unreliable at
that point because the system was so
loaded down so this is a great tool it's
a little bit expensive I think we've
seen people run tests that cost
thousands of dollars on this if you want
a thousand browsers it's gonna cost you
a lot of money but you know sorry try
starting a thousand browsers it's not
easy cool so now we've decided we've
we've written some code we've measured
the performance and we've decided to
optimize and so I'm assuming that we're
all here because it's a good idea to
optimize and you know we've verified
that it's necessary to optimize and
we're not making that decision in
advance so how do we fix these various
issues like what's how do we optimize
our scaling and performance plans and
I'll talk about that for a little bit to
make sure I'm not running out of time or
anything crazy like that cool excellent
so first thing you should do is put your
site behind a CDN you're basically crazy
to not put your site behind a CDN ever
since cloud front started doing proxy
you can just put your site behind Claude
front and Claude front will proxy every
request to your application server
through it and if you send cache headers
that say cache this page for five
minutes
cloud front will not ask you for five
minutes for that page
and always page caching is better than
fragment caching but people are nervous
about doing page caching and they should
be because it's hard to make a site that
really feels dynamic if everyone's
seeing the same version of the page but
it can be done right I mean we just use
cookies and we use JavaScript and we
render everybody the same page and we
start showing and hiding things that are
interesting so we can't cache everything
but we can cache a lot of pages some of
our biggest traffic pages should
probably be cached like if your home
page is not cashable that's probably a
bad sign like your home page should
probably be cacheable even if it's like
only cashable for one minute maybe you
have some statistic on your home page
that you want to change you know it's
probably okay if people see the same
page for one minute
I mean Akamai our cloud front will only
request that page once a minute
so that's massive you could be getting
thousands of hits and you know you're
only rendering once caching obviously
has problems right sweeping is difficult
if you want to make a really dynamic
site when you know somebody changes
something on one page it expires that
everywhere you know you're likely to
potentially make mistakes and you could
use TTLs on that but then like if I
display the same piece of information on
several pages they'll expire at
different times so users will see
different things so it kind of makes
like a weird experience if you have TTLs
going on and what happens when you
deploy code like I think there was some
rumor at Facebook that if they ever lost
all their memcache servers they would be
unable to recover because their site was
slow enough that it could not rebuild at
all so how do you deal with deployment
do you like expire the entire cache or
do you let people see old versions of
the site and new versions of the site at
the same time depending on which page
you're on so caching is like easy in a
lot of ways and it keeps your code
looking really clean but it has some
major downsides I mean you're always
going to have to eat that cost once and
if I have a thousand or a million pieces
of content that are popular in my site
is caching is really not feasible
because you know I have I can't catch a
million pages I have to hit I have to
get a million hits anyway at least once
to generate each of those pages so
optimizing becomes a much better
solution the biggest way that I
optimizes garbage collection that's like
the first thing I do when I come into a
project if you're not using Ruby 187
Enterprise Edition with the right
garbage collector variables you're
throwing away at least 50%
of your time in terms of optimization
because this like my test suite goes 50%
faster the rails environment loads you
know 50% faster moving 192 if you I
guess you can use it but it has to be
like really really new version of it I'm
not even sure any of the released
versions of 192 support these
environment variables yet but if you
don't know what I'm talking about look
at the Ruby enterprise documentation and
look at the Twitter GC settings that's
the one I always use and I find it you
know in my benchmarks it's unbelievably
different the other thing that I
sometimes use is perf Tools dot RB this
is cool like this is before we installed
Ruby Enterprise Edition 187 we're
spending 44 percent of our page time on
unlike a garbage collection so it gives
you a great little graph of what's going
on we also notice that we're spending
11% of our time doing active record base
inspect so this caught like a bug in
some plugin called declarative
authorization that we never would have
found because we just couldn't figure
out where all the time was going but
that big box told us exactly what you
need to know and we ripped out that
implementation and everything was fine
again so again a word on scalability
like most parts of the application are
gonna scale linearly your load balancers
scale linearly just add more load
balancers use round robin DNS app
servers more or less in rails scale
linearly we can probably add a hundred
app servers before this becomes a
problem key value stores like memcache
they scale linearly to memcache just
hashes the keys right and to figure out
which server to talk to you're gonna
have ten servers and you get ten times
the throughput that being said right you
could have a lot of traffic to one
single memcache node if you have a
really hot piece of content so it
doesn't necessarily scale linearly but
it's somewhere between linearly and
logarithmic document stores are sort of
the same way like Mongo Mongo knows
exactly which server it should talk to
to get any piece of content when you
when you do a sharded database now your
actual relational databases are the
biggest bottleneck so you should be
thinking about optimizing your database
access which is probably not news to any
of you but you want fewer queries and
you want faster queries and eventually
you don't want to database but I
hesitate to suggest that solution to
anyone even though it's really awesome
to not have a database databases are
really good at doing a whole bunch of
stuff
so I hate to rip out a database and
replace it with something else so like
you know all the standard things for
optimizing database access like look for
you look for things in in web classes
and group buys and order buys make sure
you have indexes on all those make sure
you have multi column indexes I five
this query at the bottom here make sure
I have an index that spans those columns
in the right order and the order is
always fun and you can look up
multi-column indexes and my sequel and
such range queries are bad generally
like these two range queries where the
latitude and the longitude are being
checked what my sequel can't do nested
range queries basically it has to do
something called an index merge so you
got to watch out a little bit for range
query so you can do one range query
easily but when you nest them you end up
with doing some crazy merging on your
indexes temporary tables are obviously
bad I think people know that but you
know they're created implicitly all the
time so you're gonna want to use tools
to figure out how your database access
is going you're going to want to use
like explain write which is the my
sequel command to analyze a query
there's a whole bunch of these tools out
there rack bug is really cool and that
it like gives you a little bar on the
top of your site to like analyze your
database usage query viewers the same
thing just gives you like a list of all
the queries on the page and which ones
did temporary tables and stuff like that
New Relic will do this in developer mode
which is kind of awesome and the slow
query log also a pretty good source for
figuring out which things are going
really slow so New Relic as I was
mentioning or like he likes a great tool
but for example right here in this trace
it has no idea what happened between
user dot find and avatar dot find it's
like I have no idea where all this time
went and this is because New Relic
doesn't instrument everything this is
where perf Tools comes into play right
you install perf tools and you find out
exactly what happened between those two
times but it's very expensive and you
don't run it on production whereas New
Relic you run on production so it's a
good tool but there are limitations if
you don't instrument everything and you
know if you're talking to some service
the New Relic does an instrument and you
want to know how much time you're
spending talking to that service it's
pretty easy to instrument it they just
don't instrument everything by default
because they have no idea what you're
going to talk to another great thing to
do is prefetching data right so we
obviously want to make as few queries as
possible we don't want to bring back a
hundred rows and then execute one query
for every one of those rows we want to
bring back a hundred rows and all the
data required in a second select or a
third select or maybe a single select
Rios lets you decide when you're when
you have an association like if I want
to bring my friends and their friends I
can do this with includes or I can do
this with eager load it sort of depends
on if I want one select giant select
statement or three simple select
statement you almost always want the
first one do you normalizing your data
is also kind of an awesome solution you
know counter caches are a really common
thing in rails you do have to be a
little bit careful about deadlocks and
I'll discuss that in just a second but
denormalization is the enemy of you know
you're repeating yourself so it sounds
like a bad idea but sometimes you really
need a piece of data a lot closer to you
even if it means if I'm duplicating that
information or like maybe I put maybe
maybe I have a product catalog and I
want to have a whole bunch of
information about a product that
actually comes from like ten different
tables but I put it in solar if I put
all those fields in solar that I care
about I can execute one solar query to
bring back all the data that I need to
render a product catalog and you can do
searching you know on it on it at the
same time because it's solar so I use
solar as like a materialized view which
my sequel doesn't have materialized
views so I used solar for that unbanded
data is terrible so there's this is
something I saw on the hump on the
startup company that was that before
pivotal we had some code that like you
know when when a certain event happened
it iterated in real time over
potentially an unbounded number of
things and then called some method on it
and this brought down a production
database for like fifteen minutes until
we could figure out what's going on what
had happened if you look at this code is
that once the fifth vote was reached
there were thousands of actions and like
the person who caused that cast that
fifth vote was sitting there waiting
well the system was iterating through
all these actions and adding points we
replaced this was a stored procedure so
you can see how far that got us but the
last thing about databases is right
issues
you may not be realizing you're causing
walks but you're causing LOX all the
time
when I open a transaction and I go you
know update some row maybe I changed the
state of my order to like paid or
something like that and then I go talk
to my payment gateway for five minutes
that lotro is locked for five minutes
until the transaction is committed so
nobody else can lock that row which is
fine if it's my order but let's say I
have an inventory count for some product
I don't want to be updating the
inventory count for that product and
then going and talking to a payment
gateway for even three seconds because
that means nobody else can buy that
product for three seconds so you end up
like you know with some hot product you
might end up with you know ten or
fifteen threads waiting in line for me
to give up my lock so that they can go
lock it for three seconds each and
that's how you fill up all your database
connections and your site grinds to a
halt
you can also explicitly lock things with
select for update but most people
realize they're locking rows when they
do that so the key really is like use
after commit or do something send your
emails outside of transactions anything
that could take a long time keep that
out of the transaction because you're
locking rows you're almost definitely
locking rows unless you're doing
straight insert statements you're
locking rows inside of transactions and
the last thing that can happen is you
can get dead locks
most of the time these are really hard
to figure out what's going on but it's
almost always that like people are
grabbing rows and they're grabbing
multiple rows in different orders so if
I like lock my user and then your user
and you lock your user and then my user
and we both try to do it at the same
time we'll each get the first user and
not the second so if let's say there's
let's say I need to actually lock five
different records often sorting by ID
and then lock them that way so that at
least like there's some deterministic
order if we all try to lock the same
records one of us will get the first one
and then nobody else can continue until
I mean you can imagine scenarios where
there's a multi-way deadlock still DHH
have this deadlock retry plugin for
rails a long time ago it's I guess it's
an okay idea to retry a transaction if
it dead locks because maybe this time I
will be able to get those rows I'm not
sure that's a really good idea because I
think that might end up involving you
going to your payment gateway multiple
times which could be really bad it sort
of depends on what's in the transaction
and maybe the last technique that we
sometimes find ourselves using it
pivotal is
like and this is the hot right
everybody wants to render their views in
JavaScript like we'll send down JSON and
we'll just render our views in
JavaScript and we'll render like this
really simple static page then the
static page will maybe make some
requests to get some JSON and then
render the view which is really cool I
think it's a little bit it's a lot of
work right it's a lot of work to do this
and it's kind of hard to test but with
like jasmine or any of the JavaScript
testing frameworks you can actually you
can actually build a pretty good site
mostly in JavaScript I mean Twitter's
built their new site entirely in
JavaScript Facebook is basically a giant
JavaScript app so this obviously can be
done it's just harder and I think you
you don't want to go this route until
you have enough load to justify this
route most of the time you can render
these view server-side and everything's
fine okay that was pretty fast but I
think I'm about I'm about done I've got
a couple minutes for question answers
anybody have anything they want to bring
up so the question is how do you deal
with asynchronous processes like what
I'm sort of interpreting that as when I
would not do it right away
like I always write something
synchronously before I write it
asynchronously because it's so much
easier right and it's really easy to
later you know you put your logic in a
method and you can synchronously call
that method or later you can make it
asynchronous with rescue or any of these
queueing systems so I I would say why
build something asynchronously when you
could build it synchronously I guess the
the caveat to that like don't be naive
what if it did an unbounded number of
actions like sending an email maybe
isn't really unbounded so I but if it
did like an unbounded number of things
potentially that would be a good time to
make it asynchronous from the get-go
yeah so just to repeat for it for the
the camera yeah I I think you're
completely right you're saying that the
JavaScript is a very scalable platform
because you get to harness all these
people's browsers we have this
conversation just today on a project we
had like a bunch of Facebook users
potentially are visiting a site and we
need to get their social graphs and pass
it through to another third party and we
said we could do that on the back end or
we can make their browsers all download
their own social graphs and send it to a
third party without us so that way we
don't have to worry about any of this
stuff so it never hits our site so yeah
we're like there are plenty of cases
when you want to leverage people's
browsers is a giant compute forum
and Ajax makes that all possible or JSON
P if it's across domain stuff like that
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>