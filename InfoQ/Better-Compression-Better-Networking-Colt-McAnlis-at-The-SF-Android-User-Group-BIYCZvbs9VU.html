<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Better Compression = Better Networking - Colt McAnlis at The SF Android User Group | Coder Coacher - Coaching Coders</title><meta content="Better Compression = Better Networking - Colt McAnlis at The SF Android User Group - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Better Compression = Better Networking - Colt McAnlis at The SF Android User Group</b></h2><h5 class="post__date">2015-08-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BIYCZvbs9VU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey hello Oh real quick note so fun
stuff happening I work for Google and
raise your hand if you've seen Android
performance patterns videos not too bad
about half nice nice nice basically my
job at Google now is to tell all of you
how horrible you're writing your
applications and basically try to shame
you into stopping that really that's it
i'm reviewing probably about a hundred
to two hundred applications that come
through the Play Store a day for
performance evaluation and whatnot and
y'all got some problems uh yeah so more
on that a little bit later to fremont
thanks JJ everybody look at JJ shame sir
anyhow what is this bald man doing
already all right so we are going to
talk today about compression uh raise
your hand if you've written huffman
algorithm okay arithmetic compressor
hands up okay bwt transform Oh couple of
the back nice nice nice uh lz77
compressor 78 LZ SSL ZW l ZF RL DW all
right this is that's a gift man right
there Jeff Jeff gift I don't know okay
I'm serious it's gonna get better the
more beer you have all right so today
we're going to talk about compression
with respect to networking why because
it's probably one of the most important
things that you're not doing right now I
mean we'd like to spend a lot of time in
the networking landscape talking about
batching and prefetching and latency
overloading and how to design for high
latency environments and all its other
crazy stuff but we forget this
fundamental property that your stuff is
too big on the wire because y'all don't
understand how compression works at its
core so let's get started on this first
off you have to understand that assets
that you're sending to your users are
bloated I again I'm doing the analysis I
can see what you're doing right I know
what's going on and this is harmful for
three reasons the first one is not
everybody's on 4G right there's a lot of
people out there that most of the world
out there is on a 2g network okay and so
the longer of course the slower the
connection the longer it takes to
transfer the data right and that
actually goes into the second part of
this which is that the longer it takes
to transfer the data the long
the cellular radio is active in your
Android device communicating to that
Network thus draining battery think
about that that's a double whammy right
so you get someone who's on a lower
speed network and you're basically
screwing them over more right because
your assets are too big right and I
think about that i got the 4g guys get
to run around with you know high battery
all day and if it's 2g connection get
out that's bad of course this leaves us
to number three which is probably the
most haunting haunting part of this
whole process is that like only in like
four percent of the world are unlimited
data plans so think about that for a
second right the rest of the world is a
pay per bit policy for their cellular
networks which means every bit your
application is transferring to their
device they are paying for right now
would it be amazing if one day they woke
up in a Google Play Store told them how
many bits yet they were paying for from
your app don't make me do it so we'll
hopefully that's scaring a lot of you
right i mean all of you here if you're
not on a mobile anyone from Europe
anyone on a there you go right how you
guys have had a fort like horrible
roaming charges there was a great web
page that came out probably bout two
years ago they were showing like the
cost to load like microsoft com was like
four dollars for someone on a regular
data plan in Europe just to load a
webpage because of how large it was in
the cost to load bits your applications
force your users to pay for something
when you transfer data if it's not time
it's battery if it's not battery it's
money right bad compression makes your
users unhappy I'm gonna tell him one day
fix it before then deal deal okay yeah
drink more all right so we're gonna
we're gonna fix that right so we're you
all are not gonna have bloated assets
anymore you're gonna do the right thing
you're gonna be the smart thing you're
not gonna piss off your users you're not
gonna piss off the bald guy the one bald
guy in the room really weird
statistically speaking nursery at least
be like two more in here so you know I
don't know if this bison today filter
that down in the lobby okay all right so
most of you are introduced to the cons
cept of data compression in like
computer science 101 or data structures
class 10 to where you generally see
something called a Huffman tree right
invented by David Huffman back in the
late 60s who was actually a student of
Xiao and fannin and basically the
concept is given some symbol right you
can encode it with a number of variable
bits in order to compress the data and
then that's it that's all you're taught
some of you may go on to master's degree
programs or PhDs and whatnot in you
actually implement gzip right but that's
really in there's a big problem with
that is because there's a huge gap
between that theoretical concept of a
compression data structure and actually
what you're doing day to day in the
trenches building Android applications
and the gap is so large and no one is
covering it right I can give you I can
give you this idea right now no
significant compression technology has
come out since 1990 and that was when
gzip became mainstream think about that
like if gzip were at eat like a human it
could drink now right probably have a
couple duis maybe did some stuff in
college is not proud about right like
we've all been there like think about
that where else in computing technology
has it been so stagnated for 20 years
and the reason is that there's just been
a general loss of focus on these magical
compression algorithms at all levels of
computer science curriculum and so let's
talk a little bit more about that huge
gap yeah sure to click that earlier so
compression basics so entropy this isn't
this is the term you need to know this
was defined by Claude Shannon the father
of information theory back sometime
after World War two when he was allowed
to actually publish all the work he had
done for information theory for the
allies during that time entropy defines
as defined it's a statistical theory
that says the number of yes or no
questions you need to ask to describe a
piece of data in a set who's confused
everybody's nope nobody nobody was
confused about that I had to read that
book like six times before I understood
what the hell he was talking about you
all got on the first time so I'm that
good of a teacher pulse well or lying or
there's more p
happening think of it this way binary
search right finer in schertz everyone's
right the binary search so let's say
we've got a number line from 0 to 15
okay and we want to lurk for the number
10 okay so of course we check the middle
value and we say is it higher or lower
simple yes or no value right is it
higher or lower well in this case it's
higher than the value we checked right
so we get rid of the stuff that's on the
lower end and then we ask the middle
value again is it higher or lower than
this amount of value right simply yes or
no question its lower is it higher or
lower again right higher is it higher or
lower again lower right and of course at
this point we've now found the number 10
interesting enough if you actually
notice these bits that we've logged on
the side it's actually equal to the
binary number of 10 right this isn't
like weird magic voodoo it's the basic
construction of how information theory
exists inside of our modern computing
environment right a binary search over a
closed number range produces the binary
value of that data it's a simple yes or
no equation defined by Shannon way back
in the day and since then there's been
really again no huge significant changes
from this theory you can think of
another way I guess who who's played
guess who there's some 80's kids in here
that's good to know none I'm not the
oldest person in here fantastic guess
who if you haven't played it it's a
really weird game you play as kids right
basically each of you are given like a
row of people and you're get you pick
one of them and you take turns asking
each other yes or no questions like does
he have hair is it a male or a female
right yes or no questions and then based
upon the response you get rid of some of
your values right now the interesting
thing is if you look at the guess who
game my wife won't let me wear that
t-shirt around anymore if you leave the
guess who game as a form of information
theory right if each one of your yes/no
questions exactly eliminated fifty
percent of the options you would be in a
binary information system right yes no
question remove fifty percent of the
question of the potential results right
so fifty percent had hair and fifty
percent didn't not like this room right
that would be a binary system now where
things start to get interesting with the
guess who game is when you get skewing
right so let's say there's only one bald
person in a room that would ever happen
and you say is the person bald
the answer is yes well you've just found
the person with one bit one question
gave you one answer that was it of
course if you said no well now there's
you know 29 other people that you have
to then start asking questions about
when you start skewing data like this
this is where we get into the concept of
variable length coding statistical
compression the idea that the
probability of an attribute on a piece
of data is skewed based upon that data's
existence lots of redundant mumbo-jumbo
again Theory will get to the important
stuff here in a second right so the
problem is this is all we've been taught
in fact most of you probably haven't
been taught how binary really works most
of you were taught how to convert to
binary and how to convert from binary
and that's a great exercise for
everybody but as far as information
theory is concerned nothing who actually
had an information theory class at
University one two three four all right
there's a lot of smart people in here
only five of us have taken information
theory class that's insane it describes
you why no significant things have
happened in compression in almost 20
years we're a mobile environment pieces
of data are flying around the world
every second of every day all around us
and we haven't done anything with
compression in 20 years yeah we're gonna
fix it so let's talk about not the
theory anymore let's talk about the
application right so there's a long line
of things between that theory and what
you need day to day to make your appt
stop sucking and me stop being mad at
you yeah the first one is archivers
we're gonna talk about general data
archival technology right so you have to
understand this is the general lay of
the land when it comes to compression at
the bottom of our rung is something we
call statistical compressors right these
are things like Huffman arithmetic
compression variable length codes
basically these taken a set of symbols
the probability of how frequent that
symbol is and assigns a variable length
code word to it so if bases itself on
the statistical skewing of a data set
the more skewed it is the better the
compression you get right the less
skewed it is the worst compression you
get that's the whole point of this layer
of data that's all these transforms do
and quite frankly like the reason
there's only three of them is because
there's really nothing else you can do
at that point right like you have this
simple simple transform above that we
have something called contextual
transforms these are things like a limb
Posey lz77 LZ 78 lzw lcss lzma lzh am
right a burrows-wheeler transform moved
in front Delta encoding run-length
encoding right basically this let set of
transforms doesn't compress anything all
it does is seek to transform the data so
that it's more skewed for the
statistical compressors that's it that's
all it does no compression is occurring
at that point right so think about that
that's kind of interesting right we have
this base level concept of information
theory and how to produce compression
and then we've got a whole field of
science that just says how can we screw
with our data more to make it more
compressible right above that we have
probably some of the most amazing
research in compression technology since
the 90s but it's not actually anywhere
close to being usable by average humans
right now no no we're close like it
takes years to do a lot of this stuff
right this is really like Markov
prediction by partial matching context
modeling l Peck what these basically do
is they'll analyze your data it run some
statistics on and say hmm I think that
this would be best served by running
lz77 on it or who this it looks like a
bwt transform would work best basically
it's a meta-level that analyzes your
data to figure out what algorithms to
apply to it to get the most compression
out of it right this is considered
cutting-edge right now if you go to
encode are you which is like the place
for compression nerds on the planet
right now it's run by some Russian dude
who's crazy about compression about
compression sorry any Russians in here
huh Russia is cool um this is the stuff
that's the cutting edge here of course
the problem is all of this LPL girms all
time because it actually has a neural
net built in the compressor right so
what it will actually do is it actually
analyze your data feed it back into a
neural net and the next time it sees
data that's similar like oh oh hey I got
some better results from that over there
so effectively it's applying artificial
intelligence to compress your data
better
we probably got another 12 years until
that's going to be able to run on
anyone's device because it roughly uses
about 12 to 18 gigs and Pappy right now
if you get some good results now above
this we breach into a different area
that I'm not going to spend too much
time on right now and this is data
removal so if you're in a lossy
compression set so audio video images
things like that you have to understand
the human sensors they're pretty dumb
right we have more capacity for
information representation in our data
on our computers then the human eye and
ears can actually see and a lot of times
we're very easy to trick we're pretty
much dumb primates right and so the
lossy data removal stage actually gets
rid of data that we just can't process
or may not care about or may not notice
and of course then that could be fed
into the data modeling phase or the
contextual transforms or the statistical
compressors this is how this is how the
modern industry of compression works top
to bottom this is all you need to know
right anytime someone talks about an
algorithm it fits in one of those four
buckets period that's it so let's talk
about how this works in reality gzip
anyone know gzip yay gzip is a comprised
of two stages lcss is a contextual
transform and and Huffman encoding as a
statistical transformer that's it 20
years we've been riding that guy 20
years we even riding that right and
that's it the backbone of HTTP
technology is built on those two
algorithms and that's all right uh
winrar any hackers in the house hollar
yeah winrar I'm surprised people were
like yeah me love winrar i torrent stuff
what about it one fight winrar is
actually a variant of LZ called lzw
mixed again with Huffman encoding right
so it does a little bit different
technology in the front but then spends
it to the same statistical encoder bzip
visit to actually actually does
burrows-wheeler transform throws that at
a move to front algorithm which is
another contextual transform that models
that actually works really well with bwt
so there's two transforms there before
throwing it at huffman in fact up sorry
this is inaccurate bzip with no number
actually uses Huffman bzip2 switched
over to arithmetic compression won the
patent expired on it
years ago lzma 7zip anybody 7-zip some
people yeah some of you download movies
that's cool um I'm not judging yeah
alpha bears I'm gonna say anything lzma
actually uses a variant of LZ there's
actually like three or four variants
that all balance between two ones and
Eve and actually applies a markov chain
predictor any math majors in the room
markov chains are fantastic fantastic
data probability models and so it'll
actually create a markov model for that
and then throw that at an arithmetic
compressor and then z pack is probably
one of the most amazing compressors out
there that's actually winning every
single competition with compressing the
human genome z-pak l pac are at the top
of that and those all use context
modeling and so basically it says let's
analyze this data figure out which one
of all of those sets work best and then
apply and then keep going and keep going
and keep going like if anyone if you
talk to anyone in the genetics fear
about human genomes and stuff like that
they all know that algorithm because
it's the only way they can actually get
their jobs done right now so this is it
this is this is the day to day stuff
this is the connection so when you're
using gzip or using lgm a or b zip this
is what's going on over the hood so now
let's talk about the practical stuff
there's three main things I'm gonna talk
to you about today that you're gonna fix
and be awesome yay notice more beer has
happened since last time I asked that
right yeah where y'all gettin it is cool
it's cool so number one is images images
are by far the easiest way to screw up
the bits on the wire these are the most
bloated things you were sending to your
users and I know it because I can
download it and look at it and I can see
it should not be that good looking right
so first off the first important thing
is let's talk about file formats for a
minute if you're an Android developer
there's a large bias for you to use PNG
files stop it stop it no more no no no
no more PNG files they suck and I'll get
to that in a second PNG files are really
only useful if you absolutely need
transparency right and let me ask you a
question if you're putting transparency
and all your image is what are you doing
right you don't need that transparency
right
don't use PNG unless you need that right
so reserve that for the assets that you
absolutely have to have some transparent
like a how the people just left like he
said bullshit everybody out the bald man
said screw PNG all the desserts screw
that guy um that's right I'm used to
that uh jpg.jpg fans anyone okay jpg you
if you don't need transparency you
should be at minimum using jpg
absolutely there's there's no excuse
here if you're not if you are sending
images that don't have transparency to
people and their PNG's I'm gonna find
you I'm gonna punch in your ear and I'm
gonna take a video of it i'ma put on
Twitter okay the truth is you should not
you should be using jpg now web p run
applause if you know web p some people
okay wimpys fantastic because it
actually represents the middle ground
between PNG and web and jpeg it allows
you transparency it allows you a
lossless version of the data but also
allows you lossy as well and so with a
single file format you basically get the
best of both worlds between jpg and PNG
right so here's the gist if you don't
need transparency if you need
transparency use PNG and then feel bad
about yourself as a human right good if
you don't need it then you need to
evaluate whether or not jpg or web p is
what you need right so at bare minimum
you're already using the wrong format
fix it yay next we're not done yet
reduce quality okay there is a direct
correlation between the quality of the
image and the file size I don't care
what your designers say they're wrong
they're wrong images don't need to look
that good none of you will lose like
ninety percent of your business right by
introducing two percent quality error in
the images you send down that's not how
it works right image quality is a scalar
value for all these lofty compressors
basically when your artists export it
they get a little slider like ah oh 75
yeah right oh I don't like how that
looks 100 better ship it no no 75 is
great it's perfect you wonder why humans
are dumb we're dumb primates the way our
human our cone
rods work right is we don't detect a lot
of problems and you can crank that
quality level down really far before
users will start to complain about it
right case in point PNG right this is a
16 pixel x 16 pixel PNG file saved from
Photoshop right at 24 bits per pixel
it's 2.7 4k all red there's nothing
hidden there 16 x 16 right the reason
for this is that P&amp;amp;G hide shit in its
file format right it's a block-based
file format where photoshop can be like
hey everybody i like pizza and then your
phone is like hey everybody you took
this photo at the beach we and all this
crap is crammed in your PNG files right
this is why when you actually go file
save for web you actually get it down to
121 bytes raise your hand if you know
for damn sure right now which one of
these two your designers are using that
is why this is such a problem right no
one thinks no one looks at the back end
of this stuff okay so let's talk about
pngs and why they're so crappy for a
second so how PNG's work basically it's
your pixel data and we apply the deflate
algorithm gzip to it and you get
lossless image compression there is no
quality degradation at all which is why
your designers love them out of it
because it looks amazing right they
spend all that time making those images
it looks great but you're paying for
your users are paying for it right how
jpeg works is it applies one of those
data removal pre-processing steps before
passing it off to a lossless processor
and that's how you get that data right
web p works in a similar fashion there's
a lot of things that occur to remove
data and remove things you don't care
about before passing it off to
compression now but the good news is you
can actually get lossy pre-processing to
your PNG files is actually really simple
you just apply a lossy preprocessor and
then hit save
that's it there's no there's nothing
past that that's it let's do that do
that right so the idea is that you can
actually go through and take your PNG
file and actually apply a vector
quantization algorithm or a
color-blocking algorithm or a truncation
565 pattern any of those things will
actually work to reduce the dynamic
range before actually saving it and
you're actually going to get a massive
amount of better image compression as a
result in the good news is you don't
have to write this stuff yourself this
is such a common problem I'm gonna blow
your mind is gonna be amazing everybody
everybody get your various it's gonna be
great this is such a common problem that
over the past 15 years over a hundred
and forty three separate applications
have come up on the internet that just
do this how many of you knew they
existed that's like what maybe a fifth
of the room this has been such a problem
for development of applications for the
past 20 years that every single one of
these people took it upon themselves to
write their own version of the algorithm
right so a bare minimum if your
designers absolutely want PNG files and
they're absolutely locked into it trick
them okay and here's one of these
algorithms to apply a lossy p process
you can actually crank that quality
level a lot further down than they're
willing to give space on and here's
proof perfect proof right there's two
images exported from Photoshop this is a
parrot from the Kodak image set right so
I can use it publicly thanks Kodak on
the one side there 164 k the jpeg
quality said it a hundred right fun fact
if you're just grabbing jpeg and you're
actually like link compiling it into
your server side code or link compiling
it into you know your application 100 is
the default uh right this is the same
image at 70 quality at 45 k now i know
we're sitting in a room you've all been
drinking heavily hopefully and you know
you're on a projector and a digital
ratio and all this other stuff but you
really can't notice significant details
on this in an information star of
society where they bring up an image
look at it for three seconds and then
swipe away think about how many bits
right now you are wasting sending to
people that they look out for three
seconds
swipe away how much money you are paying
for your content distribution network to
send those bits down there and how much
they are paying to receive them now I
got to get props to Twitter I did some
analysis y'all getting a little right
Twitter actually does do a good job of
pre compressing their images before they
send them down right especially the
thumbnails I found that most of y'all's
thumbnails are sleek impressed it around
60 quality level 65 somewhere in there
right we're actually really good because
when I go to other social apps it's not
that good right which actually brings us
to something else reduce size physical
dimensions you got to remember
thumbnails take up less room on the
screen right less pixels which means
that there's less chances for the human
eye to pick up those small little nuance
to introductions for reduced quality in
fact this is actually a quality 40 image
that was output at 96 by whatever that
one was right and it looks really good
because it's that small on the screen
now of course when the user clicks on
that you probably want to deliver them
high res 1 but the fact is that you
should not be sending down the high res
one at 4 megapixels to decompress it
into main memory all the way to 32 bits
per pixel so then resize it down to 96 x
128 if you're doing that I don't want
you to raise your hand I just want you
to drop your head in shame so I know who
you are I know a lot of you are doing it
because I can see your code well I can't
see your code I can see how your code
works but the truth is that a lot of
people are doing this they're actually
sending around 4 megapixel images and
going oh well I'm gonna need it in case
they click on it and I'll just resize it
you know I'll just render it at this
resolution that's a horrible idea i've
got a video up on Android performance
patterns that talks about all the crazy
stuff that android has to go through
it's actually get that to a different
sides including all the memory burdens
the garbage collector events that are
thrown off because of that you're
basically screwing your entire memory
heap because you're too lazy to pre
compress this stuff pre reduce the size
of this stuff on your CDN side stuff and
I say lazy because it's bad nobody ok so
everyone's like I'm guy he's right
between that line where I should feel
bad and actually feel like he's yelling
at me I don't know that's okay it'd be
better I go better so the fact though is
that like if you're sitting around for
megabyte images that you
should feel bad about this and a lot of
people are I don't know if you're in the
room but the other thing is uh you
should really be optimizing your data
your image size based upon the device
sending down a 4 megapixel image to a
device they can't actually physically
display that is just as bad right
sending out a 4 megapixel image to a
where device oh why right why would you
do that this is just idiotic right you
know and then sending it down is it
thumbnail like you need to have buckets
this is what I like to tell people like
if you're serious about emerging markets
if you're serious about the next five
billion Android users coming out of
India you're serious about China
connectivity problems about South Africa
and South America you really need to
move away from the mental model that an
image is a static thing that your
designers have domain over no this is
all arithmetic this is all numbers here
what I like to tell people's you really
need when an image comes to you it needs
to have multiple versions on your server
right you need to have 14 different
resolutions thumbnail size quality
levels all of these things cloud storage
is cheap right and if you send the
optimized images to people you're gonna
get it at a cheaper distribution costs a
lot of people like to complain with me
on this they go no no you don't
understand all the extra computation and
the or storage overhead it's totally too
much money for me to do that versus just
sending the 4 megapixel image down I go
no it's not it's not you're paying to
send for megs and your user is losing
money too you're pissing off your user
and I'm gonna tell him i said to
somebody at google i/o and they're like
oh please don't i don't know you ok hey
I'm so I like to say that you know
having a really good dynamic content
serving machinery on your server side is
one of the most influential things you
can do to reduce data size number two
serialized data I'm gonna say one thing
here and if you disagree with me I don't
care if you can't tell that trend so far
json is horrible horrible horrible when
it comes to compression horrible look at
all this JSON and XML are human readable
serialization formats hang on for a
second you're writing code compiled into
executable code to create data to send
on a network connection
other human can read why the humans not
getting it in reading and I'm the
computer is getting any reading it look
at all this crap we have in here spaces
quotes carriage returns lines Colin's
like bracket look why none of this is
needed it's absolutely not needed XML
and JSON are some of the biggest
offenders they're like in my pan like
sins against the humanity of the
internet right like this is horrible
that we are sending around these huge
blobs of JSON and some of you in here
I'm not gonna say who you are but I know
who you are cuz you're wearing your
t-shirts right I know that you're
sending these types of blobs to your
server about three times every 10
seconds yeah you should feel shame I
know you're in here I've done analysis
so the fact is that this stuff is not
needed it is absolutely not needed if
you have a back-end that's returning
JSON and your seared actually using that
holy cow it doesn't even matter if it's
gzip compressed so now someone in here
is gonna walk up and go actually that
JSON is gzip compressed by the HTTP
stack and what I'm going to tell you is
shut up and getting to that I told you
told you it would happen um so basically
this is this is the reason to hold on
hold on oh here we go getting to it so
the problem is this is that remember
that gzip is built upon to compressions
algorithms LZ SS and arithmetic encoding
l zs s is basically a backwards multi
token matching window okay which
basically says I'm gonna try to find the
next 10 tokens in the past 32 k bytes
now if I do not have a match in that 32
k bytes I have to just know you just
literal token it out right so let's say
that you've got structured data right
like you've got a this is what we call
an array of structs form right where
you've got like nana this let's go back
sorry now I'm all over the place hahaha
so you've got something like this
rebelled like ID and gooey and is active
and balanced in picture when you page
that into your compressor the next ID
value is more
32k away so it actually can't find
another string to actually match and
this is why in the future I was getting
to this I actually say like you need to
adopt what we call a struct of a race
form where you actually put all of your
ID values in a contiguous set and all of
your goods in a contiguous set that way
when gzip comes through and actually
tries to do matching in its 32k window
it'll find more matches and actually
compress your data better so if you're
absolutely caught on using JSON at least
do the right thing and transpose your
data so you actually get real
compression and can I go back yes I did
it mom can be proud of me now sorry yes
I have more things to talk about by the
way okay okay which is great for
debugging you shouldn't ship that is a
human reading that on the other end is
it aha then why would you send it around
it's human readable for God's sake we
were computers we're building them use
them mmm there are great solutions fear
not mortals there are great solutions
number one proto buffers anyone use
proto buffers proto buffers are
fantastic there a little bit tricky to
use during debugging yes right but they
are actually a very good serialization
format that produces binary files that
are very small a problem though they
suck on Android the default
serialization mechanism that actually is
used on Android actually has a lot of
memory turn overhead and extra API calls
they basically just bloat your call
stack and it actually doesn't produce
the best a runtime environment in
serialization and deserialization time
that's why we have nano proto buffers
inside the Android source code if you
actually go digging around the source
code we don't use proto buffers we use
nano proto buffers you can actually just
take that file out of the source code
and put it in your own app and
everything works the same way and in
fact the Nano product buffers are
actually amazing because there is
literally
less memory allocations during
serialization and deserialization and
less API functions listed for the data
set so it's going to reduce your decks
sighs it's going to reduce the amount of
memory that it uses in the overhead it's
basically what the Android team uses for
their day-to-day stuff of course my
personal favorite is flatbuffers now for
those of you who don't know I actually
come from the games industry where I was
a GPU programmer so the only thing I
worried about every single day was how
fast to render images to your screen
while you're trying to kill those aliens
like that's all I cared about it was all
about data and performance you can kind
of see how I got here and crazy huh
anyhow flatbuffers was actually built by
one of our team members who focuses on
building software for game developers
and because we were so anal retentive
about performance in the games industry
much more than the application world
right now this thing is badass and it
works completely fine on Android it's
just as fast as proto buffers and
everything else in fact oh sorry this is
tangent really quick by the way this
does not apply to the XML that your
Android app uses for like layouts and
stuff like that all of that XML data
that's being used in there is actually
compiled into binary serialized data
that's actually fast to load and
actually put into your apk so that your
load time is actually extremely quick
funny how we did this for our stuff when
I for yours anyhow back to flatbuffers
okay so flatbuffers are fantastic you
can see this graph here we're actually
comparing a binary flatbuffers versus
protocol buffers rapid JSON and XML and
this is encoded file size of course you
can see JSON is horrible XML is horrible
and protocol buffers basically beats
flatbuffers in in both of these
scenarios right now here's the reason
though why you're going to use
flatbuffers this is the encode decode
times anybody spot the flatbuffers time
yeah now that company in here who sends
three JSON packets every ten goddamn
seconds imagine how much time is taken
to serialize that data I know Jake
flappers is a fantastic serialization
format you absolutely should be using
because if only this reason the encode
time in decode time is just non-existent
right even though as you can see here
it's probably not exactly on par with
protocol buffers like you get a little
bit of over
end so this is one of those great places
where you get to make a trade-off right
you get to say like hey is this a
time-critical serialization piece of
data or is this a size critical piece of
data at all your developers like options
so there's one free sorry all the people
in the back are still standing like what
are we standing for this guy there's
some seats up here nobody hi oh you hex
oh they okay I just saw like one finger
come up and then put down I was like
that's you guys have more beer um
structive arrays do I need to do this a
little bit more structive arrays raise
of structs recovered next got it so now
let's talk about number data uh
numerical data is the bane of my
existence right now it is by far some of
the hardest stuff to compress all of you
in here who are sending around JSON data
probably have some array index that has
a list of numbers in it you know user
IDs or inverted index is to search query
results that get passed back and forth
between your servers all sorts of
numerical data the problem is numerical
data from an entropy perspective is way
harder to compress text data tends to be
self duplicating the word the tends to
occur multiple times the in the English
language the letter E is forty-four
percent the most probable letter of
everything else right so if you have a
block of text you're going to have a lot
of redundancy their number data though
says I don't care about that right most
of the time you have single instances of
a number that's not duplicated anywhere
else remember statistical encoding works
by taking advantage of the fact that
there are duplicates symbols so in
America beta basically screws you over
right goo id's all that stuff you're
kind of bones and this is where we like
to use delta coding right delta coding
super simple concept if you're not
familiar with it basically we take a run
of numbers right and we subtract n from
n minus 1 right so the first number is 1
the second number is 3 so we subtract
the first number from the second number
it gives us to right so you see the
middle row here is 3-1 6-3 8-6 10-8 and
you get the fall of rome at the bottom
now the reason that this is considered a
contextual transform is again it's not
actually compressing anything but
returning the data into something that
actually takes less bits to represent
the original one up there took roughly
about four bits per number to represent
it because the log two of the largest
number there 10 is for as we saw in the
previous slides however the log two of
the bottom set right there if you
subtract one from everything so you get
0 to 2 there's only two bits so by
applying Delta encoding we actually just
compressed our set in half for like five
numbers no data redundancy there at all
just simple statistical movement or
simple mathematical movement right no
stats and decoding is actually extremely
straightforward right way again we start
from the minimum value and just add up
as we go and boom we get our data right
back so very slow algorithm in terms of
parallelization unless you build in
reset points along the way but this is
the basic concept now Delta coding for
numeric data though can get you into a
lot of trouble first off let's say
you've got some low number of datas and
then a huge one there in the middle well
when you apply delta compression to it
it really doesn't help right you still
have this huge honking number that
screws up the rest of your log to
information theory for the rest of your
set or worse you end up in situations
where things are not linearly increasing
and when you do your subtraction point
you end up with negative values in your
data set so now you're really screwed
because not only do you have to
represent your positive number line but
somehow represent your negative number
line as well and this is where delta
compression most people are like well
like that and just move on to something
else because what am I supposed to do
with this I'll just let gzip try to do
something with it I don't know right but
the truth is that they're actually some
cool solutions here so first off if you
ignore subtraction instead use X or you
actually get a cool result right so
let's take that same data set their
subtraction leads to some negative
numbers X or however right just leads to
some numbers in the same data range
without negative values so you still
have to represent this larger number but
at least you don't have to have the
mental concept of wearing your code
negative numbers are coming from right
everything is unsigned at this point
another cool little thing here that I
found a couple months ago is called
frame of reference Delta encoding this
one's really neat basically the concept
is you have some long stream of data
here and you've got some minimum value
and you choose that minimum value and
then subtract that from every other
value in the data set giving you a data
stream that's much lower right so
because I've subtracted 107 which is the
minimum value from everything I now get
a data set that doesn't need eight bits
per pixel or poke
value it actually only needs what is
that four five five bits per value so
thirty percent savings there just from
using a frame of reference Delta coding
that's pretty cool right that's that's a
neat little trick to have in your
handbag when gzip is failing on you this
is really meant for stream splitting so
if you've got a large number said a lot
of ID's this frame of reference stuff is
really meant to say hey let's actually
break up streams of your data where
these minimum values can be best applied
and so for that top stream there you
actually end up getting two streams you
know 107 is the minimum value and 125 is
the minimum value ending up listing
those in the Front's you know what the
minimum value is to apply to everything
and then Delta encoding the rest of them
so boom again here you end up with 107
and 125 or large values where you can
move those away and encode the rest of
them in much much less bits again no
statistical craziness here because we
can't we have to find other correlative
ways to compress our data now patched
frame of reference is the natural
extension of this because what the hell
do you do what the hell do you do with
those 10 7s and 125 right like that's
kind of weird like to be just me we're
just given another data set right like
it doesn't really help you there so
patched frame of reference will actually
do something a little bit different so
let's say we've got one to 10 256 okay
and when we do our deltic encoding we
get 118 246 and so we don't really get
any savings there what patch frame of
reference will do is it'll say hey let's
take this exception let's mark an
exception value which is the one that's
way too high right and actually leave
its some of its bits in the array and
truncate the rest of the bits to another
array right so here 246 is actually in
binary eight values of 111 1 and 0 1 0 1
if we leave the 0 1 0 1 in the original
array we actually get 1 185 then we take
the upper bits upper for set bits and
that actually equals 15 we put that
another ray and now the entire thing can
actually be compressed with four bits
per value across the board nobody
building compression yeah gzip beer all
right so uh compression is crazy a
little gets unison compression is crazy
it's it's it's my personal addiction
because I honestly believe that people
don't learn enough about it and
given the fact that we're all mobile
right now and how many applications are
out there building stuff on the cloud
and socializing individuals and finding
new ways to connect humans we're
screwing up in a big time because we're
not going back and obeying the
fundamental problem the the rules of
data compression and networking when it
comes to asset size so there's a lot of
stuff out there if you want to learn
more number 1 i've got a great series of
videos out there where r i explain all
the basic algorithms of compression it's
called compressor head if you search for
it on youtube you find a good six to
eight videos where I talk about Huffman
encoding variable length codes
arithmetic compression markov chains
burrows-wheeler transform adaptive
arithmetic compression transforms and
lz77 like basically i'm taking that data
structures to a one style course but I
do it with Legos and silly putty instead
of actual code kind of fun number two is
there's a great article I wrote a couple
years ago called image compression for
web developers if you've got someone on
your team who doesn't understand
anything i just talked about just give
them that article it'll basically run
them through the basics of how image
compression works lossy versus lossless
jpeg web p PNG how all that stuff comes
together and hopefully tell them to pull
their heads out of their asses with all
due respect number three a crappy I
invented a GPU texture format for video
games if you're a video game developer
in here and you're looking to save
residency size on GPUs go check out the
image format that I actually made it
won't actually decompress the CPU memory
at all it'll actually keep the
compressed block format and actually
sample it in the shader so you actually
get a huge redundancy in size there then
there's also texture wrangling which is
a talka game i gave it game developers
conference which talks about how i did
an analysis of a thousand top games in
google play and basically found that
seventy-three percent of all textures
used by those games were PNG files and
then i rant thusly and of course there's
a great talk I gave all my talks are
great you like how I said that a couple
years ago they talked about gzip and the
fundamentals of gzip for the web stack
and how as a web developers and people
sending data around gzip is just not
going to cut it for the next world of
things you need to think outside the box
I talk a lot more about PNG and why
that's such a failed format there too a
little bit more about JSON and
and of course text compression for web
developers is another great thing it
just helps people understand sort of the
basics of archival compression and of
course flatbuffers if you're interested
in the flatbuffers format I've got a
video on that to where I talk about the
internals how it works how you can go
get started with that stuff I do videos
all the time roughly about one a week
now Android performance patterns you
want to make your apps better you want
to keep your users happier you want more
money and an awesome house with a tiger
and an elevator watch my show it might
help and also make sure you join our
Google+ community as a hair set up
senator basically every couple months or
so we do a community shirts here turn
around turn around we do community
shirts where we basically put brand you
with Android performance patterns swag
right give it given the tour giving the
tour yeah let me see here yeah let's see
both sides the see both sides yeah yeah
round of applause here yeah who thought
performance could be so sexy right so
the perfect the G+ community is
fantastic because there's a lot of us
people in there who just anal retentive
about performance if you've got a
question you want to see what other
people are doing just stop by there and
check it out if anything is just a
resource to make sure that you're not
doing anything too stupid and then you
know by the shirts we come out cool
announcement if you love performance we
are doing a performance only conference
at the big Android BBQ in dfw texas in
october myself chet haase rato Meyer
Timothy Jordan we will all be there
right there will be some surprises that
we will talk about we will unveil some
new performance stuff that you've never
seen before it's going to be a huge
awesome party there's gonna be tons of
Googlers there go get your tickets now
register book your flights it's cheap
it's dfw who the hell goes to dfw it's
it's really fantastic the big Android
BBQ people were gracious enough to let
us come in there and just trash the
place this year so make sure you come
down there and watch what kind of
trainwreck it is so with that my name is
Colt McAnlis thank you for time we'll
open the floor to some questions I think
we have some time yes all right
questions I saw you had a question up
over here hair to the side guy so very
complex question i'll try to repeat it
for everybody else how do you compress
text data since most the talk was
image station image data in serialize
data and the context that he provided
was if you're doing a bunch of
server-side processing with gigs and
gigs of text data how do you properly
compress that information and then he
mentioned a sdhc or a shared dictionary
header compression or acronym acronym
acronym acronym yeah yeah and why is it
more people encoding it let me start
with that one first not only people know
about that because the the performance
of it is is kind of contextual like to
get really good compression with the
shared dictionary header model you a
need to have a shared dictionary that's
agreed upon between a server and
multiple clients right and so you get
you got the overhead of transferring
around the shared dictionary and keeping
that up to date and then you get the
overhead of like if something goes out
of sync you're basically screwed because
you can't decode anything and so it's
actually a really gnarly set up to get
done the right way and then you leverage
all of that implementation overhead
against just G zipping it and people go
like man it's good enough I think just
gonna lay down and absorb some sun rays
now to get to your other part of it
which is like how do you compress data
for server sides that's doing processing
on all this stuff that's a great
question and google has been trying to
solve this in many different ways if you
actually pay attention to our blogs and
what we talked about we've shipped
faster better versions of gzip called
softly get fili and bertolli which are
all create the same gzip format but
produce the results faster and produce
smaller results right at the cost of
more memory overhead and a lot of those
things were built so that you know
people can process large amounts of text
data and encoded and decoded extremely
efficiently and still have you know a
resident is some data server somewhere
this guy is getting this could be like a
huge conversation where I talk about all
the nuanced complexities with choosing a
archival codec for a particular piece of
data I'm not sure we should get to that
right now with other questions pending
good next uh it would would I recommend
all of the things I said today if your
app also works on iOS and the answer is
yes web p works on iOS flatbuffers works
on iOS nano buffers doesn't because it's
a Java only implementation and proto
buffers I think there's a port of
objective-c
but basically if you're looking at web
p.jpg encoding and flatbuffers
absolutely those things are all there
and even if you're running on iOS you
should still be cranking your quality
settings down and resizing the damn
images and not using JSON so I
absolutely recommend all that in the
back to circa so two-part question I'm
going to repeat it for you the first one
was SVG format right where does that fit
in the picture here number two is HTTP
to HTTPS PD and the compression codecs
are involved there so I'm in target with
SVG I didn't talk about SVG because
mostly people have realized that the
image or the designers have realized
that the image quality that they can get
from an SVG image isn't what they can
get out of a JPEG image right and so
typically they tend to offer their stuff
in Photoshop or upload it from a camera
or a picture of food or whatever right
and those things are called raster based
images that don't naturally convert to
SVG format now SVG is a fan tastic
format if you're not familiar with it
basically rather than actually storing
your image in a row and blocks of pixels
it basically uses a human readable text
format to describe the types of
graphical transforms to apply to
recreate that image right so basically
when you load it into memory this is why
a lot of people don't use it when you
load it into memory you then have to go
through and draw and draw and draw and
draw and draw and draw and do each one
of those little draw operations to
recreate that image now if you're doing
something the cool thing about SVG is
you can you know from one simple file
you can create a 16 pixel x 16 pixel
icon or a 4 megapixel by 4 megapixel
image it's the same pipeline right you
just spend more time processing pixels
and the modern SVG encoding zor codecs
should use the GPU to do that which
would be super fast but I can't
guarantee that across every version of
Android or iOS or the web for that
matter i know i would like to say
eventually but yeah who knows the second
one HTTP two and speedy I got to be
honest with you I'm not in with those
teams as much as I should be I've
dabbled in speedy a little bit and kind
of went like I'm going to get back to
this a little bit later so I can't
really answer or much on that right I
apologize more questions yes sir
fantastic question so I gave a
recommendation to optimize for
of screen ratio and whether or not it's
a JPEG versus a fizzle purses a large
screen you know should you also be
taking into account network connectivity
that's fantastic that's that's really so
a lot of developers have the mental
model that they like upload some static
images and the CDN is basically just a
directory style format and then they
just grabbed you know it's too complex
to figure out what the matching
algorithm is to figure out what the
right image to grab down is once you
move over to the mental model of like
crap I gotta have like 30 different
versions of these to optimize for my
image transfer you create something
that's a little bit more scalable right
because when you get that first request
for a 1 27 x 33 resolution image from a
device in you becca Stan that's never
because that's the phone they have there
and you don't have it on your server
right you'll calculate it cash it and
send it back and then when that phone
blows up there and you get tons of those
requests now you've got it cached around
once you build that infrastructure you
start getting more control over what
types of assets you have how you cash
them how you send them out now one of
the most important things here is how
you're designing your application for
high latency environments and
communicating that back to your user and
so if you've actually designed it in the
right way and said like you know what if
you're on a high latency environment I'm
just going to send you grayscale images
right or I'm going to stop sending you
images and just send you the color tone
of the image right once you start
playing those games and you make that
mental jump like oh wait I have to send
the image on every time that's stupid
right like you can actually your mind
opens up you're like oh wait I can do
this I can do this I can do this I can
do this and then you can start taking
advantage of lower network environments
and how you want to design your app
around that specifically on Android how
would you check for network connectivity
I actually have a video about that
coming up in a couple months the basic
idea is that if you actually grab
connection manager and then do a call to
get network subtype it'll give you an
enumeration right that enumeration can
actually then be checked against
telephony manager telephony telephony
telephony manager and you'll be able to
see if it's an edge network or an LTE
network or stuff like that which gives
you the best uppercase value but in
reality your network connection is gonna
be really crappy compared to what that
value should be like LTE because you
know server may be slow or they may be
far away from a cache data center or
something on those lines what I like to
suggest you do instead is actually
Oh as the user is using your application
and there's a known data type size so if
you've got a 128 pixel x 128 pixel image
and you roughly know that's about 1k in
size and they're on an LTE it should
roughly take about this many
milliseconds to do and so you start
recording that over time and when you
start seeing those requests get up and
up and out of the range of what you
would expect and you notice the network
changes or something happens then you go
oh wait wait wait something's wrong
let's back out let's design a different
pattern and send some different images
down communicate to the server add data
to your request like that yeah Android
performance patterns yes sir to repeat
the question you know there was a time
when PNG didn't exist and it was just
jpg and then we were said like oh you
should totally TP ng because it's better
and if you set stuff up the right way
with the right compression you can get a
similar value similar quality and
similar compression I know is the best
way to put that so remember that the PNG
at its core is a lossless compressor
right now if you apply a lossy
preprocessor to it right then you're
basically playing this weird game of
like I want to lossy pre-processed
something and then losslessly compress
it huh jpg already does that and it does
a much better job of it too you can
remember the JPEG codecs have been
around and being refined by very
anal-retentive photographers for over 30
years now right and that format and the
JPEG to format in the J big format like
these are very hard core compression
gurus who are optimizing what's being
removed the PNG lossy stuff though is by
hobbyists right like Kodak has not come
out and said here is our PNG lossy
preprocessor right and so sure I'm sure
there's a data set out there that given
the right preprocessor and you know the
right data set that the quality level
and compression sizes could be
equivalent between the two but I don't
think that that's anywhere close to
being reality for 99th percentile of all
the assets that you're going to generate
over the lifetime of your career before
you get bold of course I'm bald some
preference goes again this winter jacket
Rena pause by the way for Twitter for
having us here then this mess up the
place
thank you for letting us mess up the
place everyone knocks something over
before you leave fantastic question has
there been sort of large neural net
research learning machine research done
on finding the optimal quality level for
the human eye for jpeg images that come
through no however in the games industry
we had something very similar to this
basically what we would do is we would
have our designers sit down and look at
like classifications of textures so
there would be like well there's a UI
texture this is a an environment texture
that's mostly dark this is an
environment texture that only works in
the desert world and they would
basically sit there and test some value
ranges and say you know for this bucket
of classification seventies probably
good right or some value between 70 and
80 works right and then during the
export process when that artist would go
export right we would say oh what bucket
does this fall into and then we would
actually do a recursive quality checking
process where we would export it at the
upper value and then check the PSN are
and then the next step down on the next
step down the next step down on the next
step down and we basically find the
lowest value we could get before PSN are
when under the threshold that the
designer had set for that bucket of
images and so basically by giving your
designers a little bit of freedom here
and saying like you know what these are
thumbnail images that are high
resolution or you know these images tend
to be tagged with the word food or stuff
like that you can maybe get to something
where you have this stair stepping
pattern to find the right PS and our
threshold but other than that every
image is so different that unless you
have some human tweaking it right now or
you've got some crazy you know other
stuff that we don't know about it's
really tough to find the right PSN our
metric or SSN IM if you're from firefox
and one argue about that firefox yeah if
that answered why no one has done it
well I mean we've done in the games
industry for years it's just kind of
standard par for the course for a lot of
the stuff we do none lately though
because everybody's been going to mobile
and people forget about these basic
things but I mean if you're getting an
xbox 360 game that's not using the unity
or unreal engines chances are someone is
sat down and thought a lot about you
know how these images should be
compressed and you know how it fits on
disk because remember got a DVD and
everything's outfit there and so there's
a lot of anal retentive attention of
detail that's taken into these textures
and so that's basically the processes
that we would book that you have your
hand up first
so a question about the support of
hardware rasterization for canvas SVG
and CSS so as of like chrome 32 or
something like that GPU is the dominant
way that all CSS and HTML are are
rendered and rasterized it's all done
through the GPU paths canvas for Android
has been GL compliant as of like
honeycomb maybe a little earlier than
honeycomb and so basically unless you're
running on like Android wear which is
one of our only software rasterize
devices out right now you're pretty much
guaranteed using the canvas is going to
using the GPU if I could go check out
any of roman ghee or chat houses talks
they talk a lot of deal about how
they've actually moved the software
rasterization process over the GPU and
the canvas is completely supporting that
now a fun note though there's a great
Android performance patterns IDEO called
performance of custom views where we
explicitly talk about the caveats of
performance for those things so for
example if you're if you're using a
canvas and you're tracing a path we
don't cash that that's the most
expensive thing you could do in the
Android platform with respect to
rendering like we can't cash that data
so every single frame we have to
recreate that path and the worst thing
is we actually don't create a path of
vertices we actually create a texture
that's the entire size of the canvas and
then punch out where your path is and
apply that as a rendering process later
so we go over some of that so check out
the Android performance patterns video
that more questions yes sir so too I
think the question you're asking is like
I figured out an automated way to go
through and check out the texture format
stuff for all the apks that I was able
to grab is there a way for other people
to do this and how could I maybe use
that to ya so how would we improve those
applications to be found problems in so
so for the game development side of it
the gist was that if you're using images
that aren't being uploaded to the GPU SH
probably using web p instead of PNG's
right because again web p gives you the
full matrix of features that you want
but it actually gives you better
compression better lossless quality all
these other things right and then if you
are using GPU images you should probably
using be using the crunch texture
compression codec crunch is a fantastic
tool set built by Richie eldritch you I
used to work with back on the
halo wars video game basically it starts
with the GPU compressed texture and that
applies a lossless preprocessor to it so
you get a smaller wire size format and
when it decompresses it goes right into
the you know one-quarter resolution GPU
format after that and so how finding the
middle ground between those two diamond
yes sir so so the question was that he's
read that web p is a better codec but
you're not finding a built-in adoption
in the photo editing tools we have all
of the plugins for all of the the main
tools on on the page so support is there
you know I won't get into you know the
politics around software's and
supportive formats and all the other
craziness involved with you know just if
you if you walk into half of the tech
Valley companies here and just say the
word web be like someone to like throw a
chair at you for no reason and like JPEG
for live why what's shut up then why hey
whatever anyhow so sorry more questions
yes sir so if I so if I understood your
question it was going one way and then
you said something else that kind of
confuse me you're asking about like what
if you're generating the image on the
device and then sending it to the server
and then copy ated with like well what
if you're grabbing it on a computer and
sending of the server and is the
solution different between the two you
know fantastic question so if if your
phone can capture you know 19 megabit
pixel images even though it can't
display it you know what do you upload
to the server so that other devices they
could display 19 megapixels could
actually take advantage of it that's a
fantastic question and this is where we
get into a lot of the caveats of this
thing in reality what I would tell you
is don't upload that file until the user
is at home on Wi-Fi right like that is
bare minimum because if you're trying to
upload that you know on roaming charges
you're screwing everybody other than
that though you have to find out as like
a like are we ever going to display this
at 4 megapixels right and if the answer
is no then again waiting for them to be
on Wi-Fi and at home and then maybe
doing some down resolution and then
getting it up might be the better idea
or again if you've got a better cloud
compute and you know they're on Wi-Fi
just upload the whole thing and let the
cloud compute ache care of it but the
general gist there is don't upload it
now upload it later when conditions are
much better right and it's fine it
communicates that GG user like hey you
took this picture I just haven't sank it
to the cloud yet if you know you're in a
good
are willing to take the bandwidth on
that please click this button like
something along those lines of
communication is probably the best best
rapper more questions yes sir so the
question was I thought Android optimized
PNG for use pulse I actually talked
about this in my texture wranglin talk
so I actually went and looked at the
source code and talk to the engineers
the AAA apt tool like the android asset
packer tool will only convert PNG images
to a more optimized PNG format if they
fall in like this specific type of
bucket like if it's a this type of
texture without any transparency and it
knows that it could get away with a 565
format right or that it was flagged in
some way in reality that's only some
percentages of the pngs that you're all
are doing and that's only for of course
when it's built right and so some subset
of your icons are going to fall in this
bucket actually none of your icons going
to fall in this bucket because all of
you are using transparency in your icons
right so absolutely none of them are
going to get packed by the aapt tool
yeah I talked a little bit more about
that if you want the larger I've been
hit in the head since then so I forgot a
lot about it but it I give the full
discussion that are getting hit in the
head a lot yeah I'm thinking about yes
sir so the question is hey WTF Google's
kind of encouraging PNG because it's
kind of required by the Play Store and
also other stuff fans has a question and
in reality you know any software doesn't
have it shipped together right like
point to one developer in this room who
have actually have their shit together
right none of us really do sorry Twitter
much love Twitter no but the truth is a
lot of the stuff that we're doing with
PNG for the Play Store is actually built
towards what's the easiest way for you
to get us data and then we actually will
convert it to web p on our side or for
sending it back down to the users in
fact if you're actually running the Play
Store in chrome you'll notice those are
web p image is being loaded and if you
actually look at the network stream to
your device one of those web p images
that you're sitting around and so we
take in the PNG because we know it's
going to be the highest resolution as
opposed to you screwing with compression
on your own and making it bad right and
then the store gets blamed for your bad
image data lets you know our side kind
of takes care of that for you now i
don't think that is a pass by any means
because we're definitely not doing that
the right way for when your compressor
you when you're actually compiling your
Android application it's while your pee
and G files you're putting in all of
your resolution folders are definitely
not going to that same pass you know and
I think that's kind of a cop-out we need
to fix that on our side you can also do
more question I saw some other hand back
here that has an S election yet yeah got
it so the question is like when you're
actually storing on disk the drawable
formats like PNG what is that actually
doing so this is a lot of working parts
here I'm gonna try to make it as fast as
possible first off understand that
you're you're going to be bloating of
course you're you're the data size right
so the APK that comes down to the user
is going to be much larger on disk your
decks size is going to be much larger
and your load times are gonna be much
larger right for those basic things
which is again unfortunate i think it's
we need to get our stuff together with
respect to that and fix it a little bit
better now beyond that I'm not sure if
you're asking another specific question
like like should we be doing x or y ou
gots your you're asking about do that do
the level flags you pass in to create
bitmap and Android have enough have a so
you're talking about like the pixel
format from bitmap pimp oh yeah oh yeah
oh yeah totally yeah so sorry we're on
the same page your hair is better than
mine that's why I took longer for me to
catch up with you so yes absolutely so
in android bitmap info has a really cool
little flag that's like hey what format
do you want this in so by default just
you all know even if you actually load a
JPEG file android loads it at eight
eight eight eight and that fourth eight
is alpha right so even though you're
only using red green blue we're putting
an alpha channel in there when we load
it into main memory that's because we
need sort of a contiguous data set that
we can manipulate for all the other AP
is and you don't have to worry about
that as a developer of course the
problem with that then is it actually
ends up on the GPU as 32 bits per pixel
as well however if you can load your
data and you know that it's only using
three channels you should actually set
that 565 flag and I've got a video on
Android performance patterns we're
actually talking about the right way to
do this and the wrong way to do it in
fact there's a really cool trick you can
do where typically what will happen is
you're like loaded off of disk and then
you'll convert it to 565 of course so
you're getting double memory copy there
and all the extra overhead but there's a
way to do it right off of disc 2 5 6 5 4
4 4 4 is another format that i recommend
turns out I shouldn't have done that
Ramon G actually got pissed off of me
because that's actually depreciated in
the platform now and a 8 is a decent
format if you're if you're just looking
to do gray scale and the cool thing is
with a 8 and 5 65 is that those exist on
the GPU as 16-bit and 8-bit per pixel
images themselves so you're actually
getting a better residency and CPU
memory and better residency in GPU
memory which means when the device is
now constrained by memory your app is
less likely to be the one that gets
evicted well if I change everything to
jpg will AP piece a paper no no aapt
will only convert PNG files that fit the
specific pattern of being able to not
have transparency and potentially go
from a 8888 format to a 565 format and
that's only some subset so if you moved
everything to JPEG apt would completely
ignored that's the only thing you get
out of a apt which is kind of a bum deal
yes yes sir to repeat the question is a
networking libraries like ok HTTP and
some others have like built-in support
for JSON is actually really convenient
cuz the handle serialization a lot of
other stuff for you so serialization is
a really misused process inside of
Android I got a video coming out on that
too I see a trend here and in reality
the default JSON and XML serializable
xin side of android are really bad and
so the ones in ok HTTP are a little bit
better but they're still using the
serializable class as a as a type and so
that's actually a pretty big problem
what I do recommend is if you actually
care about your users and you care about
the data you're sending around
converting that stuff to you know just
grab the raw blob from ok HTTP and then
using flatbuffers which by the way will
serialize it to a Java object for you so
you're still going to get that
serialization path to a Java object than
you can do query on in fact here's the
really cool thing about flatbuffers you
can actually query values from the flat
buffer stream without deserializing it
first right so you can actually leave it
in its compressed format and get
information out of it without having to
serialize it to an object that's really
cool because you're saving you more
memory so like one more question I'm so
sorry it's like eight o'clock I got to
give it to you because you've been
waiting for longer and you asked me to
speak here so a great question you know
have we and Google ever talked about you
know doing something
smarter with apks then sort of the brute
force method the answer is yes and why
haven't we done it is I can't talk about
sorry there's there's a lot of people
inside of you know that I work with and
I talked with your you know we're all on
the same page of like listen we got to
do something better here like we just
have to do something better and every
time we come to like what about this
solution fuck what about this loot what
if we did this like you know and it's
it's literally like when you have an
ecosystem this big and you're trying to
back porch or you know innovate in this
way you end up with all these crazy copy
it since I how well legal won't let us
do that huh you know so-and-so won't let
us do that well that violates the term
of service well we can't do that in
Germany you know and like it there's a
lot of little weird nuances uh in my
perfect world we would move away from an
APK model to something that's like APK
like where you have your core
distributable code that's actually
bundled in packaged with a little bit of
overhead and then we find some really
good way to give you access to a CDN
right that they can handle image
streaming and everything else and like a
front-end that can do that stuff in the
right way because I mean for game
developers that's actually not going to
work right because how much the way the
content works for the majority of
application developers in our ecosystem
they send images and they send json
blobs right and we can't control your
server side that is doing the JSON blobs
but hell we can compute images we know
how to do that but that's in my own
perfect world take that with a grain of
salt and you know when I propose that
I'm like off well we can't do that in a
sample so you know hey I'm really sorry
if you all have more questions you want
to contact me I'm sorry I got to go this
is how to get hold of me + copan callus
on google+ at de Roche on twitter thank
you so much everyone tonight really</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>