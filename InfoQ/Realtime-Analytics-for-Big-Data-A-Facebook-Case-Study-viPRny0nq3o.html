<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Realtime Analytics for Big Data: A Facebook Case Study | Coder Coacher - Coaching Coders</title><meta content="Realtime Analytics for Big Data: A Facebook Case Study - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Realtime Analytics for Big Data: A Facebook Case Study</b></h2><h5 class="post__date">2011-12-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/viPRny0nq3o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll start first of all by introduction
to myself my name is Nattie Shalom
anyone here know what's the meaning of
Shalom yes
peace what else anyone I know that there
are some Hebrew speakers or at least
know the name so it also means by and
goodbye and go away depending so you
could pick up the one that you like
hopefully I'm going to do some more of
the peace part of the world specifically
talking about different element that
usually are debatable in this world like
no sequel MongoDB vs. Cassandra vs.
HBase and all those double things that
we used to debate so I'll try to put
things more in perspective and kind of
put some more out say engineering
thoughts into how you peek with the
right technology in one of the choices
hopefully it's going to be more
interactive so this is really more of a
framework for discussion rather than a
presentation and the way it's going to
be laid out is that I'm going to first
of all introduce a topic that is called
real time analytics which is not
necessarily new but not very known at
least not in engineering perspective to
everyone then I'm going to pick up on a
case study from Facebook that built and
announced a very impressive architecture
for doing their real-time analytics they
will go through the motivation what
brought them to do what they've done and
how the architecture is actually built
and then hopefully we'll have kind of an
interesting discussion which we in which
we're going to analyze that and see how
we can make things better at that part
I'm actually going to bring my own
experience and I'm trying I'll try to
draw a solution when I face that same
problem how I draw the solution but
again the idea is not to take that and
say this is the right solution but
mostly to encourage other type of
thinking
when we're dealing with this type of
challenges with this type of problems
and hopefully again we'll have that
interaction so you could interrupt with
that question things that I'm suggesting
or not suggesting and come up with
different suggestions hopefully it's
going to be more entertaining and we'll
end up towards the end if we'll have
time it will even see a demo that shows
some of those principle that I'm going
to talk about so first of all what is
real-time analytics and why is it
important why is it important today
while becoming such a topic even so
generally speaking if you think about
real-time analytics the past mostly in
financial industry we're using it mostly
for risk analysis and the idea of
real-time means time means something
meaning that if I'm doing a risk
analysis of a portfolio and I know that
much my portfolio is not doing well
meaning that I'm losing money and I know
it only a day after that time between
the time I actually starting to lose
money to the time I'm actually knowing
about it cost me money so the the faster
I can actually get to the point where I
know that I'm losing money and I can act
upon that obviously it's the time that I
can actually save money that that in
itself was a small niche in the market I
mean not everyone was running that type
of risk analysis and the question now is
why becoming a hot topic today what if
what have changing the market and
several things that have changed first
of all if we look at the laws of the
social media what happened is that our
profile the profile of each user here
actually each user here you have a
roughly an average 80 pages in facebook
if you know it or not so there is much
more information about the profile of
the user that is now public knowledge it
wasn't really before a lot of the
services that we are offering our sought
services that are actually served all
the web and our services are starting to
compete on how well they know their
users and if you look even on search
engine Google versus being versus others
the more they profile their users the
better the search algorithms becomes and
the better the search result that
they're getting is better so there is
this hidden worry field like between
sites on how well they know the users
now well they can actually respond to
them it's not just how well they respond
to that and give you better quality it's
also a conversion a war if you like a
conversion meaning that out of the
traffic that is coming to my site how
many of them will actually act upon my
site and do something that is meaningful
for me and that's kind of out say the
general theme for a lot of those
analytics and today if you build a SAS
company or if you build a SAS product if
you don't record every action on your
site and you don't monitor it and
analyze it then you almost don't have a
right to exist and that's kind of the
things that have changed the analytics
from a Briton lilyc specifically from
the point in which it was a niche play
in the market to something that is
almost fundamental you can't really
build anything without it
that kind of explains the background why
this topic is interesting today more
than it was in the past right now what
I'll try to do is kind of sketch what
what I think is the common pattern in
real-time analytics how things in
general in analytics and I took the
slides from Twitter for example and I
try to break down some of these
statistics that they gather when they
build their analytics and what and and
there is a dimension here an interesting
dimension that you'll see here actually
two dimensions that are interesting so
the first one is things that are called
counting and basically collecting
counters about the users that are doing
tweets and so forth I can count for
example the number of requests a day and
I can count the number of average
latency and all those tough things
obviously these are this information
that is continuously updated not
something that I can gather and then
look at that I need to see that all the
time continuously updated through graphs
and all those type of things then there
is correlation which gives me a little
bit more of a correlation between those
numbers so for example I can ask
questions like how many users are coming
from desktop versus mobile and so forth
and there is a research which is more of
a long-term kind of view I'm looking at
things from the last year how many
people from different continent actually
accessed my site
do I have duplicates or any anomaly in
the in the data that I collected and
things on that line so this gives you an
interesting dimension so if we look at
the dimension we could see one dimension
is time obviously the first elements the
counters are things that I'm counting
right now and means something right now
but if I look at that a year ago a year
after doesn't really mean anything it's
not important anymore the things that
becomes important is really those things
after a period of time which is the
correlation the train analysis and all
those type of things so there is an
interesting thing about the time
dimension here okay when we look at
things here we look at them in real time
we look at here we look actually at the
aggregated version of that same
information not at the same level of
resolution that we're talking about here
if we look at that-air from a technology
perspective we'll see that the way we
manage and handle each of those pieces
of elements is very different in the
traditional world we used to have data
warehouse an OLTP OLTP was known to be
the transactional system meaning the web
planning of the application this is
where as a user when I click something
it goes to a system it goes through my
web server goes to a database and this
is where my transaction is happening the
analytics was gathered from that traffic
of those projection into a data
warehouse and on that data warehouse we
use to build things in a certain way so
that it will fit to a certain type of
analytics that we were running and knew
beforehand how they actually going to
look like and that's that was the
pipeline of information that most of the
website at least of today was running on
so for the real-time part we used to use
something that is mostly event-driven
we'll look at those technologies in a
second we were looking at things at the
very high resolution meaning every click
of users matters versus if we look at
correlation we're doing mostly Edel
queries against databases so we're
restoring
data in databases we're not doing it in
an event manner we are storing it first
in a database and then we're running
store procedures and other things that
we'll look at all the data that we
gathered and basically generate those
spreadsheets that we were looking at to
build our report and obviously this was
the batch proud of the processing we
were actually crawling across all that
information actually passing it from the
database into somewhere else into a
specialized set of databases and
starting to crawl that and crawl that
and crawl that until we get the
information because we have to process
massive amount of data now the things
that happen in the past few years is
that the volume of data that we wanted
to record again remember the case study
where you have a SAS website and you
want to record every operation or every
user became vastly bigger than it was in
the past and therefore a lot of the
technology that we used to start to
suffer from a scalability issue and
that's why we started to see in this
area for example things like Hadoop
coming along and a dupe was basically
trying to solve the problem on how do we
run those batch analytics on massive
amount of data and obviously that
problem started to pop up here and here
as well and this is where we started to
see things like no sequel and here we
starting to see things where actually
happened even before that which is the
in-memory things and things that I'm
going to mention in in a second but
that's kind of the general idea the
pattern themself remained the same it's
mostly how we implement those bottom
that became different and let's see what
are the alternatives that we were
looking at obviously there is the
centralized database that was the
universe at the time and obviously the
one size fit adults model didn't really
fit itself and we were facing several
challenges that I don't think worth even
at the time to go through that I think
it's pretty obvious the other part is
called complex event processing how many
people here heard about complex event
processing what if you okay so a complex
event processing think about it as a
reverse query right usually we going to
a database and we run a query
against the database and get a result
with continuous query think about the
top of a market for example this is a
classic query that is fits well with the
complex event processing you want to
know the stock for example that is the
that is at the top of the market right
now if you're going to continuously run
that query against the database against
all the stocks we're not we're not going
to be very efficient right every time
we're going to analyze the entire market
but we already know who is second is
first right now so all we need to know
is the Delta right what have changed
from the point in which I know who is
leading right now so only need to know
if someone have passed him or not and
how far is positive and that's a classic
case where continuous queries try to
build that view based on the changes
rather than based on looking at the
universe all over again and Oliver again
and build the world just by looking at
the universe again and continuous
queries basically holds a period of time
and then every vent it looks at the
desert correlation between the event
that happen and the data that it has and
update the table if you'd like the score
that it has based on those events that's
in a nutshell what continuous queries is
all about and that's why I said it's a
reverse query because we it's not us
that is asking for those queries it's
mostly driven by events it's something
that is already sitting there and
someone is collecting the data and
building the results that if you would
like continuously and generating that
updated report if you would like what
was the challenges with that because of
that model the model itself was trying
to do a lot of correlation through a
stream or events the advantage of that
is that it was more efficient because we
didn't have to go and query a large set
of data but the disadvantage is that it
was very centralized by the architecture
itself because we had to do a lot of
joins when the event was coming and
therefore we had to put all of those
information all of those events through
a centralized centralized server which
was the complex event server and that
becomes in itself a bomb like okay so
we'll see later on how we can deal with
that
the other period is in-memory data grid
and in-memory in itself if we look at
the trends we have memory capacity today
even the terabytes of data that we can
buy even in a single server so obviously
there is an advantage that we all know
about putting data in memory and and the
fact that it's actually can be much
faster than having the data in disk the
challenge in that is usually the
capacity okay if we're running into the
capacity that I just mentioned earlier
where we called every operation that the
user interact we can only store a few
hours a few days of data worth of data
in memory to actually make it worthwhile
otherwise we're starting to get to a
point in which if we starting to store
petabytes of data it's going to be very
expensive very hard to manage from a
size perspective so it provides value
but it doesn't really solve all of our
problems and this is where no sequel
come into the world and no sequel was
really dealing with the two challenges
one of them is the capacity and the
other one is scalability and it was
basically provides a much more a much
more cost-effective solution to deal
with the capacity aspect and also the
scaling aspect that's again in very much
in a nutshell the challenge there is the
complexity of working with no sequel and
also in many cases for scaling the read
performance that we were able to get
through that mod through that model was
fairly limited or compared to other
alternatives that I'll mention later on
but it was all centralized around the
problem how do I deal with a massive
amount of data at a lower cost and more
efficient cost if you like
a'dope and MapReduce and people when
they're usually you mentioned the name
on the latex the name adieu pops up
almost immediately it's almost a synonym
to to analytics so generally speaking
Hadoop was based or was built for batch
processing that's why I mentioned that
batch processing is not really real-time
in many ways now the interesting thing
is that that realization is now common
to everyone actually the market and I
picked few of the quotes of the creator
of the concept of a loop actually Google
was the father if you'd like of the
concept of Auto have moved to something
that is called percolator for their
search engine meaning that every time
that you're doing something you write a
blog it's you not waiting for a crawler
to actually store it somewhere and then
it goes to the index of that search
engine it actually go directly to the
index that's the difference right now
and what happens as you could see here
is that the more we wanted to be more
real-time the harder it was to do things
in a batch kind of processing model
because what happened in batches is that
the more you have more information
you're starting to build something that
is called a backlog starting to have
more and more data that you need to
process now in some cases the batch is
few hours and you count on the fact that
within the hours of the night you can
actually do those processing and you
have that window of time in which you
could actually run those batches the few
problem that one is that that assumption
that you have those few hours changes
when you start to work globally because
then you always have data that you need
to process there is no such things even
as a few hours of the night and the
other things that is a challenge is
obviously the real-time aspect the
reason there is a large set of
information that you don't want to wait
for the day afterwards that he could
actually process and know what happened
for example if a customer go to your
website you want to be able to provide
him a recommendation and what's good for
him right now because once he left your
page whatever accommodation that you can
give him is not really relevant if
someone is calling the call center and
that's actually a case study that
brought
all this discussion someone is calling a
call center and you know that that
customer is actually frustrated is
calling the cable company and he didn't
get a good service and the teller is not
don't really know who is that caller and
can't really offer him the right program
then most likely going to lose him so
you have to be able to act in real time
to actually convert on him or in the
case of the cable company to actually
keep him in the system and therefore
batch processing answers seven things
but as an answer a growing aspect of
reports that we want to generate the
real-time part of the report so the
lessons from here is that we have
different bunch of technologies each one
of them is good for something but there
is no one thing that covers everything
so if you really want to manage all
those different views of analytics the
correlation the counting and the
research we need to have a bunch of
those and then the the main question is
how do we order them how do we build
that pipeline it's like a manufacturing
power plant in many ways because if we
are good at batch and we're not good at
real-time then we're going to have a
backlog there we're good at real-time
and we're not good at how fast we can
process this amount of information then
we create a backlog there so a lot of
people have started to build those
solutions as silos and started to
realize that the main challenge is not
just how do we do batch alone and how do
we do real-time alone it's also how we
make the information flow in a way that
wouldn't create banach in the process
itself and that's a big challenge and
we're going to talk a lot about that so
this is this brings me to Facebook and
what brought Facebook to actually look
at that problem what will the challenge
is and again I thought it's a very
interesting because a very interesting
case study because it brings a lot of
those questions and also an answer or at
least one type of an answer to those
questions so for example the motivation
of Facebook when they dealt with that
system is that they wanted to be able to
count all the likes and the comments
that
it does on the pages why is that
important for them because obviously
that increases the impression impression
is how they make money out of
advertisement right so the more
effective you are in your page
especially for news companies and things
like that the more money Facebook can
make on advertisement and therefore they
needed to give you tools to actually
make your work better the same way that
Google actually provides Google
Analytics for free okay so there is this
win-win situation in which they have a
strong incentive to support you and
provide you with that visibility and
information the previous system was
providing that information in a in
accuracy of 48 hours now you could
imagine what happened when you post
something on a wall of a news company
and it happens to be that there is not
enough comments on that post because it
actually doesn't have the right title
doesn't have the right keywords or it's
actually pushed somewhere down the page
now you know that it's not effective
only after 48 hours and let's say that
you learned that it's not effective and
you want to correct it and let's say
that you even correct it after 48 hours
especially news you can't really do
anything it doesn't make anything and
therefore you have to be able to give
that feedback again close to realtor and
therefore the challenge was to move from
48 hours to close to 32nd now this is
interesting okay in one end we want to
be able to push an analytics that was
done for 48 hours into 30 second that's
a big challenge because now we need to
process the same amount of data much
faster right on the other hand we want
to be able to do things that will enable
us to do that without any failure so it
needs to be reliable as well much more
reliable than it was in the past so and
the reason why reliability is important
here because you would think that batch
and reliability you have much more room
here to play with right but what happens
when something is not reliable in even
in analytics
especially think about the counter
you're doing a-plus-plus on something
and you're missing some of those plus
plus but you're starting to get is for
example we actually have that problem in
our Google Analytics that in some cases
the analytics is 100 plus percent of
conversion which obviously is is not
something that is acceptable and
immediately when you see that then you
look at the report and is saying it's
unreliable but I can't really look at
any of the data because I can't really
trust that data and the report itself
becomes useless and therefore
reliability equals accuracy if it's not
accurate the report is useless and if if
you can't really build a report that is
accurate then you can't really it
doesn't really mean anything for anyone
and therefore reliability is important
from an accuracy perspective not same
thing that we have in transactional
system where it is important because if
we lose data then we actually lose
transaction of people and things on that
line it's more important from a
reliability from an inaccuracy
perspective we also need to deal with
not just that we want to squeeze the
time it takes for us to do the analytics
we want to be able to handle much more
data but I mean in terms of the the
amount of data that we want to deal with
or at least Facebook 20 billion events
per day which is a big number right it
all means so how do they cam around that
so first of all before that I just
wanted to give you an idea of how those
reports looks like so for example what
you could see here is a something that
is called a funnel you could see here
the number of people that was coming to
the page and actually the people that
was clicking on the like button so this
is an information that we can use and
know how effective we are in our page in
our pasti and those type of things
obviously it wasn't meant for individual
it was mostly meant for news companies
in that we got the same thing for
comments again we're collecting the data
of how many people visited our page and
how many did something
acted upon and news acted upon something
that we posted an that this is the
interesting information that they needed
to collect so what is the information
that they need to collect very URL they
need to be able to know the counters
meaning the number of people that
actually click the like clicked on
comment click on that and that enables
them to build those graphs that
generally the idea here ok that's what
that's what we're trying to achieve here
what did they evaluate it
they obviously looked at almost the
entire universe of things that you could
use they looked at my sequel and the
reason why they didn't peek at non-ai
sequel even though facebook runs a lot
of data on my sequel is because to do
those counting this plus busting my
sequel wasn't really effective of that
it was trying to solve the much bigger
problem which is complex queries and now
we're dealing with counters which is not
that complex queries and what we have in
return is that it actually not a good
fit for that type of problem in memory
counters Facebook is based on memcache
and there is a large piece of data that
is actually running on memcache memcache
itself would say the default
implementation of M because she's not
reliable and therefore if we remember
what we've talked earlier we have to
make the counting reliable because
otherwise the report itself is useless
that's why they actually disregard
adoption MapReduce is not real time
we've already gone through that and this
is the other interesting part they
actually tried Cassandra and HBase
obviously Facebook is known to be one of
the contributor to Cassandra so that was
interesting to see what they would pick
but they're also very heavy users of
HBase and every I would say even heavier
than then in Cassandra so again the
problem that we're trying to solve is
collecting counters and doing these plus
plus things continuously on those
counters in our reliable fashion on
large volumes ok so at the time when
they evaluated HBase and Cassandra HBase
had an advantage on how they manage
counters compared to Cassandra
let's see what happens afterwards but
that that will see will keep you a
little bit in tension we'll see that in
a different slide so if we sketch the
solution and I was basically doing a
reverse engineering but now they
describe the solution and build that
diagram which apparently is a closed
system a shinto what they've done we're
seeing the following architecture this
is your web this is you and you're doing
some clicks here on comments and post
and all those type of things and there
is an ajax event that is generated per
action and that ajax event is being
stored through something that is called
scribe scribe is an open source project
that is a log basically a log framework
okay and they're storing all those logs
in something that is called HDFS HDFS is
the file system again part of the Hadoop
framework any reasons why they would put
it in HDFS and not in regular file
system or storage excuse me it's more
scalable but the file could be fairly
big what else I can tell you that that's
actually not a primary reason clustering
reliability remember the reliability
okay they needed to be able to store
things again when you record something
and think about how you record even a
movie right you don't want to lose it
lose the frame because it might be the
actual the important frame that you're
losing you don't know what you're going
to lose and you don't know how it's
going to be important so the fit the
most important thing is to record
everything all those frames in the most
reliable fashion so that when you do the
editing afterwards you'll be able to
pick up the right ones and maybe throw
90% of that but still you wouldn't lose
anything because it could be that the
one that you lost is the one that is
important okay so we said that we have
to store and record everything reliably
so that later on we could actually
analyze it and throw all the rest or
throw the part that are less interesting
but we don't want to lose any frame of
the things that we're recording and HDFS
gives us that reliability aspect okay
and obviously scan
PTL is a service and Puma are not part
of the open-source framework they used
to do the aggregation so basically what
we do in retail and Puma we collect the
logs and then generate an aggregated
view of them aggregated counters that is
stored in batches into HBase now you can
see that the batch size itself is 1.5
second we'll get back to that later on
but this is an important number here
part of the reason for that number
relies on the capacity of memory that
they can actually meaning the amount of
data that it can actually store for that
batch and that's a single process now
another thing interesting thing if you
look at that picture is that that thing
that thing is issued it and that thing
is centralized now if you think about
scalability and anyone that draws a
scalability problem the first thing that
you're trying to do in scalability like
scalability in traffic the way you scale
traffic is that you put more lanes right
but if you have set in place where you
have a junction you have a traffic jam
just because of that it doesn't really
matter how many lanes you have heading
to that central junction so this
potentially could be a scalable
double-neck now one of the ways to solve
that is through that batch because we
can push the barrier and actually let
that single point deal with more traffic
than it used to but it's but it's still
a potential bomb like in the
architecture itself and I'll touch on
that later on
when we analyze remember the first
number was two billion events a day when
we actually looked at and analyzed it
and actually looked at the number like
how many events they actually deal with
per server the number was 10,000 writes
a second per server which still a big
number but not non-realistic
why is that important why do I mention
that as usually when we read it abroad
and we look at those numbers we say
well we'll never be able to do something
like that ourselves but actually you
could because really if you look at that
and probably the things that I wanted to
show you through this presentation is
that you could actually build something
better much better than that
so it's not that far higher in many
cases the story and the numbers doesn't
really tell the full details and when
you know the details you can actually
think even how you could do that much
better so a few things about the
assumption that we started with right
remember we started with the assumption
that memory is not reliable now if you
look at how they designed the system to
actually meet those 10,000 requests a
second if they would work through scribe
the log file and try to write everything
directly to disk they wouldn't be able
to deal even with 10,000 requests a
second so what I tried to do is
basically squeeze the amount of the size
pair log and the actual buffer so that
it would fit in memory itself so at the
end of the day they do rely on memory to
actually meet their goals at the
performance goals the other thing about
Cassandra and this is were coming back
to you is that at a time where they
evaluated Cassandra there was a
difference between HBase and Cassandra
and how they deal with counters but the
latest release of Cassandra actually
fixed it so the question is if they
would evaluate it HBase versus Cassandra
today would they come to that same
result
and what is the lesson here the lesson
is that in many cases we look at those
benchmark or decision-making and we say
well if facebook picked up HBase versus
Cassandra then it must be that HBase is
better and we don't even bother to
evaluate that ourselves but we forget
that the time in which they evaluated it
things may have changed and in the no
sequel world things are changing quite
fast and quite rapidly because it's a
fairly new market and therefore we can't
really take that as an assumption and I
know for a fact that if I would do the
same evaluation for those same
requirements I would probably pick up
Cassandra not HBase for other reasons as
well not necessarily just the
performance
that was the important thing to that
lead me to that point now we've seen
what the challenge was seen what
Facebook did to actually build a
solution for that challenge which is
actually a good solution all the
principles are actually in the right
place and the question is can we do
things differently and before that
questions the question would be out of
the assumption that led to late Facebook
to go with that solution could we have a
different set of assumptions that would
lead to a completely different solution
potentially and I try to kind of
question the assumption before I
actually draw a solution and obviously
the assumption can be different then you
could assume that the solution could be
vastly different so for example one of
the assumption is memory is not reliable
what if it could be reliable and I can
tell you that it could be reliable this
is what we've been doing for a long time
the decision about no sequel we can see
that the decision itself especially in
this time is very hard because the
market itself is moving very very fast
there are a lot of different solution
out there and each one has an advantage
the other one has disadvantage and we
have to be very very specific at the
time in which we're making a decision on
what we actually want what we actually
need to pick up the right solution but
it's a very dynamic world right now so
how do we deal with that can we even say
we can pick up this versus this and live
with that for the next ten years or
three years or even two years can we I'm
not sure and the other challenge which
is an additional requirements that
Facebook don't have if we need to build
a solution out of let's say it's a
product that we want to deliver then
having a set up that runs in a certain
data center that is well defined that is
well known is different than shipping a
product cheaper product meaning that you
need to be able to package it in a way
that you could clone it very easily in
different environment and it will work
the more moving parts that you have in
the solution itself the more complex
that solution can become
so this is another requirement that I
added to to that same challenge that I
mentioned earlier because what we want
is to create a solution that you could
package for that same problems almost
everywhere so how do we improve that and
what are the principles and what are the
steps that we could actually improve so
the first one that I picked up is this
one and for that point I want to have
more interactive solutions and what are
the other improvement that you think
might be good and then I'll jump into
some decisions that I made so the first
one that I said if we can assume that
memory could be reliable we can actually
put the log instead of in HDFS and log
file we can actually put it in memory
right it will be faster it will be
potentially more efficient anyone sees
the problem that yes what is it it's not
durable oh you asked the question and
you also answered it yes so the the
question at the the point was that it's
not durable meaning reliable meaning
that if one of the processes goes down
we actually lost that window of log and
we remember what we said we don't want
to lose any of that part and the answer
was also it's proudly replicated and the
answer is that to actually make memory
reliable you do the same thing that you
do with disk because this itself is not
reliable if it's not replicated that's
why you have read for example so the
same thing is applies to memory yes
you wanted to say something right so
there is the question of capacity I'm
going to actually put some numbers here
two words I think two or three slides
from here we're going to talk about
capacity but capacity could be a
challenge here right
remember the 10,000th operation per
second could lead to a huge number so
the question is how do you deal with
capacity and capacity could be a
challenge so in some cases memory could
be a good solution in some cases and it
will be dependent on the capacity of the
amount of data that we're dealing with
it might not be the right choice
but we'll actually take the numbers from
Facebook and compile them and see what
is the capacity that we need and and and
see if the answer to that is realistic
or not from a cost and from a size
perspective okay but capacity is
definitely an issue here or potential
issue here other things yes yes so the
question is did we compare between
memcache and Data great solution and the
answer is yes and there is a difference
and I'll touch on a few of those
differences one of the differences
assumption about reliability okay with
memcache at least the default
implementation now the reason a
commercial implementation of that
they've actually deal with that but the
default implemented of memcache is not
reliable meaning that if one process
fails you assume that he could lose data
or in this case we're assuming that we
don't lose data and data grid was built
with that assumption in mind they were
built as a system of record they were
built mostly as a data base in memory be
sure the data base but with the same
assumption over data base and therefore
they degrade are different in the same
in the sake that they're reliable
there is another aspect that is
interestingly with the data grid is the
API itself is far richer in terms of the
queries and the language that we can
actually interact with for example we
support JPA
every support document API and SQL other
support a subset of that but also a much
richer API than you would get with
memcache which means that unlike the log
file that we used in the case of scribe
we can actually use store the data in
this in memory log but also query that
as it flies into the system itself so
remember in the previous solution when
we actually store that information in
the log we can only see that information
only when it actually was written to
HBase up to that point we couldn't
really access that information and I'll
tell you when it's going to be important
and what type of application would
actually want to see that as it flies as
it gets into the system
why even want to bother and look at that
immediately and I'll give you some case
study for that but that's another good
point the difference between memcache
and data grids
if we look at the assumption that memory
is not reliable and reply to memcache
then the answer yes it's not reliable
therefore we look over the log file but
if the assumption is that we want to
leverage memory and by the way again in
the case of Facebook they leverage
memory in any case because they would
never be able to get to ten thousand
requests a second without forcing their
model to fit into memory size that's
what they did
remember the Puma and P tail and all
those other things what we're basically
doing again think about the pipeline we
have information coming in to our
pipeline and then someone is picking it
up then storing it now think about a
manufacturing pipeline of the car or
bottles of coca-cola or anything like
that any pipeline would do now if you
have people that are sitting in
different factories what would be the
process of passing that goods between
one factory to another right you would
be doing something in this factory
someone would need to ship it to another
factory and then that needs to go to
another factory which obviously takes
that versus if you put them in the same
building if you put them in the same
building obviously the time it takes
them to pass the goods that you managed
and the goods that the other guys in the
pipeline takes would be much faster
right so if we see an area in
architecture that requires that level of
dependency then it's much better to put
them together but if they are less
dependent they can live in China and do
something for me and I can get it a
month later and that's fine I can
actually desert it so the thing is that
if we see that in our pipelines there is
strong dependency in any case with the
components of our pipelines then we
better co-locate them in this case the
log and the Puma have a very strong
dependency we have to push all that
thing very fast and if one of them is
not working at the same pace as the
others the entire pipeline study breaks
because when we starting to get into
backlogs and therefore the decoupling
can be a diminishing return in many ways
we're not really gaining that much out
of that we potentially gain but we're
not really gaining
we're whicker's as we're as strong as
our weakest link and therefore the
optimization was to keep the same
separation logically but co-locate them
differently
meaning I'm taking the things that I
feel that are strongly dependent from a
runtime perspective decoupled from a
functional perspective and put them
together and then assembly them together
and this becomes something that we use
to refer to as a processing grid okay so
in this case we did two things one of
them remember that the Puma itself was a
centralized point we basically created
that as a as a virtual point it's not a
central server anymore and the second
thing is that we actually co-located
that which each of those data points and
therefore we can actually process the
data co-located to the event itself and
therefore we can process larger volume
of data so we solve two problems here
the centralized Puma and the speed of
processing other optimization that you
can think of is there any optimization
how we store to HBase in this case would
you you mentioned the challenge of
capacity right so if everything here is
in memory we're going to run out of data
so where would you put HBase as
Cassandra in this case what's that
perleval yes so basically what he's
suggesting is that you could actually
put and you're actually stepping a step
even further you could actually put a
Cassandra or and HBase instance pair
node here and that means that virtually
they look at one big database but each
one of them can persist and therefore
will have a segmentation of the
shortened data and the long term data
the long term data would leave in HBase
in Cassandra or whatever you and the
shortened data would leave here
obviously Cassandra in this case or
HBase will hold all of the data and
that's that's even another optimization
but that's an answer to the capacity
charge right if you remember the time
window that we have usually when we look
at the data we deal with it differently
based on the time and the resolution of
the time so in this case we could have
the first day would live in memory and
the last year and seven years would live
in disk and so forth so we can separate
things in this way and that's how we can
manage the capacity we can even go
further and say that the only amount of
data that we need to store in memory is
only the buffer before it's actually
written to HBase or Cassandra which
could be only 15 minutes or 1 minute in
which case it's not going to be huge
right because we only need to store the
amount of time before it's been
committed to disk which it's not that
big we're going to come back into that
later on so this is a code snippet of
how that thing would look like and the
reason why I put that code snippet is to
show you how simple it could be and what
is the type of things that you need to
do to actually write that processing so
we can see it's something that is very
close to people who are familiar with
JMS and MDB it's very similar to that
it's just a spring-like MDB we can
describe here sorry we can have a
listener here and we use a notation to
say that that listener needs to listen
to events that comply to a data that is
called data object that has a property
with a value of 4
in its property and then whenever there
is an event that method will call for
every data that has a false value in
this property so to build something like
these counters and on Google all we need
to build or to write is those snippets
which is very small that basically will
do count plus plus so the answer is
really to use something like right
behind in this case we do the
segmentation of the data we're not
storing all the data in memory obviously
we're storing part of the data in memory
the data could be the data only before
it the stage data before it is being
stored or it could be actually bigger
than that because we can afford to and
we going to be able to remember the
other challenge that I mentioned earlier
was that the world of no sequel is very
dynamic so one of the things that we can
gain by that is that we can decouple the
underlying data source by pulling here
something like this at a generic data
source adapter that all it needs to do
is basically takes those event and do an
insert a simple insert into the actual
underlying database in that case the
database could be different things a
different point of time and we can pick
up pick up the right one without
necessarily affecting all our analytics
processes obviously there's going to be
another layer that is going to be
dependent on the database itself and
it's going to be changed but at least we
marginalize the area in which we're
going to be dependent on a specific API
of a specific database we're not going
to avoid locking completely we
marginalize the dependency that's
basically what we gained here the other
thing that we gained in here remember
eventual consistency right and one
remember that the eventual consistency
was used as a model to enable us to deal
with scalability of right if you'd like
of scale of Rights and the way we dealt
with that and obviously without losing
at least availability of the application
itself and the way we dealt with that is
that we actually compromise on
consistency okay what was the problem
for that try to write a program that
would write something and with a seal
that the read and the write are not
necessarily consistent it's not that
easy to write an application like that
it's not that easy I mean there are
certain cases that will fit that but
there is a lot of other cases that
wouldn't necessarily fit into that the
thing that we're getting here is a
different degree of compromise between
consistency and availability and and
here what we can see is that we created
a layer that is consistent at the front
end so the application itself gets
consistent interface whenever you reheat
that data in memory we know that it's
consistent that the write and read arc
are always consistent but where it can
be more flexible and now the data is
pushed here in this case we could use
eventual consistency because the online
part of the application the one that
needs that consistency because it's very
real termina in in the sense it's not
really dependent on that actually read
their data and the counters from here
and therefore we have an interesting
model by doing those decoupling that we
can actually build a system slightly
differently in terms of the compromises
that we have we don't have to have the
entire system eventual consistent or
consistent we can actually have part of
the application consistent and part of
the data eventual consistent and this
would give us even a better trade-off
remember the cost so before I jump to
that slide those who haven't seen it
those who have seen it shouldn't
participate in I'd answer yes yes yes
yes so for example if I would try to
write something here and then read it
from here the write from here and the
read from here wouldn't necessarily
consistent because there's going to be a
time until it's actually going to get
there so the question right now again
back to your question is what is more
expensive memory or disk anyone have I
guess what is it it depends okay you
shouldn't answer that question
and one have I guess memory or disk come
on memory is more expensive yes right
well it depends it depends and I'll give
you a reason why and usually I have I
think I found a good analogy to explain
the thing that is at least presumably is
a big anomaly think about a container
now I give you a container that can
store 10,000 bucks within that container
there's a seven box now I'm telling you
that you need to be able to pull those
goods within that container within an
hour okay you're in a port you need to
be able to you getting paid for the time
it takes you to pull those boxes from
that container now I give you for the
same price a container that is ten times
bigger for that same price how long will
it take you to pull those in the right
now it's going to be 10,000 boxes let's
say that previous to that it took you an
hour 10 hours right the reason why is
because the door to that container is
the same meaning the access time to pull
those boxes haven't changed that what
happened to disk the capacity increased
but the access time didn't or at least
not at the same pace as the capacity
which basically means that we can't
really if we want to access all the data
if we want to store it and not access it
then that's fine but if you want to
store it and access it and read it then
it actually going to be sore and and
therefore there is when we measure the
cost we can't really measure the cost
per gigabyte only on the capacity we
need to measure the cost per throughput
go so for example if we need to if we if
our requirements is to keep and be able
to pull those 10,000 bucks in an hour
we'll need 10 containers versus if we'll
put a container with a much bigger door
10 times the door we could still do
1,000 boxes an hour in one container
because it has much bigger door and
that's memory okay so that explains the
difference between memory and disk and
when
Marie can actually be cheaper than discs
because if we need to deal with 1000
requests so in this case 10,000 requests
a second if I only go with disc I need a
lot of service and for memory I need
only one sir or even less disc that's
the interesting aspect here that brings
me to this one which actually also
addresses the capacity aspect and the
cost of capacity because I think a lot
of people coming with that assumption
and it did some exercise here so for
example this is a Stanford reports or
whatever I said is actually backed by a
research that was done here it's not
just my thought and obviously for a long
term data a disc is actually lower so
the the most optimum solution from a
cost perspective would be to combine the
two this is where the name Shillong
comes in read remember said that I want
to make peace between things so in
previous presentation now I said only
that now I'm saying the two things okay
so came back to the cost so if I'm
taking let's say that for every URL a
message of 500 bytes at a rate of 10 K
messages a second the only thing that
I'll need is roughly 16 gig of window if
the window itself is an hour and by the
way I don't necessarily need an hour an
hour is the time between the data that
is written in some cases I need only
need 10 minutes but I took an hour just
for the sake of exercise so if I need an
hour to start 16 gig that will cost me
$32 a month which is like a Starbuck
everyday that's the cost of having the
same throughput of facebook in memory
the other assumption in many other sites
is that most of us are not Facebook and
have even less throughput which means
that it's not going to be $32 it so it's
going to be even much more so that's
kind of ideas that hopefully will break
you from certain dogmas now you should
do things or could do things the other
thing is the operational aspect we
the third requirements that I added to
that that was coming from that other
project that I dealt with is that it
needs to be simple I can't really have
five or ten people and that was the case
in facebook they had ten people working
on that project also sorry five people
not then five people working on that
project from a development perspective
not every organization can afford that
that's the hidden costs that people are
not counting and if I want to reduce
that cost I need to be able to have a
very simple solution how I want to
operate and manage that type of
application which means that everything
needs to be automated scripted and
consistent the way I'm going to manage
things it needs to be very consistent
how do I do that
I think Edwin is here from Jay Cloud so
he's part of this solution so generally
speaking we we want to be able to
leverage the economic of cloud or
canonical scale to solve a lot of that
problem and I try to kind of put some
metrics that maps to that cost so for
example automation reduces the
operational cost the manual work and
there is a lot of cost associated with
this elastic scaling enable us to reduce
the cost because we're moving from
static provisioning to more elastic
provision on-demand provisioning and we
can leverage cloud and other things for
that and more interesting something that
most people don't really realize is the
cloud portability aspect if we are not
bounded to Amazon for example we can
pick up the cloud based on its location
because there is a latency cost if our
data center sits somewhere and there is
a data center much closer than the data
center of Amazon then we can pick up a
data center that is going to perform
much better for us it's going to be much
closer to us so it's better to pick it
up or we can be more flexible in how we
can actually beat for a price because if
we know that we can work with any cloud
provider we can go to those start
without and say these guys give me this
price how much can you give me for that
and for the type of workload that we're
talking here there would be a lot of
cloud providers that will give you a
better price than they would get from
this price perspective
so cloud portability means money it's
not just a nice feature it's actually
can give you a lot of flexibility from a
risk perspective and a and a and a
business perspective soar is the case
for a cloud-based thing which is a
different topic that I'm not going to
talk too much about it but the ability
to obviously scavenge resources when I
don't have enough resources in my data
center can potentially give me a way to
manage the cost of running my
application because I can never predict
how much load I'm going to hit if I have
a flexible solution that could get those
resources only when I need to very
quickly then I can avoid those
over-provisioning aspect that people do
today because of that certain moment in
time in year that they expect so this is
the last optimization that we'll have
into the system itself this is kind of
putting it all together and kind of
showing how all this architecture looks
like and generally speaking what we have
here is the event coming in then we have
a data grid that is basically not just
the data but it's also a processing unit
because we collocated the processing and
what we do it in this data grid is a
complete processing of the events as
they come in so we can actually also
enrich the data before it is being
stored and do all those all the things
that will massage the data so that when
we need to analyze it and read it we can
get the data actually ready for those
reports and queries that are more
long-term so we can and and then we have
something that is called right behind
that takes everything that is written
here every few seconds or every few
minutes and write it and record it into
the underlying database because it is
decoupled it could be a generic plug-in
that can plug into anything here if I
would need to pick something right now I
would probably pick a center because
cassandra has especially if we look at
the requirements of the ability to
duplicate things Cassandra is much
simpler from an operational perspective
to deploy and for us that was important
if I need to clone a solution I need to
distribute
redistributed trying to do that with
HBase is a nightmare at least right now
and Cassandra comes with a much simpler
architecture and that in my view a much
better one the analytic application
itself could actually access the
information here or here if you wish to
I mean in some cases you could only
access the information here and that's
fine could you just be or you could use
your own abstraction on top of that and
access it like that or you could have
two views where you could say the last
day the first day is always here and the
rest of the day is here and it could be
two different tables that you would look
in your reports one that he will pick
here for example the stream of events or
the stream of things that the user is
doing on our on your website you could
actually pick here and see that online
immediately as they're written and all
the counters will come from here okay so
you could actually build the solution
that has an interface to each one of
them and we can even use something and
that's most people don't even know that
we can actually access the in-memory
through a standard JPA and do things
like MapReduce in a very simple way so
this is a MapReduce that we added to a
JPA interface using the standard JPA api
so in this case what I'm doing is I'm
writing a script in groovy that will be
executed and passed from the client into
the server so each server here is going
to run that script and at the end of the
day what I'm doing is basically getting
a results obviously this script doesn't
really do anything but imagine that I
need to do a Marx or mean or any type of
counters what will happen is that when
I'm doing this operation this will run
in each of those nodes and will be
aggregated for me at the client side and
then I'm getting the results here so I
can actually do that and access the
in-memory as if it was an in-memory
database and I can actually build a lot
of the dimension of data directly from
there that's kind of where the two
combination could fit in from a
performance perspective I picked up one
of the customers that actually build the
solution like that
and that's not my side and what you
could see remember the 10,000 requests a
second so this was the number that they
picked up when they compared it to
actually that was compared to remoting
with a JB and you could see that the
numbers was five times better so
remember the containers that I mentioned
so just by doing that that means that I
need less service to meet the at least
fifth of the server's to manage the same
throughput Disqus as Facebook in this
type of workload so that was another
interesting aspect again you could go to
Giga spaces calm and find the
presentation the information and also on
your site and the demo itself will be
available for download as well as part
of that so you could also play with that
so hopefully you enjoyed it thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>