<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Spotify's Reliable Event Delivery System | Coder Coacher - Coaching Coders</title><meta content="Spotify's Reliable Event Delivery System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Spotify's Reliable Event Delivery System</b></h2><h5 class="post__date">2017-08-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XtzCDvoB82o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome oh I'm really glad to
hear to see so many people joined and
came here to listen to my book
my name is Akkad Maravich and I'm a
software engineer working Spotify I've
been with Spotify for almost four years
which is a Trinity for one software
engineer for the past two years I've
been working in a team which is
maintaining and developing sporophytes
reliable event delivery system before
that I was also an infrastructure
engineer and I was working on
maintaining and developing Spotify
gateways in my free time I like to
travel and in many places I go people
find me quite interesting and in some of
the places they're not really afraid to
show it as you can see here in the
picture but yeah enough about me let's
talk a bit about Spotify Spotify is a
service which aims to provide all of the
music of the world to its customers at a
tip of their fingers it is ubiquitous
it's available for mobile it's available
for web tests PlayStation cars many
different connected speakers pretty much
everywhere a big distinguishing feature
for Spotify in the beginning was the
contact in content it has and also the
instant playback that the users would
feel once they plus a play button ah but
as the time went on this was not enough
in order to stay on the top spot if I
needed to start in a waiting uh this is
why at some point started Spotify
started investing a lot of ian features
built on top of big data so here few of
my favorite features so we have your
release radar this is a customized
playlist which comes customized for each
users and contains the new releases new
release songs that the user is most
probably going to like based on its
listening habits
then we have fresh vines fresh vines are
the set of playlists which are trying to
predict where the music industry or the
music is going to go in the next few
months or maybe years we can actually do
that by tracking the listening habits of
the users which are marked as hipsters
seriously and this is not something that
this is not something that I read
internally in Spotify this was actually
a news article that I actually figured
out that we have a new feature and then
it it was giving me an explanation how
do we do that and finally my favorite
feature so far and favorite feature of
many of the Spotify users is discover
weekly what discover weekly does it
actually comes with the music that the
user is most probably going to like
based on its listening habits it comes
fresh every week most of the times this
is the only playlist I listen during the
whole week and also one fun fact one of
the Spotify my colleagues actually came
and said this is a conspiracy by Spotify
because by providing this and by
providing you a good recommendations but
if I can actually tell you what to
listener and well it seems like it's
helpful nigga but those are the cool
features that we use data to produce but
what is more important is that the date
is also used for producing a core
feature of Spotify which is one of the
pillars on which the Spotify is built in
order to stay legal Spotify is paying a
certain amount of money for each of the
stream which is generated to the music
right holders this amount of money which
is being paid is called a royalty as the
Spotify crew as the number of users grew
we can see more listening hours and then
you can actually see on the graph this
is the amount of royalties we are paying
across the time what the recommendation
features and the royalty calculations
have in common is that they're being
powered by data which is collected from
this pot
my clients and all of the data is being
delivered my Spotify ven delivery system
but these two are not the only two
features which are powered by our data
we have many more other features so the
data is also used for the business
intelligence teams it's also used to run
our a/b tests also do the crash
reporting and then if you've been to the
kill truck talk you can actually also
see it's being used to track the ad
click so the star of today's talk is
going to be the spot if it's reliable
event delivery system I'm not going to
go that much into the low-level
architectural details on how the system
is built but I'm rather going to give
you a high-level overview on what our
Spotify or what our event delivery
system is to do and why I also focus on
some of the key operational aspects of
the system how do we do monitoring how
do we scale the system and then finally
how do we do the testing let's start
with the high-level overview
most of the data we are having is being
collected from the spotify clients
whenever a user action occurs an event
can be generated so for example we have
an event which is being generated once
the user starts listening to a song we
have another event generated when the
user stops listening to a song we have
an event when it subscribes to a
playlist and subscribes to a playlist
and finally clicks on an ad all in all
there is more than 250 unique event
types which are being generated from our
clients we expect this number to grow
even further as we work on making the
instrumentation more approachable to the
Spotify developers many of these events
are actually varying in a different
dimensions we have some events which are
small Daz few bytes and then some events
we actually have the messages which are
as big as few kilobytes
we have events which are being generated
sporadically during the day and we have
some other events which are being
generated 200,000 times per second in
the peak hours globally and also events
are varying on the lost requirements
there having many events are okay with
occasional loss but some events like the
events which are used in the royalty
calculations are having a strict no loss
requirements but beside aquatab come on
but beside are even delivery system
needing to take care of different event
types flowing through the system it also
needs to deal with the sheer load of
messages which is being generated by 100
million monthly active users that we
have today in order to be able to
iterate faster on our system and also to
scale it easily we decided to use as
many Google cloud components as we could
so instead of Kafka we're using cloud
pub/sub to save state we're using cloud
sequel and datastore to manage
infrastructure we're using compute
engine all the data that we are
delivering is always delivered to the
cloud storage and it can also be
delivered to the big word are even
delivery system needs to deliver all the
events into the early buckets what the
early bucket means depends on the
storage implementation to which we
deliver data so in case of the cloud
storage an early bucket is a folder the
same thing in the case when we deliver
data to Hadoop in case of bigquery it's
a table and in case of hive
it's a partition one thing that I like
to point out here is that events are
being destined to a bucket or determined
to which bucket they belong to based on
the time stamp when they were received
by our system and not on the time stamp
when they were generated by our client
this might sound a bit of
counterintuitive but the fact is that by
doing this we can guarantee the
timeliness of data delivery and we can
also guarantee that all we can guarantee
with a high enough confidence that all
the events which are destined to a
specific bucket for a specific hour we
are going to be delivered to death
bucket timeliness is also an important
aspect of our event delivery system the
ROI events which are we delivering are
being used potentially by a chain of
data jobs in case we we're having delays
we're creating more delays for our
customers which in turn generates even
more delays for their customers this is
why we internally have a timeliness SLA
that we expose to our clients and we
have a process that we follow in case
the data is delayed and it also we have
a process of how to communicate this
with our customers in order to keep up
with the SLA
we need to have our system running 24/7
interesting to note here is that the
Spotify is a bit different from other
big companies we don't have a operations
team like uber or Google Google or
Twitter developers which are developing
the service are responsible
operationally responsible for serving
being a service being up and running so
I think this is a really good thing
because I think this is what pushes good
developers to be the great developers
but it's not really the fun thing to do
all the time and I definitely know it's
it's not our spouses don't really like
it all the time now for example my wife
sometimes when I'm on call she tells me
to go on a couch and a one-for-one
colleague his wife actually tells to his
kids you know your dad is going to be on
call and they were like no but this is
why we also invested a lot in having a
good one
of our system and also in trying to
automate and make er system auto recover
auto heal in many of the failure cases
recruiting of since we're having so many
event types falling to our system we
wanted to have a stricter control what
is it going to be delivered this is why
we ask all of our customers to define
which events they want to be delivered
and events which are not defined as they
want to be delivered are completely
filtered out we wanted to do this so we
would not waste resources on the events
that we know are never going to be
looked at and finally we designed our
event delivery system to deliver 100% of
data we acknowledged that there is many
different event types flow into our
system and they have different loss
requirements but we felt that designing
a system to do a single thing and do
that thing well is much easier than
actually having many more use cases
since through our event delivery system
we are delivering really important
events we use that as a less common
denominator and of just designed our
system not to lose any data but
designing system to not do that or not
do something is not good enough this is
why we also needed to build a monitoring
service on top which is tracking all the
outputs and all the inputs and based on
that in determines if we have any loss
or not the similar way we have diamond s
SLA we also have a completeness SLA and
in case the SLA is broke and we also
have an incident process and we're
having a process how to communicate this
to our customers
so in order for our system to be running
24/7 and for us to keep up with the SLA
having a good monitoring is a must with
our system we actually recognize that
there are three different types of
monitoring uh we're having system
monitoring for monitoring the general
health of the system we are having data
monitoring for monitoring our timeliness
isolates and we're also having a data
loss monitoring for monitoring our
completeness SLA so we do system
monitoring of our event delivery system
exactly the same way like we monitor any
of our back-end systems here what I want
to point out is that even if the event
delivery system is a just a big box on
it it's on this slide it is actually a
complex distributed system with many
micro services which are working
together to do the same goal we are
having a system monitoring on each of
those components and we're also
monitoring the third-party services we
are using in our system we're monitoring
CPU were monitoring memory we're
monitoring everything we could think of
we think the monitoring is really
important because it allows us to see
what parts of the system needs
optimising and also in case of incidents
it's really easy to pinpoint the actual
problem but no matter how good this
monitoring is it doesn't actually allows
us to see what our customers see and
what our customers need we did system
monitoring we're not actually monitoring
the data which is being delivered that
means that sometimes we could have
entered some really obscure bugs which
would stop delivering the data even if
the health of the system was good enough
or it was well not breaking any SLA or
not triggering any alarms
so this is why in Spotify we came up
with a data monitoring tool Allah
unfortunately it's still not open source
but what the data man does is that it
actually allows us it shows us what is
actually being happening on the actual
storage every bucket you see here on the
right is the actual hourly pocket of
data being delivered he can monitor data
on cloud storage I can monitor data on
bigquery hive I saw even on datastore
and I even saw it can monitor date on
SFTP not really sure who is except as a
TP but what it can do that too
interesting aspect of the system in
order to actually allows us to be
on-call and keep up to our timeliness
SLA we can set the alarms and we can
track historically how did we keep up
with the SLA other he does have other
features like it we can look at the
counters we can go to go and drill down
and see the resource manager and bunch
of other different interesting stuff
date Amon is definitely one of my
favorite tools in Spotify and I think it
was one of the big game changers on how
do we do ops in a data infrastructure
and finally in order to monitor the
completeness SLA we built a custom
tailor system which is actually
monitoring all the outputs and all the
inputs and then based on that calculates
if we did rate the SLA or not dumps
everything to elasticsearch and from
there we actually just graph it with
Cubana
so what this system does is that it
actually if it detects that we did over
delivery or under delivery of our data
it opens up a gyro ticket any sensor
nestled on alarm after that we have a
process we start working on it so in
this example on this slide you can
actually see what does it do so we have
over delivery for event and then it
actually opened up a gyro ticket for us
that we can actually monitor how many
because did we get and then we did took
all the things we needed to do in order
to mitigate this issue let's talk about
scaling a bit scaling is fun I mean
everybody likes it today
Spotify is having 100 million monthly
active users we recently also announced
that we have 50 million paying
subscribers all of that those users are
generating 1.5 million events per second
in a peak hours which actually
translates to more than 60 terabytes of
data delivered every single day all that
load is being split across more than 250
unique event types were having in
Spotify this gives us a lot of
interesting engineering challenges to
work with and yeah like it we came up
with an interesting design but before we
started tackling the hard technical
issues we also wanted to tackle a hard
soft issue today is what if I were not
seeing a rapid growth of the number of
users we are having we're also seeing a
growth of developers which are starting
to work with data in order to make their
life easier we wanted to introduce a
concept of ownership of the events
ownership in this context means that the
owner of the event is responsible for
the quality of the event that is being
produced and it's also responsible of
providing a good guidelines of how the
event is going to be used then
everything goes through a distributed
configuration service so the users of
the event can actually go and track down
who is the owner so they can go and talk
to them and figure out what to do with
the event owners should not care about
timeliness and the owner should not care
about completeness because this is what
we as even deliver it in Guren
with a restless one other aspect that
the owners need to do they also need to
decide where do they want data to be
delivered by default it's always being
delivered to the cloud storage but the
owner can choose for data to be
delivered to the bigquery it interested
for data to be delivered on Hadoop and
finally can choose for data to be
delivered to hive so since we have many
event types flowing to our system we
wanted to actually split them and
consume them by a separate parts of the
system we wanted to do this since we
felt that writing many smaller parts of
the system and managing this is going to
be much easier than having one huge
component that consumes all the events
from a one huge stream this allows us to
have a better visibility on how each
event is performing and actually yeah
understand what the event is doing but
of course it does have its drawbacks
so the first drawback is that by doing
this and splitting our event delivery
system and all the events two dependent
streams is that we have more
administration overhead so this is why
we needed to go and build a system which
is going to talk to the compute engine
and order the infrastructure needed for
each of the events to be delivered the
next issue with this kind of setup is
that determining the amount of machines
which are actually needed for each event
to be delivered gets a bit more complex
so our first step on it was to actually
have the same amount of machines serving
every event we're having is very fun but
of course this had a big drawback if we
would use the same amount of machines
for all the events in case of the small
events we would based much many
resources in the case of the big events
there is no guarantee that we're going
to be able to scale up and pick up the
sudden spikes of the events being
published of course a fix of this that
we wanted to avoid at all cost is to do
the manual allocation of capacity for
each of the events or to push this
manual capacity allocation to any of our
customers but of course we are lazy we
don't like manual work and our customers
definitely don't like manual work so we
wanted to give autoscaler a try since
we're already were ordering machines on
the compute engine turning on the
autoscaler was quite simple you just say
this is the metric I want you to look at
when you want to decide if you should
auto scale something or not this is the
target that I want you to keep and this
is the minimum and maximum of machines
that I want you to use we decided to use
a CPU based metric since the seem like a
well simpler solution and we just wanted
to see how it's going to work and an
iterate on it if it does not work
but first thing first one important
thing that needs to be considered before
the autoscaler is turned on is that the
components which are being Auto skilled
need to be stateless if the components
are not stateless turning on the
autoscaler is hard or I would even dare
to say it's impossible the good thing is
that we in Spotify started designing and
building all of our services to be
stateless long before we started
considering the autoscaler we did this
because by having your services
stateless it's much easier to understand
what are they doing and it's also much
easier to scale because you can just
drop more capacity as needed in case of
hardware issues you don't here you just
recycle a machine and order a new one
and that's it
the hard stuff the state keeping we
usually try to push to the third party
services which are battle tested and
which we know that can handle the load
so in this case in our event delivery
system we're using cloud sequel and also
we're using cloud pub/sub worth
mentioning here is that the cloud
pub/sub difference between Kafka and
cloud pub/sub is that with the cloud
pub/sub you can yet completely stateless
consumers and there is no limitation on
how many consumers you can have you just
drop drop consumers in your system and
they just well somehow magically
coordinate and consume the right
messages this fact that we can get
stateless consumers was one of the
biggest reasons why did you choose to
use cloud pub/sub for managing all the
streams we're having instead of Kafka so
we turned out the auto scaling and well
at some point we noticed a really
interesting thing sometimes we would
have some machines using 100% of the CPU
while other machines from the same group
would use 0% of the soup view because
they ended up
somehow in a zombie State what was
really mind-blowing for us and a bit
unexpected was well the autoscaler
thinks that the average usage of the CPU
in this case is 50% and then because we
wanted to save resources and of course
we wanted to have our machines use more
than 50% of the CPU
we wouldn't scale up even if the backlog
was growing on the other side the same
situation would be if we would have only
one machine dead machines ended up in
the zombie State it does have 0% CPU
autoscaler things while you're not using
enough CPU so I'm not going to give you
more machines and the background rows on
the other side of course we get waken up
on the night we need to fix this and at
some point I guess it got really hard we
got really frustrated and we said this
is really important problem we need to
see how are we going to mitigate this so
first mitigation the simplest possible
ever just always use at least two
machines per each event type this would
avoid of actually getting in a really
bad state we could still end up in a bad
state though but because this mitigation
alone wasn't enough we needed to
actually go and find another mitigation
how are we going to figure out which
machines are in the zombie State and why
are they ending up in the zombies data
but before I actually reveal to you what
was the second mitigation I need to
explain what is being run on each
machine which is being provisioned for a
worker to run so the first thing we
provision a machine is a completely
blank machine and then we start running
puppet when the puppet runs even it
needs to install all the packages we
need and to spin up all the services we
need so the machine would end up in
operational state so in our case it
installed Stoker it installs a
deployment eternity stalls a service
discovery configuration and it is those
a monitoring daemon once they are
installed our deployment agents take
over and deploys
our code to run on docker and ARB well
worker starts serving the data first
this process is really slow it can take
sometimes more than 10 minutes just to
bootstrap a single machine ok but not
only being slow this process is also you
wasting with too many of our resources
all of the additional processes beside
our apply mean that are run there beside
her worker are reaching the same CPU and
they're eating the same memory that our
worker should use this is actually
limiting how small machine can be used
for each worker we need to use at least
force if your machines because we need
to cpu for all the overhead we're having
I guess by now you're all wondering like
why did you went with this process how
did you even come up with that process
well worth noting is that the Spotify
just recently and that was I think
almost exactly a year ago when we
announced it decided to go in a cloud
game this is a process which we have
been using to bootstrap all of machines
in our data center we've been using it
for years it does have it all it is slow
it does use it a lot of CPU but it works
there is a lot of operational knowledge
being embedded in many of the puppet
manifesting in many of the puppet
classes and how do we install docker and
all that kind of stuff since we're the
first team in Spotify using the
autoscaler
we were afraid of actually needing to
reinvent this process and we just wanted
to use this as something simplest that
can get us forward to see if the
autoscaler is going to work for us or
not and of course there is one one thing
that I also forgot to mention which is
creating a little worse resource use
attorney should is that our deployment
agent
can deploy only single worker process on
a single machine because we doesn't
support multi-tenancy
and it doesn't support multi-tenancy
with the resource isolation since this
process is as complex our first suspect
was that the bootstrapping process is
actually creating a zombie machines and
of course the first suspect for the
bootstrapping process was puppet
nobody likes puppet it is magical and
people are often just frustrated by it
so just blame puppet but even if we all
like to blame puppet is as we did that
the truth was puppet was working fairly
well all the processes were installed as
they should and all the services were
going up as they should so the other
thing that we when we started looking
even further we figured out docker is
the main root cause of all of our
problems I was like it's like the
hippest technology ever and everybody is
talking about docker and it's just
terrible and then that's why I put it
like here it's like it's constantly on
fire and it's all constantly said
because it knows that we're getting
woken up and our spouses get woken up
and get angry at us so so so it like
there was like just so many crazy issues
so we would get like doctor proxy being
broken networking doesn't work we would
get darker getting in the fan state and
we can only reboot machine to keep on
working and sometimes it pulls the image
hundred percent but then just returns
404 and then the only restarting doctor
would work since we are not go engine
ears and since we don't want to go into
actually understanding how the doctor
works because it should be just a black
box we wanted to find a workaround that
would work best for us and enable us to
move forward so the first thing here was
that we enabled health checks from the
autoscaler
what the health checks do is that they
are actually pinging our
worker process and if the worker process
don't don't respond for a certain amount
of time they just recycle the machine
but here is a new problem when they
recycle the machine they spin up a new
machine with the same name which now
makes our puppet said puppet gets a new
key puppet master has the old key public
you know taboos rep machine and we just
get in a worse state okay we can't use
hell checks let's see what what else can
we do
kill script so what we did is actually
we use puppet to help us with
disciplining doctor
so we have our kill scripts which are
looking at the doctor state and then
based on that they wipe the doctor state
and restart it or just reboot the
machine we put a monitor monitoring
because we like put monitoring to
understand what is happening in our
system and what we saw it was terrifying
we get 10% of all of our machines being
rebooted every hour because doctor is
ending up in a different state if the
doctor is the state our machine
cannot work like the important thing is
that it allows us to move forward and no
matter how terrible it is
we're not woken up in our SL ace are
safe so let's look at the graphs how do
how are we actually performing with
auto-scaling uh so on the top graph you
can see the amount of events which are
being published you can see that the
peaks are really prominent while there
is like huge dips here is how many
machines are we using uh you can see
there are some Peaks but they are not as
prominent so the good thing about this
process is that we actually prove to
ourselves that we can save some of the
resources during the day during the
night not during the day but the problem
was that because of all the suboptimal
resource usage the fact that we have a
common container Orchestrator we does
the support multi-tenancy and the fact
that at least we need two machines to be
used
there is a lot of resources still being
wasted and this is definitely something
that we want to look at okay so now it's
a fun stuff so how many of you like
horror stories I did oh come on okay I
can skip this slide and just go to some
boring stuff so this is one of the worst
stories and I think this is something
that you should teach in schools and
this is something that you should say
like maybe write a warning with the
autoscaler like you write warnings today
when you when you buy alcohol bottles
and stuff alcohol ah
the thing is autoscaler is really
powerful but you need to be really
careful how are you using it so what our
system does is that it consumes from the
cloud pub/sub it writes to the cloud
storage so here is pub/sub here is cloud
storage and it persists um stay to the
data store once that state is safely
persisted it just sends acknowledgements
back to the cloud pub/sub okay we start
pushing more and more events our system
well was growing and at some point
datastore caught fire there are some
internal limitations which are while not
really scaling well the way we designed
our software and it it became
increasingly sluggish as it became
sluggish we were not sending
acknowledgements back our backlog grew
we started using more CPU autoscaler
dropped more machines here is more
machines you can consume your backlog
how fast ok
we put even more pressure to datastore
our backlog grows even more because
datastore just cost in a worse state
autoscaler just says here you go here is
more machines let's let's see how are we
going to deal with it so what do you
think happened next
so so so what happened is that we scaled
until we could use all the available RAM
in the project we all of a sudden we
were having lunch and all of a sudden
people actually call us like why do you
need a terabytes of RAM like her insane
people could not spin up their data flow
jobs people who could not spin up data
Pro cluster people could not spin up
machines to serve the service all in all
it was a really fun night I was actually
on call during that day and I I think I
I went home at around 1:00 or something
like that and was like completely tired
and like but nevertheless it was a good
learning opportunity and then after we
did a post-mortem we kind of talked
about it and it turns out we could
actually avoid this but just being
smarter about handling our failure cases
if we had a simple simple exponential
back-off in case we cannot write to the
datastore or maybe to cloud storage we
would shuffle down the CPU because we
would have well more time before we
consumed from the pub/sub and the other
learning was well data store in this
case was not the perfect tool for us to
use so we started using cloud sequel all
in all because our current the biggest
problem with this approach is that we're
having really suboptimal resource usage
and we're trying to figure out how to
work with it so we started looking
closely to kubernetes because kubernetes
comes is a managed product from the
google cloud
it also supports auto scaling out of the
box and it suppose multi-tenancy with
the resource isolation everything we
need but unfortunately not everything is
simple for us because if we lose the
ability to run puppet on the machines as
we would lose it if you would use manage
kubernetes we would not be able to do
the monitoring we do it today and we
would not be able to do the service
discovery which
today but we'll see we'll see what the
time is going to show and then I have
just a few words before I end on how do
we do testing in Spotify we are taking
testing seriously and we are big fans of
doing continuous integration and
deployments so what we do is that for
many of our backing services we always
ensure there is enough unit tests to be
tested before they are being pushed to
the production but because Spotify even
delivery system is a complex distributed
system consisting of many smaller micro
services we wanted to have a end-to-end
test which is going to cover the full
flow and which can be run on a single
developer machine so it would be easy to
iterate on it so what we did we actually
just well stuffed all the stuff in a
single cucumber file we have all of our
services being spinned up as a black
boxes on docker introduce some output
input and observe the output and that's
it interesting thing is that this is run
before the code is being merged and it's
also being run before the code is being
deployed to production and few times we
actually ignored what the end-to-end
test is telling us because sometimes it
can take up to ten minutes to run and we
have big incidents but when we learned
it actually saved us quite a few times
and that would be it thank you
so before we start with the questioning
we're hiring if you like working with
these interesting problems feel free to
check the site and please don't ask me
any questions about the google cloud
costa and now questions
yeah what what went wrong with Kafka
actually so we chakra went wrong and
many things so the the first thing that
went wrong is that we're having multiple
data centers and we wanted to use the
mirror makers at a point when we started
to actually experiment with the mirror
makers we figured out that mirror makers
are not really mirroring the data
they're dropping the data if the
destination cluster is down so there was
like really big thing the second thing
that went wrong is that the client
libraries were kind of hard to work with
because they were ending up in a really
weird state that we could we didn't know
how to recover from and the only thing
was to reboot the machines and at that
point we said yeah just cloud pub/sub is
just more convenient for us okay yep hi
I have a question regarding having the
team's doing operations of your services
so do you have any metrics regarding I
don't know frequency or severity of
incidence may be compared to other
players of your scale or to the time
before you switch that node so I I don't
have the numbers because we don't really
track them I mean we do have incidence
across the board every single day or
maybe not every single day maybe every
second day yeah but the important thing
would actually change for us and why did
we went with the actually DevOps system
is that we were hiring more and more
developers and there happened a few
years ago and we were not hiring what we
couldn't hire in fsor race so this is
why we pushed more operational
responsibility toward the squads so what
I think is that you as a developer when
you're developing and when you're seeing
your system in production you're
inclined to make your system much better
because you care much more about the
people you work with than just some
operations team that sits like three
rooms down or something like that and I
think that's that's what works the best
and I really think it works really well
for us
if you can just back down one slide
please from the testing yeah do you run
this on the developers machine everyone
who runs this on their own PC no but why
I said it's a developer machine is that
because I think if you can run your
stuff on your developer machine is much
easier to iterate and fix it so we were
having issues with Python in the
beginning and the biggest issue was the
developers could not run the Python in
the libraries we've developed on their
local machines but what is important
like even if it can run on the developer
machine we don't really run it that we
have a Jenkins server which is handling
world all our bills and all our
deployment and this is where this this
is being run so in a CI environment in a
CI environment yes sir because the
question is when you have many many
micro services that you work on in small
teams working on each one of them then
you have many dependencies so the
question is when you work on just one
and you want to test what you are doing
then you require all the others to be up
so I guess this is a bit specific we're
having a big system that we need to
maintain and we just happen to think
it's just easier to cut it in many
smaller pieces but the important thing
is that every team has its own Jenkins
and every team decides Howard with its
testing it's only microservices so in
Spotify we have many micro services but
there is no overall end-to-end as how
how are they being tested because it is
hard and it requires so much
communication and so much like effort
for this to work and of course it
doesn't work ever so yeah thank you ok
hi there sorry that question I had was
related to the docker
over here ok the darker percentage
failures
it seems incredibly high and I was
wondering could you give us some more
information as to the type of failures
you saw and are you happy to continue to
live with that level of failure and if
not what are you doing - trying
dressers so I can tell you we're
definitely not happy and I hate it so
much
currently currently currently we're
working with one team in the New York
which is handling the docker stuff and
we're trying to upgrade but of course
this is not simple because the upgrades
you have other bugs and like if it's
kind I don't know but the thing is like
we think that the Dockers and the
containers were a big game-changer for
us
and that we would definitely would like
to keep on working as with the container
environments as we are working today but
we don't know what could be used that's
the problem
we had a big incident a while ago with
kubernetes and one of the one of my
questions was like can we use something
else not docker for kubernetes and then
some Google engineers were said like
please please PLEASE ask ask rpm to
support rocket apparently rocket is much
better but I have no idea so I don't
know I mean a lot of the Google services
are based on kubernetes themselves so if
you have google engineers telling you
not to use it that doesn't inspire
confidence going forward so the fun
thing was like okay so we had we were
building our own scheduler on kubernetes
to schedule all the data jobs and it has
a special quirk it actually wants to run
backup jobs so at some point we started
having a lot of failures because docker
PS was taking more than 60 seconds to
return and then when we started talking
about the kubernetes engineers while
trying to solve the case they were just
mentioning that there is so many
workarounds they needed to implement for
the docker and they were so scared for
going from one version to another
because if they would go probably docker
PS would be faster but who knows what
else would they actually unleash so I
don't know I mean it's kind of hard the
expression ODIHR comes to mind yeah okay
hi so you mentioned you've got some
something like 350 event types
how did you split this interview pops up
so you have like dedicated topics for
events yes so I actually I did this talk
before I know when I talked to Danny he
said I should show you but I was afraid
this is going to confuse so many people
so because it is confusing and it's
actually 40 minutes talks so we have a
we're sending a single stream of the
events because of the legacy reason to
this service and then we're splitting
them up and publishing them to the
topics another quirk of pub/sub is that
actually if the topic is not the same as
topic in Kafka so you actually need to
create a subscription and every
subscription you create it has own state
so every event which is being consumed
by our events here is consuming data
from a specific subscription do you find
you get duplications as well say for
from so from the same topic
if you come multiple workers on one
subscriber and how do you deal with that
so here's the bigger attack
oops oh yeah I'm sorry I know so the
thing is like yes cloud pub/sub is at
least once so what it does it actually
consumes it and writes it to the
temporary bucket and then saves state
saving here is actually ensuring us to
say this is when the bucket is complete
and this is when you should start
processing and then at the end here we
have a MapReduce job which is taking all
the intermediate files and then deduct
them and then this is not enough so some
of the events can be reached right from
the clients here and then be retried so
because we are having a not client
timestamp we were actually having a
syslog timestamp we're actually just
adapting on our semantics but at the
other end here there there is another
job which is doing a semantic data thing
so we would ensure that the messages are
really data even from the client
perspective okay thank you okay
last question hi there so you mentioned
you have this configuration service
where the event type owners can decide
whether they want you to even consume
their event ID yeah of your 250 or
however many you have how many are
actually being processed and how many
are you just throwing away and are you
literally throwing them away so they're
never recoverable or is there some these
storing them somewhere and finally if I
may since you've been running cloud
pub/sub have you ever lost data so no we
haven't lost data with cloud pub/sub and
I'm personally most happy with the cloud
pub/sub from all the Google cloud
services we are using and and in order
to save resources we are completely
dropping them and we communicated this
really clearly if you're not doing your
due diligence you're not going to get
your events and they're not recoverable
well truth is they're still recoverable
because we do have stored them on the
syslog files for three days but we just
pretend they're not so that's it
thanks everyone you can still come here
to ask questions if you want it all
questions except a price singer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>