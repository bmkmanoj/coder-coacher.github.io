<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Speed up a Python Program 114,000 times. | Coder Coacher - Coaching Coders</title><meta content="How to Speed up a Python Program 114,000 times. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Speed up a Python Program 114,000 times.</b></h2><h5 class="post__date">2012-12-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e08kOj2kISU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the structure of tonight's presentation
is pretty straightforward I'm going to
talk about the problem that we had the
algorithm that was chosen to solve the
to implement the solution but it ran too
slowly and then two different sets of
solutions the first set we did in 2011
the second set in 2012 I say we mostly
because I have tapeworm hey it's a
transparent universe now you can't hide
anything my doctor is pretty upset
because I'm the only medical record in
his office still on paper everything
else is computerized now I said I know
how this works
keep me on paper then we'll talk about
future steps more things we could do to
speed up the program and the second half
of this presentation is gonna be lessons
learned I always think it's worthwhile
to spend a few minutes and learn from
your mistakes you can make them again
the problem that we're faced with is
fairly straightforward a common problem
in these sorts of enterprises we sell as
I said games we don't really sell them
we mostly give them away for free and
our games are very fun we have a huge
pile of people at our company who know
how to make things that are fun we're an
entertainment firm it's what we do
internally we have a different
definition of fun than just fun we have
a whole bunch of key performance
indicators that tell us how fun the game
is and we tweak them to make the game
more and more fun or we measure them as
we tweak the game to make it more fun to
figure out how to tweak the games we ask
hundreds of questions across all of our
games and to get the small signal from
the large amount of noise we have to
collect huge amounts of data the data is
collected from our servers on how our
guests are playing the games we don't
collect any invasive data on the user's
computer we just collect data on the
server end for each of these hundreds of
questions we're going to take all of
this giant data set we have and create
500 synthetic variations of that data
and then do a particular computation
that I still don't understand and the
result of that is the the answer to the
question we have hundreds of millions of
rows so this is a classic big data
problem I feel a bit goofy using the
word classic to describe something
that's only a couple years old the
concept of big data but technology was
quickly
beginning of last year we had a
prototype written by our chief analyst
who is very good with statistics not so
great with programming that's okay
I can't do the analyst job either when I
learn statistics in college we were
using Roman numerals
try doing square roots in Roman numerals
the best language that by the way is
inter cal if you haven't played with
inter cal this is a great time to learn
it kind of so the program worked
correctly but for just a subset of the
processing it took far longer than the
batch window we had available at the
best-case batch windows four and a half
hours it's frequently less than that so
obviously we needed a better solution
and this graph stops in March of this
year because as I said I like to keep my
job but you can see we have more and
more data coming into this program so we
have more questions being asked for each
question the answer to that question is
recomputed every night from all of the
data ever collected not a very good idea
we're fetching all of the data every
single night and the amount of data is
increasing as well so we have a problem
you can see where the problems lie this
is the guts of the algorithm by the way
if you're wondering why I keep stopping
I'm not doing William Shatner it's just
I have no idea what I'm talking about
not every joke is a winner
some are good so this is the basic
algorithm this looks like Python it's
really pseudocode after I wrote the
program I went back and do the
pseudocode just for these slides and the
code you see here in red is the outer
loop for each of the jobs so phase one
in white at the top we get the list of
jobs I'm sorry that's phase zero get
list of jobs phases one two and three
are inside the red loop phase one is the
next to whiterose talk to the database
to get the data and convert it into
numpy then phase two is the blue loop
and that's analyze the data and phase
three of these two lines here at the
bottom where we write the results back
to the database not terribly complicated
the blue loop therefore I in range 500
that's where we're going to do these 500
randomly samples of the incoming data
those resamples are the same size as the
original data set and consists of the
same data from that data set but the
rows are selected at random so some rows
might appear multiple times in the
synthetic
data and some rows might not appear at
all in the inner loop we're going to
take the random indices that we create
with the white line just above the
yellow line we use those indices to
create the synthetic data from the
original data and then we're going to do
two sets of sums over all the rows in
the first sum we're going to sum up
those rows where some condition is true
and in the second sum we're going to sum
up all the rows where the condition is
false the result of that is two arrays
somes T and somes F we pass those into
some functions that generate sequel and
write a couple of rows into an output
table again not terribly complicated you
may be looking at this and thinking a
couple of questions well you may be
thinking first off why not just run it
on Hadoop that's a good question I'm
going to come back to that later and
some some depth near the end of the
presentation and you might also be
looking at this and saying this is
horrible the inefficient code well yeah
when you start with horrible code you
can get major speed ups
that actually was not a joke
we don't do a lot of performance tuning
anymore programmers are pushed to get
features and functionality out the door
and not to take time to make the code
fast there's another way to make the
code fast called scalability and in the
second half of this presentation I'm
going to call I'm going to talk about
why scalability and to do are crippling
our ability to provide services to our
customers here's that same algorithm
where I'm just calling out what's going
on phases zero one and three are just
waiting on the database phase two where
we're actually doing the analysis is
doing lots and lots of random reads from
the original data sequential writes to
the new data and then a couple of passes
to compute the where clauses and the
sums so it's not too hard to look at
this code and figure out where the time
is probably spent I'm going to do now is
describe the speed ups that were done
first we're gonna look at the speed ups
in 2011 and then the speed ups in 2012 I
had eight weeks of time in 2011 March
April and May with a hard stop in May
because I go to New York and see all the
shows I can and then I take my mom to
one of them so I can justify ignoring
her for the rest of the year I grew up
in New England we didn't get kissed
goodnight mom would wave from the other
end of the room with the back of her
hand
the first of those speed ups is hoisting
invariant code I spent two weeks of the
eight weeks just cleaning up the code it
went from seven hundred lines to two
thousand lines in the process but the
resulting code was well-structured
duplicate facts were centralized magic
numbers were given names in several
cases there were magic numbers where the
same number would appear in multiple
places with different meanings so I had
to figure that out back that out of the
code there was a lot of invariant code
inside loops to pull out Python can't do
a lot of these optimizations the
language is too dynamic to figure out at
compile time
what it can safely do a runtime so I had
to do a lot of stuff to address the fact
that Python really can't optimize
terribly well I noticed in the process
that in here in this where clauses are
not going into any detail on there were
some numbers being computed four numbers
being computed for every row every time
and they didn't change on the various
iterations this loop so I hosted all
that stuff out in the database query
here where we get the data for a given
question job and question are synonymous
in this case we have 17 columns coming
back to the database
I took the four numbers and added them
so instead of 17 columns we now had 21
columns pre-computing helped quite a bit
give us a nice speed up next speed-up
that I did was pipeline parallelism lots
of ways of doing parallelism pipeline
parallelism is a fairly simple approach
you take a problem and you break it into
pieces and you run all those pieces in
parallel so instead of running them
sequentially you can now run them
simultaneously in time vertically in
time so I took the program and I split
into the fetcher program phases 0 &amp;amp; 1
get the list of jobs as phase zero and
then for each job get the data for it
that's the fetcher the fetcher does that
work it's kicked off when our batch job
tells it that the prerequisites have
been met and for each job it writes the
data for that job millions and millions
of rows into a little file the next
process the analyzer is only running the
CPU intensive part of the code that's
this part here the stuff roughly in blue
the blue loop the analyzer is running
continuously we have not turned it off
in well over a year and it just monitors
the directory where the fetcher writes
when it sees data has shown up it reads
the data process it and erases it very
simple it writes its results into
another directory and in that directory
the third program the writer is waiting
when it sees words all
to be written it writes them out so it's
looking for the results of this
calculation when it's season it writes
into the database
the writer runs very very quickly it's
only inserting a couple of rows the
fetcher and the analyzer were originally
taking about the same amount of time so
by running them in parallel I doubled
the speed of the system that's nice that
doesn't happen too often but it's nice
when it does however over time that
benefit grew less and less because the
analyzer quickly became a huge
bottleneck once I started doing the full
processing and not the subset that had
been in the prototype so when you have
one phase of your pipeline taking a lot
longer than the rest the benefits of
overlapping the processing go away and
then later on with the work I did this
year the analyzer started running so
quickly that it took far less time than
the fetcher again we weren't getting
much benefit from overlap processing
however when I did the restructuring I
also set it up so that I could rerun any
point in the pipeline very quickly I
could rerun my standard test cases and
by having three separate programs I
could run just one of those programs
more quickly than running all the entire
code together so I'd quicker reruns and
I had simpler programs it's easier to
deal with figuring out what's going on
with a block of code you don't
understand when there's less of it
pipeline parallelism is one simple way
to get performance easy to implement you
have a producer and a consumer pretty
simple relationship between them I also
made a fix to numpy the result set
coming back from the database in phase
one here had 17 columns a mixture of
floats in sand one string when we pass
that result set into numpy and say hey
numpy make a numpy array out of this
numpy does what we tell it to do but
numpy in the naive form requires that
all the elements numpy array be the same
type and what's the least generic type
to represent int float and string PI
object which means that later on when we
ask numpy to do high-speed numeric
alkylations it really can't it has to
call back into Python for every single
element to get the current value so I
fixed that I replaced that one string
column with an integer lookup table an
integer valued and I found out in the
process there were only a couple of
unique values in the string because it
was really a version number so this
provided a nice speed-up but at the end
of last year it was made obsolete when
they changed the database table to
replace that string column with an
integer call
was it should be I'm not arguing that's
a fine thing to do is simplify the code
but it was quite a quite an improvement
because now numpy was actually providing
high-speed numerix it converted
everything into float the least generic
type that can represent int and float
accurately reasonably accurately one
more speed up that I did last year the
multi processing module instead of doing
all the work in the analyzer in one
thread which is the way the code is
written before i parallelized it's run
too many threads the first time I did
this I spawned one process for each of
the 500 loops and of course the computer
fell over and died immediately and I
realized that for CPU intensive work
that's kind of silly anyway if you're
doing CPU intensive work you only need
as many processes as there are cores no
point in having more just be wasting
time and context switching I used fork
without exec how many people here are
familiar with fork and exec obviously a
few hands go up for those of you aren't
fork means make a child of my process
that is an exact image of this process
by provide identity same open files
everything is the same they can tell
which one is parent and which is child
by basically the process ID and that's
about it the nice thing is that in
modern UNIX systems anything after the
mid 80s fork is very low cost because we
don't actually copy the address space we
just copy some page table entries and
the present case that means that all the
children processes were sharing this
data which is read only after a fork if
the parent or child make changes to
table to pages they get a private copy
but since neither parent or child is
making a change to that numpy array
there's no copies being made the result
is I got a near linear speed up the
startup overhead the serial part of the
code is very small the parallel part is
very large so we get a near linear
speed-up Amdahl's law works in our favor
in this case here's what the analyzer
looked like again this is pseudocode
that looks like Python that's the great
thing about Python isn't it you write
pseudocode then five minutes later you
have a functioning program it's just
Python so this is pseudocode that looks
like Python and almost is for just the
analyzer part the analyzer has at this
point in the code figured out that there
is some new data to analyze at the top
of the page there the top of the screen
it reads in that data the data has been
already converted by the fetcher into a
numpy format so we can simply suck it
right in and go
then I have middle loop number one I
create all of the parallel processes and
I start them running I'm using the multi
processing package which is part of
Python Python has three ways of doing
parallel processing easily it has multi
processing it has threading and what's
the other one sub process I think and
then of course you can use pipes and
stuff and OS calls to do multi
processing in other ways as well
multi processing is great because it
just uses fork when each process is
started that first of the blue lines
there the new process the child process
is going to execute compute wrapper
which is passed in the data that was
write in from the data file and now
instead of running 500 loops in blue
we're going to run 500 divided by the
number of cores because we have number
of cores processes running in parallel
and we do our calculation and we get a
nice speed-up in 2011 when I did this I
was operating under the belief that
hyper threading wasn't terribly useful
so I limited the number of course the
number of real cores not hyper threaded
cores later on this year I tried jacking
up the number of cores the program used
until I used all the hyper threaded
cores and actually ran faster and faster
the more hyper threaded cores I gave it
so that was an unexpected benefit one
little fly in the ointment
the join call here is where we wait for
all the children to die this is a model
I'm familiar with in my religious
tradition that's basically what parents
do they wait for their children to die
and if you think about how that's going
to work that's going to operate
sequentially because it's a list
comprehension it's going to start at the
beginning and go all the way through to
the end so what happens if the first
process happens to take a long time and
all the subsequent processes finish very
quickly well in that case we're just
wasting time
all the other cores have nothing to do
we don't have any more work for them so
this is an example of statically
scheduled parallelism insanely easy to
write three lines of code but you do
have a speed bump in this case it's not
too bad because each of the children
take about the same amount amount of
time to run because they all do about
the same amount of work that was the
theory in practice I saw that actually
there was about a factor of two
variation in the run time between the
child processes and I wasn't happy but
I'm also very lazy so I didn't fix it
well lazy but then again good enough is
perfect when you get to the point where
your program is fast enough you're
finished move on there are other things
we need to worry about it is easy to
fall into a rut of performance tuning
forever and never actually ship anything
I've worked at those companies this went
fine for the rest of 2011 in early 2012
we had a problem this program had proven
to be very very popular so there were
more and more questions being submitted
to it and remember every time the
program runs we recollect all of the
data from the beginning of time so the
data gets bigger and bigger every day
combine those two factors together and
you have a problem in size in addition
management decided they wanted to do
more sophisticated analyses we haven't
actually done that but had that occurred
we'd need another factor of 10 or so in
performance the situation gotten bad
enough that by February the fetcher
alone was taking more than 24 hours to
run clearly wasn't going to finish in
the batch window wouldn't even finish in
the day so we needed more speed first
thing I did this year was take the
program the analyzer because it's the
CPU bound part and run it on a machine
with way more cores obviously that will
speed it up how could it not and it
didn't and I was vexed I was Roth I was
iured I was concerned I was puzzled I
went through all seven stages of grief
it took a week for one solid week I
ignored what the data was telling me and
stuck with my own preconceptions we just
concluded a presidential campaign in
which one side did that to their numbers
and they didn't get the results they
were expecting did they
there's a lesson there we'll talk about
that later but if elected I will not
serve so you have to be of a certain age
to get why that joke is almost funny so
after a week I said okay if it's not
cpu-bound and it's not IO bound what
could it possibly be and I had a stroke
of lightning or possibly just a stroke
that perhaps it was memory bound and not
CPU bound well how can I take that hunch
and convert into a theory how can I test
it because if you can't test something
then you don't have a theory what I do
is I took that numpy array the 21
columns of four bytes each and I've
split into three arrays one array had
the columns that really needed to be
floats the second array had the columns
that could perfectly be two by dense and
the third array had columns that could
just be single bytes most of them in
fact were just bits very easy to do that
kind of rescrew around with your data in
Python you don't have to worry about
going into a header file and messing
around with headers you don't have to
fight with your Java compiler non-stop
you just do it and it works don't get me
wrong I love Java it's a great language
for bad programmers but if you're
competent you can use Python and get
better results faster again that was not
a joke I'll take the lot and the program
sped up by the ratio of 84 to 36 big
huge huge giant clue so at that point I
went all out for reducing memory
bandwidth I used a radix sort to take
the array of random numbers which are
indices into the main data set that
we're going to use to copy that data
into the temporary data set I took that
random array and I'd been sorted i radix
sorted I loved radix sort I came up with
this in college and then figured out of
course somebody else had invented it 100
years ago but it wasn't until Wikipedia
that I was able to find out what it's
called if you don't believe me wait
until five minutes after this is over
give me a chance to get on Wikipedia and
it will be there the result of the radix
sort is an array the same length as the
number of rows the original data set and
each entry tells me how many times the
corresponding row should appear in the
synthetic data set so if the first entry
in the bin count array the result of the
radix sort has the value 10 it says that
row 0
should be used ten times if the second
entry has the value zero it says row one
should not be should not appear at all
once I have the bin count array out from
radix sort I now have a virtual
synthetic data set I don't need to
actually copy the original data into the
new data I can just use the radix sort
to pretend that I did that so I've now
eliminated one entire rich set of random
reads from the original data and one set
of sequential writes into the synthetic
data set because I don't need to
actually create it this takes advantage
of the fact that row water is
unimportant in our algorithm all we're
doing is summing up rows the order of
summation does not work it doesn't it
does not matter because addition is
commutative I was expecting a laughter
okay we'll just move on I guess not
everyone went through new math in junior
high school good for you
didn't work made me the man I am today a
man who hopes to someday afford long
pants so here's the basic change we take
our random array and instead of just
using it we passing it to the radix sort
we get back our bin counts and we get
all this nice stuff also inside the
inner loop I had that where clause I got
rid of that as well that got rid of more
passes over the data next thing I found
was that because the user time had been
reduced so much via that change the
system time was now significant system
time didn't go up any it's just that the
user time went down a lot and I profile
the code where's the system time going
it turns out that those random number
generator calls were expensive remember
we're calling the random number
generator for every single row times 500
iterations we have millions of rows
times 500 aerations that's a lot of
calls so I changed the code instead of
500 times length of data I made it 500
plus the length of data here's what I
did generate that list of random numbers
but just generate one list not 500 of
them create the bin counts as we sit on
the previous slide
now take the bin count array and append
a copy of it right to the bottom make a
list of 500 more random intz between 0
and the number of rows we're going to
use those to index into this double bin
count array at any random location and
since it's twice as long we know that we
can't run off the end no matter where we
start the result is not as random as if
we did 500 times length of data but
random enough according to both the
analyst who looked at it
and also running the code and getting
the same results again good enough is
perfect let's move on
we're not physicists who want the
correct answer we're engineers who want
an answer that's good enough the word
engineer comes from the same Italian
root as ingenious engineer a is when I
was taught in college
it's a Renaissance word we are ingenious
we don't need the right answer we need
one that's good enough at the other
extreme we could be artists or
accountants where it's what answer would
you like I'm flexing those of you who
left will be audited just so you know
next thing I did was touch the big data
only once this is a little bit tricky I
realized that the original algorithm 500
passes over the original data means
we're going to be flushing all the
caches in the system 500 times as we
swamp them with the huge data sets
coming through not a good move we don't
want to do that so I swapped the inner
loop in the middle loop now the middle
loop is going to go across all the rows
and the inner loop is going to the five
hundred iterations this slide is
oversimplified because remember it's not
500 anymore it's 500 divided by the
number of cores due to parallelism but
to make things fit on the slide I'm just
leaving the number 500 there so on the
inner loop each time around we choose a
new starting location in the bin count
array that was the speed-up I talked
about before and we have some much
simpler code here now this code turns
out to be an improvement because we're
only going over the big data set once
big improvement remember this program is
memory bound anything we can do to cut
down memory accesses is a win I'm not
going to go over this in detail if you
really care about the details download
the slides for these websites feel free
to send me an email later on my email
address is down there at the bottom you
might wonder why I'm using HTTP or my
personal website it's because I upgraded
the firmware in my router yesterday and
now HTTP doesn't work anymore Thank You
Linksys once again notice that this line
down here is really doing a vector times
a scalar and adding the result into a
vector Python really isn't set up to do
that I don't think you can even express
it with numpy maybe you can I might be
wrong but what's really happening there
is an inner inner loop because we're
going to have to iterate across each of
the elements in the current row to do
that so that's an inner inner
but there are only a few elements in the
row remember I said there were 21
elements we're only summing up 16 of
them as it turns out so the cost that
inner inner loop is pretty trivial but
we could unroll it and in a moment
you're going to see that exactly next
thing I did was I contracted with some
folks a continuum analytics in Austin I
met them at PyCon earlier this year what
a wonderful conference that was that's
probably the most useful conference I've
ever been to SIGGRAPH has got more eye
candy but PyCon was terrific I got the
hat I got the jacket I went to all the
tutorials on performance most of which
were good and I met these nice people
from Austin continuum analytics I also
got a chance to go out to Austin a
wonderful city I'm not sure what it's
doing in Texas but it's a great city
anyway they helped me figure out how to
use scythe on for those of you who
aren't familiar with scythe on it's a
program that takes Python code that has
had type annotations added to it and
generate C code from that plain vanilla
Python gets its flexibility because it
uses late binding as much as possible
the latest possible binding that makes
Python flexible but incredibly slow
scythe on does the reverse so I found
converts Python into C code where all
the bindings are done as early as
possible compile times you do as little
as possible runtime a huge performance
benefit but you give up flexibility the
result of scythe on on just my roughly
40 line compute kernel in Python was the
6,000 line file of C code most of that
was boilerplate I found out the parts
that were the actual variable code and I
hand optimized them it ended up being
after getting rid of a bunch of crud 62
lines of C code 62 lines of C code
that's reasonable to get a huge speed-up
which is what I got out of this and this
is the beginnings part would be any part
of that I'm sorry this is the inside
loop at this point in the code we have
selected one row to process and we are
on one of the five hundred iterations
and we have figured out what the current
random value is corresponding to that
row that is how many times should that
row appear in the synthetic data set and
there are only three choices for the
account either at zero which means this
row shouldn't be used at all and that's
the line at the top there if the bin
count is zero continue or the value is 1
or the value is more than 1 I
distinguish between those last two cases
because if it's one I can add the
current row to the
current accumulators and if it's more
than what I have to do a multiplication
as well as an addition you can see that
let's see this block of code here is all
about choosing the appropriate set of
accumulators remember we're doing all
500 iterations in parallel at this point
so I have to choose one of 500 sets of
sums T and sums F and I have to choose
if it sums tea or some Zef so that work
is being done here again I'm not going
to go into great detail on this you can
download the slides of your leisure and
down here I do something which is Nazi
code and is not in the actual file but I
made it look like this would fit on the
slide I'm taking the current count
multiplying it by all the fields and
adding them to the accumulator if the
count is not equal to one I do that if
the count is equal to one I just take
the fields and add them so it looks like
that on the slide to make it fit but if
we go to the next slide you can see what
the code really looks like at the top
there you see if V been count is one
just two an addition otherwise do a
multiplication and in addition this is
not rocket science this is not the sort
of code that should make you no mess
your pants good I was afraid I get a
laugh at that no didn't wanna laugh
scatological humor is not my type of
thing there is something a little
strange going on here if you can read
this code I know if you're past the
third row you probably can't even see
the slide terribly well I'm not adding
up the columns of the current row in
order I'm adding them up out of order
now the first version of this code
straight out of sight on did add up the
columns in order so take the first
column add it to the first accumulator
the second column at to the second
accumulator and so on and I had a
thought
it happens occasionally happens
especially if you are obsessive and
can't stop thinking about this stuff
please put down the computer and come to
bed oh just one more minute
really I'm almost done three hours later
please put down the computer good if you
wait long enough they go to sleep on
their own and that's my thought was that
in that obvious structure of adding up
the columns in order the first addition
is going to go to some line in the l1
cache to pick out the current value of
the first item in the row then it's
going to go to some other location in
the l1 cache to pick out the
corresponding accumulator
add the two together and write them back
into that line in the l1 cache then it's
going to want to add the second column
to the second accumulator and that's
going to touch the same lines in the l1
cache because they're probably right
next to each other no other place they
would be and then we're going to move on
to the next column and probably hit the
same cache lines again how many times
can you do that on a modern system I
don't know but I do know that the
computer inside is not behaving like we
were taught back in college what we were
taught was that the computer executes
instructions in order and that those
instructions are executed against a
simple flat address space where the
location of all where the access time to
all locations and memory is identical
and that's not what really happens on
these machines what's really going on
inside is that the computer is
aggressively looking for runtime
parallelism in the instruction stream
and using that to run instructions in
parallel so if I give it a little bit of
help then it can run these additions in
parallel and that's why I perm you'd the
order here so I do the first item in the
row sums P sub 0 plus equal V field 0 a
1 and then I skip ahead to the fifth
item index 4 because that way when the
computer is trying to get runtime
parallelism from the instructions it
will say oh these instructions don't
touch the same cache line I can run them
in parallel factor of 2 speed-up to do
that required somebody with a fairly
general background and that's my
background I'm a generalist that's a
polite way of saying dilettante which
means I don't know squat about anything
but I know very little bit about a whole
lot of stuff most of the folks in the
audience tonight are probably
specialists that is to say you're
probably really good at something hold
on just one second you shouldn't need to
know this level of detail you shouldn't
need to know about out of order
execution and multi-level caches to get
this kind of performance and that's
going to be much of the focus the second
half of my talk yes sir
mmm advance by four I hope unless I
screw up the slides the cache line is
probably 32 bytes wide if you check the
spec sheet it says 32 bytes wide I have
8 byte floats at this point in the
algorithm so I can fit four of them in a
cache line that's a hold on to a second
that's an example of false sharing and
again I shouldn't need to know this
stuff but I had to get the speed yes sir
it certainly is if you want to get the
highest performance you have to know
what's the Machine you're running on
Python makes that impossible we're too
far from machine Java PHP Perl they're
all wonderfully productive languages but
they take us so far from the machine
that we can't make intelligent decisions
and even if we could we can't tell the
language what we need to do so you got
to drop down into C to do this low-level
stuff I don't know of compiler smart
enough to do this certainly when I
looked at the assembly code coming out
of GCC it wasn't doing this and I was
asking for all the optimizations that
could give me it would be lovely if it
could and I'm going to talk a little bit
more about that near the end of I think
the last slide of the entire show kind
of speaks for itself doesn't it this is
back in 2008 I talked to some folks at
Twitter sitting across the table I asked
the guy now your stuff is written in
what and he says oh we use a lamp stack
it's wonderful it's the way to do things
okay what exactly you're doing well
we're writing a hundred and forty bytes
from your phone to somebody else's phone
okay why are you doing that with lamp
stack well it's the way things are done
I have a hammer therefore this thing
must be a nail I can pound on it and in
San Francisco you can get away with
slaughtering meter maids no jury will
convict it kind of fix you for that but
you can't kill programmers I did want to
ask the guy have you ever called outside
for 1/5 that is a long-distance phone
call what do you think the phone switch
is written in it's either C or Erlang if
it's from Ericsson is probably Erlang or
have you ever flown on an airplane that
wasn't military if it was a commercial
jet the avionics is probably written in
C use the right tool for the job don't
get tied down to one language Python is
a great language I've been programming
in Python for 12 for 10 years maybe 11
depending on how you count if you're a
physicist is 10 if it's an engine if
you're an engineer it's 11
close enough 11
I love Python you will not find a
stronger advocate in Python every
company I've been at for all probably
the last five or six years I have been
the strongest Python advocate but just
because it's a great tool doesn't mean
it's the only tool when I had a
carpenter in remodeling my condos he had
five hammers in his toolbox and he could
explain to me each one he said you
really want to know I said yes he said
I'm charging 200 bucks an hour it's a
200 hour I'm a programmer I don't get
200 an hour he said yeah when I was a
programmer I couldn't get that kind of
cash but he had five hammers each one
did one job very well the same is true
of computer languages use the right
language don't get fixed on a particular
language there are some more things we
could do to get more speed because to
get that hundred and fourteen thousand I
promised you I would have had to do more
than what I did I stopped when I got a
total of about twelve thousand because
that was good enough I'm going to tell
you why that's the case in a moment but
here are things I could do to get to 114
and how do I know it 100 14 because I
did read the specs as you were asking
it's architecture specific I went to
great detail on what microprocessors
were in these cheap-ass servers that are
from a provider gives us and I know how
fast the machine can do it can do
computation so I was still off by a
factor of 10 at least and there's no
reason we should not be able to extract
all of that performance because what's
this algorithm doing it's adding numbers
what our computer is good at adding
numbers so here are some things we could
do first item use fast or Hardware in
the old days we called it a forklift
upgrade in the mainframe days but
nowadays it's not a forklift to servers
this big and we're not even going to
take the old server out we're going to
make it a dev box so just use faster
hardware the problem goes away we can
move on to something new we don't even
have to get involved we could let our
ops team do that life is grand we can go
see Skyfall tip of the day it's not a
very good movie great thing about
working on entertainment company I can
actually go to the movies and say I'm
working now I'm not going to point at my
co-workers but you know who you are I
did not do that
but it really is not a very good movie
those bit valued columns those bite
columns are actually bits you could
combine them all into a single bite and
then pull that data out later that would
cut your row width from 36 to 30 bytes I
actually tried that on some early
versions and it slowed things down which
is surprising because the cost of
pulling the bits out is just a single
and and that should have been hidden
that should be zero cost
it wasn't it'll be interesting to try
that again with the current version of
the code more interesting because it's a
shiny toy and I like shiny things would
be item three use the vector
instructions so every l1 fetch would not
give us one operand it would give us
four or if we use some of the other more
advanced vector instructions I think you
can pull it out at a time I'm not quite
sure how the hardware does that but
anyway we should be able to get a huge
performance benefit because we could do
the work in vector mode big improvement
also the the latter half of that slide
that had the unrolled C loops on it
where I was doing multiply accumulate
there are special instructions for doing
multiply accumulate so instead of having
to multiply bin count by each value in
the row and then adding to the
accumulator as separate instructions I
could do the multiply and the addition
in one instruction the new name for that
instruction set on the Intel chips is
called F MA for fuse multiply accumulate
there are some older instruction sets
that do much the same thing those are
called Mac instructions you might wonder
why the chip vendors would implement
such a weird instruction as multiply
accumulates because that's the
fundamental you think you do in signal
analysis it's a fundamental you thing
you do in all sorts of algorithms but in
high-level languages whether there is
high levels Python or as mid level of C
code we can't really express any of
these vector concepts standard C has no
way to really talk about vectors Python
has no way to talk about vectors we have
to use extension libraries which have
some substantial limits in what the kind
of performance they can give us we could
rewrite the entire kernel and assembler
I have a life I want to do that and to
do that means again to the point the
gentleman the back raised earlier means
our code would really be bound to this
particular computer architecture that's
good for about four years and then we'd
have to think about rewriting it for
some new architecture I'm not happy
doing that kind of work
it's weeks of work to write that
assembler language I'm not sure it's
going to give me much benefit I don't
like the risk of taking on that much
work and not knowing if we're gonna get
results
a couple of completely weird ideas
on multi socket machines each socket has
local ram and can talk to the RAM and
all the other sockets over an inter
processor link we can use Python calls
that bind to the underlying Linux calls
to tell Linux make sure that all of the
RAM for this core is allocated on the
RAM attached to this socket so we don't
have to go over the inter processor link
to get to the data there might be a
benefit from that there might not be I
don't know we could also try around
playing with the scheduler to use
processor affinity maybe that's a
benefit maybe it isn't easy to try I
doubt it's going to provide much benefit
item 7 is even more weird we could port
to the GPU or Intel's new 5 chip
referred to as lrb because the
predecessor is called Larrabee we could
port to the GPU using the Python
high-level library or for even more
performance the low-level GPU primitives
which then binds you to that particular
GPU architecture that would get us a
huge amount of speed but then you got to
get your data center to provision
machines with GPUs not all data centers
are willing to do that and the code
becomes kind of unpleasant to write and
model and understand so that might be a
great way to get more speed if we needed
it the last item on the list is run this
on a cluster and my boss was all over my
butt every couple days run this on
Hadoop run this on Hadoop well bossman
the speed-up you want is this big and
our herd of cluster is big
the folks at Twitter were telling me
recently there had ill Uster is this big
which means that nevermind cluster izing
introduces a whole new set of errors
that we don't know how to deal with we
have an operations team that keeps the
networking layer running and the
operating system they are running and
they have tools to do that
also some monitoring tools and learning
tools and predictive maintenance tools
and it's wonderful at the apps layer we
have a team of people that's us who keep
the applications running and we also
have tools to monitor that some of our
tools are very good some of our tools
suck rocks but at least we have them at
the cluster layer there is none of that
there is no tradition in our industry of
how to manage clusters finding people
who can do it is extremely difficult
read impossible there are no good tools
for it and when we cluster eyes our code
we're introducing a new layer with a new
set of failure modes that we really
don't understand terribly well I'm going
to come back to that in just a few
minutes so why didn't I do any of those
things good enough is perfect at this
point
analyzer was taking five to ten minutes
it's still taking five to ten minutes
the fetcher is taken 15 hours so pretty
clearly speeding up the analyzer anymore
was pointless at 9:00 a.m. the pretty
printer runs to generate reports from
this if we don't have this stuff done by
9:00 a.m. it doesn't really matter well
if the fetcher is the bottom like what
do we do we actually worked on this last
week one of my co-workers and I put a
little bit of time the first thing we
tried was static parallelism fetch a
bunch of jobs in parallel the same
technique that you saw a few slides back
very simple to do the problem is some
fetch jobs run quickly in some run
slowly so that roadblock that I talked
about with static parallelism became a
big issue or we could have tried running
it in parallel we could run all these
hundreds of fetches in parallel I tried
that last year crashed the database I
have worked at three database companies
I've consulted for fourth I do not
understand why in 2012 we are putting up
with this from databases that
cannot do what they've been advertised
as doing I should not be able to crash a
database by sending ordinary requests
over the wire it's okay if it sends back
a failure code to say you're an idiot
stop it but I shouldn't be able to crash
it the database we have is a commercial
database you've probably all heard the
name I'm not going to mention it here
because it's such a piece of crap I
don't wanna give them free publicity it
has been a disaster be very careful
about databases I have yet to come
across one that actually does with the
vendors say one company I worked at you
would recognize their name it's a great
big round name I went out with a sales
guy and he felt compelled to lie about
the product even though the product
already met the customers needs and did
everything they wanted the sales guy
felt the need to inflate the numbers by
a factor of 10 he couldn't tell the
difference all right order other ways we
could speed up the fetcher I'm not going
to go over all these but we could load
directly into numpy those folks that the
continuum analytics consulting firm I
referred to they now have a database
loader that goes straight from the
results set into numpy bypassing the
needless conversion from database
results that primitives to Python and
then from Python primitives back to
machine primitives we could make another
attempt at speeding up the piece of crap
database now that we have a dedicated
resource who's an expert who used to
work at the database company I asked
this person why did you leave and he
answers because I can make a ton of
money selling you people services for
this piece of crap database another
thing we could do completely different
idea is instead of fetching all the data
every day cache all the previous data
and just fetch the last 24 hours worth
of data big win we will probably do that
in the not-too-distant future and then
an even bigger change why are we even
sticking around with batch architectures
this is 2012 it's not 1974 the first
time I used an ATM I took out 20 bucks
and I went into the bank and I asked to
tell her what's my balance what do you
think the balance show did it show the
minus 20 not until 3 o'clock the next
morning with the tellers terminals
reflect the State of the Union because
of batch processing why are we still
doing this this is insane we're building
online systems real-time systems our
tools should also be online in real time
the pretty printer is also a problem
because the batch job was finishing
after the deadline so we fixed that by
speeding up the fetcher we now also run
the pretty printer multiple times a day
in the future we will have the analyzer
or the writer trigger the pretty printer
it has a command-line interface so we
can do that and I have that code working
and then we installed new versions of
pretty printer and they broke their
command-line interface they claim it
works but they actually said the release
notes it works but the user interface is
now completely different thank you so
very very much you broke all of our
scripts no reason why they did that by
the way the release notes didn't say and
the enhanced value you get is blah it's
just somebody came in and change the UI
lessons-learned I got about 15 minutes
left
here are the topics for the remainder of
the talk the first two about are about
computer architecture well I have to
tell you my von Lohmann story I had a
job as a computer technician at the
Institute for Advanced Study in
Princeton this place where Einstein
spent the last part of his career and a
bunch of other smart people I was being
given a tour of the school of Natural
Sciences where I was doing my technician
work and the guy giving the tour
professor Bacall took me on the library
and I said isn't this where john von
neumann invented about half of modern
computing alan turing to the rest and
this is a gross oversimplification but
it's ok and professor Birke all said
yaha he did his work here I said
shouldn't there be a memorial the call
pointed to the other end of the library
said yeah oh yeah so I go and I take a
look in behind the bookcases a little
plaque I said why why why are you hiding
the plaque
and the call looked at me and he said
Von Neumanns work had practical
applications I'm also going to talk
about scalability why it is generally
not the way to get programs to run fast
and why I believe we are being led down
the garden path with the best of
intentions towards scalability and it is
killing us as an industry I'm going to
be it on batch jobs a little bit more
partly because they're stupid and partly
because I have a really good slide and
sharp tools for cutting is going to be
the the launchpad to the last slide so
let's talk a little about computer
architecture this is what we were taught
in college here's the memory hierarchy
it ends with disk and tape that's not
the way computers have worked since the
1980s for microprocessors the 1970s for
many computers in 1960s for mainframes
so that was a lie when you were taught
it it was never true for anybody in this
room unless you are really really old
this is the real architecture I've taken
off the bottom two layers discs and tape
this is what's really going on in terms
of memory hierarchy the second item from
the top physical registers that's there
to allow the microprocessor to do that
aggressive extraction of parallelism
from the instruction stream at runtime
it allows instructions to be executed
out of order it allows instructions to
go both ways down a branch even before
we know which way the branch is going to
go and then throw out the path that
should not have been taken it allows us
to implement hyper threading it's
wonderful everything else below that is
for hiding memory latency that's kind of
interesting I was wondering why is
latency such a big deal so I looked at
the experience I've had programming
computers not for 30 years but actually
coming up on 40 thank you so much if you
want to feel old go to the Computer
History Museum in Mountain View and
you'll feel fine until the last five
minutes when they bring you up to the
current day and you look at some antique
computer and say so that was the one I
learned on oh crap I feel old when I
started doing this stuff in the 70s
early 70s memory and CPU were equally
matched in a single instruction cycle we
could do two memory accesses one for the
instruction one for the data work
perfectly over the intervening time
memory got 10 times faster that's a
logarithmic scale on the y-axis memory
got 10 times faster a little bit more
CPUs got 3,000 times
hence that huge gap off on the right
about a factor of 300 times so yeah we
got to hide memory latency the memory is
slow so what does all this tell us it
says that Ram is not really random
access memory anymore it's not true that
all locations take the same amount of
time to access it depends on where
you're accessing them and how recently
adjacent locations have an access to
pulling the thing you want to the cache
wouldn't it be cool if your program
could give a hint to the CPU to say hey
CPU in a few instructions from now I'm
going to need this location so why don't
you start fetching it now a prefetch
instruction you can do that but Python
can't allow you to express that and C
does not allow you to express that
either there are a bunch of instructions
like that that the machine has but our
languages do not let us get to so we're
giving up a lot of performance that the
hardware can actually provide the lesson
from this first line here is go ahead
and use memory just don't read it or
write it in particular avoid random
reads go sequentially because Ram is
kind of like disk now random reads are
ten times slower than sequential reads
and try to avoid writes when you do an
occasional write here or there there's
that's fine the chip has a special
buffer called a write buffer to hide the
fact that writes are very slow but if
you do large vector writes the write
buffer fills up and after it fills up
which occurs very quickly it's a small
buffer then all succeeding writes will
run at memory speed and your CPU slows
to a crawl
access vectors in blocks that are l1
cache friendly you really need to start
thinking of your program not as having
access to a giant array of RAM because
it's so slow but really in terms of
having access to the l1 cache if your
work doesn't fit in the l1 cache
performance is going to go down the
drain you're going to be running at l2
speed or l3 speed or worse Ram speed try
to use hardware primitives which is why
we use numpy instead of these beautiful
fuzz balls that we get from python where
how big isn't it now it's 80 bytes for
NIT something like that they provide all
sorts of wonderful services to us but
for high-performance computing we can't
afford the overhead the machine has high
speed instructions for its primitives we
need to stick with the primitive so
those instructions can be used write
code usually in C that takes advantage
of knowledge of the out of order
execution and the nature of the cache
verses RAM and touch the big data once
only it's a big wind because memory
overhead is such a problem another
lesson is that speed has value for a
project that I cannot describe in any
depth we spend we think about 300,000
month on servers if we could speed up
that code not by a hundred thousand or
ten thousand or 1000 but just by a
factor of a hundred we would free up 297
thousand dollars a month that would pay
for a lot of programmers or a lot of
additional game developers or if I got
10% of it I could afford long pants so
if we start thinking about performance
as something that we're responsible for
instead of just saying to the operations
team oh add more servers in the problem
will go away because our stuff is
horizontally scalable we lower our
capital expenditures or operating
expenditures would also lower our co2
emissions so maybe New York would not
have to drown in the next storm that's
funny you weren't watching TV I have
friends who live there and it was not
very pleasant for them nevertheless and
clusters are hard anybody remember that
old Barbie with the pull-string you pull
it and she says math is hard
while clusters are hard over on the left
is a cellphone snapshot of a piece of
paper attached to the whiteboard in my
group Business Intelligence with the
five prime directives for bi Captain
Picard got by with one we have five and
off I'm left for each one of those five
prime directives an example of where we
failed to meet that directive because
clusters are hard I'm not going to go
into details you can read the slide as
well as I can and send me an email
something's not clear the point is we
don't know how to make this stuff work
very well batching why are we still
doing batching I love this picture I use
it in every slideshow so the reason I'm
using it here is to beat on batching but
really even if I had no reason I'd still
show you this great photo you can sort
of see the pig standing out in front of
the Python one of them is involved one
of them is committed
thank you two shows nightly bring your
friend tip your waitress when you do
batch programming jobs might fail and
they might fail silently in which case
you find out when you come in in the
morning and your customers or clients
have sent you emails saying or they
might fail loudly and then you get a
3:00 a.m. phone call in my case was a
5:00 a.m. phone call and my boss
actually wanted me to call other people
on the team and wake them up at five
o'clock in the morning and I said new
not unless you pay me like a manager
that's what managers get paid for I got
that phone call because we happen to
have one of our guys in China visiting
his significant other and he happened to
be monitoring our system because what
else would you do when you're in China
visiting your significant other so he
called me to tell me there was a problem
with a batch job don't like those
late-night phone calls and if you're
doing classical batch processing which
starts at midnight and runs until 8:00
a.m. or so you've just given up a lot of
your compute time so you're gonna have
to have more servers to get the work
done in the length of the batch window
and of course batch jobs add latency you
submit a request and at some point in
the future you get a response by which
time you've forgotten what the question
was
batch jobs are 1970s technology we
should not be doing them so fast code is
cheap code save money save the planet
your machine is insanely fast the
computer that I'm using it my employer
gives me 80 billion instructions a
second before hyper-threading out and
hyper-threading is probably about a
hundred billion instructions a second
how many instructions do you think it
takes to move hundred and forty bytes
from my cell phone to your cell phone do
you think you could do a lot of those
messages per second if you had a hundred
billion of them hundred billion
instructions per second to do I kind of
think you could or you could scale it
out as Twitter is done the Twitter
cluster according to the guy in charge
of the group that keeps their servers
running I talked to him a few weeks ago
their cluster is a hundred thousand plus
servers now sure not all of them are
moving messages from point A to point B
maybe it's just 1% maybe it's just a
thousand machines I wrote the same code
in 2008 it conformed to the Twitter API
it handled the 2008 volume on one box
not a thousand certainly not
thousand last week after the election
anybody here you know we had an election
right Twitter put out a blog post or
press release disguises blog post
boasting that their system had not
crashed on election night that's kind of
like a car company putting out a press
release saying our cars do not explode
when driven on roads by people the fact
that they put out that blog post tells
you how close to the hairy edge they are
of falling over because they're moving
140 bytes from my cell phone to your
phone and that's a hard problem
makes them get all scrunching inside
speed up your code first if I can write
code in a week's work to handle the
Twitter message volume conforming to
their api's on one box so can a cluster
eyes as a last resort when everything
else is failed because cluster izing
introduces so many problems you don't
want to deal with if your business is
real-time your software should be to
batch mode is convenient for programmers
but it produces lousy results for our
clients because it introduces latency it
means we have to use more servers to
make up for the limitations of the batch
window and it makes things fail when
we're asleep online code fails when
we're awake that's a better time to fail
and make sure your tools are working
correctly remember that problem we had
with numpy on my slides earlier on where
numpy was being passed a mixture of
string float and int and therefore it
couldn't do a good job so we have all
these wonderful tools make sure they
work correctly another example of that
in our case is this piece-of-crap
database we've put all sorts of tables
in it over the year we've never
optimized it I've worked as I said our
other database companies with the same
problem the database gives you all sorts
of ways to optimize your stuff and
they're too hard to understand it's kind
of like you you're put in a garden with
a hundred pass in front of you at the
end of one path is a gorgeous prince or
princess or possibly the sheliak
corporate entity if you're in that kind
into that kind of thing
Star Trek reference how many people got
it one hand way okay I'll see you later
and at the end of the other 99 paths our
Tigers are gonna bite your face off and
you can't tell which is which so we have
this database we haven't tuned it of
course the performances garbage make
sure if you're using sharp tools that
you're using them correctly and not just
using them to pound dirt one more thing
hey I'm going to end on time
one more thing I've been doing this as I
said for quite a while and one
unrelenting theme over the 40 years I've
been doing this I fell in love with the
stuff when I was 13 friend of mine
showed me how to markup optical mark
cards they're kind of like the old punch
cards but number two pencil you mark
them up in the cafeteria and I was
hooked and by the end of the week I knew
what I was going to do the rest my life
I'm very lucky that way back in the
1970s we had computers in our junior
high school because one of the teachers
wanted to do that and she was married to
the mayor they tried to cancel that
program after a year and I went to the
Board of Education and I was like 14 or
something and I said at the public
testimony I said you should provide
money for this and I gave him a list of
bad reasons and they did afterwards I
asked one of the board members I said
I'm only 14 but I'm not stupid my
arguments were terrible why did you give
me the money and I said we cut the
budgets of dozens of programs and no one
cared to show up at you so you got the
slush fund okay if you don't ask you
won't get so for 40 years we have been
coming up with better languages better
operating environments better tools and
in the late 90s we took a turn a bad
turn it was the dot-com boom and we were
very concerned with making websites that
wouldn't fall over if they got very
popular I don't know if anybody here
remembers that eBay fell over a lot when
it got popular early on so the industry
consensus became programmers don't worry
about productivity about performance
just worry about horizontal scalability
and then we can throw the performance
problem over the wall into ops and they
can improve performance by just adding
machines scale it up scalability is the
solution but here's the thing as
programmers we have dozens of tools in
our toolbox for performance I've shown
you a few tonight all we gave the ops
team was one tool add more computers
what's the result the result is Twitter
has over a hundred thousand servers in
its data centers and that's just insane
they're not the only ones Google's
building huge data centers Facebook
Apple Microsoft lots of other companies
including some I can't talk about our
building data centers New York Times was
running out an article a couple months
ago about how data centers now use a
significant fraction of our nation's
power which means we're burning coal or
oil to power these things you know what
happens when you burn coal every time
you turn on new coal power
plant you condemn a couple hundred
children in the world to die from asthma
we should not do that
I say this because I used to work at a
company that did high-efficiency
lighting and that was part of the
argument that I made there we don't
really want to be burning coal we don't
want to be building data centers and yet
our data centers are filled with racks
of servers we're running out of power
running out of chilling capacity running
out of physical space we should not be
doing this efficient code is a
competitive advantage and as programmers
we should take back from the ops team
the responsibility that is rightfully
ours to write high-performance code to
do that we cannot have people in the
situation that I was for the speedups I
talked about tonight you should not have
to know this low-level mind-numbing
detail about computers I like this but I
have no social life never have tried it
once didn't like it
ugly bags and mostly water as far as I
can tell anybody get that joke thank you
which series thank you my boss when I
was making fun of before he said he saw
Star Trek and I said Oh what do you
think what do you like about he said oh
I love Star Trek with Captain Picard
that's you mean there was another series
before that we need to take back the
stuff that we know how to do well that
we have the tools to do but we need
better tools to do that
and one tool in particular that I would
like to see in Python is a
high-performance numeric vector class so
if you give Python an expression that
says y equal ax cubed + BX squared plus
CX Plus D it would know how to do that
efficiently using the underlying vector
instructions knowing how to block things
in the cache knowing how to schedule
things as efficiently as possible on the
machine in question it could do the
stuff that I had to do by hand now you
might say why a numeric vector class why
not a generic vector class and the
reason is the underlying machine doesn't
know from generic it knows about numbers
Intel and AMD and IBM and arm have added
the vector instructions because those
are the things that clients need to do a
lot of on computers we have huge amounts
of data flowing into our systems now
whether it's sensor networks or social
networks massive amounts of data coming
in it's going to be processed in vector
form the hardware can support the
vectors but our languages aren't up to
the task so if you want to get the kind
of performance
got here either spend 40 years like I
did learning this nonsense and making
fun of database companies and Twitter
especially it's easy to do Twitter or
let's press the language vendors in
particular Python because it is such a
marvelous language for everything else
to give us the tools we need to take
advantage of the underlying hardware
this 100 billion instructions per second
machine that you can get off-the-shelf
not fancy hardware if you need more
compute by the way you can get an eight
socket motherboard and double your
compute power if you need memory you can
get two terabyte machines off-the-shelf
not fancy stuff you go beyond that yeah
then you're going to get reamed but 2
terabytes and a hundred billion
instructions per second that's a fair
chunk of compute power let's find ways
to make use of that effectively instead
of trying to scale out and fill up our
data centers with rack upon rack of
cheap boxes thank you for the
opportunity</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>