<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Go Programming Language | Coder Coacher - Coaching Coders</title><meta content="Go Programming Language - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/InfoQ/">InfoQ</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Go Programming Language</b></h2><h5 class="post__date">2018-04-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ADT25tuAx4A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much so
as cover you said hello my name is David
I'm a software developer I live in
Sydney Australia I'm also a bit of a fan
of go I've been privileged to be able to
call go my full-time job for the last
six years also help organise go for con
every year I've been involved in the
local Sydney Meetup
and I travel a bit to talk and teach
about go so this is going to be a
practical session it's going to be lots
of live coding and demos so there's
ample opportunity for things to go
terribly terribly wrong so if you can't
see the screen you might want to move
forward just to see my mistakes and
today we're going to talk about the
techniques for writing high performance
go we're going to focus on three areas
just start my timer three years in its
presentation benchmarking performance
measurement and profiling and a little
bit about memory management and the
garbage collector and really the goal is
for you to take away tools and
techniques that you can use to improve
your own go programs alright let's jump
straight in so before you can begin to
tune your application you need to
establish a reliable baseline to measure
the impacts of your changes you need to
know if you're making things better or
worse so in this section we're going to
focus on how to construct some useful
benchmarks using the go testing
framework and we're going to give a few
practical tips for kind of like traps
for young players along the way and
benchmarking and profiling are closely
related so there will be a little bit of
overlap but we'll talk more about
profiling in the following section there
are a few ground rules for benchmarking
and they're mainly about getting a
stable repeatable bent baseline first of
all the machines got to be idle this
this seems obvious so that means
profiling on shared Hardware may be
unlike the staging or dev server it's
probably not a good idea if other people
are using it similarly you know like
don't start a benchmark and then like
watch YouTube in another browser
in a browser tab or something like that
as much as the hardware manufacturers
would like you to believe that cause
independent isolated the reality is less
true for that so really if you're
running your benchmark don't touch your
laptop more importantly is power saving
and thermal scaling this is the this is
the other side of intel's turbo boost
and things like that while to abuse
basically takes advantage of thermal
headroom in the processor and ramps up
the clock speed Thermal scaling is the
opposite once the processor gets too hot
not just the fan has got to come on but
the actual clock rate goes down and you
can see that in your benchmarks
initially the same code will start to
run faster for a little bit until your
laptop gets really hot and then it'll
get cold it will get slower probably
below the original baseline and avoid
virtual machines and cloud hosting for
the same RIP for the the same reasons
there they're usually too noisy to get
consistent repeatable results if you can
afford it at several companies we've
bought dedicated performance test
hardware we've stuck it in a rack we've
turned off as much of the power
management as we can and we set the
thermal scaling to some kind of minimal
baseline and we never update the
software in those machines now this is
terrible advice from a security point of
view but probably reasonable advice from
a performance point of view because if
you update the software if you update
the kernel you're no longer comparing
apples to apples and fundamentally for
everybody have a before and after sample
and that generally means running them
multiple times so I should have asked at
the beginning of this presentation
who here has has used go I can't show
hands excellent so keep them up if
you've written a test good keep them up
if you've written a benchmark okay a few
few less than a written test writing a
benchmark and writing a test are very
similar in a test you're passing a
testing dot T in a benchmark you pass in
a testing B and this little this little
benchmark function this loop runs you
function in this case fib of 20
B dot n times what's the value of b dot
n starts at 1 and if that benchmark
function completes in less than a second
then the benchmark framework starts to
ramp up the size of size of B dot in
follows a roughly you know 1 2 3 5 10
type progression but if but it can it
can skip some so as an example let's
have a look how's that for size no and
see that a little bit bigger so on this
laptop on this day and this temperature
at this altitude it takes a it's not a
joke um my benchmarks run significantly
faster it's the coming into the summer
in Australia and when I do this
presentation about six months ago I get
about fourteen anisette forty
microseconds now in the summer we're
getting fifty five milliseconds that's
the difference of thermal scaling makes
so for this example we did thirty
thousand iterations and average time of
fifty four microseconds so a few other
kind of tips about about writing
benchmarks when you get down below below
the microsecond mark like when you're
down in the nanoseconds you're going to
see some instability mainly because of
the relativistic effects of instruction
reordering like good tree what your
processor was doing before it got to
your code is going to affect the the
observe runtime of that code and also
things like code alignment like
littering recompiling your program may
change the runtime only you know when
we're talking in nanoseconds so these
are these are relatively small amounts
of time at the other at the other end if
you have a benchmark that instead of
taking microseconds or milliseconds you
know it takes takes more than a second
or something like that you might want to
run your benchmark for longer say 10
seconds just to get more sample so you
have a more reliable average and as a
takeaway you shouldn't if you need to do
things like this you shouldn't let them
be tribal knowledge like
communicated like communicated in slack
or like hidden in some wiki you should
codify them in a in a make file or
whatever tool of your choice so that you
have I could make a bench target and
everybody uses invokes the benchmarks in
the same way so as I said for repeatable
results you should probably run your
benchmark multiple times and it's very
easy to do that with the count flag
let's do that example now so I'm just
going to run it ten times pipe the
result into a text file which will
become obvious in a second you can see
even now the numbers are changing
slightly in in a wider workshops right I
talked about this more I can afford I
have the time to run it longer and you
can actually see the thermal cycle is
the process that heats up and then slows
down so now we have our before sample we
have ten invocations of fib of twenty
let's improve fib and try to do a
comparison
so at the moment we memorize the zero
Fibonacci number
the first Fibonacci number we could
improve this code just simply by just
memorizing the second Fibonacci number
which is just someone yelled out the
second Fibonacci is to one if I have
more time there's actually a test we can
talk we can talk about that but so I
pick on Fibonacci because it's a it's an
easy recursive algorithm you can
demonstrate significant significant
improvements so here just by memorizing
the second second Fibonacci number we've
improved the performance how much we'll
find out in a little bit so this is the
process you should be going through with
all your benchmarking grab a nice solid
before of a full result and I sort of
after result and then you can use a tool
like Russ Russ Cox's bench stat which
does some nice statistical analysis on
it
let me just
okay and that is a pretty good run so
that the before sample has a variance of
about one percent that's excellent
the after sample variance about four
percent that's tolerable and we can see
we've improved this this run time by
about thirty seven percent it's got a
really good confidence value like P is
virtually zero and all the samples both
the before and after that's their N
equals 10 plus 10 would in that range
because generally with you know
collocations and things like that one of
the one of the very first runs of your
benchmark will probably be well outside
that range and so it's not uncommon to
see like comparing nine to ten so the
benchmark framework takes care of just
removing those completely irrelevant
outliers so that's the basic mechanics
of writing and analyzing benchmark there
are a few other things that you might
want to do when writing a benchmark and
one of them is you probably have a bunch
of setup you need to do like load some
data off disk compute some tables a
bunch of kind of setup which is
necessary to execute the function under
test but you don't want that time to
count against a sexual runtime so you
can use this reset timer function which
basically just resets the clock back and
then we go straight into the loop
similarly if you have some expensive set
up logic to do per loop iteration which
might be a sign that your benchmark
you're trying to benchmark something
that it's too large like if it has too
many moving parts they all need to be
reset on every iteration then you might
need to think about refactoring your
benchmark but with that said there's
quite a few cases where we do this
inside the standard library so it's by
no means extraordinary and easiest way
is to you can use stop timer and then
later on start timer to effectively
pause the clock why you reset some reset
some data structures now there's been a
lot of talk about garbage collection and
allocations and it's absolutely true
that allocation count and the size are
strongly correlated with benchmark time
like allocation cost time
obviously the easiest so you want to
kind of you don't want to know just how
fast your function runs you want to get
an idea for how many allocations it's
making because that may be a clue to how
you can improve its performance the
testing framework gives us a mechanism
to do this using B dot report Aleks
which basically records the number and
average size of the allocations for the
function and so to look at that let's
try an example this is a kind of
contrived maybe you think of like
logging some some kind of logging
example we have a request ID the client
address is a string and the current time
is a string what it actually does is not
really important for this example and
here are effectively four different ways
of constructing the string the first one
just use good old string concatenation
the second one we make a bytes buffer
and we use the F printf version in the
font package to write into it and then
we turn that buffer back into a string
this third example we use s printf which
externally you might think why it's kind
of basically the same thing s printf is
just doing what the example above is
doing for you and the last one is kind
of like the the loop unrolled version we
make a buffer we know is the right size
because we can control the size of
request ID we know the time of you know
the size will form at a time we know the
client IP address is fixed to a certain
size so we can make a buffer of exactly
the right size we append the bytes of
the request ID space the bytes of the
client address string another space and
we really kind of push the boat out with
the append format from the time package
which rather than returning time as a
string it will write it into this
pre-prepare buffer that you give it and
then finally we confer convert that
buffer back to a string and what is
somewhat off the end of the slide is a
benchmark for this now I would ask for a
show of hands who thinks it's going to
who thinks which is going to be faster
but in the interest of time let's just
find out
so as I can catenate benchmark and we
just make sure that fits on the screen F
printf s printf and lastly string
conversion which is the kind of unrolled
version and not surprisingly the
unrolled string converge version is both
the fastest in terms of both the time
and the allocations you can see it only
has five allocations what's a little bit
surprising is that there can the dumb
old string concatenation version which
if you come from Java in the world of
string builders you're told never ever
ever do turns out not only to be the
second fastest but has the second fewest
allocations so this kind of goes back to
one of my opening points which is you
know maybe don't guess measure so that's
an example of benchmarking allocations
as well as just wall time now this is
one of my favorite examples this came
from an issue which was raised you know
18 months two years ago when we bought
the GNU Compiler back-end online which
is much a much better code generator
this is a population count function for
those who haven't used such a thing it
basically returns the number of bits
that are won in this UN 64 so the
population of one bits in this bit set
and the bug was that
why sure I won't tell what the book is
I'll show will run the benchmark and you
figure out what the bug is yourself so
it's just got a regular benchmark for N
equals o for each B dot n we're going to
run pop count just of n it doesn't
really matter what the input is because
we don't care what the output is so
let's run it
screaming okay so it says that pop count
function can run in point three five a
nanosecond this is a two point something
gigahertz CPU so that's kind of telling
me that that instruction takes one clock
now I know it's fast but I don't I don't
believe that number I think that that's
kind of too fast so what actually
happened pop count is what we call and
go a leaf function which means it has no
it doesn't call anything else it is the
it is the leaf on a call tree and
because of that it makes it a really
good candidate to be inlined
and by inlining it now the compiler can
see that bench pop count and pop count
itself have no side effects
they make no change to their arguments
passed in and they affect no global
state so effectively it can be
eliminated you can't prove that I didn't
do that work so I'm not going to do it
because you can't prove it and this is
effectively what the compiler sees and
Intel CPUs are really good at optimizing
empty loops so that's effective what
we've timed which about about one cycle
to do go around this loop doing nothing
and the the reason I like this example
is that the same optimizations that we
want to make our code fast the dead code
elimination constant folding loop
hoisting all of these things which take
regular easy to read bog-standard code
on the page and finally find the
inefficiencies and eliminate them are
exactly the same optimizations that
remove the extraneous bits of this
benchmark and made it run ridiculously
fast so I put it to you that this is
actually a feature not a bug
but having said that this benchmark is
useless
so let's talk about how to fix it so
fundamentally the problem is that this
benchmark and I'm sorry I think that's a
little bit cut off from that screen this
benchmark has no observable behavior we
don't use its return value so the first
thing is probably to do something like
this so at least capture the result of
pop count just as a show of hands who
thinks this is sufficient to make the
compiler happy
not one single hand that's excellent
because this is not sufficient because
our is still local to this call frame it
still is not global state it you you
can't prove that you ran it because our
ceases to exist when this function
returns so we need to put the result
somewhere that the compiler cannot prove
that somebody else can't see it and the
easiest place for that the best way to
fix your benchmarks if you hit this
situation is something like this we
don't need to store into capital our
result every time around the loop just
doing it at the end is sufficient but
because this is a public packaged
variable there's no way for the compiler
or any kind of code analysis to prove
that there isn't another package in this
program which is observing that variable
so we can no longer just eliminate this
as dead code because it's not it's
actually changing some global state and
just to prove that for you we run it
yeah it takes about 2 nanoseconds which
is still pretty fast because it's a
pretty decent implementation but that's
more what I would expect that's eight
that's eight to ten clocks a lot of them
overlapped all right dunno can time
so testing be is useful for writing
micro benchmarks micro benchmarks are
useful for tuning the performance of
like a single hot piece of code we use
them a lot through the standard library
because the standard libraries which is
the code which everything else is built
on so we want that to be efficient but
it's impractical and more importantly
probably unreliable to construct a
testing dot B for like your main domain
like like you can't you can't reliably
do a micro benchmark for your entire
application it just would just be too
variable so we should talk about other
ways to profile whole programs rather
than just little pieces of them and in
this section we're going to explore two
profiling tools built into the go
runtime the first tool we'll talk about
is people off has anybody used people
off here good good excellent paper off
has been in go since the Year dot since
it since it came out and it consists of
two parts a piece in the runtime which
is like the kind of runtime support and
a piece run outside which is what we
called go tool paper off which is for
analyzing the results that the runtime
bit produces it talked a little bit
about the different types of profiling
we have available the most commonly used
one and what one that people probably
think of the most is CPU profiling when
you turn on CPU profiling the runtime
itself will set up with the operating
system a timer interrupt and every time
that interrupts fires it just writes
down into into the profile the stack
trace of all the currently running go
routines
when the profile is complete it's pretty
straightforward to just count up the
number of times a particular stack trace
appears and then therefore that that
stack trace that appears the most in the
profile is the one that was using the
most CPU memory profiling records a
stack trace whenever a heap allocation
is made so that is it records the stack
trace that led up to an allocation
in this model stack allocations are
basically assumed to be free and they
are not tracked in the memory profile
now memory profiling kind of like CPU
profiling is sample based by default we
sample every one out of every thousand
allocation when you turn this on you can
if you like reduce that reduce that rate
down to one out of every one a location
and something that I put here as a note
is that because it's sample based and
because it tracks the allocation not the
lifetime of of its use memory profiling
is not really
gos equivalent of like a heap dump it I
we have many cases where people reach
from every profiling like I have some
memory issue in my program I don't know
why it's using so much memory
fortunately the memory profile is not
going to give you that information or
not directly in the same way that heap
profile was would just quickly some
other supported profiles these are the
more esoteric ones block profiling is
similar to CPU profiling but it records
the amount of time a guru teen spends
waiting to run not running so it can
show you where a large number of
goroutines could make progress but were
blocked for some resource and they show
you the resources they'll blocked on
resources like that could be sending on
receiving on an unbuffered channel so if
the if you're trying to send and the
receivers not ready you have to you have
to block if the channel is full or if
you're receiving from empty one similar
situation and trying to lock something
like a sink mutex which is already
locked by somebody else talking about
meu Texas we have a profile specifically
for recording the stack traces of the
holders of contended mutexes so you can
find the Cole path to a hot lock its
recreation profile records the stack
traces that resulted in the creation of
a new OS thread it's pretty much common
knowledge that go multiplexers many guru
teens onto a small number of threads but
there are instances where you need to
need to either take or create a thread
to do some long blocking operation at
syscall some sis calls
Sego things like that and so if you have
a problem all of a sudden the number of
threads in your application spikes this
is the profile for you really the
takeaway is that on this slide these are
quite specialized tools you shouldn't
reach for them straight away you should
start with CPU memory usage speaking of
that profiling is not free it has a
moderate but measurable impact on your
programs performance and especially when
you do things like if you increase the
the memory profile sample rate will
basically grind your application to a
halt if you enable multiple profiles at
a time and this is kind of like a trap a
trap for new players you say I have a
problem with my application just give me
all that I turn them all on I want to
have all the data and I will figure it
out what you'll end up with is just a
bunch of profiles that report the other
profiles that were running so the the
simple answer is don't turn on more than
one at a time so the runtime the
interface to turning on to working with
profiles is inside the runtime paper off
package it's quite low level and for
various historical reasons the
interfaces between the different kinds
of profiles that I talked about are not
uniform a few years ago I got kind of
frustrated with this and I wrote as any
engineer would a little wrapper that
kind of solves this and this makes it
easier to profile your application it's
just easy as just adding one line either
unconditionally at the top of Maine or
maybe you want to put it behind a flag
or something like that and then
importing the package it also takes care
of things like capturing ctrl C so that
profiles written out cleanly if you quit
your application and also the kind of
API won't let you turn on more than one
profile you turn one off the first one
will be turn one on the first one will
be turned off so as a demo let's pick on
everyone's favorite program go doc and
profile that
if your vim user everyone should be
using fattier sans
vim go package you get going ports for
free you get quit on compiled unsaved
for free all that stuff so now we've
added a profile in it go doc literally
one line we know how a version go doc
which records a CPU profile anytime we
run it so that's it now go doc is doing
it standard HTTP thing if we cancel it
there's a little handling and handler in
there that caches and writes out the
profile before it quits and how do we
look at a profile have a look at profile
fantastic it's on the next slide now
we've talked about it what it can
measure let's look at the results we get
one of the really nice things that have
been waiting for for a long time in go
1.9 you no longer need the binary and
the profile as long as you have the
profile that is all you need this is
going to make such a difference in in
SRA and operations that you no longer
need to have the matching binary to a
profile that was produced by a program
if using 1.8 or earlier unfortunately
both just upgrade to 1.9 save yourself a
lot of trouble so
use go tool paper off and pass to the
profile and let's find out what the top
talkers are what are the top things that
go doc was doing um all I see is run
time stuff so like some syscall a bunch
of stuff which seems to be about the
garbage collector like there's no go dog
code in here what is what is going on
and when I talk to talk to programmers
and try and talk about profiling it
seems quite that natural of people to
use this interface but I find the
graphical interface to be much easier to
interpret so this is the same profile
but as a graph now I'll try and make it
larger for the screen here and this is
it's just called dot Cisco yep it
absolutely is the top talker but Cisco
doesn't call itself it's being called
from other stuff so let's trace back up
Elster there's some reading directories
okay so now there's some stuff about
reading directories and yeah we're still
reading looks like we're walking through
directories now this gives you a little
bit of clue about what go doc is doing
and let's not forget that this is the
Godot um the this is the version go dog
that you run on your laptop so you have
your documentation so the first thing it
does when it starts up is walk through
your go path read all the files and you
know built build tables and stuff and
this is exactly what we see here all of
the time in the first couple of seconds
so the first you know nine or ten
seconds is built walking the directory
and building building this file tree so
we talked about this is the graphic
so what we saw in people OFF kind of
matches what we know it does
you can also very easily if you
structure your benchmarks as testing be
benchmarks you can get profiling from
them for free just past CPU profile and
profile block profile or all those ones
so in this example here we're just
running the tests for the bytes package
and capturing the CPU profile on the way
out the the the tip here is the CPU
profile is going to run for the whole of
that testing binary but we don't really
want to like actually get profile data
for the tests we just want the
benchmarks so hopefully everyone knows
that run and bench reg X's so xxx
matches nothing in the bytes package and
dot matches all the benchmarks so that
basically says don't running the tests
just run the benchmark functions that
match this reg X so that's the easy way
to get a sleepier profile for some or
all of the benchmarks in your in your
package alright pretty good ok the
second profiling tool I want to talk
about is the execution tracer this was
added in go 1.5 and remained under
documented for a little bit until it's
there have been a lot of good
presentations recently over the last
year 18 months an unlike sample-based
profiling which we looked at before
because the execution tracer is
integrated into the go runtime it
doesn't just know what the program was
doing like any sample-based profiling
can do that it knows why it gets
nanosecond precision for go routine
creation start/stop why they started
what started them what stopped them
blocking and block unblocking events
it's integrated into the network polar
so it knows when goroutines start and
stop and interact with the network
syscalls obviously and a lot of detail
about the garbage collector there are a
few few Cabot's when using the trace
tool the visualization tool that we're
going to see in the next slide
actually reuses a lot of mechanisms
built into chrome it's actually part of
the chrome Java JavaScript debugging
stuff so unfortunately it's not going to
work on Firefox Safari ie sorry about
that my life was significantly improved
using this tool when someone told me
it supports WASD to move around you do
not need to use the mouse and especially
because it's JavaScript there's no right
right mouse click support anyway so WASD
super important to know and question
mark just like all Google products will
give you the list of hotkeys and the
last one is that viewing traces can take
a lot of memory like seriously
for gig is not going to cut it aching is
probably the minimum and if you're
analyzing some very large traces because
I don't forget it's going to load all
this crap into your browser via
JavaScript you you need a lot of memory
that's one of the downsides at the
moment so let's do a demo let's pick on
go dock again and instead of doing a CPU
profile we'll grab it we'll grab a trace
profile
and we'll just run go doc you know that
about the same amount of time so we want
to want to see what it looks like we saw
a paper off type profile let's have a
look an execution trace - now we have
the trace we use a slightly different
tool go to trace and the profile and
because this is reusing kind of
JavaScript stuff built into Chrome
what's actually happening in the
background is this slide is running a
little server - like a spoon feed data
into into Chrome so what we see here is
that the trace has been split into two
sections like there has its roughly a
limit on the number of events so the
larger the longer your trace transform
the number of the more events that are
in it is really the size of the
splitting let let's pick this one that's
not very interesting at all
well it is but for not reasons that are
worth going into now let's pick this so
we're going to be looking at the same
visualization that we saw here but
rather than looking at in terms of in
terms of course tax we're going to be
looking at it as a function over time
just kind of make this big enough so
from the top to the bottom we have a
list of a list of go routines and so at
this point this point in the execution
we had for running go routines which
matches the four processes here we had
about 54 that were runnable but hadn't
been scheduled yet so quickly at a
glance you can see how many great things
are running in this program the heap is
the next one and it follows that
traditional stair step garbage collector
pattern threads also show you the number
of operating system threads actually in
use in your go program interestingly out
in this error here we can see it kind of
like a sometimes the depending on when I
run it sometimes the spike is much
larger but this is this is spike in
running threads that are not actually in
the go runtime they're off actually
doing thread stuff now you can see
highlighted here this next row is
talking about garbage collection stuff
that's when the garbage collector is
actually running rather than being in
the background and the the these last
four columns represent the four
processes or the props or the pieces you
might think about them and there's
really two parts to this there's the
kind of top part which is your code and
this is the bottom path which is usually
runtime related stuff garbage collector
stuff perhaps even though like it's just
call or something like that and we can
just keep zooming in here
like that is 5 micro seconds between
that you cannot get that resolution with
sample-based profiling you can click on
any one of the anyway these little boxes
which represent a goroutine running and
stopping running and find out stuff
about what it was doing so for example
this this little piece of sweep here was
a bit of background work that the
garbage collector asked the Goa team to
do the garbage collector is not doesn't
have this kind of finite offer on state
the more allocations run ahead of the
garbage collector the more work the
allocator has to do and so we can see
here this started with something to do
with building a directory tree we're
doing some strings join in the runtime
and because that went because that
touched the allocator
the the garbage collector is in a mode
where says we have to do a little bit of
work for me before you can go back to
doing your own work and we can see all
this level of detail this is this is
invaluable so
so we just looked at we just looked at
two ways of tracing kind of like a
running program from start to finish but
this may not be this is useful but this
may not be how you run your application
for example you saw how much data was
generated you could you can imagine this
would be infeasible to have this running
in production from start running for
weeks until you do your next deploy so
be really useful if we had a way of just
jumping in turning it on for 5 10
seconds grabbing a trace leaving turning
it off and this is exactly what the net
HTTP people OFF package provides you
this line here how many people have seen
have seen will used this line in their
code yep good so when you import this
this is a somewhat questionable use of a
side-effect import but it registers a
few debugging handlers on the default
serve marks so basically just by
importing this you get a little kind of
debugging end point in your application
so to show this and to give another
demonstration of the trace tool given
I'm in San Francisco I offer you
Mandelbrot's as a micro service so let's
have a before we run it let's have a
look at this program
so attribution definitely is well this
is Franciscan boys matter brought
package which we collaborate on doing
various demos with because it is a
wonderful tool to demo with we can see
here we have the net h @ PP prof we're
also going to register a handle func on
the amount abroad endpoint and then wrap
that in something just to log the
request just so it's useful to see the
man who brought endpoint itself we make
a 512 by 512 image we create a weight
group with 512 entries in it and so we
spawn a goroutine for each row in this
mount abroad we run them all together
and then when they're all done we then
write it out as a PNG so when we get a
request we're going to dedicate I'm just
going to throw 512 goroutines at this at
this problem do it as quick as we can
send the result back that's running when
you when you hit it just get this matter
brought the size 512 512 was
deliberately chosen because it takes a
moderate amount of time on this laptop
about about 200 milliseconds so what we
want to do is you want to trace this
program while it's running now because I
would need three hands and two screens
rather than trying to like alt tab and
stuff like that I'm just gonna run a
kind of a B style HTTP tester this was
written by Jana Birju Dogen it's called
Hey and the reason I choose it is that
it supports this mode where you can say
send one per second don't just send one
at a time but send one per second kind
of QPS so if we've run that see now it's
just issuing
so now great now the application is
doing some stuff we can grab a trace and
actually catch it doing stuff how do we
grab a trace
just good old Co now cuz we've said
seconds equals five this is going to
pause for five seconds and see that
request an end point there anyway so now
now so now we have a trace of a running
application let's have a look at it it's
obviously a bit smaller so there's only
one one trace there it's not split over
multiple ones and when we look at it we
can kind of see that these these kind of
clusters of activity line up with what
we expected like like we're issuing one
requests per second it takes about 200
milli so one second two seconds three
seconds and these are probably in the
range of 372 that's about that's about
what we expect I don't have a version of
this where I push the server into
overload and we kind of look at the
behavior of that but this is a really
good way of analyzing some of the
results that we see so just before the
request comes in there nothing no go
routines running it's as soon as the
request comes in we see it spiked up to
like four hundred sixty-five hundred so
that's obviously creating all those go
routines and then you can see the CPU
getting very busy we just go routines
running the Mandelbrot function as it
slowly chews through them all and then
the program moves into the second phase
where it seems to be allocating more
memory because the the heap is going up
and that corresponds with the that
corresponds very closely with the
writing out of the PNG and we can tell
that because you can click on it we can
see down here handle function matter
brought in code so definitely this bit
which is allocating memory is running
the PNG compressor now one other thing
if I can just find it I'm looking for
there it is there it is looking for this
this is on the network line this is the
record of the packet coming in the HTTP
requests coming in
and not only can we identify that but we
can say what did you do afterwards so if
we click on unblock it will create this
little arrow here sorry for jumping
around but we can see here that this
network request directly resulted in
this go routine waking up and this girl
routine is I'll see the surf HTTP server
HTTP our Mandelbrot one so then as we go
across here all of these go routines
their incoming flow is this go statement
so you can trace the you can trace a
request both in time and also how it
interacts so this is obviously our all
these errors here are all the ghost
statements firing off these go routines
as we see there their number starts to
shoot up and then that's just slowly
processing them working its way down and
if we go to the other end we're now back
in they should be serving we can say
what unblocked you what made you wake up
and what made it wake up was the wait
group being done so there's a tremendous
amount of menace amount of detail that I
really have no more time to go into
available in the trace tool I think it
is just amazing here are a whole bunch
of links this will be online straight
away so you don't write them down now of
more talks about the execution tracer it
is a wonderful tool so with the time
that I don't have left I want to spend a
little I want to talk a little bit about
memory management and garbage collection
goers unashamedly a garbage collected
language this is a design principle it's
not something that's going to change at
this point and because it's a garbage
collected language the performance of
your program is often determined by your
interaction with the garbage collector
um
from the worldview of garbage collector
designers and authors their worldview is
that they want to present to the program
of this illusion of an infinite amount
of memory they want to free you from
having to think about freeing memory by
by promising you this world that you can
allocate an infinite amount of memory
because they because behind the scenes
when you lose track of that memory they
can clear it up and you can't prove that
they did that so you might disagree with
this statement but it's important to
understand it from the the way that this
is how the garbage collector designers
think about it and more importantly from
Google's experience with running their
production systems has come and talked
about this morning the GC favours lower
latency over maximum throughput it
really targets driving the latency
figures down and their results over the
last four or five releases ago I've just
been to drive that latency down drive
through the pores outliers down into the
into the microseconds range because
fundamentally an application which has
stopped stopped because we're GBC pores
is no different to an applications
crashed from the point of view of
suddenly trying to send it work so if
you can reduce that if you can reduce
the latency you increase that the total
amount of capacity you have to process
things now I want to give you the super
simple way to measure the activity of
the garbage collector and that is this
GC trace equals one flag this turns
these statistics are always being
collected but they're normally
suppressed so it costs you nothing to
turn on this this profiling so again we
pick on go doc we run it with this this
flag enabled so now it's going to print
out every time we do a garbage
collection and this matches what we
expected before we saw it does a lot of
work reading reading and building tables
and things like that and then into this
mode were just quiets down and waits for
requests
and now obviously everyone's application
is going to be different but that this
is like like the when I arrive at a
performance problem I just turn on turn
on this flag and I get an instant thing
like here's this program bound on
allocations or is a bound on something
else if this is just spewing off the
screen without end it's pretty pretty
clear they haven't got a handle on their
allocations okay the garbage collector
provides you exactly one variable to
tune its operation which is effectively
the size of how much should the heap
grow the default value is 100 which
means it will double every time it if it
needs to grow it will double your double
itself if you set that value higher than
100 it will grow larger every time it
needs to with with the view that
hopefully this will reduce the number of
times you have to grow we set that value
smaller it will cause the garbage
collector to run more frequently maybe
keep the heap the heap under control and
maybe use less memory there's no real I
mean 100 is the default but really it's
up to you to profile and choose the best
value for your application I think I'll
move to some conclusions so I have a
little bit of time for questions of this
possible and I want to end with some
concluding remarks which is start with
the simplest possible code I measure
profile your code to identify
bottlenecks like maybe maybe it's fine
maybe there are no bollocks but don't
guess don't don't say right I've written
some code now I need to find now I need
to find all the performance problems
with it so yeah if the performance is
good stop you don't need to do anything
you don't need to optimize everything
you should have both a upper response
time and a lower response time like this
probably little value going below 100
milliseconds or 50 milliseconds to a
webstar request humans can't really see
that see lower than that and importantly
as your application grows your traffic
profile is going to evolve people are
going to use your application in
different ways as new features are added
as new kind of social circumstances
change that make the music
so your performance hotspots are going
to move and when they do don't leave
that complex hyper optimized code there
this is an opportunity this is no longer
a need to spend complexity debt in this
place rewrite it to something simpler
and speaking about simple try and always
write the simplest code that you can we
saw in the example with the string
concatenation in contrary to perhaps
what some of you some of you may have
thought and certainly I thought when I
first did this example um it didn't turn
out to be that bad so write the simplest
code you can the go compiler is
optimized for I said normal here mainly
because I don't like the word idiomatic
I think that we kind of use that as this
your belief shorter code is faster code
like fundamentally less less code code
will less code on the page will optimize
to a smaller program which will probably
run faster go isn't C++ it's not
designed to unroll all those complex
templated abstractions um and yet
finally pay attentions to allocations
and try to try to avoid them where
necessary because allocations are really
going to in terms of their pressure on
the garbage collector go to determine
probably the scalability of your
application and I want to I want to
close with two quotes the first one by
Russ codes is I can make things very
fast if they don't have to be correct
and robbers always reliable readable
means reliable and I mentioned these
because I've talked all about
performance and nothing about any other
aspects of coding in this talk but
performance and reliability are equally
important I see little value in making a
really fast server that is touchy and
unreliable and panics and deadlocks and
out of memories on a regular basis like
I don't think anyone here is building f1
cars they have to race for you know 100
laps and it doesn't matter if you have
to completely rebuild the engine you're
writing production software that has to
be reliable over the long term so please
don't trade performance for reliability
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>