<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Twitter Streaming API - Data Mining #6 | Coder Coacher - Coaching Coders</title><meta content="Twitter Streaming API - Data Mining #6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Roshan/">Roshan</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Twitter Streaming API - Data Mining #6</b></h2><h5 class="post__date">2016-01-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Kc97hIYrNro" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone and welcome to the story
of about data mining this is part 6 and
it's about Twitter streaming API so
starting from this tutorial that would
be putting an outline for the tutorial
so you can use this notebook later as a
reference if you need to remember any
part of this tutorial code anyway a
quick introduction to the Twitter
streaming API Twitter provides two api's
one is restful api this is for getting
existing objects and they have a
streaming API which will deliver live
data this is a generalization because
you can get some historic data with the
streaming API but this is the main
purpose why it's used the reason why
would you use the streaming API is to
capture for example large amounts of
data because restful api has a limited
limitation basically to how many tweets
it can return it can be used for
real-time analysis like monitoring
social discussion about a live events so
it can have some kind of an event and
you want to know people's opinion about
that event real time you can use it for
in-house archive like archiving social
discussion about your brand so if you
have a company you might want to capture
the social discussion about your company
or a brand and finally for iia response
systems so if you have an official
Twitter account for the company and you
want to provide an automated reply like
we're looking to your question and you
might want to email one of your support
staff to take care of that or something
like this you can do that prerequisites
to follow up with this tutorial is to
have Python 2 or 3 you will need Jupiter
with ipython we
which is right now a separate library so
if you're using Jupiter for basically
three or higher you will need to install
this library separately you need pandas
numpy matplotlib for charts you will
need and a monkey demonstration and of
course you need PI Mongo library to
access this MongoDB finally we will need
scikit-learn
but it's optional because we're just
using it to count basically words then
finally we will need that TP very
important library to access that with
our API really convenient way to do it
and you will need a Twitter account of
course so how does it work the streaming
API can provide data through a stream
again HTTP response this is very similar
to downloading a file where you read a
number of bytes start to disk and repeat
the process until the end of the file
the only difference in this in the
stream it's endless the only thing that
could stop the stream there are actually
two things that could stop the screen if
you close your connection too if you
close your connection whether it's the
stream or a complete connection to
Internet you would lose you will across
that stream or if your connection is not
cable of receiving on data the server
will keep a buffer basically and if that
buffer fills up to 100% that will close
your connection because you're not
receiving your data fast enough or you
don't have the capacity to do that this
means that this process will be using
the thread it was launched from until it
stopped in production you should always
start this at a different thread or a
different process to make sure your
software doesn't freeze until it stopped
that stream the first step is
authentication so you will need to
import few libraries in here I'm
importing a number of libraries and I'm
using the magic in here to make Matt
welcome show charts inline
authentication keys you can get your
authentication keys from apps that
whitter did come so if I go to absolute
with our dot-com I have in here three
applications I would open this one which
I created for the tutorial if you go to
keys and access tokens you will see that
you have a consumer key which is an API
key and a consumer secret which is the
API secret so these are the first two
numbers you need and then you have your
toe and your access token so this has
access to your actual account so in here
I'm giving it read-only access to my
account because I'm just reading tweets
I'm not sending anything so these are
the second two numbers that we would
need for this tutorial top here copy the
four numbers in here and execute the
cell will make a connection to our
MongoDB so I'm connecting to a database
called the tweets then a collection
called streaming tutorial and I'm
counting how many records I have there
already so I have two thousand five
hundred twelve so far in this collection
so how do we start a strain basically
you will have to create a listener so
this is a listener class it looks a
little complicated but it's not really
that complicated you will have just to
extend twiba that
stream listener and basically will have
to do one thing on status so this is the
method that will be called whenever a
new tweet is received in your connection
and here I'm doing counters I'm updating
some visualization to track the
streaming process but there is nothing
hard about it the only thing that you
should know is how we're inserting this
into our MongoDB so whenever we receive
a status I'm just getting the JSON part
of that status and storing it in our
MongoDB database so let's execute that
and here I'm I'm creating a new listener
and setting the maximum tweets to 100
tweets and I'm starting a string with
this listener that uses my
authentication finally I'm using in here
how do you start streaming basically
there are two functions that could be
used for this process you can use filter
which is the better way to do it this
way you can filter what you need so I
can either follow some people or you can
track some keywords and there are few
other filtering options that you can use
you can set it as an a synchronous
process so it would not occupy your
current thread it will start in a new
thread education storm warnings
languages encoding and filter level you
can use any of these to control what do
you receive with your filter or you can
start fire hose which is which you have
actually limited access to you're not
supposed to be using it basically but
anyway with fire hose you will basically
receive every tweet that's sent on
Twitter service which is very very
overwhelming if you don't have the right
connection and right hardware to handle
that firehose captures everything is
reset so you would have to set one of
these to either follow our track I would
follow you give it basically a list of
user IDs to follow or with track you
will give it a string to track or to
filter using that string if you separate
their words with spaces this is like an
ant operator so if you for example want
to search for data mining just type data
space mining and your filter if you want
to find both any tweet that contains
either data or mining you should put a
comma between them so I would like to
track these words Jupiter Python data
mining machine learning that size big
data Internet of Things and hash tag are
just to get some of the discussion
happening about my favorite topic which
is that science so I'm passing these
keywords and if you notice with any word
that has a space in it I'm repeating
that again without the space so it would
capture people using hash tag data
mining for example so I will start this
in here I will I have a progress bar
that tracks my progress I can see that
I'm receiving 1.5 1.2 tweets per second
15 16 17 % done this is the estimated
time of accomplishing this I think it
will take over a minute because we don't
have much of a discussion happening
right now but I hope
the discussion will pick up so we can
finish this a little faster so it's
closer to 50% now and it's increasing
1.7 1.6 that's good another 20 seconds
almost that's the discussion is not very
active about data size right now -
certainly sometimes you will get much
much faster like closer to 5 to 6 tweets
per second whenever there is something
actually being discussed on Twitter so
this process is almost done and it will
give us a little report in just few
seconds ok please go another tweet and
we're done
so that took a minute 23 seconds our the
rate of tweets was like 1.2 tweets per
second we have collected hundred tweets
and now we have two thousand six hundred
and twelve tweets and our MongoDB
collection so let's look at one of these
tweets just to know what kind of
structure we have for each tweet so the
first the first thing in here is
underscore ID which is basically an
object ID which is basically the
identifier of this document and MongoDB
so this has nothing to do with Twitter
so if I create a lead occupant inside
this collection that will give it a an
ID basically
MongoDB ID so our data is here we have
contributors this is created add this is
when the tweet was sent we have some
entries here and giving assembles URLs
user mentions
some of the important ones are like ID
this is the idea of this tweet
we have source which I know this tweet
was sent using an iPhone the text of the
tweet I don't know which languages this
actually I guess as Japanese GA this is
some information about the user
so when did the user create his account
and I have some other information about
this user like name location and things
like that and his screen name just looks
very fake
but anyway Lord results into a data
frame so I will load these results into
a panda's data frame object and now I
have a table basically with all the
information that I need so I will do
accounting vectorizer now for this to
just count the frequency of each word
used in our data set in these tweets so
we can know what are the most used words
in these tweets and we can look in here
we can see in here HTTP and see oh I
guess these two parts came from URLs RT
the Internet of Things v so if you want
to 411 tweets at this word big data
Python that another text this I could
have added that to my filter words it's
fairly used actually and you can see
what words are most used in in this
discussion now I want to get out to the
if you look here and our data frame
I have source so which how did they send
this to it and here this is iPhone but
the formatting is wrong so I just want
to capture this world so I did some
regular expression in here pattern to
capture the name of the source and
finally I'm visualizing it and showing
the top 10 sources so in here we can see
20% over 20% of the people tweeting
about this topic are using iPhone 16
almost 17 percent a tie between people
using the web client and Android I have
no idea where is this deliberate I EV
TTT Twitter feed
HootSuite round team tweet deck and
Facebook so these are the top sources
being used to send these tweets you can
do all kinds of analytics and visualize
your data with once you have the
Meridian your memory earlier into a data
frame object so this tutorial is
available open source it's on github and
it's viewable from there or
ambi viewer I hope if you like this
video will subscribe to this channel or
watch the previous video about MongoDB
or the next video which is about
Facebook thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>