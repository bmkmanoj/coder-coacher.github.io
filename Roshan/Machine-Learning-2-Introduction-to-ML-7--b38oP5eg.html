<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning 2 - Introduction to ML | Coder Coacher - Coaching Coders</title><meta content="Machine Learning 2 - Introduction to ML - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Roshan/">Roshan</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning 2 - Introduction to ML</b></h2><h5 class="post__date">2014-05-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7--b38oP5eg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone I will start this listen
with a quick prediction I'm defining a
person in here who's 25 years old that's
your a degree studies for 14 years works
40 hours a week never married from the
United States works in sales white
nothing family is a male and works in
private sector this is based on data
from 1994 so this might not reflect the
marketplace today but I will try to make
a prediction whether he makes more or
less than 50 thousand dollars a year and
it tells us that he makes less than
fifty thousand dollars a year
let's look at the same guy ten years
later when he was 35 he still works the
same 40 hours bachelor degree same
occupation same everything let's see
what happens in ten years now he makes
more than fifty thousand dollars a year
if we changed the race his race to black
we can see that he makes less than fifty
thousand dollars a year
what if he was working forty five hours
a week now he makes more than fifty
thousand dollars a year but if there was
a female makes less than fifty thousand
dollars ear and let's try to make that
fifty hours still less than fifty
thousand let's make that fifty five
hours a week now she can make more than
$50,000 this might look like very bad
results or might not sound like good
results to everyone but this is just
based on census data from 1994
so this is a lesson to of machine
learning series and it said introduction
to machine learning if you have not seen
part one we discuss Python ipython
notebooks matplotlib numpy pandas and
Amazon Web Services in this lesson we'll
be replicating the results that we have
just seen in our quick prediction system
we will machine learning gear uses
regression clustering and classification
as basic tools to emulate learning and
do predictions so let's go to Amazon Web
Services and let's go to our management
console you can go to Amazon Web
Services aws.amazon.com if you don't
have an account open one that's three
and everything that we will be using
this lesson will be free so you can
replicate everything we do without
paying anything to Amazon Web Services
will be using services from the free
tier for the purposes of this lesson
let's go to ec2 the service that we'll
be using
we will launch a new instance choose
your ami from community M eyes look for
the one I left public it's called
ipython notebook server actually you can
just go with ipython and hopefully you
won't find anything besides my public am
i select that and
that's taking little bit a load that's
taking more than usual to go to actually
let's go okay okay
select again choose the micro instance
this is within the freight here you can
choose something bigger if you want that
will make your system a little faster
next we can just go to review on launch
once you are in review and launch go
down to security groups edit security
groups and open the extra reports that
we need to manage the system 8080 and
8000 review on launch
then click launch if you don't have a
key
create a new one and save it some more
safe I will use the key I already
created before
it's launching my instance now
you can see that I have tried to record
this lesson earlier that did not go well
so the first thing we do we name our
instance I will just call it listen 0 2
so I can tell it from the other one and
it's initializing now we can access it
from the public IP so we'll clap it up
public IP and we'll go to it colon 8080
that will take us to a Python notebook
server once we're there we'll put the
password the default password is ro sha
and Russia are oh Sh a n go to listen to
introduction to machine learning this
might take few seconds to load it's a
big not work so and this listen we will
learn about scikit-learn we learn how to
choose which type of algorithm to use
and we'll learn about some public data
sets that you can use to experiment and
learn with then we will be talking will
be replicating the classification
example that we have seen in the
beginning of this lesson
one important tool that we always need
is a gently it's our administration
interface we will open this link and we
will paste our IP and in the place of
the word IP
it will tell us that it's not signed
certificate that's okay
we're not verified certificate because
that's self signed user name and
password the username is root and the
password is a D mi n admin don't want it
to remember thank you and thank you okay
we will be using this tool mainly to
monitor our memory usage during this
lesson so we want to keep an eye on
memory usage we want to make sure we
don't go beyond our memory and if we
exceeded the memory we'll start using
swap and that would be a very big no-no
and any machine learning application you
always want to use only memory so let's
go back to the listen scikit-learn
scikit-learn it's a machine learning
library written in Python it's simple
efficient that has tools for data mining
and data analysis so machine learning
uses three types of algorithms depending
on the problem that you have in hand we
have clustering clustering is used to
group data into groups according to
their similarities so in here we had
black dots all over the place and a
clustering algorithm and decide
predicted that these orange points
belong to one group red points belong to
another group and red and green points
too and the green points belong to a
third one it could not cluster the rest
of those black
the screen classification and here we
have black data points and white data
points and the algorithm made a decision
function that will tell if a new data
point will be considered black or white
without knowing if it's black or white
just depending on where does it fall on
this chart so if it came anywhere in
here it's a black data point anywhere in
here it will be a white data point
that's that's another way to do the same
problem to classify the same problem
with different weights for the algorithm
the third thing that we'll be dealing
with is regression and here we have a
few data points the red data points the
algorithm will try to fit a line that
goes through them and it will predict
where does this word where it does it
think this might go and it will show you
according to the algorithm that you're
using an area of confidence so this is
an area of confidence that it might
proceed to anything within this area
this is a little chart that shows us
which algorithm to use it doesn't have a
very algorithm and scikit-learn but it's
a very good place to start if you don't
know if you don't know which algorithm
to use so it will ask you first you have
more than 50 samples no go get more data
figure predicting category or a quantity
if you're just looking at data if it's
labeled data or not you will it will
lead you to some algorithm that you can
use if it's text data or not
and it will help you choose an algorithm
again it doesn't have every algorithm in
the system but it's a very good place to
start data sets you need data sets to
work with the machine learning this is
these are some data sets to get you
started this is an archive it's a public
archive for machine learning and this is
the scikit-learn data sets that's public
so inside the each data set you will
find samples which represent records so
if data sample is about people each
sample will be one person and it will
have features or attributes and those
features represent a feature about that
person might be age it might be income
or something else so let's examine some
data we will be working with this data
set from the from UCI and this data set
is 40,000 48,000 records more than 48
thousand records it has 14 attributes
here is some information about it these
are papers that cited this that is it
you can see a very long list of papers
that cited this data set and we can go
to data folder and and here we will see
the file that has the actual data this
is the file that has the actual data
that we'll be dealing with and we can
either download this file into our
machine and upload it through a gentie
to our data folder or we can fetch it
directly over the internet so let's go
back to our listen
this data I I copied the description of
each column and data set this is small
sample of the data set so you can see
for example this person is 39 years old
works for state government this is his
weighted demographic number bachelor
degree studies for 30 13 years never
married admin clerk oh nothing family
white male this is capital gain capital
loss works for 40 hours a week from
United States he makes less than 50
thousand dollars a year so we will start
to build that example that we have just
seen the predictive model for
classification that classifies if people
make more or less than $50,000 so we
will execute the first pod sale to
import our libraries to execute any sell
any code cell click in the cell and hit
shift and enter and once you see the
number appears in here
this means it finished processing this
data so second thing we will do will
load data so we can fetch it directly
from the URL or use the path for this
file you can upload the file download it
to your system and upload it to the data
folder and using agent e and here I just
named the columns because in the data
set it doesn't mention the columns the
column names within the data set so I
have the column names in here and I'm
using pandas to read comma separated
value this is a method that reads comma
separated value files to know how does
this method work or works you can go
inside the method and hit
cap and this will show us this little
interesting hit it again it will show us
this little interesting help box we can
expand it and see these are all
parameters that you can pass to rate
comma separated value and it will help
you read the file in the right format so
will be fetching it directly from the
URL second thing we notice about file
the separator is not just comma it's
come out in space so we will use the
parameter for separator to do that
that'll pause the video and I will be
back
the second parameter is names and here
we pass a list with the column names for
the comma separated value file because
this comma separated value file doesn't
come with the headers in the file the
last parameter we're using is n rows
which is number of rows we're limiting
that to 20,000 because of the limited
amount of memory we have in our instance
so let's run down again to execute shift
and enter and we will do some math
statistical analysis of our numerical
features we have aged between 18 and 90
and this is a weighted identifier of
demographic or something like this I
read the description I couldn't get
exactly what this do education number is
number a number of years people studied
so round the 50% is ten years capital
gain almost no one made any money and
capital lost almost no one lost any
money hours per week this is how many
hours per week people work and we can
see very clearly that almost everyone is
working around 40 to 45 hours a week so
this is how we can describe our data
always remember that your data is stored
in this variable csv underscore data
visual inspection
this is scatter matrix of all the
features
i pre excuse code Celso so you don't
have to do that it will take few minutes
if you try to do it again
so I executed that before I started this
citizen and in your instance you will
find the executed version with the chart
this chart basically lists all numerical
features age waited the demographic
education number capital gain capital
loss and hours per week and it plots all
of that or scatters all of that against
the same features and whenever the
features the feature is scattered
against itself so hours per week with
hours per week it gives a histogram of
that feature so it can very clearly tell
that everyone works 40 hours a week
there is a little bump around 50 and a
smaller one around 60 capital gain and
loss we can see that it's there is no
there is no data there because almost
everyone made 0 this is education number
so how many years people studied we can
tell that almost everyone studied nine
years then 10 those are the highest
values then we have a little bump around
here which is 13 years so 13 years 14 15
and 16 finally and here
I don't understand this feature so I
wouldn't try to explain it this is age
this is just a histogram of age we can
tell that almost everyone is in this
range and it drops rapidly after that
which is around forty five years of age
so we will visualize some data we will
scatter again to get the little help box
for any method like a scatter method we
hit Shift + tab and we get this helped
box or help undo we can see that we
should pass an x and y then we can put
colors their markers color map with
there are a lot of features that will be
using alpha will be using that - this is
transparency level for colors so we'll
be using that - so we're scattering X
which is the education number which is
how many years did someone study against
age so our Y will be age and our X will
be numbers number of education years and
here we're doing get a little filter so
whenever we want to filter our data we
pass one of our columns so we pass
income and equals equals to any value
which is more than 50k that will filter
only people with more than 50,000 in
annual income we're using elf of 2% so
it's only showing two percent of the
color which is 98% transparent we're
putting a label there so we can identify
it and using red color for people with
higher than fifty thousand we're doing
the same thing for people with income
less than fifty thousand and we're
scattering them in blue color we're
showing legend and final method which
shows actually the chart and this is our
chart we can tell that
more people studied like 16 years or
most probably will be making money since
the beginning of their career people of
the 15 years around the same time but
maybe a little later people around 14
years well there is a still a very high
chance that they would be making more
than fifty thousand thirteen years
around their end of their career they
would most probably be mainly making
more than that we can tell that there
aren't much people who studied 11 or 12
years mmm
10 and 9 we can see that people who
stayed until they're well around 60 when
they actually made more than 50000 and
we can tell that almost no one before
that time is making good money but there
aren't much people in that bracket
anyway you can see the faint colors
which tell us the density of people in
here the problem with this chart
everything is scattered on one line
because no one studied like nine and a
half years or 9.3 or nine point one or
8.9 so everything is scattered on one
line so to take care of that we add a
random value of 0.5 minus 0.5 to 0.5
which scatters this line within a range
and it never goes to the next range
because it scatters only between minus 5
and plus 5 so to do that we add numpy
random uniform minus 5 to the minus 0.5
to plus 0.5 and we will generate an
array that's similar in length to our
data
it so we're getting the length of our
data set and passing that as a parameter
so it will generate a random list of
numbers between minus 0.5 to 0.5 and the
length of that array will be our list
will be the length of our data and we're
adding that to education num which is
the number of years they studied and
we're storing the result of that in
education num underscore R and D for
random to differentiate this feature
we're plotting gonna bigger and chars
that's 12 by 8 that's a big chart we're
doing basically the same thing in here
and in here but we are changing the
Alpha value to 2 impact to 10% so points
will be more clear in here we can see in
the legend we can barely see anything if
you don't have very good contrast in
your screen you want to be even seeing
the faintest dots because it's only
because it's not yet percent transparent
and here it will be 90% transparent so
it will be fairly visible after that I'm
adjusting x-axis and y-axis by adding
labels to them and adjusting text and
limit and finally I'm putting a grid and
title on the chart and after that I'm
dropping the feature that we calculated
with the random number so we get back to
our original dataset let's plot this and
see what we get that might take few
seconds to process because it's a big
chart and we can see way more
information than our original chart we
can see the trend in here where the more
people spend in studying the letter they
enter the market where we see people
entering the market around their 30s who
studied around 16 years we can see at
the
a dense area in here which shows that
some people are actually working part
time at a very young age around let's
say 18 or 17 and we can see this very
large blue area which is around nine
years where no one almost no one is
making more than fifty thousand ten
years people above 50 or they do have a
good chance of making more than fifty
thousand there isn't much people in this
area we can clearly see that now and we
can see a dense area around 13 years or
more people are making more than 50
thousand after a few years of their
career after almost less than 10 years
of their career they have a good chance
of making more than fifty thousand
fourteen fifteen or sixteen we can see
it getting redder and redder and the the
threshold for where they can start
making more than fifty thousand is down
to their the beginning of their career
basically we can see the dense area
going like this almost where this area
is the high-income area of our chart
processing Jetix features you can
process text features to convert them to
numbers and this is what we are binary
like zero and one which are still
numbers but you need to process your
text features I convert them to number
somehow to make them machine learning
compatible the way to do that you can
you can vectorize zero data which says
for example if you have three people if
you have three cities like London New
York Toronto and this will
this is would be their your users
location you can vectorize those
features to a feature with true or false
and it just says for example he lives in
London so if someone lives in London
they would have one there and a second
feature will be lives in New York a
third feature will be lives in Toronto
for someone who lives in London they
will get for the first feature 1 0 4
second feature and 0 for the third
feature if you have the there another
way is to convert it to a series of
number representing their original value
so sometimes you'll get results like
excellent good average bad terrible you
can easily convert that to 5 4 3 2 1
which would still keep the original
meaning of data with the text document
use count vectorizer it's a way to
process big documents big text documents
we will go through that in a later
lesson and finally have special values
sometimes you have emails URL phone
number username and you have to process
some of those with special treatment for
example for a URL you want to fetch that
page and process the content of that
page using text processing but you still
want to see if there is an image there
it might be a link to an image it might
be a link to a social media page where
you want to know who owns that page and
store that information to and so
depending on what do you want to do with
the data you might need to do special
treatment for those special values
we will will proceed with the this data
now we'll have vectorizer or data but we
will the first thing we will do will
drop an A which drops not available
record so and record with the nulls or
blank data it will be dropped
then it's dropping few columns capital
gain capital loss income income because
it's the result that we want to predict
and the weighted demographic and we're
dropping those three features because
they are not machine learning useful to
us because there is almost no data on
those two and this one is it has a very
large range and I'm not really sure that
it represent anything that can be
related to the result that we want to
calculate finally I'm converting that to
a dict
which is a dictionary and the output
type is record so it will create a list
of dictionaries each dictionary has all
the values in a single record and
finally it's converting that to an array
I'm adding an extra column sorry I'm
adding a new value called results and it
equals two comma separated value of
income which is this column of
commissariat value data equals equals
more than 50k so it will compare more
than 50k to each value in here and if
it's equal to it it will return true if
it's not it will return false and it
will store this list of truths and false
true and false results in
a variable called results actually now
we split data into training and testing
set so we have 20,000 records we will be
training with all our records except the
first exit for the first thousand so we
go thousand column and if you leave that
empty it will use the rest of that list
for features which is the vectorized
features from our comma separated value
data a second we have results and we're
getting all the results except the first
1000 for our test data we are using the
first 1000 so we go empty :
1000 so it will start from the beginning
and stops at 1000 and the same thing
with results let's see just one of the
records just to visit to visualize that
and understand how this look it's a
dictionary with all the features inside
it and that is what's passed to the
vectorizer that converts all those text
text or string features into numbers
basically into binary numbers will train
ayack a classification model using kN
instead of trying to learn what does
this algorithm do exactly it's much
better just to see the results learn
other algorithms and try to basically
add develop experience in
choosing the right algorithm with time
it's oh sorry we did not process this
one yet
we should execute this one first then we
go back to this one and execute it so we
have a variable called KN n now that has
a an on neighbor classifier and it's
fitted with the extraneous rain and
white training sets which are which is
the complete data set except the first
thousand records now we want to do a
prediction
so we'll predict our X test then compare
our prediction to whitest and that would
be our accuracy so let's test that find
it I'm printing just the first ten
predictions and I'm printing the
prediction itself and I'm prediction and
predicting the printing the prediction
equals the actual data so it will tell
us if it's correct prediction or not so
we can visualize at least 10 of the
results and see how do they look and
here we have a loop I promised to
explain things as we go so for counter
that's how we do loops in Python for
counter n range of 10 so that will range
of 10 is a method that would create a
list that's like this it will be 0 1 2 3
4 5 6 7 8 9 and that would be equal to
range 10 but instead of doing that it's
much easier to write arrange them so
this is done our accuracy is 81 percent
sample data predicted that this guy
doesn't make more than 50 that was
correct
false correct false correct false
correct true false it predicted this guy
made more than 50 thousand but he did
this guy made more than 50 but he didn't
and here it predicted he didn't make
more than 50 and it was true and more it
predicted more than 50 and it's true
less than 50 and it was false he
actually made more than 50 true and it's
true so him prediction is more than 50
and the actual was more than 50 we will
see better reports now once we're trying
to improve our accuracy so to improve
our accuracy there are a couple of
parameters inside this classifier one of
them is number of neighbors that it will
use so in neighbors and the other one is
the weights method so it has two weights
method one called uniform and the others
called distance and we will so will be
looping over range from one to fifteen
and we will be looping inside of that
with the uniform and distance and we are
passing the value of the first loop and
the value from second loop to the
parameter to test different values so
we'll be testing all values from one to
fifteen for N and we'll be testing all
values for uniform and distance for each
of those values so let's run that that
might take a little bit of time to run
so that's the first one neighbors one
weight uniform a precision recall if one
score and support this is the standard
report that we use to test the accuracy
of classifier support is the number of
samples for each class we have so the
number of samples for less than 50,000
was 700 the
State and for more than 50 thousand was
232 so we can see that we have way more
people making less than fifty thousand
in this sample the total number is 1,000
precision is one one of our metrics for
measuring the accuracy of classifier and
it's showing the precision for each
class recall is very important we want
to make sure we have high recoil values
that will ensure high accuracy because
recall is not just getting the correct
answer because recall is calculated by
calculating all a true positives which
is like correct detections divided by
correct detections plus false negatives
which is values that it did not do not
it messed basically so this is recall
value want to make sure we have good
record values if one score is another
metric all of them the highest value for
all of them is one and the lowest is
zero so we're looking for values higher
and closer to one this is our first one
it has 79 overall accuracy but this is
the report that we will be looking at
from now on so one uniform good values
we have distance with one it doesn't
really matter but with that more than
one neighbor it will start making a
difference between uniform distance
wetting methods and here we can see
really low recoil value that's still not
really good we can see that we have so
many
this is actually this is fairly good
actually the results that we have for
five neighbors as fairly accurate 81% is
a good number in here even if it's a
even though it's 83 overall the actual
numbers we have for recall is really bad
so that's not that's not a model that
you want to be working with well this
one still not really good that's a good
one that's a good prospect seven with
the uniform I think the best one I found
was number 11
yes this is it it's 11 a neighbors with
the uniform weights you can see the rest
of the results you can go through them
you can use a specific one of them to do
some testing some further testing so we
will use this one with the 11 and
uniform and we will keep our trained
model using those parameter in KNN
usually want to put the predictions back
into the original data set if you want
to save that as a file to send it back
to your client or something so that's
how you do that you just then you column
and you put your predictions in there as
pandas series and this is the example
that we started with in the beginning of
this lesson let's run that one last time
just to see more than 50,000 these are
all the values you can use those values
in here to define different people and
see how a changes affect they're in
come this is a little chart I get from
Wikipedia that will help you if you did
not study in the US to identify how many
years do you need to have different
levels of education so this is education
number this is the end of lesson two if
you have any questions leave them on
this video as a comment or on Twitter or
Google+ thank you for watching and hope
to see you next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>