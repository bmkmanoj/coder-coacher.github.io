<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MongoDB - Data Mining #5 | Coder Coacher - Coaching Coders</title><meta content="MongoDB - Data Mining #5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Roshan/">Roshan</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MongoDB - Data Mining #5</b></h2><h5 class="post__date">2015-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2b32-KVzBXQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone and welcome to the series
about data mining this is part five and
it's about MongoDB in many cases you
don't have to go and collect data
because you have data in the local no
sequel database that requires analysis
this could be you are an audit trail for
from your website where you want to
study visitors and trends for example
one common no sequel database is a Hmong
it is MongoDB which is the subject of
this tutorial we'll be talking about why
do we use no sequel common no sequel
database systems PI Mongo which is a
Python driver for among a DB single
documents with nested documents and
working with multiple documents so let's
start with why do we use no sequel no
sequel was designed to deal with the
problems that came up when developers
started dealing with large amount of
large amounts of data using existing
relational database also refer to as
sequel database the three main issues
with the sequel databases were changing
schema during the lifetime of your
application you will change schema you
will need some kind of new field or
doing something with your database
schema so sequel databases had fixed
schema of tables which limited the
ability to store new fields in your
table you had to change the table schema
whenever you need a new field basically
the volume of data and storage sequel
databases can store virtually billions
of records in a single table but the
problem was the overhead cost of your
CPU and memory to be able to access this
data efficiently you usually will need a
very professional kind of server or
cluster to be able to do that and it's
not
really cheap to do that with a sequel
database the frequency of adding new
data sequel databases are not designed
to assert large amounts of records in
short time especially to large tables
and especially if you have relations to
other tables and these tables are also
large you will have to do so many reads
to check all primary keys and other
tables to ensure that you're keeping the
integrity of the relationship so what
did not sequel do to overcome these
issues for the changing schema they used
document based archive or at least
document waste archives no sequel use a
JSON like format to store rows and
collection this allows collections to
store data and any schema without
changing the database so by storing each
document in its own format you're
basically allowing for virtually
changing your data and then unlimited
way basically no sequel databases use
different different distributed
clustering systems to store data and
multiple machines and distribute the
processing power over multiple machines
this is how to deal with large amounts
of data they're basically using
commodity Hardware over multiple
machines to do distributed system
distributed system for your database for
your nor sequel database they're
basically using some kind of shredding
of collections to spread the data over
multiple machines that ensures sometimes
faster operation because multiple
machines are doing the work together
it ensures that you won't lose your data
if one of your machines fails because
they're usually using some kind of fault
tolerant distribution where any server
cannot know one piece of information
stored in a single server it has to be
on multiple servers so MongoDB uses a
great file system and some other famous
systems use Hadoop for example to do
this distributed computing process no
sequel dropped the support for
relational databases basically so this
is how they're dealing with the
frequency of adding new data which
requires multiple read operation
teachers to ensure primary key integrity
and relationship there's a reduced the
cost of storing new data to simply just
writing the data on disk and generating
primary key in some systems but all
systems generate parameter key as
they're storing data and your database
common no sequel databases MongoDB which
is the subject of this tutorial
it's it comes from the word humongous
it's a cross-platform document-oriented
database classified as no sequel
database Mongo is choose the traditional
table based relation database structure
in favor of JSON like document with
dynamic schemas so this is how they're
dealing with dynamic same issue it's an
Apache it's it comes under GNU or apache
license it's free and open source
software the second one is Apache
Cassandra Apache Cassandra and is an
open source distributed database system
the main difference between MongoDB and
Apache and the next one which is
as if you are starting with no sequel
and you want to get a feel of no sequel
you will basically use MongoDB or
couchdb which are the easier to to
implement Cassandra is the main
difference between cassandra and the
HBase and the other two is the amount of
data that you can store they're all
virtually unlimited but Cassandra had
proven to be more reliable with larger
amounts of data HBase is also meet HBase
also meets that criteria Cassandra HBase
basically is same thing it was modeled
after Google's BigTable and it's written
in Java it uses Hadoop distributed file
system to operate to store data and the
processing finally CouchDB it's very
similar to Mongo but to me and it's much
easier by the way but to me the main
difference between them is CouchDB does
not generate primary keys as it stores
data it generates them as your quest
data back so you there is no consistent
primary key for your records you might
request data requested in a different
way and you will get a different index
for the same record if you're using
Amazon Web Services or Google Cloud you
might be interested in a managed service
from there amazon has Amazon DynamoDB
and Google has data store they're both
no sequel database systems and their
marriage so you don't have to worry
about the infrastructure
other options is using my secure cluster
so you might really have my secret
cluster might be interesting to you to
know that it has no sequel support
although it's limited nor sequel support
but it still does PI Mongo this is the
link to my manga documentation if you're
interested so let's start by importing
the library imported by Mongo I will
connect to my local machine create
connection to my local machine you can
connect to another machine if you just
change the host and port where you can
connect to a different machine basically
so since I'm connected to my local
machine already I can list all available
databases in my local la system I have
four databases in here I can list
available connections or collections
sorry inside the Russian database and I
can see that there is system that
indexes his a system-generated
collection so I don't have to worry
about that but the rest of them could
have some kind of social media archives
from Twitter and YouTube
I will be connecting to Twitter
collection so I'm retrieving the
collection from the database first thing
I can do is count how many documents I
have there and it shows that I have 1
million 45,000
260 records or documents inside that
collection to find a single document or
retrieve add a single document you can
just use find one this will return the
first document you have in your
collection
you can't pass filters inside a
dictionary to filter out which document
you want
but if we pass no arguments there it
will just right there in the first
document that we have in our collection
we can notice that our document is
dictionary object first feel the first
attribute we have there is the
underscore ID and it's an object ID this
is basically your primary key so let's
examine this document a little further
let's start with the object ID object ID
is very interesting this is the
documentation for object ID if you are
interested in knowing more but it's
built out of multiple variables
including the machine that generated
that process ID and other parts actually
so if we retrieve our object ID and
print the type of it and return it back
we'll get the type of based on that
object ID that object ID this is the
class object ID basically and when we
return it will see the key that it has
inside our our ID basically one thing we
can retrieve from this object ID is the
time this document was generated so we
can retrieve that document I did that
generation time and I'm just formatting
this time in Izu format you can see that
was generated 2014 and the time zone was
UTC to access other fields you can just
ask them like her accessing any
attributes and inside the dictionary I
so created that it's Unicode object the
text inside of that weight is also a
Unicode object one trick about Unicode
if you want to display the Unicode
characters instead of this kind of code
you can just print it so if I print this
text it will print out the actual
Unicode text that that you need to see
this is this trick works perfectly for
ipython notebooks and it comes in very
handy sometimes
anyway here mister documents we can
notice in our document the one we
retrieved that we have user which is a
not a single value its dictionary
basically so it's a dictionary as a
value for an attribute inside our main
dictionary so we can retrieve user if we
retrieve that we'll see that we have a
dictionary with all user information we
can access a sub attribute of the main
of this document like this so user that
very a user then accessing verified so
we can retrieve the value of whether the
users verified or not and it returns
false one trick about Unicode once
you're dealing with dictionaries so
you'll have a lot of when you have a lot
of Unicode inside dictionary and I want
to display it in ipython not work you
can print out the Unicode with this
function basically I'm iterating over
all the items of this dictionary and
returning the key and value and printing
them so we can see that I have my URL
links working and I get my Unicode
printed correctly
finding multiple documents the way this
works is by using the function find it's
the find one this will return multiple
documents and I'm passing in here a
criteria or a filter to this find method
so the criteria I'm using is user that's
verified so I wanted to go into user
attribute and check the sub attribute of
that that or the of this listed document
and I wanted to make sure it's verified
user so I want to basically return all
the tweets of the verified users and my
1 million tweets one thing we can do is
count them we have 672 messages from
verified users the thing I want to do is
I want to loop over all the messages and
return only the user data I don't care
about the tweet itself I just want to
check them get back the users data and I
stored that in a data frame to make to
make processing this easier I'll filter
out all the duplicate users basically if
a user sent multiple tweets and his ID
showed up more than once basically so
I'm dropping all duplicates using ID and
I can see that we have only 222 unique
users and these 672 tweets I think I
want to do in here I'm scattering the
followers count so how many followers
they have with how many tweets the sent
to see if there's a relation and amusing
for color map or
see map the French can't so how many
people they are following I'm printing
that on a log scale for x and y then I
think we cannot seen here that almost
everyone has less than 15,000 followers
we can see that only few people have
more than that one to maybe there's
third one here so most of the verified
users don't do not follow a lot of
people that's one thing we can notice
and we can see there is some kind of a
relation in here a linear relation the
more you tweet the more followers you
have basically and that's what we can do
it that you can feel free to explore
more data in your environment and if you
have any questions you can leave them in
the comments below this tutorial is
available open source on github and it's
viewable on and B viewer
feel free to use it for any purpose that
you have thank you for watching and I
hope if you watch if you like this you
will subscribe to this channel and
you'll watch the previous part about
Google search data mining google search
or the next part about data mining
Twitter streaming API thanks for
watching and hope to see you next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>