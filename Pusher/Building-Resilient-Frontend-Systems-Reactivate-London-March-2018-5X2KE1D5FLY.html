<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Resilient Frontend Systems - Reactivate London - March 2018 | Coder Coacher - Coaching Coders</title><meta content="Building Resilient Frontend Systems - Reactivate London - March 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Resilient Frontend Systems - Reactivate London - March 2018</b></h2><h5 class="post__date">2018-04-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5X2KE1D5FLY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey thanks for having me and thanks the
introduction yet I mean I work at
BuzzFeed I don't write potato articles
unfortunately but I've read them and
today we're gonna talk about why
resilience matters and how we can all
build more resilient front-end systems
and how we can keep our jobs when
everything is satisfied so there's a
BuzzFeed article for literally
everything this is one of them I joined
BuzzFeed like two and a half years ago I
lead the front-end infrastructure team
in London there's a few rows most of us
are in New York and the way we learn
about resilience was kind of a baptism
by fire situation like things broke
immediately after I joined but what is
resilience anyway resilience is function
in a hostile environment and this is
prescient for our jobs because the
browsers this ultimate hostile
environment right well we can't rely on
anything can't rely on the runtime and
things are built upon we can't rely on
resources or you know things to be
available any particular time and any
kind of sufficiently complex system
which we're all building these days kind
of lives in this world of degradation
where it's never really running its
optimal capacity so today we'll talk
about like like some ways in which our
systems can fail some ways that we can
build to like be more resilient to
failure and somewhere you can mitigate
risk completely so that we don't
actually have problems in the first
place
all right so how can our systems fail
one of the first ones and one of the
more catastrophic ones usually is
third-party availability so a timeline
for when things don't breaking in
BuzzFeed is I joined in 2016 immediately
after I joined dine this DNS provider
had this big global DDoS outage which
took us down for about five hours whilst
we mitigated that shortly thereafter s3
went down in a bigger way that was quite
problematic for us not just to keep the
the main website up but what
systems behind the scenes relying on s3
then we rewrote all of our systems we've
been more resilient shipped something
completely new and an hour after we went
live fastly misconfigured something and
drops our cash which we were relying on
quite heavily and it took us down for it
how oh and then there was also
repercussions for a few more hours after
that and then just to sort of add insult
to injury
AWS went down again that's three went
down again but this time we learned our
lessons and whilst it affected us for
two hours there was no issue to the
users whatsoever for those two hours so
third parties aren't bad we all rely on
them we you know we we don't want to
solve all the problems of these
companies sold for us but at the same
time we have to understand the day will
go down and like when they promise seven
9s five nines whatever that's really
indicator of the past rather than a
predictor for the future and we should
always be ready to wear work with Arlen
all right another one which is more in
we're facing is that our own both are
ways in which our systems can fail we
should be tracking them all like this is
essentially which is a great tool for
using to track key our JavaScript bugs
and something which like is often more
important than a lot of the systems and
alerting and monitoring we do there are
many times where a number of 5x X errors
will fire off a bunch of alerts in the
backend which have way less impact than
a single like JavaScript bug and we need
to be monitoring this because actually
these days our systems are more in the
in the client-side and these are where
things happen but this is good for
catching exceptions and there's I want
to tell you about a bug that we shipped
which this didn't catch which was where
we tried to switch our this is just
turning to really like telling you all
how bad those videos but there's
methodology which is that we shipped a
global configuration change in our CDN
it is to say the turn on something
called image shielding but essentially
than what this meant was that we were
going to save the money on us it's
serving images and one of the things
that we do is we serve web p images to
people on chrome right
so unless over the over the wire and we
only send out to chrome because he
doesn't work in Safari with him and when
we ship this configuration change
we sent web PZ or everyone and suddenly
images weren't working and Safari for
example and we couldn't see their sonar
errors because when an image that fails
to load it's not an exception it's just
an event like fires and so we we quickly
rolled it back because we heard from
people live is broken and we thought
about how we can really say again but in
a way that we actually know things there
things are happening so we wrote a
little simple ish system which
essentially just allows us to fire
arbitrary events back from the client
and say it give us observable
observability over when things are
breaking and then we rolled it back out
and it was fine now I got hit
and this was kind of interesting to me
because it was a fun little project to
do but also it gave me this suddenly I
can actually observe what's happening on
all these client systems where before we
were kind of guessing a little bit that
things were working correctly and now we
actually had like raw data came back
with errors than one of the most
important things is just to feed it back
so like people won't go into century
they won't go into that dashboard they
will be in slack they will read errors
they're going to slack and whenever we
have these outages instance whatsoever
we always run post-mortems afterwards
where we discuss what happens how we can
do better with it and that kind of thing
and one of the most interesting
questions for me in those scenarios is
like how did we learn about the error
like when like what caused us to find
out was it our alerts if so brilliant
could our alerts have caught it sooner
or did product support tell us and for
us like we have an engaged audience and
this is pretty much the the metric that
I care about it's like like if if
Twitter had to tell product support that
our site was broken we've done a
terrible job and that's not good
all right another way in which we all
I've heard a lot is that can break is
the network and the network is flaky no
something predictable and we can't rely
on it to be available it's something we
hear and you know the majority of
performance talks these days and it's
very true and but we haven't really had
a great way of measuring that in the
past and at least I haven't and but now
that we had the assistant we've built
for the image loading errors we kind of
had that instrumentation and so we
extended it and we turned that into a
more broad like method method for
testing this stuff so every time that
JavaScript fails to load once fails to
load images fail to load we capturing
this all the time and sending this data
back and what we found was like
validation of the things that we've been
taught in the past and we found that
just over 1% of all requests for our
JavaScript bundles will fail or time up
and that's not a staggeringly large
number but kind of our numbers that's
thirteen million pageviews per month
that won't get the JavaScript so if we
were relying on that for the users to
have any kind of experience where we're
already excluding 13 million sessions
it's not ideal right so let's say this
is all worked right everything's up
they'd written no bugs obviously the
network's fine we should be fine however
the user still has the ability to say
I'm gonna change this and that's right
they should have the ability to change
that and they should have the option to
opt out of things to opt into things and
they do and 9% of our users use some
form of content blocker and that's great
for blocking ads not that we have that
many but like yet sure let's block them
and but you know it's not just ads they
can get blocked and just under four
percent of people won't successfully get
paragraph ones and that's a big number
that's we're now talking like 38 million
pages a month and if we're relying on
that for icons or
I'd know anything really we're coupling
like the stylistic choices to actual
user experience to actually fail these
experiences kind of a lot of stuff so
what are our options okay so we could
hope for the best and it's not
completely flippant
I feel like sometimes your app doesn't
have to be like the most reliable thing
in the world trait yeah if it's an
internal tool if it's something which
doesn't need to be timely on your setup
time you could rely on the fact that s3
is up for a ninety nine point nine nine
ten a time and so that's good enough for
my for my system but if you're a company
who relies on e-commerce or maybe like
Black Friday or Christmas being your
sales for the entire year and then as
three goes down then you're really gonna
not want to be in this position so
instead we design for failure so in this
section I'm gonna talk about couple of
ways that that we do that we think about
that in which they're very accessible
for everyone else to go away and start
working with all right there's so talk
about prioritizing the critical parts of
the page first of all you've got to
think about what are the critical parts
of your pages but if we build the the
graph we start with the user request
some HTML I'm just going to talk here
hopefully it's not too and from there
we'll get some CSS some Java scripts and
images maybe that CSS also makes more
across four images maybe some fonts
JavaScript Gerizim affected some data
maybe that data comes back we use some
templates to stick it in the DOM and we
request some images we build up this
this network graph of what's required
and if if we have to get down to the
bottom to the green part before we have
a workable user experience we're kind of
relying a lot of things going right for
us to get this alternatively if we can
give an experience by the top and then
the rest is just the gravy on top then
we're in a much better position to at
least allow this basic level of you know
interactivity there's nice lovely
and I'm possibly the the main thing is
the article do we care about here's a I
tried to find the temporarily relevant
and if you know if we just get that
first bit of the graph for us you know
it looks like this which is fine
it's not great it's not gonna win any
awards but it's a starting point and
it's actually like the minimum you can
get as long as you can get our first
request through
so for us we focus on the article and
that's not always going to be the case
if we take this website and I might be
wrong about this because people can
correct me in this room but I would
imagine that the most important parts
are form I know is that am i right yeah
okay cool so really the most important
part the interactive you don't really
care about the image I really care about
the links at the bottom or the links to
the App Store you want someone to search
for something so if you can send down
everything you need to fulfill that user
behavior then you're already the the
great starting point and you can do that
in that first request the second thing
is to make errors a first-class citizen
and this is about designing your errors
and it doesn't mean designing like what
the error message will look like it
means designing the failure states like
what does your app look like when
certain things fail or you know other
words would be like something broke do I
need to tell a user should I tell them
this has happened I can I get away with
the app just you know being in a
different state for example this article
we have on the right-hand side we have
this BuzzFeed news module which is
loaded in separately and if this fails
do we need to tell the user do we care
you know if it should we just instead
slide that up and like get on with it
and like maybe some users will care
maybe some users will notice the
majority groan we need to know as
engineers we should be beaconing about
that has caused an error but for all
intents and purposes that that page is
fine that page still keeps going and the
flipside if we had this is our CMS which
is publicly accessible anyone can go
right BuzzFeed posts which is terrifying
to me there's a reliability person
however and if you fail to publish
correctly we can't just pretend that it
worked right in that situation we have
to think about failure States being more
in-your-face and we can put up something
big but we can also think about how it
falls back that we can say something to
local storage and say you can you can
come back later and finish it off there
are ways in which like we can accept
this build for it and and give some kind
of user delight all right
so that's some ways to think about
design for failure but what about like
avoiding it altogether that would be
even better right there are certain ways
we can mitigate risk I'm going to talk
about two ways they're kind of like high
level principles ways that we can that
you can take away and expand on and
think about better ways of working and
the first is building in redundancy so
we can use s3 as an example here and in
this situation we have this asset server
and for us to render the page we make a
request to this server and ideally that
returns some CSS and then we can show
the page but this server is on fire
right so instead we can't we return
nothing and the page is blank right this
is the situation that a lot of sites
were in when s3 when done so in a
redundant system with redundancy we have
a secondary asset server and instead you
know we can make this request over to a
different URL return some CSS we're
running the page and this is semi ok
right we there's two ways of comforting
this you either have feature flag kill
switch situation where you flip from one
or you can I suppose you could time it
out and running back it's better than
nothing you're gonna be in a situation
where probably the page is gonna work
this is called an active and inactive
system so 99.9% of the time your
traffic's going to the first one and
then in a bad situation you've flipped
to the second one for that for a CSS
maybe this isn't so bad but for some
situations that's can be almost as scary
as not having the first thing because
you haven't been testing this system
it's off the critical path that you
haven't you don't really trust it
because you're not using it what's
better is to have an active active
system or we can we can use both at the
same time and this is a compatible by
having like a proxy service that sits in
between so instead of having one and two
in their domains we make this request to
the proxy service asset server calm that
makes those requests to the server and
then we can return this back when this
is the system we have running it
buzzfeed so the talk wouldn't be asked
through the bottom is Google Google
Cloud and we send 50% of requests to one
50% of the other and it flows back and
forward and we a floater both when we
deploy and this this men the second time
has three went down like I didn't get
buzzed like nothing happened it was fine
he moved on and this isn't this is
applicable here to rest three but it's
flickable to every dependency that you
have it could be if you want to go to
the full level it could be your cloud
you could have AWS and Google Cloud for
all applications or you could if you use
an image service maybe to resize images
dynamically what happens if that one
goes down you're gonna serve like image
of these images with no resizing no
images at all like you can think about
ways that you have this and we've
identified like pretty much every single
dependency we have and how we have
redundancy because we've learned from
experience that they go down so the
second way the second way we can be
mitigating risk is serving a stale
content so stale content is that it's a
term to describe
continents not delivered from your
service so it's not guaranteed to be the
freshest
data you have available and we can make
it look a little by this we can use a
CDN in front of our house our server and
use that to carry some of this look so
we make a request if it goes this server
we send that back to the CDN and we send
back some cache control headers and we
save you you hang on to this for a day
whatever it might be that gets sent back
to the user and then the next time the
request is made we don't have to go
through back the server we can just go
to the CDF and this point that content
is being served to the user is still
like it's no longer coming from us so we
don't know if it's the most up-to-date
but for the most part we can be happy
with that it can be okay particularly if
this servers on fire we're gonna merge
better situation and most CEOs can be
configured with this surrogate keys for
cache control and there's some extra
keys you can add as well like stale
whilst we validate so if if I go and
fetch some fresh copy I can keep serving
the stale content whilst I'm waiting for
that first of and crucially and stale on
error so in this situation we go again
now it's on fire right because the CDN
won't get a good response back from the
server it will just keep serving that
stale content even though it's running
high like even if you cashed it for a
minute and it's been now in 24 hours we
can still keep serving this is a super
useful tool and it's good for a couple
of reasons one is like the users can see
stuff that's great obviously the second
is that when you're in an instance
situation where something's on fire this
gives you breathing space to make good
decisions and they can be kind of kind
of scary and terrifying to make these
calls when everything's burning around
you and if you know that people are
getting an experience like you can just
take a step back think about the best
way of solving a problem going forth and
yeah what if this request doesn't mate
doesn't work though what if this request
doesn't work right there never was gonna
fail like less than one person at a time
but still well we can introduce another
cash for still content yeah we can stick
a service we can go and we can use that
to store stale content and deliver that
if we can't even go to this you can and
I'm not going to go into the yes there's
so many good blogs and articles about
techniques for using this but this is
essentially the same concept alright so
we talked about you know everything's
gonna fail
sorry like it happens some ways in which
we can design for that design for your
systems to work particularly if you rely
on third parties which can like hinder
your entire business and some ways in
which you can mitigate risk altogether
and because it's buzz I wit BuzzFeed
there's been zero gifts in here there's
a bad job so I will just summarize by
it's like saying be like these people
and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>