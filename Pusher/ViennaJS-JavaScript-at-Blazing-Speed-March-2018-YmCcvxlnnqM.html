<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ViennaJS, JavaScript at Blazing Speed, March 2018 | Coder Coacher - Coaching Coders</title><meta content="ViennaJS, JavaScript at Blazing Speed, March 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ViennaJS, JavaScript at Blazing Speed, March 2018</b></h2><h5 class="post__date">2018-04-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YmCcvxlnnqM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Michel shots new from I work as a
software engineer at Google I work on
the v8 team v8 is Tim for those who
don't know is the JavaScript engine
that's powering Chrome and no chance
I've been working for Google for
seven years now and fun fact before that
I actually lived in Vienna for I think
eight years so being able to talk here
and then being invited here actually
brings back fond memories so that's
great
before v8 I worked on an open source
java vm so anything to do with compilers
or virtual machines is super interesting
for me so I love it all right but enough
about me let's talk about v8 actually
the things I want to talk about today
are our new compilation yes does it work
our new compilation pipeline our
compilation pipeline is basically a tale
of two compilation tears and compilation
tears are basically basically levels of
optimization that we can apply to to
your JavaScript application yeah
also the optimizing compiler that we use
uses speculative optimizations so I want
to talk about that a little bit and how
it's based on feedback because that
might be interesting for for application
developers how we actually optimize the
JavaScript code and one particular
optimization I want to go through is
inlining because it's it's the mother of
all optimizations so to say it enables a
lot of other optimizations and then in
the second half we'll look at two
examples first one is a higher or it's
the inlining of higher order built
instead we do and the second one is
JavaScript generators so then we'll keep
us busy for a while
yep all right let's serve if the
compilation pipeline as I said it's a
tale of two compilation tiers the two
tiers are made up of two big components
two main components one is called
ignition it's a bytecode interpreter
and the second one is called turbofan
it's the optimizing compiler so bytecode
interpreter actually means we transform
JavaScript into bytecode and then
interpret it so it's not initially
transformed into machine code but
bytecode and we run an interpreter on
that and that whole process
transformation into bytecode and
interpretation is called ignition and
then we have the second tier turbofan
which is an optimizing compiler so that
one actually generates native machine
code so why do we have these two big
things why do we need two compilation
tiers
we don't just add those because we like
complexity so much but there is a
certain trade-off between those two
things ignition on the one hand provides
us with a fast startup it has a low
memory footprint and so we we get to
execute the application of JavaScript
application quickly we don't use much
memory and we kind of can observe it for
a while and also ignition handles all
cases so that means whatever crazy thing
the application throws at us we can
handle it
turbofan on the other hand requires a
certain time of warm-up so it requires
the application to run for a while for
us to observe it and to then optimize it
and that means we take longer to
actually compile the application and we
only want to do that on a certain subset
of your application we don't want to
optimize everything because that would
just take away too many resources and
turbofan is the thing provides peak
performance we take the pieces of your
application that are really hot really
important and spend time on optimizing
those so JavaScript initially is fed
into ignition the unit of compilation
for for ignition is a function so we
compile one function at a time we parse
the entire script but we look at one
function at a time compile that to
bytecode and then start running it we do
that lazily that means we don't do it
for the entire script
once but we do it for the function that
is called first
and whenever lab function calls another
one we look at that one so it's
basically the cost of compilation is
distributed a little bit because we only
need to look at one function at a time
and don't stall the system for the
entire script what ignition produces is
as I said bytecode this bytecode is then
interpreted and ignition also does
there's additional work that it would
not need to do if turbofan was not
around so you see this this nice gift
package at the bottom feedback ignition
collects feedback so we observe the
application and we look how it behaves
how certain pieces of the application
behave and just for ignition sake we
wouldn't need that but we need it to
later on optimize the code with turbofan
so what true fan Lang consumes is this
feedback but also the bytecode so
turbofan actually no longer looks at the
source code at the JavaScript source
code it Just's based on this
intermediate bytecode that we that we
compiled that has several advantages one
is if we decide to do sugar certain
language features on the bytecode level
turbofan no long has to worry about them
so we can basically reduce the
complexity of the JavaScript application
a little bit make simple a bytecode out
of it and turbofan only has to worry
about this simple bytecode and what
turbofan generates is actual machine
code so it's actually natively running
on the processor and hopefully gets
almost close to native speed so that's
basically the holy grail of the of the
compilation pipeline to approach native
speed all right let's take a look at one
example so this is super trivial example
but stick with me it gets it gets tricky
the addition function that you see is
basically just containing 1/1 arithmetic
operation 1 addition of two arguments
that come in and it's called from
another function so let's first look
just at the first function
just edition function what does an
edition do as long as we all know it's
overloaded so it can do string addition
it could be an arithmetic addition but
it can also convert objects into
primitive values and what we see on the
right-hand side is actually the
specification so that's what Aquino
script standard dictates that a Java
Script a faithful ACMA script
implementation should do add an addition
and the actual the actual addition is
step 10 so all of the steps before our
preparations to actually perform the the
arithmetic operation and you can see
that there are certain things that are
specific to strings but also these two
primitive calls that are always kind of
expected to be done by us so okay if we
want to compile it we have to somehow
implement that so let's look at what two
primitive actually does two primitive in
the end actually gets a method on the
potential object that you pass in to be
able to call and a user-provided method
to convert something to a primitive okay
so we get a method how do we get a
method get' method actually means we
need to do a property look or
potentially on the object right so okay
what does property look at me well okay
property lookup we need to potentially
go up the prototype chain actually do
multiple gets here that's the yellow
part that you can barely see there again
and okay all of this is way too crazy
already for for it to be to be efficient
and fast so if we want to optimize we
actually don't want to do any of this
what do we want to do is don't care
about this we only want to do the
addition and to some degree that's
actually what happens what we do is we
make some speculative assumptions about
the input about the left value X and the
right value Y and that's where this
combination of these two tiers helps us
so as I said ignition gathers feedback
so it kind of knew okay I only saw
numbers at least the
at this edition site and it tells the
optimizing compiler hey you only have to
worry about numbers because I saw
numbers 500 times so there will not be a
string around to roofin it like yes can
do that and K and turbofan generates
highly optimized code for exactly depth
that kind of situation because their
port was the situation it was observed
before but then on the five hundred and
first iteration it might be that you
suddenly pass in a string or an object
and that's where it gets a little bit
hairy because then turbofan is out of
luck turbofan is like I I don't know how
to handle strings so it needs to be
optimized back to ignition and you
already see that there is a cycle
forming here so if we are not careful we
basically could run in cycles for a long
time and we need to prevent it because
every time we optimize we need to spend
a lot of time to to to perform smart
optimizations in the compiler and that
takes memory on a phone that takes
battery power so we need to be really
careful that every time we go around
this circle we actually learn something
new so the next time we go around the
circle ignitions needs to tell turbofan
hey you need to worry about numbers and
strings because I've seen both and we
had the optimization loops that's what
we call them a lot in the past so that
that that's the situation when you don't
learn and when you just run around in
circles and spend all of your time
optimizing and optimizing alright so
back to this example so far we only have
looked at the ad function but if we look
a little bit broader if we look at the
entire script or at least the two
functions we can actually see that we
can be way smarter because not only do
we know that we pass numbers in we pass
actual constants in and so let's where
in lining comes into place so if we were
to inline the addition function into the
F function we could actually look at two
functions at once so initially I said we
look at one function at a time but
turbofan actually can perform inlining
so there we can look
two functions at a time or multiple
functions at a time all right so so far
all we saved is one function call so
that's not that big of a deal so why is
inlining called the mother of all
optimizations if it's if it's just
saving as function calls the reason is
that inlining enables us to have a
broader scope it brought a view of the
of the application and thereby enables
other further optimizations to apply so
inlining runs pretty early in the in the
compilation phase and then enables a lot
of the powerful optimizations come later
to have more impact to basically apply
to a broader range and in this case one
search optimization would be constant
folding so we could just see now Oh in
this one combined function that we have
we see two constants they're being added
we don't need to worry about string or
too primitive conversion because we know
there are primitive numbers so we can
just constant fold them and that's
basically in a nutshell what turbofan
does kind of okay I took a lot of slides
to to to just to just explain that one
of my colleagues addy Osmani actually
can do that in one slide so that's the
recap we take your JavaScript we parse
it then one function at a time we build
abstract syntax tree and generate
bytecode out of it we run it for a while
to gather feedback once we got feedback
for these speculative optimizations we
if it pays off if we deem the function
to be hot we run it through turbofan
optimize it and compile it to actual
machine code so thing that comes out is
then specific to the underlying hardware
that you're running on and the eight
currently supports eight architectures
I'll leave it as an exercise for the
listener to figure out which one's all
right so that's the compilation pipeline
so far which we overhauled the
compilation pipeline over the last
year's so turbofan is a fairly new
compiler again
is a fairly new bytecode interpreter
what I mean by that is they're shipping
for at most a year now both of them my
personal definition for success that I
always looked forward to in the in the
years I've worked on turbofan was once
we can inline higher order air a
built-ins I can declare success and we
can do this now so yeah alright let's
look at higher or built-ins another
example let's look at the reduced
building of an array reduces is we've
reduced you can for example Express
summation function so you have an array
of full of numbers and you want to just
calculate the sum of it what you would
write is potentially something like this
you provide a function to this higher or
in a built-in namely the arrow function
that just takes an accumulator and some
value and adds those two together and
you provide a starting value then you
start with zero and just add on top of
zero for example the array 1 to 3 would
yield a sum of 6 all right so how do we
how does turbofan optimize this now
let's go fishing if you would to met if
you were to manually optimize this
function you would write something like
this and you'll for loop with an index
you would iterate over it and you would
also have some accumulator called sum in
this in this case and yeah so that
that's basically if you were to hand
manually optimize it now the question is
can turbofan actually reduce the left
hand side down to this and the next few
slides are to convince you that it
actually can I think so let's look at
the spec of reduce again crazy stuff
going on there a lot of additional
additional things that we didn't see in
the hand conversion and the reason for
this is that the spec in
annalee wants to be generic so you can
take the the second order the higher
order built-ins of array and use it on
other objects you can come up with your
own array implementation and just use
the generic array functions and the spec
actually mentions this in a note so
there is this one note which says oh
reduce function is intentionally generic
it does not require that this value is
an actual array object so all of the
craziness that we saw in the in this
that we see in the specification where
we have to worry about - string
conversion the has property is callable
check all of that is intentionally there
and people are using these functions in
a generic way so we need to be faithful
to respect if we are faithful to the
spec we end up with something like this
the function that we passed in is still
being allocated at the beginning of foo
we have this addition function then we
extract the length out of it then we
actually have to check is the function a
callable is it callable can we call it
otherwise we throw a type error and then
inside the loop you actually see that we
have to do and check whether the
property is on the area itself or up the
prototype chain again that's dictated by
respects so we have to do that and we
see that we have a function call in the
loop so especially the things in the
loop are worrisome because they
potentially get executed many many times
so if we want to really optimize that we
should get the loop body as empty as
possible so the first step we see here
is basically that we in line T did err a
reduce function itself to roof and at
that and on the right hand side we now
see the steps that are applied
afterwards by turbofan to actually get
rid of all the overhead that we see in
there so one such step is for example
okay the call ability check is now kind
of redundant because we see that thing
we're calling is a function so we don't
need to check this this is basically
that
so turbofan can remove it then we
actually can inline the second call we
had inside the loop so we can we no
longer need to actually call a function
we can take the function body and inline
it so basically now we are with inline
twice already so far this only got rid
of the of the of the call but in a next
step we can and this requires some
explanation so we can assume certain
certain
certain invariants of the area to hold
so array internally for us can have
multiple shapes it can it can contain
objects it can contain double numbers
floating-point numbers it can contain
integers and another aspect it could
have hold or it can be packed and packed
means you don't have any holes it's a
contiguous area it's really something
that you normally call an array and this
is pects Maya basically means it's an
array that is packed that doesn't have
holes and that contains into integral
numbers integer numbers so no doubles
and that holds for the area we saw in
the example before right because it
contained one two three nothing else
so no holes and integer numbers and
since ignition observed this turbofan
then relies on it and that that's the
point where it relies on it it puts in
checks and if this check fails it D
optimizes it actually says ok I now need
to jump back to ignition now initially
this looks kind of useless because we
don't gain anything from that itself but
it's again an enabler for optimizations
that we apply then so in the next step
for example we can now see that since it
is packed
there cannot be any holes so this check
whether the property actually is on the
array or not is now redundant and since
this check is in the loop this is super
beneficial because that means on every
iteration we can assume the element is
still is in the array we don't need to
walk
the prototype chain and everything just
becomes way simpler and also on top of
that we can now see that since it
contains since the area contains
integral numbers we can actually also do
addition of numbers so the addition
again is specialized as we saw in the
previous in the first example and so by
now the loop body actually already gets
smaller and smaller and with the last
step we can also remove the shape check
because in the loop body we see that the
shape of the area cannot change there is
nothing that writes to the area that
deletes elements from the array or that
that somehow changes its shape like for
example if you were to freeze this array
this would be a major shape shape change
for example and so now we can again
remove a check inside the loop and the
loop body now is really it's really
optimal and actually resembles the
handwritten version that we had before
and so this is almost the same as we had
in the handwritten version before and
actually this Det edition inside if you
would not inline the area we'd use the
addition inside would in the handwritten
example also also be lower to a number
addition so the loop body is exactly the
same as we had in a handwritten example
all right so I threw this at a I wrote a
simple micro benchmark just for this and
and measured the difference of the
handwritten version and the the one that
is cool called from manual the dark blue
one and the light blue one is the one
with with the area reduce and I compared
an old version of me eight that didn't
have T the area on the higher-order
built in in lining and there you see a
major difference so using the higher or
the array built-ins cost a major slow
down there but we have a recent version
so six
26s was released I think a week ago or
two weeks ago you see that there is
almost no difference differences of this
tiny scale in micro benchmarks they can
just be order of instructions in the
machine code so that they are completely
negligible it could also be the other
way around
so for all intents and purposes the
speed is same as the handwritten
JavaScript version so I hope I convinced
you that true fan actually can turn the
nicely
idiomatic JavaScript second order era
built-in into something that is as fast
as the handwritten version yeah it took
us I was so tough and it took us I think
four years to get there so - finally we
started writing from we we started to
write turbofan from scratch basically
the only thing we reused from our old
compiler was the assembler so that thing
that assembles machine code that outputs
the machine code I mentioned we have
eight architectures so we don't want to
write eight new assemblers but other
than that everything in turbofan is is
brand new and shiny alright let's do a
recap so I just showed one example with
the array reduce but the same principle
applies to basically all da-rae built in
so you can think of the higher order
ones so like map filter every some
reduce there are also other built-ins
that do similar things that also have
loops in them for example where
basically you have exactly the same
example slightly different that is for
function apply or function call so all
of those get optimized in the same way
and again our optimizations are based on
feedback that we got during during
ignition run we in line known call
targets so that means if you if you add
a certain use site of the second order
built and always use the same function
we can inline it so that has a major
advantage if you if you don't put
different
functions into into one place in an
array every for example also it assumes
the shape is unchanged of the area so
basically if in your function you start
deleting elements from the array that
would defeat such optimizations or if
you if you change if you for example
start putting double elements into this
array within the function all of that
would mean that we couldn't remove these
checks that are inside this hot loop so
avoiding this shape change is if it's
good for performance and also we
specialize the arithmetic so if most of
the time you perform integer arithmetic
we learn from that and CEO we can
produce better machine instructions if
then once in a while you throw an N in
there or a double number just to spice
things up and we need to do PM eyes
again and learn from that
that doesn't mean you cannot mix doubles
and numbers you can totally do that it's
just about the consistency of doing it
if you do it once in in in once in
thousand occurrences probably we
optimize before if you do it every other
time okay we learn about it
so having consistent arithmetic in
inside you're inside the functions you
pass through these built-ins also helps
us to optimize better alright so that
brings me to the final part of my talk
and that is JavaScript generators so
just by show by a show of hands who here
has used JavaScript generators directly
already okay half of the room I would
say who here has used async functions
for example little bit more ok so
JavaScript generators they're building
building blocks for async functions so I
will mostly talk about generators and
all of what I say basically applies to
async functions as well because they're
based on generators
so before we talk about generators we
need to talk about JavaScript and the
heap and when I say the heap
I mean basically our garbage collected
heap so before I worked on turbofan I
actually worked for you for years on the
garbage collectors and also whenever I
can throw some slides about objects and
the object heap in there I will do that
and so this slide basically shows that
while we're running JavaScript we
populate this this grab bag of things
that we call the object heap and inside
there you might have arbitrary objects
that the application allocates like an
object literal or if you say new array
all of this is a JavaScript visible
object we call this J s object and let's
draw drew it yellow on the slide
it basically means okay the application
can actually look at this object touch
it it's visible to the application it's
useful to the application same for
functions they are also just objects
floating around in this heap and then on
the C if we also have internal objects
that only the VM is allowed to touch and
look at but they're also part of this
whole garbage collection scheme so that
has the advantage that we don't need to
do explicit memory management for for
our internal metadata metadata we can
rely on the garbage collector to handle
both application provided objects as
well as daemon metadata and the
interesting part is even the code that
we generate the machine code is inside
this heap so that means the garbage
collector can also throw a machine code
if it's no longer used so if you have
functions that get optimized and then
all of a sudden you no longer use them
the code gets also collected and
actually if the function itself dies
that holds on to it let's look at an
example and this example is is basically
trying to explain where variables can
can live so sometimes they can local
variables can be on the zone on the
machine stack they can be local to a
function and sometimes they actually
live on the heap so they
take part in this garbage collection
scheme in this example we have a create
function with a local variable X that is
passed in as an argument but the X is
also used inside another function right
inside closure and what that means is
that when create returns the value of
the variable is still hold on
held onto by by F so that means it
actually needs to live somewhere that is
longer than the lifetime of the create
function and what this means is
basically the first time you call this
create function with for example a value
of 23 for X we remember this value 23 in
a canonical location in the heap and
this canonical location where we store
these variables is called context and
the function binds this context and if
you look at the bigger PD entry for for
closure
it actually kind of defines closures
like this it says closure is a
combination of a function and its
execution environment so the function
itself the behavior is encoded in the
code object and the execution
environment the variable surrounding it
is encoded in the context and also we
put the feed back there so we have
another object where we gather all of
this feedback about how the function
behaves if you call create again you get
a second closure but the code actually
can be shared so that means when we
optimize the function f later on we only
need to optimize it once and we can we
can share the underlying optimized
implementation even if the values are
different for the context and also the
feedback is shared so that means if you
have multiple closures and you you you
use the arithmetic in them consistently
optimization again benefits so why am i
talking so much about closures when we
actually want to go to generators reason
for that is that
this scheme where we store variables on
the heap is actually pretty pretty
useful when we talk about generators
because generators are functions that
can be suspended mid execution if you
yield inside a generator it kind of
returns but remember it's its execution
state and the execution state is kind of
the same as this context that the
closure binds so let's look at it and
finally look at an example with a
generator so in this case we've
generated with to yield points and one
return so that basically means it can
produce three values and after that it
will just say I'm done the first time we
instantiate the generator a J's function
for it is created and the generator
object that is returned actually points
to that function so every time that
generator is then invoked again and
resumed again we know which function to
execute and the context is also stored
by the bettor generator object so that
means we can resume the function with a
certain execution environment now all
that's missing is basically to tell the
function where it should resume right
because if we if we resume it from the
top it would have a different behavior
so we also store this continuation point
and the way we store it is actually we
just number them we number all of the
yield points we give them numbers from
zero to whatever and that allows us if
you then call next on this generator for
the first time we basically call into
this function
it knows oh my resume point was zero it
runs until the next yield point returns
a value and updates this continuation so
the next time we call it we know where
to continue inside the generator also
all of the variables inside the
generator our context are located so
that means we don't need to worry about
restoring them because they're already
put on the heap they're already put in
the place where we can
per system and that just continues on as
long as you resume the generator it
always suspends itself at the next yield
point so that's for the object model now
how do we actually model the generator
function itself so again I'll try to
explain with with pseudo JavaScript this
time this translation is not done by
turbofan but the ignition so it's it's
done the very first time you compile a
generator we do some translation to it
to actually generate bytecode that more
resembles as if you would have written
this switch case on the right hand side
this switch case contains some some
magic that you wouldn't be able to write
a normal JavaScript like accessing this
continue this continuation point this
special number and it also there are
some scoping issues with it so we
basically need to create the scope
around the entire function but the
bytecode for all intents and purposes
has a switch inside so every time we
call this function we read out the
continuation point dispatch to the
correct point and can continue running
there now the advantage of letting
ignition do that is as I mentioned
before we can actually optimize this
function we don't need to teach turbofan
anything about generators because it
just looks at the function that has a
switch inside with this magic load of
the continuation point and that is super
useful for us v8 engineers because we
don't we we we don't need to worry about
generators and turbofan at all because
to refine is complex enough without
throwing generators in it the very first
implementation of generators used a
different model and basically we were
never able to optimize the generator
functions now with this we can all right
so to recap for generators all the
variables are stored on the heap so that
means there are no longer local to the
stack of the function accesses two
variables are a little bit slower
generators are the main building blocks
for async functions so all of this
applies to async functions as well
internally we translate to the switch
but other than that all optimizations
apply so basically the only drawback you
get from generators is this slightly
slower variable axis but the slower
variable axis you might have experienced
with functions already because as I
showed in a previous examples this
context binding is not context
allocating of variables is nothing new
so in essence the upside of that is we
can optimize generators pretty good so
or not you shouldn't be afraid of using
them they should only get faster in the
future all right and then we have to
generate objects with the additional
values so there is a tiny overhead of of
additional memory to that we need to
hold on to but also that is negligible
and you buy into the same machinery that
was already there for context allocation
of variables again so there shouldn't be
any any major surprises in memory usage
with generators and with that we're
almost done
already out of a voice but I just wanted
to remind you that you can actually
follow us on Twitter now so over the
past let's say one year one and a half
year two years we try to improve our our
developer outreach we try to reach more
people we try to not be this this closed
of hidden be a team that only looks at
their at their micro benchmarks we try
to look at real world websites I hope we
improved over the past years we have a
Twitter channel now and we have
developer we have a deferral person now
and so please follow us tell us what are
your problems let us know we really want
to optimize for the real web we still
have benchmarks that we optimize we
still care about benchmarks because
that's that's our way of reasoning about
about performance but we really want to
hear your experience about for example
generation so second order built
and by the way it's this v8 v8 yes it's
not that don't follow them
alright with that I'm finally done
thanks for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>