<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Art of Giving and Receiving Code Reviews - JS Roundabout - March 18 | Coder Coacher - Coaching Coders</title><meta content="The Art of Giving and Receiving Code Reviews - JS Roundabout - March 18 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Art of Giving and Receiving Code Reviews - JS Roundabout - March 18</b></h2><h5 class="post__date">2018-03-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ua4cT-sVI7A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi okay so a brief introduction my name
is Alex and I'm a senior software
engineer at Imperial College I've worked
in a number of different organizations
of various sizes actually and I've seen
really variable code review practices
which I think vary a lot according
mostly to the attitudes and
personalities of the people within the
teams but also broader organizational
and cultural practices so I want to
offer up some motivation for doing code
reviews as well as some tips for doing
them well out of interest who here has a
regular code review practice okay
good so about half of you but a lot of
people a lot of people are not doing
code reviews so a brief introduction to
why you want to do them so what is the
motivation for doing code reviews well
the first thing is that they're
extremely effective at finding defects
in your code it's really simple but it's
incredibly true so these are some
examples from a book called code
complete by Steve McConnell so in one
software maintenance organization about
half of one line changes where an error
before they introduce code reviews when
they introduce code reviews only 2% of
those changes were an error so that's a
massive reduction in defects just by
introducing code reviews into your
organization
similarly okay this one is a comparative
study so 11 programs developed by the
same set of developers about half of
them about five were developed without
reviews the other six were developed
with reviews when you compare the
programs at the end the first five have
an average of 4.5 errors per hundred
lines and the six that have been under
review have only 0.82 errors per hundred
lines so again a massive reduction in
defects when you introduce code reviews
and finally another organization large
organization 200 people reporting a 90
percent decrease in defects after
introducing reviews so it's pretty
consistent we're seeing like an 80 to
90% reduction in defects just by
introducing code reviews which is
amazing and so that's a really
measurable kind of motivation for doing
code reviews there's other things which
are a little more amorphous a little
harder to measure but I think still
really really important ok so the first
one is learning opportunities and this
goes this goes two ways so on the one
hand if you submit your code for review
you're likely to get a lot of really
helpful comments back from your peers so
I'm right now coding in Python which is
a new language for me and every time
pretty much I submit a code review I get
a bunch of helpful helpful suggestions
from my colleagues were more familiar
with it things like libraries that I
wasn't aware of helpful utility
functions I didn't know of maybe just
alternative syntax options or terse or
syntax that they're throwing out there
so I know that I learn a lot from those
code reviews and it also goes both ways
so you learn from reading other people's
code I'm a self-taught developer I
imagine probably many people in this
room are and we will have learned a
great deal from reading other people's
code well reviewing your peers is a
great way to just spend some time
reading other people's code you might
see a neat way of doing something that
you hadn't done before you might
discover a new function a new style you
might be you'll just develop your coding
skills in general and this applies not
only to junior developers but actually I
think to developers at all levels future
proofing your codebase so again this is
a little bit hard to measure because by
definition if the payoff is in the
future it's really hard to measure it at
the time of implementation but what do I
mean by future proofing on the one hand
you can increase your bus factor by
having other people multiple people look
at code the bus factor for those of you
who aren't familiar is the number of
people in your organization who would
have to get hit by a bus in order for
your project to fall apart and actually
I really really hate this terminology
because if one of my colleagues was hit
by a bus I don't know about you but the
last thing I would be thinking is oh
thank god someone else knows the code
base right so I prefer to think of this
as your Hawaii factor so this is you
know if one of my colleagues or how many
of my colleagues would have to win the
lottery and take a wonderful early
retirement in Hawaii for my project to
fall apart okay so you could increase
your Hawaii factor you ensure that code
is readable this is obviously super
important for future proofing again
because the developers working on a
project are going to change over time
and you want to make sure that anyone
can step into this project and get up to
speed and dive into that code without
massive overheads and maintaining code
standards again an incredibly amorphous
aim but but there's something that's
incredibly important as well so whatever
the best practice that
you bring into your what you know
there's as we have seen in the previous
talk that's obviously many debates over
best practice in code standards but
whatever those standards are that you
adhere to as an organization a great way
an important way of enforcing them is
through code review and you can also
through the through code review also
explore and develop those code standards
as they change over time okay so back to
that really measurable thing defect
finding there's a few simple things that
you can do to make your code reviews
more effective according to that metric
and this graph comes from a study that
was done by Cisco Systems in conjunction
with IBM I've taken this from a blog
post they wrote sway I don't have access
to the underlying underlying data but
they have some nice illustrative
principles that they they themselves
threw out of the data and the first one
is is really intuitive I think you're
probably all recognized as soon as you
see it that fewer lines of code are
easier to review and in fact what they
found is that under 400 lines of codes
you find a lot more defect so this
y-axis up here is defect density so it's
not the total number of defects but it's
the number per thousand lines of code
okay so this is a sort of standardized
measure regardless of the size of your
pool request the size of your code
review and basically what you can see is
that as these get I mean the important
thing to note is just that as these
reviews get longer and longer but
especially after four hundred lines of
code the number of defects found
massively drops off so why is that
because the reviewer simply can't digest
and is probably overwhelmed by that
amount of information and I think it
looks like from this from this
particular study it looks like 400 is a
reasonable cutoff there's other proxies
that you could use for this it's not
always obvious how many lines of code
are in a code review so maybe number of
files changed as a sort of heuristic
that that sometimes works you could say
I'll have 10 have 10 or more files
change in this code review maybe that's
getting a little big maybe I'm gonna
find that hard to review making sure
that code reviews are single features or
single units of work they don't comprise
multiple unrelated changes in one code
review that's also very much related to
this this this idea of pull request size
but size is a really critical
consideration this is quite quite
related and from the same
study and this is the speed at which you
review those owns of lines of code so
again if you review things too fast you
can see that there's quite a steep
drop-off in the amount of defects that
you actually come back as finding after
around the five hundred lines of code
per hour mark so if you have one of
those four hundred line code reviews you
want to be giving that about an hour if
you're doing it faster you're probably
not reading it thoroughly you're
probably not going to find the defects
this one is yeah so so another tip is to
use checklists in your reviews this is a
really simple idea but I know that I
only started doing this really recently
and it's made a big difference in my
organization so this these could be at
an organizational level perhaps you have
some standard checklist of things that
you want reviewers to look out for when
they're reviewing code they could also
be something you could implement on a
personal level so if you know that there
are certain things maybe that you are
prone to forgetting or overlooking maybe
you want to just enforce this create a
checklist of your own for people who are
reviewing your code these are some super
generic ones that probably work for a
lot of people things like are they unit
tests did you understand this does it
follow solid principles if those are the
principles that you adhere to but
obviously these are going to vary
according to organization I wanted to
give you an example because as I say
we're actually doing this
at Imperial so this is an example of a
poor request we are using github for
version control we do all our code
reviews by the github pull request
mechanism which is which is a great
interface and this is a repository for
so I work with I'm an engineer but I
actually work with research scientists
and those scientists are writing code
but they're not very familiar with agile
processes with version control with any
of these workflows so we we got them to
sort start storing some of their codes
reports they were writing in a method
mathematical programming language called
are inside a git repository and we
wanted reports to be reviewed as they
were checked into this repository and
initially when we introduced this it
seemed like it was mostly a formality so
these things were just getting approved
checked in and we were having problems
like weird name
that didn't make sense or code that
doesn't compile even or code that
doesn't run so this is a super super
simple checklist that we use in
conjunction with a very structured pour
request so we have report name linked to
our issue tracking software where you
have a description of what the issue was
instructions for running the report
things you want the reviewer to look out
for and then this really basic check
list is the name sensible is there only
one report in this pull request so
relating back to is there only one issue
in this pull request does it run locally
on your machine so you think these
things are really obvious but as soon as
we introduced this suddenly the review
process started making sense and we
started catching these things and saving
everyone a great deal of time okay so
all of those things are really practical
suggestions thinking of code reviews as
basically a task that one person
undertakes one person sits down and
reviews code but what code reviews
really are is a dialogue between two
people between an author and between a
reviewer more than that what code
reviews really are is a critique of one
person's work right so the author
submits their work for critique now if
someone is invested in their work which
I assume that most of us are and you
know we hope that everyone in our teams
are then that critique is going to feel
personal because it is personal it's a
critique of someone's work and I I think
that bearing this in mind I can
massively improve your code review
process and it's very easy to forget
here's a highly unscientific sample from
my Twitter followers where I asked
people how often that they feel
defensive when receiving a code review
you can see they're a couple of people
actually said that they never feel
defensive and I think that's remarkable
personally and I'd love to know more
about the factors that mediate how
people feel about code reviews I suspect
you know personality how well you get on
with your co-workers how long you've
been doing it how well you're invested
there might be so many reasons that
mediate this but basically the takeaway
message is that pretty much everyone has
experienced feeling defensive during a
code review I imagine that's very common
I imagine everyone can relate to that
whether it's because they've had an
abrasive reviewer or
or someone they just didn't get on with
or whether it's something that they feel
very regularly maybe it's something they
feel in a new job particularly or
what-have-you
I think this is an extremely relatable
experience I want to offer some
explanation of why so the endowment
effect is I don't know if anyone's
familiar much with behavioral economics
so in the endowment effect is a
phenomenon that's been observed time and
time again in psychology and behavioral
economics where people actually value
something that they own more than
something that they don't own it was
coined in this paper where basically
half the participants were given a mug
and half of the participants don't have
a mug and then the experiment has wanted
to see how highly in monetary terms
people valued this mug and it turns out
that when you own a thing you value it a
lot higher than when you don't own it so
you can see nearly double in this
experiment this is the price and dollars
up here so people who own the mug were
seen to value it at nearly twice the
price of people who didn't own the mug
obviously there's not a direct analogy
between ownership ism in the sense of
owning material things and ownership in
the sense of authorship which is kind of
the relevant sense that we're thinking
about code in but really I just want to
illustrate that you know if we can't
even come up with an objective value
system for something as trivial as a mug
the idea that we could objectively value
code and that whether we had written it
and whether we owned it in some sense
didn't come into the equation doesn't
seem likely I'd say it seems likely that
ownership in the sense of authorship is
an even more profound effect on on how
much we value that item and how
defensive we might be of that item so
there's another reason that people might
feel defensive in code reviews and this
is that they might worry that they're
actually being evaluated on the basis of
the comments they receive so you you
know developers might think for whatever
reason that a lot of comments on their
pull requests is a comment on them as a
developer or is even going to affect
their standing in their job is even
going to be like a formal evaluation of
their performance in the eyes of their
employer moving on to the sorts of
things that I think can generate
conflict in a code review and why the
sorts of
why I think code reviews can be a source
of conflict I've sort of developed this
thinking where I categorize things along
two axes so on the one hand comments can
have high reward on the other things
comments can have quite low reward and
on the vertical axis things that have a
low potential for conflict and a high
potential for conflict so I'll talk
first about this upper left quadrant
I've called this I've categorized this
as petachi
because that's I think how it can
sometimes feel to an author when they
receive ten comments pointing out that
their indentation was with two spaces
instead of four or whatever so things
like minor typos whitespace indentations
or also arbitrary preferences which
leads me to the most relatable slide I
think for me which is there's always
going to be you know multiple equivalent
ways of doing things that don't
particularly matter don't particularly
affect the performance or the logic they
don't reflect some like higher design
principle but people have these
arbitrary preferences and it can be very
frustrating when you've submitted what
you think is great code and someone
picks apart these these apparently
pedantic complaints only on the opposite
quadrant down in the low conflict but
high reward category this is kind of
coming back to those those defects we
were talking about earlier so I
characterize these these things as
factual like oh you're missing a test
for this functionality or you've
implemented the wrong behavior through
some misunderstanding or there's a bug
in this code I think these have quite
low potential for conflict because
precisely because the ownership there is
low right so someone hasn't made an
active choice to introduce a bug or an
active choice to implement the wrong
behavior so they don't feel like I said
some ownership over that code that's
just a mistake and they're probably
quite grateful for you to point it out
that they've forgotten to write a test
or whatever and they're high reward
obviously you're finding defects in the
code and finally the most interesting
category these are the things that are
both high reward and have high potential
for conflict so these relate back to
that more and more firs category that we
talked about at the beginning of
benefits so code standards and
maintainability
and scalability and future proofing so
these are things like is this code
readable what-what-what design pattered
it'd you choose is it over-engineered
what what were your naming choices and I
think these these are categorized as
opinion because basically two competent
reasonable developers can have a
legitimate disagreement over these
things and because these are active
choices that people have made these are
also things that people feel quite a
high degree of ownership over and they
probably just by virtue of the fact that
they've done it a certain way they're
probably going to be quite defensive of
that way and yeah there's sort of you
can see that there's a lot of debate in
scope for this so you know even
something like duplicated logic well
there's always a trade-off maybe between
never duplicating logic and keeping code
readable or keeping logic close to where
it's used so so these are things that I
think are really reasonably up for
debate and really worth debating and
worth ironing out through code reviews
there's one more thing I want to point
out here which is something that can
elevate conflict is just volume so even
though maybe one or two of these
pedantic or one or two of these
obviously factual comments might be
well-received I think when there's a
deluge of comments it can generally
raise people's defensive noise levels so
it'd be one way of one way of reducing
that is back to the the first slide
which is keep your code very small and
manageable and I have a few more
suggestions for minimizing basically
unnecessary conflict because you don't
want to waste your conflict points on
things like typos when really the
interesting conflict that can conflicts
got a loaded word but let's say the
interesting disagreements can happen up
in this quadrant so kind of we want to
save all our conflict resolution energy
for the things like readability and
design choice and engineering choice
okay so minimizing unnecessary conflict
have clear code conventions so if you
come up together with a set of coherent
code conventions that you want to adhere
to as an organization and you can
implement those right off the bat you're
going to minimize the number of
disagreements that arise at the code
review level or to make things that can
be automated so not everything can be
automated away I think some of those
more
questions need human input but things
like whitespace absolutely can so I
think earlier prettify was mentioned and
there's a pretty always mentioned and
there's many linting tools and automated
formatting tools that we can employ that
are just going to take that pedantic set
of comments sort of out of the picture
basically unit test can obviously catch
bugs and Latin and wrong functionality
so you know a strong tradition of always
using unit tests can catch a lot of
defects continuously integrates in your
code and a build server that maybe run
some of these things as automatic build
steps again meat just means that the
code that actually reaches code review
is just going to have less of these
minor defects or minor issues that that
a reviewer might pick up on and all the
reviews first so this is I'm surprised
by it by the fact that some people don't
actually do this so if you review your
own code the surprising thing is you can
actually be quite a good review of your
own code I can't sort of count how many
times I've created a poor request gotten
through to annotate it found like a
rogue console.log or or found that I've
checked in an erroneous file that's not
even meant to be there or yeah I've
reverted so I'm setting that needs needs
switching back or whatever so the
process of writing some comments on your
own code actually going through and
annotating it and pointing the reviewer
in the direction of the bits that you
think are new that you think are
interesting that you think merit debate
can be a really effective process at
weeding out some of those defects that
are just going to be annoying just going
to add to your annoyance level when you
get this deluge of comments back so
thinking back to the so hopefully we've
minimized the amount of conflict that
that's going to arise in this pull
request but there's going to be things
we're going to disagree about how do we
manage these disagreements in a
productive way so one thing that I found
really helpful is thinking about the
different styles people actually have
for resolving conflicts and this is this
is these are sort of five archetypes and
traditional conflict resolution theory
obviously it's not to say that everyone
fits into these exactly or at different
so that these aren't malleable or at
different times or different contexts
people don't move between these but
broadly speaking we have avoidant of
conflict altogether so I definitely know
people who either don't like to submit
their code for review or when you ask
them for a review they they just okay it
straight off the bat and they don't
provide any comments I think one
possible explanation of that could be
that they don't like conflict and
they're avoiding it
yielding on the other hand you might
yeah you might have someone who
basically never pushes back and just
implements anything off the bat that
suggested by someone else and that's
also not super helpful if you want a
collaborative process competing I'm I've
also witnessed this and I'm sure many
people have witnessed this where people
see the code review is actually a
battleground where they have to defend
their initial instincts or ideas and
it's a bit of a it's a bit of a
playground fit for people's egos and I
think that can be a big problem
compromising is is kind of like a sort
of middle ground that people go down and
then the optimal solution that we want
is a collaborative attitude so we want
people to participate in active
discussion we don't want them to shy
away from it but we also want them to
understand when they might be wrong see
things from another person's perspective
and really be interested in the end
result as opposed to any kind of sort of
engine individualistic ideas so there's
I mean as well as making your code
reviews more effective of course and
making your the outcome of those
questions about code standards more
productive there's of course another
reason that you might be interested in
these conflict resolution ideas which is
just that you want your workplace or
your team or your open-source team or
whatever context it is your community to
be like a nice place for people right
like for people to be generally happy
and it for it to be a welcoming
environment so of course there's there's
you know putting efficacy in the bottom
line aside there's also just this just a
good human incentive I think to try and
think about these things how do we move
people towards a more collaborative
style basically there are two main
strategies so one is that we want to
lower defensiveness another way you
could think of this as lower proprietary
nurse a little bit so I talked about how
people feel a lot of ownership and that
can lead them to be to value things a
little in a little bit of a distorted
way so lowering proprietary nurse a bit
lowering defensiveness on the one hand
and raising ego if someone is always
yielding or is very avoidant of conflict
so giving people a little more
confidence give
people a little more assertiveness what
are the things that we could do to get
there so on an organizational level pair
programming is a great strategy
does anyone here pair program cool so
that's one thing that can be used you
use the foster a sense of collective
ownership basically at the end of the
day it's not just one person who's
written that code so that the authorship
is shared and the ownership is very
explicitly shared this it seems really
basic but discussing things prior to
implementation so if you are going to
embark on something that's going to be a
novel way of doing things or maybe
you're introducing yet some logic that's
never been seen before in your code base
that you might be implementing new
coding standards just discuss that
collectively before you implement it so
that these decisions are taken forward
as a team
never silo code bases so don't have one
developer working one projects another
developer working on other projects mix
and match keep your developers moving
between the projects work in small small
units yeah base basically again this is
a way of reducing a sort of unhealthy
level of ownership over a particular
codebase this is kind of abstract but
the more you feel like a team and the
more you've built those team
relationships the more receptive you're
going to be to your teammates feedback
basically and the less defensive you're
going to be and finally back to that
idea of being evaluated make sure that
you do decouple any individual
performance metrics from the code review
process so you don't ever want your
developers to think that they are their
performance within the team and as a
developer is being assessed on the
number of comments they get back on
their code because then you'll end up
just with a dysfunctional process that
people don't want to participate in okay
what can you do as a reviewer one really
basic thing say thank you so the person
who submitted the code it really goes a
long way for people to feel that their
efforts were appreciated even if you
have a lot of comments about the code
itself their effort has not going to
notice and you're appreciative of the
effort another principle that I think is
I think is right is is to aim to raise
code only by a grade or two don't
overreach so if you do have a junior
developer or someone whose codes is a
little bit below below what you were
what you are
when it gets the code review try and
nagy that grade up
so if it's a c-grade code try and bring
that up to a B or maybe a B+ if you're
getting B code try and raise that up to
an A if you try and drag someone from a
C to an A+ or deeds to an A+ in a code
review that's going to be a really
painful process for everyone so training
although code reviews are a great
platform for learning training is
something that should happen aside from
code reviews and the code review process
has to be kind and it has to be
something that people want to engage in
and that's not going to be the case if
if they feel that they've basically
scored in e ask questions so this is a
simple language thing I think I missed
one thing okay so the first thing I
thought you the first thing I wanted to
say which has disappeared from this
slide is changing you to Wii
so I'm already doing that here so you
know you could say you could reuse this
function or you should reuse this
function even worse and in much much
nicer way of conveying the same messages
we could reuse this function even if the
person even even if you'll talk about
something that that person the author is
going to implement themselves just say
we it's really simple
every place you say you say we phrase
things as a question could we reuse this
function
I mean firstly it sounds friendlier and
secondly you might actually be wrong you
know maybe you've overlooked something
that author has been looking at that
logic a lot more intently perhaps
there's some subtlety in those functions
that you haven't realized and it can
actually be reused or they have a good
reason for not reusing it so phrase
things as a question justify requests so
again this is making a request rather
than a demand so instead of saying
rename this function to get reports with
formatted date something like could we
rename this to get reports of format a
date to make it clear that the date will
already be formatted so I'm justifying
it I'm phrasing it as a question I'm
inviting the author to respond I'm
inviting the author's opinion
effectively so these are some strategies
that we can do use just through our
language as a reviewer there you go the
other thing you can do as a review it is
b-positive so give some positive
feedback even if it's just a one-line
comment which is which is thanks for
writing this or this works
really well this works perfectly this is
just what we wanted or whatever but but
I usually find there's there's maybe
like a yes something's something
positive to comment on on the code
basically even if it's like this is
really easy to read well that's that's a
plus that's great comment on that you
know readability is one of the things
we're looking for don't take it for
granted
say thanks say it say that it was good
it seen it sounds really simple but at
the end of the day we're all quite basic
creatures and we all have these egos and
telling people that they've done a good
job when they've done a good job really
goes a surprisingly long way okay again
my slide is messed up I'm obviously not
very good okay so as an author what can
we do as an author because it's all very
well giving all this advice to reviewers
but at the end of the day sometimes you
will receive reviews from people who are
difficult or phrasings an abrasive way
or say things that are hurtful or just
for whatever reason it rubs you the
wrong way so some strategies that you as
the author as the recipient of the code
review can employ the as if technique
which is a little challenging but very
effective you just imagined that you was
something else so you could imagine that
you were in a different state of mind
like how would I respond to this if I
were actually grateful for this feedback
instead of wanting to flip the table how
would I respond to this if I were Mary
she's always so calm and magnanimous
when she receives critical feedback I
really what would Mary say how would I
respond if this code wasn't mine if I
was reading these comments about
something a third party had written how
would I assess the validity of the
suggestions that are being made so it it
sounds kind of simple but it's it's such
a very effective technique say thank you
to the reviewer so it goes both ways the
reviewer has spent hopefully as we saw
at least an hour reviewing your 400
lines of code so say thank you to them
for your for their time that can be
disarming to them it can make them feel
appreciated it can make them engage in a
more good faith with you and it can also
be a way of practicing this first one
right so the second you say thank you
even if you weren't feeling grateful
maybe you feel a little more grateful
annotate your review first again this
mentioned this before review your own
review basically first this can be a way
of making sure that the reviewer
understands your intentions in gay
gage them get in there with the
collaborative engagement first
effectively or if someone is not really
this is also a way of if someone is
reluctant to participate in the process
someone has that more avoidant style you
can sort of solicit their feedback and
show them that you really value and you
you want their collaboration okay so
summary optimized review reviewer
effectiveness with the few simple tricks
like checklists and like small
self-contained pull requests minimize
unnecessary conflict automate away
things like white space white space
arguments understand feelings of
ownership and don't really expect people
not to feel feelings of ownership that's
just human that's just natural and
furthermore it's probably not desirable
to get rid of feelings of ownership
people who value something highly also
take pride in it also put a lot of
effort into it also care a lot about
about the about the merit of the outcome
and nudge people towards collaboration
whether you're the organization at an
organizational level whether you're a
reviewer or even if you're an author you
can try and nudge your reviewers towards
collaborating with you so that was
really dense thanks for sitting through
it and hopefully you can all take away
some of those tips to implement in your
own code review practice</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>