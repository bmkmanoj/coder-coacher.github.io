<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction to Natural Language Processing with Python - Asyncjs | Coder Coacher - Coaching Coders</title><meta content="Introduction to Natural Language Processing with Python - Asyncjs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction to Natural Language Processing with Python - Asyncjs</b></h2><h5 class="post__date">2016-05-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IMKweOTFjXw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'm going to be talking about
natural language processing specifically
with Python just a bit of an
introduction so yeah this is what will
be an overview of what we'll be talking
about today so I'm going to give a
little introduction to who I am in case
you don't know me then a bit of an
introduction to what natural language
processing is and what's going on in
case you don't know and then to why I
think that you should be using Python
for NLP some people might disagree
that's fine
and then an introduction to like a crash
course on what the syntax is in Python
just like some things that are a bit
different you might not know about sorry
if you do and then I'll be looking at
sort of like preparing your data for
building prototypes and how to get low
data in Python then a little look at how
to explore and analyze data just like
thin things like tokenizing and then
I'll be looking a little sent a couple
of little sentiment based projects that
you could hopefully play around with
yourself and then looking at some more
advanced things perhaps so yeah like I
said I'm Jessica I work at Brown watch
that down said that just start on the
data science team I've been there for up
two years now and that's my twitter
handle and so yeah natural language
processing is a really really broad
topic I'll be trying to cover some basic
techniques today so it covers like some
topics like machine translation
summarizing blocks of text like
something that got big like some lis
which is a terrible name spam detection
sentiment analysis or a couple more like
really big fields yeah so I think
pythons great which is the main language
I use for programming now it's really
readable and so it makes to like make
really fast prototypes and it's got like
really rich support for text analysis
strings and less
there's loads of like great available
NLP libraries like NLT k-space e-text
blob and there's also some really great
pausing libraries and I've also just
added a couple of tools I like using if
you want to have a looking at sparetime
so now I'm going to do a little bit of a
crash course in case you're not familiar
with Python sorry if you are so the
first thing to know is that python has
no brackets for separating your lines of
text pretty sure they're doing
JavaScript sorry oh my JavaScript is
awful and it's really yeah dependent on
white space for indenting and separating
new lines there are generally no
semicolons so whilst this block of sex
would still run it's not very pythonic
and you should avoid it because
semicolons are actually used to separate
multiple statements like that are used
on the same line so you might use it
like that or if you're importing
multiple lines but even then it's sort
of avoided strings over in like that and
you can format strings like that with
curly braces and the format function
pretty similar to other languages I
think and then just a couple on data
structures so less so the equivalent in
javascript is arrays you define them
with square brackets and you iterate
through them as follows but a nice thing
to note is that strings and python work
a bit like lists you can just iterate
through them and slice them like that
which is the thing you'll probably see
me using quite a lot you've also got
list comprehensions which is just allows
you to perform some like if conditionals
on some data and return it into a list
so this here is just like getting all
the ones which are even numbers in the
range of 0 to 10 and that's just I
thought I'd show the equivalent of an
actual for loop a lot more verbose
dictionaries really similar to JSON
blobs
it's just key value storage
so yeah they look really similar to
Jason and you can just when you actually
read in Jason in Python it just often
comes back as dictionaries anyway he
access the values for keys like this and
it's right you in them as follows
it's also worth noting that this is
called unpacking variables I don't think
you get it you can do that now okay well
I don't know about anything else she can
I don't know about new JavaScript and
you can also do dictionary
comprehensions in Python which pretty
cool and you can also do set
comprehensions so just here you're just
iterating through the by using the
dictionary we defined here and selecting
all the ones where the first letter of
the key begins with J pretty nice and
then the last but not least data
structure sets which are just like
really similar to lists but they're
unordered and they've got no duplicates
and on the last note the comparing
values and comparing objects in python
is as follows so you compare values with
double equals and objects with the key
word is and in case you didn't know the
null keyword is just none in python and
i also include a couple of links to
coding style in case any of you are
interested in making your code super
pythonic and really annoying with peph
eight
so yeah just a little intro to getting
started with NLP and python in case you
need to that's how you open and read
text files from a local file and then
like this for the online files in case
you need to read an online text files to
process and then i'm going to do a
little introduction to NLT k which is a
really popular NLP library and Python
it's quite old and it's not often
updated now but it's really great for
educational purposes which is why I'm
introducing it here it's got like loads
of
it's got its got free book included it's
got like loads of open datasets that
free you can just use so it's great so
the first thing I want to go over is
tokenizing so tokenizing is where you
just split your document up into like
logical chunks which is usually group
like broken up by sentences so if I
wanted to tokenize the first line of
from isin alice in wonderland' we'd end
up looking like this if i use the
default and ltk tokenizer and it just
breaks it up on it looks like
punctuation and spaces the next thing is
stammers and limit eyes errs they
basically just reduce words - they're
like normalized form so like Ann would
become be and cars would become car
that's how you use a stemmer and this is
how you the lemma tiser in air okay so
they look like they do pretty much the
same thing but stammers are more naive
and they don't like they don't analyze
the text like a lemma ties it us but
they're a lot faster so if you just want
to chunk your text and just have it in a
comparable format then you're better off
using lemma Tice's if you just want to
cluster the text the similar text in
some way but like so you'll notice
things like the e if Alice has just been
chopped off but that's not plural just
because it's got an e on the end same
with like Louis but Oh Carol as well as
really crap limit izing is like just the
same as stemming so it's still like
reducing it to its like normal form this
one doesn't work so well because I
haven't added in like the part of speech
that it is shall come to later but it
just basically considers the context and
it doesn't just like do it naively and
go through and just chop off where it
sees an S
ya know just from the end and then ya
select plurals and then lemma tithers
consider the context okay so now I'm
going to look at exploring and analyzing
data so the first thing that's quite fun
that you can do without a is explore the
frequency distribution so we can try
find out which are the most importance
in our text so to do this we can just
use the freak dist package from NLT k
run it against our set of individual
tokens from item on land and extract the
top 25 most common most informative and
most common tokens from the text so
that's what it looks like it's kind of
not very informative because it's kept
Connor's and punctuation and stop words
so it's just full of rubbish really but
they've been included because they are
like evenly distributed throughout the
text so it makes logical sense but it's
not very useful for us so we can insert
a look at the opposite which are the
yeah the ones that aren't frequently
occurring at all but again I've never
heard of Brandi and Alice in Wonderland
so it doesn't really tell me about about
the text so instead we could look at the
still in a frequency distribution but
maybe looking for longer words which is
definitely more useful so like Griffin
and creatures and mushroom what's that
one yeah so they're more informative
words but perhaps not quite what we want
and the next part on Tourette's part of
speech tagging which is also known as
pulse tagging so it's where you extract
given a sentence or something like that
what whether they're like each token is
a verb or an
adjectives or just bit punctuation so
this is how we tokenize using nrt k and
it returned and this is using so loads
of different libraries have their own
like versions of how they represent the
tags which is really annoying so this
one uses the porter stemmer tag set so
like i don't know them all off the top
of my head but nnp is like a proper noun
and then vb is obviously a verb and then
we've got like prepositions nouns
conjunctions using a frequency
distribution and from learning how we
can post our sentences we can consider
the frequency distribution of the types
of tags throughout Alice and wonderland
so again not very interesting the most
common is nouns injunctions determiners
prepositions it's kind of what you'd
expect but then from that we could look
at try find more interesting words again
it was difficult earlier so we could
this is looking more informative already
so we can this go through the most
common ones we're looking at before and
extracting the proper nouns so offers
the Alice queen but it's included a
bunch of punctuation you can't really
know why that's happened without like
going through and analyzing the
individual sentences but the NL CK pause
tiger is not amazing by any means and
this is on like properly written text so
it would just fall apart on tweets so
yeah now I want to have a little look at
sentiment projects sort of working on
the building blocks of what we have a
look at before tokenization and so one
of the common approaches to sentiment
analysis while it's not super clever is
rule-based which is exactly what it
sounds like
matching finding rules in our text and
to find out the polarity of the text and
so I've just stolen a bunch of these
from
is because we're monster but I can
analyze so building from what we have
for is I'm going to take the tokenizer
and split one of our reviews the its
Captain America into tokens and then
pause tag them and turn those so that's
we can see here and then I've just built
a list of handcrafted rules and that's
definitely not the way you do it in real
life but it's good enough for this so
the first way we can do it is go through
and look at the repeat so the first
review very entertaining and a for
tighter production of Marvel's recent
output so go through and if I find one
of the words that's been in our list of
words increment the count the score by
one or if I find it in the negative
reviews in the negative rule story I
want to decrement it by one so a really
simple approach but and it just outputs
zero because well there was entertain
but it didn't find that because we
didn't lemma ties it so next up we can
add Lammert ization but there's an awful
lot to go through so I've added it to
just look at adjectives and do the same
again and it's found entertaining
because we had to entertain in the rule
set so we can just build upon it like
this and then you can improve it further
maybe by looking at words that increment
the meaning of things like if it's
really great or very great or two
brilliant
might be even better I don't know tweets
are terrible these days so and then it
from here we can see that very is it's
very entertaining so it increases the
score even more it's just like this
isn't a great approach but it's just an
example of like roughly how rule-based
approach could work but then you could
take it even further and just build upon
it so you could in a similar way we had
words increment
seeing it like very we could add
modifiers for words like not or we could
add things that decrement it like oh
it's a little bit good what for a
rule-based approach like I you get
people to like markup themselves I guess
so
get people to mark off a bunch of them
and then you can go well if it if it
agrees with them I'm probably right and
but which brings me sort of onto a knife
based sentiment analysis which is a bit
more sophisticated I suppose which is a
my words of film yeah it's a I will just
move on words are not coming okay so
this is how you can build a super simple
knife based classifier with NLT K and
NLT k9 phase works by using training
data so I've gone and find a found bunch
of tweets which are already marked up so
it will be loads of tweets that someone
has hand annotated for a very long time
and said this one's positive this one's
negative and it's very exhausting and so
then I can go through and I can split my
data into training data and testing data
so training data is what the classifier
will use and testing data is so I can
just see later if it's worked as much
like as well as I hoped and so oh yeah
that's just the polarity is just like
the positive or negative for some reason
the person sees four instead of one
oh it's so strange and I understand that
yeah I they just he's four and zero I
don't know why yeah it seems so obvious
and so yeah we split our data and I've
just processed it so that it's in a
format that makes easier for each tweet
if it's supposed to tweet I've put it in
a tuple with the tweet and the sentiment
in a list called either positive or
negative so this is how it looks now
this is a sample of like the negative
and positive it's just the tweet and the
sentiment um and now just ridiculous Oh
how's it ever going to learn now
breaking each down to a bag of words and
just make sure it's kept with the
sentiment for now but removing those
that are like really small words because
they're not going to be informative to
us this gives us an actual bag of words
for all the tweets so before we had just
a group for we still knew whether they
were positive and negative so we keep
track of them from before but now we've
just got like an honor an ambiguous bag
of words for all of the tweets which
lets us build this so now building a
frequency distribution like we did
before so we can find out the most
informative features from from this from
this group of words and we go through
and extract these features so that when
we're given a doc so sorry so that when
we're given a document we can find if
any of the features match up with sorry
so we pass it in a document like all
rock stars back home while some of us
freshen up others watch em magic Lakers
game then we'll settle
reader suite and rock in Florida okay so
we split that off and then we can go
through and find out if any of our any
anything in this tweet is matched with
things from our training data so we can
ultimately end up finding out whether
it's a ultimately classifier using the
classifier and then from that training
set we just built we can build a classic
natural classifier and this is an output
of what it thinks is the most
informative features so the features
which bear the most weight too for what
we the classifier we just built so if
something contains cancer it's probably
like it's a twelve to one possibility
that it's negative whereas if it
contains love is it ten to one
possibility that it's positive because
that's what it learnt from the data and
rightly so and but so now we've got this
we can classify some tweets we put aside
at the beginning so we can extract one
of the positive tweets and classify it
with our new classifier and it
classifies the positive one is positive
and the negative one is negative
yes
so that's that's been classified as
positive and it's was marked up by a
person as positive I haven't done like a
thorough investigation on all 100 tweets
I put aside I probably should but I
think they only put about one hundred
tweets aside and that's probably not
enough to so naive Bayes is entirely
depending on how much data for it which
also makes it quite hard to see how well
it's yeah you sorry yeah the more data
you give to a nice very classified the
better it will perform but obviously
then you've got the alternative of
having to market RAZR data and if
something doesn't classify correctly as
we'll see later it's kind of hard to see
where it's gone wrong and you just have
to throw more data at it and it if it
comes across a feature it hasn't seen
before it's not it's not going to be our
classifier or it's going to cut
quiet incorrectly so now I thought I'd
just do a little demo on like so we can
look at maybe the sentiment that
co-occurs with with along with Smiley's
using the classifier we've already built
you know loading in tweets this is a lot
of code right I'm not going to go
through and explain all of this because
it'd be really boring but basically
these are the Unicode ranges for emojis
and they're in like two different ranges
which is why they've had to be compiled
separately so it just finds any of those
ranges in a tweet I'm going through all
of this and then go through and
classifies a tweet and if a tweet exists
it finds the emoji in our dictionary and
increments one to it or increments one
to it if it's a negative tweet and then
that's the result of all our emojis
below but it will be a bit more useful
in a graph so I don't think this is
going to be super informative but we can
have a look anyway well it's not bad
so the crying emoji appears far more
often with negative tweets then then
half then pills with positive tweets the
happy emoji appears more or less an
equal number of times of both which
makes me think that it's not a very good
classifier and the crying the joyful
tweet that moji is appearing with
negative farm on the positive so I don't
think this class before I hadn't
anywhere near enough data or maybe
people around really weird tweets
ah no because I don't think well that I
don't think there were any emojis in the
data set I had this is just sorry this
is just separate data that I found which
I just gathered myself to try maximize
the amount of emojis that I could get
back but there weren't many and then
tested it against the so I've not used
the training data I've not been nice to
it it's just completely new data yes
okay yeah
so there's yeah there's still a lot
going on beyond NLT Kay like it's quite
a limited library really and I hope that
some of these demos have given you an
idea of what you can go away and do if
you're actually interested in going like
far beyond and RTK there's a lot of
interesting and better faster libraries
about at the moment like there's a
Python I recalled Spacey which is really
cool which has like built-in named
entity recognition and tokenization
which is far superior to NRT KS and you
might have heard of the Google's
dependency files that they came out
recently which is open source I think so
there's a lot more things you can look
at that a lot more relevant but I hope
this is that sort of giving you an idea
of getting started and it's not too hard
to really party Mac parts of face hot no
party m'q party face
and it's not get help okay you can this
is just a Jupiter notebook so you can
just run it yourself and it should be
easy yes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>