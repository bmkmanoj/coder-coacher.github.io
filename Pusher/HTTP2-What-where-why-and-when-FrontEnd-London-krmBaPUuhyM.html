<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>HTTP/2 What, where, why and when?! - Front-End London | Coder Coacher - Coaching Coders</title><meta content="HTTP/2 What, where, why and when?! - Front-End London - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>HTTP/2 What, where, why and when?! - Front-End London</b></h2><h5 class="post__date">2016-09-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/krmBaPUuhyM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">having it's already my name's Patrick
come in you can catch me there on
Twitter at Patrick Hammond I'm always up
for a rant about performance or chat and
come and grab me in the power BAFTA
words if you not on Twitter anything
please please talk it's good to talk you
haven't guessed it already I live and
work here in London at the Financial
Times we are the world's leading news
organization for Business and Economic
Affairs we're amongst other things I'm
helping to rebuild the next generation
of ft.com we're redefining how the FT
delivers news creating an immersive and
personalized news experience for our
users but one of the core remix when
we've been building this product is that
we know we wanted to make it extremely
fast we've done a lot of research and
know that could speed correlate directly
with engagement of our users and
therefore people coming back to our site
reading Newton more news subscribing
more giving us more money and paying my
wages but I'm not actually here today to
talk about the product development of
that I'd love to be able to talk to that
but I'm here to talk to you about how
we're making it as fast as possible and
more specifically one of the underlying
technologies that we're using to deliver
the news as fast as possible so
hopefully this may look slightly
familiar to many of you in the room
especially if you're a developer since
their inception web pages have been
delivered over the same way over the
network over the same way for tens and
tens of years we have tcp/ip at the
bottom HTTP at the top when we use the
transfer protocol to deliver our HTML
CSS and JavaScript that form the web
pages that you build on a daily basis
and your users interact with and as
application design ISM developers we've
rarely had to understand even how the
bottom half of this stack fits together
and how it communicates with the top
layer and by analyze that's remained the
same for about 20 years now but this is
beginning to change and it's quite an
exciting time because of that and that's
again what I'm here to talk to you today
for the first time in nearly over 20
years we now have a new version of the
underlying transfer protocol of the
Internet the protocol that all our
websites use to deliver their assets and
the simplicity I think of the HTTP
protocol hasn't has the reason why it's
been so long right it was actually from
its initial design from tins berners-lee
in his colleagues at CERN such a well
designed specification that it didn't
really need to change that smile it
actually lasted us so well we now have
fridges watches even cars that
communicate with each other and the
servers around the world using this
transfer protocol you might find that a
bit scary that cars are talking it to
each other over HP but I think it's an
amazing testament to to the protocol and
why it's actually survived so long but
as things get older you know most things
start to sign show some signs of stress
and to meet these new challenges the web
of 1991 is very different to the web of
2016 and so to meet these new challenges
it's starting to show some signs of
stress and so that's why in 2012 the
HTTP biz which is a working group part
of the IETF announced the new initiative
to create HTTP 2 it's taken until now -
last year for that working draft to
climb out and be finalized and that's
why exciting literally last year we have
a new point - version of the underlying
transfer protocol of the internet and
this is very much of the back of the
work that Mike bleachy and colleagues at
Google did in the Speedy draft might
talk about a little bit about that later
ever time so that's what I'm going to
cover today is the why the what the
where in the web why do you need to know
about HTTP how does it work underneath
and what why you should know about that
to help you with your daily basis but
most of you are probably asking and I
definitely ask myself this as well and
why do we did we need a new version
right I'm still building websites every
single day delivering them they're
working our users loving them coming
back why do we need this and it's quite
common and quite right for you to ask
that but I have to let you into a little
secret that we're all trapped in the
lolled sense of funk security that our
websites are in fact using the network
extremely inefficiently and why is this
hopefully the next sections going to
explain that I mentioned earlier that
the web of 95 this is ft.com Tech in 95
is very different to the web of today
our users expecting a much more
immersive experiences hundreds of
resources different mediums images video
interactions but all of this is coming
at a cost when we deliver it down the
network to our users as you can see it's
very very different that is literally
just text on a white they didn't have
CSS then this is all just tables and it
was probably one single file probably
two at most and it's completely
different to what our users are
expecting we have to hire hundreds of
people to be able to deliver this now
and live up with the rest of the
competition that we have in the world
and so our average web pages are now
making over 80 requests per page and
that is just that's not it that's the
95th percentile is more like 300 plus we
are at the peak of a website obesity
crisis and the trend as you can see here
is not stopping anytime soon let me just
let that settle in 80 requests HP 1 and
1.1 simply was not designed to cater for
this sheer amount of requests going down
the pipe like this is probably just 1 or
5 the first website that Tim berners-lee
ever made was a single document and it
was linking to another one that was it
just one request so whilst we've seen a
great increase in the available
bandwidth that people have over the last
few years a lot of us here we're very
privileged to live in London we have
things like Virgin Media that have cable
going into our houses that have now
something ridiculous like 200 megabits a
second pipe going into your own living
room yet we haven't seen the same
benefits and improvements in latency now
what is latency so latency is
essentially can be boiled down to the
time it takes from a request to go from
your mobile phone
the way to the server and back again the
round-trip time and mobile networks by
their pure nature and very highly latent
things you are a rat if Mobile's in the
name right you are mobile you're walking
around it's going to take a long time
and when's Dougal were building we're
experimenting with speedy Mike bleachy
and it's a great article down here that
slides will be online later they found
that whilst you increased at the speed
of bandwidth it plateaued there was a
threshold where that had no no longer
had any more impact on your page load
time whereas there was for every 20
millisecond improvement in latency there
was a near linear improvement with the
page load time of your website and there
are many good reasons for this you know
as we've seen it's 80 requests the
average page is composed of many small
resources which require many connections
many TCP connections each with their own
overheads and the performance of each
one this is very very closely tied to
your round-trip time hb1 the data was
defined as an ASCII character stream of
text so you can actually see it it's
just it's English this the text that you
can go by its ASCII so because of this
we must send and receive the requests in
exactly the same order that we sent them
so we have to when we send a request for
image 1 we have to wait for the server
to respond of that data so we can
allocate the data to that request in the
browser until we can send the next one
on a single TCP connection and this
phenomenon is known as head of line
blocking this could be analogy like this
is you could be in a bank you have two
people in front of you waiting for the
cashier you are blocked by them and
however long it may take to process them
at the cashier until you can go and we
have exactly the same problem with TCP
connections because the data is ASCII we
can't
interleave it because the things might
get mixed up the cashier might get
confused so we have to wait for that
response before we can send the next
response now this is a fundamental flaw
well some would say in the design of HP
one and most importantly why we have a
ripple of effects so to get around this
issue browsers started to open more than
one TCP connection for the same host
it's like okay I can't I can't send a
request in this TCP connection so I'm
gonna have to open another one so we
started off by opening two and then that
wasn't enough so now the specification
says that we're allowed to have six open
TCP connections to the same host name to
overcome the head-of-line blocking but
this comes at a great cost to our users
right each connection incurs a full TCP
handshake if you've ever seen me talk
before I've explained in depth about
that but on your UK average 3G
connection that could be up to a
thousand milliseconds just to open the
TCP connection and the connection then
we if you're over secure network that
may add the TLS handshake as well and
then each connection competes with the
underlying network resources that
bandwidth and thus causing potential
congestion on the underlying network
link and that wasn't enough right so we
stopped at 6:00 we're like no but I need
I need to optimize this as much as
possible so we started to create hacks
and anti-patterns to overcome
head-of-line blocking so one of which is
concatenation i've heard all the cool
kids today like to use react angular
jquery bootstrap and mootools
all in the same application in fact
except just talking about my dear friend
oli is actually using an architect like
that at the moment he's no longer my
friend
and so why create t have the overhead of
creating five connections when I can
concatenate that into a single file and
therefore I only need to open one TCP
connection for this and this is great
and I still do this to a daily basis
most of you will have build processes
that can catenate in these files
together but this comes at a cost right
that's more CPU and memory overhead for
your low powered mobile device to
download and parcel that even if they
only need to actually execute two lines
of mootools
you've just incurred the user from
downloading all of those bikes and if I
change a single line even let's add a
semicolon to a line and then
invalidating all of those bikes even
though I only change one bike in that
file I'm forcing the down the user to
download images actually came first
before this and images were the main
reason why we actually started opening
more connections because in Kannada
early noughties they were the main
medium that we were throwing loads more
at and so we started to borrow ideas
from gaming developers in the 80s and we
started
sprite image together so rather than
having a 200 HTTP requests over probably
six TCP connections I can concatenate
that into one sprite into one but the
problem here is I probably only need to
use one image on my page and so I've
just forced the user to download 200
pixels maybe even 2 megabytes of data
when all I wanted to just display that
one image and again the invalidation
problems here is I changed my designer
decides that this devil actually needs
to be green we've just forced the user
to redownload all of those images not
benefiting of having a cache at all on
the user's device and then we decided
that 6 hosts wasn't even enough right
and so we started to shard our TCP
connections so here this is literally
taken from Flickr a month ago this is C
1 C 2 C 3 C 4 at static Flickr host com
this is actually all residing back to
probably the same IP address or the same
server even and this is called domain
sharding so this is tricking the browser
to thinking that these images are
actually on different hosts so it will
open up another 6 connections so Flickr
is probably actually got 24 open TCP
connections right now and this is coming
out a cost and by creating those
additional TCP connections we're
actually completely flooding the
underlying TC network and Etsy I should
have had a link to them Etsy have done
some amazing research here as to the
threshold point at which you actually
start to cause performance issues rather
than gain them by - domain sharding and
finally we started to realize that
actually let's not create a TCP
connection at all and let's inline that
resource into our document so we can
send it down in the initial one and it's
quite ironic because I've sat in this
exact place talking about why this is
such a great idea probably about 2 years
ago and has been an evangelist of this
and I'm now realizing that it's probably
a very bad I did because when you base64
encode an image for instance you're
actually increasing the size of that
image by 33% so you're actually forcing
that user to download more and again low
powered devices are going to have a very
hard time and lots of memory going to be
consumed too
convert this back to its native format
hopefully you've learnt and why latency
matters it's all about latency where
latency occurs what head-of-line
blocking is the front most fundamental
problem with h1 and the hacks and
antipatterns that we created because of
head-of-line blocking so now that we
know that how can let's have a look at
Hector 2 to see how it's overcome these
problems in what specific bits of the
design of the specification have made it
so brilliant and so to start with I
thought this is quite good this is taken
from a great books very simple so online
free open source book called h2
explained by Daniel Steinberg he is the
one of the maintainer of curl and he
works at networking team at Mozilla and
he was explained the problems or the
manifests as essentially that the HTTP
biz hat working group had the remit and
the requirements they had when they were
building the specification and that was
obviously to be less sensitive to
latency fix the pipelining and head of
line blocking problems that we just saw
eliminate the need to increase the
number of connections and they're all
those anti patterns to each host and
this is really important to keep all
existing interfaces your eye formats and
schemes I can't stress this enough when
we're designing a new version of the
spec it would have been great ok you
know those periods inside URLs like
dub-dub-dub dot ft.com I don't like them
I want to turn that into like a tilde
dub-dub-dub tilde ft we would have
broken the internet right so we can't
change the scheme the semantics the
methods that we use that most restful
api is talk to each other we couldn't
change um most importantly it had to be
made within the ITF PageSpeed working
group Google laid some amazing
foundations with the speedy research but
it was for them and it was in a
controlled environment we wanted like
most good website Ephesians to be
developed in the open and let other
people add to and ultimately make it
better so to do this they've outlined
what I think are the six core features
that make up page 2 which is
multiplexing a binary data
format prioritization header compression
flow control and server push and for
this it because this is front in London
for front-end developers I think the
most important things to understand a
multiplexing prioritization and server
resource pushing at the heart of all of
h2 our streams streams essentially a
virtual channel with inside the TTP
connection there only which carry
bi-directional messages so their streams
are virtual they don't really exist they
can contain messages which are complete
sequences of frames and messages are the
things that most closely map to a HTTP 1
request or response and then you have
frames the building blocks of it all
frames are the data payloads in their
binary and so we have frames frames make
up messages messages are transformed
within streams and a connection can have
multiple streams as I said this is the
building block which is a frame each
frame has a type such as I am a header
frame or I am a data frame or I am a
push frame all frames share an common
nine byte header field which is this so
regardless of the frame type they'll all
have this same header field and the
header fields really important because
it declares its type obviously saying
I'm a data frame its length in bytes
because remember I said it's a binary
format and the stream it belongs to
so a stream identifier and maybe it's
priority which lives inside the flags
and so the date within the frame is
actually represented as binary and and
this is really important because it
allows a frame to declare its length say
I am 20 bytes long and this is the
reason why we no longer have to open TCP
get more TCP connections because that
means that I can interleave frames on
the same connection if the cloud the
client say the browser is consuming that
connection and it doesn't care about say
stream for it looks at the length of
this and it can just skip forward that
many bytes in the buffer and not care
about them and it can interleave frames
put them in their own buffers and we
stitch them back on the other side this
is the most fundamental piece of
information about
a ch2 this is what allows it to overcome
the latency problems by interleaving
binary denta frames on the wire the only
downside to it being binary is that we
can no longer inspect HTTP requests or
responses on the wire so you getting
used to being out in your dev tools to
be able to see the text of the raw
response
unfortunately that is going but there's
some ways to get around that I'll
explain later
so here we can see how closely it maps
from an h1 request to h2 so we'll have
the header area and HTTP one that
probably will make up a couple of dead
of frames and then the payload which
will probably make up a couple of data
frames so all communication is now
performed on a single TCP connection and
this is why it's so so much improvement
over latency and here we can see two
frames interleaved with each other
different streams on the single
connections whilst this is going on
I can send head of frames requests for
stream 6 and c5 whilst I'm still getting
data streams to team 3 between 4 and
this is true bi-directional multiplexing
happening right here this animation is
also extremely long sorry
let's just wait for it to go cool so
over the years because of the
inefficiencies of h1 browsers have had
to overcome and create their own
optimizations of how they optimize the
requests that they send whilst they're
rendering a page and they do this by
resource prior a critical resource
prioritization most browsers will have
this baked into them and so as your
browser pauses the HTML document it will
find the resources and when it finds the
resources it's going to push these into
a queue essentially before it actually
sends them off on the network now most
browsers are clever enough that even if
they found image one first before they
found main dot CSS in the ordering of
the source they'll prioritize the CSS
file higher because they know they need
that to be able to paint to the screen
so actually what they're doing here is
they're creating artificial latency that
they're purposefully delaying the
request even though they sent it and I
kept on going on about how latency is so
important and they're actually on
purpose creating prioritization here now
with h2 because it doesn't matter the
order that we send them in and we - on a
single connection I can just fire off
the request as I find them which is
amazing so we've already reduced that
artificial latency that results
prioritization incurs but the eagle-eyed
in the audience will see though ok but
what if I did still send image one first
doesn't that mean that I'm going to get
image one back first as well um and so
so obviously it's faster but this is
where h2 dependency tree weighting and
prioritization comes in and this is
actually part of the specification so
now as the browser is finding those
files it will apply a weighting number
which starts off with 16 this base64 and
then it can increment that and so here
we concede that main dot CSS has got the
highest weight and inside main dot CSS
when we got it back we found icon lit
CSS and so even if this is passing like
we've only got half of it we can tell
via a dependency tree that this is
linked to this so you shouldn't send me
any frames for that until I
got all the frames for that and what's
even more clever about this is that it's
truly dynamic so if you imagine that we
have a tab for google.com open and
there's a h2 connection for that the
user opens another tab for google.com
and focuses on that tab firstly the
great thing about h2 is we can we can
share the same connection but suddenly
all the resources for that tab become
much more important than the ones that
still might be on the wire here so in
real time we can change the stream
weighting via frames and tell the server
actually now these ones have become much
more important another use case you
could imagine even on your website a
user hovers over a button they click on
the button that opens up a carousel and
the carousel is a big hero image
suddenly that hero image its priority
becomes much more important than any
other data and we can communicate to
this to the server via prioritization
this is extremely extremely important no
longer do we have to try and trick the
browser and hide resources from the pre
parser because we can actually get there
the browser to do this work for us um in
introducing HP 1.1 so 0.9 didn't have it
we allowed us to start adding metadata
to our requests and responses and via
headers and we started to have a wealth
of metadata and because HTTP is
stateless there is no state maintained
unfortunately we have to send this data
especially things like cookies on every
single request even if the servers
already seen that cookie and this is
exactly how login systems authentication
works on on most log on most websites
and so I have to always send this cookie
and so to address this in the h2
specification they invented hate pack
which is a compression algorithm
specifically designed for HTTP now there
could be a whole talk just about hates
pack and because it's really really
intelligent but I've just explained to
you basically how the basics work so the
first thing is that the client and the
server maintain a static lookup table so
just like a database table on either
side of the connection on the client in
the server and in the specification
there is detailed the most commonly
occurring key value pair rings for
headers and the assigned a
ID number and so whenever you need to
send that value you actually don't need
to send the key or the value at all you
just send the ID because the server will
know exactly that but the really
intelligent bit is for the duration of
that h2 connection for that single
connection both the client and the
server maintain a dynamic lookup table
so the first time I send this cookie
header on the on the server it will be
assigned a key of 64 the next time I
want to send that same thing I only need
to send the key of 64 along with those
because the server's already seen this
now the problem here is this is actually
maintaining state between a client and a
server and that's why things like see
the ends have had quite a lot of trouble
with implementing h2 and then to send a
new one I just send that and it will get
assigned that and then all of this data
is then Huffman encoded which is the
same algorithm that the compression
algorithm that gzip uses to reduce that
footprint even more it's incredibly
incredibly intelligent please if you're
interested in to that kind of thing go
and check out the H pack spec and
finally before moving on the final
feature I want it to discuss about h2 is
server-side resource fishing as we saw
earlier I have to send a HT HTML request
my get request to my index file wait for
the server to process that I then get
that back I start pausing the document I
find the CSS file and then I send a
request for that now that's extremely
inefficient because I've had to wait for
the server to process that even though
as application designers and developers
we know that the next thing that the
browser is going to request because of
resource privatization is main CSS and I
always know that when index dot HTML is
request the next thing they're going to
request is that so what if we could
Intel into intelligent Lee tell the
client that so with h2 I make that
request whilst I'm processing that may
be that dynamic file I can push the the
resource the data for that main that CSS
file even before the clients even got
any of the index file and before it
would normally pause and find it so
we've dramatically reduced the latency
that it might take for that normal
round-trip now the eagle-eyed and the
audience would notice that the push
promise frame so this is a new frame
type just like
the header and data frame the post
promise frame must be spent sent as part
the specification before any data frame
of the initial resource and why is this
because you might get into a race
condition that if I had sent the data
frame first the client might be so fast
that it would then find the CSS file and
then you're actually sending more of the
bytes too many bytes as possible so as
part the spec push promised frame must
be sent first before any data frames
with that file now the even more keen in
the audience would have noticed that
what if the client already had that main
dot CSS file in its cache so we now have
a new frame type called reset stream so
this is the client saying nope okay
thanks for that I don't need any of the
data frames for that stream because I
already have it in my cache so that's
even more efficient and we can use reset
stream quite a lot if say for instance
the cache headers are already still not
there not stale then you could reset
may-maybe Ajax requests during the
connections lifetime and this is
incredibly powerful contrast that so
much this is truly bi-directional
multiplexing of 82 connections in full
effect right here
so we've learnt the building blocks of
h2 which are streams and frames and
binary data late data framing resource
prioritization header compression via H
pack and server-side resource pushing
now hopefully that wasn't too much
information for you to take in and I
promise you that was the the most techy
crazy bit of it all of this talk but the
more I I've what I personally feel about
this that the more I've learnt whilst
working with h2 and reading the specs
the more I've become to be amazed by its
design and it is truly incredible
compared to h1 and they've done some
amazing work here and I've only
literally just skimmed the surface in
it's a very large specification but I do
urge you to go in and have a check and
read it a bit more but hopefully I've
given you enough there to take home and
apply to your day to day work so the
most processed thing I'm going to show
you today is the current browser support
landscape we've got a global average of
17 here in the UK we've actually got 77
which is
amazing Safari finally jumped on the
bandwagon at nine and that brought up
the stats massively and this is this
alone was enough for us at the FT to
know that this it was worthwhile
investing in our interests who here is
using has deployed h2 into production
one person great and two awesome
that's good to see but you know that's
hopefully why I'm here to try and trying
to to get you all to on the bandwagon a
much debated issue feature of h2 was its
requirement for TLS that you have to
serve be serving a website over HTTP and
probably that might be a reason why most
of you in the audience aren't using it
yet and the original speedy
specification it actually was a
requirement but they dropped that in the
H Spears working group and it no longer
is but and so it's got they've got a lot
of stick for that and I actually agree
with them one because I think we should
be making the world a more secure place
but to in the speedy experiments they
found that many of the old middle boxes
and proxies so the intent the Internet's
actually made up of lots of cables and
boxes in they get eaten by sharks under
the ground under the sea and if those
proxies didn't understand the packets
that they were routing they would drop
them so in the speedy experiments they
saw between ten and fifteen packet loss
for all connections and that's why they
enforced here less is by correctly
secure tunnel you're ensuring that there
are no middle boxes can be inspecting
the packets who hears heard of let's
encrypt awesome that's great
and so you have no reason for your site
not to be delivered over HTTPS now where
there is a free open typical 480 that
makes it so easy for issuing and
reassigning certs ilio be really good if
you get someone to come and talk about
let's encrypt because I think more
people need to know about it so once
you've got Taylor set up you start
observing your own traffic this when we
start a directory it's obviously gonna a
lot more was what our stats were like I
said we started to talk to our
stakeholders explaining the benefits of
HTTPS and h2 and what might happen to
those other users the server support is
looking really good
most of you probably serve your websites
using nginx or Apache they both support
it now
this is actually about to change I need
to update that jetty if you're using
Java when I is for Windows the support
is all there for the actual servers
sadly the CDN landscapes not looking
that great but it's getting better
Akamai supports it CloudFlare no no sign
from Aida West cloud front yet fastly
I've done it they've actually got the
best implementation at the moment and
I'm not being paid to say that even
though I am a customer there it's just
that they've chosen the best h2 server
something called h2o putting Akamai's
just about to release push but I heard
we all like to deploy our software in
the cloud these days then it white rains
a lot up there well the service
providers their internal networking
stats are very bad Google App Engine and
the only people that have native support
for it
AWS have not mentioned anything on there
a lbiza because Heroku is on a duress
we're not going to see anything there
anytime soon I don't think um so that's
great you've upgraded your server
software maybe for some of you that's as
simple as that
how can I start using this and their
approach that we first took at the FT is
- it's probably easier you probably
already serve your static assets your
images of your CSS of something else
other than your application server so
why not just put a CDN or a proxy in
front of those and you don't need to
worry about that because that's going to
probably take a lot longer to upgrade
your origin or stick a proxy in front of
your origin that can speak h2 this is I
know a lot of businesses have going for
this approach this is in fact how fastly
did it as part of their CDN work and
then finally hopefully in a couple of
years we can just everything all of our
servers will be speaking at natively so
once your h2 capable it's time to start
considering how you're going to optimize
your resources and on what you want to
push so here's your typical critical
rendering path we have to request the
HTML file we then pass it we get the CSS
we have to block wait for that then we
get our fonts we have to block waiting
for that so obviously to me the most
important things should that you should
be pushing are your critical resources
and look already the impact that has on
the timeline the amount of latency that
we can be reducing
there's been a lot of debate within the
w3c of how as developers we declare the
resources that we want to push so the
most common way of doing this now is via
the link header and using the rel
preload attribute so this is me saying
that style dot CSS I want you to push
this and Apache nginx and h2o all taken
this as their implementation of pushing
resources you can also do that by an
element but that's probably far too late
the client would have already found that
so you should do it as a header myself
and people like your voice actually who
he wrote preload think that actually we
need our own semantics because the
semantics are preload are slightly
different to what you want to push so
we're pushing for a rel push
specification mmm so now you you've got
it working you want to start seeing if
it's actually doing what it's won the
web page test which is the toolbox of
most performance engineers now has
native support for it you can see
multiplex connections within the
connection view you can also for the
Firefox agents on web page test you can
see pushed resources it took me about
three months to realize that in chrome
you have to right-click and enable with
the protocol and the network panel
literally took me three months and
that's the only way in chrome that you
can tell whether or not a connection is
so here you can see our serving our
document over h1 all the resources over
h2 Firefox
put it where I would have thought it
would have been in the network
connection in the headers and I
mentioned earlier because it's binding
which for a binary framing we can't we
can no longer expect the actual response
body and so unfortunately we're going to
have to start becoming more friends with
tools like why sharks as front-end
developers and I this is quite scary
when I say this to a lot of people I was
as well this is a really good blog post
explaining how to set up wireshark
properly and use the TLS encryption but
this is great here you can see header
frames on the wire and actually the data
of that so it's decrypted the binary and
the TLS certificate and now I can
actually see the data so the only way to
do that in dev tools at the moment is in
chrome they have in chrome net internals
you can actually inspect the
ht2 session and here the headers and it
needs a lot of work especially as more
people are going to start to having to
use net internals they it needs so much
love and if you've implemented h2 and
you want to know how much of the
specification your server or CDN is
living up to it there's a really good
test suite h2 spec on github that you
can run against your deployment and see
whether or not good and how much of the
spec you've implemented so far I'm sorry
for the whirlwind tour at the end there
I've already run over time I told Lily
I'd only be 30 minutes I'm already 35 so
hopefully in that section you've learnt
the browser support the TLS requirement
considerations when you're thinking
about choosing your h2 server what you
should be pushing how you can push it
and the torque current tooling landscape
around it so to end with I just whilst
HTTP here is here a lot of you aren't
using it yet but there's already some
problems with it there I think that
people are starting to iron out and it's
only by us trying and feeding back to
the working group into the browser
vendors will this become better and so
one of the biggest problems that you
might have noticed with push is that
there's it still can be extremely
efficient that I'm sending resources
down to the browser even though it might
actually already be in the browser's
cache so what is his name I've forgotten
it's yeah yeah Kenji who wrote h2o
server has come up with the cash digest
specification and this is going to be a
new frame type and this is a way of the
client communicating to the server what
resources it hat already has in the
cache for that host name and that's
extremely powerful not just for h2 think
about how CDNs can use this it's amazing
so it basically used an algorithm to
compress down all of those values into a
single digest and sends that up in a
single frame on the h2 connection it's
very powerful and I talked a lot about
head-of-line blocking earlier and whilst
we've eliminated head-of-line blocking
we've only done that at the at the
request layer actually all we've done is
pushed down the inefficiencies to the
TCP layer and so a lot of deployments
especially
friends of mine at fastly actually don't
like h2 because all they've done is
pushed the problem lower down the stack
and so Google thought of already
realized this many years ago and have
moved on and they have now opened the
specification for quick which is h2 on
top of a UDP connection not a TCP
connection so when you're browsing on
google.com today and many of the Google
properties you're already not using
speedy or h2 you'll be on a quick
connection and it took four years as I
said to show that the history line at
the beginning for h2 to become so I'll
probably be back in about four or five
years to talk about quick and why it's
so great um so ultimately its new
there's a lot of answer question
unanswered questions when should we
start the optimizing assets what's best
to push how is this going to play your
service workers and how does it affect
my website and so leave on that note I
just wanted to share some of the
findings that we've had at the FT and I
think by only us talking and people
sharing and people writing blog posts do
does the web evolved so we ran a split
test when we first deploy it takes to be
two and we so on mobile tablet desktop
and compared the 95th percent of the
page load event and you'll see that
actually it doesn't have that much of an
impact on desktop but it has a dramatic
impact on mobile by nearly five seconds
and I think this is amazing because it
proves that it truly was designed to
battle latency and that is the problem
that we have a mobile we don't have
latency process on desktop so then we
measured the round-trip time to the time
it takes for all of these average
connections and you can see that the the
greater the round-trip time the better
the impact h2 had on the overall load
event and h1 got slower and slower and
slower and finally this is actually
where desktop did which we've started
some experiments with push and you can
see here that we shaved a thousand
milliseconds of our start render by
pushing CSS files and I've been working
in performance a quite a long time now
and I've never had a single technique
that's been able to cut that amount of
time off so it's it's incredible go and
use it common check to me afterwards
I'm so sorry I've won over thank you
very much
oh that's more the performance basic
still matter right a slow website on h1
is still going to be a slow website on
h2 you need to think about the
performance basics optimize your first
render compress minify optimize reduce
DNS lookups you see the ends to reduce
your latency and come to london web
purse we host it at the FT I'm a host
sorry I had to plug that if you want to
find out more about performance technic
techniques come to that thank you very
much that's the end of five</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>