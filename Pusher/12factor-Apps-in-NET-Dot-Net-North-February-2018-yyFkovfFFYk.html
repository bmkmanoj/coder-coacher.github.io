<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>12-factor Apps in .NET - Dot Net North - February 2018 | Coder Coacher - Coaching Coders</title><meta content="12-factor Apps in .NET - Dot Net North - February 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>12-factor Apps in .NET - Dot Net North - February 2018</b></h2><h5 class="post__date">2018-03-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yyFkovfFFYk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so quick got a question how many of you
have heard of 12 factor apps okay was it
just how many heard of actors before you
discovered I was to go talk about 12
factor apps okay and how many of you are
using to fight rap methodology okay okay
all right at some other questions as we
go along but well that's a good starting
point this is me most of that is very
dull
this is AMOLED heater by the beard
though when I was pointing out is this
the rhetoric what I said earlier right I
I started speaking in 2003 and dotnet
because I was a C++ dev and don't net
came out and we were all very excited by
dotnet because frankly getting anything
done as C++ was hard could took a long
time to load code to write and running a
user group because that's what I thought
let's all get together and talk about
talking that this new exciting thing and
I ended up running a user group because
all the Americans had user groups and I
said where is that London don't know
user group and someone said there isn't
one but I'll turn up if you if you if
you start one so I've got this American
guys and they said okay you know what
were developed mentors they were cool
factor old training company they gave us
a venue and then we quickly discover we
didn't have enough people to speak we
couldn't find microsoft import and I
don't know some place to fly in to kind
of talk try to talk to ourselves right
and that kind of started me on this path
and you start off by doing small
15-minute like one called lightning
talks and you get to bigger Torsen well
I think one of the problems with
speaking event like this I tend to find
is that you are the content right you
know your stuff back about technology
the slide decks there to help you
navigate through the biggest thing you
need is confidence and unfortunately the
heart the East way to get confidence
it's me to speak enough times in front
of people that it doesn't intimidate you
anymore but I would really recommend
doing it I have been very fortunate to
you know go to lots of places big
different conferences travel around me a
lot of people who I would probably
otherwise have just sat in London and
never met anybody alright and so I would
really encourage you to speak if you can
okay that's I work for
what kind of a chef went alternative we
were sent to work between organizations
the main point of showing the side is
not to try and sell you huddle justice I
don't work with some kind of consultancy
I am not selling you and he you know my
particular anything at all I'm just
trying to give you the benefit of my
experience and how we work all right so
what talk about first of all we'll talk
about the context which sort of well why
12 fighter apps should become more
important as an idea those of you that
said you know I've seen I've heard of it
you've noticed so in the last 12 months
or so there's been more of a kind of
like all this just well fighter apps
thing right quite often mentioned in the
same breath as cloud native who's had a
cloud native yeah okay well to
acknowledge it as well okay where did it
come from right its origins what are its
goals what why would you care about 12
factor apps as an idea right why I
listen to the whole of the talk when I
go home after pizza
it's what face what are they said what I
used to this - I've got sent people in
and I would do all the 12 factors one by
one right as I started and it turns out
when you're giving it like a conference
and you've got like an an hour and that
gives you five minutes per fact it
doesn't really work very well so now
what tend to do is really divided up
into three big groups design build and
release and manages a pit trap there's
only build and release and manage I talk
about them holistically within those
three buckets and then at the end what
kind of review exactly what the factors
are I will show some code throughout all
this part of the talk right so actually
show you some apps built in that style
that's that's relevant here in a
conference so that if you are at the
back and you have trouble reading just
or hearing me just shout from the back
and say too soft or I can't see too
small because what happens to cases I
get feedback forms and the person that
backs I couldn't see or I couldn't hear
in and it's kind of like well say
something okay so at the back I can fix
any problem all right
well I do offer to hear that you can go
and get haven't get most of my
presentations don't sink the unless you
got a lot of bandwidth don't sink the
presentation the stretches grab the ones
you want directly and the codes
available there as well right so create
the back of the room right now and you
thinking there's no way I'm gonna be
able to see Ian slides or other code you
can always grab it and look at the key
laptop me right
they don't take photograph you done
ready good okay cool
why do you care so when we talk about
cloud native you quite often see some
cloud and some diagrams about kind of
pivotal pivotal has one for example talk
about Clara friendly cloud ready cetera
and cloud native is kind of the epitome
right and one of the pillars of cloud
native is discards 12 factor apps
they're saying effectively if you want
to be cloud native that's the style of
application you should be building so
what is cloud native
I think the first question really to ask
is what you mean by cloud who's running
their kit on the cloud right now who's
not running the key in the cloud that
may be a better questions remember you
and you're not running because you data
center is what is it you know you're
such scale things are more efficient or
just not got around to that kind of we
had such a sunk investment in kit that
you can't switch so elastic on demand
and self-service so they're three the
scene is the three characteristics that
are key to cloud success the first is
this elastic in other words I can eat as
much as I need or I can slim down as
much as I want so I can respond by
success success of my application or
business by provisioning additional
capacity or I can effectively slim down
when I was no one actually there and
saved money okay it's on demand in the
sense that if I have a traditional model
in a dead center so so some of our kids
actually still on datacenter and what
happened is there if we want to spare my
capacity is we basically phone up and
say we need another server and they say
good we'll buy you one
it'll be in the rack in about six weeks
time raise your ticket so you get the
piercing that fits in there in the rack
then they you say aren't you service
here that I still gonna basically
install it all and we're up and this
another two weeks so it's not really
kind of on-demand right so then that
gets you into what a capacity planning
effectively you as a software architect
or operations guy I had to sit there
going well given there's an eight week
lead time I obviously need to predict
ahead of the point where I'm gonna need
that piece of tin that I need it right
and that's why I teach on the cloud
because essentially I just say I need a
new piece of tin and someone who's spun
up for me and you know it's relatively
short time window depending what
platform you're on or what operating
system you choose that to actually
happen okay
and it's obviously self-service right I
just press the button to make it go I
don't have to make phone calls and put
in tickets I have to send someone down
into the bowels of the air-conditioned
basement to go and do the work
themselves and their other positive
issues for businesses around things like
capex versus all pay except it's not
really relevant to our conversation but
the key things you want to talk about is
the fact that we would we could see the
cloud as being an elastic condom on our
self-service okay native we mean first
it's been designed to run a such an
environment right we are saying we are
going to run an environment so lastik
what does that mean what does it mean to
be the fact that sense you want to spin
up new instances right it's on demand
want to do that basically by saying now
I want a new instance or application now
and self-service
I want basically a new instance the
application or a press a button to make
it happen okay and exploits that
environment so it's an application that
says given that I'm running an
environment that has this ability to be
elastic on demand and self-service how
can i exploit that how can I make
basically that work to my advantage
okay so pivotal has a maturity model
everyone s to have maturity model right
for all basically things that we look at
but it's not a bad one it's one pivotal
kind of one of the people that really
push an active my only problem with a
little bit is that cloud native says the
micro services and API first design are
what you get when you are cloud native I
think that particularly micro services
is a useful tool to help you get
especially to become cloudy
it's a different story but cloud roadie
is essentially talking about saying
we're recognizing that I'm running on a
virtual machine maybe or a container I
don't necessarily have permanent disk
access and the platform was managing my
port to the next work networking I to do
this just to lift and shift cloud
friendly says essentially right now I'm
going to try and leverage the platform
so I'm gonna put a 12 factor app I'm
gonna try scaling horizontally and I'm
gonna use the platform for availability
that really means looking at things that
are placing so rabbitmq with sqs and SMS
on they ws or is your service bus right
saying why am i doing all that hard work
myself when you'll do it for me and then
effectively get into cloud resilient
when we refined to say well there's
weird stuff happens now I'm in the cloud
like I get partition failures when I was
on my rack it was well well and good
right effectively because might they
have had multiple machines but I never
had network partitions between them
suddenly now I'm actually on the cloud
all that kept theorem stuff that I kind
of thought it's never really happens
suddenly seems to be happening a lot to
me right so this is what about
resilience it's saying I to actually
understand had to deal with those kind
of distributed system issues because I'm
running in that environment and finally
get cloud native so a lot of what I'm
seeing is people are saying actually
this stuff is not just an applicable if
you are running in the cloud it's also
applicable if you are thinking about
running your application in a
containerized environment so doc ka et
cetera right because those environments
are also trying to say well I want to
basically maximize the resources and I
want to do that in an elastic on-demand
self-service manner and what they're
saying is essentially here is my pool of
hosts within that I want to basically be
able to elastically expand a given app
in other words create new instances
across my fault of hosts and I want to
essentially do that in a self-service
manners on demand right quite often
actually responding directly to pressure
on the application itself
even though a lot of people you hear the
truck stop at the context of cloud
native be aware that it's also very very
useful if you're in a kind of a
container based environment as well okay
who's who's who's running docker tool
anyway non production production Gav
who's running doctor and production air
are those people and uk8 swarm Nomad
what I okay sorry okay so it's not the
top of this talk but your list people
like handsome and blogging about
containers you know she does that
recently so can containers I'm a
significant change that you will
probably want to be abreast of and the
top of particulars again some other
things that micro services where
containers are now being seen as
essentially a deployment mechanism
aligned to that kind of movement and
some people are saying that you know k-8
is a significant a development in the
industry as Linux so yeah it's worth
worth putting down your to-do list for
the year is containers alright and what
the focus of really 12 factor apps is to
a large extent what the focus of cloud
native or containers as London is
elasticity and that's really what we'll
try and show you how we work towards
that kind of elastic model tonight bar
12 our traps okay I don't have to read
this don't worry just a screenshot for
example so I'm 12 fighter axe originally
started at Heroku so Heroku are a cloud
providers platform as a service provider
I recently they basically started in the
Ruby world but now they're basically
more polyglot and Adam Wiggins one of
the cofounders he's no longer with
Heroku example in some writings
he looks at essentially apps that have
been deployed to Heroku over time by
various customers and try to isolate
what made some of them more successful
than others what were the common
patterns and approaches they used which
way which way that way the more
successful and the 12 factors were
essentially distilled from heroic whose
understanding of success what it takes
to be successful in the cloud how did
you do it so they published it article
then to say hey if you want to build the
cloud bear these things in mind so like
any of the 12 factors themselves as an
overall set of principles that's 12
fighter apps that are fundamentally
aligned to so the first thing is that
really one of the things that they're
trying to get towards is this idea that
says I come and sit in your team on day
one and my new job and I want to start
working how easy is that story because
really the the ideal is I say where's
the repo and you say here you are here's
the address of the red phone yes great
I'll get clone that I'll build it and
I'll run it that took me five minutes I
am ready to begin as a developer at your
organization how many of you had stories
at that today that right ok how many of
your stories that are more like yeah on
the wiki somewhere someone documented
the last time they did this but it's
kind of out of date over the last year
there was some email sent round about
that I remember now if you search from
Matt I think he sent him around about
how you install certificates to get the
right Hannah who has stories like that
more ok right so that's one of our goals
is to try and prevent that happening
have a clean country underlying
operating system what does that mean
well she honest with you that nowadays
this a little bit hazy but what we're
really saying at heart is I want my
application to be deployable without the
use of things like application
containers like is
roots of weather actually going to you
with that nowadays even modern cloud
platforms so the eye thing here is
really that I don't care very much about
the underlying infrastructure under ploy
to I'd have to be agnostic to that as
much as possible really I want to
basically say I don't want to touch that
at all I would rather be independent
topic so good especially along with the
container world of saying for running
the container I don't really care what
operating system that container uses
right just the sandbox process so it's
not a virtual machine it takes some
kernel libraries binds me to my
application right but declaratively all
I really care about is a certain number
of small touch points that basically
give me an application that I can run
and what we're saying is over time the
operating system fades from our view we
don't really care we care about running
the CLR LAN JVM or the Python you know
runtime etc but I don't really care
about the operating system provides a
life support to me and so that model is
increasing in that your application
needs to be as divorced as possible from
understanding the underlying platform so
things like talking to the file system
for example and how you do that right
that needs to be stuff where you need to
start moving away from taking
dependences on them on the expectation
that your lat you're deploying to a
given environment and the minimizer
divergence between development
production neighborhood continues to
point for maximum is your team so some
of this is about the fact that you know
does production but like staging and the
eyesore a lot of cases outside the cloud
environment is no because generally I
can't afford to have two sets a kit that
look the same in the cloud environment
this is actually more practicable
because you can submit up a server an
environment looks similar but even there
that's not that common but it's also
about some things that are not necessary
obvious from that statement for example
if my current version of the source
income the apex of where development is
is a long way were removed from what's
running in production
because of time we don't release that
ret that frequently that creates
cognitive dissonance for your developers
who when they see a production problem
now have to remember how the code used
to work two months ago when the last
time they released rather than all the
changes they're made in the last two
months right
see also bad things about frequent
releases it's also about not having
different personnel and the kind of
DevOps movement right working basically
in those different environments up and
down the stack of you or release process
so it's a more differences you have the
harder things get for you and you can't
read them the back scope that scale up
there significant changes to development
architecture or tooling practices in
other words I want to build an app that
will scale from the get-go
I don't want to have to suddenly say wow
eid to rewrite the whole thing now cuz
we're gonna have to scale it ok so how
can we try and avoid that problem right
from the beginning ok so what are the 12
factors because you've been sitting
listening for quite a while now and I
haven't mentioned any the 12 factors so
I don't have some idea what they are so
I won't read about because you can read
but I've grouped them with some colors
so I've done that because I think
they're easy to talk about in groups so
one group essentially really is about
how we design our application and that's
the group in green ok those are some
constraints were going to basically
asked you to follow at design time to
how you structure your application in
terms of dividing into modules and how
you actually host your application such
that we can meet some of the top actor
goals
the blue ones are essentially about how
we build our application and they're
really about that model of making it
easy for developers to join the team
making it easy for us to essentially
minimize the divergence between
production and other environments and
this is this is about runny ones in red
are about running our application right
we'll talk less about these tonight just
for time burly that's a lot about stuff
like saying we're all to standard out of
stuff I'll come back to those later I'll
give you a brief like two minute rundown
of those but so let's start by talking
about how we design reflector apps what
are the key issues that we want to talk
about what may be different what you
understand so the first one I used to
talk about is what's called port binding
so port binding and anyone know what
port binding is you don't have a stab of
that export services by port binding so
here's the idea when I have my
application it needs to talk to the
outside world in some way to do
something right my code is running it's
running on a machine how does it
interact with anything so it's a web
server essentially it communicates where
the outside world was a port 80 port 443
right so essentially it listens on those
ports and interacts for basically with
those reports via a requested response
so it's okay to an HTTP app it's very
straightforward to understand what I
want you to do is essentially say I'm
gonna run an application and the words
gonna talk to the outside world it is
going to essentially have ports we say
export Vong a port binding what we mean
is the application itself should be
should say I want to talk to the outside
world via port but the act have
effectively bind choosing what ports
they are and the URLs that they are
located at should be something you
actually do when you just deploy the
service so those of you look at
something like docker dr. obviously
essentially mirrors this kind of model
in the sense that whatever you run
inside a given container you can choose
to essentially change that port on the
host so you may be exporting effectively
port 5000 from your container and the
host may say well actually I'll treat
that to the outside world as port 80 so
you bind it differently but essentially
the idea is your simple little
application its first port O'Call would
be
to say okay I'm gonna have a port and
I'm gonna receive requests so without
return responses to that don't worry
this will make more sense from a little
bit the second thing is it should be
self hosted alright so I don't want to
have some kind of thing that my app has
to run in I want to bed I just basically
press you know to run the executable or
whatever basically run the basically the
interpreter and run my code spin it up
and get it going and it starts listing
on the port for me okay if I have
multiple parts of my application and the
way they talk to each other it's vice
entually fact than the other
applications exposing a port right so
you come in and call my web service
basically on port 80 and it says I will
nod to services request I'm going to
call a couple of micro services that's
fine they'll be listening on port two
and I'll call them bar HTTP okay so
everything is talking to everything else
for a port so what we're talking about
HTTP at the minute was an example
because it's fairly common ground we
don't necessarily have to be using HTTP
as a protocol so for example I may
decide that two of my apps will
communicate via messaging rather than by
say HTTP like JSON or rest in which case
essentially I'll be using now and keep P
protocol but I'm using a different port
I can't what rabbits default is for 370
something like that I think right so
again I'm talking to a poor I'm
listening on a poor basically for
messages right or I'm sending too
upsetting message to a poor you connect
to the broker over a port you say here's
a message but I'm still listening to
ports and the key to understanding this
idea is this line in the bottom right so
I read if those in the back avoid
hosting an application server like I is
so one of the things we used to do a lot
particularly inside of Java and.net
world was we ran in application
containers is or you know J watch my
eyes a cover that's basically a tomcat
Apache right we ran these big beasts and
the problem with that is it doesn't
really work
in the modern world it's kind of a
falsity so you want to get away from
that and say actually my little app will
host itself and just expose some ports
and let me explain why so this is all is
everyone kind of familiar with the lis
diagram yeah joy okay so the call comes
in from the internet and it hits HTTPS
which is part the operating system on
Windows and then it says okay what I'm
gonna do is I'm going to go to the
service so sexy and so is no sex he's
basically publishing service and I asked
you through the application host
configure the application the host
config will tell me about all the sites
that are is is hosting all I guess we'll
then say oh yes I'm hosting all these
sites and the way that you instantiate
this particular dotnet site effective
etcetera is you're going to run use the
winners activation service or I'll
eventually to run this w3 worker process
or when I'm more working with you worker
processes and that will effectively load
up your application and run it okay and
then you get all sorts of complexity
here where essentially which which are
important to the conversation around
well I may have multiple threads you
know may multiple processes generally
the thing to take away to understand
it's generally what happens is we run a
number of processes as inside the
container okay so it's not uncommon for
you to have a essentially four or five
different sites potentially running on
one instance of I is effectively at
least subsides and so you're saying well
I'm going to run w3 with WP I'm gonna
run all these things and hosting them in
this taking of Tecna all right a nice
application containers work and you can
find different variants in different
languages right this is docker so a
docker Apodaca host right so it's a UNIX
box actually a my UNIX box has something
called the docker daemon so basically
that's a process that runs basically in
the background and you can talk to that
from the docker client we show you that
happening later and the dock of client
says you can read docker files and so
it's got a whole range of commands
running sec dr. peter super to have a
dirac docker PS right so basically you
talk to your doctor demon virus but the
bar a rest api if actually the exposes
outside of the host and then what
happens is in response
the instructions you've given it will go
away to a registry will basically grab
an image we'll use the image to create a
container and we sped up hollow the
container so what's a container so a
container is not a VM right that's why
first thing first thing to understand
and container is essentially a sandbox
process running essentially on the host
that's why I wrote you but you can have
an entirely different flavor of Unix in
your container - in the host it's
because essentially all that really
matters is the kernel and you basically
build a set of libraries into your
container that do things like say I want
to access to file system set or other
operating system based services that
allows you to run the advantages of
course when any one of these containers
essentially falls over and dies its
sandbox didn't take anything else with
it
and I've what effectively I can do the
other thing which is so cute so I can
see what I've got what are the computer
capacity I have on the host I can share
it by sitting amongst these forest
containers and therefore I can get
maximized my usage of my instance so I
don't know if you know but I think
roughly on average speaking the
utilization of a ec2 instance for
example in DES is somewhere around 30 40
% which kind of means you're 60% that
you just money you did giving to Amazon
right the other 30% you're actually
paying for but this each percent is just
idle it's not doing anything and part of
the the pressure towards containerized
workloads is to actually move that
number all the way out to about 90%
right to try and say I want to increase
the utilization of my existing instances
and people talking about having
effectively their costs by doing that
but you can see that in this model I've
effectively got a set of containers
right would it make a lot of sense for
me to run IAS in one of those containers
probably not because is is doing pretty
much the same thing that this is doing
ok is is essentially saying hey I've got
basically a whole set of containers I'm
gonna run them for you in peds demand
so our alternative approach here is to
say each one of these containers talks
to the outside world over a port it says
I'm gonna expose ports on the container
the host effectively can remap that
expose that to the outside world but
these ports let you effectively call in
and get hold of these sandbox processes
and I can deploy them to multiple hosts
and when they appear on that additional
host I just say I use this port I'm
bound to that port right on that host
for communication so that's why we
basically talk about port binding right
because your app set I started to look
much more like this and much less like
this okay true backing services attaches
also so what is a backing service this
is the second second kind of like
principle so backing services anything
your application talks to so the obvious
thing is all the other services that you
built right you've got your micro
services architecture obviously because
why not
and you built 10,000 services your
Twitter and you're looking for lis your
Deathstar diagram effectively it will
just serve essentially there's a good
idea that way that approach but but all
those things are basically backing
services to you so anything you talk to
over a port is a backing service all
right but there are other things that
you what you might want your database is
essentially a backing service from the
point of view this I taught that I
taught it over a port right
it says service I put a report on just
say hello right click give it something
like you know Mongo whatever got a bid
and essentially I'm talking over that
via REST API because that would
obviously the way to what databases but
it's another service they talk to you
but there are others your file system
okay in the cloud and in containers you
cannot rely on the file system on the
instance that is running your code right
that file system data is not backed up
any way useful for you okay when the
machine goes down and AWS there buddy
see what happens is they put you know
cheap as chips machine
in the shipping containers and wait for
them to fail all right and they will
fail when it goes down on you all the
data on that file system was lost as you
backed it up somewhere so generally
don't do that generally you do something
that create a state block storage or s3
depending on your storage requirements
and you store your data out there
instead right so you say well my what
the abilities of file storage is a
service and I'm going to ask you for
that service and store things to it I'm
not going to assume that an operating
system dependent file system is the way
to go so I am breaking as we took I said
earlier my relationship with the
operating system I say I want to store
stuff I'll ask for the storage service
same is really true a little bit in
containers where you don't tend to talk
to the fast and the container very much
because it's ephemeral you quote just
wrote Maps something on the host what we
tend to do is create volumes put two
volumes instead right and that tends to
basically mirror the same way you're
taking and looking at these are
treatment services right anything you
need is a service that you have to talk
to in service environments particularly
so several some problems just contain
this right if I'm an AWS lambda or or a
sewer function what's happening just can
this containers so what happens is your
code is placed into a container there's
a pool of them essentially one of them
is spun up in response to basically
something coming in over a port or I
missed that whole thing that ports again
right when it comes in over a port talk
an instance of spun up or one said to be
out of a pool I get it I can do some
work on it returns the pool right next
request comes in I may get that same
container I may get a different
container I may get a new container
I cannot rely on what is on the file
system on the consistent local file
system and an awl and their owners your
function now some folks are naughty and
they say oh I'm gonna test to see which
one we got back and I'm gonna play
around with the fight with it with the
file system on the container because you
can access it same weak nexus in
container I can write code that calls it
because the
underlying container goes yes I can do a
filesystem requests I'm going to write
it's basically here we don't really know
where that is it's ephemeral it may be
on the host but you've got no guarantee
you have whether you'll be moved to a
different host right the schedge they
may just say go taking off this host and
I'm putting you on this host right
goodbye to the thing you wrote locally
to your filesystem
so you don't have that guarantee anymore
so all the things that you depend on has
to be treated as a service now or just
you need to provision any storage that
you want to use right you're gonna have
to ask for it as a service to get
provided to you no sense soul it's a
ports and backing services okay let me
do a quick demo then this is the world's
most over designed hello world
application built by me so he basically
got it so don't know core application
and we essentially have oh no man that
one let me just switch the versions in
github can you make it a bit simpler
make it a pony oh yes because I've got
an out directory somewhere in there hang
on let me just because I was actually
checking it all worked I've created a
few out directories you'll see me create
those later and a couple of them will
stop me one of them somewhere he's got
itself into the get rep I don't know why
and that causes me problems nope okay
that's just
okay
what
so great ignore godÃ­s work because it's
basically been pulled down but no Mon
ratings Adam greeting score will you
care about a minute so greetings f is
basically the extranet course I and
greetings Corps as a cell libraries to
go with it so quickly as controller is
essentially so the idea behind this site
I will show you essentially the test
file is that we're going to base the
post hello world or hello Mary or hello
Baba hello sue to the server which is
going to store them for us
so we can see all of our greetings in
future a fabulous application which will
obviously make me a lot of money after I
could have my and my first and initial
coin offering okay so you pretty much
what you expect from a controller we've
got basically a set of rest endpoints we
can get essentially by ID get all of
them we can post a new item we can
delete it this eases because it's needs
to be over engineered some libraries I
work on called brighter and darker which
is basic cqs framework fairly
straightforward to understand it's got
post so the command processor just says
wrap up basically the request we've got
from the outside to say here is
basically a new greeting turn it into a
command saying here's a new greeting and
pass it over to our command handler
effectively which will deal with it for
us and these come on handlers are
essentially over here in here partially
is just so I can show you something
other than just out code so what happens
here as we say we're going to take that
basically command we're going to spin up
a entity framework context we're going
to essentially create a repository and
in that repository we're going to add a
new greeting based on that command write
all code you've kind of seen before I'm
sure a thousand times
we're obviously at a completely async
pipeline because this is you know 2017
and we'd like to be async and this these
were reliability aspects that you can
see on are just basically the because
does for you builds a pipeline and you
can essentially insert things in the
PATA so just let's just run some poly
policies to do things that retries in
the circuit breakers so you can see all
that began like a brighter we you can
see how that all that works okay so as i
say that's probably the the world's most
over engineered had a weld app okay so
the other thing to understand here we've
we've got basically docker compose file
who's seen docker compose before just
okay not so much as fashion right okay
so we've got a number of services here
we've got a web and we've got my sequel
just using my sequel because it's
footprint is lower than the sequel
server in a container so it's easier and
less pain for my Mac and essentially
says a database if actually what talks
database the interesting things to
understand its what we're saying here
actually is we are exposing this port
5000 it's actually something in our
application runs on five thousand and
we're exposing there's four thousand you
can see we're doing the same thing here
with basically they're a database so
both of them are running on ports their
environment variables just note that
we'll talk about what those are later
since you compose this abuddin network
so we're effectively saying the web can
see base of the database right we have a
depends on to make sure that bridges up
for we start running web site okay so
what I'm gonna do is build that so I'm
just gonna what I'm doing here technique
I'm with using just to build locally to
build locally and viral publish were in
essentially out to an out directory and
then we'll pick that up and we actually
build the application in darker so you
can see we've got a few docker files in
here as well I think there's one or two
these these were just to see mainly
because you can see with the clarity of
nature of what we're talking about well
fight two apps right so essentially were
saying expose the port 5,000 that's how
we communicate with the outside well
we're bottlers this is port binding
right and that effectively gives us the
way that we talk to the outside world
right let me just see if this is going
to build for a second so doc is gonna do
its usual check i've already basically
got downloaded images because who knows
what kind of connectivity or might have
had and you can see they're afraid
got my sequel running and I've got the
web running right and you can see down a
bottom potentially I don't know you
might stand up at the back to see it but
those are you can see the bottom can see
that it says I'm listening basically on
port 5000 right so this idea that we're
basically listening on ports and we're
self hosted and you can see that in the
code over in program CS this is the
course house entity and an output in
that core application we spin up kestrel
and this is yesterday used I sent the
graciousness for debugging but we're not
hosting Liars at all here right it's
just casual standing on its own two feet
and saying we're gonna expose basically
a fan port 5000 now in a in a real app
you want to put nginx in front of this
because you don't because you want to
things like SSL termination properly etc
Rob or proxy caching but for our
purposes will work fine okay and then
you should be able to see or running
effectively here don't care the back oh
cool
we're running essentially here
effectively the my sequel in the web
okay now so we're gonna do you I think
I've got a few in there ready we're
going to fire a request the database and
we're gonna get to bring back to us
essentially what the contents are what
greetings has already got okay so you
can see Karis responded to us the 200 we
already have a range of greetings that
I've been I've been testing over time
the database is mapped to a volume in
docker compose automatically in the my
sequel and that means essentially will
persist between actually requests we
don't have to date yourself is just part
of the my sequel that you've got image
okay so let's say I want to put in hello
Alice I can do hello Alice right I get a
post request gets your back if I see it
tells me but you would have just created
and I can then go away and I can see
Alice okay
all right if we go over here a little
bit bigger then you may better see those
you the back is a bit awkward there's
nowhere I scrolling up but you can see
that the wave one is essentially
responding to requests when I'm running
this how-to container I'm exposing a
port I'm talking to another and to the
sequel database which that was just on
the service exposed by a port right and
I am essentially ephemeral I'm inside
containers so I am not relying on the
only the underlying services okay first
is in concurrency so most you've heard
this before so what you want to do is
execute your app as one or more
stateless processes and the stateless is
the important part of this note we're
going to scale in this fashion right one
restates what it won't be stateless well
state unfortunately prevents you from
scaling right so this is a bad word by
the way we should really probably say
state aware more so apps that a state
list don't read don't deal with state
what it means essentially is you're
going to go to a backing service and
retrieve the state or pass the state
around inside your HTTP requests but
you're not going to store the state
locally on one individual server from
historian we looked at one individual
server is of course that you don't have
to have a sticky session to that server
succession to that server is not going
to scale because I can't redirect to the
traffic that's overwhelming that server
to new services I bring them online
okay so we'll always stateless in order
to give ourselves the most flexibility
to be elastic I can't be on a stick
without being stateless I have to be
able to read it the traffic to all of my
notes when they appear not specific
notes you can use basically the file
system memory space for a per session
kind of cash in other words if the
request comes in and I want to basically
shove something away in memory and then
later on a few steps later access it
that can work right because you're
saying within the context
that given a request and response
lifecycle I want to use the file system
or the cache device I hold some values
for me but don't use basically local
resources for that if you need the file
system you know you look at s3 or EBS
like that if you need basically a cache
look at ElastiCache Redis just really
cache of some sort right to get it's a
backing of service out of process to you
okay task queues anyone knows what task
queues up so we can use different
processes to handle different workloads
so one workload is the HTTP requests
coming in another workload might be
effectively a long-running background
task being handled by a worker process
an application may experience peaks of
demand that cause become overloaded
unable to respond okay so this is what
happens right a request comes in another
request comes in another request comes
in and for some reason my application is
slow a typical reason might be
I've got resource contention on the
database right it's quite in the always
the databases that source your problems
and I taught and I'm waiting to get
basically my connection from the pool in
order to do my work after a while
essentially what may happen is I may
begin to either shed DB connections
because effectively I can't find one in
time so you start to get errors saying
DB connection timed out and effectively
your frustrate requests or essentially
you'll have a lot of lot of pool stuff
but the thread switching between them
will mean essentially that nothing's
being searched for right a long period
of time when your entire time context
switching and eventually you can't meet
new requests coming into your
application when she at some point your
queue and then you're getting your
screen of death
anyone seen DB connection pooling issues
at scale anyone seen the yellow screens
of death see if you have okay all right
other application when he was
dependencies becomes overwhelmed by the
sheer mouth of traffic right the
solution is to offload some of that work
generally we have a rule for example a
hydraulic can't see in the back I can
explain what the rule is saying that
really any piece of work must execute
under 300 milliseconds or you offload it
300 milliseconds it's roughly a good
rule of thumb for the ability of your
web server to respond quick enough that
it isn't overwhelmed by the rate of
incoming requests
well it's processing current requests
okay that's generally your problem if
your incoming requests exceed the rate
of which you're dealing with the
requests that you already have
effectively you will get a backlog of
requests which eventually will overwhelm
you so as soon as you start to have some
feel that getting a backlog and this is
good a rule of thumb but you may find
that essentially that even that's not
enough what you what are your options
effectively is the load balance right to
say I'm gonna hand this out by
increasing the number of instances of
the web process that I have see you say
I'm just gonna keep upping the number
misses and I say she's not bad you know
approach in elastic environment because
you say well it's for anyone on demand
ok but I can't get expensive the other
alternative is to start saying well I'm
going to queue those requests and deal
with them in my own good time
and what I will do is give a response
back to the user to indicate that I have
received your request but I've not yet
completed it so if you're an HTTP to our
to accepted you say I've received your
request but I haven't finished it and
you put get back a link header that says
you can go here to monitor basically the
outcome and that at the end of that link
is either a resource that either says
60% done or whatever progress marker you
want to use that's me or says the source
or effectively says well I'm 404 because
I am the resort or crating until you can
finally recreate me in which point you
do a redirect to recreate it okay and
the way you offload those requests is
generally to use message oriented
middleware of some sort though you can
build your stuff on things that redness
right so you say I'm gonna take this
work and I'm gonna put it on a message
queue
than what we received is a variable rate
and then essentially because I can
control the rate which my worker process
reads off the queue and processes work I
can turn that variable rate into a
constant rate and that constant rate is
nicely predictable because I can say
well I can support five worker processes
and it means don't get a scripted bill
from Amazon right so what again instead
is basically I know that I'm running up
to five worker processes and that's
where I'm going to stop and everything
else is basically queues up waiting now
obviously the downside of this approach
is that obviously somebody's work is
being handled asynchronously and you can
get a bunch of consistency issues but
from the point of view of many requests
that your users for fulfill that you
can't you're under 300 milliseconds and
she user expectation is quite often this
would obviously take some time I'll come
back and look at it later when you're
finished right so you can just give them
a response it says I can't get it out
okay so that's a task II put my
something onto the queue a service
endpoint consumer reads from it you
source eventual processing so look about
guaranteed and at least once a guarantee
delivery says I will eventually give you
this message I've received the message
thanks for you put on the queue I
received your work customer I will get
to it it's guaranteed now we're gonna
keep this one for you and at least once
at least once means a consumer we
delivered the message so the temperature
at least once and exactly once
so exactly once says there's one message
one consumer will get it and they'll
process it okay at least one says
there's one message at least one
consumer will get it and process it so I
sort of stupid systems and how we
actually cope with basically failure so
for example in some cases we don't know
whether or not somebody received we may
start to resend to everybody in which
case effectivity may receive a copy
because you didn't know whether had
anybody actually actioned it right but
there are ways of effectively achieving
exactly once if you need to otherwise he
sent reliant idempotence in other words
essentially I can replay the message
another tie
to avoid essentially the issue of that
replay causing you problems so you can
do that my thing is like have I seen the
message before is the idea in a database
discard it
actually if I reapply this change it
will be the same so doesn't make any
difference okay so the key X is a buffer
the bayseal a secret right of
consumption
it's really easy to capacity party right
yep and yet tolerate processing less
than the Y alright okay I will show you
how he works so although I work a
brighter and original using bright and
darker areas by dark basically support
this for you they support our skis mass
transit and service bus of the and
supposed to the other things the bow see
you may well have heard of it that
basically support this model as well
you're in the Python world celery
basically if you're no GS well Seneca
would be equivalent tools okay so let me
switch to a version that has app and
work right so we had that worker but we
didn't really look out earlier so we're
gonna look at working now and have a
look at what it's doing so we won't go
into too much detail on worker but
essentially a workers just another
process that we can run its a dotnet
process again effectively and it's got
an entry point but it's not explicitly
looking at a port there we'll show you
how we do that later so essentially just
a console application so nowadays we
tend to use console applications a lot
because you want me to cross-platform
obviously things that Windows services
don't exist so you have to rely on
something else
to keep you up in a UNIX environment
either in a container don't bother
because the container itself is the unit
you can restart actually it's a service
if you're just on flat UNIX you stuff
like supervisor okay so essentially what
we do in our main was you create
basically a single dispatcher just
brighter provides for you and we say
receive they're just means by
so we're going to receive messages from
a queue and effectively we'll keep doing
that until by secretly you hit a key
which point was was stopped
I've actually and will end okay so most
of this coach does configuration of that
and I don't want to basically bore you
with that that stuff but this is the
thing that's worth having a quick look
at the dispatcher is created by this
fluent application and down here you can
see what we're saying is we're going to
talk to a connection see if I can you
can see and that connection effectively
is exposing some information from rabbit
and it's saying here's the name of the
connection as a routine key so the key
thing to understand and rabbit is but
but on say rabbitmq I subscribed to a
topic the routine key and I say to
rabbit give me all the messages that
match that and what is it creates a
queue for me for my application and it
passes down my copy of all of those so a
sender or producer sends to ride that
message with the given key and everyone
that subscribed to that key gets their
own individual queue then which is
passed okay however I can share that
queue between multiple consumers so they
can process that queue because you can
lock individual records as you're
working on them then the next consumer
gets the next item in the queue
okay so just going to create some keys
we're going to listen on them right now
you'll see that what it's doing there is
it's talking about a regreat command so
read Greek an one is something new we've
added in here right absolutely and I
rewrite command effectively just this
come on essentially says I'm gonna give
I have a Guin basically I'm gonna say
here is a keyword value that's naughty
but that it works and when I basically
add a greeting now I am gonna say
raising new greeting rewritten commands
so basically brighter gives you some
stuff as well as to read off to use the
poster cues so earlier we were using a
send write and that just says handle it
in process post means handle it out of
process so it's going to say put this
onto
but send this to basically to rabbitmq
and the worker process we'll read that
off the queue and and do some work with
it and the worker process is simply
going to write to the console when it
receives that message okay it's going to
put out the greeting idea agree to agree
to it pretty much as you receive thank
you so this means that when I run this
file here when I run this this time and
add something what should happen is that
that will go to rabbitmq and when it
goes for a bit and cue it will then
effectively want the consumer will then
read that basically off the queue and
then handle it okay okay compose the
other docket composed to note is
essentially you got a bit more
complexity in here as well as the web we
now have a worker role okay
we also have rabbitmq here and we have
basically my sequel so we have some more
infrastructure built up and the worker
effectively has its own docker file we
saw earlier just basically the flat
dotnet exposing an entry point that's
been built okay you can see we're also
using some environment variables okay
so quite a bit later but one thing we
don't want to do essentially is rely on
configuration files so by using
environment variables we're very easy
with something like this given the
container application to set things that
point to other parts of our network
better alternative in production systems
is to use a configuration management
server like etcd or console but that
topic is a bit later in the slide don't
we'll come I just want you to note that
briefly now we'll come back to top the
topic for later so what I'm gonna do
hopefully I've got time to do this is
I'm gonna stop these two and I'm going
to just build cuz I've got some
additional artifacts
I'm just gonna build the additional
projects and then we'll basically bring
it up when you should see then with your
docker PS we've got a lot more roll Wars
this time around than previously we've
now got our worker process so remember
we're talking about as we're saying
we've got a web process but we want to
offload some work to a worker process
we're gonna use a queue because that
means we can simply say I'm gonna act
back to the user and say I've got your
request in fact in this case I've added
something right I've done an ADD and
added your get your greeting I got some
additional words has kept it to carry
out I don't to do that basically for you
right now I can do that basically
asynchronously so I've posted onto a Q
and the Q can handle that so although
our request is very simple you can
imagine that maybe do something more
complicated like for example doing an
eat sending an email right that's not
not an uncommon thing I want to send an
email I don't want to make much to make
you wait around why call the SMTP server
I'm gonna act back your request put
something on the queue and from the keep
when that process the item on the queue
I will call the SMTP server and send out
your mail to you're given your Valentine
or whatever okay being trying be topical
there okay so we will do docker compose
out build so we'll bring up that sweet I
may have to kill rabbit
I always think I've got a local rabbit
no it's fine okay so you can see all the
workers in this printing out but if we
did go to the other tab and Dee I'm
sorry you know dr. Pierre you will see
that we've basically running a worker
and web as well as rabbit and my sequel
okay so now when I go to tests HDP and I
add a greeting let's just do hello well
the time being okay we should see
over here it's a little bit hard to see
but it should refuse that sell you on
the process don't worry
okay so again unfortunately one of the
problems are doing this these demos is
that the it Scrolls to the bomb the
screen is about 20 other things for you
before you can actually see it those are
the back but can you see essentially we
said we received a greeting messages
follows hello world Britain fortunate to
follow the gooey detector there right
you can see up here by the way there's
other stuff were pumping out basically
by default at a brighter for you and one
of the things we're pumping out is that
we were a CD message basically from that
cue and that's the contents of the
message that we were received over the
wall I all right so because we basically
turn an info level debugging you can
kind of see a lot of information but
that's gone over the key if I send
another one for example Alice send a few
so we respond quite quickly right
there's no real delay effectively to the
response you're seeing in the other
window twenty seven milliseconds twenty
milliseconds right but over here you'll
see a whole range of these coming into
West your offload Network that work on
to the worker process so this is idea
essentially we've got basically we've
already been through we said well we're
going to basically create these apps and
these apps are going to have a number of
factors that make them suitable for use
in cloud or container environments and
we said a basic app will be a kind of
stand-alone
process that exposes itself by report
and what we want to do is run one or
more of those and some of those will be
maybe web processes something to be work
of processes and they may communicate
with different protocols so HTTP to get
in and out sort of rest or AMQP for
messaging which we just saw right and we
said you know some advantages
essentially of this kind of stateless
these stateliest processes is that well
we can say well i'm going to hand this
work off and run
and press over here and that's going to
do some work for me but the other
advantage basically is that I can begin
to use this to scale and remember at the
beginning of this conversation we were
talking about this idea saying you know
elasticity is the key to understanding
what cloud its elastic on-demand
self-service and that containers are
becoming elastic on-demand self-service
so we haven't really kind of shown that
at all right so so how are we getting
the elasticity so all these individual
processes what we do is we prefer to
scale by creating a new instance of the
process so it's not to say under 12 back
to wraps don't use threads right but
it's saying don't rely effectively on
vertical scaling so don't rely on
essentially getting increasingly bigger
and bigger and more powerful pieces of
tin to run that one process on with its
spinning you got more threads for the
more CPUs you've got use a model where
essentially you say I'm just gonna
create more processes because that
enables us to actually work with things
like schedulers much more easily right
so in a world where effectively I have
this massive multi-core machine my
problem essentially is I've got one of
that machine right and it's essentially
a point of failure what I really want is
multiple machines so if I lose one
machine are continue to have capacity
and it's much easier essentially to have
lots of machine small machines it is to
have one big machine in terms of that
capacity and providing and if I have
individualized processes then I can
allocate them essentially to that array
of machines and say well you run here
you weren't here this was hi Derek and I
scheduling an orchestration is about all
right I'm gonna guarantee I'm gonna run
12 instances of your process I've got
four hosts to run Amman I will
distribute them amongst those hosts ok
and now you go to 12 it's one of them
goes down I will bring you up to 12
instances again by essentially
provisioning more than I lose a host and
you can have six instances and the two
machines maybe but the process model
lets us have some flexibility that we
don't really get with threads and
because we're essentially stateless we
share now
thing that we just introduced more
capacity by effectively AGC more threads
now obviously there are a scaling
bottlenecks or there always are so your
database for example might be a scaling
bottleneck you may need to move to
should be databases that can et cetera
but and this thing that demonize
demonize use the offering system process
manager what we mean by that is the
processes that we run shouldn't be
thought of as demons in the UNIX world I
mean essentially Windows services as
well Windows well as she relied
basically on our ability to change the
proces differently so as you getting
through that containers or even into the
world of the cloud really were
externalizing our ability to do that
scheduling essentially to outside the
application maximize our buses to farce
telephone graceful shutdown so if I want
to scale my problem is going to be that
I don't want something where I say I'm
gonna bring up a new instance in an
elastic and self-service and on-demand
manner it will be ready in 10 minutes
when it's finished building the model
right that's obviously a problem because
you can't react to scale at that kind of
lag the same is also true if I were to
take something down right I don't want
to buddy meet the peak demand and say
2:00 to 3:00 in the afternoon when the
US and UK customers are all online at
the same time and then you and then say
ok well we can take all the traffic down
now because the UK has gone offline yeah
they'll be they'll be down in an hour
you know right well that's not good
because that's what money we're paying
for so really it wants better start and
stop very quickly so our process and
make sure it can do that ok start a few
seconds and stop gracefully what am i
stop gracefully well generally if I want
to kill a process there's a question
which is what work is it currently doing
and there are two major strategies right
one is allow
taking no new work but allow all the
work I have to run out or just
essentially abandon the work that I have
and assume essentially that that work
will be be resumed on restart the
application so for example if I'm a web
endpoint
there's nothing gonna keep that request
for next time around so
the best model would really be as I shut
down to say okay I'm not gonna receive
any more HTTP requests but I was service
the ones that I have so for example to
turn I can nginx
that's nginx is model effectively
Internet Explorer say I will kill the
exist I will let the existing processes
finish before I shut down there's
nowhere for the work to go using a queue
you could just say it doesn't matter I
can kill the process because if my
message-oriented middleware doesn't
receive an acknowledgement it will leave
the item on the queue after it basically
unlocks it after a suitable timeout so
when the app when the process restarts
the world will be available for again to
pull off the queue all right it's enough
on brighter to think I work on monkey
purchasing we actually follow the nginx
model and let the work run out and
that's mainly because we don't want you
to lose the investment in time you've
had already processing work so we try
and say what it will take taken in your
requests but will process the existing
work that's sometimes controversial with
people in huddle because it means some
times it's difficult to kill the process
in an adequate time because it's doing
some big big job in which case you have
to go and kill it aggressively by you
know burning with fire there we go
but right so you can see here the idea
basically of competing consumers where
we're effectively saying well what I the
rate of one consumer reading off the
queue is not the constant rate that I
seek I've taken a variable right to
moved it to a constant rate but my
constant rate needs to be greater than
the Kanako achieved for one consumer so
I'll get multiple consumers through the
queue so they're gonna read messaged off
the queue and all I need for that is my
message or in the middleware to support
the idea and if the first consumer is
consuming this item the second consumer
says well that one's a lot I'll I'll
consume the second item and the third
consumer says are the first two a lot
I'll consume the third item right so you
have to have support for that in your
key manager you may think well surely
all majority middleware comes with that
forever mq will something like Kafka
won't write CACI I see well that kind of
work yourself right
this term is very simple so one thing I
can do with docker compose is I can say
well run them just have the one instance
of my web worker role I'd like some more
now what we need to if we do this right
now this one that's now how it uses the
nginx as a reverse proxy we may show
that working I don't know whether or not
we will manage to get to see that okay
and so when I now do docker compose so I
don't have to rotate or take down the
ones that are currently running it
should figure out for me what I've added
and then bring them into the mix he said
and I think compose
oh look another reverse proxy okay when
do they're back we should have just in
and no you don't want to bring up my
additional workers do you okay let's try
to shut those down we will go old school
and bring them up explicitly
okay cool
now well I'm running here as you can see
is a webinar worker what I want to do is
actually increase the number of
consumers on my cue so I'm going to this
time pass in a scale parameter and say
worker equals three this should I get
the syntax right start up some
additional workers you can see now
worker two and worker three now when I
come over to my tests and I add a new
request
I had a few actually because otherwise
that scores off the bottom on the screen
you guys won't see what we should see is
a range of workers and now dealing with
those requests so we've got up here
worker three dealing with OS worker two
picks up then Alice
worker three is picking up our some
worker one is picking up Alice right and
so up here somewhere that's probably
actually some other ones there's Mary
basically from working one and Mary
there so can you see that what's
happened is that the multiple workers
are now picking up work when we
basically raise that greeting right that
we greet then what happens now is that
there are three workers and those three
workers are all reading from the queue
so when we send any one message through
any one of the three could potentially
receive it and start working so that
one's gone basically to worker to
and that one's gone to work on one right
so what happens is they're really in
turn so that's effectively given me
scaling by just saying in increment a
number of instances of that process that
I have no sense so there's a demo here I
won't actually do but just because I
don't have a helps or internet
connection and stuff like that well tell
you what would happen and it's fine you
won't you won't miss anything so let's
talk about basically they talk about
design factors we're talking about
essentially how we build our
applications in a 12 fighter app model
to really take advantage of the
elasticity that's available in cloud or
container environments so we build them
as processes that listen on ports that
are self hosted and that can be that
talked to everything around was a
backing service assuming that the
environment that they are in is
essentially not to be dependent on and
may have separate web and web worker
roles that other roles basically would
be on that and that we scale the
stateless processes by simply increasing
the number of them right let's look a
little bit about some things that are
recommended really around how we build
and release our software so it's only a
one code base in revision control many
deploys so essentially when I come into
your organization what I want to do is I
want to say where is this app there's
clearly a raffle that that app belongs
to and I go and do you get clone and I
have the app I don't go get clone on
twelve different repos and you say you
have to build this one then that one and
then one other I effectively said sure
doesn't doesn't work there's just one
roof oh that's our app and essentially
yeah why not is one wrapper multiple
refers are just for the system right
forgot multiple wrappers I have this
root system as a bit of controversy at
this point and the controversies about
web and worker okay if I have a web and
a worker process and they're working on
the same workload is that one repo or
effectively is that to repos is it
because they're too
processes that two different approaches
they all put your efforts kind of pens
on your model I think so we have a model
whereby we say that that worker process
that you saw earlier
it's Paulette the same bounded context
as the web process you can access the
same two main model and therefore
essentially ought to ship along with the
web process it's what we would call a
continuous integration boundary because
if I change the schema say the
underlying database both of them will be
impacted so they must therefore be bar
at this time see I found them I ship
together so we would say if it's part
the same CI boundary effectively as far
as saying bounded context then basically
the app that they're both part the same
rep oh right
if effectively they are not then
essentially they're separate repos so we
would tend to target as much as
basically saying one I think was one rep
oh I've tend to say one bounded context
equals one rep oh but you may have
multiple processes in that bounded
context but that is just something we've
found that works for us more because
otherwise you do end up building
multiple records which is currently
against the point for one given
basically bounded context and what will
one CI boundary right share customer no
dependency mention not a shared source
in wrap oh so they don't like go
programmers basically so those you don't
know goes in 1980s language where copy
and paste reuse is king
where effectively we haven't got
generics we Cox we cut and paste and we
don't have pet package manager so we cut
and paste so the guys that wrote go
breathes heavily copy and paste reuse
the rest of what has that has package
managers for good oil-like because you
can remove things that l-pad and spread
one up but um what was saying
essentially is we want you be if you
have shared code then you should use
your package manager facility to do that
so rather in your organization than
saying okay well basically just you know
this wrapper effectively bolt the code
into this repo and then build your
application so then you know get this
subdirectory and what you want to do
effectively is say if I've got some
shared items internally exposed to by
the package man
internally get pro gate or something or
just get a file you can you can suppose
basically a folders and you didn't you
get feed if you if you want to so just
place to make your package Boas and you
get packages and ultimately this kind of
model leads you down the world of things
like you know everything is you get
octopus right deploy but yeah so
basically just use your package manager
for your own dependencies internally and
deploys running instance the ant using
the same wrapper
so the idea what happens is this I want
to get a clean machine on that clean
machine I only think I want to basically
potentially pre-install is the operating
system and a copy of your runtime right
and then I want to load your app onto
that and essentially summon your
dependencies so that I know that this
instance only has the dependencies that
you need to run nothing else I don't
want to ship the SDK
I just want to ship your app and it's
dependences nowadays you can also use
two-stage builds in docker
which kind of help with this model
effectively you load all the sdk you
need to build stuff onto one docker
instance you build it and then
effectively you copy the build artifact
over to another container right but you
don't want to basically ship all that
other stuff and so you just want to
essentially pull everything in apart
from the git clone to get the source
code and then pull down the package
manager dotnet has a lot going for it
and this would donate core because
essentially you have done that restore
and don't know build now calls done that
restore automatically done that
understands his principle places like
python effectively you have kind of pip
freeze effectively and click install
requirements not text factory what
people do so eventually you create you
create a text file and basically with
the freeze then he just install it
different models but all doing the same
thing okay so what i was going to show
here but I don't have a internet
connection setup it's just I've got a
repo for that particular project and you
can go and I can get cloned basically
and pull that project down into a temp
directory and then build it just kind of
the idea and it would build and then do
a restore activity and pull the packages
off the internet and effectively build
that bill for you just to show you that
whole life is built that way
which didn't I said to find this is yeah
package manager right pip but yeah but
virtual ends yes obviously you may need
basically isolation of if you want to
install this is the bit doesn't tend to
happen so much nowadays so if I was
stalling say for example Python onto a
Linux machine I've got a couple of
problems one is generally quite offering
some not got buns he's probably got
really using Python as a result using
either OpenStack on it and I don't want
to really install my version of Python
over the operating systems version of
Python has to break things second I
might have multiple Python applications
running on my machine so I sit them from
each other so H land was a way of saying
this Python uses this version at runtime
these libraries it's all isolated from
each other right we don't really have
that in net well we tend to have I see
more of our model of saying you know
what's in the in the directory that
you're in kind of controls that process
for you instead but avoid things like
global assembly cache right global
assembly cache means I do not know what
version I'm running with effectively so
the the mantra is clone restore build
and run right that is the the the gold
standard process for a twelve factor app
so git clone doesn't restore then they
build on it a run right okay so actually
that config still convict in the
environment okay how many of you store
tons of like you know plat for not
platform environment specific variables
in web config files or other config
files for dotnet effective etcetera or
or any files in Python land or whatever
right and how many of you have some
slightly overwrought process that takes
the certainly
huddling in at the world where the
majority of some of the build scripts
dead they're all written in you know
sake in cake or no they were written in
Nant or rake or whatever was the flavor
that of the day their main job is
essentially to say I go and get these
values and I merge them in basically to
the config file
they don't actually build anything
because it must build just does that for
you right or what Petra well what they
mainly do is basically build these
configuration files and it's kind of
crazy because we have these overall
build process they're just really around
template merging and the other problem
is what am i shipping right really what
I want to do is build something and say
that is my build artifact I've tested
that on you know dev QA staging whatever
you call your various environments
production okay it's immutable the same
thing that the QA around against was the
same thing the effectively that we're
about to deploy the problem with
essentially doing all these models of
effectively saying I want to basically
put configuration in configuration files
is that you're not shipping immutable
artifacts anymore
they should be immutable artifacts to
each environment and that results in
every so often you should be in
completely the wrong set of
configuration values into production so
don't do that
we talked about configuration we we
split li means something that variant
deploys so you for example don't make
your config file has some things like
assembly redirection etc and that's
ready to do with it the app needs to run
and that's how it does it what we mean
is stuff that virtual environments that
were database located whereas other
service I depend upon located what
feature flags are turned on great not I
need to a seminary direction that stuff
doesn't vary by environment right your
assembly redirection these do not vary
because you're in production or in a dev
environment so it's a fighter app we
don't check any of that into the
repository and those secrets no checking
in basically our AWS keys into our
github repos because people find that
pretty fast
so model climate perhaps you wish to
come up with a story in environment
variables okay
environment verbs are very easy to work
with in the sense that you know the
platform has them many every language
and library has an easy way of saying go
and get me an environment variable value
and they can be configured quite easy
and things like docker or docker compose
he saw do you remember seeing I can show
you if you don't in docker compose
father was section I was setting
environment variables because my Cove
was then reading the environment
variables and that means that
essentially the compose fault she's
doing the orchestration is injecting the
values needed in for that particular
environment some people there are some
problems to environment variables in
terms of really genuinely using them
beyond say non production environments
and that's picket that's because some
folks don't like the fact that mostly
they can appearance that traces so
therefore you can see the secrets and
also can get difficult to manage right
how do I manage basically rolling out
the changes to basically no particular
secrets etc locations to environments
how do I manage rolling out of our
environment variables where all my X
process instances are so now there's a
lot more people starts to move the stuff
over to configuration servers etcd
console vault if you want to
particularly secure instead but take
this information out of your application
and put it in the environment okay
things that routing table is not go to
the configuration right so anything
anything that basically might one of my
rules there would be where I normally
rebuild the application to deploy this
if I would rebuild the application to
deploy this it doesn't count as
configuration even if I might for some
reason the story externally okay you
separate Bowden run stages this is kind
of extension of that so I build
something and then later effectively I
take that and put it in an environment
it reads the config for the environment
it runs yeah this one you can tell the
Purser's are written there doing with a
lot of PHP developers who tend to debase
it just change the production code base
lovely okay so what I'm gonna do is talk
about the ones you talked about and I'll
cover the other three that are not
listed okay sweet at the beginning we
talked about what the gold trifecta apps
were so I'm gonna run through those
goals again hopefully have a clear
understanding of how what I've been
talking about now relates to achieving
those goals so use declarative format
set of automation to minimize time with
caution about joining the project okay
so we're not I mean by that is
effectively really are what I want to
use is you know things like docker or
even an sploshy have to get stuff
running okay clean contract maximum
portability don't depend on things in
the operating system right use backing
services don't depend on the file system
being available or are config file being
available think in terms of how am I
going to basically provide that service
in a cloud environment or effects VR
container environment okay
she's floating on cloud platforms
minimize divergence between development
production right so that's just
basically this mantra of saying we
particularly you know don't wait too
long don't have too many differences in
personnel and don't have too much
difference between how I deploy my code
in different environments so things like
containers are really useful here
because I can essentially say well look
I'm going to deploy to exactly the very
similar environment always the same
environment gonna run in production okay
and we can scale up so that significant
change to tuning lighter deploying
practices so all the model we took up
the design different processes except
refractor or about running this elastic
scaling that we wanted to achieve okay
that's okay so those the twelve these
the thread and talk about so you can
read those nine he should have hopefully
recognize we were covering them so the
code base dependence is isolated that's
none of those needs and you get out of
something or pepper
still config in the environment other
words basically use environment
variables or a config server i remember
that everything we're depending on
should not be considered a local to the
machine which is running it should be a
backing service that talked
build and run are separate items make
sure that basically you ship immutable
artifacts into run one or more stateless
processes be stateless North allows to
scale use talks don't host ourselves in
containers remember we're like to
basically ship to a model where
effectively port binding a smarter serve
lists containers etc scale out by the
process model right so essentially
you've never got stateless processes
just increase the number of them
basically and scale out rather than
trying to scale up for an individual
machine fast stuff upon grace were shut
down we want to be elastic so we're
gonna need to be fast okay key
development stage in production as soon
as possible this we talked a little bit
earlier we're just saying the more
different theories in other words the
longer the deployment pipeline takes you
the more risk you entire or people not
understanding what the production
environment is like the more rapidly you
can release changes out through your
schedule the greater UD risk your put
your the difference between your
production environment and your
development environment but it also
means essentially simple you know they
have the same people do it right
don't create this pass over the fence
divided between essentially
staging and production where we say here
you go now
great we're off to do something else
right you don't have to run it here and
here use that knowledge in production
treat logs event streams so you'll
notice that when I was running docker
we're getting extreme stuff out to the
console it's just logs and so if you
like to stand it out and what happens in
the docker container is that gets stored
on the host etc logs and you can pick
that with that stuff like file BT cetera
and push it across the Cabana you run
down a container right but what you
don't want to do essentially is write
logs to a local disk on the assumption
that you can go and retrieve them from
local disk there are two problems with
it one is look this might not be there
to at scale with lots of processes lots
of micro services there's an awful
number of machines they're gonna have to
track to find something right so what
you want to do is just push things to
standard out and then rely on something
effectively taking that stream
from a standard out I'm putting it
somewhere you can go and then basically
investigate it right be that you take it
for a standard out you cop it somewhere
and he you said an awk on it to use
Cabana on it basically in the elf stack
right but treats pitchers quite often
you know the problem here is if I'm
honest I'm running service for example
as your functions or basically a double
slander now I would log into that
basically a farm the local disc where
the hell is that gonna be that container
could be could be gone before I can read
my logs right I have to log to something
that it's a stream that I can capture
and take off that and basically look at
later
yeah so treat your loggers stream load
to standard out and then pick up that
log and then base write to somewhere
else and the final one is where an admin
management tasks one off processes all
this all this is actually talking about
is saying if I have some admin if I have
some tooling that relates to my app
included is the same repo is my app and
basically makes sure that it's tested
along with my app otherwise what tends
to happen is that my tooling which gives
in some separate repo gets forgotten
about when I want to run that tooling I
realized that it looks basically like
the version four of basically my app and
modifies the way we did things there but
I'm now on version seven and running
this fixed tool is going to be a
disaster because you no longer work that
way right so if I have that kind of
tooling it should live in the same repo
as essentially the processes effectively
that's supporting make sense</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>