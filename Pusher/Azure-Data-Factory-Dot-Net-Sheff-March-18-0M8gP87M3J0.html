<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Azure Data Factory - Dot Net Sheff - March 18 | Coder Coacher - Coaching Coders</title><meta content="Azure Data Factory - Dot Net Sheff - March 18 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Azure Data Factory - Dot Net Sheff - March 18</b></h2><h5 class="post__date">2018-03-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0M8gP87M3J0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">if you consider that a software
developers what we essentially do is
capture data and move it around for
people something like as your data
factory might save you a lot of time it
might make your life a lot easier
so I'm just going to jump straight into
it and hopefully it'll work because I'm
going to create it all from scratch so
I'm in Asher and the first thing we do
is just create a new resource as your
data factory so is anyone of these def
actually before one person which version
did use yeah well I'm going to talk
about vision - if you used version one
then I feel sorry for you because it's
really bad alright okay so version one
have a lot - left a lot to be desired in
my opinion version two is very good so
once you've created your resources click
author and monitor and it takes us into
this nice like web ID environment thing
the first thing I'm going to do is
configure git repository this is one of
the really nice features of data factory
so right now it supports VST s it's
going to support a table in the future
and I imagine maybe more providers after
that and what this means is that all the
changes and you know all the things I'm
going to create today in the next 10
minutes they're going to be committed to
source control straightaway so even
though I'm using this really nice you I
am using this web you I have to download
anything but it's still persisted so I
can collaborate with you know my my my
team mates and stuff and it's all backed
up for me so they have vertical scenario
that we're going to use today is I'm
working for a semi large organization
and non-technical stakeholders asked me
to create a regular CSV export of deer
because we've got this database but he
doesn't want us to use SQL to see it
right so I could go in and spend a day
might create in a CSV download area of
the admin portal
or I could spend 10 minutes ago in a
data factory that pumps it straight into
his machine that's what we're gonna do
today it's pretty simple the first thing
we need to do is wait loads we're going
to create an integrated runtime and one
of the important things to take away
from this is that the data factory
itself doesn't do any of the computation
as your does the computation for you if
you if you're moving data between cloud
services and those cloud services then
do the computation if you're moving data
from on-premises stores then it's it's
that service that will you know do the
competition but one thing we can do is
tell us your - to do this in a certain
region so just to demonstrate I won I
wanted or - to do all my runtime stuff
in the UK self whatever reason it might
be a legal reasons they might be
limitations of the platform but
everything I do who's going to be
running UK soft it Center the next thing
I do is create two linked services and a
linked service is just a pointer to a
data store so it stores the credentials
and after that we don't have to worry
about the you know the credentials so
we're going to create a sequel database
gonna call this database you case on and
I'm going to select it straight from
Azure subscription don't know to share
and just enter the Krebs
 do I do that test that connection
so there we go we've kept it to the date
of this successfully the next thing we
do is I'm going to create a connection
to a file share that I've created
already in ezreal storage
so in this hypothetical scenario my
non-technical stakeholder has this file
share mounted on his computer already so
one Wednesday when this CSV is export
would he'll have it straight away I'm
going to call that foul sure going UK so
I'm just going to get these kids out of
here now I did mention before that this
is all stored in source control but one
nice thing that the effector does is it
omits passwords from that so you want to
get any sensitive information in there
so that's cleared now that it's the next
step for me is to create a few data sets
and a data set is it's a pointer to a
set of data like for example an SQL
table inside the link inside the SQL
database which have this create a linked
service for so I could have three or
four SQL data sets pointing at the same
linked service using the same
credentials but at the point in two
different tables on the database on the
idea I'm going to create this one I'll
call it orders now see here all I do is
select my database linked service and no
just oh I just picked my table and
that's it we can import the schema and
there you go we've got five columns
they're all bugling names because some
rubbish developer named than three years
ago but we can sort out later I'm going
to create another date so that this data
set is going to point to a CSV file in
table storage just file share so we call
this CSV file but one thing we don't
want to do is overwrite the any CSV from
previous rooms so this is where we can
make use of dynamic content in data
factory so if I click on to the file
name field here and click add dynamic
content I can sort of use this domain
specific language here too like a create
dynamic content like for example name
the csv file after the the idea of
libram the pipeline room so it's
dynamically generated and it won't
overwrite any previous exports so I'm
just going to call this one if you can
all see that and it's pretty
self-explanatory it's just going to get
there the current date change it to a
nice format up there in idea after input
dot CSV amend so we've got a we've got a
late we've got a dataset put into a CSV
file which will be unique on every
pipeline run
don't know why it's doing that Oh now on
the skin section obviously for a
non-existent CSV file we don't have a
schema so I'm just going to create it
and here I'm going to give in the
columns last names
customer ID theater it created it
completed so I can set a time for these
columns these are date times these
integers and that's done I'm going to
create one more dish that this datasets
going to get the first the value of the
first row in a table called the
watermark table and at the end of the
pipeline room we're gonna update the
watermark value to the current date and
all that means is that when we are
getting the data that we're gonna export
we can make sure we're not getting a
data that we've already exported in past
rooms so again we're going to point to
an SQL table and call this one watermark
of this boat and
there we go point to the watermark table
will import the schema I've got one one
column called value starts flow now this
is the last bit we're gonna create the
pipeline a pipeline is a logical
collection of activities and your groups
given multiple activities you can chain
them together and that turret is do the
heavy lifting so the first thing we're
going to do I'm going to use a we're
going to create a lookup gonna call it
get watermark and as the name suggests
it's gonna it's going to look up the
value of the watermark and then we can
pass on to the next activity in the
pipeline so the source data set being
the watermark table we're just gonna get
the whole table but we're going to keep
the first rule and that's that one then
the next step in the pipeline is to of
course sequel to CSV we're gonna copy
the data from sequel into a CSV file
thus that's what we're here to do so
this time source data sales orders but
instead of getting the whole table which
that would do I'm gonna use this query
here and this query has dynamic content
and it just like we use previously so
select all from the order table where
DCA hold it created is greater than the
first row value from the last activity
it's pretty simple that's the source
data set the sync is the destination
data set and we're going to pump it out
to a CSV file everything else is fine
and here we import the schemas to create
a mapping and they're all in order so
it's months it's figured out nicely for
is like CID is customer ID and it's
gonna include them all so it Maps all
nicely so that's that done and all we're
gonna do here is talk about along there
and then as a sort of cleanup phase
we're going to do two things the first
thing is we're going to instigate this
stored procedure which I'll call
update watermark so that's the stored
procedure that's on the sequel database
that just updates the watermark value to
the current date there you go set what
mark that's at London and then I'm also
gonna make a hit CTP request I'll call
this one notify slack so this is one of
the really cool things about data
factory you can call it you can create
HTTP requests and then use the value of
the response or not and that basically
allows you to do anything you know HTTP
is the language of the internet so you
can do anything that you can in a
browser and one of the really nice
things that you can do with that is you
can trigger another pipeline at the end
of your current pipeline which is why
they made it it's really powerful so I'm
just going to make a post request to
this web hook here and this is a body
that I've prepared and again in this
body you can have dynamic content you
could even insert the CSV file we've
just created as an attachment to this
notification I'm not going to do that
memo so that's that done fingers crossed
they'll work through validate
everything's fine so it's going to hit
debug yes that's it starting the room so
down here is our output window it's
gonna refresh maybe 20 seconds go only 5
rows in this database it's not gonna
take long but one of the things that
makes this really powerful is you pay
for what you use and it can scale up to
to work for whatever you need it to so
you can you can make it work on really
massive data sets if you need to which
is one of the reasons why this might be
a more suitable solution even if it's
just a one-off activity that you're
doing because this can do it probably
better than you can yeah I come online
all over
so as you can see it's got the water my
value it's actually create the CSV file
already and right now it's updating the
watermark and isn't it found slack its
refresh that so it's done most the the
slack notification step failed and
that's because when you notify slack it
doesn't return a JSON or something but
the activity itself wants a JSON object
responds I don't care if it fail because
I know that the suck notification got a
cent so if you're going to dine at a
chef's luckily you can see that you've
all got a notification telling you it's
been done and I was going to mount this
file share myself but chef album's Wi-Fi
was blocking me from doing that
annoyingly but if we go over here and go
to file share manually not her that's
the wrong file sure oh well little bit
blue
so look there is that's the CSV file
which I just explored and there is no
fue is in there so that's that doing so
we confirm they works we confirm that we
get in slight notifications so now what
we want might want to do for example is
create a trigger on it so I can say
every Monday and I'll say weekly Monday
hey a.m. so that's going to run every
Monday a.m. now last thing to do is
publish it and if we come into the done
that Shepherd dotnet chef project in
bsts and go on to doughnuts chef this is
the this is the data factory which was
created so under the hood data factory
is just a bunch of JSON files
it's just storing configuration and it
you know there's not much to look at
really it's just what we typed in that
slide done cool that's it for me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>