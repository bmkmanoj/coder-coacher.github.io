<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Binary Respository and Node Streams - LNUG - November 2017 | Coder Coacher - Coaching Coders</title><meta content="Binary Respository and Node Streams - LNUG - November 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Binary Respository and Node Streams - LNUG - November 2017</b></h2><h5 class="post__date">2018-03-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G6YZYevjYeY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I wanted to talk to you about a personal
project of mine I've been working on for
two or three months
it's called binary repository and node
streams which is what I implemented in I
wanted to back up my data I wanted to
store them in a you know backup as in
backup I wanted to store my backups in a
remote location they should be readable
only by me I thought this was you know
quite important because if they're going
to be in a remote location and the I
should be able to access them lots of
things gets leaked nowadays and hacked
so if I can keep on under control and
I'll be happy I'm also a line looks
loving hippie so line explore is a must
and open-source as well because there's
no point in trusting your data to a
commercial company for them to go bust
to be able to not get you data resource
usage should be limited and sensible i
cobbled together the solution before I
had really high CPU usage not one I
programmed so I wanted to keep that
under control and also I wanted
versioning in there so if I put rubbish
data over good data it wouldn't
immediately lose my good data to replace
by rubbish data or if I found out much
later on that I'd replaced my good data
then I want to get back this was one of
my inspirations but we've get you can
put anything in there it's really good
but if you was to put in like an mp3
file and it was ten Maggie wouldn't be
happy if he was put in a movie file and
it was a gig it would really really
really not be happy also it doesn't
encrypt things which is a bit of a
problem so those two requirements that I
had were failing to be met put together
a process of the design basically I knew
that if I looked at all the files and
check the modification times and the
sizes of all those files I could make a
good approximation of what I needed to
back up this is how a sink works if
you're familiar with
once I have a list of things I need to
back up I can do an sha-256 checksum on
those files to create an ID or part of
an ID I can then break those big files
which are potentially a gig or so
into multiple parts I can then encrypt
them using good old fashioned GPG so at
the moment it's probably works on Macs I
don't have a Mac as seen earlier my
pointing and clicking lack of practice
it's tested well on Linux I don't
believe it will work on Windows but who
uses that after that you can upload them
all to s3 or local those to implement it
or anywhere else it's all quite modular
it only backs at the data at that point
so you're collecting all these parts of
files which are identified by part
number in sha-256 them there's no file
names which would be a bit of a problem
if you wanted to bring back your data
there's no well as none of the other
things which you need to be able to
check again which is modification time
and size but the next stage is it
collects a load of data as much as it
can like 64 mega I think is what I said
it to for our part and 2056 make for a
commit once it's got a committee it will
then upload all the metadata again
encrypting it and uploading it to s3
local or wherever what does it look like
if you win it so this is a terminal
session how do i zoom in do I command
shift + that's good enough for now so
it's a terminal app because it ones
anywhere so if you had a if you wanted
to use it the first thing you need is an
s3 bucket so go off and gray one then
start team-ups because everything runs
in the tea mugs
as a first user you would create a
directory to put your data and then you
would initialize it you would then copy
a load of data into that directory and
then you can look at what you're about
to upload which is a blade runner
soundtrack you'd then upload all that
data so this is going up every every
file in there is getting checked against
the list of files which are not there at
the moment it's gained sha summed and
then uploaded into s3 at this point it's
not uploading the metadata the file
names and things like that it's just
uploading chunks of data if you look at
the data directory you can see there's a
whole bunch of things identified by sha
somes and part numbers and if you were
to want to get your data without having
to trust anybody else including me you
can look at a commit file finally sha
sum if I know what object inside the
book it you want to download pipe it
into GPG and there's your your data
which in this case is a track listing if
you do push all the metadata will be
uploaded along comes then you can see
the commit file has been added to the s3
bucket as well along comes another user
he can also create directory and
initialize it pointing it the same as
three bucket
oh yeah and then he will do a fetch
which will which will get the commit
file itself well at the moment he's
downloaded the commit file and it's now
currently downloading all the chunks of
the data itself
is actually downloading the data itself
now once it's got all the little bits of
encrypted data they will then go off and
decrypt the hole up for the one commit
it downloads per commits and then
decrypt we're going to change that file
because the diodes are listing which was
the tracklisting is a bit ugly so we're
going to make the changes we need to
make we're gonna head and save it and
then we're going to push that change
back up that's the change going up of
the day to itself then you're going to
push the commit back up the first user
comes along and then there's a fetch to
get the the changed commit file sees
that the track listing has been modified
and then downloads a track listing if we
then look at that
track listing we will see we have the
data that the other user changed so
that's a quick demo now I have to figure
out how to get back to the browser
anybody know how to get our full screen
on a Mac escape mmm
doesn't work
ha ha thank you
so how does that all work so the whole
process is a stream and it's a node
stream to be more specific in those
streams you have a readable you then
have zero or more transform or duplex
steps and you have a writable at the end
the code that you do to do that process
just looks like this pretty much so this
is on the outside looking in you have a
grain instance of your reader which
would generally be a subclass of streams
unreadable you then create a subclass of
transform or potentially duplex and then
you create a subclass of writable and
you just do dot pipe pipe and it flows
from the reader to the writer with not
having to mess around with any messy
code to manage that flow it just
automatically works for you but you have
to program it in the style which it says
so first thing you need to do is create
a reader space pad works better this is
what the reader might look like it's
probably about the most basic
implementation you can do so in this
situation all I'm doing is I'm
sub-classing streams not readable the
only thing you have to do there are
other things you can do but you really
have to implement the underscore read
method which receives the size which you
can just blindly ignore if you wish but
it's a recommended size which the system
is requesting to read if you ignore it
and just put the whole data in your
memory usage will go up but it might
still all work if you don't put too much
in there any case they need to call the
push method with data that you want to
push into the stream and when you're
done you call a push method with null
that's it nothing more
this is why transform method looks like
or transform class sorry again you only
have to really implement one thing the
underscore transform method has three
parameters chunk encoding and callback
chunk is the date that you want to
process encoding tells you the encoding
of the data if it's not an object
so the streams framework and node is all
for reading files and reading HTTP so if
you can do a lot of really low-level
stuff with this but if you're doing
object mode which is what I'm doing here
most of the time you can just ignore
encoding and there's a callback to all
you do is you take the chunk you
transform it and then call a callback
after you've finished so you call push
in the middle you don't have to call
push once you don't have to call push at
all you can call push multiple times if
you think about it you might be taking
compressed data and returning or feeding
through uncompressed data so you can
have a header you might not be calling
push at all for a while and then once
you've got the data you're going to be
calling push many times for every bit
your byte you actually import so there's
no requirement to have the same amount
pushes as transformed steps first of all
you have a writable the writable like
transform has junk encoding callback but
there's no push because there's nowhere
else to push it so you just call the
callback when you're done dealing with
the chunk that you have fed what I
haven't covered is duplex because duplex
is more complicated but duplexes are
only a combination of read and write in
one basically you write to what you
write to the saw site and you read from
the destination side so transform is
really a simple version and two Plex but
not powerful very easy mistakes to make
in streams because it's quite low level
it doesn't seem obvious when things
break
if you ever transform you must always
call the callback if you don't call a
callback the whole thing just stops
similarly if you don't attach her right
it will keep flowing for a bit and then
it will stop because you will notice
is not actually the data that you're
sending through isn't going anywhere
also this is one that got me because
it's pretty much undocumented in the API
and github will give you the answer
eventually but the API hints because you
just have pipe methods that you might be
able to take two things and pack them
into one I'll take one thing and pipe it
into do you can't do that because it's
waiting for nulls at the end of the
stream and if you do that you end up
just with one nor going to one place and
knowing you're going to the other place
and it's all mess the way to implement
that is to use duplexes and create more
classes with so you have basically a
writable on one side and then you might
have two readable on the other side of
one object it's a bit fiddly and it's
not so beautiful but it does work it's
often best to make yourself an
equivalent of map filter reduce in there
I created a little set of functions
called stream - that has those map
filter reduced functions in them I'm not
advocating people use it but I did put
it as a reusable piece of code for me to
use there are things like this just from
the video yes but I was using typescript
so I had to kind of make my own or like
definition files which is more trouble
than mine your own quite often this is
the project that I was implementing it's
called binary posit Orion it's on github
soaped and sauce could do with some Mac
testers because I clade I know I'm doing
with a Mac it does work on Linux
everything is implemented in JavaScript
except GPG which is probably not viable
to implement in JavaScript and requires
challeng out to the shell thank you for
listening
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>