<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scraping With NodeJS and a bit of Golang - LNUG - October 2017 | Coder Coacher - Coaching Coders</title><meta content="Scraping With NodeJS and a bit of Golang - LNUG - October 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Pusher/">Pusher</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scraping With NodeJS and a bit of Golang - LNUG - October 2017</b></h2><h5 class="post__date">2018-04-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DQRt1FpzmDo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today my first day a new company but
innovations Alex here CTO you want to
join us come talk with him but as I said
the product are going to talk about is
from completely different companies so
company last five years go flub it you
might hurt about them and you might not
so they used to do sales on demand which
he you would provide them URL from
Amazon and they would try to find you a
better price they were stuff happening
in the background that I will mention
that I took part of they transform down
the model to be full-fledged marketplace
and they are try to compete with Amazon
so what we really are going to talk
about about history of the project
because this project is like six years
old about how we separated the concerns
for the service that was already there
introduced the micro services split
everything to those free scrapers spider
and downloader what what we see as
potential pain points what we
acknowledged as a problems afterwards
and the architecture and the QA so long
time ago it all started as a PHP
crawling of Amazon mostly you can
imagine how bad it was and how few
requests they could do at the same time
but it worked and everybody was happy
and then they started having more
clients and then they arrived there so
through going through scrapey through
other products they created a scraper in
python and twisted it wasn't bad it
worked but as it worked business started
having different requirements for the
scraper which means like two different
departments wanting different data and
they'd have to branch and they end up
having four branches that were
unmaintainable if they wanted to do fast
changes if the HTML changed or anything
and the speed well it was fine in the
beginning but as some of the pages
described starting to filter stuff out
it turned out it was really hard to even
the back and see what's happening on the
network layer and to try to tackle
problems we spent with friends with one
of the colleagues that
I used to work with us two or three
weeks trying to even understand how the
how the headers were mixed up between
different requests so around two years
ago two and a half I became head of
internal services in flobbit and I
inherited two projects by scraper and
establish called TMS which was supposed
to basically manage the pool of proxies
so as we were already having problems
with scraper and the business has
growing requirements so at the time they
were doing 50,000 requests per day but
they wanted of course to do more and the
pie scraper wasn't getting them there so
we look at this and we knew that we had
the most problem the network layer and
we know we will have a long battle ahead
of us so we started by splitting the
network layer out and this was the first
time that we introduced in the whole
whole part of this project note so we
create service called downloader that
were still talking with the old service
giving us proxies but managing managing
with the connections managing retries
managing headers spoofing browser
rotating user agents doing all this kind
of neat stuff doing the we we got to the
point where we are actually sending bro
you were agents of actual browsers
trying to mimic behaviors of browsers
depending on the user agents we were we
were deflating content and stuff like
this so this became very neat and we
started introducing at this point
Jenkins pipeline this is stuff that I
use on different projects previously so
I started introducing this here as well
as a build tool so we did download there
everything was fine we understand what's
happening on the network layer right but
there was still this other part that was
in Python and it wasn't getting any
better in terms of performance because
we could do now how around 500,000
requests per per day but we couldn't
scale this up unless you would just by a
larger larger boxes so we took a look at
it and cut it in half separated concerns
that the content analytics went to
spider and the caching and crawling
future product intelligence other
sources of enhancement went to another
layer in note and with that we
completely managed to get of get rid of
button it was like the most successful
thing I managed to dis company to get
rid of that so what we actually arrived
at in the end this is gonna be they're
gonna be free graphs like this this is
the most complicated part of this talk
so you would see first we would read
from the business from our cue inputs
the job then we'll understand if we have
this in cash or not then we will forward
this to Spyder saying hey please give us
content for this spider would just
forward to download there now no there
will assign proxy to do the job
give yourself back just cascading up to
the client the database updates queue
for performance reason was completely
separated as a different sub process of
scraper and dusted right so the scraper
part we use typescript as I said we use
our mq as measure of accessing the
queues single scraper could use from
multiple queues or could use one queue
you would set up what's the prefetch
that you want to do the same codebase
would be shared with the update process
that will be separate thing but sharing
all the models or the other stuff with
the main codebase we small goes to to
manage this as I said we had by this we
created layer that gave us ability to in
future do different kinds of
integrations do different kinds of
scraping and I will get into this a bit
later on this is not part of the main
presentation but part of the stuff that
we did there and it's interesting we
exposed GC I know it's not not the
casual thing to do but we found out that
when we saved massive logs will ugly
connector to instant we kind of have to
do is to kind of keep our memory on some
decent levels otherwise we'll just go
out of the way just
lovely stuff so go back a promise well
we kind of structured this so that the
callbacks kinda would outline the flow
of application which means you could
easily browse and find where you what
you're doing at and we promise when we
would interact with models of stuff that
we wouldn't kind usually models but
kinds that we kind we don't mean much
more to interface or we don't actually
need this to understand the logic of the
application we try to keep the balance
between both of them and the same goes
with with functional and object-oriented
we try to actually have modules that
would be derived the object-oriented but
the way we would handle the data was
completely functional so we would focus
more on on having reliable metals that
you can test easily just putting putting
data in and such this as I said spite
the scraper does only to think so
checks if something is in cache does the
proper procedure and send stuff outs to
go like I won't be talking much about it
we choose this because well power of
parallel processing really we had as
imagine you have a single string large
content that we needed to do CSS XPath
selectors and then reg X on it it's it's
not good for notes at this point so we
we actually gave it to to go and it's
very go itself if you did if you if you
don't if you're doing typescript the
goal we will find it very like you will
find you're at home with some small
stuff closer and lasting so it's the
same note you're doing problem you're
doing I thinking notes properly you will
just kick off your go stuff but that's
not part of this presentation and then
you life to the to the third term is
that we cut this is the oldest one that
we started separating first so it's
actually consists consists of three
internal loops now this service should
be separated to actually to two
different and this is why those three
separate loops are there but they are
designed this way that they do not
create blockage for each others so as a
job comes in we moved back to it for a
second as a job comes and it goes to job
manager that kind of prepares the that
accident lock resource provider which
acts like an interface for extra service
in the future to give us any proxy and
then it will mitigate this with proxy
manager we'll get to this logic in a
second will mitigate proxy manager and
just give it back whenever it's ready to
the job manager so this was kind of the
- the - loop that the for free loops are
really there and whenever we have read
the job we would send this to worker and
note so the main process spawns
configural number of workers actually
does request and aggregations so we
would send we this was act as a gate and
as aggregator try to avoid blocking
callbacks because this needs to be fast
we started with the stream capability so
you would actually streaming parts of
stuff from workers to the master process
and master process was sending this back
to two clients but we started we started
doing G's if we started doing other
stuff so it kinda we had to aggregate it
there we did also for legacy reason both
detection on the side of service this is
purely because of legacy reason again
the GC exposed to just have control over
memory because of the way it's fraught
link from up to down the exposing of GC
and calling the forcing the the garbage
collector to kick in bit more often it's
not causing massive problem of
performance because the number of
requests the described on top is able to
handle concurrently it's already kind of
limiting the work that goes down so this
is not creating a massive issue and we
structure this so this could be one
process or this could be this could be
once one server running the downloader
or this could be multiple servers
running with each a proxy for example
and sticky sessions so what we
implemented in behind in the part of the
process is responsible for actually
managing proxy was a lock that would use
mainly Redis for this
this will allow this us to spawn
distributed processes on different
architectures and even if they would try
to use the same proxies for some reason
they would be blocked so it's like a
multi-level step that you will be
checking this if we can actually use the
resource pretty on the beginning because
we are we were actually doing this for
the for the PI scraper to work with it
and PI scraper on the beginning had two
sessions implemented which means you
could follow with the same user agent in
the same session the next request
implemented JWT tokens that would give
back actually the resource that you that
you use for scraping the proxy this use
for scraping which means if the sticky
session doesn't work if the node is down
whatever it goes if you pass the GWT
token you still have this resource you
can force the downloader to use this
resource and although they will still
check with Redis if it's not look and
when you were and it would kind of be
able to to move it around it of course
creates some race conditions but some
acceptable level and it was only the
like the case when the sticky sessions
completely often something doesn't work
and yeah there was the part that on our
site was possible for spoofing browsers
forwarding updates for forwarding for
doing updates for the compression like
whatever you wanted this is this is the
the the part of the system that would be
doing all this hard work so as I said
traffic comes to the loader goes to
charts by the child process and got the
content back go to spider this goes back
again so in this lock which was provided
proxy manager it's kind of interesting
structure that we so as I said Rock
which was provided was was only
responsible in interval of time to see
it was keeping the the number of
requests that came in for proxy and was
actually getting for proxy manager if it
failed to get a proxy was getting the
the release time of a proxy so it was
checking the the number of items that
were first in the queue was order hashed
map would you be checking all the top
proxies until it moved past the current
timestamp to receive proxy manager if
they were released of something change
with them and plus the situation when
you would have the proxy to use
previously that you passed well JWT
and using this one apart from whatever
proxy manager got from the system that
manages actually the the proxies so as I
said you will receive the proxies you
will see the proxy to go to tapenade and
then job manager would decide the
process that should one of the workers
this would actually do this it will just
be simple checking of how many how many
jobs are unaccounted for in worker so
you have three processes you have two
two and one it will pick the one that
has one you just set it there nothing
really complicated though on the worker
there will be extra settings like how
many conquered cross it actually can
handle if we catch those if we put on
the like interlock here those requests
in the worker to basically every aspect
of this downloader is configurable so
you can play it play with it on the on a
very different many different levels we
you would use as I said because we did
like four awards and stuff so we will do
connect the socket and a normal request
would allow us to to have bit more
flexibility on what we're doing on the
network layer if it fails we would
request a new proxies x times if it
fails x times x times give it a new
proxy all of those things kind they were
built in so that it this elements should
be as reliable as possible on the
network layer for the other parts of the
system to them don't worry about it
so what we learned as modules if the
content is invalid there is something
wrong with formatted you won't get a
actual response you'll get HP error and
so long another request you can of
course do the implementation in the net
library but that's that's the decision
that the node foundation did to not give
you the wrong response if you use HTTP
module so yeah
clearing interim hopes for the tests we
use this hard way our mocha stuff like
stopped exiting tests because of the
internal loops you would have to have
control over it when we write wrote our
test cascading down timeouts in terms of
we had to
you had to fine-tune the conflicts
between all the free services to don't
have like a random happening even that
were hard to diagnose we did part with
C++ atoms in the in the downloader so if
you can imagine we are aggregating the
payloads there gzipping stuff and then
we have also dislike 8000 items
proxy j-jason array with proxies then we
are moving from one stock to another
like cool down ready cooled and ready
this is not really memory option like
not really optimized for note in terms
of memory so we did have a play with
specialist ad on terms of memory why
spot on terms of processing time and
goes up we try to do is try to use the
chrome v8 like the the variable sits you
actually to have the same kind of
ability just using the internal v8 items
we kind of failed there though this is
kind of the part of the process then
probably future team should be looking
at when they want update stuff and it
was it was actually predictable if you
think about it that you have a 70 code
from v8 to standard string and then you
copy back makes all sense that it took
like more but the memory was spot-on and
as I said the other thing was that we
needed to really to fine-tune the
faulting from up to down we couldn't
just set random number and expect it to
work properly so we needed to have
proper number of workers for the proper
number of jobs that you would set down
we would diagnosis by looking at our
logs and what's happening and how the
whole system performs yeah
what would I hope we could dump from the
start because we kind of started this
from the bottom up we never really
bothered with like tracing the single
requests from the when it enters the
services till it ends which means by I
by like by whatever we are scraping we
would understand like this is the ID
that we're scraping on this page but
it's in 50 gigabytes of log that we
currently created there
it's just hard to spot what you're
actually doing more magic is affinity so
we know how long the single curl through
proxy takes to the domain we know how
much spider takes but there like a lot
of small places where those law there
were metrics like this would give us a
benefit to understand where we can
optimize separating concerts and
downloader this this part of the
rotation of the proxy that we did is a
is a perp like now that I think about it
it seems to me let's like a perfect
example of things that you should
separate to service right what we were
doing this is the first time we are
doing the service to work with another
one and this wasn't really like our goal
I just wanted this to be reliable to to
work with the problem with the previous
service as much as comparability that we
can have because there was only one part
in developer they're working on this
stuff so to try to try to mitigate this
pain and better integration with TMS
proxy intelligence this kind of goes
together with with the tracking of
whatever is happening so we we actually
did extra process that was kind of
socket IO spawned we would actually
start gathering stats but we save this
SQLite because the initial goal was
50,000 a day and then we did started
doing 50,000 an hour and has SQLite
locks that stopped stopped being at all
acceptable solution so completely switch
it off we did we did kinda like because
you got from time to time both
detections
and this service isn't handling them
this service is just saying we've been
bought the tected sorry error of course
it will retry and other stuff but if it
fails it fails as both detection so what
we did without having this process and
without having be a bit crude about it
we did analyze our own log to search for
those those about detection and push
this to the previous Python systems in
the format do they understand I think
yeah it's part that is connected with
that TMS is probably their I don't even
know why it's TMS to tell the truth
because
it's its system to manage proxies and
they call it tester management system
but this is nothing to do with TMS this
is just internal name that the companies
is actually using for it but yeah this
is probably the stuff that they need to
tackle on again the the R&amp;amp;D on C++ Adam
it was really promising what we did
didn't have time to spend on it more and
as I said the analytics for proxy users
this is probably the direction the
project should go at this point
so topologies it's at this point it's
like a lego really you can you can have
it UHA proxy or you can have this party
in the WS part on some VM it really
doesn't matter at this point what
matters is that you just do the config
properly and you can set anything
anywhere you can have you can have that
pot in Amazon this has all free services
or you can have free ports or you can
have service numbers on the desk go
spider that uses CPU more and the other
one puts on on the Box in your office
there is no limitations really to how
you can do a topology this port in the
network the the most limiting part would
be downloadable downloader can handle
distributed locks so whatever goes
really and whatever can reach rmq on the
input no limitations this is example of
a AWS topology that we did for floppy
direction not using this now but as you
can see you would contact you would be
talking with other services by rmq this
would come to to the sets of scrapers
how many would set in deployment then
set of spiders then it would be
connected to whichever downloader is
connected with the input queue in
scraper talk with Redis wherever Redis
is can be in WS can be something else
there's you can really you can really
play with this and I was saying the
scraper update as they're like cool it
uses the same docker image a scraper is
just actually calling a different file
from it with the same conflict
everything else is the same
so this example stuck for the for the
AWS
form of course on Rancher kubernetes
with docker everything built and managed
by the by the Jenkins pipelines the
first scraper using Mongo spider
download their radius and TMS you can
all spread this and basically we started
with one monolithic application VM so in
terms of metrics two core machine on we
they use currently Melbourne I think
they changed name to something else I
don't know what the new name of
Melbourne is maybe some of you know so
two core machine prefetch of 40 on
scraper around 80 jobs a minute which
gives you around 700,000 a day on the
eight core machine where we're doing two
times sixty gives you almost millions
and this is not the limit of this
machine you can still push more this is
just the number the business wanted they
wanted 430 million during doing a week
and this is what we accommodated them
with yeah as I said we started
struggling with 550 thousand a day so
that's kinda nice progression there the
cars that were gathering so the payloads
that we scraped roughly currently from
this 30 million a week stake seven
gigabytes in Mongo this is with around
two or three million pages that are 404
so you need to calculate this and every
day sends around 12 gigabytes looks
lovely
so because we do this like this and we
we kind of did it on purpose because we
had already plans and we kind of did
tests with other stuff the scraper is
now is now able to integrate it with
we've bought the descriptor and the
downloader is able now to integrate with
both networks so normally the TMS gives
you the list of proxies you can use
those proxies as you see fit but if you
give one of the processes both network
nobody says you know and as the proxies
has a cool-down from 2 to 5 minutes
randomly you can set this this BOTS
Network proxy to be 10 microseconds
which means you can in every cycle you
can you still use your proxies plus you
use this one and
still works fine we did test with this
and this is really cool thing they never
got it out the scraper would be actually
talking with a system that has the flood
Chrome extension installed and we did it
over socket IO with the Chrome extension
and you can actually ask the chroma
station to first of all to send whatever
the user is sync now so whatever they
viewing or do arbitrary other pages
which is probably less legal but it's
doable it's doable we just downloaded
with tor so you don't want to pay for
proxies you don't care about speed Hey
intro works
we tested also the downloader has also
extra kind of worker to send it to to
the chrome driver process that this we
did and this is part of the other stuff
that I used to do in flub it we did like
outer ordering from Amazon which means
you would send me a niacin and I would
log in check out check basket put an
address in that you need go to review
page the prints Kindle review page so
the troops of the review page to the
agent I did would say it's fine and I
would say I would just buy it so and
then we actually managed like in test
environments to pull this into
downloader then you would be feeding the
proxies again from the manager passing
this to actual browser and browsing not
with not with pseudo peril but with
actual browser there and this box that
is running currently this 13 in a week
this is the usage everything is taken
and if you think about it it makes sense
the first of all the the scraper that
that he connects with Mongo it does the
30 40 prefetch but this is the optimal
number of which its it doesn't compete
anything over CPU the next one is you
don't see this but you have your spider
so whatever would if you do enough jobs
whatever their resources are left on all
the CPUs that go will just fetch it will
just spawn as many go routines as it can
and allow to work
so again heavy stuff and downloader of
this stuff is probably the smallest if
it goes for the CPU it's it does only
when it does actually decompress and
send requests but like almost all of
this stuff is actually taken by by
putting stuff to Mongo and by analyzing
code and in spider
so almost nothing for the requests that
you're doing there are a lot of people
that work on this I have to find thank
you thank them now I mean there's only
one person I'm at from the bottom of the
list
it is currently here but they all kind
of put their effort in the whole project
and from the start of PHP version 2 to
the current proper micro service and
yeah thank you guys cool sorry if you're
interested in this I know they're hiring
that's the name of the new city own flub
it's you should contact him you can say
that you seem very nice dog</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>