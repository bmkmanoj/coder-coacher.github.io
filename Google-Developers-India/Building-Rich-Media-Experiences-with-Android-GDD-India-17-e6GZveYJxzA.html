<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Rich Media Experiences with Android (GDD India '17) | Coder Coacher - Coaching Coders</title><meta content="Building Rich Media Experiences with Android (GDD India '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Rich Media Experiences with Android (GDD India '17)</b></h2><h5 class="post__date">2017-12-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e6GZveYJxzA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody welcome I'm Paul saxman
and I lead the Android devices and media
developer relations team dan galpin is
gonna be joining us for the later half
of the talk the third part of the talk
Dan really doesn't need any introduction
besides just who he is to a room of
Android developers
so today we're going to talk about
building rich media experiences with
Android so what are these rich media
experiences that are talking about media
being audio video playback and recording
is used in almost all types of apps in
Android with only a few exceptions this
means that the Android media api's are
some of the most widely used by
developers and users outside of the core
framework so it's important to pay close
attention to the media experience while
building apps because an inconsistent
media experience can draw negative
attention to your app so we're gonna
talk break this talk into three parts
the first part is about media app
fundamentals I'm going to talk about
four very important media API is that
you need to know about I mean there's a
lot of important media API is but I'll
just going to focus on focus on four the
most important and then I'm going to
talk very briefly about bringing apps to
the TV because we have some update since
Google i/o and I want to talk about with
an Android TV in the last part which Dan
will present on is tapping into pro
audio and on Android so there's a new
API called OBO that we released not too
long ago and developer preview that we'd
like to developers to kind of try out
and get our get feedback on so four very
important media api's those four
important media api is our media session
audio focus picture-in-picture and wide
aspect ratio screens the last one not
exactly an API but it's something that
is very important for media apps to keep
in mind again these are only for the
most important API so obviously you know
media player whether it be the media
player or exoplayer also very important
but these these four are very
fundamental to building a really core
experience on Android so the first one
media session allows media controls
external to your app a media session is
very important to Android devices such
as TV and wear
as it allows media playback on the phone
to be controlled by these devices so if
the user starts media playback on their
phone they plug it into an Android auto
capable head unit those media controls
that they get on the head unit will
control the media session on the phone
the same is true with Android wear if
you have an active media session on your
phone you can actually control that
media session using your watch on
Android TV Android TV can actually
provide controls for media being played
in the background if you're playing
music it's also the media session is
fundamental to the Google assistant on
Android TV so so if the user wants to
speak play or basically transport
controls into the remote using the
assistant of those transport controls
are passed to the app using the media
session a media session is also used
across the Android system to provide
users with the ability to control
playback while an app isn't necessarily
in the foreground for example is used by
the lock screen interface so if there's
an active media session the lock screen
interface will show media controls to
allow the user to control that it's also
used by media style notifications these
are the notifications that that allow
control for media playback well it's in
the background media button input' and
so media buttons are buttons that you
typically see on Bluetooth headsets play
pause forward and backwards those media
button inputs are passed to your
application via the media session API
and also picture-in-picture uses the
media session so if your your your
application is shrunk down to a small
window size and picture-in-picture mode
if you have a media session the user can
actually get playback controls directly
on the picture and picture window so the
design for apps playing media in the
foreground is pretty simple
fundamentally they have an activity and
some player to add media session support
you first need to add a media session
you instantiate the media session this
tells the Android system that your app
is is or will be playing media you can
then add media session callbacks which
allow you to buy and transport controls
play pause seek and forward sent to the
media session to your player and finally
you need to add a MIDI controller which
is how you bind meaty controls in your
app
so if you have a media button in your
application are you buying that to the
media session so if the user presses
pause in your application use the media
controller to send that to the session
that session will update the media
session callback so you can actually
adjust the playback there but then all
these other external interfaces that are
using the media session will all be
updated at the same time since you use
the media controller so apps allow audio
playback in the background are quite
similar in their use of media session
however they have a service a service
for background music playback are you
there for you move the media controller
to your activity so you only really need
the media controller when you actually
have an active interface otherwise those
interfaces like the the media
notifications the lock screen or the
lock screen controls are all done
automatically using the the media
session API so setting up a media
session is pretty straightforward
basically you have to instantiate a
media session here I'm using
mediasessioncompat which is part of the
v4 media media compat library so
basically support it across most Android
devices
then you set flags about what you're
what you want your media session to
handle here where I'm sure the three
options are transport controls again
this play pause play pause stop seek as
well the media buttons so these media
buttons from Bluetooth headsets and also
cue commands cue commands or for adding
things to it like a playlist removing
things for a playlist or moving between
items in a playlist and then you
actually have two active or set your
media session of active and this tells
the system that this media session
should is available to be interfaced
with the other part that I mentioned is
that you have to listen for media
controller input in your media session
so basically you set the callbacks which
is a class that you have to create
yourself the media session callbacks are
very straightforward they basically have
all the media controls that you'd expect
all the transport controls the media
button controls and also the transport
control interfaces and basically once
your app receives these events you just
pass them on essentially to your media
player using the media controller is
likewise fairly straight for
you instantiate a media controller again
using media controller compat and then
you can use that media controller to
actually fire off fire media control
events or transport control cue or media
button events if the the media session
happens to be in another in another
process so let's say you have a
background service that is running the
media session but you want the media
controls in your activity you can
actually get a token from the media
session in your service pass that up to
your your activity and then use that
when you instantiate the media
controller media session is a fairly
complex API what I'm sure - you're
really just scratching the surface but
this is really the fundamental thing is
actually to add this playback control
into your application the other things
that is the porous media metadata I mean
you can push it information about the
media that's playing to the media
session that will show up on the lock
screen or show up in the notifications
or on your watch or your your auto head
unit syncing playback State meaty queues
this is playlist so you can add add
playlist to the media session and this
less other applications that are
providing interfaces into your app know
about that
meaning controller callbacks me to style
notifications a lot of different API is
built in there getting started with
media session basically understanding
what I told you today plus all these
other features there's a lot of things
you could do so developers at Android
comm we have a full section on building
media apps and this covers mostly mostly
the media session api's and related
api's we also published recently four
part articles one of my co-workers
actually Nazmul Idris a four part
articles an understanding media session
so this is really probably where you
want to start because this breaks down
the api's like I did here but in a
little bit more detail and also we have
several github samples the media
controller Media Browser service and the
universal media player these all show
various aspects of using the media
session api's as well so if you're
looking for code this is a great place
to start the second API that I want to
talk about is audio focus there's a
sharing control of audio output any app
can output audio at any time on Android
and output outputting audio needs to be
cooperative basically the system doesn't
necessarily mediate control of the
speaker and this is what the audio focus
API is for the audio focus API has been
around in Android probably since the
beginning but it was updated with
Android oh and I'll talk about the
differences here there are three steps
to ensuring a great audio experience the
first step is before you play audio ask
for audio focus and only play audio if
the audio focus is granted it's very
important to if audio focus has been
declined to actually not stop start
playback if somebody else asks for audio
focus deep with the focus loss so
basically be ready to hand off audio
focus from your app at any time and when
you're done with audio focus release it
this one tells all of their applications
that they can also now they can actually
take control and use the auto focus if
they need it
so requesting audio focus basically you
need to get the the audio manager from
the system this is partially new so the
audio attributes has been around since
API level 21 but this is the first time
with Android oh that the audio
attributes are used with audio focus so
the audio attributes tells basically
tells the system why you want to play
audio and what type of audio that you're
playing and this look gives the system
the ability to make decisions about
audio focus without necessarily kind of
kind of bothering the applications so
for example if you're playing music in
the background now you you press audio
focus and you're playing music in the
background if the system needs to play a
notification sound for example it will
automatically duck the music playback
and won't necessarily call into your
application and this is this API here is
actually new the audio focus request so
so basically when you actually request
audio focus you need now need one of
these auto focus request objects you
pass your audio attributes you you
create your audio focus change listener
this lets you respond to the auto focus
changes and then you actually request
the auto focus by passing that to the
audio manager grabbing this result is
very important because the next step
like I mentioned is you only play audio
if focus is grant
so this result can take three different
three different values it is either
failed the system did not want to give
you audio focus and this could because
the system is actually you know has
exclusive access to the audio at that
time you know an example of this is
might be a phone call if the users
actively in the phone call the audio
focus grant request could be granted and
at this point you can go ahead and play
your audio and the third state is the
audio focus request is delayed again
this is if the system is has exclusive
hold of audio and but you can get audio
gain later on and this is actually that
audio gain is passed through to the
audio focus change listener so the
audios focus change the listener again
is how you listen for other apps to
request audio focus or if you all you
focus is handed back to your application
you'll get the audio focus game through
the auto focus change listener and these
are the four states these have existed
for a prior to Android oh so so
basically you're either getting another
app is asking for giving you giving you
the audio focus back
you're losing audio focus for an
indeterminate amount of time which means
that you should probably clean up your
audio your media playback a media player
the auto focus loss could be transient
which basically means you play you pause
playback and then the last bit is that
the it can be transient but you can duck
which means you can you can basically
turn down the volume temporarily and
then the last part that I mentioned is
releasing auto focus is very important
because this is when you tell other apps
that you're basically done using it and
they can continue their audio playback
if necessary again a lot of great
resources for getting started with audio
focus we have a whole guide in there and
our developer Docs I'm managing auto
focus also very recently the same author
nazmal it interests wrote a three-part
article on understanding auto focus he
breaks down these different use cases I
mean he also talks about the new api's
as well and also we have several samples
both for testing and demonstrating how
the audio focus should work in your
applications
the third API I want to talk about is
picture-in-picture
picture picture allows your app to
actually continue media playback even
after your application has been paused
so you can see in the example here the
user is doing a video videochat when the
user actually goes back to the home
screen the video chat window shrinks
down stays on top of the home screen and
also stays on top of other apps this API
is really important for media apps
especially video apps because if the
user is watching a movie for example and
they want to go to the basically check
something in their calendar and they can
continue playback without actually or
they can check their calendar without
actually distributing the disturbing the
playback if there's three steps
essentially just to supporting pit
picture-in-picture all you have to
declare pip support and your activities
you have to switch to pip mode as needed
and you have to hide non-essential UI
while in pip mode the way this looks
declaring pip support you basically do
this in your manifest using the supports
picture-in-picture manifest attribute
another important thing to remember
about this is you should always if
you're supporting picture and pic mode
you should add this config changes line
because what this sells the system is
that basically don't reinitialize or
relaunch your app during it mode
transitions the next thing we want to do
is switch the PIP mode basically on user
demand this is the a button so let's say
you have a button you're in application
the user can press to put your
application in to pip mode to do that
basically implement your onclicklistener
on your button you get and called the
enter picture in picture mode picture in
picture mode method that's part of the
activity and that that message it takes
a params which is the picture in picture
params of a picture in picture picture
and picture params are useful because
you can actually set actions on top of
the the picture and picture window so
these are buttons that the user can
press and you can also set the aspect
ratio so for example if your screen is
in portrait mode but you want the
picture and picture window to be in
landscape mode you can set an aspect
ratio to define defined that it should
be an aspect or in landscape mode
another great thing you can do too we
highly recommend this is you can switch
a picture in picture mode
when the user hits the home button I
mean you do this by using on user leave
hint and basically you call the exact
same API here but once the user hits the
home button this will actually take your
app into pit mode again this is a very
nice user interface for our media
applications or movie applications when
the last part is hiding non-essential UI
you know the activities and fragments
now have on picture and picture mode
change as methods and these methods are
called when the application either goes
in or out of picture in picture mode and
before what I mentioned about hiding the
hiding the fullscreen UI is that's
basically hiding everything besides the
video of you or what you're using to
actually render the video and again you
can be essentially if you're using media
session your application if you have an
active media session you can hide all
the controls and there and what the
media session or what the Pitbull show
is media controls that will be passed
into your media session so another
really good reason to use media session
a lot of great resources on getting to
learn more about picture-in-picture
I definitely recommend so that the
picture-in-picture guide in the
developer the and our developer Doc's
have been updated it has all this
information in it we also release two
articles recently one of my co-workers
Benjamin Baxter making making magic
moments with picture-in-picture
basically describes the fundamentals of
using picture-in-picture and the
usability around it and he also did a
follow up on navigation patterns which
talks about using things like the home
button to switch the app into
picture-in-picture mode and the last one
I was going to mention the last API
again this isn't exactly an API but wide
aspect ratio screens so basically if you
don't do the right thing your
application is going to be stuck in
sixteen by nine mode even though that
the the device screen could be 18 by 9
well that means as your application will
only show up in the blue area on the
screen and that the red area will be
just rendered black essentially by the
applet or by the system and why the
reason this is very important for media
developers is if you're playing back
video and the user wants to view that
video in landscape on their full screen
and they see these bars on the sides
it's going to be a fairly negative user
experience 6
actually since these phones are kind of
these wide aspect ratio phones are kind
of premium devices so the way you want
to actually filling the white actual
aspect ratio screens or go beyond the
16.9 basically target API 26 or later
with 26 plus devices the max expects
ratio is the native aspect ratio of the
device another thing that you can do if
you can't yet target 26 plus if you
target 24 plus your apps by default or
your activities by default are resizable
resizable what's used by the system for
for a multi-screen and basically if an
activity is resizable it means it
supports any aspect ratio including
these widescreen aspect ratios if you're
not targeting 24 plus you're not
twittering 26 plus you can also go in
and make your app resizable and the last
thing you can do is you can actually
specify a maximum access aspect ratio
but this is the last resort what you
should really do is one two or three
above so making your app resizable again
this is only for 23 or under target API
because at 24 or above these are on by
default you can set these manifest
attributes either in your application
and it'll apply to all all activities or
you can specify it on the activities
themselves I'm specifying a max aspect
ratio again if you're targeting 26 plus
the mass that max expec ratio is the
native aspect ratio of the device but
for 25 or lower you can use this
metadata tag to specify that your app
should support these wider aspect ratios
on the value 2.1 it's basically you know
18 point 18 divided by 9 is going to be
2 so using 2.1 means that you'll use not
only to 18 point 18 by 9 but even wider
screens the problem with this I mean the
reason that we say you should make your
application or make your activities
resizable instead of using this though
is because you know what happens when
there's 2.2 aspect ratio screens you're
gonna have to go in and update your
application so ideally you'd actually
support this directly before I call
Danna onstage I'm gonna talk really
brief
about bringing some updates about
Android TV so at i/o this year we
announced a new Android TV home screen
and we also announced that the Google
assistant was coming to TV since then
most of both have launched the new
Android TV home screen is available on
the Nexus player and the Google
assistant is available on Android TV
devices since Android am I believe so
integrating Android TV for ways to
integrate on Android TV and the
assistant you can actually integrate
content in the watch next row you can
create channels and programs you can
implement global search or you can
implement media sessions global search
provides search to the assistant on
Google TV or Android TV and the media
session provides media controls via the
assistant I'm not going to go into the
technical details about all of these
specifically because we cover a lot of
those at Google i/o and the API hasn't
changed but the first way that you can
integrate is you on the new home screen
is the watch next row this row is
intended for views by showing content
the users in the middle of Ewing new
content say a new episode that was
released recently or next episode in a
series and developers can add content to
the next watch next row and apps can
also allow users to add content manually
so if the user is inside your
application browsing content they can
click on a button and ideally click on a
button inside of your application and
have that content added to the watch
next rip apps can also create some
programs which can be played on the home
screen a channel is a row of programs
that your app maintains applications get
one default channel Channel row so when
the application is installed and you
populate a channel that will actually
show up automatically on the user's home
screen applications can have multiple
channels though and it's up to you to
kind of determine how many channels you
want and how you can figure those the
first way to integrate with the
assistant on Android TV is make your
content searchable using a global search
provider so if the user is either
searching for content or wants to play
content so if they use the Google
assistant and say play Big Buck Bunny
what the assistant will
it'll use the global search providers
across all the applications and look for
that content if your your application
has an exact match it'll actually show
up at the top of the list if your
application has multiple matches it'll
actually show up below so so ideally you
can actually find find an exact match
for the user for these queries if
there's only one application has an
exact match and the user is requested to
play the content that that content will
actually be automatically played within
your app the user doesn't have to go
through the selection process and the
other thing that I mentioned too is
voice based media controls via the
assistant so if your application
supports media session and the user
invokes the Google assistant while your
app your content is playing they can
actually issue transport controls via
voice so play pause seek for word seek
back word so this is very important
another very important reason to
definitely consider using the media
session when you're building your media
applications so a lot of areas for
getting started with a new Android TV
home screen and the assistant on on
Android TV so we have several guides in
our developer content and we also for
Google i/o we built a code lab which
goes through all the important parts
about integrating with the channels and
programs and also recently we released a
Leanback sample on the shows you how to
use the assistant and tie that in with
the the Android TV Leanback API and with
that I'm going to hand off to Dan who's
going to talk about a new exciting Audio
API that we have
alright or not I don't have much time
and I'm losing my voice so please I
apologize for that but it's great to
have you all here
Pro Audio is something I'm very
passionate about so first of all you
know we did it we added a new Pro Audio
API to Android oh and you know we
already had opensl es but we wanted to
find do something that was easier to use
and easier to improve now all the audio
api is end up going through audio
flinger friendÃ­s framework but since
making huge changes there could
potentially break lots of code a audio
has a new service where we can do some
pretty radical things and get some
performance enhancements and actually if
you're running the developer preview on
a pixel or pixel 2 device you actually
can get really reduced latency thanks
for this framework so very very cool now
a audio uses the concept of streams in
this case flowing from the mic to the
app and then back to headphones and
these audio streams are really easy to
create we've made this API much more
straightforward using this builder
design pattern so basically you can set
this up and then make multiple streams
which is pretty slick in this code we
have the a audio create stream builder
which gets you a default stream which is
you know probably a stereo output stream
and here's how you request specific
values for direction and an for format
and once you set up a stream you call
open stream on the Builder pretty
straightforward and now if you don't set
things like sample rate in format you'll
then have to query them and finally
you'll get the frames per burst from the
audio stream to get the optimal read and
write size okay now let's stop for a
moment cuz I know your eyes are all
glazed over and you're like alright this
is only in the oh release how is this
really helpful to me but we've actually
released a library to help and this is
really what I'm excited about so this is
a C++ API first of all it's called oboe
and it works almost identical to a audio
and it works on both old and new
versions of Android so on old versions
of Android it's going to end up calling
into opensl es and this really makes it
easier to write
low latency audio applications on all
Android devices so here is what a audio
looks like and here is what oboe looks
like for the exact same code so as you
can see here once you've used one it is
really easy to use the other now in a
audio and oboe builder a buffer
reference refers to an array of these
kind of bursts of audio data and this
data is written in these bursts to the
DAC now when you look at this diagram
the app has generated two bursts of data
and the first bridge will be written now
as you might expect
you can start and pause or stop your
stream all with these think reduce calls
and honestly for most use cases that is
optimal but we do have a function for
synchronizing if needed and the real
question is how do we get to hit it in
and out of these streams so let's take a
very simple use case here for apps that
don't need super low latency you can use
blocking writes like this here in a loop
so basically the blocking write function
returns after either the number of
frames is written or an error happens
now the blocking write can timeout so we
might only get part of the frames
written so we need to make sure we're
handling this case and when you need
lower latency you can actually you need
a higher priority thread to do your
audio processing or perhaps even one
that's it's handled with scheduled with
sched fifo and so when you write this
callback function OBO will pass you the
stream parameter any data you need the
audio data buffer and the number of
frames and then you can render your
audio right into the audio buffer on the
high priority thread and you set this
callback up in the Builder so the
question is how do you combine multiple
streams you know such as if you're
making taking two or more input streams
and you know sending them to an output
and we recommend using once out put
stream as the master callback and and
then you do the callback from the master
stream now since the current oboe
implementation does not yet support
audio input I've switched back to the a
Audio API here and in a simple example
we're just passing data from input to
output we can can read the input from
the input stream with
and with a zero time out to do a
non-blocking read and that goes right
into the output buffer and it may take a
couple of calls for these two streams to
synchronize there we are that's what I
wanted alright so again so you read from
the input the output stream finally
let's talk about how you would do
dynamic latency tuning and this is
really important you're doing these very
low latency audio applications so I'm
looking back at that previous diagram we
have these two bursts ready to be
consumed by the DAC and we're glitching
because of preemption we may want to add
another puff burst to our render we can
detect overruns or under runs we've had
in our output stream like calling get X
run count and if it's changed since the
last time you checked you've got a
glitch so you can just add another burst
to your buffer size so once again here's
a basic Oh bro OBO program using a
blocking right very straightforward and
here's the equivalent code you would
need to do that in open s yes and this
is only a partial set of code so we
really have simplified what it takes to
do to do these kind of professional low
latency audio applications and that's
all you have for me so thank you so much
for coming and yet again I'm sorry my
voice is a little scratchy</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>