<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prabhakar's Keynote: Reinventing Productivity Using AI | Coder Coacher - Coaching Coders</title><meta content="Prabhakar's Keynote: Reinventing Productivity Using AI - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prabhakar's Keynote: Reinventing Productivity Using AI</b></h2><h5 class="post__date">2018-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MkMH-MATEU4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I want to introduce Prabhakar Agumon for
our next keynote talk the talk title is
reinventing productivity using AI
Prabhakar Raghavan is a VP of
engineering at Google Raghavan is the
co-author of X books randomized
algorithms and introduction to
information retrieval he's a member of
the National Academy of Engineering a
fellow of the ACM and I Triple E and was
link professor of CS at Stanford
University I have taken his in Rajeev
Motwani z-- class and marveled at the
problem sets in your book in 2009 he was
awarded a Lauria honoris causa from the
University of Bologna
from 2003 to 2009 Raghavan was the
editor in chief of journal of the ACM he
holds a PhD from UC Berkeley in
electrical engineering and computer
science and a Bachelor of Technology
from IIT Chennai
prior to joining Google he worked at IBM
Verity and Yahoo thanks Prabhakar for
making it in person great thank you
listening to that you might be mistaken
for thinking I'm still a scientist I'm
not I've defected to the other side my
day job is responsibility for Google's
productivity applications including
Gmail Google Docs and several others and
what I'll talk about today is some
thoughts on using AI to enhance this
nebulous notion of productivity alright
so see here's what I'll cover today
there'll be four themes in this talk and
quickly blast through the first three
and dwell a little bit on on the fourth
of these themes which is a case study
involving a product we actually built
okay and one that hopefully many of you
use all right let's begin with the first
John McCarthy won the Turing Award in
the early 70s and in 1987 he wrote a
great CSM article which are
all to read where he says this is what I
would have liked my Turing Award lecture
to be except I didn't have time to write
it okay but 16 years later everything he
meant to say then is still true right
and here's a quote from that where he
says unfortunately for our science the
problem of generality in AI is almost as
unsolved as ever although we now have
many new ideas okay and this was written
1987 and those thirty years ago okay
the first point I want to begin with is
30 years later we could still say
exactly the same thing the late McCarthy
is no longer around to see it but 30
years later we still don't have
generality niak the fact that you can
build a machine to beat the world go
champion doesn't really tell you how to
build a machine that can take a passage
of text and tell if it's a joke in
that'll really appeal to humans right
and so this secondly thinking about you
know how should we think about this
discipline right and to me AI is as much
an engineering discipline as a science
right and I know half of you will say
sure I get that and the other half will
be outraged saying I'm doing all this
hard science and you don't appreciate it
because you cannot divorce the science
from the engineering but in fact this
this is very true we've got a growing
arsenal of tubes Jeff talked about PPU
order ml and these are all things that
you can put together but this is no
longer the kind of state that say
software engineering is right and even
software engineering is not at the point
where it's systematically doable this
hard problems we have problem solving so
you have to start almost from scratch on
every instance a bug come on in I'm
going to talk about your work with your
slides in a few minutes okay
good so what are these what does this
mean so you get these things called
annoying things called engineering
constraints right which is the gap
between science and shipping artifacts
right and and I'll actually highlight
this for the second half of my talk
great now here's a somewhat
controversial statement right I claimed
that computation is really not a limit
okay what I mean by that is the most
challenging air problems don't linger in
a state where computation is not enough
Moore's law catches up you get new tools
like you know all the ones that just
talked about all right
much less slowing down I know it's no
longer exponential but it's still
growing fast enough that we're okay
Moore's law could not continue forever
not only for reasons of physics but the
consequences suggests that the
semiconductor industry will overtake the
world's GDP
if Moore's law continues and that's
clearly not going to happen right all
right so so I can again see a lot of
academics getting upset at this saying
it's all very well for you at Google to
say you know computation is not a limit
but for us it's a real limit right but
let me put it in a slightly different
way right so Jeff talked about the
remarkable advances we've seen with deep
neural nets in the last few years but if
you think about it that's happenstance
we stumbled on this thing we don't know
that there is anything fundamental to
deep neural nets that makes them
universal in any sense right it's very
possible
there are predicates that are learn
about in the pack sense they're around a
burden but deep neural Nets cannot learn
them right so there's nothing sort of
fundamentally you know divine or deep
neural Nets so it may well be that we
still in a state of looking at the wrong
representation okay and and so that's
what I mean you're trying to find enough
steel to build a bridge but in fact you
should be building a ferry perhaps I
could so now that has probably managed
to upset
everyone I've been thinking of
productive work and and the things that
we do in the name of productivity right
and I have on this slide laid out some
of the things that we end up having to
do right so this productive work ranges
from the mundane to the sublime right
deciding of an email is spam that's
pretty mundane a human can do it easily
but soak in a machine right so all the
email providers give you that as a
primitive and have been for many many
years they do an excellent job of it
right slightly harder is filing a
document deciding where a document out
of several folders should go that's
still a multi-way classification problem
you say well I can you know use machines
to go pretty good job of that right what
about writing a short email okay what
does it take for a machine to write a
short email and on your behalf okay
that's a bit harder you would admit
right you associate some germ of
creativity with it right prepare the
materials for a meeting you're going
into a meeting call together everything
you need for that meeting the documents
the material facts what is the argument
over right the state of the art today is
you can probably have a pretty good
guess of related documents but that's
not the same as preparing for a meeting
right and then all the way at the
sublime and writing a great symphony it
is I think you'll agree with me it's
still in the realm of deeply creative
work that we don't associate with
machines okay
so where are we in terms of the state of
the art right if somewhere here we're at
the point where machines can write small
small emails for you and I'll talk about
the a project we built around this but
there's still a lot of dot dot dots up
here and one of the ways I think about
what is progress mean is moving up on
what it takes to be truly creative truly
beyond the reach of machines in this
spectrum this is just one dimension but
in the realm of productivity
I'm arguing that progress with AI is to
keep pushing up this boundary and
conquering more and more of the
territory up there right so so let's
think about some of the things that
might be up there before you become beta
for right think of the job of a museum
curator he or she has to take a picture
and decide if this is a genuine Picasso
and you think about that and say well
maybe that's barely recognizable okay
here's an artifact the museum is about
the choir how much should we evaluate
that those are judgments and and you
look at that and say maybe we can
inversion that machines can get good at
tasks like that right so you're moving
up this task those of you that are
familiar with the workmen Gant's how
many people here have heard of the
answer for your view right let's start
to recognize that once you can start to
do a curators job we can throw against
it and maybe start to develop beautiful
pictures right that are truly artistic
okay so you start to reach up there so I
would start to put the job of a curator
higher up in the stack it's a fairly
specialized task right but the the
general setting I want you to think
about is productive work really ranges
from very trivial stuff to really hard
stuff and our job as the community is to
keep pushing up this boundary okay and
now I'm going to spend the rest of my
time on a case study in in this area
oops and the case study is a product we
built it's multiplying gmail and Berlin
is somewhere in this room I saw him come
in there he's at the back wall it was
the lead engineer who led this work in
fact I've stolen many of his slides and
he find them very familiar so all right
so so what's the problem we were trying
to solve here right the thesis was that
a lot of the time you get an email and a
machine should be able to generate
suggest a short response right and you
can if you dwell on this for a while and
many of us who were in
all in the project and thought hard
about it I part of the reason is not all
of the email you write is truly creative
a lot of participative predictable and
if it's predictable we should be
predicting right so there is a large and
some of you can even identify this
behavior in the course of your day time
there are many messages you can just tap
and respond because it's got a very
short response saying yeah sure I'm fine
with it yes I'll meet you tomorrow
whatever right and then there are some
that you have to think really hard about
because you'll offend a person by saying
something pithy and so you want to wait
until late at night right a very
carefully constructed essay and so on so
this work it's not aimed at those
carefully constructed essays it's really
at the the short response that many
emails deserve and get right so the the
problem statement is given an email
suggests some number of responses that a
machine generated okay okay so of course
email data is private all the data that
we try to touch in our field of
productivity is private and you don't
get to look at into individual emails
right so you look at aggregate political
relations and then you try to
reverse-engineer you know response fares
to incoming emails yeah now you have to
do a ton of pre-processing those of you
work with language familiar with all of
this nothing here is surprising to you
throw away you know so sanitation's
sign-offs and so on frighten and get to
the message body so here's one of the
first examples that ball intent is
teamed up with in in zurich right so the
incoming email said I have a call that
fever and I'm coughing so it's playing
at home right and the short response it
got was hope you feel better soon okay
perfectly reasonable seems human
generated seems to have feeling actually
it was written by machine right so how
does this work and again essentially you
look at you know France in the incoming
email okay so it has grams like a cold a
cold hard
hold up fever at fever and and you want
to spit out something like fever too
soon right
once you see this level of formulation
you really are trying to find this kind
of incoming email response engrams okay
and what you should be thinking about is
LSD and if you're familiar with LSD em
if not just think of it as anything that
that does machine translation that takes
in a string coming in spits out a string
going on right of scores
strings that can be sent out right and
that's exactly what we did and there was
actually paper in this by B Nielsen lay
in 2015 that describes right there is
also ball in this colleagues have at
least one maybe two papers now that are
pretty detailed two papers on how this
multiplied system was built the last one
was kdd last year I think so so go check
it out right good so that seems pretty
straightforward except actually building
a product from this it's hard and this
is where the point the fourth team that
I wanted to touch on that the user
experience actually matters a great deal
is something that I want to stress for
the remainder of the talk right so a
problem seems like incomes and email
outcome phrases and you want to pick the
best suggestions okay so the first thing
is you ask is it even worth it right can
you get enough precision and accuracy
that this is actually worthwhile right
can you actually guess exactly what the
user meant to write and the answer is
you don't actually have to write if you
are close enough linguistically they'll
say fine this isn't exactly what I would
have written but it's close enough that
I'll take it yeah and that's something
we that turns into something of a
self-reinforcing behavior because as
users start to use the system they
reinforce some of the hashed
linguistically hashed versions of
statements right so I'll see you shortly
see you soon these are all variants of
the same thing and the machine starts to
predict one of them and people tap it
then that starts to get reinforced in in
what they see what gets generated later
right now the most emails have
simple predictable responses right and
turned out from an analysis that enough
did that the answer is yes there are in
fact emails where Gmail smart reply will
not predict anything because simply
because we say we throw up our hands and
say we don't know how to do this okay
and that was one of the early offline
experiments that the team did okay good
how many replies to be suggest because
you're scoring the place and for a
variety of reasons including the UX
right we said three we don't want to
make the user pick between 30 responses
and hope that one of them is good so
smart reply and Gmail actually picks
three responses they fit nicely on the
screen we initially push it out in
mobile devices so we really gave us a
budget of three short responses okay
good now one of the things we found in
early tests is you put this out and
people are afraid of using it and this
is where the user experience starts
coming why were they afraid of using
because they were afraid that you touch
it and the reply goes up instantaneously
and it you wanted to change this one
character in there okay and so we had to
train people into understanding that a
tap response doesn't go away by the way
it's still an editor mode you can go and
then tweak it and so on and over time
people have gotten pretty good at saying
well this isn't exactly what I would
type but I picked this and then I'll
tweak it and and people actually see
people do that love okay this let me
just finish one more thing things right
this is one point I want to make here
because a lot of us here doing the
scientific work right tweaks in the user
interface actually sometimes gave us
bigger gains in users acceptance than
some model improvement okay so you could
get a model improvement that improve the
quality of your responses by five
percent perhaps but just saying you know
what don't worry this isn't going to go
away without your further editing
increase the propensity for users to use
by much larger amount right so user
interface tricks actually end up
mattering a lot okay and this is not a
PowerPoint that's easy to appreciate and
they'll try to make it again
okay here are some early observations
right so here's one of the early
experiments that ball Indus team showed
me essentially you generate the three
top scoring responses and one has an
exclamation one has a period and one has
nothing and you're like okay this is
kind of useless because it's saying the
same thing three times yeah so fine you
can get rid of you know this and so what
the team built was a module to generate
linguistic variants so they call this a
diversity problem so once you have a
response you throw away anything that
looks like it and try to generate the
negation for instance a bit so if the
top responses works for me then you say
it doesn't work for me yes and so
there's an explicit module the distance
yeah sometimes the the training corpus
would give you perfectly
you know well scoring responses that are
not what you would like the computer to
suggest to you right when is the report
you I have no clue okay and when a
machine says that people start to not be
too happy I don't know what you're
talking about
right so we have to watch out for some
of these things the this way I'd
summarize this set of things in the team
went through many weeks of experiment in
this is you cannot just take the three
highest-scoring suggestions but then we
had this problem that we call the I
Lobby problem in the training corpus
there was a preponderance of responses
that say somewhere I love you okay and
so a lot of the times you have here we
were at Google trying this out and the
response suggested was I love you and
you get your email from your colleague
and it says so so we had to strip this
out and more generally build a notion of
family friendliness right because the
training corpus can have all sorts of
junk in it and you don't necessarily
want to have all of that show through in
your product right because simply
because it isn't appropriate
there's also sensitivity we had
situations where somebody would send an
email saying I'm really sorry to say
that a friend Joe has passed away and in
those cases it's safest to just back off
completely and not say anything
stop proposing a response that might be
awesome right and we've actually done
that right and gotten people very upset
right so there are many incoming emails
where you want the sensitivity to step
up and not guest responses now here's
some meta comment I'd like to make right
there are things that humans will
actually type but you will not accept as
a suggestion from machines in general
our expectations of working AI seemed to
be higher than our expectations of our
human friends right we say humans are
fallible there whatever right but
machines aren't allowed to make the same
mistakes right because I mean my
nightmare was we would launch this
project a product and the newspapers
would be full of Google AI does this
really bad thing right and somehow
googly eye is held to a higher standard
than any human any one of seven billion
humans on this planet right but that's
the the way we have to condition
ourselves to think about this okay good
Thanks okay so we launched and we were
actually surprised by the the fairly
ecstatic response you know this is a
bunch of Twitter samples but there was
one journalist I think I forget for
which public a business week or whatever
it turned it to bloomberg businessweek
who who wrote this beautiful article on
how he spent an entire week responding
to all emails using smart reply only in
some cases totally mystifying his
correspondence but he thought it was it
was a great lark and so a whole week of
smarter place in fact the numbers that
we see are nothing like 100% usage but
over 10% of Gmail responses today as
selected by humans are smart apply so
think about the metric here it's not
just did the machine make a good
prediction we use a prediction that was
good and it was accepted by the human
right so that's a fairly high bar right
so going back to my where are we in the
spectrum of addressing creative human
work right we have a little bit of a
measure here that says for some swath of
work humans don't need to get creative
machines can do it and
users actually accept these right since
the original launched in English we've
now launched it in several other
languages most recently in French
there's a point here I want to make you
in a moment
right okay so so let me get to the point
of my final slide but before I do that
right so to frame what I've been saying
right raising human productivity is
actually a huge and rich area for
machine intelligence and often it's the
limitation of my imagination right you
have to ask yourself can you use
machines to write emails before you're
going to solve the problem right and
computation is seldom the real
bottleneck right if we do this the dream
the utopian dream is that you can
actually free up more time to focus on
really creative work there's lots of
studies by firms like McKenzie that say
things like about 5% of our time spent
on creative work and the rest in mundane
stuff right so hopefully we can expand
that 5% so that what we do day is get to
be much more creative right so the user
experience in the user interfaces is a
huge factor and really well products
this is something you get to really
understand right you have to set the
right expectations for users where they
still feel in control that machine
learning is there to help them
but not to take you know replace and
completely and write emails on there we
have autonomously right now we don't
actually have good methodologies for
principled analysis of these UI choices
right
and yet these are some of the biggest
gains that you often get Thanks
and conversely a great algorithm can be
killed by about UI separate point I want
to make transferring across languages is
hard work right so so you might have
wondered well once you did in English
why don't you do it in AD other
languages natural language processing in
general has this problem that just
because you solve the problem in English
doesn't mean it works right so family
friendliness
maybe you solve it in English but it
doesn't mean you solved in French in
Portuguese in Italian all of this other
stuff right
there's a ton of other important
considerations that need more study like
the safe aggregation of training data
with if anything went went over
backwards in this case and maybe we can
get better results if we could do safe
aggregation but because it's private
user data we got to be super careful
what happens what's the effect of
personalized training models this one
other point I will finish with here
right which is you still want debug
ability when you get that bad report and
for us bad reports usually happens
because somebody gets really upset
they'll email some of the CEO of Google
and Sam I really hate you and next thing
it's in my inbox and deal with it so
right and then I call up Baal or whoever
and say hey deal with it and if it's not
debuggable right it's super hard and one
of the challenges is if you say well I'm
going to advocate everything in your net
and life is happy is you often lose some
of the debug abilities so you've got to
be super careful alright and they stop
with that I'm happy to take a couple of
questions so thanks prop occur again
great talk
so when you talked about again the
desire to have greater generality right
in terms of how we apply AI isn't there
another serious problem that we still
need to solve for even very specific
problems about robustness that even
though we get very good results on
whatever test data and so on often these
solutions even things where and of there
are headlines saying we have achieved
human parity or beaten humans and so on
right you change a few things about
speech recognition and machine
translation and so on right and things
kind of go really south so isn't there a
danger I mean we have already seen
things like polarization fake news and
so on that that if you don't solve these
problems and start making things more
kind of general and unleash all these
general AI solutions that you could just
kind of completely mess up so so I'll
take a point
and I wish I could say there's a silver
bullet in the works on many of these
issues especially you you mentioned fake
news I don't know that we figured out
the right societal objective function
right because this you know think of
this as a multiplayer game and everybody
has the rational self-interest and the
question is what is it that you the code
encode System Operator should optimize
for right in particular are you the
right person to make the call of what
societal happiness should be right and
so we have difficult ethical challenges
here that I don't know how to answer do
you see a lot of sort of kind of role of
customizability so all these a I chose
they're like sort of yeah I mean there
is one aspect of transparency which is
the glass box model but also how much
control would you like to have as a user
of the AI to give enough control to the
humans there are some senses of control
in other applications apart from email
which is self everybody understands that
but in the enterprise domain that you
understand can you give us examples
where AI tools can be better customized
or controlled by humans that's a very
leading question and it needs a long
answer so let me try to be brief right
so we absolutely need the custom and I
don't think it's just enterprise in the
notion of you know big companies and
paying right one of the domains that we
struggle with a lot of work with a lot
sometimes struggles is schools and
students right so one thing we do is
actually move a lot of this technology
right and the we're learning they're
constantly a student I mean new
generation actually have completely
different expectations of these
technologies and so the level of
customization you need they're typically
blows us away completely right and the
the thing the only leading indicator I
get is looking at behaviors and I will
give you two examples of student
behavior
that caught us completely by surprise
right lots of students that we've given
our tools to are happy writing their
essays on their phones and then they
switch to the laptop to do the
formatting okay and that's a behavior so
how would you help these students write
how do you use AI to healthy students do
their composition so that they're much
more facility right
another student behavior we've started
seeing is students are expecting that
teachers ask them questions and they
record short videos with the answers
they don't want to type how do you help
students do these things right so the
customization that it needs in these
cases is very different from what you
might think of as needed in a company
you know for email and long-form text
and so on right one of the other things
we're experimenting a lot right now is
short form text like chat text and so on
how do you belong with models for those
that actually are robust and transfer
across languages in stuff like that
right so each of these is an instance of
new behavior that's driving the need for
customization so basically I have a bit
of tangential question so you talked
about the research part of it and taking
it to the product right so definitely is
in a lot of things both good in research
while we are experimenting with the
training data sets and everything but
when it exposed to the actual users in
the productive environment you would see
a lot of different results and so
basically what is the right time about
what are the metrics that you use to
decide that it's a ready to go to the
users and if you see challenges in when
it reaches the users for example you
showed few iterations which went through
for the email so how do you decide on
nitrating on those and decide on the
metrics first of all to say that it's
kit that's a great question and one in
the case of smart reply remember ball
and I would have these long elaborate
discussions almost on a weekly basis and
set ourselves what does it take to prove
to ourselves we can go to that next step
which may only be a week or two right
and if you're working in a company some
which many of you do one of the things
you'd be prepared to do is when you fail
those metrics
you should actually be prepared to kill
the project right which is very hard
because you put months and months of
your investment into it right and my
emotional investment and you want to be
able to say you know what it didn't work
out right so for instance as we went
through that project the first metric
was what fraction of emails get
responses what fraction of emails get
short responses right what fraction of
emails get short predictable responses
right and these were analyses we could
do offline right once you do that you
say okay how often can you actually do
the prediction what kind of accuracies
can you get right what fraction of the
time do the predictions predicted
responses seem palatable to human users
right and those are metrics you can
actually measure but very often you're
like I don't believe it you know we we
did a great job but these humans aren't
accepting it right these ordinary uses
and right and you have to live with that
and it has to do with tweaks and you
know like the color of the font and all
of the stuff that it's been well studied
but if you don't put yourself in a
metric diet you're not going to really
have a good lunch right so it's a great
question a very long topic and I
actually think that in the years to come
there'll be PhD theses written on this
tough question thanks for sharing your
here thanks for sharing your experience
with the smart reply system as an NLP
researcher I saw that you will start
with the sequence to sequence model
which is a model that you will probably
get from nacl paper or something like
that but then you have to add a lot of
bells and whistles to make sure that is
actually palatable and useful to humans
including diverse answers making sure
not answering or the moving family
friendliness and so on so forth as an
event research and my question is are
these constraints that you add which are
hard constraints are these symbolic
algorithms that you create or are these
tweaks to the original noodle model
itself of what fraction of the work that
gets on from this search to engineering
is sort of somewhat heuristic in hockey
or is somewhat principled no that's a
great question and and well feel free to
come forward and answer but the answer I
give you is exactly a mixture of all of
them sometimes you have to do a bit of
each you have to do hacky things on top
right because in the end our master is
not some sort of scientific standard
it's happy users right when you have a
billion plus users to serve on Gmail
you know even 500 of them being angry
can result in a Twitter storm and you
know by the time 10% of using damage
using it so over 100 million that's a
good sign you've done something right
but I can say along the way we made so
many choices we could have gone wrong do
you want to add to that
okay I just want specific examples for
the diversity modulus my reply we
started first with oh it's a great kind
of clustering problem and all of that
and we tried working on it at some point
the engineer threw up his hand and said
okay I'm going to try now try to do it
more like ad hoc manual type of written
a to that that allows us just to hack
the thing then after that we turned
training it into like a graph based
machine learning problem and and in the
end now we're in a very principled
position but you went in a whole
rollercoaster of like very pretty hacky
halfway principled and now we feel like
oh wow this is nice solution so it's a
roller coaster that you just need to
need to go through hi my name is Shirley
I'm from American Express so it's very
interesting to curve that you called out
the signs and two aspects of these in
this in this community one is science
the other is like product / engineering
right well traditionally this been like
quite distinct community do you see that
these two like coming really close I
mean scientists are developing more
product development mindset engineers
are learning more data science partially
learning it's like that to kind of see
that these are merging okay again a
leading question I think you know
without resorting to some of the popular
buzz words like data science and so on
it's it seems clear to me that the
generation of companies that's emerging
now and the things they do a really a
blend of user insight engineering and
deep science and I don't think it makes
sense to compartmentalize them and say
hey your job is the engineering - the
science it just hasn't worked that well
the times when you smush it together and
you know there are many people like Paul
and his team who have PhDs and who
publish papers and machine learning part
what they do for a day job is write
software and that's kind of like what a
lot of us here at Google do
we have PhDs but we end up in the
engineering realm and that's the only
way it really works sorry last question
yeah hey pravakar thank you for the
fascinating talk so you spoke about
smart replies and specific to smart
replies we are talking about a user
won't think you use smart replies to
save have you seen the other way around
that the other user starts because
initially the smart replies would be
what generally humans reply then then
the other side will start predicting
that probably this is this is a smart
reply yeah like for example LinkedIn if
somebody sends me a congratulations of
the new job I know it's a it's a it's a
tap of a button and automatically the
value of that message goes down totally
so that's that's a great question right
so in fact one of the internal games so
you have in you know the Gmail and other
teams is somebody sent you a reply and
say wait a minute that was a really
smart reply you didn't write that did
you write one thing that we actually
considered and decided against early on
in the design of the system was when you
pick a smart reply there's a little
thing that if there exists generated by
smart reply right and for precisely the
reason you said we chose not to do it we
didn't want people to feel like they
were dehumanized so so we we we didn't
do that now the other thing is a lot of
the times forget the exact numbers
people actually edit the smart replies
so the one way to think about it is in
Sofia QWERTY keyboard you have these
three big keys each of which is is a
starter sentence and then you finish it
as you wish with your party keyboard
right and that starts to leave the rich
explorations and how the user interface
could develop on this alright so stay
tuned for more on this thanks very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>