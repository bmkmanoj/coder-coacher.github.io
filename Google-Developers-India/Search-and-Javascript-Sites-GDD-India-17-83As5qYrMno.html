<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Search and Javascript Sites (GDD India '17) | Coder Coacher - Coaching Coders</title><meta content="Search and Javascript Sites (GDD India '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Search and Javascript Sites (GDD India '17)</b></h2><h5 class="post__date">2017-12-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/83As5qYrMno" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Amrit
I'm gonna advocate here at Google and
today we're gonna talk about search and
JavaScript sites and this is one common
question that we get all the time when
we talk to web developers how does
Google search index a web application
now a easy way of understanding this
would be to understand how Google's
search sees the you sees your website
and then it'll be easy for you to kind
of test and validate it now like I said
I'm a developer advocate I'm no angular
or polymer developer and this is gonna
be a session about Google search and not
about any particular web framework that
out of the way let me give you a quick
overview of what I'm going to talk about
today I'm gonna talk about what search
really looks for then a little bit about
rendering techniques and how do you want
to do when you migrating you are else
what are the things that you should take
care of so it still sees it correctly
and then some we'll discuss a little bit
about some of the useful tools that are
available and this is some terminologies
I'm going to use throughout first one
being Googlebot that's basically the
software that's crawling the web getting
the data rendering it and indexing it
secondly is a single page app but a
single page app what I mean is where you
send in the data is updated to a
client-side JavaScript and your
client-side JavaScript just pulls in and
dynamically updates your app with an app
shell this is basically the JavaScript
HTML and CSS combination that is
throughout common throughout your web
application which the serviceworker kind
of aggressively caches and rendering
this is very slightly different it's not
the DOM and visualization of the Dom
that I'm going to talk about I'm going
to talk about the HTML that is getting
read rendered okay
let's quickly look at what search really
looks for a naturally first if you want
to simplify how search works so it's
basically a collection of URLs in our
database we use a schedule
and the crawler would based on a
schedule would go to these URLs fetch
the data render it index it and positive
such that's what international search
works now what a search loop really want
it is unique indexable urls URLs that
are crawlable have indexable content and
links so that you have the relationship
between documents in there now if you
look at the urls for instance this must
be information that you already know a
good URL has protocol host part file
name and query string if required it is
one URL per piece of content so it does
not it is not gonna duplicate four
different content and what you are also
given here the perfectly good URLs but
the easier way of understanding this is
to kind of maybe look at what a bad URL
looks like so if you have a same you are
for multiple states that's not really
good again URL so the fragment
identifier again not a good good good
thing and I'll talk about it a little
more in the upcoming slides there are
also URLs which might have a level
elements like session IDs or private
information like user ID and password
they're also not considered to be good
now canonical URL this again when you
have multiple versions of the same page
you want to use canonical URL and say
clearly say the one URL that you need
has a representative which the index
would actually keep you want to ensure
that your redirect Java scripts your
sitemaps and links are all updated
correctly to reflect this canonical URL
and you want to be consistent about it
you don't want you don't want a
different one to be there in your index
or sitemap that doesn't that is
mandatory that you have them common
across and we talked about fragment
identifiers the interesting thing to
understand here is everything after the
hash in the URL is actually ignored so
you want to kind of use the history API
and update your browser URL to something
that is actually understandable and
indexable
and this is something that is like if
this is something that is mandatory you
also technically hash not URLs
render but they are not recommended so
in that case also if you're using them
please use a history API and update that
now this is a sample of code that sample
that I want to show where history API is
used to update the page data and and the
URL and it's very simple to do that in
your code so if you guys are using
fragment identifiers which was a very
common practice in the web for the
Google search part you might want
actually update it to URLs using the
history page API now one of the things
that in Google search that we've seen
and this is data from 2015 but still
relevant is that in more than 10
countries including us in Japan we've
seen that the mobile searches are
actually higher than the general desktop
search and using that info we've also
gone to mobile we have a query string
that indexes using a more mobile user
string we're using Chrome Android chrome
24-hour agent for that specifically now
let's talk a little bit about how
rendering is done and if you look at a
Google browser Googlebot is more like a
modern browser it renders more spaces
comfortably but it is based on chrome 41
so there are some limitations let me
talk about some of those limitations to
you it's actually stateless which means
things like local storage index DB cache
API these are all flushed out as page
bays are reloaded same happens with HTTP
cookies also so this is one limitation
that you need to know the second one is
that if there are URLs that are required
or URL references to information that is
required like a.j JavaScript or CSS
space by blocking them in robots we are
nowhere near actually rendering the page
we are not gonna reference the robots or
txt to block any access to any of this
for anything it will actually go and get
all this and render the page third one
if you are using javascript to kind of
do navigation without actually having
links we're using a chef
then this the bots will not be able to
find out the relationship between
documents using sitemap we still might
be able to map the entire document set
but the relationship between these
without the anchor tags is not possible
so that is really important here
couple of more limitations service
workers are not supported indexdb web GL
web SQL these are some of the other
things that are not supported with
Googlebot right now the recommendation
here is to fall back and use polyfills
and whenever you're testing your site
use Google consider Googlebot as a
separate browser chrome 41 and test
against rome 41 to see if everything is
working right for you now another thing
to keep in mind is that Googlebot caches
aggressively but because of that it
actually would catch your JavaScript
pages some of your exacts responses so
in case you're changing some of these
things it's best to kind of let the
system know that and one other way to do
that is to kind of have versioning for
URLs so we are able to understand that
there's a change one technique is to
actually append the hash of the file to
the net to the name of the file so that
when the file changes your hash is again
regenerated changed so we know that it's
a different one and we it is less it is
a move in the cache and pasted again now
in case there's a failure for rendering
client-side client-side JavaScript what
really happens is we fall back to the
data that we caught from the server
which is the HTML that was sent from the
server this means if you're using single
page architecture or single page apps
for the using in their architecture you
would want to test thoroughly if your
client sized is actually gonna fail the
client-side pool is gonna fail it would
mean that your entire site might be
indexed but all of the content would
look the same and it'll again get
dropped from the index being duplicates
so that is another area that you want to
kind of be careful with this is an
interesting thing is this something that
browsers are very resilient about so
finding this error even though it's a
simple one is a little hard
so if you have you have head and body
tags right so if there's any tag that is
meant to be inside the body tag and by
mistake is put in the head
Googlebot rendering actually fails so
what would happen is it might
prematurely complete the head tag
thereby your content might not be
actually added to the index now one
thing which makes it quite difficult to
find the simple error is that your
browser will not show you any errors for
this most browsers are resilient around
it so look out for this problem and
we've seen a lot of developers having
making this mistake and then not having
their content surfaced well on the
search index now for hidden content now
one of the things to keep in mind is the
mod does not do interactions on the page
so if you have JavaScript which loads
data on over all click through that
would not actually be indexed so if you
have really important content that needs
to be indexed one strategy to use would
be that you render these any UI elements
but toggle the visibility so in case you
need a click to click to kind of show
visibility that's still fine but not
click to go fetch because the bot is not
going to do any interactions on the
server so he didn't content it is better
to actually have the UI elements
rendered but not i but toggle the
visibility if that's what the pages
design demands now with rendering
performance there are limitations here
also now but in terms of time but that's
computational time not so things like
set time or set interval which adds to
the rendering is not going to be
affected it's not going to affect it but
computationally there's a cap on this so
you might want to look for a slow
performing third party API so the number
of requests that you're actually doing
to a server to fetch you fetch your
resources you might want to minimize
that and the third thing you could do is
to use fast CDN service which makes the
computation time again less so these are
things that you want to keep in mind
when you're looking at rendering
performance you don't want the bot to
timeout and Miss indexing something's
looking at him to try and bring
strategies
go through some of the options that we
have again I just want to call out that
rendering is basically the construction
of the HTML and not the visualization of
the Dom okay I just want to keep that
separate here so the data plus template
to the HTML and not Dom to the pixels
just want to visualize that again if you
look at rendering it's it's kind of hard
to debug Googlebot rendering so some of
the sites what they do is they actually
render just for the bot that's a bad
idea
that's not cloaking we we definitely
discourage that it's very tricky to
maintain it and also debug it if you'd
while as your site is evolving so let's
not do that but let's discuss some ways
in which we can render so you have the
client-side server-side rendering which
is basically how traditionally actually
your web is working where all the HTML
generators the server and then send to
the client and then we sliced or your
client side where you send in a
half-shell and then the using javascript
you go fetch the content dynamically and
render it there there's a third way
which is called a hybrid model in hybrid
model what we do is you would send the
HTML first render full HTML down that
that from the server side and from the
client side for any additional data you
might call using any Jack's you might or
a JavaScript you might just pull that
data back data and then visualize it
there if you look at all these three
strategies there are frameworks that
make hybrid hybrid dozen easy but there
are frameworks that help you do that one
of them is angular Universal the other
one is to use isomorphic Java scripting
so these two techniques are what mostly
people use for rare hybrid
development and what is which one is
better
well server-side is the safest because
you know exactly what's actually going
to the client and what what will be
rendered if you look at a hybrid that's
kind of ideal and but a lot of you guys
are doing client-side rendering for that
one please test it thoroughly it is
important that you test it really really
well the same reason that you don't want
errors to come in and not just load or
the bots just seeing your app shell in
that
yes now another place is when you're
migrating things as you are actually
migrating into the new structure what
you wanna watch what are things that you
want to keep in mind the three steps to
think of first double check all the
non-google dependencies that's the first
one if you have URLs that are not clean
which is basically using a fragment
identifier or a hash non identifier you
want to clean them up use a stew API
clean them up and the third one is if
you're changing URL do redirect it on
the server if it's a client type if it
is if it is something that as a client
is actually doing you must be like using
a fragment identifier sort of a thing
then it must be performed on the right
side now when a URL gets changed ensure
that you have redirects done correctly
why if it's client-side using square
fragment identifiers and there's not
delayed like you would have a top move
it says you're going to move further in
five seconds those sort of delayed to
the lyrics and interstitials don't do
them that's a bad idea again update your
old sitemap and the new sitemap and
ensure those those links both those
links are not in robots we are not kind
of avoiding access to them in Rob
robots.txt I let them be available for
the bots and ensure all the canonical
URLs are updated correctly this is
mandatory section some of the useful
tools when you when it comes down to
monitoring and debugging Google search
console how many of you guys have used
this before some of you guys have the
other should check it out this will
allow you once you've verified your site
here this allows you to diagnose what
has being crawled rendered and indexed
and it also helps you kind of look
through your aggregated data use target
that some settings like targeted
internationalization things like that
can be changed here it also helps when
you want to remove some content so if
you want that all these features it's
important that you actually verify it so
a removal of content can be done really
fast are requested and acted upon really
fast by using this console so verify
your site using Google search and so
first don't wait for the moment where
you want to remove it do that up front
now fetch as Google there's another tool
that's there how many of you use this
fewer but still the other should check
this out
so basically what this allows you to do
is it will allow you to run a fake URL
provided a URL and fetch it like how
Googlebot would actually do it it will
let you see the server-side HTML and
also a screenshot of the screenshot of
the page that is finally created which
is a visualization of it you will not be
able to look through the Dom or HTML
that's generated though so that's one
part that you need to be aware of you so
debugging is a little tricky and you can
also visualize it in desktop or as a
smartphone either ways you could do that
now when you're looking at debugging the
client-side rendering with this it is
possible so one way of doing it is you
intercept all your client-side errors
and log in to your log in to a server so
when Google such as Google is done if
there is a rendering error that happens
your javascript is going in in intercept
that error take the relevant data and
then put it to your logging server then
again you should be using a development
server for this and don't use your
production service for this because it's
going to be expensive what I was
actually talking about intercepting
clients in duress you want to take that
push it back into your logging servers
and then from there then test it with
the Google sorry fetch as Google tool
this is a sample code which basically in
case of an error would tell you the line
number the error number pulls a stack
trace if it's available and then uploads
it to your server something as simple as
this is good too as a start and you can
add more things as as you feel to your
solution now for sitemaps it's best that
you invest time in actually separating
half separating separate files for
different routes and templates because
it makes it easier when it's updated use
lost mod field and update this to ensure
that we understand that the sitemap has
changed and again it needs to enter and
you also
to ensure that every time you changed
your canonical URLs match the sitemap
correctly you shouldn't have differences
there so that is all indexes up right
now if you are looking at a single page
architecture it's again like we said it
is a little tricky to kind of do the
client-side part so you can use tools
like puppeteer which is a headless
renderer to kind of render it as Chrome
you can provide it and provide it tell
it that you want to run it a chrome 41
which basically will run it like the
Googlebot and render and show you the
information let me show you some code
for that so this is so by the way
puppeteer is a third party a JavaScript
library which will actually fetch render
and index and show you the stuff so this
is basically how we want to kind of use
puppet a standard JavaScript now quickly
recapping all of this the key takeaways
to look at for from all of this is that
you wanna clean your URLs first so
that's really really important second
understand how rendering is done and
what Googlebot really sees look at all
the techniques that we talked about here
earlier look at all those limitations
and ensure that your site is working
with respect to all that and it's taking
care of that have the if you are a
single page architecture look at the
client side pushing the log pushing the
errors from your client side rendering
into your login service to understand if
there are any errors is happening at the
Googlebot side again have a different
draw them have a template for every
template and route have a different site
sitemap file and when you are in your
test cases please consider Googlebot as
chrome 41 from godwyn and tests have
test cases for that as a separate
browser altogether using these
techniques you'd be able to kind of make
your SEO for your pages really better
and for pwace again this same thing
applies Peter this is just another web
page so these same techniques and the
concepts that we talked about do apply
to that also</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>