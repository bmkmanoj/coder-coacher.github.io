<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Containers, Kubernetes, and Google Cloud (GDD India '17) | Coder Coacher - Coaching Coders</title><meta content="Containers, Kubernetes, and Google Cloud (GDD India '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Containers, Kubernetes, and Google Cloud (GDD India '17)</b></h2><h5 class="post__date">2017-12-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Br-c523DXYw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">[MUSIC PLAYING]

KC AYYAGARI: Hello.

Good evening.
It's been a long day,
and hopefully you've
had a lot of good
sessions since morning.
This session, we are
going to actually start
with understanding
containers, Kubernetes,
and, yes, as well
as Google Cloud.
So just before we
start, just a word
to understand where we are.
How many of you know
about containers?
Excellent.
How many of you know
about Kubernetes?
Excellent.
And Google Cloud?
Yeah.
You are experts than me then.
You already know everything.
But let me try to give
a different perspective.
So this is our agenda.
We will not be diving
into Kubernetes as a whole
or understanding the bits
and pieces of Kubernetes.
But we are trying to understand
why Kubernetes got out
or containers got out.
And what is the use case
of these containers,
and why we have to use them.
And, yes, I'll also
show you how easy it is
to create a Kubernetes cluster.
And I'll take you
to Google Kubernetes
engine, which is managed
by a Kubernetes cluster.
And we'll wrap it up.

So if you understand
the current practice
how we create applications,
generally we--
it's been quite
some time we have
been doing the same kind
of application development.
If you look at the current
application practice
development is we have
a couple of web servers
where you put your code on.
And you have a database, which
the web server will be calling
continuously to the database.
And there will be always
a batch processing
happens from the
database to whatever you
want it to basically
churn the data out.
And you have a cache where you
have your caching requirements.
This is a traditional app.
And what is the
biggest challenge
you see if you are building
these kind of applications?
Point number one,
how many people
here had an issue of you
waiting for the test team
to deliver-- or test team
waiting for the developers
to complete the code
and give it to you?
How many?
Few of them.
This is a common issue.
And the second one is you have
a couple of pieces or couple
of modules in your application
which are dependent or not
dependent to each other.
But still, you have
them, and code them,
wait for the builder to
succeed, so that you can push
your Canary or binaries out.
That's again an issue,
because even though you
are working on a module which
doesn't have any coordination
or doesn't have any
dependency, you still
have to wait for the
entire development.
So if you take that as one
app, the entire scenario which
I showed you as one app,
the app actually has--
first of all, the
monolithic apps
has an issue that it requires
large resources, for instance.
Basically you need to
put these heavy apps
into a single machine.
You can't divide them,
because they are not
built into services.
They're a single app.
You have to put them nearby.
The second one, these
are hard to scale.
And the biggest problem I see--
because I have been working
with a lot of developers--
the biggest problem I see
is it's very, very hard
for the teams to coordinate
with each other to build
these monolithic apps.
And, yes, you always have issues
with SLOs around performance
and availability.
All that comes
into the forefront.
Now, how are we
going to solve this?
Microservices and containers.
The next version, the next
level, of what we are doing
is create--
can we modulize this?
For example, you have a
different complete app.
If it can break down into
comparatively small services
which can be managed, where
every development team would
focus on a service by itself.
That would create much more
frictionless in your own code.
So microservices is essentially
taking the entire app,
managing them into small,
comparatively small services
the development
team can focus on.
The best part of this
kind of architecture
is you can scale them
or the services as
per your requirements.
You don't need to
scale the entire app.
So for example, one piece
of the service which
actually is one of scale--
example in the
e-commerce website,
if you have a huge number
of orders coming in,
which model do you
think has to scale?
Or the management?
Or the processing engine?
Rest of them has
to be-- it's OK.
You don't need to scale
them as big as possible.
So this gives you independently
scalable model, which
is a really lucrative feature.
Now, the problem-- now coming
to where you can deploy these
services.
Now you have-- we established
that, yes, microservices
are going to be the future.
But where do you want to
put these microservices?
Again, there are issues.
We started with shared
machines-- shared machines
as in you don't have a
virtualization layer.
It's just a machine with
kernel libraries and apps
installed on it.
So the issues with
shared machines
is there is no
isolation as such.
There is no namespacing, but
you do have common libraries.
The biggest issue is that
highly coupled apps with OS,
they're tightly bundled.
So this works.
It's not like this doesn't work.
But let's think about
the scaling back
of these shared machines.
It's very, very difficult
to actually scale them up.
So then came virtual machine set
up, where what we tried to do
is we took the same
applications, virtualized them.
But the key challenge in
virtualization or virtual
machines is you
still have kernel
attached to libraries and the
app, which makes them heavy.
So how much time?
You take a-- create
a virtual machine.
It's not easy.
Still today, it's a
heavy lifting job,
even though you have the best
servers or the processing
engines available.
So this means you've
got some perks,
like you got some isolation.
But it is still inefficient
and hard to manage, as usual.
Now, that's where
came containers,
where the kernel is taken out.
The libraries and app are
moduled into a single entity.
And you just run them.
The best thing of this
architecture is the apps
and the libraries are
easy to scale basically
and easy to create.
It takes a fraction of
second and easy to scale.
So it just covers
all your issues.
So first of all, if you
change your architecture,
your monolithic
architecture, to containers,
you have a container
which actually can scale.
Now let's talk about
what we have been doing.
Let's talk back--
let's go to history.
So for the last 15
years, in Google
we are creating a lot of
powerful infrastructure.
It's been 15 years we started
building this company.
And, yes, we are kind
of a startup of our own.
So what we have to do is create
our own hardware essentially,
assemble them, get the
best hardware engineers.
And we build the best and
powerful world's factors,
fastest, powerful
infrastructure in the world.
Now, I wanted to give
you some background.
How do you think we are
actually creating this?
Or what is that we are
running in this hardware?
It's containers.
We actually run all our
services on containers.
So if you look at the
Search, Gmail, Maps, YouTube,
and Chrome, all are enabled
on these data centers
using containers.
So this is a kind
of a slide which
gives you technical innovation
we are bringing to table.
How many of you know
these names as such?
How many of them you
recognize the names?
Quite a lot, yes.
That's great.
So these are-- for
people who doesn't know,
these are actually white
papers which Google created.
You can just go ahead
and google them.
These are white papers
which Google released
which expand upon the open
source implementations
at a later period of time.
So we are kind of pushing
the boundaries in terms
of managing the huge data.
By end of that, the
entire infrastructure
is built on the
microservices architecture.
To give you a perspective, there
is a white paper called &quot;Borg.&quot;
If you look at it,
it is a white paper
called &quot;Borg,&quot; which is our
implementation of container
engine, how to orchestrate
these containers.
So &quot;Borg&quot; is there for 10 years
before even the docket exist.
And this is the
way we used to run.
And we started running--
today we create two billion
containers every week,
and it is growing.
Now, how we can achieve
that is essentially--
how we can achieve that
is basically all this.
I'll show you
architecture diagram
of how the Borg looks like, and
how we, in a simple command,
actually can translate into a
working piece of code there.
So let's just establish why
we need containers here.
First of all, they are fast.
They are portable.
And they are quite efficient.
And they can run anywhere
in the environments.
This takes away the
concept of deployment hell.
The concept of
deployment hell means--
I'll give you a backstory.
For example, if you
remember a ship company
where it wanted to transport
huge number of goods
from one place to
another, it has
to pack those goods according
to how they are created
and how they have to be packed.
So that's the biggest issue,
because they created that,
and they actually
transport that.
The packaging has to be
aligned with the goods.
Now there came-- if you remember
the latest shipping dockyard,
you'll see everywhere in a
block now which is a container.
So this container allows
you to put any sort of-- you
can manage any sort
of goods in that,
close them, give
it to the shipyard.
They will transport it.
They have a common
mechanism to transport it.
So the same isolation--
take the same analogy.
Put it into your code.
You, as a developer, pack your
code, put it into a container,
give to the DevOps guy-- where
they will actually manage it
on any virtual machine,
or any infrastructure,
or any hardware.
That reduces friction between
a developer and a DevOps person
who wants to deploy,
as well as a developer
who has complete control on how
to pack his code without that
getting changed over
a period of time.
So these are the beauties.
Or these are the best advantages
you get out of the containers.
But do you think everything
is green with the container?
No.
There are some always
limitations for this.
The biggest limitation is how do
you deal with multiple servers?
There are a huge
number of servers
where these containers or
clusters has to be maintained.
How do you deal with
multiple servers?
What if a server fails?
Or what if the container fails?
Or how do you define what
a container failure is?
So essentially you
keep on scaling it up.
The container itself
will become messy.
You can scale them up.
Or what if they fail?
Or what if the
entire cluster fails?
Or how do you scale
up on a cluster?
So these are some of the
challenges always comes
with when you're
using containers.
But the solution for that
is manage the containers.
Now, how do you manage it?
It's very difficult. So
let's create a program
which can manage the container--
what essentially is container
orchestration engine--
which boils down to Kubernetes.
Kubernetes is something
which can orchestrate
the entire suite of containers.
So Kubernetes and Google
Kubernetes engine essentially
are two facets.
Kubernetes is essentially
you can run in your laptop.
I would say 100% open source
framework, which you can do.
But Google Kubernetes
engine is a manager service.
So essentially you don't need
to worry about the master node.
I'll explain what the
master node means.
So look at these two analogies.
One is maintained and
managed by Google.
Second one, you can
run off your own
by managing the entire
cluster of your own.
So if you go back
to the history,
this is the precursor of &quot;Borg.&quot;
This is the white paper of
entire cluster management
engine.
So if you look at
this white paper,
it was very clearly written
the architecture of Borg.
So let's take-- let's imagine
this is the data center,
and you wanted to--
you have thousands
of applications running here.
How do you want--
as a developer, you wanted
to deploy a Hello World
application in this
data center where
that application, or that
particular microservice,
will use 100 MB of
memory and 1/10 of a CPU.
So the way we started writing
these kind of deployments
is using simple code.
So this piece of code will tell
Borg master node, hey, master
node, go ahead and
deploy 10,000 or whatever
the number give you.
Deploy these many
instances of microservices
on the cluster of
data center, where
it has to use the RAM of 10
MB, and disk 100 MB, disk size
or whatever we have, and CPU of
0 point, which is 1/10 of CPU.
Submit this to the master.
It will directly go
ahead and create it.
It's so easy.
So you don't need to worry
about how you have to manage
the entire data center there.
So this is how the
architecture looks like.
A binary is a
piece of code which
your application is built on.
You take the binary.
You have a
configuration file which
talks about the binary, which
is sent to a repository.
Now the same binary will be
given to the Borg master.
The Borg master
will take the binary
and deploy that into
the masters, the nodes.
Every node here, the bottom
four pieces which are
represented here--
every node is same Linux OS.
And every node has
a Borglet, which
is a kind of program which
can listen to the Borg master.
So you have suite
of machines down,
which has Borglets
installed on it.
The Borglets will
take care of listening
to the master where it
can deploy their nodes
and start running them.
So this is how we were able to
achieve the same small piece
of code which I showed
you in the earlier
slide, this slide, which we have
passed on to the Borg master.
It will start deploying n number
of microservices directly.
Now, once it happens,
the Borg master
starts creating Hello
World application
all over the data center.
You don't need to worry
about scaling it up.
You just need to tell
the configuration what
size it has to be
used, push it across--
as simple as that.
Now, where Kubernetes
comes here is
Kubernetes is kind of a
container orchestration engine
which supports multiple clouds.
And it is inspired by--
it's direct development of the
Borg, which I just showed you.
So the Borg, which is
used by Google, the same--
I'll show you an analogy.
The same actually--
the architecture
is taken out and
created as Kubernetes.
It's completely open source.
It is donated to an
open source community.
So the end goal of Kubernetes
is don't manage your machines.
Manage your applications well.
So Cloud Native Computing
Foundation essentially--
you can't run entire
containers individually.
You have to orchestrate.
You have to monitor them.
You have to distribute them.
So there are so many
open source projects
which are found under
Cloud Native Computing
Foundation, CNCF, where
Google donated Kubernetes
as one of the projects to it,
which is part of orchestration.
So right now the
Kubernetes is in 1.8.
The stable version is 1.8.
It has 4,500-plus contributors
and about 3,000 GitHub stars
in it.
And there are a huge number
of contributors to it.
So essentially which
boils down to the point
that if you are building
your apps on Kubernetes,
there is a high chance
that the same app can
be run on a multi-cloud
environment, built on anywhere.
So that is added advantage
you get out of it.
So 10,000 view foot.
Basically if you
look at Kubernetes
on a very high level
as an end user,
this is how it looks like.
You have a master node.
Users are you people
who are going to talk
to the master node, developers.
And the nodes are
essentially machines.
So if you look at the
machines, every machine
is installed with kubelet,
which is a service designed
to talk to the master node.
And the users can talk to the
master node using either API
call or a CLA command line.
CLA actually again calls
API behind the lines.
And user interface, you can
build your own user interface
using the APIs and can actually
go ahead and call with that.
So if you look at the four
pieces in the master node--
etcd is kind of a database,
CoreOS database, where
it can store the content;
state of the nodes
which are available;
schedulers, which
schedule the desired
state configuration;
and controllers actually.
So now let's come
to Kubernetes, how
the same model of Borg
translated to Kubernetes
looks like.
You take the docker image.
You build your application
as a container,
which is a docker image.
Push it to docker private
GitHub repository.
And push a configuration
file to the master node.
The configuration file
is written in YAML.
So everything in
Kubernetes is declarative.
Just submit a YAML file.
It will understand
how to reorganize.
The Kubernetes master
node will figure out
a way to do the rest
of the work for you.
So the YAML file is
parsed and taken.
And then, according to the
YAML file which was submitted,
the individual services
will be pushed across.

Those services are taken
away from the GitHub
private repository hub.
So the declaration
happens on the top of it,
where the configuration
file, which is a YAML file--
once the declaration is made and
pushed through to master node,
the master node takes--
with the help of scheduler--
actually go ahead and
pushes the services
into different machines,
which are hard machines, which
are virtual machines, and
keeps on running them.
So as an end user, all you
just need to care about
is you make an API call.
An API call is just an
API call or a CLA command
line, anything.
Just make an API call to the
Kubernetes engine or Kubernetes
cluster.
It just works.
So all you just need to is
understand how many command
lines you have and keep on
trying them and creating
a machine of your own.
So before I go into
deployments and replicasets,
let me give you a
background on the smallest
commodity in Kubernetes.
The smallest commodity in
Kubernetes is called pod.
So what's a pod?
Essentially if you look
at you are the end user,
that's the Kubernetes
master node, which is going
to create orchestrate pods.
So you don't need to
create pods yourself.
Pods are created
using replicasets.
So a replicaset is
essentially what
creates the pods, which
are individual units.
Pods are compresses of
multiple containers.
So just to make you
understand, the containers
are the smallest pieces
which runs a smaller service.
Pods encapsulate the containers,
one or multiple containers.
The same replicationset
will actually create pods.
And there will be
deployments which actually
creates replicationsets.
Your job is to create a
template which hierarchizes
all this facts and send--
using the commandlet,
kubectl create, and send it
so that the deployment
will be created.
So if I send a kubectl
command, which is create,
with the name of my name
app, the deployment template
looks like this.
It just keeps on--
replicasets of three.
And my application name as my
app with the version we run,
it just creates three
services, or three instances,
which is a desired straight
configuration of the version we
want application of my app.

What if I change the
replicas to four?
It just create one more.
So all you just need to is
focus on the YAML template.
Update it.
Send it to Kubernetes.
It will take care
of replicating that.
What if I kill one of them?
If I have three services,
what if I kill one of them?
It just maintains the state.
So any hardware failure,
any service level failure,
is already taken care.
Now let's-- you wanted
to upgrade this.
You wanted to upgrade the
v1 of app to v2 version.
You just need to have
an upgrade command which
actually starts creating.
You would have to create
another replicaset, which
is v2, which slowly kills the
v1 and starts upgrading that
to v2.
So you have in place
upgrades where,
without downtime,
your application
is moved to version two.
The individual services
will get upgraded.
Now, if you look at
the pods are mortal.
Every pod which is an individual
unit of container is mortal.
They die.
They're bound to
die, by the way.
Now, how do you give IP
address to these pods?
So how do you manage them?
How do you communicate them?
The services essentially provide
a stable IP address and DNS
name resolutions to pods.
So these services
will essentially
help you to communicate between
two or three different pods,
internal communication.
And they also can
be used for actually
having them communicate
to the actual users.
So each microservices can
be individually scaled.
This is how the end
diagram looks like.
If a simple monolithic
app can be converted
into different services,
which are containers,
and the service
between them will
be used to communicate
or sort out traffic
between the services there.

So essentially the
right architecture
is the Kubernetes will help
you to schedule or orchestrate
the containers.
Containers are easy
deployment services.
And microservices actually
take your monolithic app,
break down that into
pieces, so that it
can be very well-managed.
Now I'll show you the
exact look of what
a Kubernetes engine looks like.
The same Kubernetes
service which
you deploy on your
local machine,
if you take them-- so Google
has its own manager container
engine, which is called Google
Kubernetes engine, which
has all these features
built into it.
So we essentially have our
OS built for containers.
The Linux is developed and
optimized for containers.
You get a private
GitHub repository.
You get a fully managed service.
It's a fully managed service.
And you have Docker
Image Support.
Other than that, the
best way to create
is go to Google Cloud Platform.
Go to Kubernetes engine.
Go to the cluster.
Create a cluster and
just key in your details.

Select where you wanted to
deploy this cluster and what
is a machine type you need.
You can customize
your machine types.
So maybe you can select I want
30 to course and 160 GB of RAM.
And I want maybe seven,
whatever the size you want.
So a single click of this
cluster, or this service,
will create you a
cluster size of seven,
which we entered there.
And every virtual machine has
[INAUDIBLE] and 165 GB RAM
overall.
And it creates a
cluster of its own
where the master node is
managed by Google Cloud.
So you don't need
to worry about what
if the master node goes down.
The master node is managed,
and the cluster comes in.
There is also a concept
called federation.
Which means if you federate,
container federation
where you can have almost all
the clouds connected using--
you can deploy
your microservices
in every cloud using
just Kubernetes.

So I think we are just out of--
just in time.

So in a nutshell,
Google Kubernetes engine
will actually provide
you a cluster in seconds.
It's a fully configured
cluster, a fine-grained control
over the cluster designed for
multi-cloud, as well as you
can actually use five nodes with
no additional cost for that.
There is a free tier
available to it.

So anyone who wanted or
interested to practice,
go to the code labs.
Just search for Cloud.
And search for
Kubernetes in this,
where you get a
step-by-step approach of how
to create a container engine,
basically an application.
In a few clicks,
the entire process
is actually given
here with screenshots.
Just go out and try this.
You will see how
easy it is to create
an entire microservice
architecture
service on Google
Container Engine in a very
short period of time.

And thanks for coming here.
And I'll be there around.
If you have any questions,
feel free to talk to me.
Thank you.
[MUSIC PLAYING]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>