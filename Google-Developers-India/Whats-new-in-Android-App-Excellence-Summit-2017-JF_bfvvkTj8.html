<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Whats new in Android (App Excellence Summit 2017) | Coder Coacher - Coaching Coders</title><meta content="Whats new in Android (App Excellence Summit 2017) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Whats new in Android (App Excellence Summit 2017)</b></h2><h5 class="post__date">2017-08-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JF_bfvvkTj8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">SPEAKER: Ladies and gentlemen,
please welcome Amrit Sanjeev.
AMRIT SANJEEV: I know,
right after lunch.
You guys are all settled in.
I will not let you sleep.
I'm going to be
really loud, and we're
going to talk about Android
for the next 30 minutes--
Android O, to be specific.
What I want to do in
the next 30 minutes
is not to cover a
comprehensive list of things
that we actually
released in this version,
but to highlight a
few things that you
might want to grab your
attention as changes that
might break your apps, or
changes where you might
need to adapt your apps to, and
also some of the new features.
So two buckets of
information-- things
that you should pay attention
towards when you're running
compatibility tests, and
the new features that
are released in this version
so that you could actually
augment to the experience that
you already have on your apps.
Let me dive into it.
Timelines-wise, if you
haven't started testing on O,
you should scramble up,
really start doing this now.
Because as for the timeline,
the versions have been released.
The final APIs are out there.
This is a good time for you
to start testing against it
and start building
for the next features
so that when the
final release is out
and users upgrade their devices
your apps work as expected.
So from a developer
priority perspective,
you need to do it in this order.
You want to start
with compatibility.
So in compatibility, what
you really need to do
is to check how
your app is running
against any version of the OS.
Is everything
running as expected?
There are some changes that I'll
talk in the first slides, which
you might want to
take a deeper look at
to understand whether that's
affecting your code base
and you're working towards it.
But you're not targeting
O at this level,
you're just ensuring that
if somebody upgrades,
your existing app
works as intended.
Once you've done that, the next
phase would be building for O.
That's where you want to add the
new features that we released.
That's where you want to target
to O. So do it in this order,
don't do it together.
Don't do it like where half
of compatibility is done
and half new features
are out there.
That might lead to your
app stability going down.
So try to follow this
order in which you
will update your apps.
Let's dive deep into it.
The first things,
they are things
that you might want
to kind of look
at and seriously
deep dive into--
look at your code, analyze
the behavior of your app,
and ensure that nothing breaks.
First one I want to call
out is notifications.
This is something all of us
have in our applications.
Is there any app that does
not use notifications here?
One hand out of this
whole audience raised.
I hope you start using it.
This is a good way of
engaging your users.
But again, when the
team looked at it,
there were three things that
the team actually evaluated.
One was attention-based sorting.
&amp;gt;From a user's
perspective, we need
to show the notification that's
most relevant to the user
at the highest importance.
That's one thing.
The second one is that
we show stability.
And third is in terms
of animations and things
like that.
So the user experience
continuously improves for them.
But the thing that you
want to kind of be sure of
is to use notification channels.
Notification
channels basically--
let me call out a scenario.
All of us have
apps on our device,
and we get a lot
of notifications
from these developers.
Some of them are not very
relevant for us, some of them
are relevant.
It could be a bank app which
says offers on personal loans.
It could be that they also
have a security update.
But regularly, if you're getting
this personal loan, or a change
in interest rate,
and things like that,
and you will say
that, oh, I don't
want to listen to any
of the notifications.
You will also end up missing the
security update, or a phishing,
or a fraud detection,
or something
which needs urgent attention.
You might miss that
notification too.
But with notification channels,
what developers could do
is to bucket their
types of notification
into different channels.
And then the user can specify
properties on this channel.
I could say that I don't want
to get any of the marketing
messages, I want to get
all the security updates,
if there are two buckets
like that decided.
So that gives a lot more
control to the user.
The user can also
say, I don't want
to listen to any of
the notifications,
or I want to see all the
notifications from this app.
But this avoids the problem
of the common notifications
that you would actually put out,
which certain users don't like,
than removing the entire
channel altogether.
They say that, OK, I don't want
any notification from this app
anymore.
Note, with channels, you
have finer control over this.
The other thing to note is,
if you don't use notification
channels and you target O, your
notifications won't come up
by default. So this will break
notification system if you've
built one and you don't
add channels to it.
So that's something that
you've got to think of.
The second one is in terms of
the attention-based sorting
that I talked about.
Now, any ongoing activity
would be actually--
in O-- would be
sorted to highest.
So that needs the most
amount of attention.
It could be something like
a foregone service that's
happening right now, a
call that is continuing,
a media player that is running.
These will have the
highest priority,
and they'll come the highest.
Then it would be things like
person-to-person communication.
Your chats, things like
that, would come next.
Then you have
general information,
general notifications
under that.
And the last one would be just
informational notification,
which would be like
file download completed,
things like that.
That would be sorted to last.
So you want to kind of
specify appropriately
what a notification
type is like too.
In O, there's one more
feature called dots or badges.
We call it dots.
It allows developers to kind
of show the notifications,
if the launcher
allows it, as a dot
on that note in the app icon.
So when you long-press
the app icon,
the top non-action notifications
are shown as a dialog,
like the shortcuts that you had
in N. If you would long-press,
you'd get the shortcuts
in the app, right?
Very similarly, you also get
the On Action Notification
up there.
This avoids your notifications
being shown along
with everything else, but this
requires the launcher also
to support it.
But this is a good
feature to add to your app
so that you can actually
specify those notifications
and the action comes on.
Because on the icon,
you'll have a dot
signifying there's some action
that you might want to take.
So that's something
that you want
to think about notifications.
Quickly looking--
this is one area
where everybody wanted to put
a lot of attention, background
limits and location limits.
These are two areas where
Android developers really
had to change some of the code
to fix, or to make their apps
work around [AUDIO OUT]
way as they did earlier.
In O, if your app is
in the background,
then it cannot start and run
a service in the background.
Or foreground app can do it.
And the intermittent
state of somebody
starting in the foreground--
if an app is in
the foreground, it
started a background service.
And then the app closed, doesn't
mean that your service gets
killed immediately.
For a couple of minutes
it's allowed to run.
And after that, it gets killed.
You cannot infinitely
run background services.
And this was done primarily
to kind of keep battery
life of devices in check.
Because every app,
or a lot of the apps,
are doing a lot of
work in the background.
These consume battery, resources
not only from a battery,
it also includes RAM.
All of these are consumed by
these background processes.
Now, that doesn't mean you can
run anything in the background.
You have to run it as
a foreground service.
So if your app requires
to do something
without having it
on foreground, you
need to run a foreground
service rather than a background
service.
What it does, it will put a
notification informing the user
that, hey, this is happening.
If you would look at
Maps today, Google Maps,
and you will start
a navigation, you
will see a foreground service.
And the location tracking is
based on a foreground service.
And that notification
will remain throughout.
A green notification will be
there in your notification
trail all the time.
The second one that
I want to call out
is on broadcast limits.
There are only a set of
broadcast limits allowed
in O. Basically, anything
outside of that, whatever you
declare in your
manifest, the ones which
are most commonly
going to affect you,
network state change
listener, read SMS.
You wouldn't get these
broadcasts any more in O.
If you would look
at the read SMS,
there's another API
called SMS Retriever
that you should use in order
to get the same functionality
of reading an OTP.
But that will also allow
you to read without having
SMS permission granted.
The only read OTP
message will be allowed--
sorry.
The app will be only allowed
to read that particular OTP
message using the
SMS Retriever API.
So there are changes,
but there is also
workarounds or altered
APIs that you should
use in order to solve this.
So if you have OTP and
using Read SMS API,
if you run it on
O, you're not going
to get that particular broadcast
so that you have the trigger
to start fetching from the SMS.
You have to use
the SMS Retriever
API to move around it.
Location also has
certain limitations.
In background,
your own location,
basically, you're only going
to get updates only a couple
of times--
R. And you need a lot of-- you
need a trail of the location.
Instead of requesting
every single time, what
you might want to do is
to use the bash API--
which will give you a
list of the trails--
known or tracked locations
over a period of time.
And that will allow you to like
maybe trace the user's path
and things like that.
When you want to
do that, you want
to use the Bash Location API.
You also want to kind of use
Geofencing API in conjunction
with the location, the
fused location provider,
to kind of work around
this limitation of having
it coming up only a few times.
There are a few recipes
that we'll be publishing,
and you can take a look
at that and understand
that in more detail.
When it comes to location,
you can also get location
from a foreground service.
If you're running a foreground
service, there's no difference.
It runs in as it is
running on API 25.
So no difference would be there.
But when you're doing
it in background,
then you have this
limitation coming in.
Geofencing has certain time
limits changed right now,
which makes it 10
times more efficient.
But instead of a couple
of hundred milliseconds,
it's now about two minutes is
when it actually gets updated.
Roughly around that.
So it has become efficient.
But at the same time, it
will take a little more time
to kind of update users to you--
I mean, update
the events to you.
And from a sensors perspective,
we've done a lot of work in/or
for that.
We have dynamic
sensors this time,
which allows you to add
external sensors to your device
and read it.
And your apps can read natively.
You might want to
add an environment
sensor to your device.
And then you're asking
to read this natively.
And there's a lot
of standardization
on the interface that
has happened with respect
to dynamic sensors.
Direct sensors for virtual
reality applications,
that's something that
has happened here.
The calibration algorithms
for contextual APIs
have been improved.
There's a lot of
machine learning models
that were applied on it to
kind of make it more relevant.
Earlier situations,
like, oh, I was driving.
Then the car
stopped, it started.
It might detect me walking.
Or car's really slow, it's
detected me as walking,
and things like that
are now avoided.
And a lot more realistic
perspective of it
is actually shown due
to the machine learning
models that were applied on it.
And again, some of the low
latency body detector APIs
are there.
And there's something called
as Android Sensor Hub.
How many of you guys
know about this?
Just a show of hands.
Very few?
OK.
In a lot of times,
I mean in Android,
one of the things
that has happened
is there's a new
chip that is added
to the whole stack which kind
of does the sensor management.
It's not run on the
main application chip,
but on something smaller,
which is more power efficient
and can run more frequently.
And that is what is supplying
all the sensor data back.
So this is something
that allows a lot of new
and faster tracking
of information.
Like watches like this can track
every push-up that you take,
or every bicycle that you do.
It can actually
track it accurately
because it can get
that movement and learn
much faster the patterns.
And they are very power
efficient because of that.
You should read up on that.
We have a whole page on
the documentation on DAC
about it, developerandroid.com.
And that is a pretty interesting
thing to explore it on.
When it comes to
support library,
one of the major changes that
has been done with the 26
release is that the minimum SDK
supported, or the minimum API
level supported, has
been moved up to 14.
If you have apps that
support before that,
yes, this might be something
that you want to take care of.
But if you update to the latest,
the advantage of doing that is
you have 1.4K less methods from
a method count that gets added.
Because user support library,
the amount of methods that get
added to your DEX limit, or
your ever-approaching DEX limit,
if you're not multi-DEXed yet,
this reduces by 1.4K, again.
And a lot more
classes can be removed
with this particular
level that is upgraded.
We also have a maven
repository, a new Google
maven that we've added.
This means that every time we
update the support library,
you don't have to download
the entire support
library to a local maven,
which was the case before.
Now you can add like one maven
line in the gradle, which
will automatically evaluate,
compare and automatically
fetch the changes for you.
Again, an issue tracker is
there for reporting issues,
and interacting, and
getting data much faster.
So if you find issues with
play services, libraries,
or the support libraries,
please actually
add it to the issue tracker.
Don't assume that
everybody knows it,
because adding and adding
priority to a particular bug
helps us also prioritize.
So a lot of times,
developers know about.
Then they go check
the tracker, they
see all those bugs
are already filed.
And they leave it at that,
but add a comment to it.
Because the more people say
that this is a problem, the more
importance that--
it gives us data to understand
that it's more important.
So use the issue tracker
to track any issues
that you find while
you're actually
working with these libraries.
Support library also has
things like XML fonts,
downloadable fonts.
This is a really cool
feature that we've added.
It has the entire
Google Fonts catalog
as a font provider, which means
800 fonts, access to 800 fonts
without you bundling
them down into your app.
Which means, if you are actually
using a font which is there
on Google Fonts, you
can actually specify,
and a font provider will
fetch that information
and show it to you.
This has two advantages.
One, your app is not
bundling that information,
so your app size reduces.
So you have five
fonts, each 500K.
That's 2.5MB.
You can remove that
2.5MB out of your app.
Secondly, the fonts
provider, if you
have five apps which all
use five of these fonts,
all of the five apps
will load these fonts
into memory, which means
more memory is used.
Five copies of the same font
gets loaded into memory.
When you use a fonts provider,
only one copy is shared.
So the amount of data that
the user downloads reduces.
One time, the font is available.
That's shared across all
the applications, which
means the memory
footprint is less,
so the entire system
actually runs much better.
Because of all these reasons,
this is something that is--
and it's, again, all the way
backward compatible to 14,
so you should take
advantage of it today.
Emoji compatibility library--
this is a very interesting one.
A lot of times,
you've seen this tofu,
which is basically this
square box with an X on it.
They call it tofu.
I don't know the rationale
for it, but it's a nice name.
Let it be that.
That's basically
because the font.
So basically, emoji
is not supported
by the platform at that point.
And the emoji font file is
actually part of the OS,
or it's part of the build.
So it can't be
updated dynamically.
Now, with the emoji
compatibility library,
what happens is the way
it checks is that, when
a particular Unicode is
passed or that emoji, it
checks in the system
font, system emoji font.
And if it's not
available, it'll actually
look at a compatibility
font and then actually show.
So that way, you can
dynamically update it,
or the system can dynamically
update your supported emojis.
And less of tofus will
start coming on screen.
Autotext view,
autosizing text views,
this was the third-party
libraries a lot of us
have used, where third-party
libraries did this work.
Now it's part of this
support library, which
means you should use that.
And then remove
the third-party so
that your method count, as well
as your library dependencies,
reduce.
This allows for your
characters or the text
to be sized according to
the size of the text field.
So if you're supporting
multiple densities,
multiple screen sizes,
from a TV down to a phone,
then this kind of
helps you automatically
scale it to the right way.
This is quite useful.
A lot of times, people have used
third-party libraries for this.
And now, we've just
added it to the support.
So if you're using it, see
whether you can replace it
with the support, so you
reduce the method count for it.
With that, let me jump onto
some of the new stuff that
is actually added--
the adaptive icons.
If you have a global
app which supports
a lot of launches, what happens
is each of these launches
have their own type
of icon configuration.
Like some of them are square,
some of them are rounded.
Some of them are rounded edges.
So what happens
is, a lot of times,
in order to be like
everybody else in the system,
these font files have
to be added to your app,
and then appropriate
launch icons
have to be shown
based on the launcher.
With O, one of the
things that happens
is you can actually provide the
background and the foreground
assets.
And then a mask is
applied by the launcher
so that it would look
consistent with the launcher's
specifications.
So that way, your icon
specification, one
specification can be
given, that can be used by.
And the mask can be
added by the launcher
and then have consistent specs
coming out for all the icons
that are there for the apps.
So this is one.
And you can also add
behavior to it to show--
like the one I've shown
here, it's just moving a bit
or pulsating.
Things like that can be
added to express or bring
attention of the user.
Pinning shortcuts
is another thing.
Like you had shortcuts in N
where you long-press the icon
and you get a list of shortcuts.
Pinning shortcuts is basically
a quick way to go to a task
by adding an icon to the screen.
So when you actually click on
a pinned shortcut, what it does
is it launches the app into
the task that you want.
Basically a shortcut you don't
have to long-press and select
the shortcut.
You can take the pinned
icon, pinned shortcut icon,
and click on and directly
launch the app from there.
So this kind of helps you
do things much faster.
Picture-in-Picture-- again,
now supported on the platform.
This allows you to kind
of resume or continue
your video playback.
That's a classic example.
You see it in
YouTube where if we
do a playback where
another activity is there
in the background
for you to work with.
So you can actually show
picture-in-picture sort
of a mode.
It was on Android TV earlier.
When somebody could
browse the channel,
the current playing
channel would actually
show as a smaller picture
on the TV interface.
The same thing can be done in
tablets or phones starting now.
When you look at
multiple windows--
in Android O, one
of the things that,
if you have multiple
displays, your app
can select to use both of it,
or you can move your display
from one window to the other.
It allows you to use both
screens more effectively.
But the thing to remember there
is only one of the activities
is going to be in
the Resume state.
It's not running two
activities parallelly.
No, that never happens.
It is just one activity
running, but two displays
can be used more effectively.
And color management--
those who are actually
working with photography
apps, or video reproduction,
or anything that has to
do with accuracy of color,
now there's a wider
gamut of colors
that are actually supported.
And there's a whole
talk on it at I/O
that you should actually
watch if you want
to get the details of this.
This is really
interesting stuff.
[INAUDIBLE] actually
talked about this one.
The whole video,
it's a good watch
if you want to
take time on that.
And I talked about
picture-in-picture earlier,
so let me move on.
Autofill framework--
there's a lot
of times when typing on a
small screen is cumbersome.
You are in a moving
vehicle and you're
going through a bad patch of
road, you won't be able to type
in your email ID or password
properly with a small screen.
That's where a lot of these
apps in scenarios where you're
logging in, where
your email is typed in
or your credit card number is
typed in, those sort of things
are places where you
wanted Autofill to happen,
where the forms
can be autofilled.
So now there's an
Autofill framework
where your apps
can take advantage
of by providing hints
and asking you to fill.
Or you can write
application using
these APIs to behave as
the app that actually
stores the information,
basically provide
as an autofill provider.
So earlier, autofill
providers used
to take the permission
of write on screen,
and then write on top of
screens and things like that.
You won't really need to do with
a much more secure and better
performing way.
Better performance is
attained through the new APIs
that are there.
So you could actually use this.
When it comes to
animations, there's
a lot of the emphasis
given on natural motion.
And there's a lot of physics.
There are two basic ones that
are there, a spring-based
and also the bounce.
So there are two animations
that are given in the first set,
but a lot of it is actually
real world physics based.
So things will
actually behave more--
the animations will
look more natural
if you apply on that using
these animation libraries
when you actually do it.
And again, this is
part of support,
so you can actually get it
backward compatible again.
Media-- media apps have a lot
of new things that are there.
They have metrics, which
get codec duration, all
these things.
They have better
playback, better seeking.
There's improvements
in all these spaces.
In recording, instead
of locking down
to one track, one audio
track and one video track,
now you can add multiple
tracks using an Add Track call.
Multiple tracks, your
audio and multiple tracks,
you can mix stuff.
So all these new features
are there in the API.
This is something that
you should explore.
And this opens up a
lot of possibility
to create new types of
apps and new experiences
on existing apps, new
features that earlier were
very difficult to come with.
And also, these metadata
tracks that are supported.
So all these things
are going to help
you increment the
experience that you already
have on your apps.
With WebView, one of the areas
where people want a lot more
help.
I've heard a lot of people
complaining about this.
But the WebView has
a couple of new APIs.
One of them is a version API.
Basically, you can create
the version of WebView
that you're going to run
your code on and then
decide to run differently or
take a different code path.
So the version API will allow
you to get that information.
Then Google Safe
Browsing is there,
which, when you align to
the meta tag, what happens
is it will check
against the Google APIs,
whether it's safe to kind of
take the user to that site
that you're requesting
the WebView to load.
So that is something
that you might
want to turn on in
your applications
to avoid any phishing and
any fraudulent activity,
because there's a
WebView on that.
And then there is a
termination handler.
This is really cool
because there is a handler
and the new one is
multi-process-enabled,
what happens is if
you render crashes,
that crash gets
propagated to your app,
and your app crashes sometimes.
Termination handler allows you
to capture those information
in terms of low memory crashes,
or because of low memory,
render thread being
removed by the system,
or render thread
crashing itself,
those sort of situations can be
handled with using the render
thread API and not allow
that error to propagate back
and crash your app so
you might react to it.
You can also set
process priority
to the threads, the
multi-process one,
so that you decide
which one gets
more priority on your screen.
And if your WebView is
not of that priority,
you want to reduce
that priority down.
So those are some of
the things that you
want to do with that
WebView part of it.
This helps you kind of
improve stability of your app,
and also ensures that you
have a lot more-- the user
has a consistent
experience, even
when he is actually
using a low-end device,
or it's actually
browsing to sites
which you're not very
comfortable with or not very
sure about.
This Safe Browsing
API will help.
Your version API will allow
you to kind of fragment--
I mean, there's a
fragmentation that you see
and you find a difference--
I mean, you want to
run different code
for a different
version of WebView,
use the API for doing that.
And with that, let me
come to the last slide
for contextual experiences.
We have Wi-Fi Aware, which
is a new platform, which
allows you to kind of
discover devices around you,
connect, discover and connect.
And you don't really need
an APN point for this.
That's the best part of it.
So these devices
could-- like it could
be that I have some file
that I have on the device.
I could just share it
with some of my friends
here by just discovering their
device and sharing it directly.
I don't nearly need
to be connected
to a common APN or anything.
You can use this and share
information that way.
The second one is the nearby
connections, the semantic type
support.
That's useful when
we're dealing with IoT
and experiences with respect
to contextual experiences.
It really helps you
kind of work with it,
avoiding all the
complexity under the hood.
And one of the things I
really want to call out
is the awareness API, which
is drastically improved.
You really want to kind of go
back and try it one more time
if you have left it
earlier, because there's
a lot of algorithm
changes that has happened,
and its detection
is much better.
And it also has newer types
of activities detected.
Like this watch, for
instance, basically
detects walking, running,
push-ups, sit-ups.
All these exercises it is
able to detect accurately.
Basically this is all
improvements in the API.
So you might want to
kind of see whether that
is going to add experiences
to your application.
I know I've talked lot
about a lot of things.
And this is by no means a
comprehensive list, like I
mentioned before,
but these are some
of the things that you
might want to kind of look
at in order to kind of improve
the experience that you have.
And don't forget to run the
compatibility tests right now,
this week, or for
a couple of weeks
till the final release is out.
Because that's really
important that you
test your apps
against the latest
build and see that experience.
Like the background limits,
the broadcast location,
these are all areas
where you might
have to rewrite some
part of your code.
And that is something that
you want a little bit of time
to think through the
algorithms, think through it
and test the behavior changes.
So it's don't wait till
the final version of the OS
is out there and users
start complaining.
With that, I'll close.
Thank you.
[APPLAUSE]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>