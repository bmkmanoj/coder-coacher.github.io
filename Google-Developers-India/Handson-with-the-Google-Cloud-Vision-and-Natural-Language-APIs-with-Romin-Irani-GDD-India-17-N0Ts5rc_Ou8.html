<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hands-on with the Google Cloud Vision and Natural Language APIs with Romin Irani (GDD India '17) | Coder Coacher - Coaching Coders</title><meta content="Hands-on with the Google Cloud Vision and Natural Language APIs with Romin Irani (GDD India '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hands-on with the Google Cloud Vision and Natural Language APIs with Romin Irani (GDD India '17)</b></h2><h5 class="post__date">2018-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N0Ts5rc_Ou8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">[MUSIC PLAYING]

ROMIN IRANI: All Right.
Good afternoon, everyone.
Let's start today with the
instructor-led training
on Cloud Vision and
natural language API.
So my name is Romin Irani,
and I'm a Google development
expert for the cloud.
What we are going
to do today is learn
about the cloud machine learning
APIs that are out there.
But before we begin,
it's important
that you get a context
in terms of what
is offered when it comes
to using machine learning
on the Google Cloud Platform.
So what you see
over here right now
is a spectrum, where
on the left hand side,
you are seeing TensorFlow.
So those of you who are
well-versed with machine
learning, understand
some of those concepts,
wish to write your own
models and train them,
and you are
comfortable using that,
you could actually go
towards this site--
write your TensorFlow models.
Or on the other
hand, even if you
want to host these
TensorFlow models
in a fully managed service,
which is Cloud Machine Learning
Engine, on TCP, you
could do that too.
However, the focus
of this session
is going to be on the
machine learning APIs
that are just a REST call away.
But behind the scenes, right--
I mean, it would
look pretty simple
that it's just a REST call.
But they are actually powered
by fairly powerful models,
and there are
different kinds of APIs
that are available depending
upon what you want to do.
So what you have is
Cloud Vision, speech,
the natural language,
translation,
and the video intelligence APIs.
You might have even seen the
demo of the video intelligency
API [INAUDIBLE].
Now all of these are
available via REST APIs,
but what we will be focusing
on today are these two APIs--
so the Vision API and the
Cloud Natural Language API.
So the format of this session,
the way we have structured it,
is that we'll go through some
of the basics and features
that these two APIs provides us.
And once you see some of these
things working a few demos,
then you'll go to codelabs.
And in those codelabs,
you'll try out
both the Vision API as well
as the Natural Language API.
And while there's a prerequisite
that you could use your Google
Cloud account for using these
or finishing up the codelabs,
we also got accounts to
distribute for the duration
of this session.
So let us know, and we can
share some of those credentials,
so you could get
going either way.
Yeah?
OK.

So let's focus first
on the Vision API.
If you look at an image--
if I show you an
image, it probably
is going to be easier
for you to maybe detect
what's in the image
from various angles.
What are the objects,
or colors, or maybe
is it a known landmark, or
are there faces in the photo?
It could be all sorts of things.
It's quite easy
for you to do that.
But for the computer, this
is one of the tough problems,
and the Vision
API is pretty much
geared to address that for you.
So a single REST call
away, and it gives you
a wealth of information.
And what is that information?
But before we look
at that, let's first
understand a couple of examples
of how it is being used today.
So maybe that could
trigger a few ideas in you,
and probably that you
can take it from there.
So the first one
is Pete's Dragon.
And the whole idea
of this application
was that it's an
app from Disney,
and they tell you to
find some photographs.
So they give you a clue--
that take a picture of
a cupboard in your room,
or a chair in your
room, or a table,
everyday objects in your room.
So it's almost like it's a game
that you play step by step.
At every step they ask you
to take a picture of a clue
with your mobile
phone, and then it
validates for you whether you
actually took the right picture
or not.
And it does that by
superimposing the photo
of Elliot, the dragon.
And the way it does
it behind the scenes
is it takes your image
which you captured.
It runs it through
the Cloud Vision API,
and there's a feature
in the cloud region
API for label detection.
And it finds out whether, OK,
is that an object of that type
matching?
If yes, it's right.
And it superimposes the
Elliot dragon photo on it.
Another example that you
see is of a mobile app
from realtor.com,
and probably this
gives you a hint,
to a large extent,
that the user is taking a
photograph of a real estate
listing.
And one of the features,
again, of the Vision API
is the whole OCR, so
it sort of finds out
the text in the image.
So in this case it
will find out what
the advertisement
is about, the phone
number, or the exact listing.
Then it goes, hits the back end,
finds out the listing details,
and then shows it
to you, so that you
get a better information for
what that listing is about.
So these are a
couple of examples.

Now what does the
Vision API do for you?
So multiple things-- and
a few of these things
you will be trying out
even in your codelabs.
So one of the things it
does is label detection,
and label detection is
used to sort of classify
the image in terms of
what are the objects that
have been found?
What is the image about?
So it gives you a
list of various labels
given by priority.
The other thing
is face detection.
If it finds any humans faces,
it's able to identify that.
Some of it's detection.
It's not recognition.
And when it gives you
information about detection
it also tells you
where the faces
are in terms of in
the photo, including
features like where are
the eyes, the nose, all
of those details--
plus even things like are
people happy in the photo?
Is there joy, is their
sorrow, is their angular?
Any of those
expressions also, it's
able to detect to
a large extent.
You saw an example of
the OCR, but there are
a couple of ways it does this.
One is when it comes
to large street signs
and things like that,
that detects the thing.
But even when there are
documents or images where
the document has a lot of text
in it, quite dense information,
it will be able to even give you
based on blocks and what text
it found at each level.
So it does it just for
simple street signs
but as well as dense
text documents too.
It's got a feature for any sort
of adult content detection.
It's also got
landmarks, so if it
finds any popular
landmarks across the world,
we'll see an example.
It's able to even
tell you that yes, it
is this landmark, including
the latitude, longitude
information of where you
would get that landmark.
And it also can help you
recognize famous logos.
So if it finds--
even if you've given
an image of a logo,
it'll probably predict
quite accurately
that this is logo for, let's
say, one of the car names
that you know--
things like that.

OK, so let's take a look now
maybe some of these features
that we talked about.
So whether this [INAUDIBLE]
looks at labels,
it does face detection, it
does landmark detection.
And at the end of the
day, what really happens
is that you make a REST
call, and in the REST call,
obviously you would
ask for what is it
that you want the Vision
API to do for you?
So in this case,
you're looking at--
you send JSON a
request that says
I want to do face detection,
and here is my image.
So you could either
upload the image,
or you could even point
it to a GCS, Google Cloud
Storage, link, and
it'll help you then
start processing the image.
In this case, for the
face detection feature--
and that's common for most
of the other APIs also.
It gives back a JSON
response, and you
can see some of the
rich information it's
providing to you in terms of--
like, for each of the
faces that it found,
obviously it's condensed
over your for one example.
But things like even headware
if someone is wearing a cap,
or is there like joy
likelihood, which I mentioned?
There's also a confidence
factor that's out there.
And by landmarks, as I told you,
there's just one snapshot given
you of a left eye, but if you
run it on a sample image--
and you're free to try that when
you do the codelabs-- is you'll
see a left eye, right
eye, nose, ears,
everything is sort of
mapped out for you.
But again, if you look at the
calls that you need to do,
it's constructed JSON requests.
Make the REST call, and you'll
get back a JSON response.
And you can either deal with
the JSON response as is,
but there are also
client libraries
that are available
that will help
you just probably partial and
martial across to the JSON
objects.
That makes it much more simpler.
And you could call
this from anywhere,
so that could be even a web
application, or a command line
app, or an Android app.
It's pretty much wherever you
could make a REST call from,
you are good to go.

Let's look at
landmark detection.
So in this scenario--
hopefully, if you're
from Bangalore or even a
device, you would probably
recognize this landmark.
So we just passed
the image to it.
And if you noticed, there
are a few key things
that are important here.
One is, of course,
that it identified
what the landmark is.
The other thing is also about--
if you see the latitude,
longitude that it provides--
and when you'll
try out on the web,
you'll see that it superimposes
the Google Maps thing on it,
so you can actually
see an aerial view also
of the landmark.
And the thing that
you see over there,
the MID that points to the
Knowledge Graph API in Google--
so potentially, if you wanted
more information for it,
you could even then search
the Knowledge Graph API
and get the information
on that object.
So that's face detection,
landmark detection.
And there are some
couple of more features
obviously over here.
I'll cover a few of them.
But just maybe so that
you know it's not just
about label detection.
It's not just about
just face detection
but many other things, for
example, document text.
I gave you a few examples
of how to use it.
Croppings is
interesting, in a sense,
that when you provide an
image to the Vision API,
it can even tell you what is
the main object in the image.
So if you've got multiple
objects and maybe one of them
is a dominant object
in the picture,
it can even point out that
this is a dominant object.
And by croppings, it
actually can give you
a bounding area around
which you could pull out
the rest of the image.
Web annotations, it's an
extremely cool feature.
We'll get to it in a moment.
It's actually not just about
telling you about the labels.
So for example,
let's say that I show
you a picture of a building.
And if your tell the Vision
API, OK, tell me the labels
that you find in
this image, it'll
probably tell you
it's a building,
it's got flags,
things like that.
But can it tell us more?
Is this a building that
it knows about basically
from its whole database
that it has built up.
Are these images on the
web anywhere, right?
Can it form some
relationship somewhere
from the Knowledge Graph?
The answer is yes, and we'll
take a look at that example.
All right, so just focus
first on the image right now.
Anyone has a clue what this is?

Of course it's a building.
It could be somebody's house.
These are some of the
labels that maybe you
could come up with.
Maybe you're also
seeing some sky,
you're also seeing some trees,
things of that nature, right.
So that's the label
detection part,
but what about the
web annotations
that I was talking about?
Can it tell us something
more about the image?
So see the kind of
information it returns.
So it says this is a building
in Lands End Bandra in Mumbai.
Probably some of you missed
recognizing the image now.
And it's also
telling you a score
by which it fills like what is
the probability-- the higher
score, it's much more confident
of what it's told you.
Take a look at the next
thing that it finds.
Hopefully, this
gives most of you
a hint now about what it is.
It doesn't stop there, or we
don't want it to stop there.
You've got Bollywood.
You've got [INAUDIBLE].
You've got Bangalore.
It's quite interesting,
right, to get
this kind of information.
Now again, for each
of the entities
that it finds that is linked up
into the whole Knowledge Graph
thing, you'll also find that
it's linking to the entity ID.
Now these entity IDs,
as I mentioned to you,
is the same Knowledge
Graph that you
can query to get more
information about it.
So this way, it's
a great way for you
to even find out
more information
about the image that
you're possibly looking at,
not just the label
detection features.

Now web annotations gives
you something more, not just
about the information
you saw right now.
And then I'll tell you
how it could be useful.
So the same image, it
gives you three other types
of information.
One is that are there
any similar images
found, which is about
pages with matching images,
or is there a
collection of images
that it knows about where
exactly the same image is
there.
Somewhere else on the web
that's the full matching images.
And there are also
partial matching images,
which is that images
of the same object.
So for example, the Monet
Building, not the same image,
but similar images maybe
taken from different angles
also present in the
links given before.
So one is to say, OK, it
gives you this information.
Now just think about how you
could potentially use it.
So one of them is
that you could check
if this is an original image,
not again there anywhere else
on the web.
So that's one way where if
you've got a bunch of images,
you want to make sure
it's original content,
original image, you
could run it by this API
and also get if there
are any matching
pages, matching images,
and things of that sort--
to detect if you are
not, like, inadvertently
taking an image, which
is not allowed for usage.

OK.
Now of course, you could try
these things in your browser.
We'll let you do that when you
are doing your codelab also.
So codelab-- as well as you'll
find that it's pretty much
available to you by going to the
respective product pages itself
and getting the information.
So try this out in a
[INAUDIBLE] but be aware
that you could even
see the whole thing
work by simply going to the
product pages for Vision API
and uploading your
images over there.
And it gives you
all classifications
whether it's labels, whether
it's web annotations,
whether it's any other kind
of content including the JSON
response.
So maybe you could understand
what kind of fields
you may need to parse out.

Now let's look at the next thing
which is the Natural Language
API.
Now this is another
interesting API,
and it's more to do with how you
deal with English text or any--
it supports several languages.
But for example, if you are
given a bunch of sentences,
what could you possibly
understand from that?
So maybe one of the things--
and some of you are aware,
you could do some sentiment
analysis on it.
You could also maybe
extract out entities,
but let's understand what
this set of sentences
is talking about.
You could even do individual
sentiment analysis
on each of the entities.
So we'll take a look at
several of these examples
and just see the
whole thing work.
And there is, again,
a codelabs, which
you can try out to see the
whole thing work in action.

But let's understand first,
again, how one of the customers
has used this
Natural Language API.
So again, what this
client or customer
of the Natural Language API does
is that focus on the top right.
So they've got
these widgets which
you could put on
your pages and maybe
ask for very quick feedback,
whether you like the service,
or how do you rate us about a
particular service or a page?
And you will find that,
obviously, one of the things
you could take as feedback could
be just a rating from, say, one
to ten or one to five, but
maybe that doesn't give you
too much information.
Like, what does a tree mean?
What does a formula mean?
Instead, if you see, they've
also got a free form text
where you could potentially
give your sort of a review,
and in the review maybe you
could write something about it.
OK, the pricing is too high, but
the product was shipped well.
You could possibly
write sort of a review.
I'll show another example where
I've taken a review of a hotel,
and they have described the
different services that they
liked and did not like.
And we'll see how it does.
But in this case,
now the challenge
here is that how do we
identify all the entities that
are present in the feedback
that has been given, right?
So maybe one of the sentences
is complaining about the price,
or one of the sentences is
talking about the shipment
process was pretty good--
or some other categories,
which you see over here,
about usability, or
any of the other things
that you might want to classify.
Now using the Natural
Language API, the way it works
is they use that.
They analyze the
text of the review
that the user has entered.
They extract out what entities
are being talked about.
And for each of the entities,
the Natural Language API
also gives you a
sentiment in terms
of a score and a magnitude,
which we will see in a while.
So they use that information
to then classify it accordingly
and sort of give
this information.
So just think about it
where you could easily
bubble up which areas of
your company or organization
are having potential
problems-- or even otherwise,
that people are talking a
lot of positive about one
particular department or one
particular kind of service.
And by visualization,
you could just bubble
this up or even notify the
right departments or people
in a much [INAUDIBLE] manner.
Again, this is all being driven
just by the normal reviews
that we also write at some
of our sites or services
that we use.
And again, I'll
show you an example.
You will also be doing a
codelab, where you can either
experiment with the text that
has been given in the codelab
or do try out your own review
and see how it all does it.
And to understand the things
that it does, maybe let's
take one of the
statements over here.
And how many of you
are Harry Potter fans?
Quite a bit.
And how many of you know that JK
Rowling has the same pen names?
Maybe some, right?
OK.
So again, over here,
right, maybe just look
at the statement
as-is, and maybe you
could think of some
of the entities.
By entities, let's
think of maybe nouns,
proper nouns, locations.
Is it a work of art?
Is it a landmark?
Is it a monument?
So there are various fixed set
of types that it can identify.
And at the same time, can it
identify any similarities,
and can it give some additional
metadata also in case
it knows about it?

So look at it now
in this fashion.

So this it has
correctly identified
when you pass it to that.
All these three things
refer to the same person,
and it's pointed out
the same Wikipedia.
So this is additional
metadata that
has brought an ID into
the Knowledge Graph API
as well as a link to
a potential Wikipedia
entry in case it is there.
The other thing is
it's identified--
so just observe the name.
It's observed the type and
then of course related method.
So in that case that's a person.
In this case,
British is a location
and some information on that.
And Harry Potter is
obviously a work of art,
so it's identified that as well
as any other related metadata
for you.
And you could try out also
with your other things
in the codelab, and you
would find that it's
able to classify it quite well.
But this is not the
only thing it does.
Like I mentioned, one is to
extract out the entities.
The other thing is also to
understand an overall sentiment
for a sentence.
And again, don't
mistake this for--
like, if I give
your paragraph which
has multiple lines of text,
would it just give you
an overall sentiment,
or can it also break up
the individual sentences and
give entity sentiment analysis
at each level?
Yes, it can do all of
that, but right now
we are just looking
at a statement that
said the food was terrible.
I will not be going back.

Now just by looking
at the statement
and maybe understanding
it, probably you
know there is a bit of negative
sentiment in it in in any case.
Now the way it
returns sentiment is
a couple of these attributes
which you see over here.
The first thing is the
score, and the score
is a value that ranges
from minus one to one.
And 0 is neutral.
And if it's really
negative, the score
would tend towards minus one.
And if it's a really
positive comment,
it would tend to go
towards plus one side.
And magnitude is
just the amount of--
like, the way you amplify
it in terms of how
the statement is constructed.
So in case the
sentence is longer,
and there's a lot of angst
probably on it or even
appraise-- the other way also--
you would see this value
range from somewhere from zero
to infinite.
But typically, think of it that
the score is minus one to one,
zero is neutral, and a
little bit towards negative
which starts showing
a negative sentiment.
But remember this is for
the entire statement,
but let's say if I gave you
a bunch of these statements,
you could even see the sentiment
for each of these statements
too--
and including the objects.

Analyze syntax, this
is the other feature.
Well, let's focus first
on the morphology.
So morphology
typically is the way
the words are structured,
dependency of one
word over the other, so that
you understand the meaning
of a sentence much better.
You understand how
objects are really
related instead of doing a
word by word substitution.
You need to understand--
like for example,
it could mean different.
For example, the word
meeting is, like,
are you talking about
the meeting that we
are having right now, or are you
talking about meeting someone?
So it could be different, the
meaning of that, depending
upon the objects involved and
what your overall statement is.
So it gives you that aspect.
The other thing is it
also helps you understand
the different parts of speech--
extracts all of that
out whether it's
like indirect color
which you see--
a noun, a verb, a pronoun.
There is also the thing
about the parse label
where it tells you--
like, if you look at
the statement carefully,
it tells you what
is the root thing.
It's about helping, right,
which is a nominal subject.
It's the API that's
helping whom?
Us.
So if you start setting a bit
of grammar, probably all of this
starts to make sense.
And this is the entire
structure that's
written to you in JSON,
where you can potentially
look at any kind of information
that you're interested.
So one of the demos that
I can show you what we did
is that all the GDD in the at
tweets that were happening,
we just used the Twitter API to
get a real time feed of the GDD
tweets--
sorry, GDD India tweets
that were happening.
We used the Natural Language
API to parse out the sentence.
We tried to search that--
are there any
adjectives that people
are talking about the event?
And if there are
adjectives, let's group them
and see what are the adjectives
that are most talked about as
far as the event is concerned.
So that's the kind of stuff
you can potentially do.
This might-- when
you look at it,
probably you'll feel that,
OK, but how could I use it?
This is an example of how you
could potentially do that.
And I'll come to that.
But there's one last feature
which is recently released,
and that's got to do with
content classification.
Now all you did is you passed
just that same statement,
and it has got a list
of over 700 categories
in which it can classify
your content automatically.
So in this case, it just
classified it correctly
with a confidence
score in terms of that
it's to do with children's
literature or children's
interest.
This, it did it on its own.
You didn't give it any hints
apart from just the text
that was there.
So if you've got, say, hundreds
and millions of documents,
and you just want
to know probably
what is the categorization
of these documents?
Give it a try, and it could
then even give you that info.

So this is the demo that we had
on-- like I mentioned to you--
that we just search
for all the--
not search, but it
was a real time feed,
so as the tweets happen, if
they contain the #GDDIndia tag,
we will simply run
it through the NL API
and dump it into BigQuery.
BigQuery is the whole--
data warehouse available
in the cloud from Google,
and you could potentially run
it on large amounts of data.
Of course, we don't have that
much data for a BigQuery kind
of a scenario here, but still--
what we did is just
take all those tokens,
dump it in BigQuery, and we find
out what are the adjectives,
and do that.
So let's see how we did.

So if I just show you
the table in BigQuery--
or let me maybe first show you
just a bit of code quickly.
It's not-- you know,
you would be probably
even amazed at the amount
of code that's there.
So think of it like
all we are doing is--
this is, again, not
just JavaScript code.
And we set up some initial
Google Cloud credentials.
We point out to where in
BigQuery what dataset,
what table, the
information needs to go to.
And the stuff starts over
here, if you noticed.
All we are doing
is of course you
would have comma
separated terms,
but we are searching
for this hashtag.
And why is that
[INAUDIBLE] streaming API?
If there is a match,
we ignore retweets,
and we simply go and
call our own function
to process that
text that it got.
And in that, what we do is--
this is actually
the JSON request
that we are making to
the Natural Language API.
So we are actually using the
raw JSON request response
in this case and hitting
the API endpoint directly.
So all we are doing
is we're just saying,
OK, here is the tweet text,
and could you extract,
for me, the syntax?
That means all those
tokens around parts
of speech and things
like that, because we
are interested in adjectives.
And also extract the overall
document sentiment for us.
If everything is OK, we
simply construct the role that
needs to go in to BigQuery.
So think of it-- even if
you don't know too much
about BigQuery, just think of it
like our database into which we
are inserting each of these
tweets that we are interested.
We just bring it in
a streaming fashion.
We are saving our tweet
text, the time stamp--
we are also saving
all the tokens
that we get back,
including and also
the document sentiment along
with the score magnitude,
right.
And that's about it.
And then we insert
it into BigQuery.
In BigQuery, if you'll
see, this is the table
into which all
data goes, but this
is about it, what you
have written over here,
where we are just--
and again, this is one of the
cool features of BigQuery,
where even if you
got JSON data that's
dumped into one of your
columns, you could potentially
use simple JavaScript snippets
to extract and emit out
the column data even with some
transformation if you want.
So what we do is we go
through each of those tokens.
We are finding if
it's an adjective,
and we are taking the
lemma or the root, word.
So if it's like runs, running,
or excited, exciting event,
probably it means the same word.
So we get a common
word and save that.
And then all we are doing
is simple SQL stuff,
where we are grouping by
adjective, auditing it,
and things like that, right.
So if we run this--
let's take a look again.
I have just run it at times
while the keynote was going on
and things like that.
Run the query.

This is too little
data for BigQuery,
but it'll suffice
for our purpose.
So here are the results
that we are getting,
so if you have been
tweeting about the event,
and you used some
of these adjectives,
thank you for giving the data.
All right.
OK, let me get back
to the presentation,
and, you know, we're
almost, like, there.
In fact, we are almost done.
So the next thing
we have, obviously,
is starting the codelabs, but
before we start the codelabs,
just a few things--
maybe I'll show you
one or two more demos,
so you get a better context
of the features that
are there in these APIs.
So let me show you a few
straightforward examples.
And this will also be the
way you could try it out
in the browser.
So we have got one
screen for where
your codelabs are, and we'll
help you out to do that.
But at the same
time, remember you
could go to both the product
pages for the Vision API
as well as the Natural Language
API, which I'll go through now
and just show it to you how easy
it is for you to even just try
it out.
And it's fun at times.
So I was talking--
so I'm at the Natural
Language API page,
and you'll find that
it provides you--
just put in the text you
want to analyze and simply
click on Analyze.
Now let me take that hotel
review that I was talking about
to show the power of the API.

So I've taken this
directly from one
of the reviews I found for Taj
Vivanta, written by someone,
to just see how it works.
So I'm going to
take this whole text
and just put it out over here.
And now just maybe try to
understand what it's doing.
So again, this should
give you some ideas.
Maybe you've got sort
of an application
where users put in
comments, and you
want to analyze the
sentiments of the comments,
or whatever the main objects
people are talking about,
things of that nature.
So in this case, just
look at the review.
Maybe read it for a while.
I visited the Taj.
The rooms were good.
But I want to show
you a few things.
However, the check in
wasn't very pleasant,
as there was a huge line, right.
So maybe one of the sentences
is a little bit, I would say,
negative.
And the live [INAUDIBLE]
eggs were awesome.
So there's all kinds
of things going on.
There are multiple things
that are being talked about--
entities.
Think of those words, entities.
And each of these--
and this block of text,
think of it as a bunch of
sentences, each sentence
is having its own entities,
and potentially different
sentiments around each of them.
So what does it do if
we ask it to analyze?
Take a look.

So first just look at
the-- or maybe I'll just--
here, just take a
look at the entities
that it's come up with.
And by entities, think of it
as proper nouns, common nouns,
for now, and it's
figured those out now.
And of course, with an
entity that I've told you,
it also tells you the
type of the entity.
Let's look at sentiment.
Sentiment, as I've told
you, just keep in mind
there's a score, and
there's a magnitude.
The score minus one to one.
Zero is neutral, right?
So let's look at each
of the sentences.
Of course, it gives
you the sentiment
for the entire
document too, but let's
start looking at maybe you are
interested to make sure there
are no negative comments
about your product
if you've got a product page
and people give you reviews.
Maybe you want to know, are
there any negative reviews?
And if there are
negative reviews,
what are the things that
people are not happy about?
Are there specific things?
So in this case,
you can easily see
that the text mentioned
that the check-in wasn't
pleasant, blah, blah, blah.
So that sentence
is probably saying,
yeah, it's a bit negative here.
Similarly, you know the higher--
or the more positive statements
around maybe the food
they liked--

things of this nature.
It's pretty nice.
Now look at this.
This would have been awesome,
but sadly it wasn't there.
So it's a little bit negative.
It's not, like, becoming
extremely negative too.
So think of it this way, maybe
apply it to your products
maybe if you're having a
product page with reviews,
or you just want to
analyze a block of text.
Another example could
be you got emails,
and you want to analyze, at
times, for training purposes
or for any other purposes
that which emails
possibly had people who
are probably not happy when
they wrote that email?
What were the things
they were unhappy about?
There are multiple things.
Hopefully this triggers
some of the ideas in you.
And of course, syntax, you get
the whole dependency graphs
and other things.
And remember, this
whole dependency graph
is also given to you
as a JSON response,
so you could then maybe
pull out some adjectives
like we saw in the Tweets demo.
So that's natural language
API you can try out.
The other thing, Vision
API, I'll let you try out.
But again, same, you go
to the Vision API page,
and you could easily drag
drop any of the images
from your machine.
So I'll take a
photograph I have here.
I'll add these for
individuals anywhere.
Thank you.
OK.
And you'll find that all of
them were extremely happy,
so it's even identified
the joy emotion in each
of these individuals.
So thank you.
OK, all right.
So let's get going
with the codelabs.
And we have a few things--
you know, just
homework to do before--
by homework, I mean
just a few checks
before you can get
going with them.
So there are two codelabs.
Let's start with the
Vision API first.
So in the meanwhile,
while some of these things
are getting set up, maybe
spend about five minutes
or something like that
and take a look at--
visit these product pages.
And if you would like, take
an image from your machine,
because as long as you
have net, you can use this.
The codelabs, we'll
get to it in a while.
Either the vision page or the
natural language page, just
open up in the
browser, and you'll
find that form where you
can maybe upload an image.
See the kind of
results it gives.
Always remember
the results would
be with a confidence
score or probability,
so the higher the score, that's
more accurate as far as it
works.
[MUSIC PLAYING]

I'm just going to go
to the Vision lab.
So if you've been logging
in, and if you logged in
with a new account, let's
focus first on the Vision lab.

OK, everyone just focus here
for maybe five, 10 minutes,
so you understand what Google
Cloud Console project is about,
and then how do you enable the
APIs, and things like that.
So if you are already
familiar with it,
maybe you could move on.
But for those of you--
just a few things
will help here,
so that the second lab
also can get done quicker.
So I'll assume that
you've got your accounts.
I'll assume you've been able
to log in to the Cloud console,
and that you'll see a project
that's set up for you, right?
So that completes this
step around making sure
that your project is ready
under your account in GCP.
Now the next thing you need to
do is that, for your project--
by the way, there are quite a
few APIs that are available,
but for each project, you
need to enable them first.
So when you say
enable it, you'll
need to go to-- you
could do it with me,
or you could even do as per
what you see in the codelab.
So you have to enable
the specific APIs
that you plan to use.
Now in this lab, we are
using the Vision API.
Similarly, in the
next lab, we are
going to be using the
Natural Language API,
so you'll need to enable that.
In this case, since
it's Vision API,
I'm logged into my
Cloud console over here,
and I'm under my sample project.
So from the hamburger
menu, you can see something
called API end services.
If you click that
and either go there--
but then within
library, I'll show you
something that makes
it easier for you
to sort of zoom in or navigate
to the specific library.
So go into the library--
what you see over here.
And for example, if you
just type the word, machine
learning, it nicely
shows all the APIs
that you could enable
for your project.
So in this lab for
the Vision API,
you'll see the Google
Cloud Vision API over here.
Just click that.
In your case, if
you've not already
enabled it, which might be
the case for your project,
you'll have an Enable button.
In my case, I've enabled it.
That's why it shows
it as enabled.
But simply go through the steps
that I mentioned in the codelab
over here, where it tells you to
visit that particular library.
And click on the Enable button.
That will enable the Vision
API for your project.

Sounds OK till now, everyone?
OK, all right.
Once you're done with that,
there's a very interesting--
one of the very nice features
of Google Cloud Platform, which
is called the Cloud Shell.
How many of you are aware
of the Google Cloud Shell?
Quite a bit, OK.
But for those of
you-- just think of it
as your development
machine in the cloud.
There's a VM that's
provisioned for you--
a lightweight VM
of course, but it's
provisioned for you with most of
the developer tools already set
up for you.
So if you just want a VM
quickly, be at the terminal,
maybe have node,
Java, Python, Docker--
all these sort of
tools are available.
Google Cloud Shell is
an excellent feature.
So in fact, in both
the labs, we are not
expecting you to do
this from your desktop,
or from your laptop, or your
local machine, in short.
All we're saying is let's launch
this VM on Google Cloud Shell,
and let's do
everything from there.
The beauty of that is that
the credentials are already
set up for you the moment
the Cloud Shell comes up.
So that's the next
step you can try,
and you might sometimes get
lost as to where this icon is.
So once again, just
notice I enabled my API--
the previous step--
the Vision API.
And in top bar here,
see this icon over here?
You'll see Activate the
Google Cloud Shell, right?
Just click that, it'll
probably take a minute.

What it's doing
behind the scenes--
and if you visit the
Cloud Shell documentation,
you'll find that it
provisions the dev
VM for you and with
some amount of space,
and most of the
developer tools set up.

Be a bit patient at times.
Everything should come up.
And once you are done,
boom, you're right there.
And this has actually
even-- since you've
been in your active project and
then activated the Cloud Shell,
it would have made the
connection that which
is the project that
you're working with.
So the project IDs
is what you will see.
Also, everything
is set up for you.
Now, let's look
at the next step.
So just make sure the Cloud
Shell has come up for you.
How many of you have been able
to bring the Cloud Shell up?
OK.
For those of you, if it's still
provisioning, just be patient.
It should all come up.
OK, now the next
thing is that now--
now think of where you are
running this application from.
So you've got a VM,
you're right there,
you want to write
some code, or you
want to probably hit
some kernel commands.
And you need to invoke the API.
So you can't just go ahead
and invoke the API as is.
There are multiple ways.
You could have an API key, or in
case it was a web application,
it could have also been
an overt kind of scenario.
Or if it's a server
to a server thing,
you have something
called a service account.
But for our case right now
here, all you are doing
is you're going to hit
it by the Call utility.
Everyone familiar with
cURL and just making--
or Postman-- similar
kind of thing
if you use those
plug-ins, right.
But we'll be using cURL.
And again, cURL is all
set up for you as far
as the Cloud Shell is concerned.
So all we'll be
doing is-- when we
hit the endpoint, one
extra parameter we
need to provide in the request
parameter is the API key.
And this step in the codelab
is about provisioning,
or generating, the
API key for yourself.
So just go through
it step by step,
or you could even see
where I'm doing that.
So all I'm doing is
going to the API services
again and credentials, but
it's there in the codelab.
Just follow it carefully.

And I've already generated
a key for myself,
but you'll see
that when you type,
you have these three options.
And the one we are going
to be using is the API key.
You could click that, and
it generates an API key.
Please just copy that,
because the next step is then
about how do you file these call
commands and set the API key.
That's what we'll be doing.
Just follow the steps slowly.

And remember this is a very
important command, which
you see down, which is you're
just setting an environment
variable.
So in your Cloud Shell
just use an export.
For those who are familiar
with maybe a Windows
thing, it's set, but since we
are on Linux here, it's export.
And don't make a mistake
with that underscore
and forget the space.
Just keep it as this, so API_key
equal to whatever value you got
from the credentials screen.
How many of you have
used other external APIs
from various web services--
public web services,
and you would
have known the concept of an
API key that it gives you,
this is exactly the same thing--
where we have a
project provision.
We've got our
Vision API enabled.
And now, since we
are just calling it
from somewhere outside, we just
don't want anyone to invoke it.
So we have enabled the
credentials for an API key,
and that's what we are
passing in the request.
That's how it authenticates.
Once you get your
API key set up--
now this is another
thing that exposes you
to some elements of the
Google Cloud platform
if you have not used it.
So they have a concept
of a storage service.
It's called Google
Cloud Storage.
Now, the images that you will
be passing to the Vision API,
we are going to be uploading
some sample images that they
are making possible over here.
So we are going to be using
the Cloud Storage service.
Again, this is a service
you'll find in your console,
as it's pointed out over here.
So just take us--
I can show you over here.
You can go down to the
storage area and storage.
Alternately, a quick
way to navigate
would be by just
trying out from here,
and you could build
it to that also--
either way.
Or navigate through
the hamburger menu,
at least you'll come to
know the tons of services
that are there.
But go under the
storage section in case
you're not able to locate that.
I'm just pointing out over
here, it's under storage.
And you'll see storage,
just click that.
So that's where
we are right now.
Please go to the storage.
Create a bucket.
A bucket is nothing
but a sort of--
think of it in a very
loose way like as a folder
or a collection of your objects.

So create the bucket.
It'll just be like a container
folder for your objects.
And the objects, in this
case, will be our two files
that we want to upload for
analysis by the Vision API.

You'll also see some
interesting things--
how we are making it
available from a public angle.
So I have multiple
buckets here to do that,
but if I go to the
particular bucket here--
hopefully you should
be at this stage.
Once you are able to
create your cloud storage
buckets successfully--
and just click on
the Upload Files.
Take a look over
here in case you're
having difficulty
finding where that is.
It's here, Upload Files,
just click that and upload
your files from there.
They'll get uploaded here.
And all you need to do to make
it public is simply check mark,
or enable, the public link.
That's a cool way of even making
many other documents available,
in case you want, to the world.
OK, let's go fast if possible.
I understand there could be
some slowness in network.
Also, there are
quite a few things.
I mean, probably you've
not reached the Vision API.
If you'll look at, we are
at Google Cloud platform.
We've learned about the project.
We have learned
about enabling APIs.
You've learned about
the Cloud Shell.
And you've now
started to figure out
another service, which
is the cloud storage,
and yet to come
to the Vision API.
But it's good.
You're at least
getting introduced
to a lot of these
other services.
Have you enabled it
for the public link?
Yes, no, available?
If you punched that,
you are in the browser.
You should be able to
see your image, yes?
Try it.
It's the same thing like
accessing any other public URL.
OK.
So the next step is to create
your Vision API request.
Maybe don't worry about
the Code Editor for now.
There is also a code editor,
by the way, in the Cloud Shell,
if you want to use that and are
not comfortable with the VI.
So how many VI users here?
OK, how many of you
know how to exit VI?
Not bad.
So I think we are good.

All right, so think
of it this way.
The Vision API is a
public REST API available
for you to invoke.
So you need to just
compose the REST request--
a JSON request-- so not REST
request, a JSON request.
And it's straightforward,
if you see.
All you're seeing is
what is the image I
want to pass to the Vision API?
And what are you providing?
You are providing the
GCS image URI, right?
So think of it like--
again, this is a notation,
so it's Google Storage
and then your bucket
name slash the filename.
Remember this is
a request JSON you
need to create on your machine.
Please do not do
a copy paste as is
unless you have named
your bucket the same,
which will not be the case
because of uniqueness of bucket
names, so please
figure out what's
the exact bucket name you have
typed and modify the request.
Otherwise, you will
face this issue
that you're not allowed
to access that bucket
and things like that.
And what are the features
you want from the Vision API?
All you want is just
give me some labels out.

So please create this
request.json file.
And see the node, right?
Pay special attention
to this node.
This is where you
could possibly trip.
Use your own bucket name and
modify that request.json.
Save it in that respective
folder on Cloud Shell.

Once you are done
with that, it's
all about making this cURL call.
You're using now
just cURL right now--
a utility that helps you make--
for example, in this case,
we are doing an HTTP post.
Again, don't worry too much
about all the parameters,
but they should be
somewhat readable--
that we are making a post HTTP--
invoking the post method.
And then we are saying that
the request is a JSON format.
And see this add request.json?
That means the request JSON
is present in this file.
And we are invoking
the HTTP Vision API.
And look at the last parameter,
which also passes the API key.
Make sure that you have done an
export of the API_key varaible.
Otherwise, you might end up
with a permission denied.

If all is well, you should
get some response like this.
In my case, I have a whole
cURL request ready here.
I'll go into my Cloud Shell.

I'll go to the demo folder.
I'll set in the API key.
Sorry.

If you want, you could
even take a look.
I'm just setting the API key.
Oops.

And I've got this entire set
of JSON that I've got ready.
So if I do a Vision
label request,
you'll find it's the
same thing, which
is that there's a
donuts.png image,
and I just want the
label detection.
So all I'm going
to do now is file
this cURL command, which you
also have in your codelab.

And I get back the results,
as you can see from the API.
It's telling me that
there's powdered sugar.
There's da-da-da-da-da.
And the image also looks--
like, if I go to my bucket--
hopefully, you already
seen the image by now,
but if I see the public link,
that's the close-in image.
So you passed this image.
So what we did is
we took this image,
we put it into the GCS bucket,
we passed it to the Vision API,
and we said, can
you tell us what
are the labels that you find?
And we get the required results.
So how many of you have
now at least made a call--
seen some results come up?
OK, good.
So the next two or three
things which you'll see,
this is for the label detection.
But if you go to
the codelab quickly,
you will find that the next
things are about the whole web
detection, which are the
matching URLs, pages, partially
matched pages that
you see, or images.
And there's also the face
and landmark detection.
You could try out one of them
maybe in the interest of time,
or just try probably a face
detection API or the web
detection.
Whichever interests
you, go for it.
So this is a sample image
that has been provided.
And that's the selfie.png.
Or you could even upload any
of yours into the cloud bucket
and try it.
You've got about
20 minutes to go.
And so, let's try out maybe--
how many of you are done
with the first thing
of doing the label
detection successfully?
OK, let's keep at it.
A few things to check-- make
sure the bucket name is right.
Make sure the file
names are correct.
Make sure the API_key
environment variable is set.
And then of course, make sure
your cURL command is correct.
You should get, sometimes,
the error response
if permission is denied.
Maybe the hint is
probably something
to do with the API key, or
the bucket name is wrong.
It belongs to someone else--
things like that.

How many of you started with the
face and landmark detection--
beginning to do that?
OK, just keep at it.
We'll probably give this
another five minutes
to try out the labels and maybe
the face detection at least.
And of course,
12th step is about,
just you could
explore other methods.

And you have got one
more codelab, which
is the Natural Language API.
Again, that explores
multiple methods.
Whether it has to do with
analyzing syntax, or analyzing
entities, or the
sentiment analysis,
you could try out
any of those also.
But we'll get to that.
Let's give another three, four
minutes to try and at least get
your label detection right,
or the landmark, or the face
detection.
Remember it has
multiple features,
so it's either labels, face
detection, landmark detection,
croppings, web annotations.
You could even maybe try an OCR
if you want to, but of course--
everything has a different value
for the features that you want.
If you observed the JSON
across all of these,
you will just find a
difference-- of course,
you are pointing to
the URL for the images.
But the only thing
that's changing
is, what are the
features you want
the Vision API to give you?
[MUSIC PLAYING]

OK, so this API we're
just seeing right now
is the Vision API, but there is
also a cloud video Intelligence
API, which processes the video.
And now it goes through the
thing, goes through each
of the frames, it tells
you which of the object's
present at what
point in the video,
and labels also at that point.
That's a different API.
It's not the Vision API.
Maybe there is also
a demo, if you'll
notice, in one of the boots.
But if you go to the
machine learning APIs--
so just like there is the
Vision API and Natural Language
API of the Speech
API, then you'll
also find the video intelligence
API, a separate one.
But it functions in
a similar manner.
You have to provide
a URL to the video,
and it processes it from there.

I am not sure of all
the things it can do,
but what I have seen it do
is almost on the same level.
It detects the objects.
It detects the labels
basically, and at what point
it's happening.
So as the video
is playing, you'll
see it's a very cool demo,
where it's showing you, OK, it
found at what point.
By that, I mean at
where in the video,
so you can directly
zoom to that.
So potentially if you have
a security scenario where
you have a webcam, and you want
to just take the whole feed
and say, let me know if
there were any people?
And it would probably point
you to the right location
in the video, where you
can directly [INAUDIBLE]
and get it.
The next lab is this one,
entities sentiment and syntax
analysis.
Again, just-- it
says 20 minutes.
We have 15 minutes.
Maybe you are fast.
You could do it.
So again, it's all about taking
a piece of, or a bunch of,
sentences and then
analyzing it for syntax
or entities and
things like that.
Again, what we are
going to do here
is most things you already
got in place now, right.
So this lab again
progresses in the same way.
You will find that you already
have your project in place,
so you don't have to do
anything special there at least.
Your project is already there.
This step is important.
Please do that.
Just like how you
enabled the Vision API,
please go back to the APIs
section in the console
and enable the Cloud
Natural Language
API in the same fashion.

Maybe your Cloud Shell
is already activated.
If you wish, you could
generate a key again.

Absolutely make sure that you're
clear on enabling the Cloud
Natural Language
API and also ensure
that you have the right key in
place setup, export, API_key,
that's all done.
Sorry?
AUDIENCE: [INAUDIBLE]
ROMIN IRANI: This
is for the project,
and what you've enabled.
There are also other fine
ways you can fine tune it,
but right now it's the project,
and the API's enabled for if.
You just issue the
thing against it.
You can also restrict
in many other ways,
but for now this should do.
AUDIENCE: [INAUDIBLE]
ROMIN IRANI: I believe
the previous one
should work just right, yeah.

So remember you're going to
be using the same project.
You're just enabling
the API there,
and you're going to make a
similar kind of cURL request
for the different methods that
are available in the Natural
Language API.
So whether it's finding out
what entities are there,
or finding out a
sentiment analysis,
or even analyzing the
syntax of partial speech.
Just pick one of the things that
interest you maybe and do that.
All the calls are in a similar
manner, but just take one.
Maybe if you want to see
partial speech, try that.
If you want to see
sentiment analysis,
try the step number nine--
things like that.
And 11th is also
interesting, where
you are passing a bunch of
text that is Japanese language,
and it's able to
detect the entities.

So try any of these methods.

So I could do the
sentiment analysis thing.
Just pass that.
Analyze sentiment.

[MUSIC PLAYING]

We have about 10 minutes.

I'm just going to try out one
other sentiment analysis one.
And in that, I'm just
passing this text.

Is the best book,
everything should read it.
Pretty positive sentiment
or negative sentiment?
What do you think?

Positive?
Somewhat?
Or actually more.
So let's pass this request.

And what we have is
an overall sentiment.
And if you see the score is much
about zero so minus one to one.
And it's also breaking down
that the whole sentence--
basically, sorry, the
whole text that you
gave into the two sentences.
And you'll find
that the sentence
is the best book is having
a much higher score still.
Even this one is a slightly
positive sentiment.
Everyone should read it.
Nothing negative about
it-- nothing neutral--
maybe a little bit positive--
So.
It even gives you at
each sentence level.
Again, this feature's
broke down the sentences.
You could even do
an entity analysis
to see if, like, are
we talking about--
what are we talking about, and
what is the score and magnitude
for each of that too?

So this is actually
a text that's
in Japanese, which you see down.
And now we're just
going to be doing
a analyze entities on that.
And even that, it goes
and correctly gets it,
including the Wikipedia
entries for these entities.

So one of them is a person.
One of them is a location.

So yeah, we have got about--
we're almost done with our time.
I'm still around,
so we could still
discuss any other questions
that you may have,
any other demos you
want to see of this API.
And thanks once again to the
assistants who helped me.
Thank you.
And I'll speak to you later.
Thank you.
[MUSIC PLAYING]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>