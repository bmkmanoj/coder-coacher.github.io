<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Android Meets TensorFlow (GDD India '17) | Coder Coacher - Coaching Coders</title><meta content="Android Meets TensorFlow (GDD India '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Android Meets TensorFlow (GDD India '17)</b></h2><h5 class="post__date">2017-12-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Qa_0l94Tvls" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thank you for taking a time
for our system Android meets tensorflow
I'm Kaz I'm developer advocate for
Google cloud team and for the first part
of this session I directly talk you
about how you need machine learning the
power of machine learning running on
your mobile phone applications and in
the latter part Anita will talk to you
more detail about the case of wall light
which is the light weighted 2 version
specially designed for the your mobile
and embedded systems so these are the
agenda while email matters what is the
problem Lara why Amaral matters for
mobile applications for mobile
developers and this is right I have been
used already in the keynote I think to
tell the difference between AI a machine
learning a new one network so I won't go
through the PT same things my thing is
that neural network is having the
booming or the breakthrough of the
algorithms so that in a Tron 2012 we
have been seeing a breakthrough at
around in the area of neural networks
and we are spending much resources and
times on developing a new neural network
technology and yeah this is the same is
right I have used so you can think your
network as a function just like the
function as you write in the java code
so it takes any kind of data and it
gives you an output so you can replace
these cat-and-dog
images we see your own data acquired
from the mobile phone for example if you
have mobile phone you have the
acceleration sensors you can try using
acceleration sensors as an input for the
neural networks and try using neural
networks to classify what kind of
movement you are sensing with those
sensors or mobile phone so this could be
an one use cases of a neural network or
mobile and we have been using the neural
network especially deep learning
technology for over 100 production
projects now only the traditional
classic machine learning algorithms but
also the deep neural network model has
been already on production in many
places at Google
the especially for mobile use cases such
as the images image analytics or all the
OSHA or speech-to-text fix to speech
translation natural language processing
motion detection the machine learning
empower of machine learning could be so
important because it matters to lessen
the traffic and to get you faster
response with mobile applications so you
can think the machine learning could be
an one kind of the compressor or encoder
for your auditor for example if you're
taking images or photos with your camera
the easiest way or an IV implementation
could be just sending all the images to
these servers and at service you can
apply neural networks for image
recognition or Hardware under analysis
but if you could have running your
neural network model inside your smart
phones you can understand the meaning of
its images the what kind of the object
you are having in the images of camera
for example if you have the cat in the
camera images instead of sending all the
JPEG images or compressed images to the
server you can just send the text to
labels such as cat ghe to the server so
that can compress the amount of the
traffic you have to send to this server
significantly like a 100 times or a
thousand times the faster and smaller
than the original image data and you can
apply the same techniques to do any
other kind of the data for example if
you have a motion sensor data from
mobile phone rather than sending the law
of motion sensor data directly to the
server you can learn small machine
learning model running on the mobile
phone to extract a certain feature
feature vector that represents here the
patterns or characteristic of the
moistures so that you can compress the
data much much smaller so and the result
is that you can get less traffic and a
passer response on mobile applications
and to do that to implement the the
machine
or a I powered applications running on
mobile phones it may be easiest in the
fastest way is to use tensorflow
which is the open source library called
machine learning as I talk as I
mentioned in the keynote session
yesterday kisaburo is some google
standard framework for building our new
machine learning or AI basis basis
products so this is a standard play
mapper machine learning in Google and
created by Google brain team and we have
open sourced it are in tensor product or
algae in 2015
Tesla power is scalable and portable so
you can get started with down logging
tensorflow code on your laptop Mac or
Windows and try out with some sample
code and then you can move to your
models the production level use cases by
using GPU or 10 GPU or 100 GPUs chance
approach is scalable so that you can
either have to change major part of a
defensive role called to to learn the
distributed training on the large amount
of data such as terabytes of data and
another thing another benefit you could
get with tensorflow is the portability
so after training the testing for model
you can bring the model which consists
of direcor tens of megabytes of data
that could be ported to the mobile
phones or embedded systems Android iOS
or Raspberry Pi so even the mobile phone
or Raspberry Pi doesn't have any
internet connection or a cloud
connection it can learn 10 super model
inside it to do smart decisions or make
a prediction while inference on data so
first of all with those benefits cut
popularity it's the most popular deep
learning framework in the world right
now and we have many serious companies
like Airbus welcome mortgages MOBA DC is
a subsidiary of interviews who's now
focusing on building an ASIC or
customized ships running for 10th floor
or Dropbox eBay those companies are
actually using his law
but if you want to bring the technology
into your mobile applications there are
some challenges you have to face because
neural network is big compared with the
other classic machine learning models
because deep deep learning you have to
have multiple layers like a tens of
layers between input data later and the
ultimate result so that the total amount
of the parameters and amount of the
calculation you have to do is can be big
for example the inception version 3
which is the one of the popular image
classification model that requires to
have the 19 1 megabyte comedy parameters
weights and biases and also if you use
tensorflow without any changes by
default which consumes like 4 megabytes
or the binary code so if you want to
bring your mobile applications in
productions you don't want to have users
downloading 100 megabyte of the weights
and binaries when you're starting to use
your applications you may want to
compress everything into rack at 20
megabytes or 10 megabytes or maybe a few
megabytes so we have to think about the
optimization for mobile applications
things like pleasing graph quantization
memory mapping and selective
registration freezing graph means that
you can remove the all the variables
from the details of flow graph convert
it into any constants mr. Sutro usually
people have the you weights and biases
so the parameters inside neural networks
as a variables because you want to train
the model you want to train the neural
network in its training data but once
you have fish training you don't have to
have those parameters in variable you
can put everything into constant so that
by converting from variables to
constants you can get much faster
learning time removing the checkpoints
quantization is another optimization you
can you can take for the mobile
applications quantization means that I
think there's no other side ok yes I
is quantization means that you can
compress the precision of the each
various in parameters weights and biases
into a small Europe procedures for
example by default we use the SATA 2 bit
floating point numbers for representing
any weights and biases but by using
quantization you can compress that into
an 8-bit integer and by using a 2 bit
integer it must be McCann shrink the
decider with the parameters much much
smaller and especially for the embedded
systems or mobile systems it's important
to use the integer numbers rather than
the floating-point numbers to do the
order calculation such as much
multiplications and additions between
matrices and vectors because hardware's
for floating-point requires much larger
footprint for implementation so test
overall already provides here primitive
data types for supporting quantization
of parameters and operations quantizing
and d quantizing or operations that
supports the contest values so with that
I directly pass this stage to Anissa
she'll be talking much more about the
details of all right which easier new
LAN time we will have for specially
designed for the mobile applications
so hello everybody we now understand
that machine learning adds great power
to your mobile application so with great
power comes great responsibility
so armed with this power let's talk in
detail about tensorflow light and see
what it takes to build an app using
tensorflow light so you probably heard
this many times and now you already know
the tensorflow light is a lightweight
machine learning library for mobile and
embedded devices tensorflow works well
on large devices and tensorflow light
works really well on small devices which
is we built this so that it's easier and
faster and smaller to work on mobile
devices some of you who attended the
code labs earlier might be wondering
what is tensorflow mobile and what is
tensorflow light you should view
tensorflow light as an evolution of
tensorflow mobile tensorflow light is
like the next generation and this is
catered to be really small in size and
apt for smaller devices so we support
some you'll hear more about this in
detail so how did we go about developing
tensorflow light we spoke to the mobile
vision team found out what their needs
are to build a really strong
infrastructure we talked to our partners
in Android to understand how we can
leverage custom hardware acceleration
and most importantly we listened to you
guys and took feedback from our
developer community to see what should
be prioritized and how we build
something really well for mobile and
we've tried to incorporate a lot of the
feedback from all of you so we came up
with three goals we wanted to have a
very small memory and binary size so
even without selective registration we
wanted to keep the binary size small and
we wanted to make sure that the overhead
latency is also really small you really
can't wait 30 seconds for an inference
to happen by the time that model is
downloaded and processed and
quantization is the first-class citizen
we support quantization and many of the
models support our quantized models so
this these were the three main goals
tens of low-light this is the high level
architecture as you can see it's a
simplified architecture and works both
for Android and for iOS this talk will
be focusing mostly on Android today so
this is lightweight performs better and
leverages hardware acceleration if
available so to better understand how to
do how to write a model let's consider
how we build a model using tensor flow
light so there are two aspects one is
the workstation side and the other one
is the mobile side and let's walk
through the complete life cycle so the
first step is to decide what model you
want to use so if you want to use their
already pre trained model then you can
skip this step because you've already
done the model generation one option is
to use a pre trained model the other
option would be to retrain just the last
layers like you did in the code lab
earlier today or you can write your own
custom model and train and generate a
graph this is nothing specific to
tensorflow light this is as good as
standard tensorflow where you build a
model and generate graph depths and
checkpoints the next step this this step
is specific to tensor flow light is to
convert the generated model into a
format the tensor flow light understands
a prerequisite to converting it is to
freeze the graph has just talked about
freezing the graph so checkpoints has
the weight the graph def has the
variables and tensors freezing the graph
is a step where you combine these two
results and feed it to your converter
the converter is provided as part of the
tensor flow tensor flow light software
and you can use this to convert your
model into the format that we need once
this step is completed the conversion
step is completed you will have with you
what is called as a dot light file dot
light binary file so now you have a
means to move the model to the mobile
set you feed this sense of low light
model into the interpreter the
interpreter executes the model using a
set of operators and it supports
selective operator loading and only and
without these operators it so it's only
about 70 kilobytes and with all the
operators it's about 300 kilobytes so
you can see how small the minor resizes
this is a significant reduction from
what the tensor
which is over 1 1 megabyte at this point
so you can also implement custom kernels
using the API so that we have provided
and we'll talk about it in a few minutes
so if the interpreter is running a CPU
then this can be executed directly on
the CPU otherwise if there are hardware
acceleration then it can be executed on
the hardware accelerated Hardware as
well so the main components of
tensorflow light are the model file
format the interpreter for processing
the graph a set of kernels to work to or
where the interpreter can invoke a set
of kernels and lastly an interface to
the hardware acceleration layer so the
model as I've said before spare the
tensorflow light has a special model
file format and this is lightweight and
has very few dependencies and most graph
calculations are done using 32-bit
floats but neural networks are trained
to be robust for noise and this allows
us to explore lower precision numerix
the advantages of lower precision
numerix is lower memory and faster
computation and this is vital for mobile
and embedded devices this using lower
precision can result in some amount of
accuracy loss so depending on the
application you want to develop you can
overcome this and use quantization lost
you're in training so you can get better
accuracy so so quantization is supported
as the first class citizen in terms of
low light and we also have a flat buffer
based system so we can have speed of
execution flatbuffers so flat buffer is
an open source Google project and it's
comparable to protocol buffers but much
faster to use it's much more memory
efficient and in the past when we
developed of applications we always
thought about optimizing for CPU
instructions but now CPUs are moved far
ahead in the memory and writing
something efficient for memory is more
important today so this is a flat
buffers is a cross-platform
serialization library and it is similar
to protobufs
but it is designed to be more efficient
they you don't need to you can access
the data without with without unpacking
and
there is no need for secondary
representation before you access the
data so this is aimed for speed and
efficiency and it is strongly typed so
you can find errors at compile time the
next component of tensorflow Lite is the
interpreter it is engineered to be lower
work with low overhead and on very small
devices and we have very few
dependencies and it is easy to build on
simple devices like I've said before
we've kept the binary size to about 70
kilobytes and 300 kilobytes with
operators so it uses slide buffers so it
can load really fast and the speed comes
at the cost of flexibility tensorflow
lite supports only a subset of operators
that tensorflow has so if you are
building a mobile application and if the
operators are supported by tensorflow
light then the recommendation is to use
tensorflow light but if you are building
a mobile application that is not
supported by tensorflow light yet then
you should use tensorflow mobile but
going forward all develop we are going
to be using tensorflow light as the main
standard ops and kernels let's talk
about that has it has support for
operators and saw used in some common
inference models the set of operators
are smaller like I said and so not every
model will be supported then in
particular we provide a set of core
built-in ops and these have been
optimized for arm CPU using neon and
they work in both float and quantized
hewants these have been used by first
party google apps and so they have been
battle tested and we have done handoff
we have hand optimized for many common
patterns and we have fused many
operations to reduce the memory
bandwidth so it helps if you if there
are ops that are unsupported we also
provide a C API so you could use custom
operators and you can write your own
operators for this finally interface to
target custom hardware is the neural
network API tensorflow light comes
pre-loaded with hooks for neural network
API if you have an Android release that
supports Ã±Ã± API then tensorflow light
will delegate these operators into any
API and if you have an Android release
that does not support an API
it's executed directly on the CPU so we
neural network api's are supported for
Android with 8.1 release in Oreo which
we just announced me to a couple of days
back so it will support various hardware
acceleration you can get from vendors
for GPUs for DSPs and CPUs and it uses
tensorflow
as a cold technology so for now you can
keep using tensorflow to write your
mobile app and your app will get the
benefits of hardware acceleration
through your network api it basically
abstracts the hardware layer for ML
inference for example if a device has m
DSP it can transparently map to it and
it uses neural network primitives that
are very similar to tens of lowlights so
it's architecture for neural network
api's looks like this essentially
there's an android app on top typically
there is no need for the Android app to
access the neural network API directly
it will be accessing it through the
machine learning interface which is the
tensorflow Lite interpreter and the the
nn runtime the neural network runtime
can talk to the hardware abstraction
layer and then which talks to their
device and runs various accelerators if
like I said before if nothing is
available then we run it on the CPU so
how do you use the tensorflow lite
this is the same high level diagram we
showed earlier and let's just walk
through a high level code on how what
needs to be done to make your model work
with tensorflow lite
so the first step is generating a model
this is standard tensorflow code nothing
has changed if you have written a model
using tensorflow or use the same thing
write a model that's generated that the
like are using tensorflow
the next step we said is to convert the
model into a TF lite format so the this
is the simple code you call a convert
function to actually convert the buffer
to a flat buffer format the converter
can be used in command line after the
model is generated but it is a
convenient
to put it in your Python script so you
can find errors earlier if the model
cannot be converted for some reason so
again some operations are not supported
so earlier you convert you know ahead of
time instead of you running training for
days together and then figuring out that
the model is not supported by tensorflow
life so adding it to your code is a good
practice the output of this is a binary
file binary stream so you can just write
it into a file and get a binary file
format so once you have the in your app
this is the app Gradle file that you
write you say uncompressed because you
don't want to add an extra latency to if
you have compressed your file to
decompress it and then access it the
repositories is the maven repository
saying where tensorflow is available and
the dependencies for your app will be
tensorflow light which has to be linked
which will be linked to the air so we've
provided with tons of flow light the
demo application as well as any a are on
maven you can download and see
integrated into your app and see how you
can use it so now once we have done this
we are now ready to use the model on the
device so - we probably provide both c++
and java api is for this talk I've just
talked in added Java here but you can go
look up documentation on github to see
the C++ API as well so you initialize
the interpreter with a model file and
then run model into inference and if you
have custom ops we provide four
functions for custom ops want to
initialize the OP there - uninitialized
off to prepare the OP by giving inputs
and then finally to invoke the kernel
you can refer to the github
documentation if you want to see more
details so we release tensorflow lite
two weeks back and as part of tensorflow
Lite we provided C++ API so converter a
set of built-in apps the demo app and
also we have supported mobile net both
float and quantized versions inception
v3 for float version and also the smart
reply text model that's also open
sourced
so be sure to go check out and I have
added links here please feel free to go
refer to this documentation for
tensorflow light and also download the
code from contrib under the tensorflow
master github repository so this is how
the sample app has you know just given
you some screenshots from the sample
demo app you can download the app and
test it for yourself and as you can see
it's you know identified the objects the
in the top three choices with a pretty
high accuracy rate so some stats i've
already told you that the interpreter
without operators is about 70 kilobytes
this is actually 15 times smaller than
tensorflow
so that's why we say it's very suitable
for mobile and embedded devices and it
is also four times faster to load so
between quantization and float quantized
ops quantized models are 4x smaller and
they're also 50% faster to load on your
device so future work we want to improve
and add additional tooling and make it
more user friendly like I said we
support only a subset of ops if you want
to see what are the subset of ops we
support it's available on github
I can tell you the path later on under
the tensorflow Lite documentation and we
want to extend this ops but we still
want to keep true to our goals by adding
selective registration and other
mechanisms to ensure that the binary
size does not increase we want to target
smaller devices not just Android and iOS
devices but also Raspberry Pi and other
smaller devices and eventually maybe
even do on device training currently we
do on device inference but eventually we
want to do while own device training as
well so that's about it thank you very
much and go check out the tents of low
light code
so I'd like to wrap up this session by
showing the actual use cases of the the
chance of parole application running on
mobile I'd like to introduce an very
interesting application this is a
actually a production applications for
last one year called gymnastic exercises
Quora made by a japanese application
vendor called velocity named acoustic
evidence' yeah was renamed this is
available for both Android and iPhone
and actually I have to explain a
participate round in Japan somehow we
everybody dragged me of the people do
the morning exercise demons has
exercised by listening to the music on
the radio so it's very common to do the
morning exercises for millions of people
in Japan every day these applications
measures the movement of your exercise
the arms and everything to score how
well you have been doing the exercise
and to implement these applications they
have built their own tensorflow compiler
this is comparable to chansik alright
which will be we are now developing
butter because this vendor has a very
talented expertise on the tensile floor
so they were able to build their own
test flow compiler just like this apple
light that shrinks the model from the
tens of megabytes into a few megabytes
like a two or three megabytes so that
they were able to put the whole tensor
flow model running inside these small
the Android applications this compiler
does the many of the optimization
techniques I have mentioned record like
a decreasing Yi or the quanta
quantization or the removing the old
unnecessary binary parts everything so
let's take a look at the actual
institution live demonstration mode
exercise 4 so can I switch to year
so this is the applications where you
can choose your values kind of with the
exercises and I will be playing the most
standard around so this is the music I
also embarrassing and I'm just like that
so let's stop
it's enough it's enough you know so now
the teaser for model is trying to
evaluate how well you have done with
exercise and you can see the budget here
that is the evaluation by the tensile
for module inside these applications
it's a real thing okay so go back to the
slide yeah that's that you saw this
score yeah
the Kaurava is actually measured by the
attention for motor running inside here
the mobile phone applications and so the
technology is now available and you can
do the same thing with sensible light
whether it's available so I hope maybe
coming maybe this year and next year
we'll be seeing more and more on Android
and iOS applications that learns the
real deep learning model running inside
it to solve the real-world problems so
that's it thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>