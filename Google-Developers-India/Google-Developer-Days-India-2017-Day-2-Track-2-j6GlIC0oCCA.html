<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google Developer Days India 2017 - Day 2 (Track 2) | Coder Coacher - Coaching Coders</title><meta content="Google Developer Days India 2017 - Day 2 (Track 2) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Developers-India/">Google Developers India</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google Developer Days India 2017 - Day 2 (Track 2)</b></h2><h5 class="post__date">2017-12-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j6GlIC0oCCA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">     Test the captioning.
     Test the captioning. &quot;The 
show will begin shortly.&quot; 
    &quot;The show will begin 
 show will begin shortly.
        The show will begin 
shortly. 
    The show will begin
 shortly.
The show will begin shortly. The
 show will begin shortly. The 
show will begin shortly. The 
show will begin
 shortly. The show will begin
 shortly. The show will begin
 shortly. The show will begin 
shortly.
 The show will begin shortly. 
The show will begin
shortly.&quot;  shortly. The show 
will begin shortly. The show 
will begin shortly. The show 
will begin
 shortly. The show will begin
 shortly. The show will begin 
shortly. The show will begin 
shortly. The show will begin 
shortly. The shortly. The show 
will begin shortly. The show 
will begin shortly.

 (No audio)
 if you want to build anything 
like this, it's definitely 
something you should be checking
 out. So what can use Android
Android Things for. Really, it's
 ideal for powerful and 
intelligent devices. It's not 
just about little things that 
support switches and lights and 
whatever, but about complex 
applications. Things like audio 
processing, video processing, 
doing on device machine 
learning. These are the kinds of
 cool things that you can
 build with Android Things. 
You've got enterprise, 
businesses and consumer devices 
that people are going to use at 
home. IoT's going to be a big 
thing. And if you saw my talk 
yesterday, I talked about how, 
you know, we had desktops and 
laptops and phones, IoT is going
 to be bigger than those. It's 
something to be 
apart of. 
There's all kinds of cool things
 you can do with it. Let's get 
started. Making hardware -- so 
IoT devicesapart of.  involves 
some kind of equipment that you 
P don't have to give to a KMEFR 
of some customer of some kind. 
It's really hard to make good 
electronics. And the reason is 
that -- this is a little screen 
shot, the circuit board of it. 
If you look at the board itself 
where the red arrow is, there 
are little tracks there.
 You have to do a good job about
 making sure the electrons 
travel at the speed of light 
correctly and arrive at the same
 time. 
    And so, doing this is really
 tricky. It requires very highly
 skilled engineers that know how
 to design electronics. I don't 
make hardware. I have no idea 
how the stuff is done. And we 
want to try to make that easy. 
    It requires very skilled 
designers to make nice 
electronics. We're going to show
 you how we helped
 to do that. It requires you to 
buy large quantities. If you say
 I want to make ten of these 
devices, they'll charge you 
$1,000 each.
 software, turns out, that's 
tricky, too. Everyone's building
 the custom IoT devices with 
their own handmade
 Linux distributions. And this 
is becoming critical for IoT 
devices. Now, there are these 
vulnerable, insecure devices out
 there in the wild that hackers 
are breaking into and it's 
becoming a problem for companies
 who make these devices because 
your device was
 used by -- it's going to update
 those things 
properly. You can make devices 
knowing that used by -- it's 
going to the best can be done 
right now. 
    Finally, you don't want  to 
have to hire a huge number of 
engineers on your team whose job
 is to maintain Linux kernels 
and keep up to date with patches
 for Open SL, or something like 
that.
We take care of that
 for you. We support the ability
 to take the same design and 
take it all the way to the 
production. And that's what 
we're going to talk about for 
the rest of this talk.
    Firstly, the design of 
Android Things, not based on a 
single board computer, it's 
based on a
 SoM. 
    It contains the core 
computing pieces that are 
needed. So the CPU, the memory, 
the WI-FI chip, and so forth. 
And you attach this board to a 
larger breakout device during 
development so the top one there
 and that has all of the 
breakout connectors that you use
 to attach things to. And that's
 for testing. When you're ready 
to move into production, you 
take the developer board away. 
And you make your own custom 
board that has just the 
components that you need. 
    And this is nice because the
 custom board that you make only
 has the pieces you want. You 
only pay for the parts you need.
And you don't need to pay for 
this huge development board that
 you're not using any of the 
parts for it. 
    And the nice part is that 
the SoMs, the small computing 
modules, they're made by someone
 else. They're made in large 
quantities. So you can buy them 
at good prices. And then, the 
boards that you need for 
yourself, you can do a run . And
 because they use low-speed 
electronics, it's not adds hard 
as I showed previously. It's a 
lot easier for electronics 
designers to build for you. You 
can focus on that part and it 
makes it easy for you. 
    And finally, we have what's 
called a BSP, board support 
package. And the board support 
package is software from Google 
that makes that SoM and that 
board of yours work by running 
Android Things on it. 
    We provide the kernel, 
libraries, necessary drivers and
 everything to make it work. And
 as long as you stick to the E 
sign of the SoM, everything will
 work for you. It helps you to 
focus on you make your board and
 we take care of everything else
 for you. 
    The other thing that's nice 
about SoMs, you don't need to 
worry so much about product 
certification and testing. 
    So traditionally, you have 
to get the entire board 
certified and tested. There are 
regulatory agencies, like in the
 US, that's called underwriters 
laboratory, they do testing or 
FCC certification. And doing all
 of that certification is very 
expensive and it's very time 
consumeconsuming. 
    However, if your design is 
based around the SoM, the SoM's 
been certified. And it makes the
 certification process for the 
rest of your hardware a lot 
cheaper, quicker and easier. 
    It makes things easier for 
you and you can get your product
 to market quicker. 
    So with the way we've 
designed Android Things, it's a 
little different than the way 
people have traditionally used 
AOSP to build their custom 
devices. We've broken Android 
Things into two parts. The parts
 managed by Google. Things like 
the Android framework, the Linux
 kernel, we build that and give 
you binaries for that and 
they're distributed as part of 
the board support package. And 
the second part is managed by 
you as a developer. You build 
APKs, like Android Studio, and 
write apps and user drivers. And
 that's the part you control. 
And we've
 separated them so the part 
managed by Google has the right 
permissions to do everything. 
But your part you run as 
developer, that's run with 
restricted permissions, the 
Android APIs to communicate with
 everything, which helps to 
isolate your code off from the 
rest of the system so if there 
are any security problems, it 
tends to keep them 
compartmentalized so the whole 
system won't get attacked if 
your app has a problem. 
    That's one of the nice 
things about the separation is 
we handle certain parts, you 
handle certain parts, and you 
can focus on the parts that are 
important to you. 
    What happens is that when 
you upload an APK that you've 
built, you -- you upload to a 
developer console. And it's 
combined with the framework with
 Google. And we build a single 
fimware image. And so E, we sign
 and verify the images. Sign 
them on the server. And then, we
 verify them when the images 
arrive on the device. And we do 
this to make sure the images 
haven't been tampered with 
during transmission to the user.
We don't want a rogue third 
party changing the firmware on 
the way to the device. 
    They're also protected from 
corruption so if the images are 
corrupted during transit, 
they'll be verified that way. 
    But also, when the device 
boots, we check to make sure it 
hasn't been tampered with or 
damaged on the device, as well. 
You don't want to boot an image 
that's been corrupted or 
tampered in any way. The next 
thing, we provide A/B updabbing 
rollback protection. We actually
 stage it as a second image, put
 everything in place, reboot the
 device and switch over to the 
other image. 
    If for any reason if we boot
 this image, if it fails, the 
system will roll back to the 
previous version. And that way, 
if a corrupt build is pushed or 
if you make a bug or something 
that causes the device not to 
boot, it'll roll back so that 
the device will be able to get 
future updates and you won't 
break the device with your 
updates. 
    This is a nice feature 
because it's something that's 
hard to build yourself. We 
provide all of that for you. 
    And finally, we provide 
automatic security updates. 
Android has a huge team of 
security engineers whose job is 
to look for vulnerabilities in 
Linnux kernel and all of the 
other framework, and you get the
 benefits of having all of those
 engineers working for you. 
    We make the updates, we then
 push them out to your devices. 
And so, that you don't have to 
worry about keeping track of all
 of these little things. In the 
past, if you were doing builds 
from AOSP, you would have to 
merge the patches in yourself 
and build your own images and 
push them out. This is very time
 consuming, hard to do. We're 
trying to make that frictionless
 and easy for you. And we take 
care of it all. 
    Finally, we provide the OTA 
mechanism to get the software 
updates out to the devices. The 
images are signed, built on the 
server, we then push them out 
and it uses a system that we've 
built for ourselves. And it 
works very well, it's very 
reliable and very easy to use. 
    You don't have to worry 
about it, you can focus on 
building your applications. 
    Let's look at some of the 
more technical things for how to
 build Android Things. The first
 part, this is like a diagram 
here that shows the traditional 
Android Framework model. There's
 all of the different parts that
 form the system. We've kept 
most of them as part of Android 
Things. But what we did was is 
we removed some of the core 
pieces that aren't needed. 
    For example, in Android 
Things device, we've removeded 
all of the different 
pieces. 
We've taken all of that away, we
 want to save space, make the 
system boot quickly. But we've 
provided all of the rest of the 
Android framework and bits and 
pieces you're used to dealing 
with, that's available for you 
to use when you're building your
 IoT devices. And also, you 
know, the UI libraries you're 
familiar with in Android Studio,
 that's all 
available, too. 
    So Android Things supports 
UI but displays are actually 
optional now. So I always think 
about Android Things as like a 
phone but without the screen
 on it. Different IoT devices, 
some of them are embedded into 
different things, some of them 
have displays. You don't have to
 have a screen, but you don't 
not have to have one either. 
    You have the same interface 
libraries. And it's based on 
activities and services, just 
like you're familiar with for 
writing Android Apps right now. 
Same concepts are there for you,
 as well. One little thing to be
 aware of is you still need to 
have a main activity because the
 main activity is responsible 
for handling keyboard and mouse 
events and so forth. You still 
need something to be handling 
that. 
    Because screens are 
optional, there are times when 
if you need to request 
permission, for example, we 
don't pop up a permission 
dialogue box on the screen. It 
might not be possible for the 
user to hit okay on it. Android
 Things gives permissions and 
it's automatically grounded so 
you don't have to hit the okay 
button. 
    Finally, when you're 
building an Android Things IoT 
app, you're not necessarily 
going to be using a touchscreen 
anymore, but you need to think 
about what kinds of alternative 
input device modalities you need
 to use. Now you have the 
ability to build wearable 
devices, you could have a 
microphone, you could have a 
camera, you could have a GPS 
sensor or compass. 
And there's all kinds of input 
modalities you could consider as
 part of building your device. 
Don't just lock yourself into 
using a touchscreen. You could 
use any kind of input device. 
And it's pretty much all of the 
options you can think of are 
available. And the Android SDK 
through the framework, things 
like sensors and so forth are 
read in exactly the same way as 
you do on the phone. 
    And then, also, you get to 
develop with the power of 
Android. You're using the same 
Android SDK everyone uses for 
phone development, you're using 
Android Studio, and you get to 
use any language supported by 
Android Studio. If you're 
excited about calling, you can 
use it on Android Things, as 
well. Google Play Services, 
Firebase and TensorFlow and 
Google Cloud IoT and embed the 
Google Assistant into your 
device. All of these things that
 Google provides For phones are 
available on IoT devices, as 
well. I've had a lot of IoT 
developers come up to me saying 
they're excited to put 
TensorFlow into something like 
this. This is something I 
haven't been able to do before. 
You can now do machine learning 
on device. You don't need the 
cloud, you don't need WI-FI, and
 you can do it on the device. 
That's something a very low-end 
microcontroller can't do. It's a
 big win for Android Things. And
 as I said, it really helps you 
to -- all of these tools and 
libraries you're familiar with, 
it's all available to you. 
    So the most important part 
is adding hardware to it. Let's 
go through some examples of the 
APIs that are available for you 
to go about doing this. And so, 
when you look at the developer 
board, we actually support NXP 
devices, some of you have 
received a kit to play with at 
home. We support NXP devices, 
also raspberry pis right now. 
You can plug mice, keyboards, 
things like that in. We support 
speakers, microphones going in 
via the audio jack or USB port. 
There's an ether net connector, 
WI-FI, there's different HGMI 
outputs and connectors for 
cameras. We support all of these
 things on the module. Whatever 
you want to use to connect stuff
 up, you can use that for 
whatever your application is. 
    And every developer kit also
 has a breakout header pin. So 
this is the connector pin 
diagram so you can see there's 
all kinds of connectors we'll 
talk about in a minute. But 
every single pin, we have APIs 
that will talk to that. So you 
can add additional devices to 
the lines right here. And then, 
you can integrate them in 
whatever way you want. 
    So there's a whole bunch of 
different SBEFR facing
ing  protocols. We support 
general purpose I/O, I2C buses, 
I2S. And these are different 
connection technologies that 
hardware designers use to 
interface different devices. 
You'll have seen these. These 
are important because they're 
very cheap and high-speed ways 
of getting things connected to 
your devices that's easy to 
interact with directly. We're 
going to go through more 
examples of these later. 
    And then, also, we have 
support for what is called user 
drivers where we can add to the 
Android Framework and take 
sensor data. We can take input 
devices, and we can actually 
inject those events into the 
Android Framework so they can be
 read and used by applications 
using the same sensor framework.
If you've got a sensor that 
connects via I2S, we can inject 
into the sensor framework so the
 code that uses it just sees a 
regular Android sensor just like
 any other sensor connected on a
 phone. 
    So here's some examples that
 show how different kinds of 
things would be connected. So if
 we had a motor that was 
connected, that would talk 
directly to your app. If you had
 a sensor, you can see that the 
example I talked about before, 
it connects for I2C. It then has
 a user sensor driver. It puts 
it through the sensor manager 
into the framework. And if you 
had a GPS device, you would 
connect via serial port. You 
would write a GPS driver around 
it. And it goes through location
 manager into the app. And an 
interesting thing is that Google
 Play Services has something 
called fuse location devices. 
Because these events are 
injected into the framework, 
Google Play Services can use it 
to help enhance those, as well. 
This is one of the cool spinoffs
 of the user driver framework. 
    What kind of hardware can 
you interface with? We talked 
about concepts like GPIO and so 
forth. I'm going to show you 
quick examples. GPIO is 
basically binary one and zero 
signals, tending to be like zero
 to five volts or 0 to 0.3 volts
 and low and high values. And 
they're done -- they're used by 
things like buttons when you're 
inputting into your device. You 
can make lights turn on and off,
 rotary switches, sensor 
devices.
They all use just simple signals
 coming in. And Android Things 
can read those for you. 
    We support what's called 
PWM. The nice thing about pulse 
width modulation, you use 
varying -- signals with varying 
modulation widths. And you can 
use it to control the intensity 
of an LED or move the server 
motor around. We have native 
support for PWM and the boards 
had actual PWM hardware on board
 that can generate the signals 
reliablely. They're useful for 
motor controlling and things 
where accuracy is important: We 
have serial ports, RS232 so you 
can connect them up. And a lot 
of devices like GPS units and so
 forth, they use UART style 
input. They all have at least 
one serial port connecter on it 
and you can connect more via 
USB. 
    We also have SPI and I2C, 
there are companies that make 
dedicated little boards with 
sensors on them and so forth 
that talk via these high-speed 
serial buses. 
    So these are useful for 
times when RS232 isn't fast 
enough. And you can connect all 
kinds of sensors directly into 
Android Things. And we handle 
those directly. 
    So that's sensors. How do we
 go about building activities 
within Android Studio? 
Everything in Android Things is 
based around an APK, just like 
an Android phone. You would plug
 it into the laptop, hit the 
icon and puts the APK on your 
device.
You unplug it, you plug in your 
Android Things device and it 
works in the same way. You get 
to use the same studio tools and
 plug it in, works over ADB and 
works. There's a few little 
differences you need to be aware
 of. 
    The first thing, you need to
 include our Android Things 
support library. It has a few 
extra things that are not part 
of the standard Android 
Framework, such as the
 peripheral I/O. The next thing 
you need to do, on the bottom 
there, you can see Android 
manifest xml entries. You need 
to add an IoT launcher, intent 
filter. And we do that to tell 
Android Things that activity 
that's tagged with an IoT 
Launcher is the one we want to 
start up at boot. So the device 
boots up, finds the first IoT 
Launcher, starts it up. And that
 runs on the device from that 
point on. 
    And you should only have one
 activity on the DWIGS. If you 
have two, then only one of them 
will start up. It's the way of 
you flagging which you want to 
run on start up. And that's the 
one that sets up the devices and
 gets things
 going. In many cases, I've 
taken small
 toy examples and it works. 
    So if you want to add 
peripheral I/O, we talked about 
GPIO, let's say button presses 
from a switch connected to the 
device. We wire the switch up to
 the pin, the GPI pin on the 
device. This is the code we then
 use to do it. It's similar to 
handling a button in the Android
 UI, except now real, physical 
buttons. We use the peripheral 
manager service. We create one 
of those. 
    We then call Open GPIO with 
the name of the pin. We call 
some methods that set the 
directions. We want to do input.
It's very important you specify 
the correct direction because 
you can't read on an output pin 
and vice versa. We then, also 
specify Edge Falling, the 
purpose to say I want this code 
to trigger whenever the signal 
drops from high to low, but not 
from low to high. 
    So it only goes on the 
Falling edge. You can specify 
&quot;rising&quot; and also both. Then you
 register a callback, and you 
can see an on GPIO method, and 
that will be called when the 
signal drops. 
    So that's it YOURVGS, you've
 got a couple of lines of code. 
And then, you can do whatever 
you want once you've received 
that notification. And then, you
 can do different things in your
 code depending on what you want
 to do. And all of the 
peripheral I/O devices, I2C, all
 of that, they're implemented 
with a similar API, as well. 
    If any of you have done 
embedded Linux before, you have 
to go through the file system 
looking for different files, 
this is a lot easier. And it's 
handled by a dedicated service 
that's isolated from your 
process so that it helps for 
security, as well. 
    So we've talked about 
drivers and user drivers. We 
also have a peripheral driver 
library that we've written. So 
some of the engineers on my team
 as well as people out in public
 have been writing drivers for 
different devices that plug into
 Android Things. On GitHub, we 
publish these drivers so they're
 available for you. Many common 
sensors and devices and so 
forth, the drivers are already 
available. You don't need to 
write them yourself. We've 
handled these things, reading 
the data sheets, implementing 
the code so you don't have to 
worry about it. Check out that 
driver library if you want to 
find examples of code that makes
 it easy for you so you don't 
have to worry about reading the 
data sheets. 
    So in summary, Android 
Things helps to power Android to
 help you make your 
applications, make them easier 
and faster than you could have 
done before. You don't have to 
learn a new tool chain, you can 
use your existing experience. 
And it is something you can get 
started quickly. You can take 
one of our kits, go home, and 
have something working in a 
couple L of hours. You can get 
started quickly. It's something 
I encourage you to check out. 
    The hardware and software is
 managed by Google. So these 
SoMs are -- we work closely with
 the semiconductor partners, we 
help specify what they will be 
and we support them with 
kernels, drivers, libraries. All
 bundled up into a binary we 
combine with your APK to ship to
 devices. And because we handle 
all of that, you don't need to 
worry about handling thee se 
patches and so forth. With the 
help of our developer console 
where you push them to the 
devices, we handle all of the 
updates and all of the security 
and everything for you so that 
you can focus on building your 
application. 
    There's a lot of complex 
things involved with, you know, 
updating devices and making 
everything work. We have some of
 the most amazing engineers I've
 ever worked with that talk to 
me about security about Android 
Things and it's amazing the 
level of detail and care they've
 put into the design of the 
system. It's very nice. 
    As a developer, you can 
focus on making your app and 
what makes your product great. 
So what's next? As I mentioned 
before, we have a sandbox area 
outside across the street there 
where you can check out our 
demos and chat to us and 
definitely take advantage of 
that while we're here. 
    And also, later on, you're 
going to have questions. We have
 some communities online where 
you can go to ask questions. So 
on G Plus, an IoT developers 
community, also hacksterI/O, and
 that's a great place to find 
other ideas of what people have 
been working on. And you can 
look at the code and read the 
documentationdocumentation. It's
 a great place to see fully 
built little ideas you can 
study. 
    And then, also, we have the 
Android Things documentation 
itself developerandroid.com. And
 a whole bunch of samples on 
GitHub that are showing best 
practices in how to do different
 things involved with the 
Android Things kits. 
    Also, I do a lot of posting 
on Twitter and Google Plus. I 
talk about IoT, Google Assistant
 and also the Assistant SDK. 
Definitely, follow me there, if 
you want to keep up to date with
 all of the new things 
happening.
We're always doing new things. 
For example, yesterday, we 
launch LD Android Things. It has
 a bunch of fixes for previous 
issues, but also, has a lot of 
things for making fast UIs 
possible and it supports cameras
 and you can run a lot of the 
Android samples. Google Maps is 
on it. Check out Developer 
Preview 6. There's a lot to try 
out. That's it for today. So 
thank you very much for coming. 
I look forward to seeing you in 
the sandbox. Thank you.

    JENNY GOVE: Hello, everyone.
My is Jenny Gove, and I'm 
honored to talk with you about 
Mobile Design. I'm going to 
start with telling a story about
 our two mobile platforms. The 
Mobile Web and Native Apps. 
Setting what you're seeing here 
at the conference in context. 
And looking at design patterns 
and principles, we're going to 
talk about Flow and Navigation. 
We'll be looking at system 
patterns and material design. 
And then, we'll look at some of 
the Mobile principles that we 
have that are based on user 
data. And we'll end with some 
pointers for user testing. 
    So the focus of this 
conference is on the latest and 
greatest technologies that you 
can learn about and apply to 
your work as it should be. There
 have been many topics such as 
those that you see here in the 
Word Cloud. But in some cases, 
you've gone into real depth on. 
Including performance, exploring
 IoT and Android Things.
Google Assistant, conversational
 apps, details of progressive 
web app and more. 
    You've covered technical 
issues so far and tools and 
features that you as developers 
need to be aware of. I'm going 
to be taking a different 
approach for the first part of 
this user experience talk. 
    We're going to take a look 
at a short history how the tech
 media has characterized media 
and mobile apps. 
    And we're going to see how 
they've engaged with this over 
the years and taking us to the 
point where we are now the 
Native Apps and Mobile Web 
matter very much.
There are use cases for each, 
and they're both developing and 
interesting in important ways. 
    Let me start by asking you a
 question. When was it that the 
web was first declared to be 
dead? Think about it. Any ideas?
You could do a search for it, 
which ironically, you'd probably
 use the web. It was in 1997 by 
wired&quot; magazine. And they had an
 article they titled &quot;push, kiss
 your browser good-bye. The 
radical future of the media 
beyond the browser.&quot; And then, 
in 2010, Steve Jobs made this 
statement. People searching on 
their mobile devices as they do 
on desktop. What's happening, 
they're spending all of their 
time in 
apps. 
    The web is dead, long live 
the internet. And then, just a 
few short years later, there 
were headlines
 like this. &quot; 
Mobile App Usage Increases in 
2014, as Mobile Web Surfing 
Declines.&quot;

apps are killing it, the decline
 of the Mobile Web and things 
like that. Why were there 
headlines like this? What was 
fueling it? A study published by
 a company called Flowy. And 
they found only 14% of the time 
on mobile device is spent on the
 web, while 86% is spent on 
apps.
And studies found similar. Take 
a look at where the stats on 
this lie today. Pretty similar, 
today we know on Mobile Devices,
 only 12% spent on the mobile 
web, while 88% is spent on the 
apps. This is a study by KN 
comescore. How are people 
spending time with apps? They're
 spending their time in 
primarily in social networking, 
music, games, multimedia apps 
and more.
    Two-thirds of the time are 
spent using apps with regards to
 entertainment or communication.

deal of time people spend 
usingness an apps, it's not 
guilty easily distributed. Back 
in 2014, too. But when people 
started looking at this more 
closely, there were tell tail 
signs that the Mobile Web had 
great things going through it, 
too. 
    When they started to 
research more, started looking 
at the data, they found that the
 Mobile Web accounted for more 
traffic for many verticals than 
Mobile Apps. Thischart, from a 
year later in 2015, shows that 
retail and finance and travel 
typically see 50% or higher 
proportion of their visits from 
the Mobile Web. With data like 
this, people to question some of
 those more simplistic stats put
 out that seem to imply that 
native apps were becoming the 
important platform on Mobile. 
This provided further detail and
 that led to the headline, &quot;the 
web is dying. Wait, how are you 
reading this?&quot;
I love that one. And then, there
 were lots of u-turns. Is the 
web dying?
 No. 
    Mobile Web might not be dead
 after all. Long live the Mobile
 Web. And &quot;wired&quot; in 
contradiction to 2010, in 2014 
claimed that the Web is not 
dead. And then, two years later,
 the web is not dead, after all,
 Google made sure of that. 
    And that reported about the 
advancements of activities that 
can be done in a browser and the
 early successes of progressive 
web apps. 
    The article ended by 
saying,&quot; the truth is the app 
universe and the web are not 
mutually exclusive. We use 
both.&quot; Maybe the web is more 
alive than ever. 
    Now, more recently the 
headlines have shifted to focus 
KUS on apps. They're tempered 
now. Learning lessons from the 
rounds of headlines that came 
before that were focused so 
assuredly on the death of the 
web. 
    Authors and editors now the 
analysis is more complex and 
there are multiple changes 
happening. Changes in the web 
and the app eco system. And 
opportunities with AR and VR, 
virtual reality, apps themselves
 are also evolving and 
cooperating more activities and 
more services in some
 cases. 
Increasing the time spent within
 them. Now, we get headlines 
like this. 
    Very alarmist, right? The 
app is dead, long live the app. 
It's all about channels in 2017.
This one, I think, is the most 
important mobile apps aren't 
dead, they're evolving. All of 
these things are evolving. 
    We're at an interesting 
juncture. There are native apps 
doing very, very well that broke
 into the market very recently, 
ride sharing apps and food apps 
and more. 
    There exists huge 
businesses, such as Myntra and 
MakeMyMakeMyTrip and Voot that 
built their businesses on the 
app. And we know some businesses
 app-only that have been 
successful. 
    Indeed, our company, Google 
has released a number of 
app-only products, as well. We 
have our Trips app for planning 
and booking travel. Areo food 
ordering and home service app 
and Tez, our Indian-based
 payment app. There are other 
businesses that recognize as 
mobile web focused or turning to
 the web now. The large Indian 
retail store, Flipkart and Jio 
Cinema. And they have gone to 
the web for the first time with 
a progressive app. And then, I 
want to highlight West Elm. 
They've never developed a Mobile
 -- Native Mobile App. 
They're VP of innovation 
explained this clearly at a 
retail conference earlier this 
year when he said that people 
just don't buy a couch on their 
phone every day. 
    And another way of saying 
this is, you know, make sure you
 understand your user behavior 
and do what's right for your 
users for your business in a way
 that really meets those users' 
needs. 
    Indeed, his insight is very 
spot-on. In a study I conducted 
earlier this year, participants 
often explain that they tend not
 to value apps unless they 
decide they could use them a 
lot. 
    It was all about frequency 
of them. Because of the download
 process and
 so forth. You know, adds 
something else to their phone. 
Some of them say their phone, 
and apps take up storage on the 
phone. 
    In our studies, we found 
this was a burden for some in 
the US. And other studies have 
found that in India, 33% of 
users run out of storage
 every day. We know from user 
research, users like the 
convenience of launching apps 
from the home screen. If it's 
something they will turn to 
regularly. If you've been 
following the PWA development, 
launching from the host screen 
is no longer the domain of 
native apps only. But also, can 
be available for Web Apps as 
seen here with the TRI
TRIvago app. When making the 
user experience across both 
experience, web and app, making 
them both better and kind of 
making up for the deficiencies 
that each have, and, as part of 
doing this, we've created more 
tools for developers that can 
help with user experiences. 
Like, polymer elements for the 
web and material design for both
 web
 and app. There's an 
availability of tools to have 
the speed and performance on the
 web which is so critical for 
user experience. 
    And for native apps, there's
 Android Studio with the tools 
built into that. And Firebase is
 a suite of tools to aid app 
developers as can be seen here. 
Here, we have things like test 
lab and analytics and Firestore 
which is being talked about here
 at this conference and A/B 
testing and many more tools. 
    So the two platforms, Mobile
 Web and app are addressing the 
user experiences that they have 
been deficient in. 
    We're helping users discover
 an experience native apps more 
readily for encouraging 
developers to breed instant 
apps. 
    And you can see an example 
here. And we're helping 
developers create apps with 
speedier downloads that take up 
the storage on the device. 
    On the web, we're 
encouraging developers to create
 offline experiences so that 
experiences on the web stay 
engaging. And the progressive 
web technologies that enable 
this, service worker and the 
Cache API not only handle 
offline, but also provide 
resilience to network
 problems. 
    Encouraging developers to 
reengage users through 
notifications, as well. As you 
can see here in this example 
flight app. These capabilities 
for engagement and reengagement 
on the mobile web, they didn't 
exist before. So we're really 
trying hard to think about this.
    So the changes to both of 
these platforms are fantastic. 
As I said, it's evolving. As 
you've already heard at this 
conference, many sites are 
pushing these modern features of
 the web from AMP to Service 
Worker, to AMP Home Screen, 
better payment systems. These 
companies want to deliver better
 mobile website experiences. And
 this is just a map of some of 
the samples that are raising the
 bar globally. And India has 
been especially rapid in 
adopting this technology. It's 
leading the world in many cases 
in terms of the progressive web 
app development. In addition to 
some of these cutting-edge 
technologies at the fundamental 
level, we have to provide really
 great mobile experiences. Great
 design. We do this through the 
provision of creating great 
design -- I've chosen to review 
elements of mobile navigation 
because that's common no matter 
what vertical you're working 
within. 
    This should be applicable to
 many of you. Let's talk, first,
 about Android System Patterns. 
These patterns can be used for 
creating strong navigation and 
flow. 
    We'll start with the Up 
Button shown here. It takes the 
user upward in the hierarchy 
until they get to the home 
screen taking users through the 
previous screens they've used. 
The back button, that works 
differently to the app button. 
Whereas users remaining in your 
app. The back button navigates 
in reverse, chronological order 
through all of the history of 
the recently viewed screens. -- 
the fact it does other things, 
as well. The floating window 
such as dialogues. It dismisses 
contextual action bars and hides
 the on-screen keyboard so you 
don't have to navigate back 
through those things. Those are 
some of the main kind of system 
patents  Patterns for Android. 
Let's look at material designs 
that help with
 navigation. Many different 
elements that can help with 
navigation and flow, both in the
 web and for apps. 
Starting with Top Tabs. Quite 
familiar with you. It's a useful
 navigation element. And YUFRZs 
can switch between different 
categories of content easily 
with Top Tabs. It's good when 
you have not that many 
categories. 
    It provides a mechanism for 
promoting awareness, as well as 
the alternative views within 
your app. So this illustration 
shows some of the different 
types of Top Tabs. And we find 
that using a label and icon 
works best. We'll talk about 
that more in a little while. 
It's also possible, of course; 
to provide a bottom navigation 
bar. And with Top Tabs, enables 
switching between views and 
promotes awareness of 
alternative view. 
    The bottom navigation is 
located in an ergonomic position
 and we see that young people 
are more used to holding devices
 on one hand and therefore, 
getting to the bottom navigation
 bar more easily. What if your 
app has a lot of categories of 
content I did have lots of these
 different tabs, but it's not 
functional. So, then, we can use
 the navigation tool. It enables
 quick navigation between views 
that, perhaps, are unrelated in
 this case and allows for a deep
 navigational structure. And it 
can reduce the visibility of 
frequently visited destination. 
    This is Nested Navigation 
when we have a hierarchy we wish
 to display. It's a 
deep-and-a-half gagsal  
navigational structure. It's an 
advantage when you have these 
unrelated views. 
    And what do you do if you 
have this deep navigational 
hierarchy? Well, the navigation 
also enables you to expand that 
hierarchy. The drop-downs open 
up the hierarchy. Some apps 
start off with that navigation 
expanded on the first view 
mobile so people can see what's 
there. 
    Now, this is
 Embedded Navigation, a directly
 expandable surface. This 
calculator in this example, it 
uses an accessible panel to be 
navigation aid to a supplemental
 view.
 This image has the navigation 
panel expanded, the function 
panel and overlays a span of 
people. 
    In the CLAPGS collapse 
state, you would see a peek of 
that so the user knows they can 
expand it. That's the embedded 
portion. It's not formally using
 Tabs or Bottom Navigation, for 
example, to label the content. 
It's just directly expandable in
 the surface. That's another 
element of material design. 
    Floating Action Buttons, 
these allow users to action the 
most common or primary action on
 the screen for that screen in 
the application. 
    And the icon may be 
animated. The Floating Action 
Button animates as an expanded 
piece of material by default. 
Here, we see the floating action
 button used for sharing.
 And used to add something here 
to use to add a note. 
    And, perhaps, for play or 
pause in this application here. 
And lastly of these, gestural 
navigation, between content 
using, users can swipe to 
navigate. The gestures include 
touching and dragging the screen
 horizontally left or right or 
vertically up and down or 
zooming in and out. 
    When do you use this? This 
is recommended when you do have 
related content, right? It's 
sort of naturally ordered 
relationships such as 
representing consecutive, or 
views of similar content
 types. 
    Gestural navigation. These 
are the material design 
navigation elements. We looked 
at Top Tabs, Bottom Navigation 
Bar, Navigation Drawer, the 
Nested Navigation, the Expanded 
Navigation Draw and the Embedded
 Navigation. So you can find out
 more about these within the 
material design guidelines 
online. 
    Now, over the last few 
years, we've completed many user
 research studies developed -- 
to develop three sets of mobile 
design principles. That's 75 
design principles in all that 
we've created. 
    And we did many years of 
tests for this. And we wanted to
 use a testing at this scale as 
Google because when you 
releasing these principles to 
developers and designers around 
the world. And we wanted to be 
really SHUFR sure they would be
 robust. 
robust. 
robust. sure they would  robust.
Our user studies took place in 
our user lab. And  participants 
were prompted to visit a key 
conversion app to six different 
sites during the session. Each 
session lasted an hour to an 
hour and a half
 per person. We had participants
 bring in their own smartphone 
so the phone was not unfamiliar 
to them. And both iOS and 
Android were used in equal 
proportion. 
    They worked through tasks on
 the site or the app, and the 
method we used was traditional 
usability testing. So that's 
when the facilitator asked the 
question, in the case you see 
here, the facilitator's asking 
for them to order dinner for 
tonight. And the participant 
speaks out loud as they're doing
 a task. And what this does, it 
provides us insight into the 
understanding, provides us with 
details about when they get 
stuck or lost, we can see what's
 happening. And this is what 
makes it different than 
collecting your data or survey 
work that's, you know, about 
attitude and not really about 
observing behavior. 
    So for all of the three 
studies, we conducted around 250
 usability sessions. And we did 
this in collaboration with the 
third party company. We tested 
100 mobile sites for the app 
study we tested a hundred apps. 
And focused on retail, we tested
 apps. We tested 50 of each. And
 the participants have longer 
sessions. We really wanted to 
get in-depth. We have an hour 
and a half. So the sites or apps
 tested from large retailers to 
information providers and 
service providers. And we looked
 at many different types of 
services like travel and 
e-commerce. We looked at 
groceries and news sites. 
    We looked at food ordering. 
We looked at the category of 
home and garden, transportation,
 legion services, such as 
insurance, and more. Those are 
just some examples of what we 
looked at. 
    So you can get to all of the
 principles that we've exhausted
 from the studies. If you want 
to be able to get them, I 
suggest you take out your phones
 and take a picture of the link.

principles of Mobile Web design,
 Mobile App design and Mobile 
Retail for Mobile Web and App. 
And the reason we have two links
 for each of the first two was 
we produced the second set, the 
same principles, but the second 
link for each of the first two 
has developer resources embedded
 in there. So links to APIs and 
so forth. 
    So the first ones, you can 
pass on to your colleagues. And 
then, the
 second one -- 
    In order to understand it, 
we'll touch on these principles.
Talking about the user
 experiences. It's allowing you 
to grab the content. So before 
you ask them to sign up or 
select other kinds of 
experiences. It's personal 
information from them, it's 
important to provide them with 
excellent experience. 
    And these are the  sites and
 the apps, providing personal 
information up front. And we've 
seen this in recent years, some 
sort of immediate payoff it's 
really clear about. Things like 
the delivery information. If I'm
 going to ask --
 and the low brand recognition 
and knows where the value 
proposition is. 
And the customer information 
really isn't needed.

users are being registered. 
We're going to go down one. They
 can get into the experience.

much later on. 
    This one is similar. It's 
about putting a barrier in 
between people's experience. And
 the guidance here is, like, 
don't let promotions fill the 
show. It's often us more 
interested in making the 
promotions to users than users 
themselves. We're showcasing
 -- 
instead of something you see 
like this on the left-hand side,
 better option is to go with a 
small banner. This doesn't 
interrupt the user -- 
instead of something you see 
like this on the left-hand 
experience. 
However, your best bet is to 
make a fantastic Mobile Web 
experience. Capability, users 
can work offline and more. 
    So just adding some data to 
this example. I can give you 
some data from Google +. A 
couple of years ago, our Google 
+ team said this experience, 69%
 of users simply abandoning the 
whole experience. That's a huge 
dropoff, right? 
    They knew
 about this. And here it was. 
This is what it looked like. And
 the team changed to a much 
smaller, less intrusive app 
panel at the time.
It looked like this. And that 
resorted
 in the increase by 17%.
And now, they don't have a 
banner at all on their site. 
They went with the idea of
 providing it. No matter what 
the user wants
 to use. And then the third of 
these that provide the value to 
people is about connections. And
 I think you probably talked a 
little bit about this at the
 conference already. 
    Users can get stuck when 
they deny permissions with the 
proper questioning of the app. 
And they often permissions -- so
 of them, we ask permissions at 
the beginning of the experience 
in the app and  users don't know
 what they're asking for and 
often deny. To mitigate this, 
communicate the value of the app
 of the device. So the user 
understands it. And this is 
possible in iOS and Android. And
 for Android, it was enabled. So
 make that permission directly 
--
and the user will be more likely
 to answer. 
    Let's have a look on the 
left, you see the user is right 
up front when they open the app.
E-commerce app. They want to use
 location. They don't have the 
context for it. 
    In this one, they are in the
 app. They're using it. And at 
this point, they want to search 
for stores, they hit the button.
And it's a much more obvious 
user wide. 
    So that's the last of those 
experiences that really --
 (no 
audio) ignore or temporarily 
dismiss permissions, more than 
90% of the time. 
    Soaudio) ignore or 
temporarily  permissions, this 
makes you 5 times more likely to
 accept or deny the text rather 
than temporarily dismissing and 
ignoring them. 
    So you've heard at this 
conference about the importance 
of loading the home page and 
other pages of the app.
 They better have, really, key 
calls-to-action. It's about your
 site or users. And in our 
studies, we tested some
 similar calls-to-action like 
this. Try it now or learn more. 
    I'm seeing these kind of 
things more recently. But it's 
too fake for the user and the 
user can't be convinced of what 
they came for. 
    The design on the right here
 is a lot more clear to the 
YUFRZ if you're going to a real 
estate site and you're likely to
 be there for renting, buying or
 selling. 
    Figure out those key 
calls-to-action for your product
 and feature them prominently. 
And this is something that you 
can find out for user testing, 
too. Sometimes, we're so 
embedded in our products, we 
can't recognize it ourself. 
    So secondary and calls to 
action. And we talked about the 
use of navigation and other ways
 to search the navigational 
elements. 
    When we think of menus, a 
lot of thought has to be put 
into organizing and labeling 
menu categories to be used. 
First of all, menus should be 
short. 
    No user's going to want to 
scroll through menus. In 
addition, our studies have 
shown, if you don't think 
carefully about the menu 
categories, then we don't 
reflect users' mental model of 
those categories for your topic 
area. 
    This is where the user 
research activity of doing a 
card sort can come in. We can 
get into the minds of users and 
have them the topic in the 
category and see how those fall 
out. And it might be quite 
differently than the way we 
would organize it. 
    And perhaps, reveal the best
 structure for the organization 
of our content. I give the 
example here, we saw things like
 this in our studies where, 
here, the menu categories have 
kind of muddled the product and 
the activities. This creates a 
conflict in users' minds. 
    They're looking for, like, 
men's hikng boots, is it in 
men's footwear or hiking? Think 
about your categories like this.
    On the right-hand side, you 
see that they've focused on the 
activities. Much cleaner and 
doesn't give users this complex 
in the mind. 
    Now, providing text labels 
and visual keys to clarify 
visual information is really 
important. So let's take a look 
at this. We have icons there, 
unlabeled. In your head, think 
about what you think they might 
be for when you're booking a 
flight. 
    Take a little moment. What 
would that plane be for and the 
check mark and the star and the 
person? You labeled them all. 
Okay. All right. So let's take a
 look. Think to yourself whether
 this is what you thought. 
    For the first one, it's 
Trips. The second one is Book. 
The
 third one is Club, like the 
Airline Club and the fourth one 
is accounts. You probably got 
that one correct. It's a bit 
more standard. 
    But we really found despite,
 you know, thinking that some of
 these icons were super, super 
common that generally, when 
we're testing them without 
labels, it isn't the case. So 
it's surprising, things that you
 think will be common like icons
 on the menu, cart, or store 
locater. 
    And also, labels for -- it's
 important to label actions, as 
well, like &quot;filtering&quot; and 
&quot;sorting.&quot; And these were 
universal. They're not 
necessarily well understood. So 
icons are labeled much more 
lightly to be used appropriately
 and it's been happening in 
navigation. 
    And being responsive with 
visual feedback is important so 
that users know the results of 
their action. When users add an 
item to the cart or submit an 
order, the lack of feedback can 
cause them to question whether 
their action has been processed,
 or not. 
    Apps provide a visual 
animation or another type of 
visual eliminate the guesswork 
for the user. And you can see 
this here in the positive 
example. This is a negative 
example here. Nothing happens, 
basically. But we have a toast 
or a snack bar element popping 
up to give the user this 
information. 
    And lastly, we're going to 
look at transitioning between 
apps and the mobile web and 
where that's important. And you 
have to do it making 
frictionless. Here we see users 
checking in for a flight. In 
this app, they're moving them to
 the mobile web. But the 
experience, really, disjointed 
and has a very different look 
and feel. 
    And the designer has really 
thought about this. The teams 
have worked together. Often; 
it's different development 
teams. And the experience is 
much cleaner. Again, this is a 
navigational thing. It's best to
 try not to move people from one
 platform to another. But when 
you really need to do it, make 
sure that it's speedy and the 
benefit and contrast
 to any -- 
    Just to recap, again, move 
the barriers, registration pro
 propromos, those are things 
that can stop users in their 
tracks. 
Organize and label your menu 
categories, provide labels to 
clarify visual information. Be 
responsive with your visual 
feedback and make app to web 
transition. 
    Here, again, if you didn't 
take a picture over it last 
time, the Mobile Web principles.
And the second of each links 
have additional resources added.
So that brings me to the end of 
the principles. Let's talk about
 the benefits of User Research. 
So your apps and your sites need
 user testing. Your project 
isn't an
 exception. You need to do that 
because we always learn some 
things. You can start small, 
though. Start by understanding 
the user journey. 
This is important. This one is 
the -- look at new customers and
 existing customers. And it's 
important to understand that 
current process so you can get 
into their mental model and 
understand that. And see how 
those experiences might map or 
not map to your experience and 
what they're going to get 
muddled to. 
    The design, the content, the
 navigation, the flow and the 
usability. I want to give you an
 example. I have a user research
 that worked on this product 
AREO, the food and home service 
ordering app that launched here 
in India and it's active here in
 Bangalore. Many studies of food
 and home service in India on 
the early concepts to get the 
app right. 
    The research that she 
conducted and understanding the 
experience they needed to build 
to fit in with users' lives. 
Specifically, we tested the one 
on the left. This was not the 
released product. It exposed all
 of the services that AREO 
offered. The usability studies 
and found that people could not 
get a quick overview in their 
heads of everything that was 
offered. We felt that they 
could, like, 
design wise -- but when we user 
tested it, we couldn't do that. 
    But also did a card sort, 
helped with renaming the 
categories and localizing the 
content and the service 
provided. All that design work 
resulted in the design you see 
today. Because the testing that 
was found to work well for 
users. And in addition, the 
research helps to identify where
 the imagery needs to be 
localized. Things like within 
the drawn diagrams here, there 
was the inclusion of meat and 
alcohol and we decided to 
exclude those in the end and 
also decided to make the color 
palate more vibrant. 
    If your app already exists, 
it's helpful in understanding 
the user journey, with and 
without your technology. This 
really helps our AREO product, 
as well. 
    And you also need to test 
your app on-site. And there's 
all sorts of biases for testing 
with family and friends, it 
really is, like, a suitable 
place to start if you've never 
got started before. And it can 
lead you to that a-ha moment. 
    This can often be the a-ha 
moment. You realize, why,  they 
don't understand the label or 
you have other insights. As your
 testing gets more mature, you 
can iron out some of those 
things and do this more formally
 and with proper screening. 
    How do you get started in 
user testing? We've got great 
resources. Again, take a photo 
of this link. Google Ventures, 
done a lot of supporting 
start-ups doing User Research 
and a lot of great resources at
 this link. 
    So summing up this talk, the
 story of Mobile Web and App is 
still evolving. I think it's 
really, really exciting. We're 
looking to see where it's going 
to go next, and we're working to
 make sure that the experience 
on both platforms is really as 
best as we can make it and 
making those decisions the 
platform has. They're getting 
much more comparable. For 
example, both platforms can 
deliver a speedy user 
experience. The web, service 
workers and the Cache API and 
other technologies can provide a
 really reliable experience, 
whether that's on 4G or 3G 
networks. Both platforms can 
support the functionality to 
take photos, for instance. And 
the use of location services and
 more for both platforms. 
They're available on the Mobile 
Web as well as app.
 The Native Apps, they're 
becoming more discoverable. That
 being the big issue. From being
 serviced on Google Search and 
in our app formats on search 
net,
 as well. 
As well as obviously being in 
display. And through the 
capabilities we're having with 
instant apps to have an instant 
experience without needing to 
validate. 
    So the most important thing,
 for development teams to focus 
on is to create experiences for 
the best features. And we've 
reviewed the best practices for 
mobile design and provided links
 to more so you can follow these
 Best Practices for your design 
and development work. 
    And remember to conduct your
 user research to improve the 
mobile experience you're 
building. Start with 
understanding that user journey 
and design with that in mind, 
and then, use a test to provide 
you and your team with the best 
opportunity to create a really 
fantastic; best experience in 
your vertical for your users. So
 that ends our experience design
 and navigational patterns 
today.
If you want to be in touch, I'm 
going to be available in the 
Mobile and Web Design area. You 
can come along to the area and 
we can chat. And you can contact
 me at any time on Twitter 
@JennyTwitter @JennyLG, I want 
to thank you for your time 
today.

Hi. How are you? Are you having 
fun? I'm having a lot of fun. 
It's very exciting event. You 
all have a lot of energy. And 
it's really great to be here. My
 name is Silvano. I'm a 
developer engineer on the 
Actions in Going L and  Google 
Assistant. And what we're going 
to see in the next 20 minutes or
 so are the features, like 
things that we are in Actions on
 Google, since we announced it 
almost a year ago. If you missed
 the overview of Actions Google 
Platform, yesterday, my
 colleague gave the developing 
conversation. It's on YouTube. I
 strongly recommend you to watch
 it if you want to see a more 
organic description of the 
platform. 
    So let's start with a little
 bit of history of the platform.

announced it on December 8, 
2016, which means, next week it 
will be our first birthday. 
    And what we told you was 
that
 you could start extending by 
writing apps in Google. 
Initially, there was work only 
on the Google I/O in May when we
 did the second batch of 
announcements.
 Using the Assistant App and the
 transactions and payments 
review. 
    And then, like in the second
 of our first year of life, the 
platform is starting to get 
momentum, more announcement, and
 the announcement is getting 
larger and larger. And we are at
 more locales. I think, now, we 
support 16 locales.

lot of these speeches. Let's 
start by looking at these 
highlight reels of my personal. 
I  divided them into two groups.
The first group is what we did 
to make your life easier. Make 
the life of anyone that's trying
 to develop an app and Actions 
on Google easier. 
    Making it extremely easy for
 you to create certain types of 
apps. For apps, there are trivia
 game where you ask questions to
 user and you sort them based on
 the number of correct answers 
they get. Personality quizzes 
where you ask them questions, 
the personalities so at the end 
you can tell them, oh, you're 
very brave and also, you are 
very cute or stuff like that. 
    And Flash Cards, which, 
basically, you can ask yourself 
a series of questions to inport 
the learning of a topic you're 
learning. 
    You don't have to write a 
single line of code, you don't 
have to write a conversation. We
 have implemented all of that 
for you. And the only thing you 
need to do is provide content in
 a Google Spreadsheet. By a 
template that Google provides to
 you. 
    Let me show it to you. Let's
 see how it can create here on 
stage in two or three minutes,
 I'd say. So this is the Actions
 on Google Console. When you 
want to create an app on Actions
 on Google. If you're very 
familiar with it, yo uh can
 recognize --

 same. The 
the second row first thing we 
did
 for you, and you can get the
 preview of in the way -- and in
 the way they talk to
 the user. Let's try the regal 
queen. Do we have audio?

sounds. And if you wanted to try
 the robot. 
    This
 And so on. And basically, you 
can see how your apps will sound
 like to the
 users. Let's go to the next 
page. We get the link to the 
template that we have. All we 
need to do is make a copy of the
 template.
 on some questions, even as a 
follow-up to get them more in on
 what the developer answers. 
If they wanted to add one 
tier, it could be short in front
 of a 

long tier. Instead, the answer 
is false. Once you change the 
content, the only other thing 
you need to change is the name.

duplicate and, especially, you 
can't use the same type of 
template. And
 so -- the URL of this sheet, go
 back, hit next. 
And then,upload it. 
    Now, the template is 
validated in the content, 
uploaded successfully, we can go
 on and create the app. And once
the app
 is -- there we are. So now,
 we can. This is -- there is an 
element.
This is a simulator that allows 
you to test your app.
 Or could have a speaker, and 
you have the same user that you 
use to create the project. You 
test this app on the speaker.

 what happens. (No 
audio) all you need to do is go 
back to the overview, you need 
to have some information for the
 app. These are things like, 
what is the name of your app? 
What are some of the branding 
information, like, the log, or 
if you have a website you want 
to link the
 user to, the template, we 
understand one that you can just
 use. And then, you will be 
done.
 And we -- your app will be 
reviewed in a faster queue and 
available
 to everyone.
Was it three minutes? Five, 
maybe? Really fast, right? Going
 back
 to the
 presentation. Want
 we have an API to do that. An 
example in your case is your app
 providing whether the report, 
one of the first things you will
 ask, probably, this
 code -- and that, for example, 
 presentation. Want 
is something -- and you want
 to 
remember. Now with this new app 
we've had, the first time that 
you actually get a code, you 
will ask permission to the user.
If they confirm, they reply, 
yes.
 You can add arrays. And then, 
we would just be sending it back
 to you all the time so all the 
time you can assess that 
information. For example, in the
 next time that the user is 
using your app, you can check if
 you have the information and 
keep all the code that is asking
 them, can I get
 the information?
Another thing that we add the 
SSML Simulator. I don't know if 
you're familiar with SSML, 
stands for Speech Synthesis 
Markup Language. Basically, 
allows you
 to give some text and read it 
to the user. What you can do 
with SSML is tell the text what 
you want it to say to the user 
but also, how you want us to say
 to the user. You can add music,
 you can add sounds, and there's
 many more cool things that you 
can do to give your app a much 
better and more engaging 
experience. 
    It was hard to test it. 
Because we didn't really give 
you an easy way. Like, you had 
to write
 some of the xml you wanted to 
try. Put it in your app. We 
added an audio tab in the 
simulator where you can just put
 the SSML and test it and see 
how the agent would be reading 
it. 
    So, for example, if we look 
at this first example, we have 
some speech, custom sound.
 Ask then, the changing rate of 
the speech, change in the speech
 of the speech and change in the
 volume. And then, there's other
 things that we can do. Like,
 we can spell the word. 
    This is another thing that I
 just show
 you. So this is the simulator 
that I was talking about. It's
 in the simulator.

agent would render it.
 you want me to replay it? Is 
that better? Yeah?
 Do
 of the things you want to do, 
and when you're at the SML you 
have, you can incorporate it 
into your app. 
    Moving back to the 
presentation, we now move to the
 second thing, which is things 
that we are -- improving the 
quality, allowing to do very 
good. And the first one is 
SSML.
And we have some elements that 
will allow you to do downlayer. 
You can add different tracks. 
So, for example, you can have a 
background.
 With say, I want this to start.
 I want this loop to be played 
until the end of You the speech.
 And the next example
 we have
 is -- and what the example is 
doing is exactly that. They are 
displaying the loop displaying 
until the end of the speech. And
 then,
 we are -- 
for waiting for an answer. And 
then, finally, the answer with 
additional emphasis. And this is
 another one that it's rather 
enjoyed if you actually listen
 to it. 
    And so, you can get a pretty
 good atmosphere to your app by 
using, like, changing inflexion,
 remote control and how it is 
presented to the users and also 
background music that makes the 
atmosphere that much better. 
    The next thing I wanted to 
talk to you about is there's a 
feature I like a lot. And it's a
 feature that allows you to 
transmit a conversation that's 
started on a speaker through
 a phone. One is, in some cases
 maybe the speaker, but you want
 to show an image to a user and 
now you can do it by saying, 
hey, can I use an image that 
will better convey my message? 
Or in some cases, exclusions. 
So, for example, when you want 
to complete a transaction or a 
payment. 
    And it works a little bit 
like this in practice. This is 
actually showing one
 of my --

you want to -- you can start 
asking what you want to get on 
the speaker, you can complete 
your own on the speaker.
 And then, the conversation 
through your phone and you can 
finalize the contraptions on the
 phone. 
    And the code is quite easy. 
Like, there's two passwords. The
 first part, for the user the 
way it was, the API that allows 
you to check whether to use as a
 device with the capability that
 you are testing. So in this 
example, you
 want someone. You can ask the 
user, can I transfer to the 
phone? I want to do it in this 
case, for example, I want to 
show you some sample images. 
    And then, in the next step, 
all you have to do is enter the 
user response. If they've given 
consent to you, the transcript, 
just show the image. And if they
 didn't explain them, I'm sorry,
 but I cannot allow to do this. 
    The last thing I wanted to 
talk about is a very recent 
addition that was announced in 
November '15. And it's updates 
and push notification. Feature 
that allows you to engage with 
your user. It's in a development
 right now. Which means, you can
 test it, you can use it in your
 test ops. You cannot, yet, use 
it in your production app. 
    We have two models of 
engagement. One is daily 
updates. So, you can ask your 
user if they want to subscribe 
to the daily update from your 
app. So, for example, let's say 
you have an app that is telling 
jokes and you want them to 
receive a joke, new joke every 
day. You can use this feature. 
And the system will 
automatically pull an update 
from your app. 
    The way it WORVGs is you 
need to configure.
works is you need to configure. 
When they receive a 
notification, and asking for an 
update to that specific intent. 
And if the user's content, you 
basically are ready. At some 
point, you'll start receiving 
up-to-date requests from Google.

there is no guarantee that even 
if you ask a user to
 register that. Google will 
decide when to show it to the 
user. We want to make sure that 
the user is engaged.

standard model that allows you 
to send updates to the 
    The other model is a
 user
 whenever -- one of the tasks
 to be they have to pay a fine 
because they parked in the wrong
 spot. The fine will expire, 
they haven't paid it yet. You 
want them to know that, A, you 
have one week to pay this thing.
 And pretty much in the same 
way. 
This is model S exactly the same
 code that you've seen. 
    What's the main difference? 
The way you ask for the 
permission is different 
function. In this case, is ask 
for update permission, you still
 need to specify what you want 
to use for this. And once you 
get the consent, you need to 
save the ID of the user in a 
database like Firebase or 
whatever it is that you are 
using to store data from your 
app because that user ID is how 
you be able to target the user 
to receive notification. 
    When it comes to sending the
 notification, this is an 
example of how you would send a 
notification usinging JavaScript
 code. But basically, you need 
an access token that you 
generate from a service account.
 You just need to structure the 
JSON of the notification, which 
is quite simple. You just need a
 type and the user that you want
 to target that you've 
enabled the identification. And 
it's just an HTTP request. 
    When they receive the 
notification, the notification 
appears as a system 
notification. And when
 the user, it into the 
conversation using the intent 
that you specified for the 
notification. 
    So that was it. My six 
highlights from the things. If 
you are interested, these are 
some resources you can use to 
keep an eye on what's happening 
on Actions on Google, the 
Developer Community, the 
Developers, Communication and 
the Twitter account. Thank you 
for being here with me and not 
going to 
lunch, I guess. 
[ Applause ]
    And the last thing 
is, if we will be at the sandbox
 in the afternoon. Come and ask 
us question. 
Thanks. Bye. 
      e two three four. 
       GUS CLASS: It's been 
great to meet mere. 
This talk is about the 
fundamentals of Google Cloud IoT
 Core. I'll introduce a concept,
 I'll tell you how our product 
works and we'll go over the 
building blocks of the platform.
My name is Gus Class. Earlier 
this year, I was spending my 
208% time on
 Cloud IoT. 
Recently, I started dedicating 
my attention to it. In the past,
 I contributed to android  
Things, and Google Sign-in. I 
also do arts and crafts with 
electronics and circuits. Hit me
 up on Twitter or find me in 
office hours after the talk if 
you have questions or want to 
chat. 
    So things are increasingly 
becoming connected to the 
internet. In fact, it's becoming
 implicit that devices with any 
kind of computing capbilites are
 internet connected. This means 
different things to the 
developers who are working with 
these devices. And industrial 
and commercial domain to do 
things like measuring control 
factories and run
 bike share. 
Developers are internet 
connecting their projects for 
quantitative self, remote 
control and home automation. 
Consumer products are frequently

monikered smart. I think there's
 more than just the internet
 connectivity relevant to this 
trend. Here, you can see my 
robot friend. There are sensors 
on the front of the little
 fella. At some point he was 
connected to the internet. To 
the enthusiast, taking a row 
bolt like Mr. Scruffles and 
tying him to a computer to make 
him smarter is what Cloud IoT 
means. The Android Things KPL 
Station. It straddles the 
domains I'm talking about today.
All the way from home brew to 
consumer devices and the 
commercial space. For the 
consumer devices developer, 
Android Things Weather Station 
can be updated. If you're a 
developer and a consumer and 
devices domain, you probably 
want to address capabilities to 
your devices by providing 
software updates after you've 
shipped that device. You're 
probably also thinking about 
adding predictive capabilities 
to your devices and enabling 
choreography across devices 
where you control one device or 
many devices in synchronized 
fashion.  
    So you want to control which
 devices get which changes and 
be able to roll back should 
there be issues with
 the
 change. This is a factory in 
the heyday had meters and valves
 connected to 
tanks. In the modern factory, 
such mechanisms are controlled 
with sensors and actuato
 RVLTs. 
The cloud controls the valves 
and does this control based on 
predictions that are made from 
sensors that are distributed 
throughout the factory. 
    So you warehouse that sensor
 data and analyze it to make
 predictions to maintain your 
factory making your factory 
safer and more efficient. This 
is Cloud IoT for commercial
 solutions. Just a few devices 
when you're getting started to 
thousands or millions of
 devices. It addresses the needs
 of IoT is automatic scaling. 
Increasing capacity is as easy 
as turning a dial or checking a 
box. We're also a leader in 
analytics and data warehousing. 
This goes hand in hand with 
GoogleGoogle scale. For lots of 
devices or being able to 
transmit data to control a large
 number of devices is Google's 
bread and butter. Google's also 
an industry leader in the 
Machine Learning and artificial 
intelligence. It's the baseline 
for feature detection and 
images. 
    Security. If you're not 
careful with IoT, your internet 
of things can quickly turn into 
a botnet of things. They design 
systems they cannot be abused. 
This is why you choose Google, 
maybe in deployment years and 
years from now in the future. 
Let's go over the platform. 
Cloud IoT Core connects your 
devices to cloud to allow you to
 get the features on Google 
Cloud down on your
 devices. You don't need to 
build out devices with excessive
 power or computation 
requirements because Google 
Cloud operates as the brains 
back in your devices. You don't 
need to build out devices with  
devices. 
    So here you can see the 
product in Google Cloud that are
 relevant for IoT. Google Cloud 
IoT Core connects, operates as 
the identity provider and bridge
 to the devices. Pub/Sub is 
one-to-one and the critical 
entry point into the rest of our
 cloud. Pub/Sub is
 globally distributed to make it
 a natural throughput. With data
 flow, our ETL solution, you can
 migrate to warehouses, Bibbing
 BigQuery, or Spanner. You can 
drill down into the data using 
data lab or data studio. 
    And finally, you can use 
cloud machine learning. And 
Machine Learning APIs given your
 device data or classify 
unstructured data. This is how 
we deliver on the promise of
 Cloud IoT. You're probably 
wondering how it works in 
practice. It's helpful for 
describing -- it's helpful 
describing what we're providing 
to clarify what we're not. You 
can see this Cloud IoT. You're 
probably wondering how it works 
in practice. It's PIR here. It's
 also not
 a device. It's not an operating
 system. We work very nicely 
with a lot of existing solutions
 for devices. 
    It's also not a new 
protocol. It could be bridged to
 new protocols, but not protocol
 specific. Also, not a new 
radio.
This is not, like, this is not 
tied to a specific hardware 
radio, like, some sort of 
Laura-based solution. We can 
work with other radios. Here's 
how it works. On the Google 
Cloud side, you have device 
provisioning, which is 
controlling which devices have 
access to your cloud resources. 
    And you also have 
choreography. And choreography, 
I like to think of as 
controlling individual devices 
or multiple devices in concert. 
To facilitate this on the device
 side, we've taken a 
standards-based approach which 
is the Google way to do things, 
right? By shows this because 
it's supported on many devices 
and developers are familiar
 with this. The kernel library 
is dominant so you can do HGTV 
easily. And the next is the 
MQTT. Now, this is an industry 
standard that is very efficient 
on over the wire communication. 
It's got a small memory 
footprint and it's a ratified 
IoT standard. If Yao you've got 
a device you're working with 
already, there's a good chance 
you've got an MQTT library for 
it. Using the standards based 
protocols, you can connect your 
devices to our cloud. And so, 
let's see which services are in 
Google Cloud IoT Core to provide
 this functionality. The first 
is the device manager. 
    So the device manager lives 
oven the cloud side and controls
 which devices have access to 
cloud resources and it's the 
choreographer for communication 
and synchronizeation across the 
devices. And then, on the device
 side, we have the protocol 
bridge, we call it the protocol 
bridge because it's not an 
actual server, it's like a 
server-like service that allows 
you to communicate using the 
available protocols with the 
cloud platform. Now that you've 
seen the product from a 
high-level perspective, let's 
drill down and look at the 
pieces in practice starting with
 the device manager. 
    So the device manager 
organizes devices using a 
hierarchical resource model. 
Registries that in turn have 
devices and then these devices 
have associated with them a 
credential and configuration. 
    Multiple credentials can be 
associated with each device so 
that certificates can be rotated
 or revoked without transmitting
 credentials to devices and 
still maintaining that 
connectivity. 
    So then registries 
encapsulate devices from the 
protocol bridge to Google Cloud 
Platform. These are always 
mapped one-to-one with a
 Pub/Sub topic. You write to 
this in Google Cloud IoT and 
those messages will appear in a 
Pub/Sub queue. So registries are
 regional as indicated by the 
path, you can see I've 
highlighted the Asia East region
 used inside the address for the
 specific device. 
    And generally, you want to 
aggregate your data from a 
single -- like a single given 
project to a Pub/Sub topic. We 
don't anticipate many scenarios 
where you could be creating 
multiple within the same 
project. You're thinking create 
lots of registries.

one-to-one with your projects. 
And the device manager is the 
first -- the next thing I'm 
going to talk about. This lets 
you programmatically manage your
 devices. There's a model 
letting you create, delete, 
list, describe or update them. 
And there's also an IAM 
provisions model that allows you
 to delegate the control of 
these devices to
 other people. You can have a 
manufacturer so when the device 
comes from that to you, you turn
 it on the internet and it's 
able to connect to Google cloud.

representation on the right. And
 you can tell things like the 
last time the device connected. 
You can tell whether or not it 
was able to send messages to 
Google Cloud and so forth. This 
is useful when you're debugging 
but also allows you to know what
 your fleet of devices is doing.
And it allows you to get all 
sorts of analytics that you 
would not -- you may not be able
 to get on other platforms. 
    So we authenticate devices 
using a JSON web token or JOT.  
Verified server side. Basically,
 the client needs its private 
key in order to produce an 
authorization credential that is
 verified on the server using 
the public key you register with
 the server. Multiple 
credentials can be associated 
with the device so you can 
revoke certificates or expire 
them without losing device 
connectivity. We do not provide 
a mechanism for distributing new
 credentials in the wild because
 if a device is fully 
compromised, the best way to 
discover the device is through 
mechanisms such as a secure 
radio or an over-the-air update 
using the operating system you 
install on your device. And this
 is going to be implementation 
specific for now.
    Okay. So the device 
configuration. A device 
configuration is the means of 
communicating changes to a 
device from the cloud. 
Configuration change messages 
can vary from simply modifying a
 device parameter, such as, 
like, fan speed. To 
reconfiguring a device's context
 with a new one, such as 
uploading a new TensorFlow 
model. Configuration changes may
 -- are acknowledged by the 
devices when they come down and 
as soon as, like, as soon as the
 device actually connects and 
reads the configuration change, 
we considered it acknowledged. 
    So in HTTP, you get this 
configuration changes by making 
a get request to a URI and MQTT,
 you subscribe to an MQTT topic 
and receive those changes as 
they come. And this way, I think
 the MQTT has a slight advantage
 over HTTP and you don't have to
 pull to get those configuration
 changes. 
    Enough Slides. Let's see 
what it's like to provision the 
device. Sorry, my computer 
locked. This will take
 one second. All right. So let's
switch over to. This is the new 
IoT Core section. If you 
navigate on the left and find
 give a device registry an ID. 
And I'll call it &quot;hello,  it 
there. You haveGDDIndia.&quot; And 
registries are regional. I'm 
going to set to the region 
closest to where we are. Asia 
east one. Again, associated with
 each registry is going to be a 
Pub/Sub topic. I'll choose one 
I've created but you can create 
the topics on
 the sample code. This is -- if 
you look at the actual script  
the fly. I'mitself, it's open 
SSL. I'm creating a key that's 
used with Java. 
    And so, I get the public 
key. -- you can do this 
programmatically using the API 
or console with G cloud. But for
 today, I'm going to show you 
how to do this
 using the Console, which is 
relatively new. And I'm going to
 call my device, GDDGDDIndia. 
And that should be
 the key. And if I did 
everything right, the device has
 been added. Okay. Let's return 
to the Slides. Now that we've 
added a device, it's time to 
start thinking about connecting 
to that. Connecting that device 
and then communicating from it. 
    I'm going to go over the 
next part, which is the device 
protocol bridge. So there are 
three classes of messages that 
are sent over Google Cloud IoT 
Core. The next one, the updates 
sent down to devices. If you 
want cloud to device 
communication, this is done 
using configuration messages. 
AndCore. The next one, the  then
 the state message, the third 
type are persistent and shared 
between the device manager and 
the device. These are used for 
things like confirming and 
acknowledging configuration 
changes. That's why I chose this
 cute little receipt here. 
    Let's start with publishing 
messages using the HTTP bridge. 
Authentication is performed 
using -- is performed on the 
HTTP bridge using a token. You 
PATSZ pass inside a header. And 
this contains the JOT, which is 
the authorization credential. 
And the token has an expiration 
of an hour. And that expiration 
needs to be in sync with 
Google's time servers. Specific 
to HTTP, you should be using 
lower case headers and setting 
cash control. To send messages 
to cloud, you're going to post 
to a global end point passing in
 the identifier for your 
specific device. 
    So requesting configuration 
from the cloud in HTTP, you're 
going to have to authorize, 
again, using a JOT token and 
receive it by making a get to 
the URL I'm showing here on the 
screen. And this will correspond
 to an individual device. And in
 this way, you can send a 
specific configuration down to 
an
 exact device. You also request 
a specific version. As I said 
before, Google Cloud stores 
multiple configurations so you 
can roll back. 
    And you can request that 
specific version or you can -- 
you can send zero to request the
 latest configuration set on 
your device. 
    All right. One more demo. 
Let's connect the device, send 
some messages and see how that 
looks in the
 console. 
    Remember, I created this -- 
I created this private key and 
the public key from the -- from 
the Python sample, aisle going 
I'm going to use a private key 
to create a credential and 
connect to Cloud IoT Core. And 
then, I'm going to send -- I'm 
going to transmit messages that 
will appear in the Console. 
    And so, if you look here, 
I'm calling this -- this is the 
same sample code that we ship 
it's available in the 
documentation. And you'll see 
that I had the registry ID, 
which we
    Now, if we return to the
 Console here we can refresh the
 device and see the changes live
 inside of the Console. This is 
useful for debugging. 
    I'm calculating and 
transmitting to Google Cloud IoT
 Core.
 A published subscribed 
standard. 
    When you connect to MQTT, 
the user name is A published 
ignored. As with we're using a 
JWT with a PATSZPATSZpassword 
and an expiration of an hour. 
And the MQTT ID, we provide a 
path to your device. 
And then, after you connect, you
 can publish
 telemtry data. 
    So you want to use the 
telmetry topic to publish 
frequent data and use the state 
topic to publish data shared 
between devices and the device 
manager. 
    And so, again, if you have 
something that you're publishing
 frequently, like a heartbeat 
data or sensor data that you're 
reading from temperature thing 
very frequently, you're going to
 want to use events and send 
those state messages less 
frequently. 
    Next thing, we're going to 
talk about is the MQTT bridge 
for subscribing to topics. So a 
device can only subscribe to the
 MQTT topic, devices/device 
ID/config. 
    When you connect the first 
time using MQTT, you'll receive 
the latest configuration. You 
can request a specific 
configuration if you -- if you 
request it using the -- by 
passing a subtopic to the 
configuration. So configuration 
is acknowledged when the MQTT 
message is confirmed on the 
device. And this is qOS1. You 
can anticipate that sometimes 
the configuration may come down 
to your device more than once. 
    One more demo. And this one,
 I've got halved ware. Hardware.

little bit so you can see this 
better. 
    So here's
 my little device with a donut 
on top of it. And I've got an an
tenna loosely connected to it. 
    I was feeling really brave. 
I made this demo on
 the plane. 
This is an Android Things 
project. And I have inside of 
the Android Things, I'm using 
string resources to configure 
the topic and the device ID. 
    And then, I'm going to have 
to configure it with the 
credential that we just created.
And so, I just -- this, again, 
copied the private key into 
resources, into raw resource on 
the Android App. And now, what 
I'm going to do is start the 
Android App and this will -- 
this will, hopefully, connect 
successfully. I'm carefully 
reading the little log cat here.
That looks good
 so far. Now, I'm going to use 
the developer console to send 
the configuration down to the 
device. So the configuration 
message here, if you can switch 
over to my laptop screen for a 
second. 
    So this is an RGB value that
 is colon delineated. When I 
click send to device, this 
should transmit that message 
down. And the device will 
subscribing to that MQTT topic 
will read that off, parse it and
 set the LEDs on here to red if 
it's working
 correctly. Can you dim the 
lights
 up here? Let's update it a 
couple more times. 
I'll turn it back off. There it 
goes, again. This is a demo I 
threw together. This is really 
performing. Here's another
 demo.
That's itgoes, again. This is a 
demo I threw together. This is 
really for my demos. Thank you.

clicker. Okay. Great. Well, now 
that aye I've shown you the 
basic building blocks, that's 
all there is to the platform. 
It's kind of boring what you 
really want it to be. 
    Because, the really -- the 
most useful things are really 
simple. Now that I've shown you 
the basic building blocks O ever
 the Cloud IoT Platform, let's 
visit how offerings come 
together to light up scenarios. 
I call Introduction to IoT 
Patterns. In this basic example,
 devices connect to a Cloud IoT,
 Google Cloud IoT Core, and 
then, they -- all of those 
messages that come in go into 
that Pub/Sub topic, which is 
connected one-to of had one. And
 then we're using data flow to 
migrate to BigQuery and Big
BigTable. You can do those 
analytics in our powerful data 
analytics tools DataLab and Data
DataStudio. Let's look at the 
next example. 
    Here we have device and 
device communication. I imagine 
this is things, like, a doorbell
 on your front door and inside 
of your house, you've got a 
chime that gets act uated. To 
Google Cloud IoT Core which is 
connected one-to-one with 
Pub/Sub. And anytime that a 
message comes into, when that 
Cloud Function executes, it 
makes a configuration change 
using the Google Cloud IoT Core 
product back into another device
 which would ring the doorbell. 
And so, the next one I want to 
talk about. Let's talk about 
some Machine Learning scenarios 
here. Imagine you have a device 
with a camera and it's connected
 to Google Cloud IoT Core 
whenever a picture is taken a 
message is sent over to MQTT. 
And then, that message would 
then appear inside of Google 
Cloud Pub/Sub which is connected
 to Google Cloud IoT Core. 
Whenever the messages come in, a
 Cloud Function is called. And 
this makes a call to the Google 
Cloud Vision API which 
classifies that data and sends 
you back labels about that 
particular data. 
    And then, that label comes 
back, that structured data or a 
control message is sent back to 
a device. As an example, imagine
 you've got a door to your 
garage and a cat. And raccoons 
keep coming through that garage 
door or the little kitty door 
and breaking in and eating your 
cat food. You could  filter out 
the cabbing raccoons. You'd run 
through, when the Cloud Function
 returns, it would be populated 
with labels such as cats and 
raccoon. Only when the cat tag 
is present and the raccoon tag 
is absent would you unlock the 
door and allow the cat in. 
    And so, we have one more 
example with Machine Learning. 
And so, you have a device. 
Imagine you have all of these 
industrial, this industrial 
equipment that's out in the -- 
that's out distributed across, 
you know, your country. 
    And device tends to -- this 
message up to Google Cloud IoT 
Core. The oil viscosity and oil 
station, or the temperature, and
 all of that data goes into 
Google Cloud IoT Core which 
triggers a cloud function. This 
is the sensor data over time. 
And that's passed into Cloud 
Machine Learning engine model 
that then makes a prediction. 
    From that prediction, we 
then send control those devices 
from Cloud IoT Core or we use --
or we actually use another 
notification system to send 
messages down to the users as 
relevant. 
    As an example, imagine that 
you've got a carbon monoxide 
detector in a factory. Rather 
than going off when the levels 
reach a dangerous threshold, you
 could warn people when the 
levels are rising. 
    And so, these are the kind 
of things you can do with a 
Cloud IoT Core without having to
 -- without having to worry 
about having that functionality 
done to your device. One more 
example because I want to talk 
about Android Things in 
Firebase, it's listed to changes
 in a realtime database. It's 
connected to a light and 
potentially smart other consumer
 products and you want to do 
things, like, window shades and 
sound system so when you set; 
press a button in the house, you
 want for -- you want for it to 
be ready to play your movie. So 
it dims the lights. And it sets 
your surround sound up 
correctly. Device one transmit 
to Google Cloud IoT Core. You 
PRETSZ press a button or turn on
 your blu-ray player. It's 
connected with Pub/Sub and then,
 we use Dataflow to make sure 
the messages are coming in 
order. 
And that will then pump a state 
into a Firebase Realtime 
Database that's been listed by 
Android Thing device, and it can
 set all of those -- all of 
those Widgets inside of your 
house up to the modes you 
configured for them. We're using
 Firebase here because we want 
to share that state across all 
of those devices
 so that all of the changes can 
happen
 at once. Our beta has been 
public around a month. It's a 
great time to look at the 
platform. All of the 
documentations available here. 
And I recommend you start from 
the docs and try out the sample 
apps. That's all I have for now.
Feel free to ask me questions on
 Twitter. I left my Twitter 
here.
Post question to Stack overflow 
using the Google-Cloud-IoT tag. 
I'll be in after hours in 
conference hall 3. That's it. 
Thank you all for listening. 
[ Applause ]
       PAUL SAXMAN: Hello, 
everybody. I lead the
 Android Devices and Media 
Development part of the team. 
Dan Galpin will join us for the 
second half of the talk. Today, 
we're going to talk about 
building rich media experiences 
with Android. 
What are these rich media 
experiences I'm talking about? 
Media, being audio/video, 
playback and recording is used 
in almost all types of apps and 
Android with only a few 
exceptions. This means that the 
Android Media API are some of 
the most widely used outside the
 Core Framework. It's important 
to pay close attention. An 
inconsistent media experience 
can draw negative attention to 
your app. 
    So, we're going to talk, 
break this talk into three 
parts. The first part is about 
Media App Fundamentals. I'm 
going to talk about four 
important APIs you need to know 
about. I'm going to focus on 
four of the most important. 
Then, I'm going to talk very 
briefly about bringing apps to 
the TV. Because we have some 
updates since Google I/O that I 
want to talk about with Android 
TV. And the last part Dan will 
present on is tapping into PROE 
audio. There's a new API called 
OBO. We'd like our developers to
 try out and get 
feedback on. 
Four important APIs. Media 
Session, Audio Focus, picture in
 picture, the last one, not an 
API, but something very 
important for Media Apps to keep
 in mind. These are only for the
 most important APIs. Media 
player, whether it be the media 
player or exo player. Also, very
 important. These four are very 
fundamental to building a really
 core experience on Android. 
    The very first one, media 
session allows media controls 
external to your app. Media 
session is very important to 
Android devices such as TV and 
wear as it allows media playback
 on the phone to be controlled 
by these devices. 
    If the user starts media 
playback on the phone, plug it 
into an Android Auto capable 
head unit. The media controls on
 the head unit will control the 
media session on the phone. The 
same is true with Androidwear. 
If you have an active media 
session on the phone, you can 
control that media session using
 your watch. 
    An Android TV, Android TV 
can provide controls for media 
being played in the background 
if you're playing music. And 
it's also, the media session is 
fundamental to the Google 
Assistant on Android TV. If the 
user wants to speak, play, 
basically transport controls 
into the remote using the 
Assistant, the transport 
controls are passed to the app 
using the media session. 
    Media Session is used across
 the Android system to provide 
users with the ability to 
playback. For example, it's used
 by the lock screen interface. 
The Lock Screen interface will 
allow the user to control that. 
    It's used by Media Style 
Mode notifications that allow 
control for media playback in 
the background. Media button 
input. Buttons that you 
typically see on blutooth 
headsets. Play, pause, forwards 
and backwards. They're passed to
 your application via the media 
session API. And also, picture 
in picture uses the media 
session. Your application is 
slunk shrunk down to a small 
window size. If you have a media
 session, the user can playback 
controls directly on the 
picture-in-picture window. 
Design for apps playing media in
 the foreground is simple. They 
have an activity on some player,
 to add Media Session support, 
you first need to add a Media 
Session. This tells the Android 
system that your app is or will 
be playing media. 
    You can, then, add Media 
Session callbacks which allow 
you to combine controls, sent to
 the media session to your 
player. 
    And finally, you need to add
 a Media Controller which is how
 you bind media controls in your
 app. If you have a media button
 in your application, you bind 
that to the media session. 
    So if the user presses pause
 in the application, use the 
media controller to send that to
 the session. That session will 
update the media session 
callback so you can actually 
adjust the playback there. 
    But then, all of these other
 external interfaces that are 
using the media session will all
 be updated at the same time. 
Since you use the media 
controller. 
    So apps that allow audio 
playback in the background are 
quite similar in their use of 
media session. However, they 
have a service, service for 
background music playback. Are  
-- you only need the media 
controller when you actually 
have an active interface. 
Otherwise, those interfaces like
 the
 media notifications are done 
automatically using the Media 
Session API. Setting up a media 
session is pretty 
straightforward. Basically, you 
have to instashuate  media 
session. Basically, supported 
across most Android devices. 
    Then, you set flags about 
what your -- what you want your 
media session to handle. Here, 
the three options are transport 
controls, again, play/pause, 
play/pause/stop, seek, as well. 
The media buttons. So these 
media buttons from Bluetooth 
headsets. And cue commands, like
 adding things to a playlist or 
between items in a playlist. And
 then you have to set your media
 session up active. And this 
tells the system that the media 
session should -- is available 
to be interfaced with. 
    The other part that I 
mentioned is that you have to 
listen for media controller 
input in your media session. So,
 basically, you set the 
callbacks, which is a class that
 you have to create yourself. 
    The media session callbacks 
are very straightforward. They 
basically have all of the media 
controls you would expect, all 
the transport controls, the 
media button controls and the 
transport control interfaces. 
And basically, once your app 
receives these events, you pass 
them on essentially to your 
media player. 
    Using the media controller 
is likewise fairly 
straightforward. You  can use 
that media controller to fire 
off -- fire media control events
 or transport control, media 
button events. 
    If the media session happens
 to be in another -- in another 
process. Let's say, you have a 
background service that is 
running the media session but 
you want the media controls in 
your activity, you can actually 
get a token from the media 
session in your service, pass 
that up to your activity and 
then use that. 
    Media session is a fairly 
complex API. What I've showed 
you here, really scratching the 
surface. This is the fundamental
 thing to add this playback 
control into your application. 
The other thing, of course, 
Media Metadata. You can push 
information about the media 
playing to the session 
information. That will show up 
on the lock screen, 
notifications or your watch or 
your auto head unit. Media 
queues, playlists, you can add 
playlists to the media session 
and lets other applications 
providing interfaces into your 
app know about that. 
    Media controller callbacks, 
media style notifications, 
different APIs built in there. 
Getting started with Media 
Session, basically understanding
 what I told you today plus the 
other features. There's a lot of
 things you can do. 
Developers@Android.com. And this
 covers mostly -- mostly the 
media session APIs and related 
APIs. 
    We also published, recently,
 four-part articles. One of my 
co-workers, actually, 
four-part on understanding media
 session. 
This is probably where you want 
to start. This breaks down the 
APIs like I did here but in a 
little bit more detail. Also, 
several GitHub samples. Media 
controller and universal media 
player. These show various 
aspects of the media session 
APIs, as well. This is a great 
place to start for code. 
    The second I want to talk 
about is audio focus, sharing 
control of audio output. Any app
 can output audio at any time on
 Android. And outputting audio 
needs to be cooperate 
 And this is what the audio 
focus API is for. The audio 
focus API has been around in 
Android probably since the 
beginning, but it was updated 
with Android O. And I'll talk 
about the differences here.
    There are three steps to 
ensuring a great audio 
experience. The first step is 
before you play audio, ask for 
audio focus and only play audio 
if the Audio Focus is granted. 
VFR important to actually not 
start playback. If somebody else
 asks for audio focus, deal with
 the focus loss. Be ready to 
hand off audio focus from your 
app at any time. And when you're
 done with audio focus, release 
it. 
This one tells all other 
applications that I recollects 
can take control and use Auto 
Focus if they need it. 
Basically, you need to get the 
audio manager from a system. 
This is partially new. The audio
 attributes have been around 
since API level 1. And this is 
the first time the audio are 
used with Audio Focus. 
Basically, tells the system why 
you want to play audio. This 
gives the system an ability to 
make decisions about audio focus
 without necessarily kind of 
bothering the applications. 
    For example, if you're 
playing music in the background;
 you request audio focus and 
playing music in the background,
 if the system needs to play 
notification sound, for example,
 it will automatically duck the 
music playback and won't 
necessarily call into your 
application. 
    And this is -- this API here
 is actually new. The audio 
focus request. So basically, 
when you actually request audio 
focus, you now need one of these
 request objects. You pass your 
attributes. You create your on 
audio focus change listener. 
This lets you respond to the 
audio focus changes. And then, 
you request the audio focus by 
passing that to the audio 
manager. 
    Grabbing this result is very
 important. Because the next 
step, like I mentioned, you only
 play audio if Focus is granted.

different values. It has either 
failed. The system did not want 
to give you audio focus. 
    This could be the system has
 exclusive access to the audio 
at that time. An example of this
 might be a phone call if the 
user's actively in the phone 
call. The audio focus grant 
request could be granted. And at
 this point, you can go ahead 
and play your audio. And the 
third state is the audio focus 
request is delayed. 
    Again, this is if the system
 has exclusive hold of audio and
 -- but you can get audio gain 
later on. 
    And this is, actually, that 
audio gain is passed through to 
the audio focus change listener.
So the audio focus change 
listener, again, is how you 
listen for other apps to request
 audio focus. 
    Or if you audio focus is 
handed back to your application,
 you'll get the audio focus gain
 through the audio focus change 
listener. 
    And these are the four 
states. These have existed prior
 to Android O.
 You're either getting another 
app is asking for -- giving you 
the audio focus back, you're 
losing audio focus for an 
indeterminant amount of time, 
which means you should probably 
clean up your media playback or 
media player. 
The other focus laws could be 
transient which basically means 
you pause playback and then the 
last bit is that it can be 
transient but you can duck. 
Which means, you can basically 
turn down the volume 
temporarily. 
    And then, the last part I 
mentioned is releasing audio 
focus is very important because 
this is when you tell other apps
 that you're basically done 
using it and they can continue 
their audio playback if 
necessary. 
    Again, a lot of great 
resources for getting started 
with audio focus. We have a 
whole guide in our developer 
docs managing audio focus. Also,
 very recently, the same author 
wrote a three-part article on 
understanding auto focus. He 
breaks down the different use 
cases and also talks about the 
new APIs, as well. 
    And also, we have several 
samples for testing and 
demonstrating how it should work
 on your audio applications. 
    The third API is 
picture-in-picture. It allows 
your app to actually continue 
media playback even after the 
application has been paused. You
 can see in the example here, 
the user is doing a -- a video 
chat.
When the user actually goes back
 to the home screen, the video 
chat window shrinks down and 
also stays on top of other apps.
This API is important for media 
apps, especially video apps. If 
the user is watching a movie, 
for example, and want to go to 
the, basically, check something 
in their calendar, they can 
continue playback without 
actually -- they can check their
 calendar without actually 
disturbing the playback. 
    Three steps, essentially, to
 supporting picture in picture, 
you have to declare PIP support 
in your activities. You have to 
switch to PIP mode as needed. 
And you have to hide 
nonessential UI while in PIP 
mode. 
    And the way this looks, 
declaring PIP support, you do 
this in your manifest using the 
picture in picture. Manifest 
attribute. And the other 
important thing to remember 
about this is you should always 
if you're supporting picture in 
picture mode, you should add the
 config changes line. Basically,
 don't reinitialize, relaunch O 
your app during PIP mode 
transitions. 
    The next thing, you want to 
switch PIP mode on user demand. 
Put your application into PIP 
mode. To do that, basically, 
implement your button. And call 
the enter picture in picture 
mode. Picture in picture mode 
method that's part of the 
activity. 
    And that method, it takes up
 PRAMs. They're useful because, 
you can actually set actions on 
top of the picture in picture 
window, these are buttons that 
the user can press. And you can 
set the aspect ratio. If your 
screen is in portrait mode but 
you want the picture in picture 
window to be in landscape mode, 
you can set an aspect ratio to 
define that it should be an 
aspect or in landscape mode. 
    The other great thing you 
can do, too, we highly recommend
 this. You can switch the 
picture in picture mode when the
 user hits the home button. Use 
this by using on user leave 
hint. And you call the exact 
same API here. But once the user
 hits the home button, this will
 take your app into PIP mode. 
Again, this is a very nice user 
interface for media applications
 or movie applications. 
    The last part is hiding 
nonessential UI. The activities 
and fragments have on picture in
 picture mode change as methods.
And these methods are called 
when the application either goes
 in or out of picture in picture
 mode. Before what I mentioned 
about hiding the full screen 
UIs, basically hiding everything
 besides the video view or what 
you're using to render the 
video. 
    And, again, you can 
essentially if you're using 
media session in your 
application, if you have an 
active media session, you can 
hide all of the controls. And 
what the media session or PIP 
will show is media controls that
 will be passed into the media 
session. Another good reason to 
use media session. 
    A lot of great resources on 
getting to learn more about 
picture in picture. Definitely 
recommend so the picture in 
picture guide in the developer, 
on the developer docs have been 
updated. It has all of this 
information in it. We also 
released two articles recently, 
one of my co-workers Benjamin 
Baxter, making magic moments 
with picture in picture, 
basically describes the 
fundamentals of using Picture In
 Picture and the using around 
it.
And talks about using things 
like the home button to switch 
the app into Picture In Picture 
mode. And the last one, last 
API. Isn't exactly an API, but 
Wide Aspect Ratio screens. 
    Basically, if you don't do 
the right thing, your 
application is going to be stuck
 in 16 by 9 mode even though the
 device screen could be 18 by 9.
What that means is your 
application will only show up in
 the blue area on the screen. 
And the red area will be just 
rendered black, essentially, by 
the system. 
    And why -- the reason this 
is very important for media 
developers is if you're playing 
back video and the user wants to
 view that video in landscape on
 their full screen and they see 
these bars on the sides, it's 
going to be a fairly negative 
user experience, especially 
since these phones are kind of 
-- these wide aspect ratio 
phones are premium devices. 
    So the way you want to 
actually filling the wide aspect
 ratio screens, going 16 by 9, 
target API 26 or later. With 26 
plus devices, the max aspect 
ratio is the native aspect ratio
 of the device. 
    Another thing you can do if 
you can't target 26 plus. If you
 target 24 plus, your apps by 
default or your activities by 
default are resizable. 
    Resizable that's used by the
 system for multi-screen. And 
basically, if an activity's 
resizable, it supports any 
aspect ratio, including the wide
 screen aspect ratios. If you're
 not targeting 24 plus, you're 
not targeting 26 plus. You can 
make your app resizable. And the
 last thing you can do, you can 
actually specify a maximum 
aspect ratio. But this is the 
last resort. What you should 
really do is one, two, or three 
above. Again, this is only for 
23 or under. Targets API because
 at 24 or above, these are on by
 default. You can set these 
manifest attributes either in 
your application and it'll apply
 to all activities or you can 
specify it on the activities 
themselves. Specifying a mass 
aspect ratio. Again, if you're 
targeting 26 plus, the mass 
aspect ratio is the native 
aspect ratio of the device. But 
for 25 or lower, you can use 
this metadata tag to specify 
that your app should support 
these wider aspect ratios. On 
the Value 2.1, basically, 18 
divided by 9 is going to be 2. 
Using 2.1 means you'll use not 
only 18 by 9 but even wider 
screens. 
    The problem with this, the 
reason that we say you should 
make your application or make 
your activities resizable. 
Instead of using this, though, 
is because, you know, what 
happens when there's 2.2 aspect 
ratio screens? You're going to 
have to go in and update your 
application. Ideally, you'd 
support this directly. 
    Before I call Dan up on 
stage, I'm going to talk really 
briefly about bringing some 
updates about Android TV. So at 
I/O, we announced the Google TV
 was coming. Both have launched,
 the home screen is available on
 the Nexus player.

four ways to integrate Android 
TV and the Assistant. You can 
integrate content in the watch, 
you can create channels and 
programs, implement global 
search. Or you can implement 
media sessions. 
    Global Search provides 
search to the Assistant and the 
media session provides media 
controls via the assistant. I'm 
not going to go into technical 
details about all of these. 
Specifically, we covered a lot 
at Google I/O and the API hasn't
 changed. The first way you can 
integrate on the new home 
screen. This row is intended for
 showing content of users in the
 middle of viewing new content, 
say a new episode that was 
released recently or a next 
episode in a series. 
    And developers can add 
content to the next -- watch 
next. And apps can allow users 
to add content manually. If the 
user's inside your application 
browsing content, they can click
 on a button, ideally click on a
 button inside the application 
and have that content added to 
the watch next row. 
    Apps can also create awesome
 programs which can be displayed
 on the home screen. Programs 
that your app maintains. 
Applications get one default 
channel, channel row so when the
 application is installed and 
you populate a channel, that 
will actually show up 
automatically on the user's home
 screen. 
    Applications can have 
multiple channels, though. And 
it's up to you to kind of 
determine how many channels you 
want and how you configure 
those. 
    The first way to integrate 
with the Assistant on Android 
TV, using a Global Search 
Provider. If the user is either 
searching for content or wants 
to play content. If they use the
 Google Assistant and say play 
Big Buck Bunny. Cross over the 
applications and look for that 
content. 
    If your application has an 
exact match, it'll actually show
 up at the top of the list. If 
your application has multiple 
matches, it'll show up below. So
 I  ideally, you can find an 
exact match for the user for 
these queries. 
    If there's only one 
application has an exact match 
and the user's requested to play
 the content, that content will 
actually be automatically played
 within your app. The user 
doesn't have to go through the 
selection process. 
    And the other thing I 
mentioned, too is voice-based 
media controls via an assistant.
If your application supports 
media session and the user 
invokes the Google assistant 
while your content is playing, 
they can issue transport 
controls via voice. So 
play/pause, seek forward, seek 
back ward. This is an important 
reason to consider using the 
media session when building your
 media applications. 
    So a lot of areas for 
getting started with the new 
Android TV home screen. So we 
have several guides in our 
developer content. We also have 
for Google I/O, we built a code 
lab that went through the 
important parts ability 
integrating with channels and 
programs. Recently, we released 
a lean back sample that shows 
you how to use the assistants  
and tie that in with the 
leanback API. 
    And with that, aisle  I'm 
going to hand off to Dan who is 
going to talk about a new 
exciting API that we have in 
Android.

 time and I'm losing my voice. 
First of all, we did -- we added
 a new Pro Audio API. And we had
 Open. But we wanted something 
easier to use and easier to 
improve. Now, all the audio APIs
 end up going through Audio 
Flinger for the framework. Since
 changes could potentially break
 a lot of code, there's a new 
service where we can do radical 
things. If you're running the 
developer preview on a pixel or 
pixel 2 device, you actually can
 get really reduced latency 
thanks to this framework. So 
very,
 very cool. Uses the concepts of
 streams, flowing from the mic 
to the app and then back to 
headphones. These audio streams 
are really easy to create. Made 
this 
API much more straightforward. 
You can set this up and make 
multiple streams, which is 
pretty slick. 
In this code, we have the A 
Audio which gets you a default 
stream. And here's how you 
request specific values for 
direction and for format. 
    And once you set up a 
stream, you call Open Stream on 
the builder. Pretty 
straightforward. And now, if you
 don't set things like sample 
rate and format, you'll have to 
query them. And finally, the 
frames per burst from the audio 
stream to get the optimal read 
and right size. Let's stop for a
 moment. I know your eyes are 
all glazing over and you're 
like, this is only in the O 
release. 
How is this helpful to me? But 
we've released a library to 
help. And this is what I'm 
excited about. This is a C + + 
API and works almost identical 
to after that accident Audio. 
And works on old and new 
versions of Android. Old 
versions of Android, it's going 
to end up calling into Open 
SLES. This really makes it 
easier to write 
high-performance, low-latency 
applications on all Android 
devices. So here is what A-audio
 looks like and here is what OBO
 looks like for the exact same 
code. As you can see here the, 
once you've used one, it is 
really easy to use
 the other.  AAudio, this data 
is written in these bursts
 to  -- it's two bursts O data. 
And the first burst will be 
written. 
    As you might expect, you can
 start and pause or stop your 
stream all with the asynchronous
 calls. And honestly, for most 
use cases, that is optimal. We 
do have a function for 
synchronizing if needed. 
    And the real question, how 
do we get data in and out of the
 streams? Let's take a simple 
use case here. 
    Perhaps, the don't need 
super low latency, blocking 
rights like this here on a loop.
Basically, the blocking right 
function returns. The number of 
frames is written. Now, the 
blocking right can
 time out. We need to make sure 
we're handling this case. When 
you need lower latency, you can 
actually -- you need a higher 
priority thread to do your audio
 processing. Or perhaps, even 
one that's
 scheduled with Sked. OBO will 
pass you the stream parameter, 
any data you need, the number of
 frames and then you can render 
your audio right into the audio 
buffer on the high-priority 
thread. And you set this call 
back up in the builder. 
    So the question is, how do 
you combine multiple streams? 
Such as if you're making, taking
 two or more input streams and, 
you know, sending them to an 
output. And we recommend using 
one output stream as the master 
callback. And then you do the 
callback from the master stream.
    Now, since the current OBOE 
implementation does not support 
audio input, I've switched back 
here. And the simple example, 
we're passing data from input to
 output. We can read the input 
from the 
input stream to do a nonblocking
 read and that goes into the 
output buffer. There we are. 
That's what I wanted. So, again,
 you read from the input to the 
output stream. 
    Finally, let's talk about 
how you would do Dynamic Latency
 Tuning. You're doing the low 
latency audio applications. 
    So looking back at that 
previous diagram, we have the 
two bursts ready to be consumed 
by the dac. And we may want to 
add another buffer to our 
render. 
    Can you PATSZ me  the other
 clicker? You can just add 
another burst to your buffer 
size. So once, again, here's a 
basic OBOE program using a 
clicker? You can just add 
another burst to your buffer 
size. blocking right, very
 straightforward. Here's the 
code. This is only a partial set
 of codes. We have simplified 
what it takes to do these kind 
of professional low latency 
audio applications. And that's 
all you have from me. So thank 
you so much for coming. And 
again, sorry my voice is a 
little scratchy. 
[ Applause ] 
   Mete Atmel: Hello, everyone. 
My name is Mete Atamel.

   MARC COHEN: Namaskara. 
   Mete Atmel: Thank you. Today,
 we're going to be talk about 
fundamentals of the Google 
Platform. In 30 minutes, it's 
going to be a fast-paced talk. 
We're going to have a lot of 
things to talk about. Hopefully,
 you'll learn something new 
today. 
   MARC COHEN: Yeah, we'll start
 with a little bit of history. 
There's a really cool website 
called the waybackmachine. If 
you plug in the Google Cloud 
Platform or search for Google 
Cloud Platform from back in 
those days, you'll see what the 
landing page looked like. This 
is it. How many people were 
using Google Cloud Platform in 
2012? Okay. A few diehard folks 
there. It was very simple back 
then. There were only four 
products in the entire platform.
So it was pretty easy to get 
your -- to wrap your head around
 it and understand what it could
 do. 
    Nowadays, it's a little bit 
different. We now have over 80 
services. And that list 
continues to grow every day. In 
fact, there's so many different 
capabilities and services that I
 can't fit all of the little 
product icons on
 one slide. It's nice we have 
all of these. It's more than 
nice. It's really enabling and 
powerful. You can build almost 
anything you can imagine in the 
cloud nowadays. 
But it comes at the cost of a 
lot of complexity and a lot of 
kind over cog anynitive load. It
 takes quite a bit of effort to 
figure out what all of these 
services are, when to use them 
and how to use them effectively 
in your applications. And that's
 really the point of this 
session. We're going to take you
 through an overview. We're not 
going to go into too deep into 
anything but give you a taste of
 the capabilities we have in 
Google Cloud Platform. 
    To start off, we will cover 
all of the compute options. 
   MARC COHEN: Thanks, Mete. 
Let's say you have a piece of 
code, what options do you have? 
That's what we want to cover 
here. At the high level, when we
 talk about compute, there are 
three distinct ways of running 
your code. And this is not just 
Google Cloud, but basically, any
 cloud you have these options. 
Back in the day, when -- before 
cloud, when you want to deploy 
your application, you would get 
a machine, you would decide how 
much CPE you would want, how 
much memory you want. You would 
get a hard drive and decide how 
big that should be. Then, you 
would install the operating 
system and after that, you 
install the libraries that you 
need on top of the operating 
system. And then, finally, you 
would get to install your 
application. 
    With virtual machines, it's 
pretty much the similar thing, 
except it's virtualized, it's 
not a physical machine anymore. 
You probably don't have it 
yourself. You have it in someone
 else's data center. It's the 
same idea. You pick your CPU, 
your memory, your storage. And 
then, install the operating 
system and it's yours. Your 
responsibility to maintain it 
and to run it. 
    More recently, we have 
something called Containers. The
 idea is that instead of 
virtualizing operating system an
 old way up to the application, 
you want to virtualize your 
application and its dependency. 
Since you're not virtualizing 
the whole operating system.
 In serverless, you don't care 
about virtual machines, you 
don't care about containers, 
piece of functionality or an 
application.
And you just In want to deploy 
it and let someone else manage 
it for you. It becomes someone 
else's problem, basically. How 
do to deploy the application. 
You're focussing on your code 
and let someone else manage that
 for you. And one thing to 
mention is that most people -- 
they start with Virtual 
Machines, they're switching to 
containers because it makes 
sense and they're much more 
agile than virtual machines. 
Most people, nowadays, are in 
the container world. But 
containers by themselves are
 not enough. You need health 
checks, redundancy. You need a 
way to find a configuration and 
get your not enough. You 
configuration to your 
application. 
    Because of all of these 
things you need to do in 
production, Open Source projects
 which try to run your 
containers in production and 
make it easy for you. And even 
containers by itself is not 
enough.
 Make sure they run. But then, 
you need
 more than that. You need some 
kind of dashboard, log-in, all 
of the kind of things you need 
to do. So that's why we have 
something that is an Open Source
 project.
 This logging and monitoring, 
and service authentication and 
all of that good stuff. In terms
 of what options on Google Cloud
 to 
deploy this. The more 
customizable things are, they 
end up being less managed and 
less customizable. 
    On the higher customizable 
side, we have compute engine. 
You can get an Linux machine or 
windows machine and support 
multiple versions of Linux or 
windows. And once you have the 
machine, you can install 
whatever you want on that 
machine. It's your machine to 
maintain and keep up to date and
 all of that kind of stuff. 
    Of course, installing all of
 this software on your computer,
 it takes a while. So we have 
something called Cloud Launcher.
It's the marketplace for 
solutions to deploy. If you want
 to deploy, for example, work 
press, there are solutions for 
that so you can just find a 
solution and with one click just
 that solution to Compute 
Engine.
Let me quickly show you how this
 works. 
    Here I am in Google Cloud 
Console. This is where all of 
the Google Cloud products are. 
And we are interested in compute
 engine. When you get the 
compute engine page. Here, you 
can see all the VMs, virtual 
machines that you have. If you 
want to create a new instance, 
you create instance, and then, 
you can give your instance a 
name. 
So, we will call this -- we will
 make this bigger. We will call 
this instance India. And you can
 choose where to deploy. Let's 
choose somewhere in Asia. You 
can customize the machine type. 
Basically, choose how many from 
1 to 64. And also choose your 
memory. On top of this, we also 
have preconfigured machine types
 so you can choose a 
microinstance or smaller 
instance. I pick one of these. 
Next, you want to choose the 
operating system. So we have all
 of the flavors of Linux and 
windows, as well.
 TLSTZ an  application image for
 that. You can create your own 
images and you can  use it in 
your own project. I'm going to 
use Linux instance and allow the
 best traffic and hit create.
 Just clicking here and you can 
directly go right in there. 
    And also mentioned Cloud 
launcher. If we go here, there's
 something called Cloud 
launcher. Just clicking here and
 you can directly go right in 
there. 

This is a solution for deploying
 Windows server, IAS, and SQL 
Express. Give
 it a name, and then, oh, let's 
say India 2 because I already 
have one. And this time, let's 
choose Australia. And just keep 
the defaults, hit -- well, so 
with this one click, I can get a
 Windows Server, SQL and deploy 
my applications to Google Cloud.
    All right. So that's Compute
 engine. In the highly managed 
part, we have Cloud functions. 
And the idea is that you define 
a function with inputs and 
outputs and you also define how 
the function should be 
triggered.
 It can be a Pub/Sub message. 
When it goes to the topic, that 
triggers the function for you. 
And that's it, you deploy the 
function and Google maintains it
 for you. And you don't really 
care where it's running, how 
it's running, you express where 
it should live and that's it. 
Show you an example of this, as 
well. 
    If we go back here under 
compute, we have Cloud 
Functions.

see the functions you have 
deployed. I have a function 
called &quot;Hello, World&quot;. Once you 
click here, you'll see the 
function implication, how many 
times people hold your function.
You can see what kind of trigger
 it has. This is a Pub/Sub 
function. You can see the social
 function and it's a simple 
function that takes an event and
 logs out the events message. 
And you can even test it right 
here.
You can trigger the event and 
say, let's pass a message and 
say &quot;hello, India.&quot; And if you 
hit test function, this is in 
the cloud and it'll fetch the 
logs and you'll see the log out 
once we get the logs here. 
    So that's functions. 
Functions are really great and 
they're a really easy way of 
running your coding in the cloud
 in a way that it's, like, 
completely infrastructure is 
completely transparent to you. 
Sometimes, you need more than a 
function, you need an 
application. For that we have 
App Engine. You deploy your 
application and you don't really
 care the way that application 
is running. It's more than a 
function. You can have multiple 
services in app Engine, you can 
have a front end and back end 
and deploy the whole thing 
together and App Engine manages 
that for you. 
    Let me show you that 
quickly, as well. In Console, 
it's the same place. We go here.
There's a dashboard. So you have
 multiple versions of the 
application. And this version is
 getting 51% of the traffic, 
this is getting 49%. And you can
 see the traffic, CP utilization
 and things like that. You can 
deploy App Engine, just one 
default service. And under the 
versions, you can see the 
different versions. Two versions
 running on two instances, two 
VMs, and this is auto scale by 
default. 2 to 20 so you don't 
have to do anything special for 
that. 
    If you want to change the 
traffic allocation so that, 
let's say your new version gets 
all of the traffic, I can easily
 change this to 100%, it's 
saved.
And this will direct all of the 
traffic now to the new version. 
    That's App Engine. And the 
last thing I want to
 talk about is Kubernetes 
Engine. Basically a way to run 
containers in the Cloud. And it 
can run anywhere. 
But we tried to make it really 
easy to run in Google Cloud, 
that's what Kubernetes Engine 
is. We maintain the master and 
give you an easy way to create 
your cluster and schedule your 
containers. 
    It sits in the serverless 
world and the virtual machine 
world. Somewhere in the middle, 
you can use Kubernetes Engine to
 do that. And we have tools, we 
are container builder and 
container registry for 
containers. You can create in 
the cloud using Container 
Builder. And they're saved in a 
place called Container Registry,
 a private space for your 
containers. And you can deploy 
to Kubernetes or App Engine 
easily. That's how you run your 
code in the cloud. And one thing
 ICHT to  point out, when you're
 running the servers in the 
cloud, you're running in the 
same network that powers all of 
these products. The same network
 that powers Google Search, 
Gmail, Map, serving one billion 
user each, you're running in the
 same network, as well, as all 
of the improvements and 
optimizations we do in our own 
services, they benefit you 
incorrect directly because of 
that. And this is what our 
network looks like. You can see 
of the edge end points and the 
cables around the world. It's a 
truly global, highly optimized 
fiber network all around the 
world. 
    Okay. So that's how you run 
your code. Let's take a look at 
how you store
 your data. At
   MARC COHEN: Thank you, Mete. 
I had the feeling that some of 
the folks here are looking at my
 graphic and wondering what 
these things are.
 It's a young crowd. 
Actually, it's pretty 
complicated stuff. Anybody that 
implements a real world storage 
system knows about this. There's
 all kinds of consistency 
issues, acid see manual  
semantics, scaling issues, rely
 complications that crop up. And
 there's a lot of different ways
 to store data, right? 
    This is a very high-level 
summary of all of the different 
products and what they're good 
for. It's a bit of an eye chart.
Not going to walk through every 
product one at
 a time. I will kind of break it
 down into some categories. When
 I look at this, I have the same
 reaction I had with the 80 icon
 slide. I just want to store 
some bites, why do I have to 
learn this stuff? I'm going to 
go through a few different 
subcategories to hopefully help 
clarify this. 
    The first category I like to
 think about is structured data.

spreadsheet.
 Where you have well-defined set
 of fields and the rows in the 
spreadsheet correspond to the 
instances of the data and the 
columns
 in the sch EMT 
schema. And it's driven by SQL. 
And we have two different 
products in this domain. We have
 Cloud SQL which gives you the 
ability to create managed 
database servers in the cloud. 
You're still thinking about 
servers, and you have to decide 
if you want My SQL, the backup 
of the Database, the 
administration of it, the may 
not  maintenance keeping it up. 
And managing the servers is 
taking care of it for you. And 
Spanner is a level of 
abstraction above that. It gives
 you a true database as a 
service capability where it's 
global, has all of the semantics
 you would expect from a 
relational database management 
system. And it's multi-region, 
very Scalable and has really a 
tremendous set of value for a 
database service.
    The next category is 
unstructured data. This is also 
called no SQL. It's a little bit
 like library. You're storing 
data in a library or books in a 
library, I should say. So 
there's a very efficient index 
to find any book on any shelf. 
But the contents is totally 
unregulated, right? The 
librarian doesn't care what's in
 any particular book and there's
 no uniformity. All of the books
 have different content. And so,
 it's kind of a little bit like 
what a NoSQL Database is like. 
The key gets you to the value 
you care about, but the content 
or structure of that value is up
 to you. 
    And we have data store, 
distributed key value store, 
Firestore, which is ideal for 
Mobile applications and realtime
 response scenarios. And Big
Table well-suited for huge 
volume, transactional, financial
 transactions, IoT, even 
sources, things like that. 
    And all of these are, again,
 managed services in the cloud. 
You don't have to think about 
how many do I need? How do I 
configure and install software? 
Just make your database calls 
and everything's taken care of 
for you. 
    The last category is an 
Object Store. It's a lot like 
renting a storage unit at one of
 these places where you do that.

something like that. And you 
know, there's a physical space 
there where you can put your 
stuff. And it doesn't matter 
what you put in there, it's up 
to you. And that's kind of what 
these object stores are like. 
    You have a bucket, and you 
own that bucket, and you can put
 as many objects in it as you 
want. There's no rules about the
 content that goes into them. 
And you have security 
protections on it, which is your
 key to the locker. And you can 
serve those objects from the 
cloud. You can serve websites 
directly from the storage 
buckets. 
    So that's kind of a quick 
overview of the different 
storage products we have. 
There's a lot more to it. But I 
did want to jump into Spanner, 
again, and just highlight it  
because I think it's really 
interesting and revolutionary in
 many ways. What Spanner does is
 bridges the gap between 
traditional relational database 
systems, which are very strong 
in terms of data integrity and 
consistency but have always been
 a challenge in terms of 
horizontal scalability, right? A
 lot of people have to do lots 
of elaborate techniques to scale
 up the systems. 
    And then, on the other hand,
 you have the nonrelational 
systems, the noSQL systems great
 with horizontal scaling, but 
often not giving strong 
consistency, or as much 
semantics. Spanner gives you the
 best of both worlds. 
    I'm
 going to quickly jump into a 
little demo on
 see it. There we go.  Spanner. 
Okay.
I've got an instance of Spanner.
You can think an instance of 
collection of databases. I've 
got one here
 called GDDIndia, and inside 
that is a database called 
University. I've tab fabricated 
a university consisting of three
 tables. I can drill down into 
these tables. Professor has a 
collection of professors, with a
 professor ID and a name. And 
basically, looking at something 
very much and feels very much 
like a traditional
Don't relational database 
management system. But I have no
 concept of where the software's
 running or keeping it 
administered or anything. It's 
all taken care of
 for me. Let's imagine the 
student table needs another 
field. Let's decide to add. A 
name, major and student ID. 
Let's add a GPA. We'll say GPA 
and give it a float type. 
Done and
 save. And spanner will 
basically tell me it's going to 
eventually -- the old schema 
will be served until the update 
completes. It's going to update 
my database in realtime. It's 
never going to take the service 
down. I can continue to serve 
tremendously high volumes of 
requests while it's updating my 
database in the background. 
Which is really convenient. The 
other nice thing is let's 
suppose I'm wildly successful 
with my database and the traffic
 far exceeds the capacity. I can
 simply drill down into my 
instance. I can click edit 
instance and there's a place 
here called Node, a field here 
called Node. I can change that 
from one to three and click 
save. I'm going to pay for more 
capacity if I use it, obviously.
But I've just tripled the 
capacity of my database. I 
didn't have to talk about 
machines where they might live, 
what I might do with them. I 
said triple my capacity. And the
 database is now going to be 
able to serve three times the 
amount of requests. 
    And that's it for Spanner. 
Next up, we'll cover BigData. 
   Mete Atmel: They're going to 
store a lot of data. And then, 
data gets really big. How do you
 deal with big data? We're going
 to cover here. So big data 
processing at Google
 started with macproduce. It was
 a paper in 2004 that came out. 
You can read it. It's a nice 
paper to read. And we explain 
how to take lots of data, divide
 it into small chunks and send 
them to multiple machines and 
process those on those machines 
and combine the results in the 
end to get the result.

like Big Table, we had 
high-level pipeline modelling. 
And we had Spanner that Mike 
just talked about. And most of 
these, they were either papers 
or they were internal 
implementations at Google. It 
wasn't available to outside 
world. Recently, of course, 
making these available to 
people. Pub/Sub and Spanner, 
services in Google Cloud that 
you can use. Back in the day, 
they weren't available to 
people. So people they seen that
 and they read these papers and 
started creating Open Source 
projects. They created Hadoop
  and all of the Open Source 
project. But we had two products
 in Google Cloud to support. All
 of the innovation at Google 
ended up with Cloud data flow, 
it's the model and also a 
service, fully managed service 
to do batch and stream 
processing. And if you are doing
 Hadoop and SPARQ in the Open 
Source, you can bring those 
products. There's a product 
where you can get a cluster in 
90 seconds and run your Hadoop 
in Google Cloud. When you're 
dealing with Big Data, there's a
 life cycle that you need to go 
through. 
   MARC COHEN: Yes. So the 
typical life cycle involves five
 steps. First off, you need to 
capture some data. And there's a
 lot of products available to do
 that. The one I'm going to 
highlight today is Cloud 
Pub/Sub, which is a message 
queue. You can have multiple 
providers and subscribers. And 
so, we can motivate this a 
little bit with a real world 
example. Imagine that we have a 
very popular website like 
Wikipedia and we want to capture
 all of the events from people 
viewing pages on Wikipedia. We 
could stream those events into 
Pub/Sub and have the events 
routed into the data processing 
phase. And the data processing 
might involve something like 
dataprep, which is a data 
cleansing, rearranging, 
normalizing service. And then, 
from there, it might be fed into
 Cloud Data Flow, perhaps. And 
it's just as Mete was speaking 
about, a processing stage based 
on Map Produce. It can perform 
mass bulk processing on batches 
at a time or streams as they're 
arriving, which is great for IoT
 and transaction-based things. 
    From there, once we've 
processed the data, we might 
want to store the intermediate 
results so we can analyze it 
without having to continually 
reprocess it. There are lots of 
places we can store it as we 
just saw. 
    One of the key ones if we're
 doing analytics might be 
BigQuery. BigQuery gives us this
 columnar database that enables 
very fast SQL-based storage. 
From there, we can use tools to 
analyze the data. There are lots
 of different tools, but the one
 I'll show you in a moment is 
BigQuery. We've got it in the 
Database and we'll do some 
queries on this. 
    And finally, we might want 
to do something with these 
analytics. We've done our 
interactive analysis and we 
might want to have a nice way to
 share it with other folks, 
co-workers, the rest of the 
world. And that's where Data 
Studio comes in, for building 
dashboards, nice visualizations 
and Data Lab, which is a hosted,
 in the cloud
 Jupiter Notebook Service. In 
the data science world, datalab 
is something you might want to 
take a look at
     Here's what BigQuery looks 
like. And what I'm doing is 
running a query on a dataset 
that actually corresponds to the
 example I just talked about. So
 I have all of
 the data for page views on 
Wikipedia from May of
 2016. And I'm going to run a 
simple query that's going to sum
 the requests. We can wait while
 it does that. And tells us 
hopefully soon. It tells us how 
much data was processed. It was 
38.6. Almost 40 gigabytes worth 
of data. It just did a scan. In 
five seconds and told us the 
number of events, as you can see
 here. I think this is 19 
billion. Almost 20 billion rows 
it scanned in that amount
 of time. Now, we'll make it a 
little bit more complicated. 
I'll do a query here for all of 
the articles that reference 
Bangalore. There are two 
spellings Bangalore and 
Bengaluru. And the way I'd like 
to find both of those is to do a
 regular expression. Here, I'm 
Bangalore. There are two 
spellings Bangalore and 
Bengaluru. And the way
matching Bengal 
or-aluru.
And this is going to run a 
regular expression match on 20 
billion rows in a database. 
    And you saw it did that in 7
 1/2 seconds. And by the way, 
this no-cash results up HEEFR is
 an option you can specify. I 
want to see the cache for
 speed reasons.here is 
an option you can specify. I 
want to see the cache for speed 
reasons. Not only a table scan, 
but a regular expression on 20 
in 20 billion rows in 70.5 
seconds. That's the power of 
BigQuery. 
   Mete Atmel: Lastly, Machine 
Learning. If you look at this 
chart, this basically shows the 
amount of Deep Learning models 
at Google with different 
products. You can see the growth
 at Google. And this is true not
 just by Google but pretty much 
all companies. 
    So when it comes to Machine 
Learning, two distinct ways of 
Machine Learning. Let someone 
learn Machine Learning or train 
a model useg Machine Learning 
and consume using an API. That's
 the easy way. But sometimes, 
the train model is not enough 
for you. Because, you know, 
maybe the train model is not 
exactly what you want. In that 
case, you need to actually build
 your own machine model. Create 
your own model, train it in 
parallel and you also want to 
serve that model. So that's the 
second way of using machine
 learning. At Google, in terms 
of using the models, a number of
 APIs. Speech API, Vision API 
and more and more that we keep 
adding. This is the way you can 
consume machine learning. And we
 are exposing the model for 
years to you with a simple API. 
Let me show you the vision API. 
    The Vision API. I have a 
demo here. You can pass the 
image and it tells you 
everything it can about the 
image. In this case, we have a 
cat image. In any Machine 
Learning demo, you have to show 
a cat. That's the rule. 
    What you get is you 
basically get a JSON, but in a 
graphical way, you can basically
 see that the Machine Learning, 
telling us this is a cat, mammal
 97%, and it's telling us it's a
 British short hair cat, 9 
%. If they have an image with 
text like this one, it can pick 
the text from the image. This is
 a traffic sign. Telling us it's
 a traffic sign and from here, 
it can pick up the text and tell
 us where it is in the image the
 text is. And I'll show you one 
more with people. 
    So when we have people in 
the picture, Vision API doesn't 
detect people, detect people's 
expressions. In this case, for 
example, it's telling us it's a 
social group. Which is right. 
But then, if I turn this on, if 
I go here and turn this on, we 
can see people, faces and then 
see that person, too. I guess 
this person is joyful; which is 
nice. That's Vision API. 
    And lastly, we want to talk 
about building your own Machine 
Learning models. You cannot 
always consume as it is. To 
build them, there's something 
called TensorFlow. 
   MARC COHEN: TensorFlow is the
 Machine Learning underlying at 
Google. It was created by the 
Google Brain team, released two 
years ago and become one of the 
most popular Open Source Machine
 Learning libraries on GitHub. 
It's powerful, you can build 
your models locally, use them to
 deploy models on mobile devices
 and it's also got great support
 for GPUs and other types of 
hardware assist. So very 
powerful. There's a link here. 
And I'm sure, we'll get the 
Slides to you through the 
organization here. 
    But there's a link to a 
really nice article by one of 
our colleagues that gives a 
great overview of TensorFlow.

quickly how much TensorFlow has 
taken off. There are a lot of 
tools out there. And it's good 
to know a few of these, I think.
But TensorFlow seems like a good
 one to invest in because it's 
gotten so much support so 
quickly. 
    The product that we're using
 to kind of make this stuff more
 approachable and more 
accessible is Google Cloud 
Machine Learning Engine. And the
 idea here is that just like in 
the case with Compute where you 
don't necessarily want to care 
about the details of the 
underlying infrastructure. You 
want to write your code and 
think about your application, 
the same kind of thinking 
applies in Machine Learning. You
 want to think about your
 model. You don't want to be 
burdened with the lower-level 
stuff that Google's good at and 
has lots of people doing all the
 time. This gives you that kind 
of abstraction layer. You can 
specify kind of a meta 
definition of your model model. 
And then you can use the G Cloud
 command for uploading for 
training.
Monitor the progress through the
 same tool we used to monitor R 
our other applications. And 
then, you end up with trained 
models you can operate on like 
in App Engine, you can test them
 through independent URLs and 
test those. It's taking you away
 from thinking about machines 
and implementation details to 
operating on your model as an 
abstraction of its own. 
    The other powerful thing, 
you're going to get access to 
the Tensor Processing Unit, 
which is custom ASIC hardware 
that Google has developed for 
its own internal use. The 
initial version internally only 
for on the order of a year and a
 half. And it was very powerful 
but only for training models. 
And the new version, which is 
becoming more available publicly
 is much faster and it's also 
available for both speeding up 
training and serving of Machine 
Learning models. 
    So that's all we have. I 
want to leave you with some 
resources. The main site for all
 information about Google Cloud 
is cloud.Google.com. I'm a great
 lover of code labs, interactive
 training tools and if you go to

G-code/codelabs, on the order of
 400 of them. And there's a code
 labs area that many of you have
 probably seen. You can get the 
cloud-specific code labs with 
G.code/codelabs. And there's a 
traininging page for training 
programs. The one important 
thing, the free tier, 
cloud.Google.com/free, you can 
get $300 worth of Cloud credits 
for the period of one year. This
 is a great way to get started 
without having to commit any 
money up front. 
    And I think we both believe 
that the best way to learn about
 this technology is to actually 
build something with it. I'd 
encourage all of you if you're 
not working with it. Try it out,
 use the free tier. Try building
 applications and see what your 
experience is like. And we 
always love to hear good or bad 
what you're finding. Let us 
know. 
    We will be
 over in the Cloud office hours 
area after this talk. That's all
 we have. Thank you very much 
for your attention. 
   Mete Atmel: Thank you. 
[ Applause ]
. 
    Before I begin, how many of 
you run websites with structured
 data? Mark-up? Okay? Fair
 few. 
In this session, we'll be 
looking at the basics of
 structured data, what 
structured data is. How Google 
uses structure data and also, 
what features are powered by 
Structure Data. We'll also be
 looking at schema.org, and 
transports. And also, resources 
that Google provides, website 
owners so they can debunk any 
structure data issues they may 
have. And finally, we'll also 
look at some best practices for 
websites which have multiple
 languages. 
    I'm sure many of you are 
aware that Google Search has 
been evolving to provide a great
 user experience for all of our 
users. Since 1998, we've moved 
from blue links to many features
 like, live scores, election 
results. 
    Today, without structured 
information from the masters and
 content providers,
 we are still limited at what we
 can do. 
Depends on website owners 
providing structured data to 
Google. Only with structured 
information Google can provide 
users with accurate and 
interactive information. 
    So in this context, 
structured data is nothing but 
the standard format in which you
 provide us, you provide us or 
any of the search engines your 
information. 
    So as an example on the left
 screenshot, without structured 
data, the user or the webmaster 
is telling Google writing about 
the movie &quot;Godfather&quot; or 
relevant information about the 
movie. Google understands 
they're talking about a movie 
and that the name of the movie 
is the &quot;Godfather&quot; and
 the director, similarly, we're 
able to tell that the recipe and
 he's given relevant information
 about the particular recipe. 
    We understand the mock-up, 
but how does Google use it? 
Well, let's assume in an ideal 
world everybody uses the same 
standard mock-up and you mock up
 all of your data and Google's 
figured out all of that 
information and provide like a 
great experience to all of our 
users. 
    But we're still not there 
yet. Currently, the best user 
experience that Google can 
provide to all of our users is 
by vertical by vertical usage of
 structured data that you 
provide us. We're able to 
provide rich and facetted 
experiences for only particular 
verticals based on the user, 
structured data that you provide
 us. We use that structure data 
in both visual and navigational 
manners. 
    So I just explained using 
some examples. So in here, we 
have three different search 
features. On the left 
screenshot, we have a rich card 
for banana bread recipes. So 
there; because the webmaster has
 mocked up using structure data,
 we're able to provide a very 
good image of what the banana 
bread will look like once 
they've made it. And we're also 
to tell our users that it takes 
one hour, 20 minutes to make it.
And also, you consume 229 
calories if you eat one banana 
bread. 
    In the middle screen shot 
about a new feature called
 Google Jobs. In this feature, 
we're able to provide a very 
good navigational experience for
 our users. This is purely 
because for each job listing, 
website owners are mocking up 
the type of job, where it's 
located and the
 pin code. We're able to help 
users search for the exact jobs 
in their particular location. 
For example, in this someone 
searching for jobs in Boston, 
Massachusetts, and can further 
filter the search by saying he 
wants to look for jobs in sales 
and retail. And another example 
of search features which
 is the first feature we've 
launched using structured data. 
We're able to tell that this 
particular, the movie 
&quot;Godfather&quot; has a full 5 out of 
5
 rating. 
    We've seen how Google uses 
it. How do you send the mock-up 
to Google? Well, there are three
 common transport layers that 
Google supports.
 JSON-LD, and microdata. Even if
 you use 
microdata or RDFa, it's still 
fine. Why JSON-LD? Well, JSON-LD
 is the JavaScript notation that
 you can using a script.
 Or you can send it as a feed. 
    Unlike HRVGS  HTML and 
microdata, JSON-LD can let you 
express your data far more 
freely. And your content on the 
layout of your page, it seems 
very simple but it's far more 
complex than how it's on the 
layout of the page. For example,
 if you want to mock up your 
nested items, you can tell that 
-- you can give the movie name, 
often event in a particular 
location. That's very, very easy
 to do with JSON-LD, but it's 
hard to do with microdata and 
RDFa. 
    Apart from JSON-LD, we have 
this vocabulary that's standard 
across all different search 
engines which is schema.org 
category. Google and other major
 search engines support and you 
can use this -- you don't need 
to worry about optimizing for 
all different search engines. 
    So we recommend using 
schema.org. But just in case if 
you use data vocabulary or 
microformats, that is
 supported.
Right now, this is the most 
commonly used vocabulary. 
    You can visit the schema.org
 website to look at the 
different data types and the 
relationships between data 
items. 
Okay. GOOK L also
  has its own
 icons. Why we have these 
guidelines, even though you 
mocked up your content using 
schema, Google faces trouble 
getting to the content. Make 
sure all of your content is 
accessible by Google.
Everything that you mocked up, 
Google can access it. One common
 example of why this guideline 
was made was because we've seen
 cases where the script, the 
JavaScript and all of that other
 files are in different folders 
which Google cannot access. But 
can access the particular 
content of a particular page. 
But the script itself is in a 
different folder. And the robots
 block Google from accessing the
 content. We will not be able to
 see the mock-up. Let's say you 
made a great recipe site and 
mocked up everything. And you've
 got a lot of good ratings. But 
if Google cannot see that 
mock-up, your search snippet 
will not show those ratings. 
Similar to this guideline, there
 are a lot of other technical 
guidelines you can take a look 
at in Google's developer page. 
We also have quality guidelines.
Quality guidelines are generally
 there to prevent abuse of 
searches. For example, there are
 many cases where the user 
doesn't see the mock-up but 
Google sees it. 
    You can have a recipe 
website where you market a 5 out
 of 5, but user doesn't see it. 
When they look at the search 
result, because Google sees your
 mock-up, we'll show that this 
particular recipe has a five out
 of five rating. When the user 
goes to your page, he doesn't 
see that rating and he feels 
duped. 
    Which is why we have quality
 guidelines to make sure the 
users are seeing the same 
content as what Google's seeing.
We also have other guidelines 
specifically to make sure other 
data types like events are not 
abused. Because we've seen many 
cases where people use the event
 schema to sell their coupons or
 something at 20% off. So one of
 the major ways we actually made
 in roads into making structured
 data easier for everybody to 
consume and everybody to 
implement very easily is by 
having a lot of middle layers. A
 lot of CMS plug-ins and 
platforms, having them support 
structure data. That made it 
very useful or very easy for a 
lot of content providers to 
create or add mock-up. Many 
people who write the content 
have structure data. They have 
rich snippets. All the features 
are shown in search very 
visually appealing. Visually 
appealing manner. 
    But they have never touched 
a single line of code. It's 
purely possible because it's a 
lot of plug-ins for different, 
and many platforms like LinkedIn
 in the platform. We don't need 
to touch a single line over 
code. 
    So from Google's 
perspective, we also made a tool
 called Data Highlighter, a tool
 where you don't need to -- if 
you're unable to access your 
code, you can go to your page 
and you can use -- you can use a
 tagging tool to highlight 
certain  parts of your data and 
test out what it is. It's 
available in your search console
 account. You can simply -- one 
caveat is your pages need to be 
indexed before you use this data
 highlighted tool. And then, you
 can try and highlight certain 
parts of the page and you can 
see if Google is recognizing it.
    Okay. So you've mocked up 
your content, but now, we want 
to test it out. How do you debug
 if your structure data is 
valid, or not? Well, the best 
tool right now we have is the 
structure data testing tool. 
    In this tool; EEFRT the 
  URL web page, the tool will 
try to validate mock-up. Ones 
you put the codes, we're able to
 put up the item types of data
 we find.ce you put the codes, 
we're able to put up the item 
types of data we find. And show 
the warnings and errors. This is
 based on the own validation we 
do in search indexing. Whether 
we look for a page and look for 
structured data, we also use the
 same validation for -- that we 
show in this particular tool. So
 if your code is valid in here, 
that means, at least on a 
technical level, there's no 
problem like your structure data
 is good to go. As for the 
different warnings and errors, 
there's generally two types of 
errors that you notice. One type
 of error is when your code is 
not at all compatible with 
schema.org or syntax issue. The 
other type of error is when your
 code is not compatible with 
Google Search features. For 
example, you've seen the jobs 
feature I've showed you, right? 
    In the jobs feature, it's a 
requirement to add the address. 
So let's say you're listing a 
job for account in Bangalore, 
but if you don't provide the 
exact address where he's going 
to work at, we're not going to 
validate that code and that will
 not been shown in a search in 
the Google for jobs snippets. 
    Apart from -- apart from the
 structure data testing tool, we
 also have a structure data 
feature in Search Console. 
Aggregates all of your structure
 data information we found 
across certain period of time. 
    And we're able to show all 
of the different data types we 
found on your website and all of
 the different errors that we 
noticed on your website. 
    Okay. That's all I have of 
structured data. But before I 
move on to internationalization,
 I urge you to visit the 
developer's page and go through 
the technical guidelines, 
specifically, because even 
though 
schema.org has a number of data 
types, Google only supports a 
certain amount of those. We give
 a list of all of the different 
search features we have and what
 are data types we provide? 
    For example, if you use data
 type in schema and it's not 
supported by Google, at least, 
at this moment, it's not very 
useful in search for that 
particular feature. 
    All right. How many of you 
run websites which have multiple
 languages? And do you have the 
same content
 multiple languages?
Or you just have a website for 
English? Okay. I'm assuming you 
have the same content in 
different languages. 
    I think you already heard a 
billion times by now about the 
next billion users and how most 
of them, at least from India are
 looking for local language 
content. 
    So many of the big websites 
and even smaller websites are 
trying to create content 
ultimately in other local 
languages. We have a lot of 
people creating it from their 
old English pages, translating 
that into Hindi and making a 
website with the same exact 
content but in different 
languages. 
    Internationalization in this
 context is making sure you're 
letting Google know that all of 
the different versions of your 
website, English version, Hindu 
version, all are the same, they 
have the same content but 
different languages. 
    Whenever someone from Hindi 
state is searching for content, 
show them the Hindi page. If 
you're already doing it, I'm 
sure you know how hard it is to 
maintain websites with multiple 
languages. It's very, very hard 
because many people employ 
different methods to show their 
content to their users. 
    But because it's very hard, 
we've compiled a list of best 
practices so, at least, it gives
 us a head start for you guys to
 make this a multi-lingual 
website. 
    All right. Let's assume you 
have a website which has -- 
which you're writing about 
Bangalore, some place in 
Bangalore. And you have it in 
English. If you have the same 
version in Hindi and Canada, the
 best way you can let Google 
know the Hindi version and 
Canada version are the same 
content as the English version 
but in a different language. The
 best way you can let Google 
know that is by the flag in 
multiple different ways. You can
 add a link in the head section 
of your page saying, the 
language code. 
    For nonHTML documents like 
PDFs, send a header response. 
And you can also use your site 
map. If you have a website with
 thousands of pages, you can 
make it easier and let Google 
know you have these thousands of
 pages of English and the exact 
versions of this page in H ICHT 
Hindi are these versions. 
    Here's an example code. For 
the first one, you're basically 
telling Google that is the 
English-Indian version of the 
particular page. 
    This page probably has 
content in INR. And then, you 
have another page, 
US.example.com and you tag it as
 hreflang tag. You're targeting 
towards US customers. And 
finally, a default page for 
everyone, every other person 
from all other parts of the 
world. 
    So some common mistakes we 
notice when people implement 
hreflang, they add the code in 
only one page. So, for example, 
all of these three ultimate tags
 that are
 added in the 
in.example.com/page. If Google 
doesn't see the back tags, it 
will assume that it will not be 
able to consolidate all of the 
signals and do not be able to 
understand that all three pages 
of the same content and 
different languages. 
    Another mistake is people 
sometimes misuse incorrectly add
 wrong or language code. You're 
supposed to use ISO codes, keep 
that in mind. 
    This was a recommendation 
that we did previously. We use 
to ask people with
 tags. Right now, it's easier 
for us to make sure you don't 
add
 noncanonical tags. We stopped 
giving the recommendation of 
those tags. 
    All right. You've seen the 
previous example where we had a 
different URL for the 
English-Indian customers, a 
different URL for US customers, 
and a different URL for the rest
 of the world. 
    That's the right way to do 
Internationalization. Many cases
 we've seen where people simply 
use cookies to assume the 
language of whoever is visiting 
your website and they'll 
directly show you that version. 
    I'm sure all of you have 
probably visited a very big 
website and if you're in, like, 
Hindi speaking state, sometimes 
you get the Hindi version of 
that website. But the default 
version is English. 
    That can be a bad experience
 for users and even for Search 
Engines, if you dot no have 
separate URLs, it's hard for us 
to figure out that this is the 
English version, Canada version,
 HinHindi version. We assume the
 language and index it. The 
content of other languages is 
not indexed and that's bad for 
you and your users. 
    Okay. This sounds very 
obvious, but we've seen many 
cases where you have the 
Bangalore page in English, but 
what happens in the Hindi page 
is boilerplate template is 
translated but the content in 
the page is not translated. 
Google doesn't use code-level 
attributes like people add. 
People doesn't use that to get 
them what language of pages. 
Google looks at the content and 
the text of a page. 
    If you have, like, say 
template is in one language and 
the content is in another 
language, that makes it hard for
 Google to understand which 
language that page is, and we 
pick one on based on the signals
 and probably show it to the 
wrong audience. Which is very, 
very bad for you and your users.
Your users.

say the English page to the 
Hindi page or Canada page based 
on cookies and other 
information. How does someone 
who lands on the English page, 
how does he visit your Hindi 
page? You've got to make sure 
there's a small option for him 
to visit the other versions of 
your website.
 Even if he lands on the wrong 
page, he can simply click on the
 language button and visit the 
English version. And finally, 
don't worry about duplicate 
content. So many people who 
because there are thousands of 
pages and thousands of versions 
of your website, you end up  
worrying about duplicate content
 and whether you'll be penalized
 for this. Don't worry about any
 duplicate penalty. The only
 issue here, those pages won't 
be ranked. But the pages that 
are detected are ranked.  
Even if you make a mistake, it's
 not going to affect you in a 
bad manner. It's not going to 
show the pages supposed to show.
That's pretty much it for my 
session. 
    If you have any questions, 
please -- we'll be at the Hall 
3A. Thank you.

 you all here. The training 
sessions are much more filled in
 than the light sessions a the 
the end of the day. Today, aisle
 going to try and do something a
 little harder than usual and 
run some code during a 30-minute
 talk and build a couple of 
serverless building using Cloud 
Functions and G Suite API.
 What is serverless?
 Possibly the most exciting and 
technology that's come up and 
one that's freed me the most of 
being able to really focus on 
building applications, right? 
    I'm old, I've seen quite a 
bit. I did weekend gaming on 
early computers in the '70s, I 
had an Atari800 in 1979, 
installed networks, Windows for 
work group, built GUI 
applications and I don't have to
 do that a lot anymore thanks to
 serverless. 
    I was looking for an 
interesting quote and I didn't 
find any. So I decided that I'd 
simply -- I'd simply write this 
up. So it is confusing because 
serverless is a very wide 
catch-all phrase and everybody's
 got a different definition. 
    Eventually, I went to 
Wikipedia. And I got this 
definition. But, of course, 
Serverless still requires 
servers. A wrong or inaccurate 
name or designation. The name 
serverless computing is used 
because the server management, 
the capacity planning is hidden 
from the deployer, the 
developer. You never have to 
provision computers yourself. 
And it can be used in 
conjunction with other 
microservices or really on its 
own. 
    But Google has been leading 
the serverless movement. And for
 more than a decade. We started 
with App Engine in 2008 was one 
of the first platform as a 
service in the market and one of
 the early tools that I used, 
especially using. We then 
released BigQuery, which is our 
managed data warehouse which 
provides analytics at scale 
using the sharding technology 
from Google we're able to run 
through millions or tens of 
millions of rows within a few 
seconds. 
    We then come out with Cloud 
Storage which allows us to 
Stouffer enormous binary files 
and serve them. Hidden in App 
Engine for those of you who 
remember was this thing called 
Data Store. And eventually, we 
made it, we packaged it in an 
API and made it available for 
you without having to go through
 the path Cloud Data Store. With
 the advent of big data, we have
 to do a lot of data processing.

 both. And Firebase, which I'm 
going to be talking more about 
today was originally created as 
a realtime database. 
    And now, it is Google's 
go-to and even I would say the 
market's go-to mobile. Pub/Sub 
allows realtime messaging at 
scale if you have millions of 
users that are exchanging 
messages, this is how you're 
going to be able to do it. 
    And Cloud Functions, to my 
definition of serverless. 
Functions as a service. For 
those in the training just a 
while ago, Cloud Machine 
Learning Engine where Google was
 recently described as an IoT --
an IA-first company rather than 
a mobile-first company. 
    In this talk, I narrowed the
 focus to the function as a 
service. And basically, if we 
look at Cloud Functions, they 
are single-purpose, JavaScript 
that I executed in a secure and 
managed node environment for 
you. 
    They're only executed WHECHB
 a specific event is being 
watched is emitted, perhaps, on 
HTTP end point you can hit. 
Deploying your code to the 
servers will create one command 
and after that, your Cloud 
Functions will scale based on 
the usage patterns of your app. 
Never worry about SSH 
credentials or decommissioning. 
In many cases, the application 
logic is best controlled on the 
server to avoid any tampering, 
reverse engineering, Cloud 
Functions are insulated. I'm 
going to introduce Google app 
Script for those of you who have
 come to Wesley's sessions, you 
understand that it provides a 
high level of abstraction to 
look at the Google API. While 
you're writing mobile apps, you 
might want to do some processing
 within the G Suite 
capabilities.
    So I'm going to try to 
create a serverless API 90 
seconds or less combining the 
power of Google App Script. G 
Suite APIs, the classic tools on
 the front end. And we're able 
using your favorite platform, 
iOS, Android, Auto Web, Python, 
you should be able to do 
everything programmatically by 
accessing the APIs as you would 
on the front end. In addition to
 the APIs, an equal number of 
more of APIs for the domain 
administrator. If you're working
 a G Suite environment and want 
to provision users or user 
groups, all of the APIs in terms
 of understanding the usage 
patterns and the analytics. 
    The other, simpler, quicker 
way to interact with the APIs is
 Google App Script, magical 
tooks of
 sorts. Without having to deal 
with the awe P authenticity 
authenticity. You know dealing 
with authentication going, 
creating credentials, storing 
those credentials in a secret 
JSON and then, eventually, going
 through that refresh token in 
order to call the API is a bit 
of boilerplate code that takes 
quite a lot of time to get used 
to. 
    Google App Script also 
allows us with HTML service to 
create complex apps, tiny 
scripts to developed web apps. 
Also, allows us to interact with
 other Google Services, such as 
analytics, Add Words, BigQuery 
and many others. 
    I'm going to look at a 
little bit of code. This is a 
two-line instruction of code 
that's going to allow me to do a
 little bit of work. 
    Let's say, I have a business
 that's selling some products. 
And I want to be able to send 
these brochures to customers 
that register either on my 
website or my mobile app and ask
 for further information. With 
two lines of code, I'm going to 
be able to find that relevant 
file inside of the Google Drive 
folder. I have the drive app, 
get folder by ID. I'm able to 
find the file name or particular
 name. And then, my second 
instruction is simply sending 
the email to a particular email 
address, a subject. Unable to 
attach the file I just got after
 having generated it as a PDF. 
    So these could be deployed 
in a number of ways. One would 
be a library, another one could 
be a Web App so I could quickly 
have an end point and be able to
 send those emails to anybody. 
    And finally, we're going to 
do a final use case which is an 
API executable that can be 
invoked using the execution API.
We're going to invoke that later
 from our Firebase environment. 
Let's try to switch to the demo.
    So I'm going to try to do 
this in 90 seconds. I'm going to
 create a new script. Just go to
 my drive. I create a script. 
Already I'm losing
 time. I quickly give it a name.
 Send marketing brochure, too. I
 save it. I'm cheating a little
 bit. 
Unfortunately, we have a problem
 with the display. Okay, there 
it is. I'm going to get the 
code. 
Here it is. I'm placing the 
code. That's not the right code.
    I told you Murphy would get 
in the way. No. Of course. There
 we go. All right. I'm going to 
run this code and go to the 
authentication flow. I'm in App 
Script, I don't have to do any 
of this. I would have to go get 
a token, etc., I'm going to 
allow this authentication. 
    And this permission. All 
right. And now, I'm going to 
publish this as a Web App. And 
once I publish this as a Web 
App, I'm going to be able to 
access that particular script. 
Now, I have a script that I'm 
going to be able to access. What
 does this script do? 
    As you can see here, in 
these top two lines, I'm going 
to get the query parameters, get
 the email, 90 seconds is done. 
So I've failed once again at 
this. I'm going to get the email
 from the query parameters and 
the file name. Then, I will 
immediately be able to send out 
that. So I've got these query 
parameters saved here just to go
 a bit faster. 
    Supposedly faster. And put 
them in here.
 And hopefully, I get an okay 
and hopefully, if I 
can find my -- and here's my 
request for information. We can 
go back to the -- we're going to
 go back to the presentation 
slides. So here was a quick API.
    A serverless API that 
allowed me to within a few lines
 of code send an email with an 
attachment. And with very little
 worrying about setting up 
servers and using infrastructure
 that was available to me. 
    So the next -- that doesn't 
scale, necessarily, as much as 
we would -- Google Apps and 
Google App Scripts. There's 
another way to do it and that's 
using, possibly, the Firebase 
platform. 
    So Firebase is basically a 
Mobile back-end user service. I 
think it's awesome. For folks 
like me that need the laborious 
stuff taken out, especially if 
you want to build cross-platform
 apps for Android, iOS and the 
web. And for those who remember 
it before acquired by Google, it
 had awesome realtime 
capability.
But since then, it's gone an 
enormously long way. And 
provides the most integrated, 
comprehensive platform to help 
developers and business power 
their apps and platform. It 
really operates along four 
dimension. One, it allows you to
 develop your apps with 
authentication, database, 
storage, hosting and functions. 
The
 crash analytics, performance 
monitoring and test lab which 
allows you to test against an 
array of Android devices in the 
cloud. 
    Also, then, once you've 
deployed your app allows you to 
analyze what's happening in your
 app with integrated analytics 
and segmentation. We can 
understand which type of user is
 doing what. And finally, as you
 understand what your users are 
doing, grow this number of users
 and we just recently released 
AI-based predictions. So you can
 use the AI engine to understand
 which users are likely to churn
 so you can make an offer to 
them. Which users are likely to 
purchase and send custom 
messaging to them. 
Noteification, and revenue 
generation with ad mobile. 
    So as part of the Platform 
Cloud Function for Firebase. We 
talked about Cloud Functions a 
bit earlier, this massively 
Scalable environment where we 
can execute little pieces of 
code for specific use cases. And
 this opens the door to great 
use cases. 
    For instance, we could send 
confirmation email to users 
subscribng and unsubscribeg to a
 newsletter. When it comes to 
realtime database sanitation, 
text moderation, sentiment 
analysis, move things to 
BigQuery. We can also look at 
tasks such as image assessing, 
bulk image, bulk email to users 
and aggregating data. 
    We can also integrate with 
third party services and APIs 
from getting notifications when 
they get repositories, being 
committed to all the way to 
creating a chatbottom. 
    We're going to look a little
 bit at Firebase Functions. This
 is the simple lest Firebase 
Function. When you initially 
initialize Firebase in the 
working directory, you'll be 
able to upload code with a 
simple Firebase deploy code and 
get some Scalable functions in a
 matter of minutes. 
    Let's switch to the demo. I 
don't know why so many blank 
spaces. 
    All right. So this is what 
it looks like. This is the 
Console. And here, you can see 
that I've deployed one Firebase 
Function, a &quot;Hello, World&quot; 
function and behind an HTTPS 
request and if I get that 
particular request, it'll reply 
&quot;hello, from Firebase.&quot; Now 
we're going to try to do 
something a little bit more 
complex. 
    What we're going to do is 
code that I had written in 
abstract, I deployed it. And 
now, I'm going to try and access
 it. 
    And so, here is my HTTP
 Always funny to see people on 
the screen, right? 
There we go. I'm saving this. 
I'm going to use -- if I find 
it, one  of the functions of the
 developer tooling that allows 
me to serve my function locally.
Instead of sending it to the 
cloud and then testing it on the
 cloud, if I'm doing rapid 
modifications, I can just serve 
this function locally. This is 
going to create a local server 
for me and it's going to take 
all of that code and allow me to
 execute
 a local -- too many desktops.
 So I required some 
authentication which I have 
abstracted here. 
    And basically, what I do is 
look at the HTTP request, 
similarly to what we've seen
 So I required some 
authentication which in the 
Google Apps Function. And then, 
very simply reading my client 
secret requesting for an token, 
and then I call my App Script. 
And what I do very simply, 
again, I've got the script ID of
 the function I deployed as an 
executable API. 
And then, using the Google API,
 API, I'll run my function 
within that script and handler, 
and the token I just got, the 
function which is called 
marketing brochure in the script
 and the parameters. Here, in 
the meantime, hopefully, my 
terminal window
 -- is that one? That's not the 
one. Here it is. And so, now 
you've seen I have access to 
HTTP function. And if I find a 
browser, I'm going to be able to
 run it. 

Unbelievable. And there we go. 
And similarly, here I'm running 
this and now I should have a 
similar result as soon as I find
 -- and it's 4:57. So that's the
 one I've just received. All 
right. Going back to the 
presentation. So now, going to 
tie it all together with the 
final piece of the puzzle. 
    Two databases are made 
available for us on Firebase. 
One is the Cloud Firestore which
 was recently released and 
better and the realtime database
 which originally made the 
product famous. 
    What's important to know is 
that you can use both of these 
databases in the same Firebase 
app. They have different use 
cases and you might have a 
project that needs both of those
 use cases. Both are noSQL 
databases, store the same type 
of data and they work in a 
similar manner. 
    Both are Mobile First SDK 
and both support local storage 
for offline ready apps. The 
Firebase realtime Database 
stores and syncs data in the 
realtime whereas the Firebase 
Cloud Firestore syncs app and 
data at a global scale. 
    So, again, trying to 
continue on this vein, I'm now 
going to replace my HTTP 
function by a function that 
executes when I create a 
document. Continuing with my use
 case, I am going to store every
 request or every inquiries. And
 as I create a new inquiry 
record within the database, I 
want the email to be sent at 
that time. 
    So let's try,
 again. This is the Firestore 
environment. This is my Firebase
 environment. And this is my 
source code. So I'm going to 
change my function that was HTTP
 function that I had exported 
and I'm going to change it to a 
Firestore create function. And 
it's essentially, you will see 
the same function, just going to
 replace that. 
    I'm going to deploy it and
 then talk about. Here, I'm 
going to deploy the code while I
 talk.
I'm going to deploy the function
 while I talk about the code. 
    Here we go. And so, what we 
can see here is that I'm 
exporting a function that on 
creation of a document within 
the Firestore Database, within 
the inquiries collection, I
 will then fire this event. It 
will allow me to take the data 
from the document and, once 
again, just take those 
particular fields that are 
important to me.
The email and the file name. And
 then, really the rest of the 
code is the same and I'm going 
to authenticate the function and
 then eventually call the apps 
Script. And it's exactly the 
same call. Let's see if that's 
going ahead. 
    So one of the things that 
you can see here is it deleted 
the &quot;Hello, World&quot;. Every time 
you reupload the code, only it 
will delete those functions that
 are no longer there. 
    So if all goes well, I can 
now come to my database, my 
Cloud Firestore and add a 
document. I'll go to add a 
document. I'm going to not care 
about the ID, which is going to 
be automatically generated and 
I'm going to create those two 
fields.

time I did this, I forgot the 
dot com. And the document name I
 want to
 send. 
Firebase. What
 is it? Firebase 
indexing. PDF. I'm going to 
refresh this to be sure my 
function is deployed.
 And so, it's still not 
deployed. Or it's --
 you know what? It was deployed 
but it was the wrong one. 
    Because I hadn't pressed 
save. I'm doing okay on time. So
 I'm going to wait for this to 
happen. I'll tell you a little 
story in the meantime. 
    I've been in India for 21 
years. And I
 live in Ch
eni. And how do you say it in 
Canada? You say Namaste, is it? 
Namascaam. I have a few guys 
watching me. You put your hands 
up. Okay, one there, two there. 
You can ask them if I'm a good 
boss. 
    All right. So now, I should 
be able to -- first time I ran 
this, it took an hour, second 
time, 30 minutes. And now, I'm 
going to end up in 27 minutes. 
That's pretty cool. There you 
see. Now, my function is no 
longer HTTP event, but it's 
waiting for document to be 
created on the Firestore within 
the inquiries collection. 
    And so, if I save this, I'm 
just going to double-check it's 
really completed the process. 
Next time, I'll work with the AV
 guys to 
remove some screens, right? Too 
many desktops. All right. Come
 on. 
    We're going to blame it on 
the internet. I need a deploy
 complete. And delete this one. 
And let's try anyway. And then, 
created the new
 document. 
    And oh, my gosh. Anytime 
now, we should
 see this. Anyway, you believe 
me when I'm telling you it's 
working. 
    I come back and show you 
later. All right. So going back 
to the presentation. I want to 
leave you with a couple of 
caveats. And actually, three 
caveats, the most important 
that, of course, this example 
was completely convoluted and 
doesn't make a lot of sense. But
 what I wanted to show you was 
that we could create serverless 
functions very quickly with 
Google App Script without having
 to worry and that you were 
going to be able to do that with
 an access a wide variety of 
Google Suites, APIs and other 
APIs and make interesting use 
cases happen pretty fast. 
    And the second one was we 
looked at it in more Scalable 
environment. One of the caveats 
that in order for you to be able
 to call Google App Script 
project within, you need to be 
within the same Cloud project. 
And when you create a stand 
alone script, basically 
automatically creates a Google 
Cloud Platform and then you need
 to change the script that it 
is.
Change the project that it is 
attached to. You find out the 
project ID of your Firebase 
project and you change that. And
 that's the only way you can 
call execution API. And the 
second one was that it does not 
support service account. What 
I've had to do in the 
background, I've had to go 
through that manual flow once 
and store the -- and store the 
authentication, the token and 
the refresh token so
 that subsequent executions were
 able to read that file. And 
I've put all of that in the 
Source Code. And I can show you 
that quickly. But that's not 
really necessary.
 All right. 
    Sorry. I'm playing around. 
Let's go back to the 
presentation and finish it. 
Thank you very much for taking 
the time to listen to me through
 my ups and downs. And if you 
have any questions, you can see 
me after the session, thank you 
very much. 
   WESLEY CHUN: Hi, everybody. 
My name is Wesley, and I'm going
 to show you a few things to do 
with the Google Sheets API. I 
have the word &quot;new&quot; in the 
title. And it's still relatively
 new. This is a great time to 
give it a look if you're going 
to be developing applications 
and use Spreadsheets. My main 
job is to enable you to be 
successful using developer tools
 and APIs. Some of you may know 
I host a show on YouTube and a 
frequent contributor to the 
developers blog as well as on 
our twitter. 
    A little background about 
me, I'm a software engineer, 
coding for many, many years. The
 most well-known thing you 
probably know about is Yahoo 
Mail. I was one of the original 
ten engineers that built that 20
 years ago, and I've been doing 
Python ever since and I'm active
 in the Python community. My job
 here is to make you successful 
using Google Developer tools and
 APIs. There's a link right 
there. In fact, we're going to 
go over some examples and each 
of the examples has a video for 
each specific use case. 
    You can review those just in
 case everything just flies 
through because this is a 
30-minute session, then you can 
always watch the video to go 
back and take a deeper dive in 
the code. 
    Okay. So what we're going to
 do is give you a little bit of 
introduction to the sheet's API 
version 4. I'm going to talk 
about what was in the previous 
versions. So you can get an idea
 of, like, how it's changed 
since then. And then, give you 
some sort of use cases as O to 
why you would use the Sheet's 
API. 
And look at three different 
examples of using the API to 
show you the variety of things 
that you can do with the Sheet's
 API.
 All right. So Version 4. 
Before we kind of, like, at the 
newest version, let's take a 
step back before getting 
started. We know that 
spreadsheets have been a 
computing killer for decades. 
And it goes well beyond the 
business world. For nearly as 
long, they've been programmable.
You know, it's no surprise that 
was one of Google's first APIs 
ever and you can actually see, 
if you can see the date really 
carefully, you can see the very 
first version of the Google 
Sheets API came out exactly 11 
years ago. That's how old this 
API is I. Okay. And since then, 
we've made a lot of improvements
 to it. And I'm happy to share 
those things with you guys 
today. 
    All right. So Version 3, 
what can you do with this API? 
You can create new Sheets from 
existing spreadsheets, you can 
write new rows of data. And, of 
course, you can read data. All 
right, but 
unfortunately, that's all you 
could do. It's restrictive in 
that way. 
    Now with V4, you know, it 
seems that with just the 
features that we saw in V3, it 
seems kind of incomplete API, 
right? We know that spreadsheets
 can do a lot more than that. 
And so, I'm happy to share with 
you some of the things that you 
will find in V4. And you can 
actually take a look at the blog
 post for when we launched it 
from 2016 to get an idea of how 
the product manager describes 
the vision of the API and, you 
know, why we made this 
improvements. 
    Let's go over some of the 
new features you'll find in the 
new version. Okay. So think of 
what you can do if you were 
sitting at your desk in front of
 Google Sheets. All of the 
things you can do using your 
mouse and computer and screen. 
Right? An API should allow you 
to do those things, too. You 
should be able to write an 
application to the same things 
that you can do while you're 
sitting in front of the machine.
 And that's really the power of 
V4. Let's take a look. We know 
that V3 can do those things. 
Let's kind of look what else you
 can do now with version 4. 
Create brand new spreadsheets, 
set frozen rows, cell 
formatting, adjust column sizes,
 you can put in formulas. 
You can build pivot tables, 
create charts, right? All of 
those things you know you can do
 from using the user interface, 
right? Now, you can do that with
 your own applications. So 
that's the really exciting thing
 about V4. 
    And by the way, it step 
keeps things simple for 
developers.
 When you interact with a Sheets
 API, you're going to use one of
 the method collections. 
    And from each of these 
collections, the first one is 
just manipulating spreadsheets 
as a whole, in general, the 
second group is for individual 
sheets, mostly just copying 
them. And the third one is 
related to values. The ones in 
bold are typically the ones that
 they are going to use if you're
 going to be using the API. 
You're going to do a lot of 
changes to an existing 
spreadsheet, or for example, 
formatting or upload or change a
 lot of data in the spreadsheet.
That's why the ones in bold are 
typically the methods you're 
going to be using the most. 
    All right. So what are some 
reasons why you would consider 
using the Sheets API if you'd 
never done so before. One of the
 main reasons is you can have a 
lot of important data inside a 
database. That's not visually 
appealing. That's not something 
you may want to show your 
managers because it doesn't 
really tell them anything. It's 
a bunch of text that they're 
looking at. It's not as useful. 
What you can do is you can take 
this data out of the database 
using the API and now, when you 
put it in the spreadsheet, you 
can create charts and have other
 types of visualizations. And 
also, you know, when you have a 
spreadsheet, you have a user 
interface that people can go and
 manipulate the data with. 
Create pivot tables or somehow 
manipulate and format the data 
in the spreadsheet. It's more 
useful than just static data 
that sits in a spreadsheet or 
database. 
    That's one use case of the 
Sheets API. Another one is, 
maybe, you have information that
 is updated in the database very
 often but you also want that 
reflected in the spreadsheet 
that you have that sort of 
matches the data that's in the 
database, right? You can use the
 API to kind of; like, refresh 
and update the spreadsheet. 
    Similarly, if for example, 
you're taking data into the 
spreadsheet, like maybe from a 
Google forum or something like 
that, maybe you want to update 
the database, right? This is a 
two-way communication. 
    In this sense, you're kind 
of using the spreadsheet sort of
 like a Database, but it's more 
using it sort of as an end to 
create visualizations for you to
 be able to look at that data 
and basically take input, for 
example, like I said from Google
 forums. That's like another use
 case you would have, a category
 for why you would use a Sheets 
API. 
    So with that in mind, let's 
take a look at three different 
ways that we can use a Sheets 
API. I'm going to show you code.
It's in Python, you can consider
 that pseudo code. The first 
case is bulk loading values from
 a database. 
    We're going to look at a SQL
 database in particular and let 
me kind of describe it. For 
those of you who are new to the 
Sheets API, you want to learn it
 really well. We have a nice 
code lab where you build a no JS
 app where is a customized 
reporting tool based on if you 
were a toy manufacturer and your
 customers are placing orders 
for toys. 
That's the app you're going to 
build if you use the code lab. 
It's available to you at the 
short link there. 
    We're going to do, we're 
going to reuse that database. 
For example, pretend you created
 this. You've done the code lab,
 created the toy database, and 
now you want to move that 
information into a spreadsheet. 
Okay? Here's just a quick schema
 to show you I have this 
database and a SQL light 
database running on the machine.
 Information like the customer 
earning, the products, how many 
toys they ordered, what the 
price was and things like that. 
So the sample data is pretty 
small. I think there's only five
 or six rows, as you can see 
here. I'm just using the SQL 
lights, user interface to just 
bring up the data. 
    What I'm going to do is 
write some Python code to import
 all of this data and then put 
it into the spreadsheet. 
    So what does the Source Code
 look like? If you came to the 
code lab earlier, you're 
familiar with the authorization 
that happened. That code is, 
like, 1 through 15. You can't 
see them here. Assuming the 
authorization goes through, this
 is the actual code that does 
exactly what we said it was 
going to do, which is aligns 18 
to 121, that creates a brand new
 spreadsheet. From lines 123 to 
28, we read all of the data out 
of the SQL database. And from 
lines 30 to 36 is the rest of it
 where what we're doing is we're
 taking the data, we're 
creating, you know, once we've 
got the rows, we're going to 
just update this. So if you look
 at like lines 31 and 32, what 
we're doing here is saying start
 putting all of the data 
starting at the upper left-hand 
corner which is cell A1, you see
 that online 32 and the data is 
all of the rows that we read 
from the database. And in this 
particular case, I choose the 
value input option because I 
want to upload the data. Now, 
you have another option, which 
is called user entered. And what
 that means is it PRENDS you're 
sitting in front of the computer
 and typing
 it in.pretends you're sitting 
in front of the computer and 
typing it in. So you have a 
couple of options here. In this 
particular case, since I just 
want to upload the data straight
 into the database, I'm going to
 use raw. And so, that's why I 
use the 
spreadsheets.values.update call 
online 31 that was in the values
 collection. We looked at that 
EFRLier. 
    And then just to prove the 
data got written, 34 to 36 reaps
 those out of the database and 
puts it out
 to confirm. It's about 18 lines
 of code, not too much. But it 
does something useful and 
something you may have to do. 
But it's an example to kind of 
help get you started.
    All right. There is a video 
behind it. If you want to, 
actually, watch this thing as a 
video, I think this is a very 
first one for the API. So, yeah,
 the very first video that 
introduces, like, one over the 
things you can do with it. And 
since then, I think I've made a 
couple of other videos after 
that. 
    So now, let's move on to the
 next subject, which is in the 
past, you could not format 
spreadsheet cells. And to me, 
it's like frustrating. Why? I 
have a spreadsheet and I have 
data in it, I want to make it 
look pretty and don't want to do
 it manually by hand every time.

find out we weren't able to do 
this in the previous versions of
 the API. I was quite happy this
 was one of the things we could 
do. We can do now in V4. And I 
was happy to go on to Stack 
overflow and add new answers, 
like, can you format cells with 
the Google Sheets API? And the 
answer used to be no, but I'm 
happy to say we can do that now.
    Take a look at this sheet. 
This is the same toy order 
sheet. The same data that we 
uploaded. Okay. These are all 
tied together, right? 
    Doesn't look really boring? 
There's no formatting at all. 
Okay. So what can you do with 
it? You can add all the 
formatting. So now, hopefully it
 works. Now, things look a 
little bit better, all right. We
 did a couple of things, like, 
we froze the top row. We bolted 
the top row, we formatted the 
prices in column E so they're 
financial values. If you go back
 and look, it's plain like that.
 And also, like the 12.5, that 
doesn't mean anything, right? 
Once we format it, you can see 
that it's now 12.50, looks like 
a financial amount. 
    And then, in the column F, 
we made fixed values and cell 
validations so it -- the status 
has to be one of these three 
options. It's either pending, 
shipped or delivered. 
    Those are the things we were
 able to do using the API. Let's
 start taking a look at how you 
would code this. Okay? So 
obviously, I should have been 
pressing this. So all of these 
things you could do. All right. 
So now let's look at the code. 
How do you send commands to the 
API? The way you do that is, 
first of all, you need to have 
the ID of a spreadsheet. That's 
pretty easy to do. 
    Either you have an existing 
one or go to using the Google 
Drive API, do a query and search
 for the exact file you're 
looking for. What happens is, 
you're going to have a 
JavaScript array of requests and
 they could be individual 
commands, different commands, 
doesn't matter. You put all of 
your commands inside this 
request array. 
    And this example I just 
have, like, updating of the 
properties over a spreadsheet, 
repeat this operation over 
multiple cells, set cell 
validation, sort this range, add
 a chart. Lots of different 
things. Many different commands.
And once you have all of the 
requests you P want to send to 
the API, you'll call 
spreadsheets.batch update. 
That's the spreadsheets method 
collection. It's what you're 
going to use to make these  
operations happen. Remember I 
had a couple of methods bolded, 
batch update is one of them. 
    What happens is, you're 
going to have a whole bunch of 
commands and send them all to 
the API at the same time. 
    One of the reasons they 
chose this design is because, 
you know, Google APIs are free, 
generally, right? And so, what 
happens is that in order to 
control resource usage, we have 
certain quota as to, like, how 
many things you can do. And if 
you keep on calling the API all 
the time just for, like, one 
request API call, another 
request, another API call, 
you're going to slowly but 
surely run out of calls that you
 can make to the after that 
accident PI. 
    So the advantage of putting 
these inside the request arare, 
you only make one call to the 
API. That way, it helps you 
minimize the number of calls you
 make overall and basically, 
gives you the sense that you 
have more quota than you really 
do. 
    This is generally the 
structure. And if you end up 
using the Google Slides API, it 
works the same way, also a batch
 update. These are two of the 
newest we have. And this is sort
 of, like, the model we're going
 after now just to help you guys
 extend your usage. 
    So how do you freeze a row? 
That's the first thing we're 
doing. That is considered a 
sheet property. The verb you 
need to use is updates sheet 
property. What are you trying to
 update? You're trying to update
 the frozen row count which is 
part of grid properties, which 
is part of the overall 
properties. 
    And you want to set that to 
one. That's the same thing as if
 you went into the spreadsheet 
UI and dragged down the little 
thing on the left-hand side so 
the first row stays the same. 
When you scroll up, the first 
row still stays on the screen. 
That's the frozen row. 
    And then, fields just 
confirms that the frozen row 
count is the field that you want
 to update. 
    Now, you may be thinking, 
this is repetitious. You know, 
why am I saying the frozen row 
count. I'm going to change the 
frozen row count, but then I 
have to say it, again, in 
fields. 
    I made a separate video to 
describe why this is the case. 
And the reason is because with 
fields, you can reset other 
fields, not just the frozen row 
count. Generally, there's a 
one-to-one matching in the 
change. But there's certain 
things you may want to do to 
reset something. 
    So, for example, if you want
 to change it so it's not bold 
or something like that, you 
would have a not bold property 
set, but in your field, you 
would leave it blank. All right.
 So anyway. The main idea is 
fields controls what thing 
actually gets changed. 
    So you can actually reset 
other values, too. All right? 
Like I said, I made a video on 
this because it's confusing. So 
I have two separate videos on 
just field masks. That's what 
these are. 
    Just because my goal, again,
 is to make you successful. And 
if there's something confusing, 
I've got to make sure you guys 
are not confused. That's part of
 my job duty. 
    All right. Now, let's talk 
about cell ranges first. So a 
cell range is made up of a Sheet
 ID and indices that describes 
the area of the spreadsheet that
 you want to change. Okay? When 
you create a brand new spread 
sheet, you get one sheet, right?
The empty one. Sheet ID, 
defaults to zero. All right. And
 so, like, most programming 
languages. When you're looking 
at rows and columns, the 
counting starts at zero. Even 
though in the user interface, 
row one through six, the index 
numbers are actually zero 
through five and the same thing 
goes for the column numbers. 
Keep that in mind. 
    Also, ranges are exclusive 
of all end indexes. Okay. That 
means they go up to but do not 
include the end index value. And
 then we'll look at that in the 
next couple of Slides. 
    And then, lastly, if an 
index is zero, you can just 
leave it out. Again, I'll show 
you that, too. 
    All right. So again Sheet ID
 and indices. The first sheet 
that's created as a default ID 
zero, everything's exclusive. 
And then, you can skip it 
depending on the scenario. 
Mostly, if it's a zero, you can 
skip it. 
    What I want you guys to do 
is look at the four cells that I
 have highlighted. And you're 
going to help me fill in the 
grid range. Remember what the 
sheet ID is? By default? It's 
zero. Good. All right. Now, the 
start column index. What is the 
starting column index
 here? 
Okay. Zero, one, two, three; 
right? Three and four but not 
five. So we can't put four 
because it's exclusive of the 
end index, that should be five. 
And then, with row, it's zero, 
one, two, up to four. Right? Two
 and three, but not four. Okay. 
So there we go. And because, 
like I said, if it's zero, you 
can leave it out. That's why 
it's kind of grayed out. You 
don't have to put the sheet ID 
there. That's only the first 
default sheet that's created for
 you. If you add more sheets or 
somehow delete the first sheet, 
all the other sheet IDs are not 
zero. You're going to have to 
put those in there. Okay. Be 
aware of that. 
    Once you guys know that it's
 three, five, two, four, all 
right, then you know how to 
manipulate an entire range. 
That's good. 
    Now, let's use that 
knowledge and bold the first 
row. It's not considered a sheet
 property. So we can't do the 
same thing we did with frozen 
rows, we have to specify to 
bold, individual columns and 
individual cells. So the command
 in this case is repeat cell. 
    I have the sheet ID of zero 
there, but leave that out. 
Starting row is zero, but leave 
that out. The end row index is 
one because I'm only going to 
bold the first row, okay? And 
then, for the column index, I'm 
going to start at zero because I
 want to start at the very left 
but zero's a default so I can 
leave that index out. 
    However, the end column I 
have to have because I want an 
anticolumn five, which means I 
need to make the end column six 
because it's exclusive of the 
final value, right? 
    That's the first part. Then,
 the next part is, well, what do
 we do with this cell? Remember,
 the command is repeat cell. We 
have the range of cells, what is
 the command we're going to do? 
What do we want to do to each 
individual cell? We're going to 
go into the user entered format,
 that text format.boldfield and 
turn it on to true. And, again, 
we also have the equivalent 
fields mask to say, that's the 
only thing I want to change. 
Leave everything else alone. 
Don't reset anything. 
    So that's it. That's all you
 need to, actually, make the row
 bold. Okay. And so if you 
wanted to see sort of in action 
in Python, looks like this. 
We've got the update sheets 
property, it's the same thing we
 saw earlier. We've got the &quot;we 
repeat cell&quot; throw it into the 
&quot;request array&quot; and it's done. 
You can see it if you run this 
from the command line, you can 
look at your spreadsheet and 
watch it change in realtime. So 
it's pretty exciting. 
    So that's a couple of things
 you can do. What about the 
currency FOMPLT formatting part 
if I'm using US dollars, doesn't
 really matter. In this case, 
the grid range is going to be 
different because I'm looking at
 column D all right. 
    So all of the numbers should
 be exactly what they are in 
your mind.
If you remember the spreadsheet.
This time, instead of changing 
the text format bold flang, the 
number format and set it to a 
specific pattern and say it's a 
currency type. And then, again, 
with fields. I confirm that the 
thing -- the only thing I want 
changed is the number format and
 it'll make those changes for 
you. Okay? Everybody good so 
far? All right. Great. And then,
 the last part is cell 
validation. I only want to have 
these possible fields of 
pending, shipped and delivered 
for the status. So, again, I 
have to give another grid range,
 which is for column F this 
time.
All right. And then, the rule is
 that, okay, notice the command 
this time is called set data 
validation. This is yet another 
command. 
    And it does exactly what we 
want, which is I only want to 
let the user choose one of these
 three values, all right. I'm 
going to set my values to 
strict, which means they cannot 
choose anything else other than 
these three. They cannot type 
anything else otherwise they get
 an error from the UI. And then,
 the custom UI means I have a 
little menu thingy so they can 
pull down and select the value 
they want. 
    So that's cell validation. 
And so, for example, if somebody
 tries to enter an invalid 
value, they're going to get an 
error from the user interface. 
It's pretty basic. And, again, I
 have a video on this one if you
 want to learn specifically 
focus on formatting cells, which
 is something I found very 
exciting to me. 
    Okay. Yep. Also, the other 
thing is that it'll also tell 
you, it'll give you a flag. Even
 if you have the strict turned 
off. It'll tell you this thing 
violates the rule, but I'm going
 to allow. It's going to allow 
it or not allow it. And you can 
choose which one you want 
depending on how strict you want
 it to be. 
    All right. So that was 
column E, column F and the rest 
of the formatting. So after you 
watch the video and you take a 
study of the code sample, I 
challenge you to upgrade it, 
okay. There's another column for
 total cost, all right. So what 
I want you guys to do is take 
this and modify that one. And 
make change it to the formula, 
right.
    You can copy the financial 
value formatting column E, but I
 want you to take what is column
 D and column E and calculate 
the value in column G. That's 
the challenge I leave for you at
 the end of the video. Just in 
your head, you can kind of think
 how you do it, research how do 
I enter a FOMPL formula instead 
of data. And you can look it up 
in the documentation and it'll 
be easy to do. That's my 
challenge to you. 
    Okay. That's -- that's one 
thing that you can do with 
spread sheet. Now, what's 
another thing you can do? This 
is the last thing we're going to
 look at. Which is -- generating
 presentations from spreadsheet 
data. Why would you do this? 
    You would do this because 
similarly to why you would use a
 Sheets API. It doesn't really 
good. Maybe if it's in a 
spreadsheet and has a chart and 
makes it easier for your 
managers to see, you know, the 
data and the value. 
    You can have a lot of data 
but not a lot of information. 
Using the spreadsheet, you can 
turn that data into information.
Furthermore, maybe you want to 
do something more than this. 
Maybe you want to create a slide
 deck to present to your 
management, right? So instead of
 just showing a spreadsheet on 
the screen, maybe you want to 
create a slide deck. And in this
 particular use case, I want to 
-- I have a spreadsheet that has
 toy orred order information, 
created a pie chart. I want to 
build Google Slides out of it. 
This is demoing the power of 
using two Google APIs at the 
same time. Here's my database, 
again, and ran the previous 
example where I formatted 
everything, I've got the cell 
validation, I've got the 
financial currency formatting, 
I've got the frozen row, the 
bold row, and then, I went and 
created a chart. You can do this
 from the API, but also the UI 
or use the explore button if you
 have seen that. 
    Okay. So we have this 
information. We want to create 
Slides out of it. Okay? Here's 
one of the Slides I want. I want
 to see a table in a slide that 
has the same information as my 
toy orders that I have there. 
    And secondly, I want to take
 that chart and make it its own 
slide. How do we do that? Okay. 
I'm not going to show you the 
whole code because it's a little
 bit larger now. But I'm going 
to focus on, sort of like the 
major sections. The major 
section is lines 20 to 28. We'll
 go and fetch those values out 
of that -- out of the 
spreadsheet. 
And then, the second -- that's 
20 to 23. And then 25 to 28, I 
want to go and find that chart 
and get that chart ID. I need 
that information to build a 
slide presentation out of it and
 the best part is when I make a 
slide out of it, if later on I 
change the data in the database 
and that chart changes, there's 
a little update button in your 
slide presentation. And when you
 press it, it'll refresh to the 
new data and the new chart. It's
 pretty exciting. 
    So you can do that in user 
interface or actually use it 
through the Slides API. You can 
request an update of the data. 
Either way is fine. 
    So now, the hard part is on 
the right-hand side. We're going
 to create this table, we're 
going to create Slides. All the 
stuff that's HIMGT highlighted 
out. Filling in the title
 slide.highlight highlighted 
out. Filling in the title slide.
 The most important part is 
filling in the table. I have, 
like, a double four-loop down 
here that will loop through 
every column of every row that I
 ran out of the spreadsheet from
 20 to 23 and writes it into the
 table that's on my slide. 
    And then, similarly, online 
76 to 91, 92, I'm going to 
create a brand new chart, give 
it the chart ID and I have to 
give all of this information 
about what size I want on the 
slide and everything. But the 
main idea, as long as it has the
 chart ID, it'll make that slide
 for me. There's a video for 
that, too. Anyway, in the very 
end, you will actually have 
these two Slides. That is what 
that code creates.
 That's it as far as the demos I
 want to show you guys. Let's 
wrap up
 now. 
Spreadsheets are a 
business-critical tool. And 
Earvin in today's world where we
 have big data, you can use 
spreadsheets,
 right? It's very flexible tool,
 it's kind of like having an 
application that does 
knew numeric and text 
manipulation on your behalf. And
 you can leverage it. Anyway, we
 have a brand new API. Makes it 
more powerful than previous 
versions which were limited in 
terms of what you can do with 
it. It's very flexible and you 
can power your apps with it. 
This animation shows that 
spreadsheet is an organizer of 
data. Sometimes used as a
 database. You're creating 
charts and visualizations. It's 
basically like a Swiss army 
knife in my opinion. And I think
 given the ability for you to 
program it makes it even more 
powerful than before. Okay? 
    So that's it. I hope to see 
you maybe later this week, maybe
 tomorrow if you're a Python 
user, I'm going to give a talk 
at the Python Bangalore user's 
group and I'll be in Singapore 
talking about Google App Script.
Thank you guys for coming and 
hope you enjoyed the talk. 
[ Applause ]
If you have extra questions, see
 me afterwards. Thanks,
 everybody. 
       AMRIT SANJEEV: Good 
afternoon, everyone. I'm Amrit, 
advocate here at Google. And 
today, we're going to talk about
 searching JavaScript sites. 
This is a question we get. How 
does Google search web index 
application? An easy way of 
understanding this is to 
understand how Google search 
sees the website and it'll be 
easy for you to test and 
validate it. 
    Like I said, I'm a developer
 advocate. I'm no Angular 
developer, and this is going to 
be a session about Google Search
 and not about any particular 
web framework. That out of the 
way, let me give you a quick 
overview of what I'm going to 
talk about today. 
    I'm going to talk about what
 search really looks for. Then, 
a little bit about rendering 
techniques. And how do you want 
to do when you're migrating, 
what are the things you should 
take care of? To see it 
correctly. And we'll discuss a 
little bit about some of the 
useful tools that are available.
    And this is some terminology
 I'm going to use throughout -- 
first one being Google Bot, 
that's the software that's 
calling the web. Getting the 
data, rendering it and indexing 
it. Secondly, in a single-page 
app. A single-page app, I mean 
where you send in the data is 
updated to client's JavaScript 
and client side JavaScript and 
dynamically updates your app. 
    With an app share, this is 
the JavaScript, HTML 
combination. But aggressively 
caches and rendering. 
    This is not the DOM I'm 
going to talk about. I'm going 
to talk about the rendering. If 
you want to simplify how search 
works, a collection of URLs in 
the database. We use a schedule 
and a caller based on the 
schedule go to the URLs, fetch 
the data, render it, index it 
and do a search. That's what an 
actual search works. 
    What does search really 
want? It is unique, indexable 
URLs. URLs that are callable, 
have indexable content and links
 so that you have the 
relationship between documents. 
    Now, if you look at URLs, 
for instance, this must be 
information you already know. A 
good URL has protocol, host, 
file name and query string if 
required. It's one URL per 
piece. It's not going to 
duplicate for different content.
And good URLs. 
    But the easier way of 
understanding this is to kind of
 maybe look at what a bad URL 
looks like. So if you have a 
same URL for multiple states, 
that's not really good. 
    Again, URLs are the fragment
 identifier. Again, not a good 
thing. And I'll talk about it a 
little more in the upcoming 
Slides. Then, also, U are URL 
might have session IDs, like 
user ID and password. Not 
considered to be good.
Canonical URL. This, again, when
 you have multiple versions of 
the same page, you want to use 
canonical URL and clearly say 
the one URL that you need as the
 index would actually keep. You 
want to ensure that your 
JavaScripts and link maps are 
updated correctly to reflect the
 canonical URL. 
    You don't want a different 
one to be there in your index or
 site map. That doesn't -- that 
is mandatory you have them 
common across. 
    We talked about fragment 
identifiers. The interesting 
thing to understand here is 
everything after the hash in the
 URL is actually ignored. So you
 want to kind of use the history
 API and update your browser URL
 to something that is actually 
understandable and flexible. And
 this is something that is, 
like, this is something that is 
mandatory. 
    You also, technically, URLs 
are rendered but not 
recommended. In that case, also,
 if you're using them, please --
use a history API and update 
that. 
    This is a sample of code 
that sample that I want to show 
where history used to update 
page data and the URL. It's very
 simple to do that in your code.
If you're already using fragment
 identifier, Google search spot,
 you might want to update it to 
URLs using the history page API.
    Now, one of the things in 
Google search and this is data 
from 2015, but still relevant is
 that in more than ten 
countries, including US and 
Japan, we've seen that the 
mobile searches are actually 
higher than the general desktop 
search. And using that info, 
we've gone to mobile. We have 
indexes using a mobile user 
string. We're useg Chrome agent 
for that. Now, let's talk a 
little bit about how rendering 
has gone. 
    And if you look at Google 
browser. It renders more spaces 
comfortably, but it's based on 
Chrome 41. There are some 
limitations. Let me talk about 
some of those limitations to 
you. 
    It's actually stateless, 
which global storage, index DV, 
Cache API, these are all flushed
 out. 
    Same happens. This is one 
limitation you need to
 know. The second one is if 
there are URL differences to 
information that is required, 
like JavaScript or CSS page, by 
blocking them in robots, we're 
rendering the page we're not 
going to reference the robot to 
block any access to any of this.
 Rendering actually to the end 
of the page. 
    Third one, if you're using 
JICHT JavaScript to do 
navigation without having links,
 we're using href, then this -- 
the bot will not be able to find
 the relationship between 
documents. Using site map, we 
might be able to map the entire 
document set. But the 
relationship between these 
without the anchor tags is not 
possible. So that is really 
important here. Couple more 
limitations. Service workers are
 not supported, index DB, web 
GL, these are some of the things
 not supported with Google bot 
right now. 
    The recommendation here is 
to fall back and use polymers 
and when testing your site, use 
Google Bot as a separate browser
 and test against Chrome 41 to 
see if everything's working 
right for you. 
    Now, another thing to keep 
in mind is that Google 1 caches 
aggressively. Because of that, 
it would cache your JavaScript 
pages. In case you're changing 
some of these things, it's best 
to kind of let the system know 
that. And one of the ways to do 
that is to kind of have working 
URLs so we are able to 
understand there's a change. One
 technique is to actually the 
hash of the file to the name of 
the file so that when a file 
changes, your hash is, again, 
changed so we know it's a 
different one and we --
 it is --
in case there's a failure for 
rendering client side 
JavaScript, what happens is we 
fall back from the data that was
 sent from the server. This 
means, if you're using 
single-page architecture or 
single-page apps using the 
architecture, you would want to 
test thoroughly. If your client 
side is going to fail, the 
client side pool would fail, it 
would mean your underside might 
be indexed but all of the 
content would look the same and 
dropped from the index being 
duplicates. That's another area 
you want to be careful with. 
    This is an interesting 
thing. This is something that 
browsers are very resilient 
about. Finding this error, even 
if it's a simple one is a little
 hard. 
    So if you have hidden body 
tags, right? If there's any tag 
that is meant to be inside the 
body tag and it's put in the 
head, Google Bot rendering 
actually fails. 
    What would happen is it 
would complete the head tag 
thereby content might not be 
added to the index. 
    Now, one thing which makes 
it quite difficult to find the 
simple error is your browser 
will not show you any errors for
 this. 
    So look out for this 
problem. And we've seen a lot of
 developers having, making this 
mistake and then not having -- 
as well on the search index. 
    Now, for hidden content, one
 of the things to keep in mind 
is the Bot does not interactions
 on the page. If you have 
JavaScript, which loads data on 
hover or click through, that 
would not, actually, be indexed.
If you have really important 
contact that needs to be 
indexed, one strategy to use 
would be that you render these, 
but toggle the invisibility. 
    In case click to kind of 
show visibility, that's still 
fine. But not click to go fetch.
Because the Bot is not going to 
do any infractions on the 
surface. It is better to 
actually have the UI limits 
surrendered but not disability 
that's what the page is 
designed. Performance, there are
 limitations here, also. But in 
terms of time, that's 
computational time, not -- so 
things like set time or set 
interval the actual rendering is
 not going to be affected. It's 
not going to affect it but 
computationally it puts a cap on
 this. 
    You might want to look for a
 slow-performing third party 
APIs. The number of of requests 
that you're actually doing to 
the server to fetch your 
resources. You might want to 
minimize that. And the third 
thing it could do is to use 
faster servers. So these are 
things you want to keep in mind 
when you're looking at rendering
 performance. 
    If you don't want the bot to
 time out and miss indexing 
things. Looking at the rendering
 strategies. Let's go through 
some of the options that we 
have. Again, I want to call out 
that rendering is the 
construction of the SGML and no 
the visualization of the DOM. 
Keep that separate here. The 
data plus template, not down to 
the pixels. I want to emphasize 
that, again. 
    If you look at rendering, 
it's kind of hard to debug 
Google Bot rendering. Some sites
 they render just for the bot. 
That's a bad idea. That's 
cloaking. We discourage that. 
It's tricky to maintain it and 
also debug it if -- as your site
 is evolving. Let's not do that.

 we can render. You have the 
client service side rendering, 
which is basically traditionally
 your web is working where all 
they generate is the server and 
sent to the client. 
    Or client side where you 
send in a half shell and use 
JavaScript, render it there. 
    And the third way, a hybrid 
model. In hybrid model, you send
 the SGML first, render full 
SGML down from the service side.
 And from the client side for 
any additional data, you might 
call using JavaScript pull that 
data back. Data and then 
visualize it there. 
    If you look at all of the 
three strategies, there are 
frameworks that make hybrid -- 
hybrid -- there are frameworks 
that help you do that. One of 
them is Angular yuan universal. 
The other is using 
JavaScripting. These two 
techniques are what mostly 
people use for hybrid 
development. 
    And what -- which one is 
better? Well, service side is 
the safest. You know which is 
going to the client and what 
will be rendered. Hybrid, that's
 ideal. And a lot of you guys 
are doing client side rendering.
 For that one, please test it 
thoroughly. It is important that
 you test it. Really really 
well.
For the same reason that you 
don't want errors to come in and
 not just load out of the bot 
seeing the app shell in that 
case. 
    Now, another place is when 
you're migrating things. How 
you're migrating into the new 
structure. What are the things 
you want to keep in mind? Three 
steps to think of first. Double 
check all the nonGoogle 
dependencies. The URLs that are 
not clean, basically using a 
fragment identifier, you want to
 clean them up. And the third 
one is if you're changing the 
URL, redirect it on the server. 
If it's a client type, if it is 
something that a client is 
actually doing, you must be like
 using a fragment, it must be 
performed on the client side. 
    Now, when a URL gets 
changed, ensure that you have 
redirect done correctly. If it's
 client-side using fragment 
identifiers and there's no delay
 like you would have a timer 
you're going to move further in 
five seconds. The sort of 
delayed redirection don't do 
that. That's a bad idea, again. 
    Update your old site map and
 the new site map. And ensure 
those links, both of the links 
are not in kind of avoiding
 access, be available for the 
bot. And ensure all the 
canonical URLs are updated 
correctly. This is mandatory. 
    Some of the useful tools 
when it comes to monitoring and 
debugging. Google Search 
Console. How many have used this
 before? Some of you guys have. 
The others should check it out. 
This will allow you once you 
verify your site here, this 
allows you to diagnose what has 
been called rendered and index. 
Helps you to kind of look 
through aggregated data, use 
targeted some settings like
 targeted naturalization. It 
also helps if you want to remove
 some content. If you want all 
of these features, it's 
important you verify. The 
removal of content can be done 
really fast.
And acted upon really fast using
 this Console. So verify your 
site using Google Search Console
 first. Don't wait for the 
moment where you want to remove 
it. Do that up front. 
    Google, another tool that's 
there. How many use this? The 
others should check this out. So
 basically, what this allows you
 to do is allows you to run up a
 URL, and how Google Bot will do
 it. It will let you see the 
server side HTML and also a 
screenshot of the place finally 
created. 
    You will not be able to look
 to the DRVGS O DOM, that's one 
part to be aware of. Debugging 
is a little tricky. And you can 
visualize it in a desk top or 
SMAFRT smartphone. When you look
 at debugging, it is possible. 
So one way of doing it is you 
intercept all of your 
client-side errors and log it to
 a server. So when Google 
fetches Google is done, if 
there's a rendering error that 
happens, your JavaScript is 
going to intercept that error 
and put it to your log-in 
server. Then, again, you should 
be useg development server and 
don't use production servers for
 this because it's going to be 
expensive. 
    What I was talking about 
intercepting clients, you want 
to take that, push it back into 
the clouding servers and then 
test it with a Google -- sorry, 
Google Tool. 
    This is a sample code which 
basically in case of an error 
would tell you the line number, 
the error number, if it's 
available, and uploads it to the
 server. Something as simple as 
this, you can add more things as
 you feel relevant to your 
solution. 
    Now, for site maps, it's 
best that you invest time in 
actually separating separate 
files for different routes and 
templates because it makes it 
easier when it's updater. Feel 
and update this to ensure that 
we understand that the site map 
has changed. And, again, it 
needs to -- and you also have to
 ensure every time you change, 
URLs matched the site map 
correctly. You shouldn't have 
differences there. All indexes 
upright. 
    Now, if you're looking at 
single-place architecture, it's 
a little tricky to kind of do 
the client side part. You can 
use tools like puppeteer, which 
is a headless renderer to render
 it -- provide it, Chrome 41 
will run like Google Bot. Let me
 show you code for that. By the 
way, puppeteer was a third 
party, JavaScript library, which
 you'll actually fetch, render 
and index and show you stuff. 
    So this is basically how we 
want to kind of use puppet; the 
standard JavaScript. Quickly, 
recapping all of this. The key 
take aways to look at from all 
of this is you
 want to URLs first. Second, 
understand how rendering is done
 and what Google Bot sees. Look 
at the techniques we talked 
about here, look at all of the 
limitations and ensure your site
 is working with respect to all 
of that and taking care of that.
 If single-PACHBLG ark
architecture, look at the 
client-side rendering into the 
log-in servers to understand if 
there are errors happening at 
the Google Bot site. 
    Have a template for every 
template involved, have a 
different site map file. And 
when you're in your test cases, 
please consider Google Bot as 
Chrome 41, test cases for that 
as a separate browser 
altogether. Using these 
techniques, you'll be able to 
kind of make your pages really 
better. And the same thing 
applies, it's another web page. 
So the same techniques and the 
concepts that we talked about do
 apply to that also. 
    And lastly, there is office 
hours that happens at 
Google.com/webmasters every 
fortnight. If you have questions
 for the team, you can, 
actually, join that and ask and 
the team will answer that for 
you. So 
this is another -- you should 
use more if you're not already 
doing that. With that, I'll 
close. Thank you, everyone. I 
have Jake who has a quick quiz 
for you guys. Jake, why don't 
you come on
 board. 
[ Applause ]
    Hey, everyone. Okay. For the
 last session of the day of the 
whole conference, I guess, get 
your smartphones or tablets or 
laptops, whatever has the most 
battery to the ready because we 
are going
 to
 play. The big Web Quiz. I don't
 know if anyone was here 
yesterday at this time. We 
played it yesterday. It's going 
to be the same sort of thing but
 with different questions. Get 
yourselves to Big Web Quiz.com 
and sign in. This will be the 
weirdest edge cases, weird stuff
 of the web platform. If you're 
not someone who codes on the 
web, I recommend playing along 
anyway. Even if you're guessing 
randomly, you might beat your 
colleagues who think they're web
 experts. It'll be fun to see if

that happens. People getting 
logged in. All right. Devices at
 the ready, I'm going to bring 
on the first
 question. Here we go, which of 
the following HTML tags are 
mentioned in the HTML housing 
spec? The question should've 
appeared on the phone.
ALT, listing, fetile and 
sarcasm. Select the ones you 
think are mentioned in the HTML 
spec and hit submit, as well. 
    What we're looking at here 
is a load of zero numbers which 
makes me wonder if something's 
not going on. The question 
hasn't appeared, has it? Okay. 
No worries, I can fix that. 
Let's sort this out. It's 
possible I have a debug mode on.
Does that say false? Debug 
false. Okay. We'll do some live 
coding. That's fine. Let's see 
if we can give it another
 go. 
Anything on the phones? Live 
coding and a conference talk. 
Excellent. Let's get this into 
full screen. The four options 
are ALT, listing, fetile and 
sarcasm. Which ones go.  do you 
think are in the HTML spec? 
Hopefully no more bugs along the
 way. Yes, make sure you hit 
submit when you've picked the 
things you think are in the 
spec. It's great to say some 
numbers which aren't zero. What 
this is telling us 78% of you 
are confident of one of the 
things being in the spec, the 
rest of them you're not 
confident on, one you're not 
confident on at all. I'm going 
to close the question in a few 
seconds. Make sure you hit the 
submit button. 
Closing the question in three, 
two, one -- we're
 out. 
    We're saying confident about
 alt, not confident about 
sarcasm at all. Interesting. 
Okay. Now, what's going to 
happen in this quiz. If you get 
a low score, doesn't mean you're
 bad at web development. All of 
this stuff I would get wrong, 
don't feel bad 
about it because those three 
tags, alt is not. Alt is an 
attribute. We call it alt tag 
all the time, but in the HTML 
spec, it's not a tag, just an 
attribute.
 It still has to be in the spec 
because some alt sites use it 
and to be a valid browser, it 
has to follow the spec. It's in 
the spec. 
    Fetile, an SVG element for 
filtering and they are part of 
the HTML parsing spec, as well, 
you can put SVG inside HTML and 
that leaves sarcasm. This is the
 parsing spec here and says an 
take a deep breath and act as 
described in the entry below. 
It's in there, it's an Easter 
egg, someone having a bit of 
fun, but it is in the HTML 
passing spec. 
    Let's do another question. 
Here we
 go. According the 
specifications, the web specs, 
which of the following will be 
requested? We have a link that 
will preload. And we have an 
image here, the source of two, 
and we've got a bit of 
JavaScript creating an image, 
giving it a source, a three, and
 a little bit of script creating
 an image but spelling it 
differently giving it a source 
of four. Select all of the ones 
that you think are actually 
going to be requested given that
 piece of code. 
    We can see from the answers,
 it's pretty confident on most 
of them. One of them about half 
of you think maybe is, maybe 
isn't.
The rest of them were all pretty
 confident on. Take a guess if 
you haven't already. Make sure 
you hit the submit button, as 
well. In three, two, one, and 
we're out. 
    All right. So what are we 
looking at? So it's four that 
you're less sure about. All of 
the rest you're going to be 
requested. The answer, it's two 
and three. Okay. So quite -- 
most of you got those right. I 
guess the one that caught most 
people out was the preload one. 
This used to work according to 
the spec and in browsers, this 
would actually trigger a 
request. But the spec has been 
changed in the past few months 
and it now requires an as 
attribute to be given a value. 
And if it doesn't have one of 
those, it doesn't request 
anything. 
    Image source 2. Now in the 
HTML spec we saw before, it has 
this line a start tag whose name
 is image, it's a pause but it 
changes the token tag to image 
and reprocessing it. It's 
aliased. 
    This is actually from mosaic
mosaic, which is one of the 
first ever web browsers. And 
they rely on it. About 5% of 
pages, we find this misspelled 
image tag there. It has to be 
part of the spec. Browsers have 
to do it in order to work on the
 web. 
    Next, we had this -- if you 
create an image element to give 
it a source, the request will 
happen straight away. You don't 
have to wait for the image to go
 into the document. This one, 
this is a little bit different. 
This doesn't work. We saw the 
alias before where if you spell 
image wrong, it'll figure it 
out. That rule is in the parser 
spec. By going to create 
element, you're bypassing it so 
you don't pick up that little 
rule. This will be an HTML 
unknown element if you made up a
 name and it won't make up the 
request. 
    Next question, here we go. 
All right. So we got a bit of 
JavaScript here. Which values 
are logged when the following 
executes? So we've
 got a loop here. Inside the 
loop, we're doing a set time-out
 and
 logging the I value. Is it 
zero, one, two, three, four, or 
one, two, three, four, five, or 
five, five, five, five, I don't 
know how many times I said that 
now. 
Or none. Select one and hit 
submit.
 One of them you're sure is a 
room that is not going to be the
 right
 answer? This happens in the 
quiz as we go along. People 
start guessing randomly. It's a 
good tactic for the questions. I
 am closing the question in 
three, two, one, and we're
 out. 
    High confidence on another 
number of there. Less keen on 
one, two, three, four, five. 
Less keen on five, five, five, 
five. The correct answer is 
zero, one, two, three, four. 
    Why does this happen? We 
take this piece of code here. 
What this will log. This
 will log five, five times. And 
the reason for this is I'm
 using only one instance of the 
variable I. I becomes five when 
it exits the loop and all of 
those set time-out things will 
run Java wards. Afterwards. But 
that's not what I used in the 
test. I used let. And it's a 
relatively recent part of 
JavaScript. Rather than being 
scoped through the function, 
that will be scoped to the four 
loop. Each iteration gets the 
value of I. That means, it'll be
 zero, one, two, three four. 
    Devices at the ready. 
Another question. Okay. CSS1, 
here we go. What happens here? 
What happens to Foo? Min height 
of 300 and max of 200. Is it 200
 pixels tall, zero pixel tall or
 crash all browsers in a 3-mile 
radius? Find out how the voting 
is going. There's two answers 
that no one has picked yet. 
That's amazing. I imagine one of
 them might be the browser 
crashing one. Probably that blue
 one
 there. We're going to get 
through all of these questions 
together. Hit submit, three, 
two,
 one. The correct anxious, 
though it will be 300 pixels 
tall. And this is a silly 
question you only know the 
answer to if you've been reading
 the spec like I did.  one. The 
correct anxious,Min height 
always wins if the values clash 
in any way. 
    Okay. Let's do another 
question. Here we go. Right. Now
 this is a tough one. This is 
one we're guessing. According to
 the spec, which of the styles 
will a user see briefly? We're 
going to set time-out. That's 
going to be queued. We're going 
to set the background to red. 
We're going to do a promise REL 
solve and set the background to 
green, request an animation 
frame and set the background to 
nothing. 
And we're going to set the 
background to blue. 
    Now, which of these 
background colors is the user 
going to see even if it's going 
to be briefly. Is it going to be
 red, green, blue or light 
golden rod yellow? Which is a 
valid CSS color. Select the ones
 you think the user might see. 
    We've seen the answers 
coming through. Pretty confident
 on one of them. Sort of 50/50 
on two of them. One of them 
you're confident is not going to
 come up. I wonder if that's the
 goldenrodyellow one. I'm going 
to close the question. Hit 
submit. Closing the question in 
three, two, one. And it's 
closed. Okay. So pretty 
confident on blue and green not 
so confident on red. Correct 
answer. I'm sorry, none of them.
None of them the user's not 
going to see any of these 
colors. 
    This is an evil quiz. I'm so
 sorry. Let's have a look at how
 this happens. So then, when 
this code runs, the first thing 
that's going to happen is it's 
going to set the style, this 
in-line style to red. But the 
browser isn't think thinking 
about it at all. That comes in 
later on. We cue this microtask.
And this animation frame, again,
 we'll deal with that later. The
 next actual style change that 
happens, we're going to set that
 to blue. Again, the browser's 
not thinking about styles at all
 at this point. The JavaScript 
stack is empty. We deal with 
microtasks. This bit's going to 
run. But, again, the browser's 
not thinking about rendering at 
this point. We're going to get 
code and animation frame 
running. This was the bit that 
was a surprise to me when I 
first learned it. It's always 
going to run before the browser 
does the next render. 
    So we're changing that 
background value a load of 
times, unset it here. But all 
that's going to happen before 
the browser paints. It's going 
to be unset the background 
before the browser thinks about 
updating pixels on the
 screen. 
    Unless it's inside the 
request animation frame call 
back. If you're inside the 
callback and request animation 
frame, that will fall to the 
next frame. 
    This behavior, you see this 
in FireFox, Edge and Chrome, 
Safari runs request animation in
 the next frame. You'll see a 
flash of
 green. Next, here we go. Right.
 Okay. This is another evil one.

 code, which of the following is
 successfully set to one? We're 
setting up a new type array 
there and setting the .9 
property, the 1.0 property, the 
1.1 property and the 1.2 
property. Which of those are 
going to be successfully set. 
Which is suggesting that some of
 them aren't, but maybe it's a 
trick question, who knows. 
Select all that apply.

there. This is a point in the 
quiz where people give up. 
That's fine. That's what I would
 do, as well. Get your answers 
in quickly. We've only got one 
more question to go after this. 
I'm going to close the question 
in three, two, one. 
    And we're closed. Okay. So 
what's the -- what are we 
looking at here? Roughly 50% 
across the board. We're pretty 
confident about 1.9 maybe less 
on the others. Here are the 
actual answers. They are the 
only ones going to be 
successfully set. And this was 
told to me. I asked the team at 
Google to sort of give me some 
of their weirdest JavaScript 
education and this is the one 
they came up with and I love it.
I didn't know about this until 
they told me. Here's what's 
going on. When you do something 
like this with an array. If it's
 a string, it treats it 
specifically, it will treat it 
as an array index assignment if 
the -- when it turns it into a 
number and back to a string that
 it's the same as val 
originally.
The canonical representation of 
a number. Otherwise, it's just a
 standard object assignment. 
    Here's what I mean by that.

plane object assignment. It 
works fine. 1.0, the canonical 
version of that is 1. The .0  
gets dropped. It works. 1.1 is 
the canonical way to display 
1.1. It matches. This becomes an
 array index assignment. And 
that's fine. But what happens 
here is JavaScript sees 1.1 and 
goes, well, I can't assign to 
the 1.1 array. Needs to be 
integers. It doesn't form an 
error at this point, silently 
ignores it. That value is lost. 
And the same happens with 1.2 
exactly the same thing. Blue my 
mind when I heard that. Okay. 
Let's do the last question of 
the quiz. Here we
 go. 
    Given this chunk of HTML, 
which resources are going to be 
requested? So we're going to 
link preload there, similar to 
one of the ones we saw earlier 
in a script source equals two. 
Media attribute href3
 and import 4. Which of the 
requests is the browser going to
 make given this chunk of 
HTML. Looking at the room of 
reasonably confident all of them
 will be requested. Maybe less 
sure. That one's moving between 
below 50% and above 50%.
We've got the lowest answer on 
the side there. This is the last
 question. So definitely guess 
if you haven't made a guess. I'm
 going to be closing the 
question in three, two, one, and
 we're out. 
    Okay. What are we thinking? 
Pretty confident one is going to
 be requested. Not so sure about
 two. And three and four, pretty
 high confidence. Okay. I'm 
sorry. It's just one.
 I'm worried people are going to
 throw things at me when you 
find out why. Here it is. This 
is correct. We saw this before. 
It's got the attribute, 
everything's going to work fine.
    This next one, this isn't 
going to be requested because 
it's -- it's script SRC, not 
source. So little bit tricky. 
And here's the other two.
 Now --
you have to have the proper 
closing tag. 
    What's happening, is it's 
going to treat the last two 
lines as if it was JavaScript 
Now rather than HTML, those are 
not going to be requested. If it
 was HTML it would work, but not
 guilty as JavaScript at all. 
Sorry, that was a trick that
 one. Let's see who the top 
three are. It was a short
 quiz. 
Congratulations to the top
 three! All that's left for me 
to say is thank you for joining!

and you battled through the 
British weather yesterday which,
 you know, I brought along with 
me. You should have an email 
with a feedback form, I guess, a
 survey in your inbox. We want 
to hear about the stuff you 
enjoyed. But also, where we can 
improve, as well. For me, it's 
been great to talk to so many 
developers who take web 
performance seriously. I want to
 bring developers from the US 
and UK here so you can teach 
them to care about the 
performance because it's 
sometimes a really tough sell, 
but not here. We're also happy 
with the diversity of the 
events. Over 30% of the 
attentendees identify as women. 
It's definitely something we 
want to continue to build on. 
    There's details over local 
events on the GDDIndia website 
and all of the talks going on on
 the YouTube channel. Speaking 
of video, roving reporters have 
been wandering around with 
cameras for the past couple of 
days and they've been shooting 
footage and here is what they
 got. And thank you so much for 
being here. Enjoy the rest of 
the weekend. Safe travels and 
hope to see the rest of you 
soon. Thank you very</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>