<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Tutorial For Beginners - 4 | Hadoop Tutorial | Big Data Tutorial | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Big Data Tutorial For Beginners - 4 | Hadoop Tutorial | Big Data Tutorial | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Tutorial For Beginners - 4 | Hadoop Tutorial | Big Data Tutorial | Simplilearn</b></h2><h5 class="post__date">2017-01-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eG53syKzO9w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to Big Data Hadoop and
spark developer introduction to hive and
Impala lesson in this lesson we'll
discuss the role that hive and Impala
play as components of Hadoop ecosystem
and how they can be used to analyze the
data first in the first topic of the
session we will look at the core
features of hive and Impala after
completing this session you should be
able to explain hive and Impala along
with the features available compare hive
and Impala differentiate between
relational database and hybrid Paulo
frameworks and execute queries using
both hive and Apollo the first lesson
we'll look at the features that are
available in hive and Impala and how
they can be used to analyze the data
both - Impala provides sequel Lite
interface that allow users to extract
data from the tube the
sequel interface is same as most
developers are familiar with this slide
you see a typical sequel statement
it's a typical sequel syntax that you
will should be familiar with and those
queries can run either against row files
stored on HDFS or they can execute
against data in no sequel databases like
HBase but the one important differences
between the two frameworks is that hive
is typically used for batch processing
like ETL jobs while Impala is very
suitable for interactive sequel so first
let's look at the similarities between
the two
both hive and Impala brings large-scale
data analysis to a broader audience and
that means you don't have to be a Java
programmer you can use a familiar sequel
interface to do your analytics it's a
lot more productive than running than
writing MapReduce programs in MapReduce
whew I the equivalent of few lines of
sequel can involve hundred lines of Java
code so frameworks that you sequel on
Hadoop are very handy in that respect
and finally they offer inter ability
with other systems one example would be
a bi platforms that rely on sequel so
you can use hive and Impala to connect
di platforms all other applications that
rely on sequel to the Hadoop framework
now let's compare the two hive and
Impala Impala is relative relatively
recent it was developed by Claude era in
2012 while five became available back in
2007 and it was developed by Facebook
both are open source Apache projects now
hive runs on top of MapReduce it is it
is an interface on top of net review of
MapReduce that enables the sequel like
interface instead of writing not
produced Java programs Impala on the
other hand provides much higher
performance because there is no
intermediate step involved when you
issue a sequel statement in Ambala it
goes directly to the data whereas hive
when you execute a sequel command it has
to be translated to a MapReduce job
which then gets submitted now life is
most suitable for structured data while
Impala is designed for a higher
concurrency and it is also suitable for
ad-hoc queries while you typically use
hive for batch processes like ETL jobs
so which one to choose when you decide
to run sequel against Hadoop well one
advantage of hive is that as it supports
a lot more sequel features than Impala
does
so it is a lot more extensible the
disadvantage is that it is much slower
than in Poland but an Impala on the
other hand you can use for interactive
interactive grades and interactive
analytics so it is more real time while
hive is more oriented towards page
processes and finally Impala allows you
to supports many users while hive is
more geared towards single user batch
like environment so is hive really like
a replica or fuh or DBMS and the answer
is no it is not while I've does support
more sequel feature than Impala it is
still quite limited compared to our DBMS
a traditional database you not list most
of them you expect to provide full ANSI
sequel support so looking deeper into
the differences of the similarities of
Impala hive in the traditional or DBMS
with traditional database platforms you
you can access individual records not
only XS you can update and delete
individual records while in power in
hive are not appropriate for those
purposes the sequel the trance in
traditional RDBMS supports transactions
while sequel the drones in Hadoop does
not that's not what the system is for it
is not a transactional platform it is an
analytical platform after all finally
support for indexes in traditional
database that's fully
expected while on loop there's limited
if any at all support for indexing
another big difference is latency
traditional databases are real time low
latency while in Hadoop Layton sees my
try especially with hive as it is more
years towards batch processing and
finally the size of data that each
platform can typically access in the
traditional database it's quite limited
maybe terabytes of data while in Hadoop
it's talking about petabytes both hive
and Impala are suitable for that purpose
although hive does does allow that a
better performance on very large data
sets so in the next section of this
lesson we'll learn about interacting
with the 5em Impala we will see how the
queries are executed and what steps need
to be taken to execute your sequel and
how we can use various interfaces so
when we compare the steps that the
processing engine needs to take to
execute sequel queries the first three
steps are very similar for both live and
Impala a sequel statement needs to be
parsed it has to be optimized an
execution plan needs to be generated now
but that's where the similarities end
all for all the subsequent steps Impala
goes directly to the data while hive
needs to take several additional steps
the sequel commands need to be
translated into a MapReduce quote man
MapReduce jobs need to be submitted to
the angel and then execute them the data
finally returned stored in the HDFS so
those are the main difference in how the
query is being executed in both hive and
Apollo let's consider a use case an
education company uses hive and a power
to analyze social media coverage and
they look for a neutral positive or
negative reviews and rank them as such
so both a hive and Impala offer several
interfaces for running shells there is a
command-line shell there is hew web user
interface and the meta store manager is
used to connect the ODBC or JDBC so or a
command line shells the hive shell is
used by line while impalas is called the
pail shown in hue UI you can access
visual query editors and later we'll see
an example how you can access them from
the menu and the Hugh provides a visual
sequel editors for both hive and empower
now
to execute the hive query in beeline
where if you recall a beeline is the
name of the shell that we use to execute
hive queries you use exclamation points
to execute a command now some of the
popular commands a exit help and they're
both verbose will show you additional
details on the query and help will list
available commands and options and this
is what the beeline shell looks like
looks similar to most other sequel
shells in this demo you will see how to
connect with beeline and how to execute
a few basic queries now let's take a
look at the demo so we use the minus u
option to execute a query the first
example execute the query that is stored
in a file or file is called simple urn
hql hql by the way stands for v query
language
the next example executes the equal
statement from a command line and for
that we use - year option now that the
sequel statement that follows - e has to
be enclosed in single quotes and the
last example shows how to continue
execution even after an error is
returned and here we would type - force
equals true so here this are a few basic
examples
so when this is what the interface
command-line shell interface looks like
very very basic very similar to any
other one note that like in many other
shells we use a semi column to terminate
the a sequel play so in the next slide
we'll see how to execute Impala commands
from Apollo shell then if you look at
the Apollo shell it looks fairly similar
very similar to be line so next demo
we'll see how to connect with hive and
the polish shell so again with Impala
you have a help option - help
if you recall in beeline it was an
exclamation point help to see the
available commands and option - to
execute a command directly from the
shell we use - cue option and again Oh a
sequel command needs to be enclosed in
single quotes now - d we use this option
to start a database now in this example
we are connecting to the settlor
database but if we do not specify a
specific database it will use the
default database whatever the default
may be defined as
an example of a query execution in the
Impala shelf again a simple signal
statement and here is the output now
next we'll see how to connect using
queue interface so under the query
editor menu you can select either hive
or empower and this will open the
corresponding sequel era in the next
demo we'll see how to connect hive and
Impala shell using Hugh so in the prior
demo we saw our we find the Impala and
hive query areas in the Hugh interface
and this is what those editors look like
the first diagram in this example we
execute a command to create a table and
on the second diagram we issue a
describe command to describe the table
and as you can see there's a very
straightforward sequel editors similar
to pretty much any other yeah another
one sequel command lines on the top and
on bottom we have the results pane and
after we asked to describe the table we
created it returns the name in the name
of each column in the name of the table
so key takeaways what you should be
taking away from this lesson
now first hive and Impala are tools to
perform sequel queries on data residing
on HDFS
directly or in the age-based no SQL
database I have an Impala I'd easy to
learn for experience sequel developers
so as we're already seeing the sequel
trail and the visual tools should be
familiar to to anyone who has done work
with sequel on any other platform hybrid
Impala solve big data problem but cannot
replace a traditional RDBMS so if you
recall when we examine the similarities
and differences one of the some of the
key differences with traditional RDBMS
is the ability to execute transactions
ability to operate on single rows of
data inserting or deleting or updating
single records hive runs MapReduce or
SPARC jobs on Hadoop based on hive QL
statements so that's an important
distinction here so uh hive QL there is
an additional step it needs to be
translated and then execute it as a
MapReduce or SPARC job in palo on the
other hand uses a very fast specialized
sequel engine that is faster than that
we use so impala on the other hand does
not involve any extra steps it goes
directly to the data and that's what it
is why it is much faster so that
concludes the lesson
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>