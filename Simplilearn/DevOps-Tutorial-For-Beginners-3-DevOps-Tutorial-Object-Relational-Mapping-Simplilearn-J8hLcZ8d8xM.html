<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DevOps Tutorial For Beginners - 3 | DevOps Tutorial | Object Relational Mapping | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="DevOps Tutorial For Beginners - 3 | DevOps Tutorial | Object Relational Mapping | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DevOps Tutorial For Beginners - 3 | DevOps Tutorial | Object Relational Mapping | Simplilearn</b></h2><h5 class="post__date">2017-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J8hLcZ8d8xM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this next lesson is on data storage and
we're going to look at several things
again - first of all look at data as a
resource as it it's very important and
it's actually surprising how many people
when picking organizations don't really
appreciate this we're going to look at
relational databases and no SQL
databases and a little bit on Big Data
so to compare three different forms of
data storage that we're going to be
using
no data is a resource a lot of people
they don't understand really what
they've got in terms of data and we're
going to have a look at some of the
issues concerned
so data is everywhere the planet is
producing ridiculous quantities of data
and every business activity basically
requires or produces data but a lot of
it just goes unused or unnoticed and it
may not be obvious what's in the data
that's useful so something that's in a
database or in a library or a repository
is clearly data but what about other
things log files can often contain very
useful information emails that's always
an issue isn't it you get these huge
email chains that where people send me
emails backwards and forwards and I find
it's very hard to find the information
in them get buried in the in the chain
instant messaging a lot of useful stuff
is actually exchanged using iron systems
a lot of people use it all the time one
that works chatting away about various
things and that information may not be
captured and so the thing is a lot of
companies are starting to realize well
actually we've got all this useful
information lying around we're not using
it but we might be able to do something
with it so ask question and what do I
have so obviously it's stored somewhere
it's obvious what else is there is there
anything interesting in the log files
for example what information is being
captured or not being captured by the
organization and what data's being
produced and use what's being thrown
away
can it be used somewhere else so there's
lots of things that we need to ask the
question for neck is to some interesting
possibilities data where is it where is
it come from it's the first question and
then you've got to get it in the right
place and in the right form in there at
the right time to make any use of it and
if the information easy to extract if
it's buried in the middle of a log file
you might have to do some fairly complex
text elation to get the information out
of it
here is an example of a case studies
he's actually based on them a true
situation with the friend of mine this
one is a it's a very very good
astrophotographer he's done some amazing
images of the sky things like camp take
it did a picture of Orion site when
Simon Orion which was something like 400
photographs all stitched together to
make a massive image but then I'm
started talking to him I thought well
actually he got a huge amount of raw
data there and taken over a period of
four years and there must be something
useful in there if something moves
changes or appears we might discover
something interesting and when actually
started them a process of trying to
extract that information once these does
anything MELAS looks interesting
this is also based on a true story with
a large retail company and it keeps its
transaction data from all of its
checkouts so basically you can tell what
was bought where and when and even down
to which checkout was used now there
could be some very useful information in
that data so let's just think about it
looking at some particular stories and
see what some patterns are confined to
sorting looking for is patterns in the
data so with an example this a
supermarket it sells sandwiches the
supermarkets in the city center show a
peak sandwich sales at certain times of
the day in the morning lunch time so
it's fairly obvious that people are
buying their lunch either before work or
at lunchtime which is fairly obvious and
so what happens is still can make sure
that they've got a plentiful supply
before the peak starts say it like makes
you as a delivery at say 7:00 a.m. and
11:00 a.m. to make sure that there's
plenty often there so that they give a
plenty of choice and get best bit of
sales and another one is if you think
about the demographics this is quite
complex as this many possibilities so
you think about the product summer
gender-specific summer range specific
some people would tend to buy profitable
the turn to buy certain products other
people other products and also the
basical premium products which one do
people go for do they pay a bit more for
top rate food or something that
interested indicate something about
their demographic in terms of earning
therefore
and maybe the quantity you can get some
idea of how many people in the household
and maybe what type mix you got what is
a young children or they're all adults
so by looking at the demographics and
seeing if a particular type of
individual so falling into various
categories favor a particular store you
can actually increase the product range
to favor that particular demographic so
that you will increase the sales quite a
good idea maybe not so obvious another
one is the position of the checkout some
people prefer a particular checkout so
it may be based on some things like
search for the technology if you have
the automatic checkout some people don't
like to use them they prefer to deal
with a real person some people prefer to
pay cash rather than buy the smart
payments mechanisms we've got now maybe
the location so for example some people
buy for younger people by certain
projects check out near the exit and pay
by phone because they want to get there
in and out quickly so you can put things
that they're likely to want to buy close
to the doors and make it easy to use and
make the checkouts that use the same
mobile phone for payment available end
of locations whether people like to use
them again make use of the data to
increase sales the big business
opportunities there based on this very
simple idea but quite difficult to
extract information
it's a useful resource if data was being
thrown away and the particular
organization decides it wants to do
something about it
and so by looking at the demographics
you can terming there is things like
when people buy things what they buy
what checkouts so you can basically
improve your layout of stores and
basically dummy up on the location of
doors you can change your product range
to suit the demographics make sure
people adopt a bit time and make sure
you got the right checkouts and the
right number of the checkout of a
different type the idea so data is a
resource now I've heard that story is
told quite a long time ago data is a
resource and people still not really
doing an awful lot about it so how do
you deal with it we can always add more
hardware if we need more power and
you've got things like internet
applications of its own have
vulnerabilities which can cause issues
of scalability the other thing is data
is constantly evolving so for example in
a database if you have a very efficient
database working top performance and
then you merge in some new data so you
have a merger with another company the
data can suddenly become very badly
organized and the database can become
very inefficient because of the changing
demographics so that's so one thing that
we can think about and there's also data
recovery if you've got databases you can
recover the whole database but can you
read recover just a part of it it's not
so easy to do and recovering the whole
database can be mean information is lost
and of course the volumes I was talking
to a DBA to Bank who told me that over
the last couple of years the database
sizes had ridden risen by a factor of
about a thousand which is quite
staggering the amount of information
that's been added to the organization
and also it's hard to find people that
are good at managing data
so think about the organization you've
probably the company's got large numbers
of databases it does every single
application have its own database that
quite often is the case and that means
that the servers are properly running it
fairly low capacity and so it may be
able to do something about that make use
of things
there's also regulation requires
extensive auditing which is getting
increasingly difficult and the Jazz
performance overhead and you cut the
situation which is rather bizarre that
in the financial sector database
administrators are not allowed to do
their job because they're not allowed by
the regulators to access production
databases which is rather surprising and
also as volumes increase in security
issues increase and you have to think
about how to improve the security and
what level of the security you want and
the Internet is this is one of my pet
subjects the Internet's got over a
billion websites and 4.7 billion web
pages asset rather a big number is new
and these keep changing of course as
sites appear and disappear and a lot of
it is actually archived the Internet
Archive does try and capture as much as
possible so the information is not lost
but what happens is of course that as
data the internet grows and more data
gets added the actual signal to noise
ratio starts to drop and if you've
noticed that if you try and do a say a
Google search for a particular thing if
you do a Google search for say a
celebrity information about the
celebrity you'll probably get lots of
hits and it's reliable information if
you try and do a an internet search for
something like how to write a particular
application in computer software or some
details of some very exotic physics or
something like that I think the file
it's very hard to find any information
whatsoever so for example I wanted to
find the definitive information about
how to calculate the position of the
planets in the solar system it took me
18 months to find it was it very very
obscurely placed and even now I have to
remember how to get search right to get
to the right place and also there's a
lot of information which is just simply
not there again another project which is
the one
earlier about the astrophysics
astrophotography there was no definitive
catalog that names all the stars there
are lots of different catalogs each
using a different name so if you take a
particular star which is one quite fond
of which is Rigel you know Ryan it has
all of those different names depending
on which catalogs you use to look it up
which gets very confusing if Rigel BCRA
it's a very odd set of notations and
there's no single catalog the other
thing is that the fight we have the
world Consortium w3c define the
standards for the web but people don't
have to follow them there's no
regulation that says you must do this
and a typical point is the read browser
web browsers typically display content
differently and some web browsers don't
display content at all so trying to make
an application work and display well on
a range of web browsers is actually very
difficult and very time-consuming and
there are surprises Internet Explorer is
notorious for not following the rules
but even then I found a recent one with
Google Chrome that I wanted to put some
equations into a web page I use the math
ml markup language only to find that
Google Chrome has discontinued support
for it because they haven't got any
people to do it it's also find hard to
find out how to do used tools in YouTube
libraries I have been doing some work on
the spring and hibernate recently for
Java and find that the documentation is
usually out of date and you try and use
it it simply doesn't work and then you
have to try and find out why it doesn't
work and what the change was to they say
the API to make it them up to date there
are certain sites such as Socratic and
kora who strive to get good quality
answers to questions to support people
particularly in the sciences but they
are fairly rare and they have trusted
users who police the site which uses
extremely good and very good answers are
flagged as exceptional which helps and
if but somebody posts a link interpret
inappropriate they delete it
so relational databases I'm sure you've
all encountered relational databases
it's the traditional storage which
they've been around for quite a long
time and it's amazing how badly utilized
many of them are so we want to look at
some issues and we're looking at it from
the perspective of improving the
performance that we're getting better
applications the model for relational
databases has been around since the
1970s so it's quite an old technology we
just have out data organized into tables
every table is a set objects of some
kind or part of objects we have the
attributes columns and the rows of the
objects so I think of it is a kind of
object model where yes I'll row in a
table is an actual object column is an
attribute to an object's table
represents the family of objects and we
have some uniquely identified rows but
what you use the keys which are
identified the columns which are unique
to make a role uniquely identifiable now
this is the first thing that's
interesting the order in which rows are
stored in a database is technically not
defined in practice if you do a select
star from table you'll probably get the
data out in the same order in which you
inserted it but that's not actually
guaranteed because it depends on how the
data is actually physically stored by
the database itself is often the case
but not always and a lot of people
assume it put the data in in a
particular order extremely it comes out
but technically according to SQL it's
not supposed to be lost at all and we
actually have normalization as well to
get rid of some rather nasty
inconsistencies which lead to data being
corrupted or incomplete or something
normalization is a way of adjusting the
tables in a database to make sure that
there are no inconsistencies there are
different types of normal form some of
the most popular ones are first normal
form second normal form and third normal
form third normal form is difficult to
understand and many people get it wrong
there is in fact a fourth and the fifth
normal form each of which is stronger
than the other so first normal form is
relatively easy to implement it gets
harder to implement second and third
normal form the consequences of over
normalizing is that the database is very
difficult to access because you have to
do a lot of table joints
the advantages you don't get any
problems associated with duplicated data
which doesn't get updated completely an
inconsistent data for the whole purpose
of normalization is to make sure that
the data and database is completely
independent no duplications no
inconsistencies and so you can guarantee
that data integrity is maintained the
first normal form is the story is that
the table has to satisfy certain
conditions the basic idea is that
there's no repeated groups or two
columns that have the same information
anything a different of a different type
or different value so if a table Reese's
personal formula ET follows the things
conditions there so there's no ordering
of the rows which is fairly well that
was not be successful in they talked
about an ordering of the rows
there's no ordering of the columns
doesn't matter what order the rows or
columns appearing for it to operate
there's no duplication no duplicate rows
that is usually enforced by the keys but
not necessarily if you don't have keys
on the database which is sometimes the
case you may have a duplicate rows and
it also says don't have anything hidden
such as like comma-separated lists four
columns that sort of thing that makes it
normal and then we have this wonderful
phrase about every row and column
intersection contains exactly one value
of the appropriate domain what does that
mean well basically we don't have to
phone numbers okay that's the reason and
if we got this particular table there
we've got phone one phone two that is
what makes it non first normal form
because effectively phone one and phone
to contain the same type of information
from the same day domain and so it's not
the right way to do things having phone
one phone two because it was what I just
phone three that causes a few problems
and if you don't have a phone - you got
a null to deal with which is also
problematic so to make this database
first normal form what we need to do is
extract out the phone numbers into a
separate tape and then we just have a
key which is the customer number which
enables you to link the two tables
together to reconstruct the original
unnormalized information in its entirety
the terminology is wonderful for these
things so a table is in normal second
normal form if it follows the first
following rules first of all it's got to
be in first normal form obey all those
rules and then we have these wonderful
descriptions of saying what tables
supposed to be so basically what it says
is that if you cut a composite key then
every column should you be dependent on
the all of it not just part of it so so
that means you don't have something
that's some only depend on parser case
so for example this table is is in first
normal form
everything's fine there but there is a
problem that we've got the loan number
and Loan account loan amount and the
loan amount is depend on the loan number
but it's not actually dependent on the
customer number okay there's a
dependency between the loan number and
the customer number with that is a
partial dependency which makes it non to
normal form so what we do is we take out
the loan numb and the loan balance which
is the partial dependency into a
separate table obviously the first table
index is the second one you join the two
to reconstruct the original first normal
form data right this is a tricky one
third normal form there are others as
well four and five but we normally stop
at this one it's them it's a tricky one
because what I think we don't get it
quite right and there's a misquote there
of Ted Kord who's the guy that you find
it which gem based on a phrase that it's
some the basically idea is that
everything is dependent only on the
primary key and nothing else so that
means that if you've got two columns and
one is dependent on the other but
they're not both dependent on the key
then it's not in the right form and
short term third normal form so let's
have a look at an example so here we've
got a two normal form database is based
on the Wimbledon results so what your
winner a nationality well if you look at
that tune or form
it's not third normal form because the
nationality is not dependent on the on
the key which is the ER okay so what we
need to do to make your third normal
form is split that out so we've got the
year and the winner
kiss sign and then we second table if
the the player's nationality and that
gets rid of that duplication and also
may get rid of that partial dependency
which transient dependency which we had
before so primary key a table in a
database doesn't have to have a primary
key they often do but it's not necessary
and some actual database tables
can't really have a primary key because
there's no real candidate for it or it
doesn't work too well so basically at
all the primary keys if one or more
columns that uniquely identifies each
row of data it can be more than one
column say the composite key and if it's
a naturally occurring piece of
information that's in the data it's
called natural keys same suggests we
often use a number it's for the
surrogate or synthetic key we just add
another column which is got a unique
number which we just generate from the
previous value and that value is then
the primary key value it does make it
difficult more difficult to to use as a
person because you put a generate or
find right the surrogate key there's no
naturally occurring things associated
with it so which number is associated
with which data there's no real easy
Association there also foreign key is a
basically a column in a table which
stores the primary key value of another
table used to join the tables together
showing the relationship there entity
relation diagram
well mentioned that tables bad beam is
set in set theory a set is an unordered
list of unique objects in the
mathematical sense if a database has a
primary key we know it's in every row is
unique so therefore it's a set and
because technically a database is
unordered then it follows the rule of
the set theory with an unordered list of
unique objects when you query a database
you also generate a set it's basically a
table in memory the result set thinking
of state basis in terms of sets helps an
awful lot in getting the the queries to
work think about the operations that you
can perform on sets and get the results
you require if you think about the set
operations you can map them directly on
to database operations so the normal
join you do is you finding the
intersection of two tables or two sets
dizzi in join and that shows the
intersection there there's a full outer
join or Cartesian join which very few
people actually use because it generates
massive amounts of information where you
associate every row of one table with
every row of the other table so you get
the combined number of rows in each
table multiplied together as a result
set then we get the left and right outer
join and there there are set operations
basically the idea is we want every row
from one table and associated with this
the corresponding row in the other table
if it exists or nulls if it doesn't
exist so it's a fairly common one the
difference between a left outer join and
a right outer join is just which table
you specify first I tend to just use
left outer joins actually cut it all go
do is change the order of the tables
turn the right outer join into the left
outer join although using these smallest
table first often helps for added join
like a Cartesian product easy answer is
yes basically if you've got like three
rows in one table and four rows in in
the other table you basically get 12
results every row of the first table
associated with every row of the second
table so yes it's the Cartesian product
it's often called a Cartesian join so
that you don't do very often naturally
because it just generates massive
amounts of information the inner and
outer joins are the most common as the
most
we have another issue with standards for
mention that the Internet's not problems
with standardization in particular that
the w3 consortium puts the rules
together about what hTML looks like and
the vendors of the browsers may choose
not to take any notice of it in a
database world we've got the structured
query language SQL now SQL has been a
standard but actually our call as a
database predates it and all the
database vendors tend to add extensions
to the SQL language for doing various
things so this means that certain
constructs are basically nog standard
SQL which is fine but certain other
operations like if you want to have
stored procedures if you want to do
certain administration tasks each of the
database vendors will have their own
syntax and command set for doing that
which is part of the effective part of
SQL but it's a non ANSI non-standardized
so for example oracle has got various
things like come bulk collects and for
all which your operations for doing bulk
data processing credibly efficient if
you use those rather than doing things
any other way and there's also things
like scalar functions you've got your
sort of counts and Max and min but
there's all sorts of other ones Oracle
has got a very very large number of
functions some of which are rather
exotic and you don't use very often the
syntax of them can be quite interesting
to say the least because they have some
very odd ways of doing the processing
you can't really avoid it if you want to
do something reasonably efficient in a
database you almost certainly need to
use the import functions and they might
also suit your application better there
the thing is if you don't use the
non-standard features you can do that
and people will argue maybe well we
should use standard features only
because we might want to port it I think
it's incredibly rare that an
organization would change database
vendors because the risk and cost of
doing that is huge
okay there might be issues their company
merges with another one and they're both
using different databases you might they
might want to try and get to standard
across the organization but that's a
pretty major undertaking and wouldn't be
done very often and the thing is if you
have say an Oracle database you'll be
paying Oracle quite a lot of money for
the connections you may as well make use
of all the features of the database and
make it very very efficient other there
is an issue which is where do you do the
processing do you put business logic in
the database or do you do it in the in
the application code the whole story's
very very controversial some people
actually say you must never put this Mis
logic in the database others say that
you may as well make you service and put
the business logic down there in the
database in terms of store procedures
but you've paid for the boat database so
you must make use of the features in it
you can do some really clever things
it's amazing how many applications do
the following they read a lot of data
out the database they do some processing
and then they put it back in again that
requires quite a lot of i/o a clean
efficient way in fact if you did it
directly in the database itself you can
do it much faster so I actually worked
on an application where a series of
database operations which are actually
written in COBOL took about four hours
to run every day I replace the COBOL
which is one of my greatest achievements
getting rid of COBOL and with stored
procedures and it ran in my 20 minutes
quite significantly faster as a result
now indexes are another subject which is
quite controversial and what is an index
it's basically a data structure which
enables you to access the data in a
particular way which is more efficient
the consequences of non indexing is that
you have to do a thing called a full
table scan which is you literally have
to go and check every single row of the
data in the table until you find it
which might take a long time the index
is a data structure which gets there
quickly by some kind of hashing or some
other algorithm that's used now if you
have an index or not doesn't change your
SQL the SQL is completely independent of
the index what it will do is it change
the way that the database will execute
things so when the database tries to
decide how to do the execution of the
query it will look at the indexes and
decide whether or not to use them and so
anything X's are also in different piece
of storage typically from the data in
the table whether there's an index on
the data on the table or not doesn't
affect the data itself
we got indexes which can be unique or
non unique so and the primary key of a
table is always associated with an index
which I'd caught me out in the past I
was working on Oracle I wanted to get
rid of the primary key constraint so I
deleted the primary key constraint and
still couldn't do what I wanted to do
because I hadn't deleted the index
associated with the primary key but when
you add a primary key to an Oracle
database it will create an index as well
if there isn't one already there and the
diagram shows how things are organized
in that you can look at past the data
which will take you to another table
which will show you where the actual
data is stored now one thing that crops
up a lot is a lot of databases in the
real world that but a lot of indexes and
they're often not used and the problem
is that if you have an index all indexes
take up a lot of space because
effectively you have to replicate the
data in the columns that the indexes
applied to they also slow down
operations such as insert and update
because when you insert a row into the
tea
ball or update a row it will you
probably have to change the index as
well and say most indexes are never
actually use because the actual database
engine itself may choose not to use the
index you can't really force it and
quite often in cases that the index is
sitting there and the database chooses
not to use it the optimizer decides it's
not going to use it so a few rules and
guidelines for indexes columns which are
fairly unique in value if you go to a
table where a lot of the column value is
the same it doesn't make much sense to
add an index because it's not going to
help an awful lot and also queries that
return small amounts data there's a
guideline of 15 percent of the rows it
returns more than that it almost
certainly isn't worth having an index
and contrary to popular opinion a full
table scan is not necessarily a bad
thing
for some database operations the full
table scan is the way forward and it's
the best way of doing it so don't put
indexes on columns that change a lot
because they of the insertion and update
problem don't put it on a small table
because you don't need it because you've
got a full scale of a small table is
going to be fairly quick adding an index
you're just going to add overheads can
like columns with lots of duplicate
values don't make any sense because the
index is looking for things that are
fairly unique not things where a lot of
things the same and one thing you can do
is you can ask the database for an
execution plan for the service SQL
statements the execution plan that the
database gives you may not be what it
actually does that's the worry that you
can say how are you going to execute
this query it'll come up with a solution
which it may choose not to do at the
time when you actually do the query
though there are ways you can say okay
what did you actually do when you
execute this query and find out whether
it use the index or not the question
about the table scan causes performance
issues it is very expensive it's a very
big table but if it's a small table and
all the data in the table is fairly non
unique it doesn't make any difference
because it's going to have to do a full
table scan anyway effectively a very
popular if you've got an index on
somebody's surname and you take a very
pila surname which many people have to
share then indexing that column is not
going to make any difference so table
scans are not necessarily a bad thing
yes they're going to involve a CPU usage
but it may just be that the database
choose to do so anyway and you can't
stop it from doing it a full scan
and another thing is a tablespace a
tablespace is a petition of the database
use storage so database storage is
typically broken up into a number of
table spaces a tablespace can contain
any of the artifacts in the databasing
so the tablespace can contain any of the
objects tables indexes anything else
synonyms any of the other pieces that
lives in the database and a tablespace
is a set of data files and these are
sort of guidelines as to when to use
them some DBAs puts tables in a
different tablespace to their indexes
for performance reasons in that when
you're run scanning the data file for
the table and for the index if it's on
the same physical disk you could get the
heads on the disk removing a lot of
jumping backwards and forwards between
the table and the index areas of the
under file system and I worked in one
organization where it was required that
the table and the index went in a
separate tablespace in fact in today's
world that argument about the the head
seeking operations doesn't really apply
because most databases are stored in
data centers the actual storage is going
to be some network storage device and
you've got absolutely no control over
which physical device the actual table
spaces are stored on so it may make no
effect whatsoever because of the way we
use our technology these days and of
course there's the one of the biggest
problems you get with some databases
instead of a special tablespace in
oracle called the undo one so when you
do certain operations which need to be
committed it puts the intermediate steps
into the undo space and I've often had
problems where I can't do an operation
because there's enough space in the undo
space to take the operation and you have
to do some very interesting things to
make that work which often remove
requires removing indexes and putting
them back on again
or rather switching indexes off doing
something and then switch them back on
again
now this is really neat when I first saw
it I thought oh this is so curious and
then started looking into a bit more
detail it actually makes an awful lot of
sense and it's called petitioning we
have the problem which sucks about table
scans and if you've got very very big
database tables which is very common
these days you can again dope with
millions of records or rows in each of
the tables now if you think about the
data what is the nature of the data in
the table so for example you might find
that some of the data is quite old and
you'll very rarely if ever access it me
you might do it for auditing purposes
but you're not going to be doing
anything too serious to it other data
which used to be probably the current
data the latest latest erm additions is
going to get changed quite a lot so
effectively you've got parts of the
table which are going to be accessed
very frequently and part of the table
are not going to be accessed much at all
so what we can do is use a petitioning
mechanism and what that does is you
basically split the table based on some
query or partial query or condition and
physically store the parts of the table
in different places so essentially when
the database does a query
it'll only access the piece of the
database table that you actually want to
use and will not use the rest of it and
it means you can have the petitions can
be in different table spaces different
storage and the SQL doesn't change
because the query still works it's just
that when you build the database table
you add the extensions for petitioning
which make it work quite well so that's
a really quite a neat way of doing it
and as the diagram shows you might say
want to break your table into pieces
based on time like months and so the
related stuff is in one place the other
stuff is just left if you want it and
petition logical or physical it's a bit
of both because you actually physically
store the different petition pieces so
that a table positioned in to say for
petitions each can be on separate
physical devices but when you do the
queries the nature of the petition the
condition that decides which petition
you're using will dictate which piece of
the data is actually accessed by that
query it doesn't affect the query
so when you do this guidelines when it's
bigger than two gigabytes so one of the
rules if it contains historic data which
is read only or effectively read only
you don't do it that's a good candidate
put that old data into one or more
separate petitions and and so that you
don't access them unless yet so you need
that data and then this also if you want
to spread the table over multiple
storage devices for various reasons like
size reasons there's another suggestion
that indexes should be petitioned and
that is when you change the data so you
delete something if you delete data or
update the primary key or insert
something the indexes may need to get
recreated or rebuilt if you petition the
indexes then it will only update the
section of the put the index which is
affected by the the change in the data
it is incredibly powerful it is now seen
to be the way forward I say a few years
ago I hadn't really heard of it but now
it's very much the flavor of the month
and put say frequently updated data into
several small petitions old data into
others and it'll only scan the petitions
which need to be scanned so the
different types petition the list
petitions you take a particular key and
specify a range of values for that
particular key so you could say if
you've got your organization works in
say three regions the key that defines
the region you can use a petition to say
right I want the a States region to be
in one position the west state in
another and central in another and so
that means that when you do operations
on any region the data for the other
regions will not be looked at in any way
the range ones usually for dates
you basically say write this petition is
for this period of time or for these
particular range of values of something
which is very popular think of range
petitions quite the communist ones and
then the other one is a hash
it's when you don't you want a petition
that you don't really know how to do it
because the there's no obvious range of
values or particular values that can be
identified which write the data up this
basically says to the database you get
on with it I want you to use petitions I
want you to use so many petitions
I don't care where the data goes so what
it does is it will use a hashing
algorithm of some kind which will decide
which rows go into which petitions and
so it gives you a kind of load balancing
between the different positions there
again you've got the same things that
when you do a particular operation
the query only affects the data in the
petition that's being used and nowhere
else how should we agree on petition
keys it's normally you'd use the key
based on something in the data so for
example the obvious one is dates if
you've got a time stamp or a transaction
date in the database then you can say
write everything that's more than say
two months old is effectively archive
data and so we can put that into a
separate petition to the the live stuff
otherwise in my fate could be done
regionally as we saw in the other
example that's specified that data for
each particular region we operate in
goes into separate petitions you can
tell basically on the basis of the data
you have to understand your data is the
most important thing it's a very
important thing to be able to do
understand what the data looks like and
which pieces are being used more
frequently than others or which ones
should be kept away from others and
choose your key usually and based on
that it's usually a like a date field or
a region field or something maybe a
project ID line key or something like
that so it's no hard-and-fast rules and
if you don't have an answer you use the
hash and then get the better database to
use for you yes petitions are definitely
based on the use case each one will have
its own petitioning rules which you
would decide on
touched on this before where do you do
the data processing do you store
procedures do you use just put your SQL
in the code I prefer to use stored
procedures for several reasons the first
one is I don't like to see SQL
statements in the actual application
code so yeah I don't like SQL in the in
the code so don't like to see Java and
SQL in the same file it's actually
cheaper in many ways to call a stored
procedure because you don't have to do
any processing in the database and you
split up the logic so in a way that you
say okay moving data around is best done
in the database it's faster rather than
dragging things across the network and
seeing some people say don't use
business logic in the database because
it puts it in a different place but if
you think about the three-tier model of
the people have think of it like a
fourth tier model where you've got your
business layer and you've got like an
extension of the business layer which is
a middle tier between the which inside
the database between the which is where
the spm SQL all stored procedures stored
and then you've got the actual database
behind that well there's a number of
tools you can reuse database deployment
yes of the dark or data integrator there
are others which are basically used for
store and access during your database
information or being able to specify
what the changes to a database look like
just quite a few tools around that can
do those sort of things okay so this is
controversial I did one program on
databases and there was nearly a fight
broke out between two sides of the room
because one said it's got to be in the
application is it's got to be in the
database and neither would agree to
disagree
so if you're transforming data reading
from one set of tables and writing to
another set of tables of doing
transformations database is very good at
it and in particularly you've got the
bulk operations which are very very
efficient so you do a bulk collect and
then a bulk right and so make the best
use of your database
try not to drag stuff over the net worth
having to put it back again think about
doing aggregations
and something's inside the database if
you're doing processing which for
displaying results stick those in the
application code there's no hard and
fast rule here it depends on use case
again which one to use now this is a
good one dynamic queries often you can't
form in a to the SQL statement at
compile time you have to actually
assemble the statement at runtime this
is fairly common particularly if you're
trying to say do a query based on say
demographics which is selected by a user
interface and you don't know which
combination of columns you're going to
use for any particular query and so you
need to build the query up but you have
to be careful if you do string
concatenation to build an SQL query
you're asking for a an SQL injection
attack which is SQL injection is
probably one of the biggest causes of
problems associated with databases and
it's amazing how many people do it you
tell them do not do string concatenation
the next thing you see string
concatenation is in there and SQL and we
have a potential problem so this example
code here we're passing in a customer ID
as a string and say that comes from a
form on a web page or some other thought
source of information which is user
controlled if we replace the actual
strip actually call that update customer
procedure with that code in there what
it does is the quote one just terminates
the the open quote and adds the one to
it the semicolon starts a new statement
and then we had a drop table and it does
so what happens if you execute that
particular statement there it will cause
the table to it'll do the update and
then it'll immediately drop the table
and commit it because the drop doesn't
implicit commit which is even worse so
things to do first of all make sure that
you use bind variables so you don't use
string concatenation thus bind variables
are basically where the SQL is sum is
wrapped up and put into insert into the
statement by something else it might be
a
prepared statement in Java or it might
be some user bime variable in the
database itself and so try and avoid the
string concatenation use by empirical
you can't always do it because you might
need to add complex say where clauses
which can't be done without it
give the database user just enough
permission that was the security thing
so to give people the minimum access
they need to do the job it's amazing how
many databases out there in the real
world are accessed by people and
programs using the schema owner of the
database that is just wrong because the
schema owner as the name suggests can do
anything they like to the database if
you only need to be able to read the
database make sure they don't have write
permission then you can't have a problem
but there's also things on either ASP
it's a software that will do it check
the input parameters to look for things
that are harmful very long strings
punctuation characters which your
punctuation is the SQL statements and
the dangerous keywords in SQL which tend
to do things like delete or remove or
change data and try and test your
procedures make sure that you can try
and break it with that potential
injection and see if it will avoid it
and now we get to another aspect of
databases again this is an interesting
one object relational mapping because if
depending on your perspective you might
think it's a good thing or a bad thing
and then put in the DevOps environment
we have to deal with it from both angles
which is morally if more interesting so
what happens is you've got objects
object graphs is just a fancy term for a
collection of objects all linked
together or certainly typically an
object model which is at live we all you
got your all of your objects and they're
contained and using objects and we need
to persist the objects all of the
objects into a database so first of all
we're going to map the objects onto
tables and then we're going to have to
query the tables the object relational
mapping software does that mapping and
generates the SQL so you don't need to
know the SQL you just work with the
mapping and it'll just do everything for
you and it's not things like lazy
loading which is where if you've got one
object which has got child objects it
won't load the child objects until you
try and access them cascade
if you've got a composite object if you
delete it you can delete all the
component parts and we've got caching as
well where you don't have to hit the
database with queries over and over
again you just store it what the lrms do
behind the scenes can be quite
interesting so where the thing works
with different perspectives from the
developers perspective it's very easy to
use an ORM particularly for new
applications you just write your code
you annotate the persistent objects and
if the ORM does nearly everything for
you from the operations perspective it's
a slightly different matter because
you've got very little control and what
the RM at she does to the database so
how often does it query it how often
does it do updates as it do bulk
operations and things like that and it
is often the case particularly things
are not configured quite right that the
ORM will cause a lot of performance
issues because it keeps hitting the
database with multiple requests which
could have simplified in some way too it
was not configured correctly the object
relational mapping provides API yes it's
done in various forms if you use
something like hibernate which is what
I've been using recently you annotate
your Java objects to say which which
classes and which attributes the classes
are persisted and what table and what
columns they put there they also provide
various API for controlling the
operation of the erm and they also
provide a query language which is not
quite SQL it's based on objects so
they're fairly sophisticated things so
yeah if you get some like hibernate it's
got a vast quantity of api's as well for
doing all sorts of things because for
example if you've got a data type that
doesn't naturally map to a particular
column or columns in the database you
can specify how it's done by writing
code that uses the API is to tell the
ORM how to do it a good question why
should we use our MS it's basically the
reason is that mapping objects onto
database tables requires a lot of
plumbing to make it happen so for
example if you use the standard for the
database API Saylor in the Java world if
you use JDBC if you want to persist an
object what you wouldn't have to do is
for every operation you're going to
has to generate the SQL for that
operation and populate the SQL statement
with the actual values from the object
that you want to persist that requires
quite a lot of code now if you use an
ORM what you do if you you do a simple
annotation of the object you set up a
configuration and make a few API calls
and it's all done for you in a matter of
seconds if you've got a particular if
it's partially set up already and so for
the developers perspective lrms are
great - fantastic via simplify the
amount of coding they need to do and
they can concentrate on the thing that
makes the money which is the business
objects so developers like them in
general although some things are not so
easy to do sometimes relationships
between objects can be quite tricky to
get right the operations guys sometimes
don't like them because they have
performance issues and impacts which
they have to deal with right so let's
see oh RM + RM s there are a number of
them around in the java world of things
like hibernate and i batis and there's a
java persistence api s and there are
several of them spring JDBC choice of
things that out there they're all
slightly different but they do the same
job you provide a mapping between an
object and the tables which is usually
done either in XML or annotation form
and then it just works so simply at the
box
with this you're completing the concept
of relational databases now rounding
will explain to you the steps involved
in creating and using a relational
database to a demo welcome to DevOps
lesson 3 lab number 1 let's go over to
our terminal and the first thing we want
to do is clone a repository with the
source code that we need for the labs
for lesson 3 so we're going to go up to
github github.com and the area is simply
learn devops specifically the repository
is devops lesson 3 pull that down good
we're done
now we're going to go into that
directory and notice that they're the
three labs already set up for us so
we're going to go into lab 3.1
and go into well let's take a look what
we got there's a subdirectory called
test DB that contains information that
we're going to want to load into MySQL
and notice if it's all GZ files they're
compressed so we need to use the new
unzip program and we want to specify any
file that ends in GZ and let's try that
again
so I misspelled it
moment and let's take a look now we see
we have all these dump files which is
great so let's go up a level and we need
to take these scripts and turn them into
executables with a chmod directive we'll
just set everything to make it easy and
there they are now executable
now what we want to do is create a
docker volume so volume is a container
whose function is to retain state and
we're going to create that on our own
computer and we're going to call it
MySQL underbar data so that's created
and we can take a look volume LS and
there it is that's our volume that was
created okay so now we need MySQL so a
doctor pull MySQL we want version 5.7
and we'll see that it comes down in
layers pretty quickly
you
so let's see how big it is
and it's 400 megabytes okay that's just
fine now let's take a look at some of
these scripts so there's one called run
server first and that is going to set
things up for us it's just going to run
it'll set it will run MySQL in a
detached container and it will cause it
to set a student database up now the
trick here is that we're going to export
these
variables into our own environment so
we're setting environment variables and
then notice with a - e option down here
we're going to set up environment
variables within the container so we're
going to take all this information that
we're running MySQL 5:7 the root
password is root PW we're talking about
a student user student password student
database and the container is going to
be called MySQL and we're going to map
port 3306 on the outside to 3 3 0 6 on
the inside but first we're going to make
sure there are no containers running
MySQL and then we'll run it we'll call
it MySQL we'll pass in all these
environment variables and then we'll set
it up and running so let's take a look
at that how it's going to work
so we'll say clear and then run server
first
and there we go it's up and running so
we can find that out by saying PS as we
did it's up and running for three
seconds now and then we want to say
docker logs MySQL we want to look at the
logs that are being emitted and that's
because we want to make sure it came up
correctly and we'll know that because
the MySQL daemon is ready for
connections so that's great ready to use
it now we are going to run another
script called run server the first one
is simply to get things set up so this
one is going to stop MySQL
it's going to remove it so stopping the
container removing the container and
then we turn around and run the image
again but this time we're going to do a
volume map of that volume we created and
we're going to map it to the internal
directory VAR Lib MySQL so that's going
to allow us to have an attached volume
and separate data state management from
the container that's actually running
the database okay
so let's clear and let's run the server
takes just a moment and now let's say
docker PS yes it is up and running so
we're in fine shape okay now we want to
run a client so let's take a look at the
script that does that function cat run
client and this is even more
straightforward we're going to run the
same image notice it's MySQL 5.7 and
this is fairly common to have both the
server and client code in one image
because you can obviously make multiple
containers from one image we're just
going to have a client container and
we're going to have a server container
so we're going to run this we're going
to start the shell bin bash and we're
going to mount a volume the present
working directory and the subdirectory
which has that data that we looked at
we're going to map that to slash data on
the inside of the client container we're
going to ask for a TTY
terminal and when it's done we're going
to remove the container so we don't need
to name it so let's do that let's run a
client and now we're in we're inside and
if we go up to data let's take a look
those are all the dump files so this is
great it's looking like we're in really
good shape so what we want to do is
leverage things we want to
use MySQL client program to connect to a
host 172 1702 that's the internal docker
network address of the other container
that's running MySQL as a server so
we're going to say the user is student
we want to prompt for a password and
we're going to connect to student ok the
password is student and we're in
now we want to let the system do some
work we're going to say source data
employees SQL and that's going to fetch
that script and execute it and that's
going to define the database that we
want and load the data so this is going
to take a little while please be patient
there we are we've loaded up
all the data
okay so now we're going to issue some
queries to see how we are going to use a
relational database first query that we
want to construct is going to fetch the
first ten records from the current
employee database department table and
this is a common thing that you're going
to do if you're new to a database you'll
want to fetch rows from various tables
and just get an idea of what the data is
kind of explore and find your way around
so we do that by saying select
everything which is asterisk from and
let me just do it this way because this
is it's a common thing to have the
keywords and SQL be capital letters so
we'll say
do it this way
that's the table name and then we want
to limit to 10 rows and these all end in
a semicolon so there we are we see what
the column names are so that's the
benefit of doing initial queries with an
asterisk you don't know what's in there
and you just want to take a look at the
column names and some sample data so it
looks like there's an employee number
there's a department number from date
and to date and let's take a look and
see if there's anything interesting in
here no there's it looks like one row
per employee per dart per department
number and so that's a kind of a partial
view of everything ok well let's get the
first ten departments so we will say
select asterisk from departments
and we're going to limit that again to
210
the number and the department name great
so that looks like a nice little lookup
table
oh so you might be wondering how you see
the tables well you can say show tables
and that's going to issue a query
against the metadata structure and these
are the tables that are in the database
so there's eight tables
all right let's keep going and let's
take a look at the employee date table
so select everything from let me type
correctly select everything from
Department
amp latest date limit 10
that's showing employee number from day
to date and that clearly indicates that
they have been in the particular
department from a particular day to
another day now this means they're
they're currently there it's a special
value you'll see this sometimes in the
schemas that there's a terminal value
and notice that this one is an actual
value mm so it's July 31st 2000 so this
is a date range stored in a table all
right now let's look at department
managers select everything from
department manager and again limit 10 so
we have an employee and a department
number from update to a date so that's
pretty interesting so we've got the
employee number we those must be by
implication managers and it shows the
department they're managing now notice
there's a department here that's
repeated so there's two different
managers for that department and notice
that there's this to date and there's a
from date so clearly they changed
managers on October 1st 1991 now let's
look at the employee table select
everything from employees
and that's showing that
flowing number a birthdate a first name
last name a gender and a higher date all
right now let's look at salaries select
all from salaries
and here you see the employee number or
their salary and when they were earning
that money so it's also a date range so
you can see the employee number one I
know 100 oh one has one two three four
five six seven eight nine notes they go
all the way down there keep getting
raises so nice set of raises for that
employee all right now let's look at the
titles select everything from titles
and we can see there's titles such as
senior engineer and there's a date range
staff senior engineer and so on here
we've got a one zero zero five senior
staff and we've got one zero zero five
staff so clearly they had a change in
titles okay now we're going to move on
to the next section we've explored the
data now we want to answer some business
questions here's the first one who is
the oldest employee and what is their
age
so let's try typing in this one this is
going to be select first name
,
last name comma timestamp diff
let's see here birth date
/ date that's today right now as no
comma there as age from employees
order by birth date descending limit one
so we're going to use the ordering to
get these guys all lined up and we're
going to just look at one because we
want to know who is the oldest employee
let's see how this works
and there it is Mario Cochran age 51
okay so the next question is going to be
which department has the most employees
so instead of typing this all in I'm
going to do a copy-paste here and then
just talk work you through the query
so the copy doesn't have the
capitalization and then I did this on
purpose go you can see it's a little
harder to read when you don't
consistently have the capital letter so
select you know as from and so on but
it's still syntactically correct and the
same so let's see which department has
the most employees and we've got
Department number five with sixty one
thousand three hundred and eighty six
employees okay third business question
which employee has the highest salary
and how much is it so let's type this
one in and we'll kind of walk through it
we're going to do a join so we need to
say a t1 table and the first name and
the t1 table the last name and the t2
table the salary from employees
as t1 so we're going to rename employees
as t1 so it's an alias or a shorthand
and we're going to inner join that with
the salaries table as as t2
on and here's the join condition on t1 m
p--
number equals t2 and number
so that'll that specifies to my SQL how
to consider joining these tables
together what values must be equal for
it to make an association between the
rows and so there's order by salary
descending limit one that takes a little
bit longer to do the join that's a bit
of processing and here it comes back so
it's mister pÃªche and a salary of one
hundred and fifty eight thousand two
hundred and twenty
and the last question all I'm going to
leave up to you see if you can figure
this out if you're replicating these
labs the question is which employee
manages the smallest apartment and how
many employees are in that department
see if you can answer that for yourself
so you've got some hints because of the
queries that we've already done okay so
now we're going to quit we type in quit
that's a MySQL command and that takes us
out of the client will exit the
container and now we're going to do a
cleanup so there's a cleanup script
and we'll take a look at this so we're
going to remove the docker image MySQL
and we're going to remove all of these
remaining containers that might be there
we're going to remove images and then
the final one we're going to remove
volumes so we're going to scrub
everything okay so cleanup it's done
great so that's the end of lesson three
lab number one
dr. data volumes in
session we've actually done this before
we've actually used our data filings
already my just erm this is going to
show you what is possible and going a
little bit more detail about how these
things work a doc and volume is not part
of the Union file system it's just a
mount point basically on the writable
layer that's created inside the
container so means that a data volume
can be shared or used reused so
different containers can use the same
volumes at the same time if necessary
you can write two data volumes if you
commit the image of the container
it doesn't write the data bars at volume
it doesn't become part of the image the
data volume is something separate but
the most important thing is of course is
that if you delete a container all
changes made to the rightful land of the
container are lost
unless you've committed it but if he was
a data volume then anything that's
written to the data volume it persists
as long as data volume exists so data
volumes is actually really two types of
them as the explicitly created ones and
also you can use directories within the
host system to become a data volume
effectively so what we do is we create a
volume using the docker volume create
and give it a name by default if you
don't give it a name it gets a rather
exotic cam ID and maybe a rather comical
name created for it and you can actually
do it create a volume by specifying it
on the run line as well so you create
any number of data volumes matching the
container and at the - - volume or - the
option to mount the volume to a
particular mount point in this case here
we've created the my data and with
mounting is in such opt slash data the
paths - /opt slash data doesn't need to
exist it will get created in fact it can
exist as well
if you wanted to so this is always used
for things like like a database or any
kind of repository that needs to have
persistent story you'd always use a
volume rather than using the storage on
the actual container itself
if you do doctor inspect on a container
it'll show you what the mount points
arson is it it produces a very long JSON
string and one of those entries is the
mounts which shows the name of the
volume which is a very long string and
where it is which is stored this is
actually stored within the darker areas
on the disk and where it's mounted to
and the mode of operation so you can
manage the volumes you can list them you
can delete them if you're going to have
some storage that you want to persist
outside the lifecycle of the container
yes you have to create it or
alternatively use a directory structure
within the computer that the docker is
actually running on so you can create
them you can give them a name you can
miss them and then you can use them and
mount them in there so it's just so the
form is - volume or - fuel - - volume
volume name : local directory on the
container itself so when the container
fires up it'll be mounted to the
read-only that's the rightful layer on
top of the stack of the Union FS now the
number of different data volumes you
have plugins which decide what they're
going to be the CERN did the actual
documentation is rather poor actually
and because these volumes have options
when you create them and sometimes very
hard to find out what those options
actually are so examples there's a GCE
docker plugin which enables you to
connect to Google Cloud assistant disks
so you create a disk within the Google
cloud environment and then you can use
the GC dot plug-in to mount that disk
into your container
does the flutter plug-in which hides
potato phones which are portable across
systems as a local persist which enables
values to be stored somewhere anywhere
on the house and a number of them there
and as also the default one which Shem
is used
the command run - I T - minus RM - -
volumes - from CentOS slash bin slash
bash this command will take all the
mount points from the existing container
which is CentOS and copied them this
command starts a new container from a
stand house image it assumes an existing
container called sent ops from which you
want to copy the mount points so what
happens is the volumes from option takes
all the mounted volumes from the
existing running container and Mantz
them also on the new container so this
enables you to take existing container
with volumes and mount all the volumes
as well on the new container which is a
good way of getting the same data on
both containers you can also mount data
volumes from a private cloud this
requires installing certain plugins
which are associated with the the actual
cloud environment you're using and so
for example docker has a Google plugin
which enables you to access volumes on a
Google cloud environment
so the alternative is to mount
directories and files from the host file
system and what you do is you just
specify the local path name colon and
then they contain the path and it will
automatically mount the local path as
into a directory which is on the the
container the containers the path must
be absolute relative to the rusa file
system and on Windows you use flash z /d
for the various tribes that you might
have on it don't use an editor and
explicit mounted volumes what they try
to do is if you mount a file equalled
the what editor might try and change the
inode of the file when it saves it and
that will cause problems and so it's
actually I would never actually mount a
file explicitly unless it was something
for a configuration or something like
that it's better to mount the directory
if you go to doing the editing on it
edit the file before you mount it
basically Nia files mounted explicitly
would probably be read-only we've got
various options that can be specified as
well with the second : so we've got like
read-only and read/write ro and RW
there are a number of other ones
available which are if you want to use
those there's most of them are not very
commonly used now interesting thing is
because it's like a union file system if
the directory which is the mount point
exists and has files in it then the
company basically does emerge and they
get copied into the volume so the files
in the container get copied onto the
volume if they don't already exist and
unless you specify an okapi option it
then it doesn't do that when you
creating images as docker file has a
volume command which specifies mount
points and if you any content in those
mount points they will get copied into
the volume when you start the container
up so that's just showing you how that
works you put the stuff in the file when
you do run the command it once you
populate the data volume
now that we have introduced the concept
of persistent dr. data volumes Randy
what demonstrate how to use them
welcome to DevOps lesson 3 lab exercise
number 2 dr. volumes let's go back to
our terminal the first thing we'll do is
create a volume that we'll use in this
exercise docker volume create name is
going to be my data
we want to bring down a light weight
distribution we're going to use Alpine
docker pull Alpine and this is tiny I
mean let's look at this thing it's 3.9
megabytes in size it's really nice Linux
distribution hi let's run
with a terminal interface and we're
going to name the container client we're
going to do a mapping from the outside
of my data which is the volume that we
just created to MNT mount run the Alpine
distribution and bin as H which is the
Alpine shell
we'll go up into the mount directory
let's take a look there's nothing there
now we're going to do a touch which
creates a file they're going to be empty
but the main point is that the files
exist we call it food text and we'll
touch bar dot text
again there there
can see that there zero zero bytes in
them so they're empty files
okay let's exit that container now we're
going to verify that the container is
stopped so we're going to say process
status a let's take a look at it and
it's been exited
now we're going to delete this
we want to show you is we've created the
state and the other volume and it has
nothing to do with this docker container
so docker PS a there are no containers
existing now let's run and this time we
don't care about preserving the client
we're just going to say remove when
you're done we want a terminal interface
and if once again we're going to map the
volume my data to MNT
and we're going to run the show
up and running
to Mt and we list and there they are
let's do this way I'm going to say that
they have no data then we'll exit and
let's take a look at the volumes of
course there it is a local driver with
my data so that's a demonstration of how
to create a separate volume mounted into
a running client add information to a
change at state and then verify that it
is in fact independent and it can be
connected to a different client that's
the end of DevOps lesson 3 lab exercise
number 2
look at the other spectrum we've looked
at relational database one now I want to
look at the other solutions like no SQL
and Big Data which are becoming very
much popular these days no SQL not only
SQL it's quite an old term the idea has
been that the ISA Khan database has been
around for say 50 years or so but the
terminal SQL was coined in the late
1990s but the actual technologies have
existed for quite a long time so
basically we've got the relational
database which is what we're all very
familiar with but we don't have to store
data in a relational database there are
other ways of doing it and some data
really doesn't lend itself to going into
a relational database which is what this
is all about big data with our friendly
elephant loops there so dealing with the
storage and processing of data which has
got there is characteristics which
really don't lend it to going in a
relational database so one thing is high
volumes a lot of data coming in very
quickly relational databases don't tend
to work to it well in that environment
and also the volumes are much bigger so
the typical relational database will go
to the order of gigabytes and then which
is possible to go larger than that but
we're talking volume in the terror and
petabytes really huge volumes there's
also constantly updating continuous
processing high velocity is called is
constantly changing and then if
something that's a very high variety
which is where you've got different
types of data like you've got text
you've got videos you've got images we
don't really want to put those into
relational databases as they tend to be
particularly images and videos tend to
be enormous and they take up a lot of
space you wouldn't really want to put
them in a normal database so what's the
value of this well we've got the idea is
we can get data from different places
different divisions external sources and
bring it all together it can improve the
quality of its like cut processing time
and make it more transparent rather than
having to deal with things that take
very long time a very large database
queries on the database can take it
quite a long time like an order of hours
try different things out what's in the
data what value can we extract from this
data separate customers
to different demographic so that you can
customize your approach to them
automation course try anything that we
saw earlier by analyzing the data you
might be able to open business new
business opportunities this is one way
of doing it to basically get your
business better or more functional new
customers more volume and detect
problems so you also distressed it as
well so what some what-if analysis which
is very useful potential scenarios so
basically we've got three types of data
in three broad categories we've got a
structured data which is what the
relational databases use they're highly
structured you have a schema consisting
of tables and column and the
relationship between tables that gives
you a structure which you can in applied
queries to using things like SQL then
we've got unstructured data which is no
schemer associated with it
text email or social media instant
messaging that kind of thing where
there's no actual formal structure and
then we've got the the semi structured
which is you've got something like log
files they've all got the same format
each record is it's got the similar
format you've got JSON documents which
are very very popular these days Jason
seems to be taking over from XML which
people don't like too much it's easy to
deal with and so Big Data typically
works with the unstructured
semi-structured although there are mu
some my expects of big data do work on
the some more structured data than than
those as well the comparisons between
the two the size we can talk about these
very large numbers the terror petabytes
and all the bigger ones still insecure
databases tend to be on one machine
which you can scale buying more hardware
which is a typical way of improving
performance issues buy more memory buy
more CPUs more storage but big data
tends to work on smaller machines and
it's you just keep adding more machines
to the cluster to get more scale and
more size and processing they basically
work on structured data big data doesn't
require that different types of querying
SQL databases obviously use the SQL
language or various labors of it the
other Big Data uses different tools
depending on how you want to do it
some of very very simplistic doing
things like Map Reduce others kind of
try to enforce an SQL like language on
top of the M the data itself a third
queering big data tends to be mainly
read operations whereas SQL databases
are typically online transaction
processing different there and the idea
is that the related bases the schema is
well-defined whereas processing a big
data can be take any form depending on
what you're trying to achieve so how
does it work typically the technologies
follow this you've got high volume low
cost data storage which is typically
lots of machines with their own hardware
honest with their own storage on it then
you've got the scalable computing we're
basically distributed across numerous
machines service makes it scale this is
all low cost and then you've got the
analytics which do the actual work so it
gives you a low cost solution to a big
problem literally a big problem big data
solutions what you've got kind of data
while you can use database data
transactional data you can use logs
social anything any form of data can be
dealt with it with the big data solution
and the thing is you need new solutions
to technology of storage to solve this
some of the issues it's seen that like
80% of the world's data is unstructured
semi-structured so it's often not used
and it's often difficult to process
scalable computing I mean by that is
that if you double your data size or
capacity you can double your processing
capabilities so it means increasing your
computing power operations to make it a
deal with the more data that is the
ideas scalable just means adding more
capacity to it that reduce will come to
in a moment
the idea is that computer power is cheap
it hasn't always been that way so it
used to be that hardware particular
memory used to be very expensive so now
what we can do is a standard computer is
a cheap solution which we can just keep
adding networks of the computer together
to increase the capacity just to keep
adding more machines to the environment
to scale it so we've got the forgot all
these network capacity then we can have
distributed algorithms which enable us
to process things in pieces on different
computers there's different challenges
use different approach
so not batch processing for some
real-time processing are different ways
of a processing data or need a different
a solution to deal with that so there
are many solutions we just need to take
the business problem and find a solution
which works for that particular domain
the issue where does it come from
well Hadoop and associated technology
started out with Google having a problem
if you think about it Google is indexing
the web which we know there's about 4.7
billion pages of information there are
you going to be able to search for
phrases in across those four point seven
billion indexed pages that's a lot
so can we structure the data well we
won't it wouldn't fit in a relational
database just think about what's
required to index the web you're going
to have to have keys which are all the
phrases and keywords and things that
you'll want to search on and then you
have the values with the actual URLs for
the pages it's going to be colossal I
mean how many word combinations would
the Google Site process in a day and if
you don't would even notice that if you
run a query on Google it happens pretty
quick do a Google search if you ask for
something really unusual it might take a
while but yes for something fairly
standard just people are like to ask
quite a bit it'll be very very fast
because if there is caching things that
happen so can we paralyze the operation
well yes you could petition all the web
pages and search each petition in
parallel I mean kind of quite a few
petitions so same ideas with the
database petitions but we just break the
data into chunks and then do the
operation on each one but then what
we've got to do is combine the results
at the end of the day and basically it's
not a unique problem because then you
of these problems have been solved over
and over again into different types of
things in fact Google themselves had
solved this problem several times over
the course of the their operation and if
you make something custom it's hard to
maintain whereas if we have a generic
solution that is much better the idea is
that not to put the problems there to
solve got very large amounts of data its
unstructured semi-structured we can
split it into smaller jobs but some
processing needs to be done locally if
we want to do it local processing how do
we combine split up the job into smaller
pieces and then bring it back together
again just want to do it on the same
machine so the idea is to store data on
computer nodes and they develop a thing
called the Google file system which is
basically a specialized file system
which is highly distributed so you can
store bits of data on a number of
different computers in this what looks
like a transparent single file system
data moved around its moved around
typically what you do is you take the
data put it into the special file system
which thing spreads it around and then
you can perform operations in parallel
on each of the pieces which is stored on
separate machines so the operation that
is perform is called MapReduce which I
mentioned earlier so what you do is you
take you add input data break it into
inter pieces and then in parallel you do
a processing operation on each of the
pieces to produce the result then what
you do is get the results from all the
different math operations combine them
together into the result which is the
reduce step terminology MapReduce was
developed to describe this operation and
Google basically produced a couple of
research papers which described the file
system and the MapReduce operation and
the two documents were basically taken
into the open source world and produced
a Duke Hadoop is the kind of open source
re-implementation of what Google
described in their papers so that a
recreation of it
typically a lot of big data operations
work on key value pairs and in fact
unstructured data is often storing that
way you have a key usually some string
that describes what the object is a
value which is whatever it might be
whether it's text or an image or
whatever it happens to be and so what
happens is that take the input data copy
it into the specialized file system this
ability to be each piece is given to a
mapper a mapper runs on each of several
machines the matter splits its input up
and creates a key value pair so let's
think about operation here where we say
trying to count the number of words in a
very big document what we do break the
document into pieces provide a mapper
and each mapper looks for each word and
produces a set of key value pairs and
what you normally do is produce in the
case of a word count operation you'll
take the key will be the word and you'll
give it a count of one and you don't
know you don't want do any actual
aggregation in the map but you just want
to split everything into pieces so what
you end up with each mapper has a whole
bag of key value pairs where the keys
the word and the value is 1 then what we
need to do get the reducer to add them
up so what happens is the outputs of the
different mappers are sorted all of the
ones with the same key are combined
together so what the reducer sees is a
basically a map a key value pair where
the key is the word that the value is a
list of all of the ones that were
generated by all the different mappers
for the same word and then the reduce
simply adds all the values together and
produces a key value pair which is the
keys the what is the actual word and the
value is the actual total count so
that's the process of MapReduce we've
got the Hadoop distributed file system
which is based on that file system the
Google designs which I mentioned earlier
and so what it does is it stores data in
chunks you actually create a Hadoop HDFS
file system on your nodes and then when
the data is you copy your data into HDFS
which then distributes it across the
different nodes in the cluster of
machines so if you think about analogy
as a file if you think of how the file
system works on a computer a hard drive
is
open up into sectors which are 256 bytes
long the file system will have a store
files in chunks which may be a number of
these sectors may be a four sector to
your 1k block and then when you snap the
file itself file is actually spread out
over a number of different places over
the disk but the operating system takes
care of all that and simply makes it
look like the file is one piece when in
fact is the highly fragmented system
it's spread all over the place so HDFS
takes that to a little bit more of an
extreme where it deliberately fragments
the filesystem spreads the file around
so the maps has run in parallel and
processes the data and produce the
intermediate results then the result
would use tasks produce final results
from the intermediate results the
reducers may be distributed themselves
so there might be several of them but
you have to make sure reducer sees all
of the values associated with the
particular key produced by the mapper
but you can easily do that and there's
also an intermediate step you can do
where by which the you can do a kind of
in-memory reduce operation after the map
a step to say from the sum of the
network traffic which would normally be
required MapReduce and HDFS each of your
devices virtual computers has an HDFS
filesystem and map program that what we
do then is we have to copy the input
files into the HTF s environment from
the local filesystem you can only do the
operations on files that are stored in
HDFS so the MapReduce operation is run
and the MapReduce operation is in the
case of Hadoop it's all written in Java
and the mapper can either be a Java
class implementation or alternatively it
can be a set of unix commands which
because there's a plug-in for the
operation which effectively creates a
map of from a set of unix commands once
the operation is started the data is
split across the processing nodes the
methods to generate the key value pairs
and then the output of the matter may go
straight into a which is running memory
basically to reduce the amount of
traffic over the network so that's often
that intermediate step the he's actually
a real reducer you use for the
intermediate step but it was
in memory and aggregates the data for
each mapper and then the resulting of
data is then aggregated properly when it
does the sort and splitting it to the
different reducers so it can be fairly
complex and what happens is then the
result is it's produced by the reducer
and it is stored in the Hadoop file
system the HDFS there is a quirk of HDFS
which I find a little bit annoying
sometimes when the MapReduce operation
works runs it produces an output
directory and it puts the results in the
output directory it's actually in part
so you get part and in something like
five zeros and part four zeros one in
the output directory you have to
manually delete the output directory
before you can run the operation again
because it will not run if the output
directory exists which is a a quirk of
hadoop mapreduce processing is highly
distributed you call it a cluster
typically which can be in the cloud or
it can be on a series of network
machines and the idea is highly
distributed to get the true parallel
processing capabilities the term
required to process large spans of data
now there's a name node which is kind of
orchestrates the whole process and one
machine basically controls everything
and the other ones do the processing the
data nodes do the processing
although it is possible to run the name
node and data nodes on the same machine
but the data nodes run the F natural
MapReduce operations now one thing you
have to set up the name node needs to be
able to talk to the data nodes and it
requires password less SSH communication
that means that you need to generate SSH
keys on all the data nodes and copy the
public key of each data node onto the
name nodes as it can access via SSH
without specifying a password and when
you run the SSH key generation you
mustn't specify a passphrase to lock the
private key otherwise that will require
a password which defeats the object and
there may be security implications here
but typically the nodes that will run on
a trusted environment of some kind
you most heard of the Hadoop ecosystem I
do please now under the umbrella of
Apache Petacchi the Apache foundation
started out pretty much as the Apache
web server and as seems to encompass all
things web and many other things
including Big Data and Hadoop is tajci
Hadoop and it's basically HDFS
filesystem and the MapReduce at the
actual MapReduce where Hadoop is got an
API which is written in Java which
enables you to create mappers and
reducers as Java programs or use
pre-existing Java implementations which
enable the mappers and reducers to be
things like Unix commands now HDFS and
MapReduce are fairly low-level and
people say that they really don't want
to work at that level it's too detailed
and finding the MapReduce operations is
quite a an ordeal quite a task and if I
decide what they are and it's a phase of
fairly low-level API so what happened is
an ecosystem is developed around it so
all of these things basically extensions
to Hadoop many of them actually require
for dupe to work and they sit on top of
the dupe to provide extra functionality
and get rid of the need to work at the
very low level of MapReduce so
virtualization enables organizations to
provision and maintain different
computing environments the environments
are still essentially so that you don't
have to update individual machines
certain line of external enhancements
such as control groups facilitate the
implementation of container
virtualization darker is a powerful tool
that simplifies the creation of images
and containers from base images double
images are constructed by adding layers
to a base image registries manage
repositories of images the repository
contains different versions of the same
image darker machine allows docker to be
installed and managed on remote machines
you
that's the end of the session today
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>