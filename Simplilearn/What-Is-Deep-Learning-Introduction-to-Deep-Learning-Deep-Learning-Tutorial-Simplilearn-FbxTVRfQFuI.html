<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Is Deep Learning? | Introduction to Deep Learning | Deep Learning Tutorial | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="What Is Deep Learning? | Introduction to Deep Learning | Deep Learning Tutorial | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Is Deep Learning? | Introduction to Deep Learning | Deep Learning Tutorial | Simplilearn</b></h2><h5 class="post__date">2018-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FbxTVRfQFuI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to the session on deep
learning my name is Mohan Han in this
video we are going to talk about what
deep learning is all about some of you
may be already familiar with the image
recognition
how does image recognition work you can
train a application or your machine to
recognize whether a given image is a cat
or a dog and this is how it works at a
very high level it uses artificial
neural network it is trained with some
known images and during the training it
is told if it is recognized incorrectly
or not and then when new images are
submitted it recognizes correctly based
on the accuracy of course so quick
understanding about artificial neural
networks so this is the way a cat does
is you provide a lot of training data
also known as labeled data for example
in this case these are the images of
dogs and the network extracts some
features that makes a dog a dog right so
that is known as feature extraction and
based on that when you submit a new
image of dog the basic features remain
pretty much the same it may be a
completely different image but the
features of a dog still remain pretty
much the same in various different
images let's say compared to a cat and
that's the way artificial neural network
works we'll go into details of this very
shortly and once the training is done
with training data we then test it with
some test data - which is basically
completely new data within which the
system has not seen before
unlike the training data and then we
find out whether it is predicting
correctly or not
thereby we know whether the training is
complete or it needs more training so
that's not a very high level artificial
neural network works so this is what we
are going to talk about today or agender
looks something like this what is deep
learning why do we need deep learning
and then what are the applications of
deep learning one of the main components
the secret sauce in deep learning is
neural networks so we want to talk about
what is neural network and how it works
and some of its component
like for example activation function the
gradient descent and so on and so forth
so that as a part of working off our
neural network we will go into little
bit more details how this whole thing
works so without much further ado let's
get started so deep learning is
considered to be a part of machine
learning so this diagram very nicely
depicts what deep learning is at a very
high level you have the all-encompassing
artificial intelligence which is more a
concept rather than a technology or a
technical concept right so it is it's
more of a concept at a very high level
artificial intelligence under the hood
is actually machine learning and deep
learning and machine learning is a
broader concept you can say our product
technology and deep learning is a subset
of machine learning the primary
difference between machine learning and
deep learning is that deep learning uses
neural networks and it is suitable for
handling large amounts of unstructured
data and last but not least one of the
major differences between machine
learning and deep learning is that in
machine learning the feature extraction
of the feature engineering is done by
the data scientists manually but in deep
learning cells we use neural networks
the feature engineering happens
automatically so that's a little bit of
a quick difference between machine
learning and deep learning and this
diagram very nicely depicts the relation
between artificial intelligence machine
learning and deep learning now why do we
need deep learning machine learning was
there for quite some time
and it can do a lot of stuff that
probably what deep learning can do but
it's not very good at handling large
amounts of unstructured data like images
boys or even text for that matter so
traditional machine learning is not that
very good at doing this it traditional
machine learning can handle large
amounts of structured data but when it
comes to unstructured data it's a big
challenge so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition
we need to process analyze images and
why subsidies and deep learning is
required compared to let's say
traditional machine learning it can also
perform complex algorithms more complex
than let's say what machine learning can
do and it can achieve best performance
with the large amounts of data so the
more you have the data let's say
reference data or label data the better
the system will do because the training
process will be that much better last
but not least with deep learning you can
really avoid the manual process of
feature extraction those are some of the
reasons why we need deep learning some
of the applications of deep learning
deep learning has made major inroads and
it is a major area in which deep
learning is applied is healthcare and
within healthcare particularly oncology
which is basically cancer related stuff
one of the issues with cancer is that a
lot of cancers today are curable they
can be cured they are detected early on
and the challenge with that is when a
Diagnostics is performed let's say an
image has been taken of a patient to
detect whether there is cancer or not
you need a specialist to look at the
image and determine whether it is the
patient is fine or there is any all set
of cancer and the number of specialists
are limited so if we use deep learning
if we use automation here or if we use
artificial intelligence here then the
system can with a certain amount of the
good amount of accuracy determine
whether a particular patient is having
cancer or not so the prediction or the
detection process of all disease like
cancer can be expedited the detection
process can be expedited can be faster
without really waiting for a specialist
we can obviously then once the
application was the artificial
intelligence detects or predicts that
there is an onset of a cancer this can
be cross checked by a doctor but at
least the initial screening process can
be automated and that is where the
current focus is with respect to deep
learning inhale
what else robotics is another area deep
learning is majorly used in robotics and
you must have seen nowadays robots are
everywhere humanoids the industrial
robots which are used for manufacturing
process you must have heard about Sophia
who got a citizenship with Saudi Arabia
and so on there are multiple such robots
which are knowledge oriented but there
are also industrial robots that are used
in industries in the manufacturing
process and increasingly in security and
also in defense for example image
processing video is fed to them and they
need to be able to detect objects
obstacles and so on and so forth so
that's where deep learning is use they
need to be able to hear and make sense
of the sounds that they are hearing that
needs deep learning as well so robotics
is a major area where deep learning is
applied then we have self-driving cars
or autonomous cars you must have heard
of Google's autonomous car which has
been tested for millions of miles and
pretty much incident-free they wear of
course a couple of incidents here and
there but it is considered to be fairly
safe and they are today a lot of
automotive companies in fact pretty much
every automotive company worth its name
is investing in self-driving cars or
autonomous cars and it is predicted that
in the next probably 10 to 15 years
these will be in production and they
will be used extensively into your life
right now they are all in R&amp;amp;D and in
test phases but pretty soon these will
be on the road so this is another area
where deep learning is used and how is
it used where is it used within
autonomous driving the car actually is
fed with video of surroundings and it is
supposed to process that information
process that video and determine if
there are any obstacles it has to
determine if there are any cars in the
site will detect whether it is driving
in the lane also it has to determine
whether the signal is green or red so
that accordingly it can move forward or
wait so for all these video analysis
deep learning is used in addition to
that the training overall training to
drive the car happens
in deep learning environment so okay a
lot of scope here to use deep learning
couple of other applications our mission
translations today we have a lot of
information and very often this
information is in one particular
language and more specifically in
English and people need information in
various parts of the world it is pretty
difficult for human beings to translate
each and every piece of information or
every document into all possible
languages there are probably at least
hundreds of languages or if not more to
translate each and every document into
every language is pretty difficult
therefore we can use deep learning to do
pretty much like a real-time translation
mechanism so we don't have to translate
everything and keep it ready but we
train the applications or artificial
intelligence systems that will do the
translation on the fly
for example you go to somewhere like
China and you want to know what is
written on a signboard that is
impossible for somebody to translate
that and put it on the web or something
like that so you have an application
which is trained to translate stuff on
the fly so you probably this can be
running on your mobile phone on your
smartphone you scan this the application
will instantly translate that from
Chinese to English that is one then
there could be well applications where
there may be a research document which
is all and maybe Chinese or Japanese and
you want to translate that to study that
document or in that case you need to
translate
so therefore deep learning is used in
such situations as well and that is
again on demand so it is not like you
have to translate all these documents
from other languages into English in one
shot and keep it somewhere that is again
and pretty much an impossible task but
on a need basis so you have systems that
are trained to translate on the fly so
machine translation is another major
area of a deep learning is used then
there are a few other upcoming areas
where synthesising
is done by neural nets for example music
composition and generation of music so
you can train a neural
to produce music even to compose music
so this is a fun thing this is still
upcoming it needs a lot of effort to
train such new love that it has been
proved that it is possible so this is a
relatively new area and on the same
lines colorization of images so these
two images on the left hand side is a
grayscale image or a black-and-white
image this was colored by a neural net
or a deep learning application as you
can see this is done a very good job of
applying the colors and obviously this
was trained to do this colorization but
yes this is one more application of deep
learning know one of the major secret
sauce of deep learning is neural network
deep learning works on neural network
art consists of neural network so let us
see what is neural network neural
network or artificial neural network is
designed or based on the human brain now
human brain consists of billions of
small cells that are known as neurons
artificial neural networks is in a way
trying to simulate
the human brain so this is a quick
diagram of biological neuron a
biological neuron consists of the major
part which is the cell nucleus and then
it has some tentacles kind of stuff on
the top called dendrite and then there
is like a long tail which is known as
the axon further again at the end of
this axon are what are known as synapses
these in turn are connected to the
dendrites of the next neuron and all
these neurons are interconnected with
each other therefore they are like
billions of them sitting in our brain
and they're all active they're working
they based on the signals they receive
signals as inputs from other neurons or
maybe from other parts of the body and
based on certain criteria they send
signals to the neurons at the other end
so they they get either activated or
they don't get activated based on so it
is like a binary gates so they get
activated or not activated based on the
inputs that they receive and so on so we
will see a little bit of those details
as we move forward in our
artificial neuron but this is a
biological neuron does the structure of
a biological neuron and artificial
neural network is based on the human
brain
the smallest component of artificial
neural network is an artificial neuron
as shown here sometimes is also referred
to as perceptron now this is a very high
level diagram the artificial neuron has
a small central unit which will receive
the input if it is doing let's say image
processing the inputs could be pixel
values of the image which is represented
here as X 1 X 2 and so on each of the
inputs are multiplied by what is known
as weights which are represented as W 1
W 2 and so on there is in the central
unit basically there is a summation of
these weighted inputs which is like X 1
into W 1 plus X 2 into W 2 and so on the
products are then added and then there
is a bias that is added to that in the
next slide we will see that passes
through an activation function and the
output comes as Y which is the output
and based on certain criteria the cell
gets either activated or not activated
so this output would be like a 0 or a 1
binary format okay so we will see that a
little bit more detail but let's do a
quick comparison between biological and
artificial neuron just like a biological
neuron there are dendrites and then
there is a cell nucleus and synapse and
and an axon we have in the artificial
neuron as well these inputs come like
the dendrite if you will act like the
dendrites there is a like a central unit
which performs the summation of these
weighted inputs which is basically w1 x1
w2 x2 and so on and then a bias is added
here and then that passes through what
is known as an activation function ok so
these are known as the weights w1 w2 and
then there is a bias which will come out
here and that is added the biases by the
way common for a particular neuron so
there won't be like B 1 B 2 B 3 and so
on only weights will be one per input
the bias is common for the entire neuron
it is also common for all the way
you of the bias remains the same for all
the neurons and a particular layer we
will also see this as we move forward
and we see deep neural network where
there are multiple neurons so that's the
output now the whole exercise of
training the neuron is about changing
these weights and biases as I mentioned
artificial neural network will consist
of several such neurons and as a part of
the training process these weights keep
changing initially they are assigned
some random values through the training
process the weights the whole process of
training is to come up with the optimum
values of W 1 W 2 and W n and then the B
4 or the bias for this particular neuron
such that it gives an accurate output as
required so let's see what exactly that
means so the training process this is
all it happens it takes the inputs each
input is multiplied by a weight and
these waves during training keep
changing so initially they're assigned
some random values and based on the
output whether it is correct or wrong
there is a feedback coming back and that
will basically change these weights
until it starts giving the right output
that is represented in here as Sigma I
going from 1 to N if there are n inputs
WI into X I so this is the plot of W 1 X
1 W 2 X 2 and so on right and there is
bias that gets added here and that
entire thing goes to what is known as an
activation function so essentially this
is Sigma of WI X I plus our value of
bias which is a B so that entire thing
goes as an input to an activation
function now this activation function
takes this as an input gives the output
as a binary output it could be a 0 or a
1 there are of course to start with
let's assume it's a binary output later
we will see that there are different
types of activation functions so it need
not always be binary output but to start
with let's keep simple so it decides
whether the neuron should be fired or
not so that is the output like a binary
output 0 1 1 all right so again let me
summarize this so
takes the inputs so if you are
processing an image for example the
inputs are the pixel values of the image
X 1 X 2 up to X M there could be
hundreds of these so all of those are
fed as so these are some values and
these pixel values again can be from 0
to 256 each of those pixel values are
then multiplied with what is known as a
weight this is a numeric value can be
any value so this is a number W 1
similarly W 2 is a number so initially
some random values will be assigned and
each of these weights are multiplied
with the input value and their sum this
is known as the weighted sum so that is
performed in this kind of the central
unit and then a bias is added remember
the bias is common for each neuron so
this is not the bias value is not one
bias value for per input so just keep
that in mind the bias value there is one
bias per neuron so it is like this
summation plus bias is the output from
this section this is not our complete
output of the neuron but this is a bias
for output for step one that goes as an
input to what is known as activation
function and that activation function
results in an output usually a binary
output like a 0 or a 1 which is known as
the firing of the neuron ok good so we
talked about activation function so what
is an activation function an activation
function basically takes the weighted
sum which is we saw W 1 X 1 W 2 X 2 the
sum of all that plus the bias so it
takes that as an input and it generates
a certain output now there are different
types of activation functions and the
output is different for different types
of activation functions moreover why is
an activation function required it is
basically required to bring in
non-linearity that's the main reason why
an activation function is required so
what are the different types of
activation functions there are several
types of activation functions but these
are the most common ones these are the
ones that are currently in use sigmoid
function was one of the early activation
functions but today the Ray Lu has kind
of taken over so Ray Lu is by far the
most popular activation function that is
you
today but still sigmoid function is
still used in many situations these
different types of activation functions
are used in different situations based
on the kind of problem we are trying to
solve so what exactly is the difference
between these two sigmoid gives the
values of the output will be between 0 &amp;amp;
1 threshold function is the value will
be 0 up to a certain value and beyond
that this is also known as a step
function and beyond that it will be what
in case of Sigma there is a gradual
increase but in case of threshold it's
like also known as a step function there
is a rapid or instantaneous change from
0 to 1 whereas in sigmoid we will see in
the next slide there is a gradual
increase but the value in this case is
between 0 &amp;amp; 1 as well no riilu function
on the other hand it is equal to
basically if the input is 0 or less than
0 then the output is 0 whereas if the
input is greater than 0 then the output
is equal to the input I know it's a
little confusing but in the next slides
where we show the rayleigh function it
will become clear similarly hyperbolic
tangent this is similar to sigmoid in
terms of the shape of the function
however while Sigma it goes from 0 to 1
hyperbolic tangent goes from minus 1 to
1 and here again the increase or the
change from minus 1 to 1 is gradual and
not like threshold or step function
where it happens instantaneously so
let's take a little detail look at some
of these functions so let's start with
the sigmoid function so this is the
equation of a sigmoid function which is
1 by 1 plus e to the power of minus X so
X is the value that is the input it goes
from 0 to minus 1 so this is sigmoid
function the equation as Phi X is equal
to 1 by 1 plus e to the power of minus X
and as you can see here this is the
input on the x-axis as X is where the
value is coming from in fact it can also
go negative this is negative actually so
this is the 0 so this is the negative
value of X so as X is coming from
negative value towards 0 the value of
the output slowly as
is approaching zero it it slowly and
very gently increases and actually at
the point let me just use a pen at the
point here it is it is 0.5 it is
actually 0.5 okay and slowly gradually
it increases to 1 as the value of x
increases but then as the value of latex
increases it tapers off it doesn't go
beyond 1 so that is the speciality of
sigmoid function so the output value
will remain between 0 and 1 it will
never go below 0 or above 1 okay that
so that is sigmoid function now this is
threshold function or this is also
referred to as a step function and here
we can also set the threshold in this
case it is that's why it's called the
threshold function normally it is zero
but you can also set a different value
for the threshold now the difference
between this and the sigmoid is that
here the changes rapid or instantaneous
as the x value comes from negative up to
zero it remains 0 and at 0 it pretty
much immediately increases to 1 okay so
this is a mathematical representation of
threshold function Phi X is equal to 1
if X is greater than equal to 0 and 0 of
X is less than 0 so for all negative
values it is 0 which since we have set
the threshold to be 0 so as soon as it
reaches 0 it becomes 1 you see the
difference between this and the previous
one which is basically at the sigmoid
where the increase from 0 to 1 is
gradual and here it is instantaneous and
that's why this is also known as a step
function threshold function or step
function this is a real value is one of
the most popular activation functions
today this is the definition of relu Phi
X is equal to max of X comma 0 what it
says is if the value of x is less than 0
then Phi X is 0 the moment it increases
goes beyond 0 the value of Phi X is
equal to X so it doesn't stop at 1
actually it goes all the way so as the
value of x increases the value of Y will
also increase infinitely so there is no
limit here unlike
your sigmoid or threshold or the next
one which is basically hyperbolic
tangent okay so in case of rail you
remember there is no upper limit the
output is equal to either 0 in case the
value of X is negative or it is equal to
the value of X so for example here if
the value of x is 10 then the value of y
is also 10 right ok so that is real and
there are several advantages of raloo
and it is much more efficient and
provides much more accuracy compared to
other activation functions like sigmoid
and so on so that's the reason it is
very popular all right so this is
hyperbolic tangent activation function
the function looks similar to sigmoid
function the curve if you see the shape
it looks similar to sigmoid function but
the difference between hyperbolic
tangent and sigmoid function is that in
case of sigmoid the output goes from 0
to 1 whereas in case of hyperbolic
tangent it goes from minus 1 to 1 so
that is the difference between
hyperbolic tangent and sigmoid function
otherwise the shape looks very similar
there is a gradual increase unlike the
step function where there was an instant
increase or instant change here again
very similar to sigmoid function the
value changes gradually from minus 1 to
1 so this is the equation of hyperbolic
tangent activation function yeah so then
let's move on this is a diagrammatic
representation of the activation
function and how the overall data or how
the overall progression happens from
input to the output so you get their
input from the input layer by the way
the numeral network has three layers
typically there will be three layers
there is an input layer there is an
output layer and then you have the
hidden layer so the inputs come from the
input layer and they get processed in
the hidden layer and then you get the
output in the output layer so let's take
a little bit of a detailed look into the
working of a neural network so let's see
we want to classify some images between
dogs and cats how do we do this this is
known as a classification process
and we are trying to use neural networks
and deep learning to implement this
classification so how do we do that so
this is how it works so you have four
layer neural network there is an input
layer there is an output layer and then
there are two new hidden layers and what
we do is we provide labeled training
data which means these images are fed to
the network with the label saying that
okay this is a cat the neural network is
allowed to process it and come up with
the prediction saying whether it is a
cat or a dog and obviously in the
beginning there may be mistakes a cat
may be classified as a dog
so we then say that okay this is wrong
this output is wrong but every time it
predicts correctly we say yes this
output is correct so that learning
process it will go back make some
changes to its weights and biases we
again feed these inputs and it will give
us the output we will check whether it
is correct or not and so on so this is a
hydrating process which is known as the
training process so we are training the
neural network and what happens in the
training process these weights and
biases you remember they were weights
like w1 w2 and so on so these weights
and biases keep changing every time you
feed these which is known as an epoch so
there are multiple iterations every
iteration is known as an epoch and each
time the weights are dated to make sure
that the maximum number of images are
classified correctly so once again what
is the input this input could be like
thousand images of cats and dogs and
they are labeled because we know which
is a cat and which is a dog and we feed
those thousand images the neural network
will initially assign some weights and
biases for each neuron and it will try
to process extract the features from the
images and it will try to come up with a
prediction for each image and that
prediction that is calculated by the
network is compared with the actual
value whether it is a cat or a dog and
that's how the error is calculated so
let's say there are thousand images and
in the first run only 500 of
have been correctly classified that
means we are getting only 50% accuracy
so we feed that information back to the
network further update these weights and
biases for each of the neurons and we
run this these inputs once again it will
try to calculate extract of features and
it will try to predict which of these is
cats and dogs and this time let's say
out of thousand seven hundred of them
have been predicted correctly so that
means in the second iteration the
accuracy has increased from 50% to 70%
all right then we go back again we feed
this maybe for a third iteration fourth
iteration and so on and slowly and
steadily the accuracy of this network
will keep increasing and it may reach
probably you know 90% 95% and there are
several parameters that you are known as
hyper parameters that need to be changed
and tweaked and that is the overall
training process and ultimately at some
point we say ok you will probably never
reach hundred percent accuracy but then
we set a limit saying that ok we
received 95% accuracy that is good
enough for our application and then we
say ok our training process is done so
that is the way training happens and
once the training is done now with the
training data set the system has let's
say seen all these thousand images
therefore what we do is the next step
like in any normal machine learning
process we do the testing where we take
a fresh set of images and we feed it to
the network the fresh set which it has
not seen before as a part of the
training process and this is again
nothing new in deep learning this was
there in machine learning as well so you
feed the test images and then find out
whether we are getting similar accuracy
or not so maybe that accuracy may reduce
a little bit while training you may get
98% and then for tests you may get 95%
but there shouldn't be a drastic drop
like for example you get 98% in training
and then you get 50% or 40% with the
test that means your network has not
learnt you may have to retrain your
network so that is the way neural
network training works and remember the
whole process is about changing these
weights and biases and
coming up with the optimal values of
these weights and biases so that the
accuracy is the maximum possible all
right so a little bit more detail about
how this whole thing works so this is
known as forward propagation which is
the data or the information is going in
the forward direction the inputs are
taken weighted summation is done bias is
added here and then that is fed to the
activation function and then that is
that comes out as an output so that is
forward propagation and the output is
compared with the actual value and that
will give us the error the difference
between them is the error and in
technical terms that is also known as
our cost function and this is what we
would like to minimize there are
different ways of defining the cost
function but one of the simplest ways is
mean square error so it is nothing but
the square of the difference of the
errors or the sum of the squares of the
difference of the errors and this is
also nothing new we have probably if
you're familiar with machine learning
you must have come across this mean
square error now there are different
ways of defining cost function it need
not always be the mean square error but
the most common one is this so you
define this cost function and you pass
in the system to minimize this error so
we use what is known as an optimization
function to minimize this error and the
error itself sent back to the system as
feedback and that is known as back
propagation and so this is the cost
function and how do we optimize the cost
function we use what is known as
gradient descent so the gradient descent
mechanism identifies how to change the
weights and biases so that the cost
function is minimized and there is also
what is known as the rate of the
learning rate that is what is shown here
as slower and faster so you need to
specify what should be the learning rate
now if the learning rate is very small
then it will probably take very long to
train whereas if the learning rate is
very high there
it will appear to be faster but then it
will probably never what is known as
converge
now what is convergence now that we are
talking about a few terms here
convergence is like this this is a
representation of convergence so the
whole idea of gradient descent is to
optimize the cost function or minimize
the cost function in order to do that we
need to represent the cost function as
this curve we need to come to this
minimum value that is what is known as
the minimization of the cost function
now what happens if we have the learning
rate very small is that it will take
very long to come to this point on the
other hand if you have large higher
learning rate what will happen is
instead of stopping here it will
crossover because the learning rate is
high and then it has to come back so it
will result in what is known as like an
oscillation so it will never come to
this point which is known as convergence
instant it will go back and forth so
these are known as hyper parameters the
learning rate and so on and these have
to be those numbers or those values we
can determine typically using trial and
error out of experience we try to find
out these values so that is the gradient
descent mechanism to optimize the cost
function and that is what is used to
train our neural network this is another
representation of how the training
process works and here in this example
we are trying to classify these images
whether they are cats or dogs and as you
can see actually each image is fed in
each time one image is fed rather and
these values of x1 x2 up to xn are the
pixel values within this image ok so
those values are then taken and for each
of those values of weight is multiplied
and then it goes to the next layer and
then to the next layer and so on
ultimately it comes as the output layer
and it gives an output as whether it is
a dog or a cat remember the output will
never be unnamed output so these would
be like a 0 or a 1 and
say ok 0 corresponds to dogs and one
corresponds to cats so that is the way
it typically happens this is a binary
classification we have similar
situations where there can be multiple
classes which means that there will be
multiple more neurons in the output
layer ok so this is once again a quick
representation of how the forward
propagation and the backward propagation
works so the information is going in
this direction which is basically the
forward propagation and at the output
level we find out what is the cost
function the difference is basically
sent back as part of the backward
propagation and gradient descent then
adjust the weights and biases for the
next iteration this happens I'd rated Li
till the cost function is minimized and
that is when we say the whole the
network has converged on the training
processes converged and there can be
situations where convergence may not
happen in rare cases but by and large
the network will converge and after
maybe a few iterations it could be tens
of iterations or hundreds of iterations
depending on what exactly the number of
iterations can vary and then we say ok
we are getting a certain accuracy and we
say that is our threshold maybe 90%
accuracy we stop at that and we say that
the system is strain the trained model
is then deployed for production and so
on so that is the way the neural network
training happens okay so that is the way
classification works in deep learning
using neural network and this slide is
an animation of this whole process as
you can see the forward propagation the
data is going forward from the input
layer to the output layer and there is
an output and error is calculated the
cost function is calculated and that is
fed back as a part of backward
propagation and that whole process
repeats
once again ok so remember in neural
networks the training process is nothing
but the finding the best values of the
weights and biases for each and every
neuron in the
that's all training of neural network
consists of finding the optimal values
of weights and biases so that the
accuracy is maximum all right so with
that we come to the end of the session
we all have a great day thank you very
much
hi there if you like this video
subscribe to the simple learn YouTube
channel and click here to watch similar
videos turner up and get certified click
here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>