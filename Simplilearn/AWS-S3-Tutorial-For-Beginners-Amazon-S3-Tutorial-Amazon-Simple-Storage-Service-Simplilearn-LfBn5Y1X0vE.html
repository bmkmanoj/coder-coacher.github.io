<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AWS S3 Tutorial For Beginners | Amazon S3 Tutorial | Amazon Simple Storage Service | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="AWS S3 Tutorial For Beginners | Amazon S3 Tutorial | Amazon Simple Storage Service | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AWS S3 Tutorial For Beginners | Amazon S3 Tutorial | Amazon Simple Storage Service | Simplilearn</b></h2><h5 class="post__date">2017-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LfBn5Y1X0vE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">one of the biggest challenges during my
IT career has been managing the storage
requirements for the applications I
supported storage space was always at a
premium which meant maintaining
sufficient quantities of backups was a
logistical nightmare this lesson shows
you how Amazon s3 changes everything it
offers a variety of practically
unlimited and cost-effective storage
that is available on demand with
unprecedented levels of durability and
availability not only that Amazon s3
also provides a way to easily host
static websites and distribute your
content around the world to your end
users with low latency so what is Amazon
simple storage service Amazon simple
storage service or s3 for short provides
developers and IT teams with secure
durable and highly scalable cloud
storage basically its file storage in
the cloud
so let's look at some of the features of
Amazon s3 the first of which is its
durability it's extremely durable and it
provides 1190 ability your data is
redundantly stored across multiple
facilities and multiple devices within
each facility we touched on 11 9s
durability in an earlier lesson but
basically it means you're going to lose
one file every 10 million years with an
11 90 durability
it's also highly available Amazon s3 is
designed for 99.99% availability you can
also choose the AWS region in which to
store your data so you can optimize
latency minimize your storage costs and
also address regulatory compliance so if
you have sovereign data that needs to be
in a specific country you can choose an
Amazon region which satisfies that need
it's also very cost-efficient you can
store huge amounts of data at a very low
cost and you only pay for what you use
and you're charged by gigabyte per month
usage there's a variety of different
storage classes available meaning you
can categorize your data and pay only
what you need to
very secure Amazon s3 supports SSL data
transfer and data encryption once it's
being uploaded you can also control
access to your data using I am and also
specify object permissions using the s3
policies
Amazon s3 is also highly scalable it
allows you to store as much or as little
data as you want the storage is elastic
so you can scale up and down as required
and only pay for what you're using
you can also configure notifications to
be sent when objects are loaded into
Amazon s3 by SQS SMS or enum lambda this
way you can set up workflows for your
files
Amazon s3 is also highly performant you
can use multi-part uploads to maximize
network throughput and resilience Amazon
s3 transfer acceleration is a new
service that allows you to make use of
edge locations to increase upload and
download times
Amazon s3 is also fully integrated with
many AWS products such as cloud run
cloud watch RDS EBS lambda etc it's also
really easy to use and has multiple
connectivity options you can use the web
console the Amazon Web Services command
line interface as a mobile application
or you can use api's or the software
developer kit
first take a look at some of the use
cases of Amazon s3 the first of which is
backup and archiving Amazon s3 is ideal
for this purpose you can store a
practically unlimited amount of data for
when you need it traditional IT
infrastructure would offer a finite
storage capacity she'd have to manage
what backups and archives you're going
to retain but with s3 that problem
disappears and you can retain as many
backups as you want for a low-cost
Amazon s3 is object-based storage this
accessible via a web interface so you
can store and retrieve data from
anywhere on the web so your users all
around the world can have access to your
files easily
s3 is great for content storage and
distribution you can offload your entire
storage infrastructure into the cloud to
minimize your costs and you can also
distribute your content directly from s3
to end-users or use s3 as a source for
delivering content to Amazon Cloud from
edge locations so you can provide fast
access to your information
it's also great for big data Amazon s3
is designed to be used as a big data
object store for things like photos
videos financial data etc using AWS
products and services you can also
perform big data analytics
you can also use s3 the host static
websites it allows you to host your
entire static website for a low cost and
it makes it a highly available hosting
solution
key feature of Amazon s3 is for disaster
recovery
it offers a robust disaster recovery
solution all data stored on s3 is
automatically replicated to a different
availability zone and you also have the
option to copy it to other regions using
cross region replication you can also
add further recovery options by storing
multiple versions of an object for point
in time recovery all Amazon s3 data is
stored in something called buckets it's
basically a folder that you can
read/write and delete objects from you
can store as many objects as you want in
a bucket for objects are limited in size
to 5 terabytes and the largest put
operation is 5 gigabytes so if you're
uploading very very large files you need
to break them into smaller chunks
buckets allow you to create granular
security so you can control who has
create delete and retrieve Commission's
to each of your buckets and the files
inside them you can also control who has
access to bucket logs which are used to
store information about access to your
files and objects and you can also
choose the region where a bucket is
stored so how do you create a bucket
well the easiest way to create a bucket
is using the web console or the
command-line interface the bucket name
you choose must be unique across all
bucket names in Amazon s3 Amazon
recommends that one way to help ensure
uniqueness is to prefix your bucket
names with the name of your organization
bucket names must be at least 3
characters and no more than 63
characters long bucket names must be a
series of one or more labels and Amazon
recommends separating labels with a
single period bucket names can contain
lowercase letters numbers and hyphens
and each label must start and end with a
lowercase letter or a number and here
are some examples of valid bucket names
for example my AWS bucket or my period
AWS period bucket or my AWS bucket
period 1 there are some bucket
restrictions by default you can create
up to 100 buckets in each of your AWS
accounts if you need to increase this
you have to contact AWS support
bucket named ownership is not
transferable however as a bucket is
empty you can delete it and the name
will eventually become available for use
again there is no limit to the number of
objects that can be stored in a bucket
and there's no difference in performance
whether you use many buckets or just
have a couple and you also cannot create
a bucket within another bucket Amazon s3
comes in a range of storage classes for
different data categories these are
Amazon s3 standard Amazon s3 standard
infrequent access Amazon s3 reduced
redundancy storage and Amazon Glacia so
let's start by taking a look at Amazon
s3 standard standard is designed for
high availability and durability and
it's used to store frequently accessed
data it's designed for 11/9 steerability
and 99.99% availability and it's perfect
for low latency and high throughput
environments Amazon s3 standard is
typically used to things like dynamic
websites for cloud applications mobile
applications or even just regular file
storage it's great for your business to
use or end users to upload photos and
videos or just be able to access your
files and objects in real time next is
Amazon s3 standard infrequent access and
this is designed for objects that are
accessed less frequently but when they
are required you need them rapidly
standard infrequent access offers the
same durability throughput and low
latency of standards and it offers a
lower cost per gigabyte but it does have
a per gigabyte retrieval fee standard
infrequent access is typically used for
data that you don't need very often but
when you do need it you need very
quickly examples of this would be last
month's reporting data or database
backups that were taken earlier this
month reduced redundancy storage is a
storage option that enables you to
reduce your costs by storing
non-critical reproducible data at lower
levels of redundancy than Amazon's
existing s3 standard storage so it's
designed the non-critical objects or
reproducible objects
that you can cope with a lower
durability and lower availability reduce
redundancy storage provides a cost
effective solution for distributing or
sharing data that's been durably stored
elsewhere or is great for storing
thumbnails transcoded media or other
process data that can be easily
reproduced so if you have a website
application that people upload photos to
and then you recreate a thumbnail and
store that you can recreate that
whenever you want so this class of
storage is perfect for storing things
like the thumbnails the final storage
class is Amazon Glacia and this is used
to archive your data that you rarely
access and you can cope with a retrieval
time of several hours it still offers a
durability of eleven nines and it has
the lowest cost available in Amazon
storage it also has a secure vault lock
feature so you can really secure your
old archives and keep them for when you
need them typical use cases would be to
store database backups from months or
even years ago or compliance data that
you think auditors might need several
years down the line I know Amazon Glacia
would have been great in some of my
previous employments where we had
limited on-site storage capacity and we
could only retain a certain amount of
database backups so when someone came
along and said hey we urgently need a
database backup from five years ago
there's absolutely no way I could have
provided it to them but the Glacia
that's definitely an option this table
just has a quick overview of the
different storage classes and the
differences between them now things that
you may see coming up in the exam are
the durability and the availabilities so
it's worth remembering the percentages
for each of the classes also here it
mentions the latency times as well so
standard standard ia and R RS have
millisecond response times whereas
Glacia has four to five hours response
times you can easily host static
websites in s3 buckets to do this you
need to configure your bucket of static
website hosting then you upload your
website code into your bucket it's run
accessible from the following URL so the
naming convention is bucket name dot s3
- website - then the name of the region
your bucket
in Amazon AWS calm now it's worth
remembrance because it may well come up
in the exam so just remember the
convention but I'll simply learn bucket
that sits in the US East one region
would have our URL of simpler Ness three
- website - us - East - one which is the
region dot Amazon AWS com
now you can also use your own domains to
point your s3 buckets and we'll look at
that in the route 53 lesson you can also
provide URL access to the objects in
your buckets so if you donate blood
website hosting this first URL would
provide you with the photo jpg object so
it's effectively the same web address we
looked at on the previous slide with
full slash photo jpg added but you can
also provide URL access without enabling
static website hosting as long as you
set up the permissions appropriately so
this URL here request the health check
HTML objects that we stored in our
simple urn bucket and you can see here
it's HTTPS s3 or Amazon AWS comm forward
slash then the name of our bucket than
the name of the file
welcome to the Amazon s3 bucket
demonstration where we're going to
create a new bucket and take a look at
some of the features available
so we'll start by going to the storage
and content delivery section and
clicking on s3 which will take us to the
s3 dashboard and we can see I only have
one bucket in my account which is called
database table so we're going to click
on create bucket and create a new one so
just to show you if I try and create a
bucket with just two characters in its
name it's going to give me an error and
says it needs to be at least three
characters long on the other end of the
scale if I try and make it ridiculously
long and click on create it's going to
say it must be shorter than 63
characters so that's just a quick
reminder and also you can't create a
bucket with underscores in so if I just
try and call that simply learn
underscore bucket it's going to generate
an errors telling me that I can't create
it with an underscore and also if I just
create a bucket called simply then it's
going to say that this bucket name is
not available someone's already using
that in the Amazon Web Services
namespace so I have to give it a unique
name so I'm going to call this simply
learn dot demo test and I'm going to
place it in the US standard region so
let me click on create so yeah there's
our new bucket and here are the
properties to do with that bucket so we
can set the permissions at the moment me
as in the database table owner has
permission to do anything with this
bucket and we'll take a look at that in
a minute and there's the option here to
static website hosting which we
mentioned and I'm going to show you
something with that shortly
then there's the logging so you can
enable logging for your bucket so you
turn it on you could select a bucket in
which to put your log files we were not
going to do that now events so we could
set up an event and it could be for a
put operation and we could prefix it
with say which folder it was going to be
in or the suffix so we could say any put
operation for a JPEG file setup and SNS
topic or an sq
q or even a lambda function then we're
going to look at the rest of the things
here at a later demonstration in this
lesson so let's take a look at putting
some data into our bucket so let's click
on simple n demo test telling me it's
empty so there's upload some files and
it brings up the upload API so we're
going to click on add files now I've got
four files here on my desktop I've got
the health check HTML file but you're
familiar way from previous lessons
there's the Cynthia learn key pair which
we use to access Windows ec2 instances
as our simply learned private key which
we use to access Linux and UNIX
instances and then there's a simply
learn logo so it's upload all of those
let's click on open so now what we can
do is click on set details and we can
choose where we want to put them so we
could say the standard storage or
standard infrequent or reduced
redundancy so I'm gonna stick with
standard and we could also say where we
want to encrypt them so we could use the
Amazon s3 master key or we could use the
key management service now we don't need
to encrypt these so let's disable that
but that's where you have that option
and then we click on set permissions and
so it's granting me full control so I
can do anything I want with these files
and I can also make everything public
I'm not going to do that yet we can set
the metadata where you can specify
common HTTP headers and content type and
things like that or you can just tick
this box and let it figure out content
types automatically which we're going to
do then we click on start upload and on
the right-hand side here we can see our
files being uploaded and they are that
was pretty quick so let's take a look at
one of them let's take a look at the
simply learn logo so I click on that and
then move to the top right and click on
properties so here we are so now it
tells you some information it tells you
that this file lives in the simply learn
demo test bucket there's the file name
hada sighs it's date who uploaded it etc
and then here there's a link which is
interesting as we talked about this is
the URL to access this file directly so
what happens if we click on this tell me
there's an error okay now why is that it
says access denied if you take a look
here and that's because it hasn't got
public permissions so we need to change
that so let's go back to our file and if
we click on the permissions option here
you can see it's just me that has access
so let's add more permissions and let's
grant everyone open and view permissions
and click on save now if we go back to
the other window and let's refresh they
are so we now see that we can access
that file directly using the URL and
there's the URL there so s3 dot Amazon
AWS comm forward slash simply learned
our demo test which is the name of our
bucket and then the name of the file now
if we go back to our bucket let's click
on simple n demo test and the properties
now here is the static website hosting
which we talked about earlier and the
endpoint for it would be this simply
learned demo test dot s3 - website - US
East one which is the region we're using
got Amazon AWS com
so let's enable web site hosting I was
asking for an index document so this is
the file the default file that will open
when you go to the website so let's call
it health check dot HTML which is the
file we've been using in our other tests
and we can set that as the error
document as well just just because it's
the only file we have that's suitable
for this okay and I click on save so now
if I click on the endpoint it gives us
an error
the 403 error and it's for the same
reason as we had when we try to access
to simply learn logo it's because we
haven't set permissions on the health
check file so let's go back and do that
so we click on the health check file we
click on permissions we add permissions
and let's grant everyone open and view
permissions and click Save ok so now if
we go back to the other window and let's
reload that there we are
so now if we hit simply learn demo test
s3 - website - US East $1 Merson - AWS
calm we get to see our health check file
which is this is a healthy web server
and that concludes the bucket
demonstration hey once become an expert
in cloud computing then subscribe the
simpler Channel and click here to watch
more such videos
- nerd up and get certified in cloud
computing click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>