<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DevOps Tutorial For Beginners - 2 | Docker Tutorial For Beginners | DevOps Tutorial | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="DevOps Tutorial For Beginners - 2 | Docker Tutorial For Beginners | DevOps Tutorial | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DevOps Tutorial For Beginners - 2 | Docker Tutorial For Beginners | DevOps Tutorial | Simplilearn</b></h2><h5 class="post__date">2017-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wRBdx-9wJoU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">lesson 2 which is about virtualization
and lightweight containers and I think
this is going to be one of the key
technologies that are going to be used
from now on list we're already using
them in fact aren't we so look at what
the principles are of virtualization and
then some changes which were made to lie
next to support teaching an interesting
way and then start looking at container
technology and docker and I think docker
is something that's going to really take
off and so what we'll do then you spend
the rest of the session looking at
various aspects of docker and getting it
ready so virtualization is very
important because it's all certain thing
issues so we've got the traditional
architecture where you have a computer
which with an operating system and
probably one or more applications
running on the operating system so with
the virtual organization we have the
hardware then we have a piece of
software which is actually usually an
operating system in a very cut-down
sense it's usually line-x based and that
provides virtualization layer and then
we can have a number of operating
systems running on top of that which are
unaware of each other's existence and
each operating system of course can have
its own applications so we have a
problem when it comes to things like
versioning you got an operation
application that runs on different
platforms different operating systems in
my run on Windows it might run on UNIX
line X OS X one of the different ones
and it might run on different versions
of operating systems so for example some
software it doesn't run on certain
versions of certain operating systems
okay for example darker doesn't run on
Windows which is anything less than
version 10 at the moment and there's
also dependencies on different libraries
and this is what causes a lot of
problems if you've got an application
which is dependent on a third-party
library then you add a second
application which is dependent on the
same third-party library and one of them
needs a later version of the library it
could break the first one which is a
real problem then you start having
different versions of libraries which
starts to get incredibly messy and
difficult to manage and maintain
so otherwise the alternative is to have
lots of separate environments that's not
really practical because you're going to
have hardware resources used up for each
one and then how many combinations of
operating systems and libraries you need
to have that and setting up an operating
system with the appropriate library
who's actually very time consuming and
we don't really want to spend all of our
time configuring machines so when you
get to larger organizations it gets even
more complex so for example within a
very large organization you'll have
requirements for say admin staff and
then for operations staff and then for
developers and it'll be different they
want different combinations of software
and and they need different access and
you remember the
the least privileged principle from
security you don't want to give
everybody the full access that say a
developer needs when they don't actually
need it because of the issues that that
raises and developers require higher
levels of access and they also require
very specialized tools which the others
will not need at all so then when you
want to say apply a patch to Windows or
something else what data library or some
software it gets very difficult so and
then doing things like meetings getting
very complex there because you need a
particular environment in the meeting
environment which is not going to be the
general case so you don't want to start
moving computers around so we had the
multiple boot machines been around quite
a long time where basically you have a
separate disk or disk petition for each
operating system
there are some issues with that you need
some kind of boot manager to deal with
it there are the Linux ones like grub
which are arena round and they do the
job very well but they have fairly
complex configurations sometimes and the
problem is if you've got linacs
running alongside another operating
system no matter what it is it can read
the other discs because lineups can read
more or less anything so bypasses all
the security of the underlying operating
system if I line up crews running
alongside it and you access its disks as
just raw devices for example so have the
concept of virtual machines now you
might think they're fairly new but
they're not they've been around since
the 1960s because the big mainframes of
the 60s use virtualization to do
multitasking so typically every single
program more or less running its own
virtual machine and each program appear
to have full access since then we've had
a number of software packages come along
which do that the row number from the
rustle around VMware is probably one of
the most important players in
virtualization because it started in the
late 1990s and it's still going strong
for virtualizing the 8086 base of 386
architecture or 8086 architecture so
what's the advantage of a virtual
machine well the great thing is you can
set up a complete environment which is
completely isolated from the others with
the exception of that lion next thing I
mentioned if you want to replicate a
virtual machine you simply copy it and
you don't have to do any installation on
the other machines or the linkers copy
the image across several operating
systems can run the same machine at the
same time as long as the there's enough
hardware resources in terms of CPU and
memory etc to do that and they get rid
of the need for multiple boot machines
because if you wanted to have a multi
boot machine you have to shut down your
operating system and start up the next
one and with a virtual machine you just
simply fire up another virtual machine
on your current operating system and
that's fine we don't need to have
multiple boot machines we can have
virtual machines which we just fire
ugh and they just work virtualization
software cut the software that does the
work is called a hypervisor and what it
does is it provides the the guest
operating system with access to what
looks like the full Hardware of a
computer sometimes it is the real
hardware and sometimes it actually
emulates the hardware and this is
actually quite useful so at certain
environments one that I previously had
very serious problems with the Solaris
in that running Solaris on a PC it
always had problems accessing the video
and it was a real nightmare getting
configured if you run Solaris in the
virtual machine the problem goes away
because the virtual machine provides an
emulated video environment which you can
easily configure Solaris to use that's
the advantage and there's two types of
hypervisor
although the distinction is getting a
little bit vague at the moment
the type 1 which is also called a height
a native or bare-metal hypervisor
is basically piece of software that
installs directly on the hardware it's
usually based on Linux VM ware was
originally based on a modified Linux
kernel and then the guest operating
systems run effectively as processes on
that host operating system C which is
the hypervisor this is the approach used
for the high end service so if you go to
a big data center with huge service in
there they'll be using doing
virtualization and they will be using
these really high-end ones like the
Oracle VM server Citrix and so on VMware
ESX all these are quite expensive
products but they enable you to run a
large number of virtual machines on a
large piece of hardware seamlessly so
that's the principle there you got the
hardware with the hypervisor sitting on
top of it and then the individual
operating system sitting on top of the
hypervisor with the applications running
on the operating systems themselves so
now because you type 2 which is a hosted
hypervisor basically what happens is the
hypervisor software runs as a single
process on a normal operating system it
doesn't matter which one it is as long
as it supports it and
what it does you do is you run a virtual
machine on the hypervisor but what it
has to do is emulate some of the calls
from the guest operating system onto the
host one and it might have to do some
clever tricks to make it look like the
guest operating systems have access to
the hardware and that things like
include things like VMware Player VMware
Workstation and Oracle VirtualBox are
type - all right so then the thing is
that the situation this isn't the type
to situation and see just to have these
two extra layer whether the operating
system underneath the hypervisor so in
terms of the technology actually the
idea is not new assault was invention in
1960s but when the intel 80386 came out
which was quite a long time ago now I
working got into the exact date it had a
very interesting approach it started up
when you slide up the CPU it looked like
a NATO 386 processor and then if you
issued certain instructions instead of a
data structure you can put into what was
known as protected mode where it was you
had the full power of the 803 86
but the window in protected mode the
80386 could create any number of virtual
80386 environments so you could actually
do that having the once in the early
days of the ages 386 I had a Windows
machine and I wrote a little assembler
program where I push the three at the
base and 386 into a protected mode and
dumped the whole of Windows operating
system into a virtual 8o 386 environment
and it worked perfectly and I have a
little console at the bottom where I
could communicate with the actual
protective mode CPU at the same time but
then what happened was that the chip
manufacturers like such as Intel
realized that this is going to be an
important technology and they added
instructions to these the CPUs which
enabled virtualization to be controlled
more more easily and what effectively
happens is that some the
special instructions can be utilized to
effectively allows multiple operating
systems to share the same hardware
without knowing about each other
it's really clear it claims stuff and
need stuff and this extension was made
to be extended page tables which solved
a big problem about memory management
and that greatly opened up the scope for
doing really effective virtualization so
what that did because of the CPU
instructions it blurred the distinction
between a type 1 in the type 2 because
if you switch the virtualization on now
that's required if the CPU supports it
which not all CPUs do if you go to the
Intel site you can find out which ones
do and which ones don't if you've got an
Intel chip the virtualization is
normally disabled in the BIOS is by
default because if you switch the
virtualization on a piece of software
running on an operating system on the
processor could actually switch on the
virtualization and take complete control
of the machine which is a security issue
but if you switch it on which is a good
idea if you don't approve running
virtual machines what happens is the
hypervisor say like VMware Player
whatever you're using will grab control
of the CPU and put itself into a
particular it's a ring of the CPU mode
of operating and then effectively all
the operating systems the host and all
the guests can use the real hardware as
if it was just there without the only
ones on the machine which is really neat
stuff to have and you've also got the
gum line external KVM and freebsd hives
they always do this as well so
effectively you run a program on a host
OS and it turns it into a type 1
hypervisor effectively working the
actual software to become the site 1 and
the OS can run 2 OSS can or Milagros OSS
can work seamlessly together alongside
each other without knowing about each
other
so yeah positional machines are good we
use them extensively many big
organizations your desktop 4bm
virtualized so what you'll have
at your place of work would be a thin
client which is usually a cut-down Linux
machine and it will go and fetch your
your desktop from a data center
somewhere which will be running on a
server and it has that great advantage I
love it where you you're sitting at your
desk and you've got all your you're
logged in and you got all your windows
laid out as you like them you go to
another desk or to a conference room or
something like that and you just pull
your whole guest up over and it's there
as if it you just transported your own
original machine you get the same
desktop layers and everything it's
really neat so that the virtual machines
are full operating systems and that
means you have to do an installation
when you start up the virtual machine
the app virtualization software you have
to do an installation of the OS from say
an ISO image and then you have to
install all the application software on
top of it and configure it which is
quite time-consuming so once you've done
it you can clone it any number of times
you like then you've got performance
issues memory is usually the big one if
you've got a machine with relatively
small amounts of memory you'll find that
the host operating system will grab some
of it and won't leave enough for the
guest which is a bit of a problem
there's a particularly nasty problem if
you say have a 32-bit Windows machine
running on 64-bit software and you try
and run a 64-bit guests OS it may work
if the instructions are enabled on the
CPU and they're supported it is possible
to do it but the problem is that the
32-bit host maps away some of the memory
first things like video and doesn't
leave an awful lot left for the year for
the guest so it's not a good idea if the
host is 64-bit then the memory issues go
away because it can address vast
quantities of memory also the virtual
machines are huge it's not unusual for
it to be sort of tens of gigabytes and
that call these problems transferring
them across networks so there are ways
of doing it the typical way
but I've done it in the past is to use a
program like 7-zip which has got the
capability of doing this to compress the
virtual machine and break it up into
small chunks of a certain size and then
you FTP or whatever the the chunks
across to wherever you want to get it to
because they're typically a virtual
machine consists of a large number of
files they will typically break a large
gift into two big chunks as well the
virtual disk so that sort of convenience
but say the transferring across the
network is in real pain
even from things like USB devices there
can be very big images so just to make
sure we know what we're talking about
here the russet terminologies which are
sometimes confused and just to make sure
we know what the effect is going on
multitasking is basically several
programs running at the same time on the
same operating system that's been around
for quite a long time fortunately we've
now got the situation where our
processes are quite complex we've got
the situation of Moore's Law where the
speed of CPUs doubles every so often but
we're hitting levels now where we can't
add much more functionality to an
individual chip because we're starting
to get your most reach down where
quantum effects start to come into
perfect where you don't know where the
electron is so you can't control it so
what's happened now is that processors
have more than one we buy a CPU it's got
more than one actual CPU unit on the
same chip so you've got a multi-core
system you've got one chip we say four
separate CPUs on the same chip and
typically the CPUs of the course
themselves I use the word core for am an
individual the processing unit rather
than the the actual chip itself
essential fussing in a chip which is may
have several each individual core is
typically have several instruction
pointers so it can execute two or more
programs concurrently on the same core
and that's hyper-threading which most
CPUs these days I have got to hyper
thread
core and then we've got virtualization
where you've got a complete computer
positioned in some way and it acts like
a separate computer which is a fancy way
for saying you running a virtualized
virtual machine simple virtual machines
on the same computer ok so that's the
basics of virtualization
I think it's been around for a while
it's good stuff but we're going to go
into a more interesting realm now so the
question what is type 1 virtualization
now I've seen a lot of a's there it's
like one on 84 bare-metal which is don't
install directly on the hardware so now
what we're going to do is look at what
happened to Linux in terms of
virtualization which is this led to
something really quite useful right so
the problem with some handling software
installation and things is that if you
have multiple application on the same
machine which we mentioned earlier you
can get dependencies which may clash
because they require different versions
and then you can't run two versions of
the two different applications on the
same process same computer same
operating system which is a bit of a
nuisance
so dependencies make it even more
difficult you do an upgrade of a piece
of a library software and you
potentially break every application of
them the one you updated for when you
remove an application what you remove
with it do you remove its dependencies
is something else using it we don't
actually it's not easy to find out
sometimes
although the package managers like rpm
and APC are help you with this process
but when we would have to get piece of
software left on a computer which you're
no longer required
taking up space and potentially causing
a security problem or or some other
issue can the insulation being reversed
say rpm and apt do help with that but
they do have some rather nasty
dependency issues sometimes it's very
hard to install or remove software with
using these tools because of the
issues dependencies are really rid of a
nightmare now we have the concept of a
UNIX jail so a rather interesting term
to use for to jail and you heard the
concept of with iPhones of jailbreaking
them well
it's the same idea because effectively
in when your mobile phone applications
particular on iPhones the applications
are effectively in little jails where
they can't get out and talk to each
other easily together a fairly self
contained so UNIX been able to do this
for a long time to reset a chroot
mechanism for confining a process to a
restricted area at the file system it's
very commonly used like for example if
you will install a a server like a name
server or a web server what you'll do is
you'll restrict that server into a
particular directory structure
underneath the the root of the
filesystem
so that if something goes wrong and
maybe there's some some kind of security
attack on these on the server then it
limits the damage that server can do and
it can only access a small part of the
filesystem it can't actually interfere
too much with other processes
it is possible to get out of the CH were
at jail in fact it's not difficult at
all if you've got root access you can do
it by creating a subdirectory and doing
a series of CDs you can get yourself out
of the jail it's not a big deal because
the CH mechanism is usually only used to
restrict applications to certain parts
of file system and they're unlikely to
be able to invoke that mechanism because
they they were typically run as a non
privilege user but now what we got you
see I you have contain the technologies
and now we can make the secure jails
which is much better and this enables
you to do a thing called operating
system-level virtualization to use very
much a thing which is getting popularity
these days so what happened is in Linux
there's a series of namespaces were
defined and as a number of them and what
happens is that a namespace wraps some
system resorts like CPUs or memory or
i/o networking with any of those
features and then what you do is you put
processes into a particular namespace or
a set of namespaces and then what
happens is the processes seem to peace
the process that they've got their own
instance of some global resource but
actually it's not the necessary the full
set of the resource and it's an
important thing for this
containerization technology and the
thing is you can move the process around
so you can create a new process one and
put the a new nation when the process is
created or you can move a process around
between namespaces and it will see a
different set of resources which is
quite powerful so that as you can guess
what's going to come up is that a piece
of software will control these resources
these namespaces and move processes into
them so they keep them separate Linux
provides a number of namespaces the C
group which we'll look at in more detail
in a moment which is called a control
group we have inter process
communication so that things like
sockets and various things that the
shared memory and things like that which
enable processes to talk to each other
without network devices we've got disks
effective amount points where you're
connecting file systems together we've
got links places for process IDs now
that's pretty key as well which we'll
see later on and also namespaces for
users and groups within the operating
system and there's also things like care
tie and sharing hostname and NIS naming
a DAT s1 so what are control groups well
it's a mechanism which was started in by
Google in 2006 because they wanted this
kind of concept of containerization so
they started making changes to the line
external which the Linux kernel team
obviously liked because they integrated
into kernel to 624 couple of years later
so what happens is is that you've got a
control group which associates a set of
processes with a set of subsystems which
typically the substitutes a resource
controller which manages resources okay
so you've got the control groups or
control phase the number of CPUs you
have one loop control group which has
say four CPUs and other one has up the
other four you've got memory can be
partitioned into other control groups
and each of those can have a set of
processes in them and those processes
which are in those control groups will
only see the resources which that
control group gives them access to
control groups will see groups are
hierarchical and so you could detach
hierarchies to things if you've got a
parent with a child Hut see group the
child inherits the properties from its
parents and effectively each control
group hierarchy is a petition of the
processes so basically a set of
processes in one hierarchy is completely
separated from the set of process in
another hierarchy so
we process belongs to one group and the
child process starts in the same one
group with its parent and you can move
them around if you wish so just as an
aside about Linux processes or UNIX
process is nothing much different there
so the Linux processes are hierarchical
every process has a parent process and
when a child process is created it gets
a copy of its parents environment
variables signal handlers and Open File
handles the root tree is the process
number one always it's usually a program
called init B it doesn't have to be and
the person number one is started
directly by the kernel and process
number one's duty is to own and can and
create other processes so ultimately
every process is owned by in it if the
child processes parent dies it gets a
new parent which is image usually and so
bringing all these things together we've
got the idea of containers operating
system-level virtualization so what
happens is you have an operating system
it'll have a single kernel and then you
can add these containers at anytime or
remove them and they basically get a
subset of the resources available on the
machine and they run completely
independently typically a container will
ask you a container is a self-contained
file system it's usually a very cut-down
version of a flavor of Linux and some of
them like Alpine which we'll be looking
at later is very very cut down the image
is a matter of a few megabytes in size
and what happens is that you build this
filesystem and you then put your
applications on top of the file system
so that you've got say something like a
server if you want to say a database
server you'll put the database software
on top of the cut-down file system that
becomes a container then you fire up the
container and that database is available
to for for use and as long as the
distribution on the file system in the
case inside the containing that you can
possible with the running colonel
everything's fine so it's lightweight
and really good stuff are using it
extensively these days because it's so
much easier than having a virtual
machine hanging around you just fire up
a container leave it there as long as
you want then just shut it down
get rid of it when you finish with so
it's a very light system operating
system-level virtualization is very
lightweight and a lot of cloud
environments use these things it's a
good way of do I prefer doing
virtualization without the full
operating system being required so the
virtualization you require a set of
tools you need the virtualization
subsystem to make it manage it you have
to create a control group hierarchy for
each container and then mount the
container into the regular file system
and then run a program and using the
chroot mechanism plus a control group to
restrict it and that's basically what
it's all about
so the containers of jails their little
overhead is the the kernel implements
the container effectively there's a
number of them about said we said shoot
sure it's been around for ages does the
freebsd jails been around for a while
where it started to get interesting with
with the Linux container else Exe
command and later the lxd which
basically a full virtualization layer
hypervisor built on MXC and now we've
got darker and darker is the leading
suite of tools for managing containers
and creating containers and it's gaining
a lot of the moments so we're going to
use docker quite a lot because it's a
great way of doing things
it's um as a lot of containers out there
which have ready-made and you can just
pull one down and use it and you can
make your own very easily
how does this kind of thing work we read
it quite neat the third Linux and the
various flavors of bsd unix i've had a
thing called a union filesystem for some
time now what the idea is that a normal
file system say a UNIX filesystem you
have a directory structure and then any
directory in that directory structure
can become a mount point and you can
mount an external piece of storage onto
the directory structure and it becomes
part of the extended file system but the
thing is when you manage the filesystem
in UNIX the nd if there's anything
inside the directories used as a mount
point that content disappears it's not
deleted which just invisible hidden
behind the mount process the Union file
system enables you to overlay file
systems on top of each other and so they
call written branches where you have
directories and files but you take one
file system and layer another one on top
of it and effectively speed the Union as
in the set union of the two file systems
and the various priorities and things so
same path directories are merged and if
files have the same name the priority
decides which ones visible and which
ones are hidden behind the other one so
that's not quite any mechanism which
makes content containerization even more
interesting so the idea is viet 1 that
the idea is you can overlay a writable
filesystem over a read-only one so
things like Knoppix have been doing this
for a while you have like a DVD image of
a version of an operating system and
when you do run Knoppix what it does is
it fires up the operating system on the
DVD image which is obviously read-only
and then provides a writable layer on
top of it you can access it with your
right to it
and this from the memory stick for
example so you have the idea of layered
images and eat
lair is typically a set of actions but
basically installing software adding
software making configuration changes
and each of those could be a layer on
top of the system so and the advantage
of that of course is that when you do
that layering if something hasn't
changed you only need to change the
layer on top so you can keep the base
image doesn't need to change and then
you just change the layers and keep each
layer separate and then merge them all
together when you come to build the
final image so let's have a look at
docker and this is basically the docker
container management tool so what is it
it's basically it's a container platform
it enables you to create containers run
images and things like that and it's
basically each container runs typically
one application if the idea a container
can run multiple applications that's
that he's allowed but the kind of
philosophy behind it suggests that every
container should contain a single
application and have a very very minimal
implementation so that there's no
interference between different
applications so dock container can save
of a base line of operating system which
is basically all the tools and utilities
needed to operate a Linux environment so
you will have all the tools like LS and
PS and the ones that you're going to
need but what you won't have or any of
the boot mechanisms processes or scripts
because there isn't a bootleg anism and
anything else that's required any
packages that are required to learn to
operate and so it's very it's a good
jail you can't get out of it we don't
know of anybody that has got that one
yet it might be possible and what we do
is you start off with an image which is
just basically a big file and or maybe a
not too big file and then the image
becomes the vulnerable container and so
outside you've got to dock as you are
many containers
you've got the operating system in
kernel and 80 else docker is just
an application that runs on your
operating system and enables you to run
containers seamlessly as standalone
applications which completely isolated
from anything else
so docker is traditionally Linux and the
it was for a long time it was only
supported on Linux until fairly recently
because it needs the functionality from
certain versions of the kernel which is
anyone after the the technology was
enabled it requires a 64-bit
installation at least at least 310 Linux
kernel and it'll run on most failures of
linux not necessarily easily on or
rhythm for the same for the popular ones
such as DPN and CentOS and and abun -
it's pretty easy to get it running
though there are some issues with the
Centaurs version I found some problems
and you can get it as an RPM on a PT
package and just install it or you can
download from docker directly as a
binary version - some versions of Unix
Linux you need to do that use a binary
version if you go to the docker website
it'll tell you what you need to do for
your particular flavor of Linux docker
for OSX came out fairly recently it was
earlier this year it now runs natively
on OSX and which is good and you just
get to the mg package which you just
install it it's built on a hypervisor
technology it only works on fairly
recent Mac's so it's going to be at
least day a Mac 2010 because it needs
the support for the the memory
management within the CPU and earlier
max didn't have sufficiently late
versions of the process that was
supported this technology it requires a
fairly recent version of OS X 1010 3 at
least 4 gigs of RAM as it does use it
some memory there is a problem
with them the OSX version which was very
frustrating when I was working on this
material there has been a problem that
Apple and the darker people I've got an
issue communicating about networking so
when you write an order on OS X which is
what I'm actually running it on you cant
do proper networking because Drucker is
not capable of creating the right
network device to be able to communicate
externally it's not the end of the world
you can still use that on OS X but you
can't do certain things we've got
containers spread over multiple hosts
there's also a version for Windows it
only works on the very latest versions
of Windows 10 and that we said in like
support earlier ones it also requires a
package called hyper-v which is a
virtualization package to work and it it
does the things visualizes the
environment so that you can run
effectively Linux stuff on Windows so it
has to effectively look like a Alliance
kernel in some respects VirtualBox for
virtual machines cannot be run at the
same time as darker because the hyper-v
package and the VirtualBox back and
application are incompatible with each
other they can't work together
so if your OS is not supported the
single-decker toolbox and Duquette
toolbox is what you have to use on OS X
and Windows before the first part of
this year when it was finally released
what it is it's a virtual machine if you
to search your box and it's completely
packaged a virtual machine running a
Linux operating system which has got
darker installed on it and so you just
run that as a normal virtual machine on
your operating system and then you've
got darker which can then run other
applications docker engine is the key
part so docker got a number of
components as will be seeing mainly
today but there was another section on
another aspect of docker in a later
lesson
its client server you've got basically a
background daemon process which
all the work and it got a RESTful API
this normally by default works over a
UNIX socket so it requires the API the
client program to run on the same
machine if you got it on the UNIX socket
it is possible to make it listen on the
tcp/ip port and then you can have remote
clients to the daka server this is one
where area where you can't work on OSX
because this is the service on OSX the
client can't connect to it from under
the machine but you can to go the other
way so the service is the daka daemon
and what it does is it it manages the
containers so it can go and fetch them
they can build them it can run them and
it can put them somewhere else so that
containers themselves where are they
stored well with a number of places
docker hub is a repository set of
repositories up on on the Claire and
that enables you to get images of a vast
number of things there are docker images
for all of the major distributions of
Linux and they've got multiple versions
of each of the distributions of Linux
they will cut down it's also got many
many applications so if you want say a
MySQL server there's a container up on
docker hub which has got MySQL running
on top of some flavor of Linux and you
can just pull it down and run it and any
other application which is popular so if
you want to jump in CI server a
continuous integration server you can
just pull one down so you don't have to
configure your own and these things are
pretty neat actually because what they
they're configured in such a way that if
you specify certain options when you run
the image as a container it was
essentially configure itself so as we'll
see in the next lesson if you pull the
MySQL container down you can first you
can fire it up in a way that it creates
a database and your user and then but
you can also fire it up
which means configured and it's got an
existing database and users they could
also think fire itself as a client and
access another container which is that
Esme server so done quite a lot of work
on making this stuff really user
friendly you can also have private
images you can create your own images
and in fact Drucker keeps its own
collection of images locally so he
doesn't have to go fetch them all the
time and the other other places where
you can get images from other vendors so
an images are a template and so it's
usually a cut-down version of a line of
operating system and you just add
applications to the image and basically
an image is a template and then you
provide a set of operations which builds
on it so you just say okay I want to add
this install this software I want to do
this do this so there's this it
basically has layers to the container
and and then you've got your fill image
which should then be run they can have
all sorts of things you can run them
start them stop them
delete them they're all good good stuff
so the images are built in layers each
each the file system it's a union file
system so what'll happen is if you go to
docker hub and Paul Sayer MySQL image
you will see it'll do a series of
downloads probably about maybe six to
ten separate downloads each of the
Downloads is a separate layer on top of
the container does it support HTTP yes
it does you might need to do some
configuration for um configuring HTTP
but yes it does one thing we will be
doing actually is building a container
which is an SSH server so we can connect
to our SSH to it so yeah it pulls the
base image it pulls the the layers and
it does the Downloads in parallel and
when they're all there it just builds it
all together into a continuing into the
base image and as a single what looks
like a single unit but it's actually
layered so if you do a well one thing
that's very powerful is if you make a
slight change it won't have to download
everything or if something slightly
changes so to give an
example I was trying to build an image
and as inevitably I made some mistakes
in the configuration when I was trying
to build it and so when I fixed made to
change to my configuration and did a
rebuild it didn't have to go and fetch
the bit he's already got if it had cache
them effectively so that minterm they'd
actual bandwidth and download that was
required was minimizing to only
downloaded things that I'd added which
hadn't already been done which is really
neat so you have a an image and use the
docker run command which creates a
container so basically the image is
read-only effectively and the container
is a copy of the image which is it an
actual container running the filesystem
so when you say run it'll first of all
look for local copy if it doesn't find
digital gonna search for it and the
container is created from the image
itself as effectively as on a separate
file system it adds a write layer to it
at the top of the file system so that
you can make changes to it
it also creates a net
interface and a scientist an IP address
the way it actually works is the docker
daemon acts as a gateway to a a private
internal network on your machine it's an
internal subnet and the containers are
attached to that subnet we look at that
working in more detail later it's
actually quite powerful you can also
connect optionally the input output and
error streams so you can interact with
the container if you wish or you can run
is the background process so the dock is
winning therefore it's run the docks
container just appears as a child
process so this shows the daemon process
here which started with certain options
and then process 1 4 10 was a child is a
child of docker which is the container
running in this case a shell which is
one way of actually starting containers
are quite powerful so to run an image
has to be on the computer and so docker
has its own little collection of images
which and if he hasn't got one it'll
either need to be created locally or
pulled from a registry and the pool just
simply does a transfer of all the layers
and puts them together into a cache and
then you run them to get the container
so this is the example it's shown there
this is we want to sent us every single
container usually has a color latest
version you can specifically ask for a
version so I say I want sent-off version
seven point something and it will fetch
you that particular version of sent off
and then you can run that container and
what you do is for many of the container
will have some kind of default program
which she's configured into it which
will run so if you just run it as a
background process it'll fire up some
application which the container was
built to do whatever that might be you
can in the case of say a send us one or
any other containers you can specify a
command
so if you go docker run sent off color
latest Who am I
it'll create a container from the image
and start it up and run the Who am I
program a little then exit and the same
were there any other command so you can
run a single command like this not
tremendously useful but it shows you the
principle typically you'll be running a
real application or quite often you'll
run a shell so that you can interact
within the container itself and make
change to it so the way to do this is to
write interactively docker run - I keeps
the standard input stream open so you
can use keyboard and - T allocates a 32d
Y device which means you can you can
actually interact with the tty drivers
inside the container and you normally go
docker run - I T or - t I specify the
image and specify the path to a shell
and that will actually run that CentOS
container and give you a shell prompt so
you get the hash prompt as a root shell
incidentally the process is inside the
container your bash shell will be
processed number one in the side the
container because the process IDs are
mapped so process ID 1 in the container
will map to process ID something else in
the actual host operating system you can
do some interrogation if you do a duck a
PS it will list all the running
containers in your machine and the
container will give it won't be given a
name by default the name is quite
comical but you can specify an actual
name you can also do a darker stats
which shows you the resource usage and
you need to terminate that with a ctrl C
there's something to try and playing
around with so it's very useful I always
do this duck or run - IT - - name and
then you give it a sensible name because
then you can use that name to control
the container so that's the first ones
running the container you can stop it
using docker stop and you can remove it
docker RN deletes the container itself
which you probably want to do wise lots
to sign because you just throw
we'll talk about files external file
systems and what happens because when
the container is destroyed everything
that goes with it goes as well unless
you've attached some external storage
which we'll look at you can also attach
to a running container so for example if
you're running one container
interactively on the first one if you go
to a second terminal and go darker
attach sent-off it'll attach your
terminal to it but you actually got the
same session so if one terminal if you
type into it in one terminal you'll see
the output in both because it's
effectively both such the same container
and there's this magic keystroke control
P control Q so detached from the
container leaving it running so that
means you can effectively walk away from
the business detach yourself from the
container and then reattach yourself
later on if you wish to so stock a stop
terminating containers you can
opportunity give me a time interval and
they main process process ID one which
is not in it if the the main application
starts in the container gets a turn
signal and then after the interval it
will get a kill signal to make sure it
goes away you can oppose a container as
well that basically puts all of the
processes in the container to sleep it
doesn't use the standard unix six dot
Sigma mechanism because officer
processes might be able to see them it
uses a control group freeze mechanism
which basically all the processes in the
C group and its child processes child
groups are put to sleep without the
processes knowing about it and then you
can resume them if you wish to with the
unpause when a container stops
terminates it actually doesn't go away
it just goes into a stop state if you do
PS docker PS minus a it'll show you all
of the containers including the stopped
ones which is very useful and you can
restart it by doing a docker start and
you've got the option of adding the
connecting me input and output streams
to it
removing containers you just do Stucker
RM sent off in that case if the
containers running it will refuse to do
it unless you add the minus F switch to
force it to stop PS minus aq command
shows just the container
Deas so that command at the bottom dhaka
RM dollar docker PS - AQ e effectively
deletes all the containers on this
current machine you can run them in the
background by using the - D for daemon
switch or detach switch and you can send
signals to the container as well using a
docker kill so let's look at docker
images and work with the images
themselves what are they so docker has a
number of images the docker hub is a
huge repository of its images for darker
basically and there's a lot of them that
up there and you can usually find
something that is a good starting point
for anything you want to build so you've
got many versions of Linux and you
choose the version as well so sent-off
called mainly latest we'll give you the
latest version of centers which can be
used as a base so any the line X term
distributions can be used as a base
image so the one we used was Alpine
which we noticed was very very small
that is a one that's been specially
built to be very very lightweight
whereas they sent us or debian or any of
the others will have a fairly complete
core of the operating system won't have
any other graphics it'll have all the
but all the utilities to be able to do
development and work it within the
container whatever you want to do you
can do a search docker search CentOS and
it'll give you any possible match that
it's got on the docker hub you can also
go to the website and have a look around
on the website to try and find what's
available so installing software in once
you inside the container what you can do
there's actually two ways of building an
image you're either do it with a build
process which we'll look at later or
alternative you go into the image like
you do with the shell and just add
software you want so you could go into
the container and do young or apt-get
which depending on which flavor you've
got to install I'm the software you want
the problem means you often end up with
a lot of stuff you want don't want so
for example I was trying to build an
image which you got the maven build
torrent and maven is basically a java
application
and so maybe dependent on on Java and it
pulled in the open JDK which I didn't
want because I wanted to use the Oracle
Java and so using yum it didn't work so
I had to install it from a tarball or
directly from an RPM or adopt deb
package which you just download so one
thing you can do is you can ask the do a
build by you getting the appropriate
document select our file from the net or
alternatively you can provide a
directory full of files which you can
access so we will look at doing that
later so it's often better not to use
your money or a PCGS unless it's
something fairly simple like the net
tools for the ifconfig command you
wouldn't want to do it for a substantial
application like maven so if you go
darker images it shows you all the
available images and docker RMI removes
an image it deletes the image from the
local repository so if you go darker
images it shows you all the available
images and docker RMI removes an image
it deletes the image from the local
repository
so the central space image if we use
that it's pretty cut down so I went in
there and I wanted to which command to
find out which where the tools were and
also wanted the ifconfig command to see
what the network configuration was
because it was quite important to me at
the time so I had to install it so I
connected to the Centaurs latest and did
a yum install the - Wiese which
basically means you don't have to answer
yes to any other questions that you're
normally wants it just automatically
says yes to everything that the young
command says and so we stored the which
program and the net tools which contains
the ifconfig so that's one way of adding
packages bikes going interactively so
you can then do a darker dish and it
will tell you which files have changed
you probably get a very long list from
that and then what you can do is you can
commit the container to an image so what
that does is we ran the container and
then exited so the container was still
there it was in a stop state which is
what it needs to be we did a diff to
check what was there and then we needed
our commit on the container slop
containers called CentOS to create an
image called center - net and that
basically turn the container into a
read-only image with another layer added
work which you see the stuff we
installed on top of the existing CentOS
and then we can remove the container as
we no longer need it and then we can run
the image just like any other image and
run look at the image of the container
in this case running it interactively
the - minus RM option on the run command
says to delete the container on exit so
if you're just doing something like
changing looking at the configuration or
something like that
just having to look around without
making any changes it's quite useful to
use - - RM because you don't have to
worry about deleting the container
afterwards but if you want to do
something like cam do a build change
something and then do a commit you
obviously don't use the - minus RM
because you delete the container you
want to commit
inspect the layers of the images using
docker inspect by default docker like to
use JSON documents JSON erase uses it
for both reporting and for configuration
sometimes and so you can use some both
images and containers you can get the
size you can get very specific with the
- RF switch it formats the Jason
extracts of JSON fields with a certain
matching pattern that's the inspect
now we don't really want to do this
manual approach we want to automate this
is what it's all about isn't it with
DevOps it's all about automation really
don't have to redo things manually so
how you create an automatic build is
this you create a directory anywhere you
like and you put any files that you want
to use for the build this doesn't
include us that you grab from the
network it just files that you want for
the build then you create a file called
docker file which defines the build
process and then the directories goes
which comes in what's called a build
context which is basically a build
context is a docker file follow plus any
associative files required to make an
image and then you build it and each
command in the build file the docker
file adds a new layer
and and it actually creates what it's
doing the build it creates containers
for each command in the docker file it
creates a new container and effectively
then moves that on to become the next
layer of the image and then you end up
with the final image so if you duck a
files got a lot of commands in it you'll
end up with a lot of layers so you'll
find that a lot of people will write the
file where it uses the command chemica
catenation double ampersand to execute
multiple commands as at the same level
so that one layer is actually to come
the product of multiple commands so if
you want to wire lots of double
ampersand in the docker files that's the
reason so every docker file starts with
from followed by the name of an image
and that's the starting point so the
build will fetch that image if it
doesn't exist from the local system you
can specify a maintainer oh she's got
the email address of the builder you
don't have to do that but for the sort
of ones that are put out on the net for
general usage you probably want to do
that this shell command defines which
shell to use you often don't do that
very often either it can be done in
either for this is a kind of JSON form
which is one way of doing it so if I'm
seeing in this case the default shell to
use
uses adjacent form in the dockerfile
there's two forms for most commands to
JSON form and sort of what's
straightforward form so then you can use
the copy command and this is inside the
docker file what it does is it copies
files from the local directory which is
the build context and stores them
somewhere in the container so in this
case what I'm doing it's in the build
contents I've got a a version of the
Java rpm I copy it to the temp directory
on the inside container remember the
source of the copy command the left-hand
side it always a directory on the in the
directory you can use wildcards as
they've done there so anything that
matches their station directory is
ending a slash and they get a little
create the directory if it doesn't
already exist so the C add command copy
things in to contain them as well again
just be in the build context you can
catholic wildcards as well
it's just another way of doing it see
but this the ad can use a remote URLs
rather than just files with the
difference and then the run is probably
the most common one what that does is
actually execute a command inside the
container that is being being built okay
remember it it's building layers of
images by creating containers and these
commands of execution the containers
each of the run commands will generate a
new layer and a new container
so we do in a young install which we
should go to the network obviously we're
doing the RPM install from the file we
copy to the temp directory and we're
also doing gay a run which is doing a CD
and a link to in this case quite a
symbolic link called maven in /op which
points to the actual installation the
commands mustn't block which is
important
and so you have to - why switch on your
rpm on your own stops it from asking you
if you want to do this the commands
mustn't block but the multiple commands
can be separated by a semicolon or a
double ampersand the difference is the
semicolon will execute always for you -
double ampersand as in the UNIX sense if
the first command fails the second one
won't get executed
so you properly use the double ampersand
rather than the semicolon form the
docker M command creates environment
variables in the image so when the
images are creating to turn into a
container and sort of fired up these
environment variables will be set is the
equivalent of pushing them into the etc
profile this is setting up the Java home
and setting up the path environment
variables the command or CMD defines the
default application so you typically
have one command you can have multiple
ones that though if you have a command
there if you run the container without
specifying a command after the image
name on the run line it will execute
that when the container actually runs
the obviously the command wouldn't you
normally be a shell it would normally be
the application that container is
providing so the build process works
like this you have to build directory
which is the build context when you do
the docker build it basically creates a
container and the build context
directory is attached to the container
as a mount point we'll see the details
of this how it really works later on and
so in fact we all the directory content
are visible into the other their
coverage the daemon and all visible in
the container that's been made the
docker script doctor file script is is
there must be there and you need to tag
created tags for the target image and a
tag is the name of the new image so we
notice that we have sent us as the base
so I've created center of Java which is
the center of plus Java and I'm
specifying the version is colon latest
you can specify a version number if you
wish
and so we especially if I duck a bill
with - T and the tag name the name of
the new image and then the path to the
actual bill context which I usually have
a start which is the current directory
obviously all the individual commands in
the dockerfile
are in uppercase and they're usually
three letters long
now that you've understood how to work
on docker images Randi will explain how
to create a dock container containing
the alpine Linux distribution with sshd
- a demo welcome to DevOps lesson 2 lab
number 2 docker images let's go back to
our terminal and in this lesson this
demonstration we're going to create an
image that contains an SSH server so
that we run it in the background we'll
be able to make secure connection from
our clients to that container the first
thing we need to do though is to make
sure that we have some keys generated so
you do that by saying LS to list in your
home directory the contents of a dot SSH
directory dot meaning hidden and we see
that we have authorized keys that means
we don't have the requisite keys so
let's generate those we use the SSH key
gin program and we want the type RSA
this is where we want them stored home
student hidden directory SSH and by
default it's ID underbar RSA we're not
going to use a passphrase
keys are generated let's double check
that by doing the list of the directory
and there we see them ID underbar RSA
which is the private key and ID underbar
is a pub which is the public key
go back to our home directory and type
clear
next we want to prepare our computer to
build an image an easy way to do that is
to create a directory focused on that
task so we're going to make a build
directory and then we'll enter that
directory
we want to do is copy the key that we
just generated we want to copy the
public key so we're going to say copy
from the root in the dot SSH directory
the ID dot RS a dot Hub file
and we want to copy that into a file
name authorized keys
because we're going to put that file
authorize keys into the image that we're
going to create and that will mean that
that particular container running that
image has a public key for our pair so
we can use the private key to
communicate with our public key and
create a secure connection
the reason we're copying that into the
bill directory is that the bill
directory is the context in which we're
building an image we can copy files we
can create a docker file let's actually
touch that and a docker file contains
the directive for creating an image
so let's edit that file
nano editor the first thing we do in
this docker file and remember the docker
file create is us defines a set of steps
the docker will follow to create an
image the first thing we want to say is
what is our base image and we want to
use Alpine and the tag we want to use is
latest with the very latest version of
the Alpine image then we're going to
instruct docker to run a command apk
update apk is the package manager for
alpine so we're saying to it please
update your local cache so that we can
make sure that we've got the latest
versions of the programs that we want to
install
thing we want to say is apk add open SSH
we want the OpenSSH package that
contains the ssh server then we're going
to say run add user and I'll just type
this in you can see what the result is
going to be we're going to create a user
and notice that we're going to use an
amp or an ampere that means continue on
with the command but only continue on if
the last command completed successfully
if it did complete successfully we're
going to say m'kay term make the
directory home student not SSH and then
also echo student :
student and pipe that to CH password
what is a docker registry a docker
registry is a location usually on the
Internet where docker images are stored
a registry contains a number of
repositories each repository contains
different versions of a particular type
of image registries contain many
different repositories one major issue
about using docker is that many of the
images come in a lot of different
versions so typically a registry will
have maybe 10 20 30 different versions
of the same image you need to choose the
correct one so it's quite important to
choose the executive right image you can
specify the version number when you pull
the image from the registry there's also
a special type called latest which means
you always get the latest version of the
image from the registry and problem is
with docker it gets in the right
versions so what we do is we have these
concepts of registries which are used to
host images and so you can go to a
registry and get the the right one
you
and so you have to think about various
things like do you really want to put
your image somewhere in the cloud
where's woody can get to it I think like
access control and you've got to think
about where the actual get registry is
going to be so registry is actually a
service it is actually usually a docker
image a registry is a service used to
manage and distribute images she's based
on a description of the image which
consists of the image name and a version
number registry is a manager of
repositories as a repository is
basically all of the versions of the
same image so typically repository only
contains one image but it contains
multiple versions of it and you have the
idea of a tag which tells docker which
repository to use and which image to use
Hubb is the default registry Tuckahoe
has a default route namespace for all
official images you don't need to
specify the name of the namespace to get
to an image just use the name of the
image : the version number is all that's
required the Rueter images on docker hub
include versions of supported lightness
distributions all the popular Linux
distributions are there at various
versions including the latest version so
for example the NGI NX image which is a
web server can be addressed in several
different ways
it can be addressed with the full name
of the docker hub plus the image column
the version number or it can be just
simply the image name : version number
labels to save an image in docker hub
you need to define a label the label
comes immediately after the URL for the
updog hub itself
the HTTP code
Hakam the first thing after the slash
the in this case the R is a namespace so
this can be either underscore for docker
hub which means it's one of the standard
images or R for user which means it's
one of the user-defined images used by
one of the clients hooked up a hub after
the R comes a slash which is followed by
the username or organization of the user
who provided the image after that is
another slash followed by the name of
the repository item so in this case it's
net kernel - AC the name of the image
after that can come colon followed by a
version number which is often colon
latest
it's not the only one there are several
the google has the container registry
splashed the cloud platform i've got
good access control and security amazon
have got the container service there's a
an organization called key which has got
free plans and pay for plans you can set
up your own registry and there are
others as well so anybody can set up a
registry if they wish to in addition to
the standard registries you can also
have a private registry anybody can
create a private registry they can
either create it on their own machines
or on the cloud private registries are
very useful for storing images which you
don't want to make public so for example
an organization would store their
internal images on a server inside their
own network automated builds work in
conjunction with a git repository what
happens is that your docker hub account
is connected to one of your github
accounts or whether get wiki accounts so
what happens is you have a docker hub
account where you store your images
you also have an Associated bit account
which is one of yours as well you
connect the two together so rather than
store the actual images in docker hub
what you do is just configure a docker
file which will build a docker image
check that into the gate repository and
docker hub will automatically extract
the docker file from the git repository
and build the image for you this is the
preferred solution and some recounts
require this to be the case the whole
idea being that you don't build the
images docker hub does so it could
easily set up a repository in a docker
hub you need to create a user login to
it and then create user name space
choose a repository name and set its
visibility so it's very easy to do you
do have to pay a certain amount
depending on how many repositories you
want to tell me basically how many
positives you want remember a repository
is required for every single image you
want to store for your organization
you need to log into the registry - and
it's not the first time you do it it'll
store your username and password in a
config file which is used for subsequent
access
now images need to be the same name as
the repository because the compromise
tree only contains one type of image so
you can commit an image to the docker
hub registry name so or you could do it
to another name okay so you can use a
new tag when you commit an image so this
one creating am an image which is
basically copying creating a java image
from a docker file called java and then
we commit it and then because of the
something flash in front of the the name
that puts it to a particular registry
and the name of the repository is
question and then you've got color on
the version number and then you just
push the image into the repository with
the right name
and you can use the image tags as well
when you use the dagger pool command so
you can pull your private image from the
github
images competitivity from the command
line which removes the image from your
local disk docker hub doesn't allow
images to be deleted present you can't
delete an individual image from darker
hub what you can do is delete the entire
repository from the settings menu on
docker hub which deletes all the images
associated with that repository
so private registries this is good big
got the local image cache so
everything's quite quick you can share
images between teams in your
organization and you can choose store
images based on say development UAT
production environment and of course you
can guarantee the registry is going to
be available because some of the other
ones even the commercial ones might
cease to exist
over the field
think was that that's specifying what
the Laplace Jesus it's probably a local
one but it could be one on darker hub if
we got the right name
and yeah the actual thing that goes
before the image name is identifies the
repository but it will check docker hub
if by default unless you spell explicit
about it
how'd you do create a registry well
there's one thing you can do is that
docker hub contains containers or docker
images which contain it registries and
so you can just pull a registry image
from docker hub and that's the starting
point for it
you
so you can now run the registry mhm
Tanner you want it as a demon process so
the dynasty switcher and Bruins it as a
background process the - P Maps the port
5000 on the container - port 5000 on the
host which means that it's accessible
from the local machine and any machine
that can get to that address on the
particular computer specify the name is
registry and execute the container
registry so that sets up your registry
port 5000 is the default port for a
docker registry and so now the registry
- identified by hosting : port name and
port number and so you put that as the
registry prefix so here is example of
we've got our local registry and so what
we do is once you've got the registry
set up we can take pulling an Alpine
image from docker hub then we can do
docker tag and what that does it
effectively creates an image with a
different name space so it grater
another image which has got the name
space localhost colon 5000 called Alpine
: latest so then when we push local
house colon 5000 slash Alpine it pushes
that image into our local registry and
now if you see the name must agree the
thing before the slash must agree with
the name of the relation which is why we
had to create another image from the
Alpine one - the one that uses our local
repository prefix the default race you
need some configuration now this is
fairly standard many images do this that
they they specify they're kind of
configurable by specifying command line
options when you run the the actual
container so you can specify environment
variables using - e on the run command
or alternatively you can override the
configuration file so the docker
registry has a Yama file which is some
change the configuration so what that
command there does we'll see this
command in more detail later the - V
command says take the the file flush
data slash config Yamma on the local
machine and copy it to the object to be
mounted onto the container as such et
cetra / docker / registry / config yml
so that means that when the containers
running is using your config dot yml
which is sits over the over the top of
the existing one and replaces it
effectively so it's actually what you've
done is mounted a file into the
container to make that work but that's
that gives you a good way of doing that
these for volume which is some basically
loadings mounting a local file or file
system into the container to the
well-defined place
when you install a registry which you've
downloaded from blocka hub you need to
make some changes to the default
configuration for example image needs
some external storage for storing the
actual images in otherwise when you
delete the container you'll lose all the
images you also need to switch on
authentication by default
anybody can store or retrieve images in
the registry without any kind of
authentication whatsoever so typically
you need to set up some kind of username
password system by default the search
facility is disabled this means you
can't search the repository for images
you need to switch this print
functionality on to make it usable the
configuration files making the changes
to the registry configuration are
complex there are sample files available
they are stored in the doc and git
repository another way of doing it if
you want to you can use a docker
registry rpm package so if you've got a
Red Hat flavor of Linux like red
enterprise Fedora or CentOS you can
build a from the RPM so you could
actually store it locally on the machine
if you prefer or you can build a
container image a couple an image from
the RPM and run to contain the from it
and it's some written in Python and it
uses the unicorn with a so-called green
unicorn Python HTTP server because the
repositories are basically web servers
with the facility to store upload and
store and deliver images
so you can do young install docker
registry either on your local machine or
on a standalone machine or in a an image
then all you need to do is edit the
docker registry dot yml file to set the
configurations that you want for the
registry and then just run it and this
is X showing how to run it on a Red Hat
machine either in a container or resume
a standalone on a normal of installation
of Linux the dock of registry package
could be installed in a dock container
there are a few conditions and
limitations on these first of all it
needs to be based on the suta Calibri
such as CentOS any Linux distribution
will work as long as it's supported the
registry can't be run as a normally
linux service which he starts at boot
time because much of the boot
configuration software is missing from
dock containers there cut down versions
of Linux file systems so you have to
start them as a daemon process rather
than a service it's also not a good
practice to try and run services as
containers should only on a single
application so what is important is to
write a script to start the registry
this must set up a few things
it must set up environment variables so
that the registry is correctly
configured and knows where its files are
you then need to execute the web server
this is the Python program G unicorn it
has a large number of parameters so the
script here is what's the start point of
the container this script is executed
when the container which starts up the
registry service completely G unicorn is
simply a web server it doesn't come with
a registry D term right so the steps
simply pull up an image at the registry
layers install the RPM is necessary at
the docker file and all that and then
just build everything in run the commit
the new image shows the steps that we
just went through when you build a
registry image the first time you run
the container it's a good idea to run it
interactively
so then what you can do is you can have
a look around see what's going on
you start the registry as a process and
then you can start you run the story as
a background process and then you
start looking what's going on look at
the log files look at the ports on the
machine and everything like that makes
sure it's everything is working
correctly once it's up and running use
docker search
docker push to access the registry so
that one will show nothing then the
Alpine being pushed in then the search
would show Alpine has been in the
registry with this we complete the
concepts of docker registry now Randy
will explain the steps involved in
creating and using docking registry with
the help sir demo welcome to DevOps
lesson 2 lab number 3 where we take a
look at the docker registry you've
learned that the registry is a location
from which you can pull images and you
might guess that it's also a location to
which you can push images and that's
entirely correct the registry we've been
using is a public registry provided by
the folks that have authored docker
but while this is convenient you may
need to have your own private registry
you've got private code and only you
should be able to pull it down so what
we're going to do is we're going to take
a look at how to run a docker registry
and in a bit of recursion we're going to
use a an image a docker image that
contains a docker registry so let's go
to our terminal
and the first thing we're going to do is
pull down this image it's called
registry and we want the second version
tagged to and that didn't take very long
the first thing we want to do is pull
down the registry image and it's called
registry we want the second version so
the tag is two and notice it's coming
down in layers
and as we've done before let's just
check images and there it is coming in
at thirty seven point six megabytes okay
that's great now let's run it so docker
run in detached mode and we're going to
do a port mapping of five thousand to
five thousand so it's the same port will
say restart equals always
and we're going to give a name to the
container we're going to call it
registry and the image of course is
registry : 2 so it's up and running
docker PS confirms that it's there now
we're going to pull another image we're
going to do this so that we have
something to push into the registry so
let's start that we're going to pull in
a rather large one it's Ubuntu and we're
fine taking the latest the : latest is
always the default so we'll pull that
down takes a little bit longer because
it's larger
there we go so let's look at the images
we have now we have the registry that we
pulled down and now we've got a bun -
that's coming in on a hundred
so the final section too for today is
dhaka machine this is actually really
neat this is where you can manage docker
instances on remote machines typically
you have a machine one particular
computer which is nominated as the
controller and then all the other
machines anywhere in the world may be in
the cloud can be managed by that machine
and you can do anything you like to
their doctor instances if you've got the
right permissions so what we've done is
some docker has a number of components
now we've basically used two of them so
far those docker engine which is the
daemon which does all the work and
there's a dagger client which is a doc
command line tool docker
so now I'm going to look at the one
called docker machine and so the docker
client can talk to all sorts of things
and could be a client to any host which
might be anywhere they might be in in
the cloud they might be on a local
machine it might be in a virtual machine
environment of some kind lots of
different places it can be but what
happens is that there's a docker machine
client which enables you to talk to the
docker
demon and can configure it in terms of
Linux docker was came out at certain up
level docker machine is often a separate
application you may need to pull it
separately it's now available for all
versions for Linux and for Windows and
Markus it is available unfortunately say
with Mac there is the issue that the
docker diamond can't listen on tcp/ip
connections so you can't administer a
Mac docker server remotely but a Mac can
be a a client from dhaka machine and run
the animist or other things as it
wearing the internet in the cloud or on
the net so why would you use docker
machine well basically to manage to
remote docker hosts so literally you
could install and update and run docker
and manage it on a remote computer from
your dock emotion host on older
versions worthy if you use the docket
all set which was used to use for
Windows and Mac and for unsupported
versions of Linux then used you actually
use docker machine to manage the docker
engine within the container
so dr. toolbox basically virtually
requires document to issue the commands
to the machine
it's any way you can manage your remote
thing and a virtual machine is a remote
instance
so docker engine is just for usually
referred to as docker
if the daemon which has a basically a
web service with an API and the
command-line client called ducka ducka
machine is its own command called docker
machine it also if you saw dr. machine
it also has a docker client sadaqa
machine is actually a separate package
on some installations or with others
it's bundled with the just docker and so
it allows the docker engine to be
managed remotely and it actually does
allow you to install docker on a remote
machine which is quite an interesting
idea it doesn't actually work I've done
it
so basically what you do is you have a
typically you allocated machine a
dedicated computer which runs dock a
machine and then I should really only be
one for your own particular environment
if you like a machine and just client it
then talks across the network to other
machines which run the docker daemon and
they become Duquette service and this
get really powerful as we'll see later
on is a thing called duck a swarm where
these things can communicate with each
other and you can do some really clever
things so to do this you have to set
things up now there is a interesting
thing that was added to Linux under the
control of the NSA in America security
organization could se Linus I don't know
if you've heard of it but it's it's a
basically a kernel feature which if we
switched on it really restricts what you
can do the problem was with Unix Linux
was that um they considered root to be
too powerful because which is all
powerful
so SELinux was added to reduce the
powers of the root user and others and
it can also cause all sorts of
unexpected problems like for example
most Linux distributions these days come
with selinux not only installed but
switched on and if it switched on in
full mode it'll stop things and
happening Doug who does have se Linux
support there you might have noticed a
see Linux which on the demon lining that
won't alive but if you have problems
switch SC Linux offered these for test
purposes and by changing a see Linux
config file disabling it and rebooting
and also be very careful about firewall
rules because I've been I've wasted
quite a lot of time working on this
material trying to get a docker to work
but because the firewall rules for
blocking everything for me and I didn't
realize that they were actually there so
I switched off the firewall rules and
everything started working wonderfully
so there's a particular port 2375 which
you must make sure it's not blocked so
then what you need to do is have a
remote user on the machine which has
pseudo access without password now that
sounds a bit dangerous doesn't it but
because that means that user can get
root access and do whatever it wants to
do and so you have to add that line to
the etc sudoers on the remote host for
that particular user
then you need to do the SSH thing you
need to be able to allow the docker
machine to access the remote machine
using SSH without a password and that
means doing the key stuff you have to
generate the keys and then copy the
public key to the authorized key on the
other and most hosts so that equaling
access fire SSH without a password again
some think it is a little risky from the
security perspective maybe that it has
to be done to do that and then you do
this this is one way of doing it there's
many different drivers for dealing with
soccer machine but this is the one that
will work on certain operating systems
and so that's a generic driver the
support for that docker machine varies
quite a bit depending on which flavor of
which machine you using it typically
only works for certain flavors of Linux
so what this is doing is creating a
docker machine and with saying use a
generic driver specifying the IP address
of the machine specifying the remote
user of the Machine and then specifying
the the key the private key of the user
which you copied under always fear the
one and then specify the HP at the
bottom is just the name of the computer
that I happen to be using at the time
what that will do is basically log into
the remote machine the document machine
will using SSH and it will create an
instance of docker it'll be at McClatchy
installed the docker server and set it
all up on that machine which is really
neat and
you can do a Duggar machine LS which
will show you all the remote machines
which are set up it so gives you
information if you install darker on a
remote machine we saw chuckle engine on
a remote machine you can't see it from
darker machine itself on another one
until you've told us about it because
they can't figure one out but you can
actually point it at one and say ok here
is a darker instance it's already been
created you can manage it for me does a
machine has a number of drivers these
are for installing remote instances
remotely there are different drivers for
the different cloud implementations such
as AWS and GCP there is a generic driver
for installing on remote hosts it does
not allow instances to start and stop
unlike most of the drivers some common
drivers are of those for the Amazon Web
service and for the Google compute
engine to send us I found I try to
install darker on it and there were
files missing particularly this one
without that file darker on centers will
simply not start I just put that file on
the machine and everything was fine
afterwards but there are some bugs
associated with certain distribution
center is only partially supportive
apparently for some reason because of
the missing script but once that scripts
there is everything's fine
okay so this is the chapter on
virtualization we finish with that come
to the end a virtualization is a great
way of managing things great things for
particular for big companies where you
need to roll out symbol and machines for
different people within different roles
then we got the Google started the
process of enhancing the like likes
kernel which was in continued onwards
the control groups and other things
which made a fairly weak insecure jail
from chroot into a very strong gel which
starting in out of then we've got docker
has exploited that serb technology and
become probably one of the most
important tools in forum doing
containers and so docker is getting
increasingly popular in many
organizations as the way forward because
he does this we plug the images and in
fact as we'll see later on you can
actually update these images dynamically
when they were running as containers you
can actually update the image and
recreate the containers on the fly the
images are layered from a base image
every time you add something keyed add
another layer to the image which is good
build them up and maybe have several
different images on the way the
registries manage the images and
repository is the
versions of the same image within the
registry and we've got darker machine
allows remote installation and
management of docker itself that's the
end of the session today thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>