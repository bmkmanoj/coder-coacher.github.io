<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Algorithms | Machine Learning Tutorial | Data Science Algorithms | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Algorithms | Machine Learning Tutorial | Data Science Algorithms | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Algorithms | Machine Learning Tutorial | Data Science Algorithms | Simplilearn</b></h2><h5 class="post__date">2018-03-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/I7NrVwm3apg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello I'm Mohan from simply learn and
today we'll talk about machine learning
algorithms machine learning is the
latest buzzword and you must have
already heard about it so in this
session we will talk about what is
machine learning and we will go a little
deeper and understand some of the most
common algorithms like linear regression
logistic regression and along the way we
will take some real-life examples now
most of you must be using snapchat to
apply filters on your photos but do you
know how snapchat recognizes your photo
on the screen and puts filters on it
even if there are multiple faces on the
photo
it applies filters in appropriate
position snapchat actually does this
using a technique called facial
recognition which in turn uses machine
learning the machine learning algorithm
detects the features on your face like
the nose the eyes and it knows where
exactly your eyes are very exactly your
nose is and accordingly it applies the
filters we will take a few more examples
as we move along and try to understand
how machine learning algorithms can be
applied to solve some of our real-life
problems so what will you learn from
this video we will talk about some
real-world applications of machine
learning we will also see and understand
what exactly is machine learning and how
it works we will also see the process
involved in machine learning the types
of machine learning algorithms and we
will also see a few hands on including
some code Python code of the following
algorithms linear regression logistic
regression decision tree and random
forests and K nearest neighbors okay so
let's get started let's consider some of
the real-world applications of machine
learning it's no longer just a buzzword
machine learning is being used in a
variety of industries to solve a variety
of problems facial recognition is one of
them it's becoming very popular these
days for security for police for solving
crime a lot of areas they
recognition is being used voice
recognition is another area it's
becoming very common these days some of
you must be using Siri that's an example
of machine learning and voice
recognition healthcare industry is
another big area where machine learning
is adopted in a very big way as you all
may be aware Diagnostics needs analysis
of images let's say like x-ray or MRI
and increasingly because of the shortage
of doctors machine learning and
artificial intelligence is being used to
help support doctors in analyzing these
images and identifying the segments of
any diseases where the focused is
another area and in fact Netflix has
actually come up with a very interesting
use case you all must be aware of the
house of cards show on Netflix so they
did an analysis on the customer behavior
they got data of their 30 million
customers information about where they
caused where they fast-forward a video
and they used this information they
provided this information to their play
rates and told them that this is what we
want this is this is what our audience
is looking for these are the areas that
interest them and these are the areas
where they get bored and the player it's
wrote the scripts accordingly this is
really really interesting and it
actually brings a new era so what is
machine learning machine learning is a
science of making computers learn and
act like humans so here when we say
computers very often what comes to our
mind is writing a piece of code or
program and telling the computer step by
step what to do but in machine learning
we don't do that the system learns on
its own we just provide past data
historical data what we call as labeled
data and the system learns during the
process what is known as training
process we tell the system whether the
outcomes are right or wrong and that
feedback is taken by the system
it corrects itself and that's how it
learns till it gives the correct output
for most of the cases obviously it won't
be 100% correct but the aim is to get as
accurate as possible so let's take you
through step-by-step process of machine
learning the first step in machine
learning is data gathering machine
learning needs a lot of fast data
especially we will see a little later
supervised learning we will see what
that is a little while but that's the
most common form of learning
so the first step there is data
gathering you need to have sufficient
historical data then the second step is
pre-processing of this data so that this
can be used for the machine learning
process the raw data cannot be used
directly so it needs to be pre processed
before it is fed into the machine
learning system the next step is to
choose a model so what kind of algorithm
and what kind of model within that
algorithm are we going to use and then
we need to train this model so before
training the model it's just like a
blank model and after training it
becomes a trained model and once we
train the model we need to also test it
to make sure it is predicting correctly
it's working fine that minimum errors
and subsequently it will be deployed and
once again it may have to be tuned from
time to time or fine tune from time to
time to improve the accuracy and so on
and so forth after training the model we
have to test the model during testing we
may have to we may find out that the
accuracy is not good enough so we may
have to tune the model we may change
some parameters and run through this
process once again perform the training
once again so it can be a nitrated
process and once the model is ready then
we deploy it to do the predictions what
are the different types of machine
learning algorithms machine learning
algorithms are broadly classified into
three types the supervised learning
unsupervised learning and reinforcement
learning supervised learning in turn
consists of techniques like regression
and classification and unsupervised
learning we use techniques like
Association
and clustering and reinforcement
learning is the recently developed
technique and it is very popular in
gaming some of you must have heard about
alphago so this was developed using
reinforcement learning primary
difference between supervised learning
and unsupervised learning supervised
learning is used when we have historical
data and we have labeled data which
means that we know how the data is
classified so we know the classes if we
are doing classification or we know the
values when we are doing a regression so
if we have historical data with these
values which are known as labels then we
use supervised learning in case of
unsupervised learning we do not have
fast labeled data historical labeled
data so we use techniques like
Association and clustering to may be
form clusters new classes may be and
then we move from them in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain target and it
is rewarded when it is moving towards
that target and it is penalized if it is
moving in a direction which is not
achieving that target so it's more like
a carrot and stick model so what is the
difference between these three types of
algorithms supervised algorithms or
supervised learning algorithms are used
when you have a specific target value
that you would like to predict the
target could be categorical having two
or more possible outcomes or classes if
you will that is what is classification
or the target could be a value which can
be measured and that's where we use
regression like for example weather
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud or not a fraud or if it is email
spam but it is spam or not spam so that
is a classification example so if you
know or this is known as labeled
information if you have the labeled
information then you use supervised life
in case of unsupervised learning we have
input data but we don't have the labels
or what
or Buddhists supposed to be so that is
when we use unsupervised learning
techniques like clustering and
Association and we try to analyze the
data in case of reinforcement learning
it allows the agent to automatically
determine the ideal behavior within a
specific context and it has to do this
to maximize the performance like for
example playing a game so the agent is
told that you need to score the maximum
score possible without losing lives
so that is a target that is given to the
agent and it is allowed to learn from
scratch play the game itself multiple
times and slowly it will learn the
behavior which will increase the score
and keep the lives to the maximum that's
an example of reinforcement learning so
we will discuss about some of the most
popular algorithms in machine learning a
few of them are listed here this is by
no means an exhaustive list there are
really a lot of algorithms available out
there but these are some of the most
common ones
what are these algorithms linear
regression logistic regression decision
tree random forests and K nearest
neighbors so let's look at each of these
in detail linear regression a little
history about linear regression Sir
Francis Galton is credited with the
discovery of the linear regression model
so what he did was he started studying
the heights of father and son to predict
the sons height or the child's height
even before he or she is born so he
collected enough data of the heights of
father and the respective sons he
plotted this data on the x and y axis
and he drew a line in such a way that
the distance of these points from the
line was the least which is now what we
call it as mean square error so at that
time this term was probably not there
but that's what he did and then he was
able to use this line to predict the
height of the child which was yet to be
born based on the height of the father
so that was the very beginning or very
initial phase of linear regression
algorithm so that was a little bit of
his
three but what is linear regression so
linear regression is a way of modeling a
linear model creating a linear model to
find the relationship between one or
more independent variables denoted by X
and her dependent variable which is also
known as the target and denote it as Y a
few examples are shown here let's say we
are going to plot the sales of ice cream
and the temperature so temperature on
the x-axis and the sales on the y-axis
and this is how the data would look and
if we draw a line in such a way that the
distance of each of this points from
this line is minimum that is known as a
regression line or the best fit line so
regression is all about finding this
line and it is called linear regression
because it is a straight line linear and
the equation doesn't have any nonlinear
component which means we do not have X
to the power of two or three or any of
this so this is simple linear regression
and simple linear regression is when
there is only of one independent
variable so there is only one X so that
is simple linear regression so let's see
how this is actually done so linear
regression is all about finding the best
fit line and the way it is done is in
recursive manner so first a random line
is drawn and the distance is calculated
from this line of all the points as you
can see in this example and that
distance is known as the error and to
ensure that there are no negative values
this is squared so we actually take the
square of the distance of the point from
the line and add it up and we make sure
that at the end this sum which is known
as the sum of squared errors is the
minimum so as you can see this is where
we start and then we keep changing the
line in such a way and calculate the
distance once again the sum of the
squares once again and here again you
will see that this is probably
not minimum and we keep changing this
unless until we get a line where this
value is minimum so here we find that
the sum of the squares of the distance
which is also the sum of squared errors
capital D denoted by capital D is
minimum so now we found the best-fit
regression line so this is a recursive
process
it's a nitrated process and this is what
behind the scenes this is what happens
when we try to do linear regression now
we will take an example of a linear
regression and we will actually
demonstrate this using Python and we
will be using jupiter notebook for this
case you're familiar with jupiter not
the wedding that would be very helpful
and if you need the data set please put
a comment under this video so that we
can provide you with the data set
before we go there just let's try to
understand what this example is all
about let's say there is a new joining
joining the company and we want to
determine what should be the pay off the
salary for this new joining and for that
we already have the labeled data which
is the details of our employees based on
their years of experience and their
salaries and for the new journey we have
the experience and we need to determine
what should be the salary so let's take
a look at the demo so before we go into
the jupiter notebook let's quickly
review the code what the code is doing
the first section is about importing
libraries some of you who are familiar
with the python is already knowing this
you don't have to go into details and
then we import the data set and then we
visualize the data just to get a quick
idea about how the data is looking and
then we split the data into training and
test data sets this is a common
procedure in machine learning process
any machine learning process and the
overall training a test process we do
with two different data sets so that's
what we're doing here and then we build
or train our model the linear regression
model and then we do the testing and we
find out what is the errors and
visualize our results and this is all
the test results look
and this is how the training result
looks all right and then we calculate
the residual stresses walls are nothing
but the errors there are a couple of
ways of measuring the accuracy the root
mean square error is the most common one
our MSE and in this case we got root
mean square error of 58 which is pretty
good and that's our best fit all right
so now let's go into Jupiter nando and
take a look at by running it like okay
so this is our code for the linear
regression demo and this is how the
jupiter notebook looks and the scored in
the jupiter notebook looks I will walk
you through the chord pretty much line
by line and let's see how this works the
linear regression so the first part is
pretty much a standard template in
pretty much all our code we will see
this is importing the required library
so numpy is a library matt Gottlieb is a
library pandas and so on so these
libraries are required each of these
libraries have a different purpose or
some of them are required for
manipulating your data some for plotting
as the name suggests matplotlib and so
on and so forth okay so let's go ahead
and import all these libraries and then
we will import our data so in this
example what we are trying to do the use
case in this particular example is we
have some historical value of salary
data and now we want to build a model so
that we can predict the salary for new
employer person who is joining new and
we will use the same the characteristics
that were available to us or the
features that were available to us and
we will try to predict what will be this
salary of this new person okay so that's
the kind of the use case so let's go
ahead and load the data and let me
introduce a cell and see how the data
looks so salary underscore data so we
have basically two features right so
it's pretty much like a simple linear
regression we are trying to do so we
have years of experience and
and in this case what we are trying to
do is the this is our predictor so years
of experience is our predictor and we
are trying to predict what the salary
would be so once again so X is what is
known as the predictor and why is that
target so number of years of experience
we are taking as input and we will try
to find what will be the salary of this
person based on that okay so that's what
we are doing to do it so I have shown
what is how the overall how the date
data that has been important her looks
now let's take a quick look at these
extracted values or extracted columns as
well so let me put in X here and see how
it looks so this basically one column
and same way if we put Y here we will
see what exactly I'm sorry this has to
be in small over case so this is the
salary information okay so this is our
data is and then we can do a little bit
of plotting of the data to play around a
little bit because we imported the data
typically we want to do some exploratory
analysis how they compare with each
other and so on and so forth how they
are correlated so let's draw a quick
plot a bar plot and see how the years of
experience are being seen here right so
this is how the bar plot looks similarly
let's do one more which is basically
more like this is horizontal bar plot
okay so how many people and with what
experience and so on and so forth so
what this shows us the count how many
records are there with a given
experience and things like that
okay so this is another way of
visualizing the data and this is a third
view and this is one more view and then
we can do a quick heat map so there are
there is only one actually there are
only two variables so there is a there's
not a lot of plotting that we can do or
not a lot of visualization that we can
do since there are only two variables
but now the less whatever is possible we
can do a quick to get a quick idea about
how the data is
looking and how the variables are
related to each other is there a
correlation and things like that all
right so once we are done with that this
is the most important part of our demo
here which is basically this is the
beginning of our training process so the
first thing before we start the model
building and model training process is
to split the data into training and test
data sets okay
now whenever we do any machine learning
exercise especially supervised learning
we never use the entire label data for
training purpose the reason being then
we will not be able to correctly
evaluate how well the training has
happened okay so what we do is we split
the data we take a portion of the data
we we call that as the training data set
and we set aside some portion of the
data we call that as a test data set we
use the training data set to actually
perform the training to train our model
and once that is done we use the test
data set to check how well the model has
been trained how accurately it is able
to predict okay now the reason is in for
our with our test data set also since
that is also labeled we allow the model
to predict the values and we compare it
with the labeled information to see
whether it is predicted correctly or not
okay so that is the reason in all
machine learning activities whenever we
perform machine learning we especially
training of a model we split this data
into training and test data set and
that's what we are going to do here now
we there is no need to write any
separate code for that there is a
readily available function for that
train underscore test underscore split
so that's what we are going to use here
it takes the data set x and y is the
data set and in addition it takes a
parameter which tells how the data has
to be split so for example here it says
test size is equal to 1 by 3 that means
it is 33% right that every 0.33 person
so one third of their data you want to
set aside for tests now there are no
hard and fast rules as to what should be
this split of test
and training data set is a matter of
individual preferences some people would
like to have 50:50 some people would
prefer 80/20 and so on and so forth so
it is completely up to the individuals
to to decide on that so in this
particular case what we are doing is we
are setting aside one third of the data
set for testing purpose and two thirds
of the data set for training purpose
okay so that's exactly what we are doing
here now the next step is to create an
instance of linear regression so the
linear regression model is readily
available so we create an instance of
the linear regression model and give it
a name like LR and then we call the frit
method of the linear regression model
now this fit method is common across all
the algorithms or any algorithm you use
if you want to start the training
process you call the fit metal ok and
then we pass the training data set now
training data set we send the predictor
as I was saying or the independent
variable and also the dependent variable
if we pass both of them and the system
will basically learn based on this so
now the training has happened training
is completed and now we need to see
whether it is predicting correctly or
not we need to test it right so for
testing we have another method called
predict again this is common across all
algorithms any algorithm you use you
will have this predict method for
testing your your model okay or for
actually predicting the values whether
during testing or when you actually
deploy the model okay now here predict
will take only one parameter which is
the independent variables right the
reason being the dependent variable is
what it will predict right so Y
underscore test we don't have to pass
whereas here Y underscore train we
passed for training purposes but Y
underscore test we do not pass because
that is what the model will predict so
we don't have the pass that and
and the model will predict and that is
what we call it as Wireless Corp red but
we will use a wireless core test to
compare with what the model has
predicted and thereby we can determine
how accurate the model is okay so let me
go ahead and run this code so it has
done with the testing or with the
prediction of the values so these are
the predicted values and now we can
visualize you can generate some plots to
visualize this information so let's plot
this training set results information
and see how it looks so what what we are
doing here is basically we have the
training data set plotted the salary
versus the years of experience right
this is the training data that the dots
basically and then this line is what our
model has so this is the best fit so
linear regression what is a really a
linear regression process we find a
equation of a line which is the best fit
so our model this is the equation or
this is the line that our model has kind
of come up with saying that this is the
best fit model okay so as you can see
intuitively it feels okay because it is
passing through pretty much the middle
of our data set okay so this is for the
training part now we do the same for our
test as well and see how it is doing or
how it looks here also it looks pretty
good because the line passes pretty much
in the middle of the overall data set
that's what we are trying to do here all
right and then how do we measure the
accuracy
so this residuals are nothing but the
errors the term residuals is nothing but
the errors we have seen in the slides as
well so we calculate the accuracy of our
model by calculating the various
residuals we have like mean square error
then we have the mean absolute error and
then we have the root mean square error
they are pretty much they're very
closely related this is nothing but the
error
and actual value right so for example
let me explain with this this particular
light so what our model says is if the
value of the years of experience right
so let me take at this point maybe if
the value of the years of experience is
4 then the salary will be according to
our data that salary probably should be
what is this may be around 58,000 right
and there are of course three points
three data points pretty much around
this four years experience one may be
around 58 or in fact two of them are
around 58 and there is one more which is
probably 60 okay now these are the three
data points whereas our model predicts
that if the years of experience is four
then the salary should be sixty thousand
let's say okay let's assume this goes to
sixty thousand which means that one of
the data points is very accurately
predicted by our model but there are two
of them which are off so there is an
error for these two values and what is
that error the error is nothing but the
distance of this point from this line
right the distance of this point of each
of these data points from the from the
line is the error okay so that's
basically for each of the values it will
correct so it will calculate rather and
and instead of taking the absolute value
we kind of take a square of that so
because the value can be positive or
negative so in order to avoid a
cancellation so there are some positive
values there are some negative values
and they get cancelled out what we do is
we take a square of that so that is
basically what is the root mean square
not the mean square error and then if we
take a square root of that it becomes
root mean square error right so this was
explained probably in the slides as well
so that is what we will calculate here
and we print it okay and these values
the root mean square error the mean mean
squared error and the mean absolute
error the lower these values are the
better so the accuracy is higher if
these values are lower so in a way it is
inversely proportional okay so that's
the way we measure the accuracy of our
linear regression model alright so that
brings us to the end of this demo and we
will continue with the other demos
alright now that we have seen linear
regression let's take a look at our next
mission learning algorithm which is
logistic regression now what's
interesting about this algorithm is that
while it says regression the name has
regression in it but keep in mind this
is not used for regression but this
algorithm is used for classification
many people get confused by this name so
you need to be aware of it linear
regression is used to solve regression
problems where we are trying to predict
a value whereas logistic regression is
used to solve a classification problem
so we are trying to find for example
whether a person will repay the loan or
not or whether we want to find whether
this images of a cat or a dog so this is
a classification problem ok so just that
you should be aware of the name so in
this slide we are talking about whether
a person credit card user or credit card
holder will default on the payment so we
can create a profile we have historical
data and of people their income and
credit card balance and we have examples
of people who have defaulted and people
who have not defaulted and based on that
we let the system learn to predict this
particular value whether the person will
default on the payment or not now how
does the logistic regression work if we
try to draw a straight line to determine
the probability so we are trying to
predict whether this person will default
or not ok so if we draw a straight line
then the predicted value can exceed 0
and 1 which is not a good idea so the
probability is calculated between 0 &amp;amp; 1
using
what is known as a sigmoid curve so it
is not a straight line but we use a
different formula a different mechanism
to find the probability and find the
value between zero and one and the
formula for that is P is equal to 1 by 1
plus e to the power of minus Z here Z is
actually the linear equation that we
used in Figure 1 which is M 1 X plus C 0
so this is fed to P and the value is
calculated between 0 &amp;amp; 1 and we will see
actually mathematically you will see
here that if Z is negative and much much
higher a very high value it will the
equation will be 1 by 1 plus 0 which is
equal to 1 so we will see here that if
the value of Z is positive and it is
some high number then the value of P can
at the most be equal to 1 because this
portion here is 1 by a very high number
let's say infinity so 1 by infinity is 0
so this whole thing becomes 1 by 1 so
that is equal to 1 T is equal to 1 so
that is where the maximum value it can
achieve is similarly if the value is
negative if the Z value is negative or
negative and very high value then this
will become very high and therefore the
denominator will become infinity so 1 by
infinity is 0 and therefore P is equal
to 0 so that's the way the sigmoid
function works this is how the graph
looks and there has to be a threshold
value which is like in this case point 5
so if the value is greater than 0.5 we
consider the output as 1 whereas if the
value is less than 0.5 we considered the
value as 0 because remember let me go
back in this case it doesn't exactly
give us a 1 or a 0 okay so we need to
keep that in mind it doesn't give us
exactly a 1 or a 0 it will cure a value
between 0 and 1 irrespective what the
value of z is it will give us a value
between 0 &amp;amp; 1 it's like the probability
between 0 &amp;amp; 1 now we have to have a
threshold and then based on what the
value
is if the value is greater than 0.5 then
we say okay this probability is 1 and if
the value is less than 0.5 the
probability is 0 so we decide that based
on the cutoff for a threshold mile so
for example if we continue on that if a
person having a balance of maybe this is
1750 then the probability that he will
repay the loan is 0.2 which means which
is less than 0.5 the threshold value
which we have said that means the
probability is 0 which means the person
will not repay whereas if the balance is
somewhere in the range of 2250 maybe
which is this red dot then the sigmoid
function calculates the probability of
default as point a which means the
output is 1 which means the person will
default on payment okay so that is how
the logistic regression works so in this
case it says that okay this person will
default it is classified as a default
IRR in this case the second one and in
the first case it is a non default so
these are the two different classes of
these two different cases so let's move
on and see how logistic regression is
implemented so this is another example
of logistic regression can we predict
whether a person is going to buy an SUV
based on their age and their estimated
salary so these are now two inputs and
based on that can we predict whether
this person will buy an SUV or not so
this is a logistic regression problem
and we are going to demonstrate this
using Python code and in jupiter
notebook and before going into jupiter
notebook let's take a look at how we go
about solving a strong and how the code
looks so this is the implementation of
phlogiston regression this is the code
the Python code I just take you very
briefly at a high level what each parts
are so first section of course is
importing the libraries and then we
import the data set and within this data
set we just take for performing or for
training our model we only take the
independent variables into one create
one vector and then we extract the
labels separately and then we visualize
our data this is how the data looks
and then we split the data into training
and test set like we did in linear
regression and we do some feature
scaling as well which improves the
performance of our model and then we
train and test the model so these are
the test results and we visualize the
test results so this is the
visualization of the training results
they look pretty good the classification
then they are reasonably accurately
classified the red dots and the green
dots and there are a few of course
miscalculations or miss classifications
but by and large it looks pretty good
and this is the visualization of the
test results again looks pretty good and
then we evaluate our model and this is
what we'll be doing in the code as well
and for this we use what is known as
confusion matrix now let's try to
understand what this confusion matrix is
I know the name itself is confusing but
actually it is very simple now once we
predict these values and compare with
the actual values we can create or
represent the results in the form of a
matrix so we have altogether 134
observations and if we put it in a
tabular form the predicted values and
the actual values first of all in order
to identify whether or model is more
accurate or less accurate the criteria
is that the values along the diagonals
be maximum so for example this is 79 and
this is 38 the sum of these should be
maximum so what do we mean by that that
means that if we have a perfect model
then this sum will be equal to 134 and
the values here will be zero so that is
an ideal perfect logistic regression
model that is of course very rare but
that is the a the sum of these numbers
are the maximum numbers should be in the
diagonals and in the other cells there
should be as less as possible okay so
here we see in this case that it is 79
plus 38 and so that is equal to 117 so
117 of them have been correctly
which gives us an accuracy of 87% so out
of 134 117 have been correctly predicted
and the six plus 11 17 of them have been
incorrectly so this is 6 plus 11 17 of
them have been misclassified so which is
about 0.13% so we have an accuracy of
87% okay
so I hope the way we calculate or with
the way we find accuracy from confusion
matrix I hope it is not clear by the way
once again a quick reminder if you need
this data set to perform this on your
own please put a comment and under this
video and we will send you the data set
all right so let's go and check in
Python notebook how exactly this is done
all right so this is the demo of
logistic regression and here what we are
doing is we have taken an example of a
data set and a scenario where we will
predict whether a person is going to buy
an SUV or not and we will use logistic
regression for this and the parameters
we will take are for example the
person's age is salary and a few other
parameters we will see very quickly what
those are okay so the first step we
import the required libraries so that's
what we are doing in this particular
cell like for example numpy MATLAB
pandas so for performing any preparation
of the data before we actually launch
into the learning process and then we
load the data so let me just introduce
one more cell and see how the data looks
so why don't we do that here and if I
say data set and then if I run this this
is how our data looks so these are the
parameters or the features as we call it
in machine learning language we have
gender age and estimated salary user IDs
also there but it's not really a feature
it will probably not contribute so we
will not be using this we will primarily
be using these three columns and in
technical terms these columns are known
as predictors and then we have the
labeled value this is known as our
target and this is the labeled value so
it has 0 or 1 so we are basically
performing of
reclassification 0/1 whether the person
will purchase or not purchase zero means
he will not purchase one means he will
purchase okay and the the other way of
also looking at it is from more from a
mathematical perspective these are our
independent variables gender age and
estimated salary are our independent
variables and purchased is our dependent
variables so in our equation like y is
equal to Oh something something in 1x
plus m2 X and so on this is our Y ok all
right so now let's move forward what we
will do next is to extract this or
separate out the independent variables
and the dependent variables how do we do
that so we load them into X capital X
and Y and what we are doing here is we
are taking the second and the third
column we are taking these three right
so gender age and estimated salary so
that is what we are taking here to up to
three and then y is basically the last
column which is the fourth column so
let's go ahead and extract that and once
again why don't we take a look at how
the data is looking so let me just print
X so we only yeah we're basically taking
two and three so what is two and three
two and three is our age and our
estimated salary yeah that's right so
these are the only two independent
variables and our dependent variable is
obviously the last one whether the
person will buy or not so we have the
data loaded now let's now that we have
data in Python let's take a quick look
at how the data look so in terms of
visualization let's visualize the data
and perform a little bit of what is
known as explore a tree analysis right
so as a data scientist whenever you get
new data you just play around and see
how the data is looking before you
actually launch it to the actual
training god the modeling part of it so
let's run a small heat map and see how
the data looks so we have just passed
the entire data set here and this is how
the heat map looks how how they are
related how the various what you call
features are related to each other
this is a scale here very dark means
basically there is pretty much no no
correlation this is in a way to measure
the correlation as well and light means
there is a very high correlation so as
you can see a value or a feature will
have very high correlation of one to
itself so the user ID and this is user
ID it has a very high correlation to
itself similarly purchased at purchase
data so the diagonals will be very high
so that is really not relevant but then
the other values is what we have to see
for example there is a correlation of
0.6 in this area which is basically if
you take age and purchased right whether
the person has purchased and the age so
that's that's a quick look at exploring
the data so that's all we are doing here
here's where the actual that the crux of
this code is so from here onwards what
we do is the first step before we start
the training process is split our data
into train and test training data set
and test data set so whenever we perform
any machine learning activity we have
let's say our label data we never pass
the entire data to the model because
when we are then when we are measuring
the accuracy whether the model is
performing well or not we will not be
able to do that if we use up the entire
data for training purpose so we split
the data into what is known as a
training data set we train the model
with the training data set and then this
test data set which is kept separately
which the model has not yet seen we use
that to check whether it is able to
identify perform the predictions
correctly or not
okay that will give us a higher accuracy
or or a better estimate of how the model
is doing all right so this is a very
common practice in machine learning so
whenever we have whenever we perform
training of the model we split the data
into training a desert now how do we do
this we don't have to write any special
code there is already a method available
which is strain underscore test
underscore split and we just call this
method only thing is it takes these
parameters and we have to specify
especially this parameter which tells
the
method how the data should be split now
when we are splitting the data into
training and test there are no hard and
fast rules how we split this data
some people prefer 5050 some people
prefer 80/20 and so on and so forth so
that is flexible and it could be to some
extent individual preferences so in our
case we are splitting this data into
7525 which means 75% of the data will we
will use for training 25% will use for
test so that is what we are specifying
here as a parameter we say test
underscore size is equal to 0.25 that
means put keep 25% of the data set aside
25% of the data as test data and
therefore the remaining 70% will be used
for training all right so let's move on
I execute this then we perform what is
known as feature scaling so what is
feature scaling usually what happens is
the the values that we have in the data
sometimes they can be some numbers will
be very large some will be very small
so that there is a huge variation so in
order to if we have that and if we use
that as it is the accuracy of our model
may come down so we have to somehow
normalize these values so that is what
scaling does feature scaling to us and
again we don't have to write any special
code for this there is a standard method
available or standard class available
called standard scale-up so we just
create an instance of that and pass our
data for scaling purpose okay so let's
go ahead and do that now so this is all
what we have done is more like a data
preparation so we chose what are the
parameters what little features we want
we did the feature scaling and we split
the data now everything is ready next is
to to start the actual training process
so this is the most crucial part of the
code so here as we said we will use the
logistic regression model so we have to
create we create an instance of
regression model so I call that as
classify or you can give any name and we
just do some kind of initialization
random value initialization so
we call this as a random state or we
assign it as a random state is equal to
zero and we have an instance of the
logistic regression model and we use
that model the fit method so any any
algorithm that we have when we want to
perform the training we have to use the
fit method okay we call the fit method
now let's say instead of logistic
regression we are using K K nearest
neighbors algorithm you will have K
nearest neighbor dot fit or linear
regression linear regression dot fit
right so that is where that is the way
it works and we passed the training data
set the X is the independent variables
and Y are the labels so we pass both of
them for the training purpose so once
the training is done if we get the
results of or the model is strained okay
the model is straight now the next step
is to test the model how well the model
is performing this is where we use the
test data set we pass the test test data
set and here there is a another method
that we have to use that is known as a
predict method so again this is a
predict method is common across all
algorithms that we have so any algorithm
we have we have to call the predict
method for performing the test or even
when we deploy it when new data comes in
we are trying to predict for new data
set we always use predict okay so fit is
only further for training okay then
predict is use for testing and for the
actual performance of the prediction now
one thing we need to observe here again
is we only pass the X part of it and not
the Y part of it so this can always
sometimes this can lead to confusion in
when we are calling fit we are passing x
and y when we are calling predict we are
only calling X the reason is the y part
is what the model will predict so you
don't have to pass the y part and so Y
is what the model will predict for us so
you don't pass here we had to pass it
because the model has to learn from the
existing values so that's why we had to
pass label as well
when we are testing it the model will
calculate this and that's the reason we
do not pass the y-value okay
so now when we run predict it will
calculate those values or find those
values and it will put it in wireless
corporate and that is what is being
displayed here and as you can see the
values can be either 0 or 1 which means
the person will buy the SUV which means
which is one widget which is a value of
1 or 0 means the person will not buy the
ICV ok now one more thing we need to
remember is while we have not passed the
Y values or the labels to the system but
we have these labels available with us
right why underscore test is available
with us that is what we have to use to
find out how accurate the system is so
it has given us the results now we will
compare these results which the system
has predicted with the actual values
that we have which is available in Y
underscore test so we will see how well
how that is done how that accuracy is
calculated in a little bit ok now that
the training is completed we can go
ahead and do some visualization of the
training result so let me execute this
code and take a look you know form of a
plot so this is how the classification
is done so these are like the class this
is like the class boundary the green
color belongs to the class 1 and the red
color belongs to plus 0 and these dots
indicate it has been misclassified so
some of them have been misclassified so
that's what we are seeing some red dots
in the green area and some green dots in
the red area right it as if you probably
it's not very clear but you can see here
that there are also red dots in the red
area and the green dots in the green
area so those are the data points which
are correctly predicted and these in the
opposite locations are the ones that
have been misclassified rather same for
tests as well so test data set also if
we visualize with plot we will see
something similar but this also gives us
at a high level whether is that a major
in a major way are there major
misclassifications
or there are only
you hear and there it gives a little bit
of an idea about accuracy as well but
then that is not sufficient for us right
so we have to quantify this accuracy so
how do we do that
we do this using what is known as
confusion matrix so it's readily
available method or a class just there
and it takes two parameters which is the
you remember I told you we will be using
white s so this is where we will be
using Y underscore test which is our
labeled of value or label data set and
this is what is predicted by our model
okay Y underscore thread is predicted in
our the predictions here these are the
values out here we saw this is y
underscore pride okay so we we passed
these two parameters and we create a
matrix and this matrix is known as
confusion matrix now what is the
significance of this first thing is that
the total number of values here is equal
to the number of data sets in your test
data okay so let's go ahead and in the
next cell let me just add up these
values so 65 plus 3 plus 8 plus 24 so
how many other there are hundred in
number so there are totally hundred
observations in your test data set that
is the first point then the second point
is the number in the diagonals right the
total value in the diagonals if that is
a high that indicates a higher accuracy
the higher that number is the total in
the diagonal the higher the accuracy
okay and if we have quite a few numbers
non diagonal locations that means
accuracy is not very high so here looks
like the accuracy is pretty good now we
can actually quantify the accuracy by
using these numbers so what we do is the
sum along the diagonals we have to take
and divide that by the total observation
so 65 plus 24 is what we have along the
diagonal and we have hundred
observations so 89 by 100 is our
percentage so we got about 90% accuracy
for this
model okay so that brings us to the end
of this demo and we will move on to the
next step
alright so the next algorithm is
decision tree and random forests they're
being taken together because they are
very closely related so what is the
decision tree mrs. Hulme tree is another
algorithm and unlike logistic regression
which is only used for binary
classification decision tree can be used
for classification as well as regression
even though it is more popular for
classification and it can be used to
classify multiple classes as well not
just binary classification so how does
decision tree work one of the good
things about decision trees is that it
is easy to represent and show how
exactly it works and therefore it is
very easy to understand as well now
let's take an example let's say a person
receives a job offer and he needs to
decide whether to accept the job offer
or not so we will use decision tree to
come to the decision to accept or not to
accept so this is how first of all our
decision tree looks it is actually an
inverted tree so the root is at the top
okay so and that is known as the root
node the node where the tree starts is
known as the root node and then we have
some inner nodes or decision nodes and
we have the nodes where we have a
decision either positive or negative
these are known as leaf nodes or
terminal nodes okay so we have the root
node and we have leaf nodes or terminal
nodes there can be multiple of them and
in between we have decision nodes or
internal nodes there are different terms
used so need not be hung up by the exact
terminology now let's say we have to use
this decision tree to find out whether
this person will accept the job offer or
not so first thing he considers is the
salary is the salary greater than 60,000
if no it's a clear decision the offer
will be rejected so we reach a decision
therefore this is a leaf node now if the
salary is greater than 60,000 it is not
a clear-cut decision because there are
probably other factors
based on which the decision needs to be
made for example what is the commute
type if the commute time is greater than
1 R then the is rejected ok so even
though the salary is greater than 60,000
so this is again a leaf not if the
commute time is less than 1 R still it
is not a confirmed decision because
there are still a few other factors for
example performance incentives are there
sufficient performance incentives as a
part of this offer if no then again the
offer is rejected and this is another
leaf node if yes if there are sufficient
performance incentives then offer is
accepted so this is another leaf not by
the way keep in mind that these are all
the leaf nodes this is a leaf node this
is a leaf node and they of course belong
to different categories in this case it
is still a binary classification so this
belongs to one class which is accept
offer and these leaf nodes belong to a
different class which is reject offer so
let's take an example and see how we can
solve this problem using decision tree
let's say we have to implement a
classification algorithm for pi Phasis
patient and the problem is a bunch of
kids have been have undergone a PI for
such surgery and we need to predict
whether hypotheses present in them or
not and how can we do this using
decision tree algorithm so this is how
the classification tree for kyphosis
looks so it starts with if the age is
greater than 8.5 so this is how the
decision tree looks so the first
criteria is the vertebra the number on
which the surgery has been performed if
it is greater than 8.5 then we need to
perform further analysis and look at
other criteria if it is less than 8.5 it
is clear that Ephesus is present and if
it is greater than 8.5 then we check
whether the vertebral operated upon is
greater than 14.5 or not if it is
greater than 14.5 then that force is
absent if not then the next criteria is
the H if the person's age is less than
55 years then kyphosis is absent but if
it is greater than 55 years
and further analysis is required to see
if the person's age is greater than 111
years which is of course almost
impossible but yes so if if no then
there is no typhus
then we check if the age is greater than
111 years which means the person is
between 55 and a hundred and eleven
years if the person is between 55 and
111 years that is this path that means
typhus is present otherwise it is absent
so this is very easy to understand
decision tree and now what we are going
to do is we're going to implement this
in Python in our Jupiter notebook and we
have a data set that is available for
this and once again if anyone wants this
data set please put a comment under this
video and we will be more than happy to
share the data set with you before we go
into the Jupiter notebook let's take a
look at what the code is doing what are
the various sections of the code
so first one of course is loading the
libraries as usual then we import the
data and then extract the independent
variables and then separate the labels
which is the dependent variable and then
we visualize the data this is how it
looks and then we as usual split the
data into training and test data sets
and then perform the training of the
decision tree and then we test that
model with our test data and then since
this is a classification problem we
evaluate this model using the confusion
matrix remember we talked about this in
the previous example as well so this is
the confusion matrix for this particular
problem and here you can see that 16
plus 3 19 of them have been correctly
predicted out of 25 which gives us a 64
percent accuracy and this 4 plus 2 6 of
them have been misclassified just to
recap once again how do we determine
from the confusion matrix whether this
is accurate or not the numbers in this
diagonals should be maximum so here we
see that out of 25 and is equal to 25
which is the total number of
observations out of 25 in this case in
this diagonal we have 16 plus 3 19 of
the values are here so that's the reason
we feel there is good accuracy which is
64 percent they have been correctly
classified
64% of the observations have been
correctly classified by this model
whereas 24% have been misclassified
which consists of this four plus the
store so that's how we use the confusion
matrix to determine the accuracy of our
decision tree model so let's go into the
jupiter notebook and take a look and run
the code and see how it looks
so this is our Python notebook for
decision tree and I will take you
through the code not line by line of
course but we will see the blocks as
always the first block is to import the
required libraries like pandas and numpy
and so on then we import the data for
this we are using kyphosis dot csv file
and then we extract the independent
variables and then the dependent
variable dependent variable is the
target which is our classification and
also we call it as the label data
whether it is whether this person
individual has typhus or not and then we
will run a little bit of initial
exploratory analysis on the data so as a
data scientist whenever you get new data
you do that so we plot a H against
whether the person has confesses or not
and then we plot the various parameters
against each other so for example this
is like H versus the number and by the
way the data how does the data look
let's take a look at the data this has
primarily three columns one attribute
against the other so for example this is
age versus the number the number versus
the start and so on and so forth so this
is just to give get a quick idea about
the overall how the overall data looks
so we can do a few more visualizations
like with respect to the age and count
the number of vertebrae that have been
operated upon and the red color
indicates kyphosis is absent and the
blue color indicates Caiaphas's is
present so this is another view and so
on so we can basically do as much of
expert
analysis and then we start training our
model so this is where we split the data
into training and test data set like in
all other machine learning examples
earlier also and we can visualize the
data or just quick take a quick look at
how the data looks just the first four
or five entries here if we do the head
that's what the head function does and
similarly Y as well we can take a look
at how the target is looking and then we
can get into the training of the
decision tree so we train our model with
the training data set using this fit
method and then we test whether the
model is working fine or not using the
test data set and this is the result of
our test data and then we evaluate the
model and for evaluating we use the
confusion matrix so just let's take a
quick look at the confusion matrix and
see what is the accuracy so we get about
16 right as I mentioned the diagonals
I'll use is what matters
we have 16 of them correctly identified
out of 25 so that gives us about 62 I
think 70% accuracy so that is using
decision tree now instead of decision
tree let's check what happens if we use
random forests we use let's say the or
forest consists of 100 trees and we do
the same we train our model and then
with the test data set do we try to test
the model and here we see that we get 19
of them out of 25 correct so that is a
much better accuracy so this is about
76% so 19 out of 25 is 76 percent
accuracy and we saw in by using the
decision tree we got only 64 percent
that is 16 by 25 was only 64 percent so
this is a quick demo of implementation
of decision tree so once we evaluate the
model the next thing we can do is what
happens if we want to improve the
performance so that's where random
forest comes into play so in instead of
using a decision tree what happens if we
use a random forest and what is the
random forest it's very similar to
decision tree only thing is that instead
of using one tree we use a number of
trees that's why it's called a forest so
we use multiple trees and we take like a
average or a voting of all the trees for
each observation and we finally
calculate on whether we finally classify
each observation based on that so in
this case it helps in improving the
accuracy so as we have seen here if we
take in decision tree the accuracy now
comes to 76 percent right in the
previous case it was only 64 percent
using just the decision tree now when we
use random forests the performance has
improved to a good extent and it has
become 76 percent so random forest
usually helps in increasing the accuracy
when we are using decision trees as our
algorithm ok the last algorithm is the K
nearest neighbors algorithm this is
again a classification algorithm and it
is actually very simple and
straightforward very easy to understand
as well in this case let's say we have
historical data of heights and weights
and we also have the labels so if the
height and weight is in this range
these are caps and if the hypes and
weights are in this range these are dogs
so this is one class and this is another
class and now if we get a new point or
new data set and we find that for this
particular animal the height and weight
if we plot it as here so how do we
determine whether it belongs to the cat
class or the dog class that is where the
K here comes into play so when we are
using this algorithm we need to specify
what should be the value of K so let's
say we say we determined that the value
of K is 3 so what we will do is we will
find out the nearest objects nearest
three objects to this data point that we
have so let's go to the next slide here
okay and we define K as three so what
happens we find the nearest three
objects
to this data point and then we find out
what is the maximum number of items
which class the maximum number of items
belong to so in this case there are two
cats and there is one dog
therefore we determine that this new
data point belongs to the cat class now
what happens if we say K is equal to
seven in that case these are the seven
data points which are nearest to this
new data point and now you see that
instead of cats dogs are more so which
means that now the point or this data or
this new creature belongs to the class
dog so this is while this is a slight
drawback but by trial and error we
should be able to find out the right
value of K and once we use that that's
how we train the model now let's use K
nearest neighbors and take an example to
solve one of our previous problems that
we did using logistic regression whether
a person is going to buy an SUV or not
based on the age and estimated salary so
before we go into Jupiter note let's
again take a quick look at the chord so
what are the various sections in the
chord as always we import the libraries
we load the data set visualize the data
we split the data into training and test
data set we do some feature scaling and
then we train our model and then we test
our model we visualize the training set
results and then we visualize our test
results and both of these seem to be
looking pretty good and then we evaluate
our model using the confusion matrix so
remember whenever we use a
classification algorithm the way to
evaluate our model to find out the
accuracy of our model is the confusion
matrix we have seen in logistic
regression we have seen in decision tree
and now in KNN as well so in this case
once again the total number of
observations is hundred and we see that
along the diagonals which is the number
we need to consider there are 93 values
which means we achieve 93 percent
accuracy
and a few of them that is only seven of
them are misclassified so this is a
pretty good accuracy compared to our
previous example so let's go and check
how this looks
give it another so this is my fightin
notebook for k-nearest neighbor
implementation and I will quickly walk
you through this code the first block is
as usual we are importing the various
libraries like numpy pandas and
matplotlib
and then we load the data set and we
split the data into training and test
data sets and then we do feature scaling
and then we start training or not and
here we are specifying k as 5 ok and now
the data set is trained now we go and
test it with our test data set then we
evaluate the model and here we get that
as you can see in the diagonals there
are 73 entries so total is 73 which
means out of total of 80 73 have been
classified correctly which is about 90%
accuracy we are getting and seven of
them have been misclassified we can now
visualize the training results so this
is these are the zeros and these are the
ones this is the training set so there
are of you
as you can see misclassifications the
green ones in the red area and the red
ones in the green area or the
misclassified values and same way we can
check to visualize the results of the
test data set as well and here again we
see there are a few little bit of miss
classification and that brings us to the
end of this fightin notebook demo for
KNN classification so let's summarize
what we have learnt in this video first
of all we started with an example of
applications of machine learning then we
learnt about the definition of machine
learning and then what are the various
steps or processes involved in machine
learning and we then looked at some of
the examples of machine learning
algorithms we took a look at linear
regression we then saw logistic
regression then decision trees and
random forests and last
but not least the k-nearest neighbors
alright so with that we come to the end
of this video
thank you very much for watching if you
have any more queries or questions
please feel free to visit us at
www.simpleandsensational.com</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>