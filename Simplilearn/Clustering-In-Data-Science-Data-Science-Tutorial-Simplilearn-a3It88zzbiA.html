<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clustering In Data Science | Data Science Tutorial | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Clustering In Data Science | Data Science Tutorial | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clustering In Data Science | Data Science Tutorial | Simplilearn</b></h2><h5 class="post__date">2017-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a3It88zzbiA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's define clustering in this stream
in simple terms clustering entails
organizing objects into groups in which
the members some aspect of theirs is
similar a cluster is defined as a
collection of objects which are similar
between them and are dissimilar to the
objects belonging to other clusters
clustering helps you understand the
natural grouping in a data set in
cluster analysis you can find a
structure in a collection of data that
is unlabeled you apply the clustering
algorithm to raw data and you have
clusters of data clustering is used in
data mining information retrieval text
mining web analysis marketing medical
diagnostics and so on this screen shows
you the differences between
classification and clustering in
classification the training data has to
specify what you are trying to determine
or the classes that is supervised
learning supervised learning uses the
training data to infer a model that you
can apply to the test data examples are
support vector machines perceptrons and
maximum likelihood in clustering the
training data does not specify what we
are trying to find out or clusters that
is unsupervised learning unsupervised
learning it requires no predefined
classes or training data inferring the
model and applying it to the test data
rely exclusively on the test data an
example are k-means and fuzzy clustering
the goal of classification is predictive
whereas the goal of clustering is
descriptive distance measures and
similarity measures are used to
determine whether two objects are
similar or dissimilar now let's look at
the use cases of clustering clustering
models have many use cases that are the
same as that for classification a few of
them are as follows grouping content on
a website or products in a retail
business segmenting users or customers
into different groups based on behavior
characteristics and the metadata
segmenting communities in ecology
finding clusters of similar genes
image segments for use in image analysis
applications such as objective detection
the major clustering methods are
distance based hierarchical partitioning
and probabilistic a good clustering
model will give you clusters where the
inter class similarity is high and the
inter class similarity is low in
distance based method for continuous
valued variables the distance between
two instances can be measured by
estimating the Minkowski metric or the
Euclidean distance you saw earlier in
the course standardizing data can help
mitigate the effects of the choice of
measurement unit on the clustering
analysis you can use contingency tables
for binary attributes simple matching
for nominal attributes mapping the range
of ordinal attributes onto 0 1 and
treating them as numeric and combining
the methods for mixed type attributes an
alternative concept as the similarity
function which compares two vectors
there are various similarity measures
such as the cosine measure Pearson
correlation measure jacquard measure and
dice coefficient measure several
evaluation criteria both internal and
external exists to decide whether it is
good clustering or not internal quality
criteria judge the intra cluster
homogeneity and/or the inter cluster
separable 'ti using any of the following
sum of squared error or SSE minimum
variance criteria criteria condorcet's
criterion category utility metric and
edged cut metric what an external
quality criterion does is that it works
out whether cluster structure matches
some pre specified classification of
instances using the rand index mutual
information based measure or precision
recall measure in the subsequent screens
you can get an overview about the
clustering methods many clustering
methods are in existence each using a
singular induction principle Farley in
Rafferty 1998 divides them into
hierarchical and partitioning methods
han and came BER 2001 suggested three
additional groups density based methods
or DB scan model-based clustering and
grid based methods
esta Vil Castro mm suggests
classification based on the various
induction principles use there are
various classifications for the
clustering algorithms as well k-means
clustering an exclusive clustering
algorithm that is data is grouped in
such a manner that one object belongs
only to one specific cluster
conceptually this is perhaps the
simplest method that can be used on new
data however it's not suitable for
nominal data k-means is a partitioning
method here in objects are partitioned
into K clusters such that each datum
belongs to the cluster with the nearest
mean there will be the greatest possible
distinction for K different clusters
k-means tries to find clusters to
minimize the sum of squared errors in
every cluster this objective function is
known as the within cluster sum of
squared errors or W CSS the formula for
computing W CSS is shown on the screen
however the k-means method is sensitive
to outliers you can use K methods
instead this is a hard algorithm which
means that one instance is assigned to
only one cluster it is a flat algorithm
that is all clusters are the same this
algorithm is iterative where in the
initial cluster set is improved by
reassigning instances this algorithm
partitions the data into K clusters
represented by their centers or means
each cluster Center is calculated as the
mean of all instances which belong to
that cluster you need to decide the
number of clusters in advance which
could be a problem as there is no global
theoretical way to identify the optimal
number of clusters consider an example
there are 16 thousand rows of data in
the data set and we need four clusters
then we will create the first K equals
four initial cluster by selecting for
records randomly from the data set as
the initial clusters each of the four
initial clusters will have only one row
of data
the intuitive formulation of the K
algorithm is shown on the screen except
the number of clusters you want to
generate that is K choose K points
randomly as the Centers of these
clusters
using euclidean distance or a similarity
measure assign each record to its
nearest cluster calculate the mean that
is the centroid for every cluster and
use it as the new cluster Center then
reassign the instances to the closest
cluster Center iterate until there is no
change in the cluster centers and let's
now see how we do k-means clustering it
using our R has an array of options for
clustering we can use the function Forte
means in the our package stats this
algorithm is known to be fast and
reliable the parameter in start can be
used to reduce the sensitivity of the
algorithm to the random selection of the
initial clusters cluster means if the
argument centers is number then in start
will decide how many random sets should
be chosen also note that the cluster
labels 1 2 and so on usually change from
one run to the other this only has to do
with the labeling of the clusters but
not with the assignment of the elements
to the clusters let's see a case study
to see how we can apply k-means we can
analyze monthly seasonal adjusted
unemployment rates covering the period
at January 1976 through August 2010 for
the 50 US states in equals 50 in the
figure shown below we overlay the time
series plots of three states Iowa green
New York red and California black
the objective is to cluster States into
groups that are alike here each state is
characterized by a feature vector of
very large dimension or P equals 416
with its components representing the 416
monthly observations assume for
illustration that New York and
California form a cluster you need to
calculate the 416 in monthly averages of
two observations each this vector of
averages is the centroid for that
cluster the sum of the squared distances
from the centroid of this cluster for
New York in California there are two
into 416 equals 832 such distances
expresses the within cluster sum of
squares let's see how to perform k-means
clustering in r1 you need to use read
CSV in order to read the data to
transpose the data as matrix of 50 rows
or States and 416 columns or time
periods 3 then create k-means cluster
with 416 dimensions by using the k-means
function the format of the k-means
function in R is k-means x centers for x
is a numeric data set matrix or data
frame and configurations and reports on
the best one for example adding in star
equals 10 will generate 10 initial
configurations this approach is often
recommended for similarly you should
create two three four and five clusters
with in start equals 10 this demo will
show the steps to do clustering using
k-means
in this demo you'll learn how to do
clustering using k-means by default the
package required for k-means is already
installed and loaded in R for this demo
we'll use the data set called iris which
is already loaded in R by default let's
prepare the data set by removing
unnecessary column species from the iris
data
you
we can use the k-means function to do
the clustering in that data
you
for more options and documentation we
can use the given function
in this screen will look at hierarchical
clustering in this method you accurse
ibly partition the instances this is
done in either a bottom-up or top-down
fashion to give you a dendrogram as a
result from a single cluster containing
all objects to in clusters that each
contain a single object is what
partitioning in several steps gives you
dendrograms are quite useful as they
give us a visual representation of the
clusters in the dendrogram units in the
same cluster are joined by a horizontal
line with the scale on the y-axis of the
dendrogram
reflecting a measure of the distances of
the units within the cluster the leaves
at the bottom of the dendrogram
represent the individual units leaves
are combined to form small branches
small branches are combined into larger
branches until one reaches the trunk or
root of the tree that represents a
single cluster containing all units
important points about dendrograms units
in the same cluster are joined by a
horizontal line the leaves at the bottom
represent individual units they are
useful as they provide a visual
representation of clusters next we'll
look at the two major hierarchical
clustering algorithms
algorithms for hierarchical clustering
are either agglomerative in which we
start at the individual leaves and
successfully merged the clusters
together or divisive in which we start
at the root and recursively split the
clusters at low merit of clustering is a
bottom-up approach in this method
initially each item is in its own
cluster and then iteratively clusters
are merged together divisive clustering
is a top-down approach in this method
initially all items are in one cluster
and that large clusters are successfully
divided both methods produce a
dendrogram
hierarchical clustering procedures
require both the distance measure and
the linkage criterion the tree
clustering method uses the similarities
or distances between objects when
forming the clusters euclidean distance
or squared euclidean distance gives you
the geometric distance in
multi-dimensional space other measures
are the city block distance chevy chef
de sense and power distance percent
disagreement can be used for categorical
data once the distance measure tells you
the distances between objects in a
cluster you need linkage rules to tell
you if the clusters exhibit enough
similarity to be tied together
hierarchical algorithm can be done in
different ways in single linkage
clustering the distance between one
cluster and another cluster is equal to
the shortest distance from any member of
one cluster to any member of the other
cluster at incomplete linkage clustering
the distance between one cluster and
another cluster is equal to the greatest
distance from any member of one cluster
to any member of the other cluster in
average linkage clustering the distance
between one cluster and another cluster
is equal to the average distance from
any member of one cluster to any member
of the other cluster
you can see how ad low merit of
clustering is implemented in computers
and end times in distance matrix is
considered with a number of the it--the
row and a J's column is the distance
between the ith and jth units the
distance matrix is symmetric with zeros
in the diagonal as the clustering
progresses rows and columns are merged
as clusters and the distances between
them are updated you can stop the
clustering process based on the distance
or number criterion for agloe merit of
clustering we use the Agnes function in
the our package cluster alternatively we
can use the h quest function in the
stats package let's look at the given
case study to see how to perform
hierarchical clustering consider 25
European countries in equals 25 units
and their protein intake in percentage
from nine major food sources P equals
nine the data is listed on the screen
for example Austria gets eight point
nine percent of its protein from red
meat nineteen point nine percent for
milk and so on it may well be that
Mediterranean countries get their
protein intake from certain food
categories which are different from the
food staples that are favored by North
European and german-speaking countries
looking at this data you need to
determine whether the listed 25
countries can be separated into a
smaller number of clusters
to perform hierarchical clustering in
our you need to run the following code
first load the cluster library into
memory by running the command shown on
the screen read the data set by using
read CSV use the Agnes function to
perform hierarchical clustering plot the
dendrogram by using the plot function
we'll show the steps to do hierarchical
clustering
in this demo you'll learn how to do
hierarchical clustering
by default the package required for
hierarchical clustering is already
installed and loaded in our a for this
demo we'll use the data set called empty
cars which is already loaded in our by
default
let's compute the distance between rows
in the data set empty cars
you
we can use the h quest function to do
the clustering in that data by distance
you
for more options and documentation we
can use the given command
density based models of clustering have
been vital in identifying nonlinear data
based on density the most popular
density based algorithm is the density
based spatial clustering of applications
with noise or DB scan here clusters are
high density regions that are separated
from regions that have low object
density a cluster is defined as a
maximal set of density connected points
the clusters and consequently the
classes are easily and readily
identifiable because they have an
increased density with respects to the
points they possess on the other hand
the single points scattered around the
datasets are outliers which means they
do not belong to any clusters as a
result of being in an area with
relatively low concentration the DB scan
algorithm requires at most two
parameters a density metric and the
minimum size of a cluster as a result in
estimating the number of clusters a
priori is not needed as opposed to other
techniques namely k-means
efficient for large databases DB scan
discovers clusters of arbitrary shapes
DD scan clusters objects given input
parameters such as epsilon and minimum
points as shown on the screen
this method locates objects within a
radius of Epsilon
from an object a neighborhood of a point
P is a set of all points that have a
distance measure less than a
predetermined value called EPs minimum
points specify the density thresholds of
the denser regions epsilon neighborhood
of an object contains at least minimum
points of objects a point P is directly
density reachable from another point Q
if it belongs to the neighborhood of Q
and Q's neighborhood size is greater
than a given constant minimum points to
determine these constraints compute the
distance from a point to its cake
nearest neighbor and plot the sorted
values
d is created by a set of points that
respects a certain degree of
concentration which is set by the
minimum points and EPS constraints DB
scan uses the concepts of density
connectivity and density reach ability a
point P is said to be density reachable
from a point Q if point P is within
epsilon distance from point Q and a
point Q has sufficient number of points
in its neighbors which are within
distance epsilon points P and Q are said
to be density connected if there exists
a point R which has a sufficient number
of points as its neighbors and both the
points P and Q are within the epsilon
distance if Q belongs to D and if Q is
density reachable from P and P's
neighborhood is greater than the minimum
points threshold then P belongs to B
given the constraints the objects can be
classified into core border and noise
points core points lie on the clusters
interior a point is defined as a core
point if it has more than a specified
number of points or minimum points
within EPS a point is defined as a
border point when it has fewer than
minimum points within EPS but is in the
neighborhood of a core point a point is
defined as a noise point if it is
neither a core point nor a border point
the algorithm of DB scan is as follows
select an element P from the database
you need to retrieve all points density
reachable from P with respect to epsilon
and minimum points if P is not a core
point mark P of noise if P is a core
point mark P in the cluster and call the
expanded cluster function the expanded
cluster is a recursive function that
finds all the points that are density
reachable from P and are currently
marked as unclassified or noise the
algorithm ends when all points have been
properly classified
now that you have a good understanding
of what basically DB scan does let's
look at how it works in our the DB scan
technique is available on ours FPC
package the DB scan implementation
offers high configurability as it allows
the choosing of several parameters and
options values moreover FPC's bb scan
has a visualization interface which
makes it possible to visualize the
clustering process iteratively on the
screen the procedure to apply DB scan
directly and the code to install the FPC
package are shown
let's summarize the topics covered in
this lesson clustering is a type of
unsupervised learning that forms
clusters of similar objects
automatically k-means clustering tries
to partition a set of data points into K
distinct clusters hierarchical
clustering clusters and units or objects
each with P features into smaller groups
DB scan clustering is inspired by the
natural clustering approach hey want to
become an expert in Big Data then
subscribe to the simply learn Channel
and click here to watch more such videos
centered up and get certified in Big
Data click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>