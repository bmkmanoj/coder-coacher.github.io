<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Pig Tutorial For Beginners | What is Pig In Hadoop | Hadoop Pig Programming | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Hadoop Pig Tutorial For Beginners | What is Pig In Hadoop | Hadoop Pig Programming | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Pig Tutorial For Beginners | What is Pig In Hadoop | Hadoop Pig Programming | Simplilearn</b></h2><h5 class="post__date">2017-07-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hve24pRW_Ps" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">prior to 2006 programs were written only
on Map Reduce using Java programming
language developers had to mind the map
sort shuffle and reduce fundamentals
while creating a program for which they
needed common operations such as joining
filtering and so on the challenge is
kept building up while maintaining
optimizing and extending the code
consequently the production time
increased also data flow in MapReduce
was quite rigid where the output of one
task could be used as the input of
another to overcome these issues pig was
developed in late 2006 by Yahoo
researchers it later became an Apache
open source project pig is another
language besides Java in which MapReduce
programs can be written pig is a
scripting platform that runs on Hadoop
clusters designed to process and analyze
large datasets pig is extensible
self-optimizing and easily program
programmers can use pig to write data
transformations without knowing Java Pig
uses both structured and unstructured
data as input to perform analytics and
uses HDFS to store the results Yahoo
scientists use grid tools to scan
through petabytes of data many of them
write scripts to test a theory or gain
deeper insights however in the data
factory data may not be in a
standardized state
this makes Pig a good option as it
supports data with partial or unknown
schemas and semi or unstructured data
there are two major components of pig
pig latin script language a runtime
engine the
latin script is a procedural dataflow
language it contains syntax and commands
that can be applied to implement
business logic examples of Pig Latin are
load and store the runtime engine is a
compiler that produces sequences of
MapReduce programs it uses HDFS to store
and retrieve data it is also used to
interact with the Hadoop system HDFS and
MapReduce the runtime engine parses
validates and compiles the script
operations into a sequence of MapReduce
job pigs operation can be explained in
three stages in the first stage the data
is loaded and a pig script is written in
the second stage the pig execution
engine parses and checks the script if
it passes the script is optimized and a
logical and physical plan is generated
for execution the job is submitted to
Hadoop as a job comprising of map and
reduce tasks Pig monitors the status of
jobs using Hadoop API and reports the
status to its client in the execution
stage the results are dumped onto the
screen or are stored in HDFS depending
on the user command developer and
analysts like to use Pig as it offers
many features some of the features are
as follows provision
step-by-step procedural control and the
ability to operate directly over files
schemas that though optional can be
assigned dynamically support to
user-defined functions or UDF's and to
various data types as part of its data
model Pig supports four basic types the
first is atom which is a simple atomic
value like integer long double or string
the second is tuple which is a sequence
of fields that can be of any data type
the third is bag which is a collection
of tuples of potentially varying
structures and can contain duplicates
finally there is map which is an
associative array the key must be a
character array but the value can be of
any type by default
Pig treat undeclared fields as byte
arrays which are collections of
uninterpreted bytes Pig can infer a
fields type based on the use of
operators that explain a certain type of
field it can also use user-defined
functions or UDF's with an own or
explicitly set return type furthermore
it can infer the field I patients on my
email information I did revitalize load
function or explore make literally
declare house using an AS clause please
note that type conversion is lazy which
means that data type is enforced at the
point of execution only pig latin has a
fully Nesta build at Amada with atomic
values tuples bags or lists and maps
this implies one data type can be nested
within another as shown in the image the
advantage
is that this is far more natural to
programmers than flat tuples also it
avoids expensive joins Pig works in to
execution modes local and MapReduce in
the local mode the pig engine takes
input from the Linux file system and the
output is stored in the same file system
in MapReduce mode the pig engine
directly interacts and executes in HDFS
and MapReduce
the two modes in which a pig latin
program can be written our interactive
and batch interactive mode means coding
and executing the script line by line as
shown in the image in batch mode all
scripts are coded in a file with the
extension dot P IG and the file is
directly executed since we have already
learned about hive and Impala which work
on sequel let's now see how Pig is
different from sequel the first
difference between Pig and sequel is
that Pig is a scripting language used to
interact with HDFS sequel is a query
language used to interact with databases
residing in the database engine in terms
of query style Pig offers a step-by-step
execution style compared to the single
block execution style of sequel Pig does
a lazy evaluation which means that data
is processed only when the store or dump
command is encountered also sequel
offers immediate evaluation of a query
pipeline splits are supported in pig
however in sequel you may need to run
the join command twice for the result to
be materialized as an intermediate
result now that we've gone through the
differences between peg and sequel let
us now understand further with an
example the illustration given is an
example to help you understand the
sequel command and it's pig equivalent
command script the sequel command focus
is on the customer table with columns
see ID and see total which is the sum of
the amounts it joins the sales table
with reference to CID where the see city
is Texas the
of CID is performed by ensuring the sum
of the amount is greater than the 2000
ordered in descending order now examine
the same function using pig in pig you
create two entities customer and sales
where you load equivalent data with the
schema you filter the customers based on
location
for example Texas both data are joined
using the CID row the sum of the amounts
of the individual CID s is calculated
now isolate those customers who spent
more than two thousand dollars later
sort the customers in descending order
now let's look at how to load and store
data in the pig engine using the command
console loading refers to loading
relations from the files in the pig
buffer this is done using the keyword
load followed by the name of the
variable for which data is to be loaded
a series of transformations statements
processes the data storing refers to
writing output to the file system this
is done using the keyword store followed
by the name of the variable whose data
is to be stored along with the location
of storage you can use the keyword dump
to display the output on the screen page
processes paid Latin statements in the
following manner first Pig validates the
syntax and the semantics of all
statements second it type checks with
schema third it verifies references pig
performs limited optimization before
execution fourth
if pig encounters a dump or store it
will execute the statements a pig latin
script execution plan consists of
logical optimized logical physical and
MapReduce plan now we will learn some of
the relations Big Data and Hadoop
developers execute filtering
transforming grouping sorting combining
splitting filtering refers to filtering
of data based on a conditional Clause
such as grade and pay transforming
refers to making data presentable to
extract analogical data grouping refers
to generating a group of meaningful data
sorting refers to arranging the data in
ascending or descending order combining
refers to performing a union operation
of data stored in the variable splitting
refers to separating the data with a
logical meaning on the screen are some
Pig commands which are frequently used
by analysts
Hey want to become an expert in Big Data
then subscribe to the simply learned
Channel and click here to watch more
such videos centered up and get
certified in Big Data click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>