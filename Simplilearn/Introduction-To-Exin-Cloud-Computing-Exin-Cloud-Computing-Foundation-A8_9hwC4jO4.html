<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction To Exin Cloud Computing | Exin Cloud Computing Foundation | Coder Coacher - Coaching Coders</title><meta content="Introduction To Exin Cloud Computing | Exin Cloud Computing Foundation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction To Exin Cloud Computing | Exin Cloud Computing Foundation</b></h2><h5 class="post__date">2012-09-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/A8_9hwC4jO4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">simply learn
your pace your place course cloud
computing foundation introduction hello
and welcome to the cloud computing
foundation course by simply learn this
course is designed for the participants
who are looking at successfully
completing the foundation certification
by exin on cloud computing these slides
contain all the basic materials required
to prepare students for the exin
foundation cloud computing exam
agenda let's now look at the agenda of
this session in this course we will
first be going through the introduction
of cloud computing and how cloud is used
followed by the how cloud is implemented
and supported we will also look into
managing and evaluation of cloud
computing in this lesson with this we
are done with the agenda of this lesson
now let's go ahead and experience the
session on cloud computing by simply
learn
introduction to cloud computing all of
us have heard of cloud computing and its
benefits we have heard of it as this new
cutting edge technology that is going to
take the world by storm we show from
here look at the concept of cloud how it
was conceived and its uses
objectives in this course we will be
going through the basic concepts of the
cloud computing platform where we will
be studying on what is a cloud the
technologies based on which cloud was
evolved how to build a cloud design
deploy and manage it we will take a
close look at technologies used before
the cloud platform their positives and
shortcomings and how due to these
technologies the concept of a cloud was
conceived we will see how evolvement in
bandwidth and processing power has
benefited us in our pursuit of cloud
technology we will examine the pros and
cons of implementing a cloud platform
that is to say situations that a company
might have to go through if they decide
to move to a cloud platform decisions
they will have to make if they implement
their own cloud infrastructure and if
they decide to take cloud as a service
we will be going through the different
types of clouds we will be analyzing the
potential concerns an organization could
encounter while moving over to a cloud
environment for example potential
security risks legal issues system
availability SLA s we will learn about
cloud computing standards and best
practices as well
have learned at the end of this course
you will be able to identify the
essential elements of what consists a
cloud platform you will have learned
about various cloud platforms
information as service IaaS platform as
a service paas software as a Service
SaaS etc you will realize that we have
been using some of these technologies in
our daily lives regularly you will
understand migration paths for cloud
adoption you'll be able to identify
positives and negatives of shifting to a
cloud environment you will see how it is
advantageous financially to move over
onto a cloud platform and analyze which
business processes benefit from this
move along with the positives you'll be
able to analyze the negative aspects as
well such as security concerns
legalities uptime SLA s etc
you would have studied business cases
where and why a company would want to
shift on to a cloud environment and the
options they will have to implement a
working model you would have understood
how to build a cloud Network
technologies and software's available to
build maintain and sustain you'll be
well rehearsed with virtualization
architecture stacks on how the
architecture works hardware required for
virtualization functionality etc you'll
be able to pinpoint privacy and security
issues that companies should be
concerned with and resolve before making
a decision to move over to a cloud
identity management Federation's and
present and how mobility can be
introduced in a cloud mobile devices
like tablets laptops smartphones etc try
out computing standards and best
practices overview
my gram shows us the methodology of an
effective working cloud environment the
most important thing to note is that all
these factors are entirely codependent
and have to work in tandem for us to
have a functioning reliable cloud
platform please elaborate on the diagram
the N is T cloud definition framework
this framework shows us the types of
deployment models in a cloud we can see
there are three distinct models of
deployment and one can make a choice of
which model to opt for private cloud
this is where an organization deploys
their own cloud in their infrastructure
instead of going for a regular network
with physical computers they buy
virtualization capable servers and
virtualized most of their servers and
client pcs which they access by using
thin clients this also saves on a lot of
infrastructure investments and since the
cloud is in-house the security and total
control still lies in the hands of the
organization this model is most suitable
for large networks public cloud this is
where the organization opts to use the
services provided by a third party
for example Amazon ec2 or Microsoft Azul
and outsource their infrastructure here
the control and security lies for the
service provider community cloud let's
say there are four organizations with
almost the same computing needs if they
do an in-house infrastructure setting
each will have to develop its own
network and maintain it which is a
costly proposition instead they get
together and pull their monetary
resources and build a cloud which only
these four organizations can utilize it
is not open to the public so this is a
community of people pooling their
resources to develop their own cloud
since there are multiple organizations
using the same cloud it cannot be called
a private cloud and since it is not open
to the public it is not a public cloud
either
then we come to the service models that
can be used over the clown in the
diagram there are three major services
listed one software-as-a-service sass
this is where the software is located
elsewhere at a remote location and you
can access it through the internet using
a browser the service here provided is
that of the software
hence the apt name a simple example in
this case would be your web-based email
Facebook etc to platform-as-a-service
pass
design platform like dotnet is rented
for development purposes let's look at
an example you want to develop an
application based on the.net framework
under normal circumstances you buy a
computer by the dotnet framework for
development and also creates an
environment for testing the application
this is a lot of money instead using
your existing computer or a thin client
you approach Microsoft Azul and rent a
virtual PC for the development rent the
dotnet framework and also rent the
testing environment once you have
finished with the application you can
just pay the rent for utilizing those
services and discard them when done this
saves you from the initial
infrastructure investment and you only
pay rent for usage this also keeps the
cost of your application down
infrastructure-as-a-service
is this is where you rent computation
and/or storage space as a metered
service this concept is based on utility
computing which has been around for
years
however utility computing never took off
due to lack of virtualization which made
this model economically unviable the
cost in utility was approximately $1 for
one hour
of usage this service is similar to a
piped gas connection or an electricity
connection let us now look at the
essential characteristics of a cloud on
demand of self-service this is a
scalable model which means you can
upgrade or downgrade your infrastructure
as needed yourself by logging into your
account with a service provider so if
you feel that there is going to be a
spike in the near future in your
workload then you can just deploy a few
virtual machines in your infrastructure
to manage the spike and once the
workload decreases then you can discard
those virtual machines you will only pay
for the usage of those virtual machines
rapid elasticity the upgrading or
downgrading in this situation takes only
minutes as existing templates are used
thus you do not have to wait for a long
period of time for the machines to be
deployed or destroyed resource pooling
all your hardware resources are pulled
together to form a strong reliable
virtual environment to meet your needs
measured service the services provided
are charged to you only for your usage
and not on a monthly or a daily rental
if you use it for say just five hours
today and three hours tomorrow you'll be
charged only for those eight hours of
utilization let us now look at the
common characteristics massive scale
clouds are generally deployed on a
massive scale
that is where the infrastructure is huge
this results in a lot of savings in
infrastructure investments homogenate II
the infrastructure is composed of
similar or identical parts or elements
virtualization the
a key component of cloud computing take
out the virtualization from the cloud
and you are looking at the grid
computing model more on this later in
the workshop low cost software in a
cloud the emphasis is on using open
source software to keep costs low also
licensing is entirely different in cloud
for virtual machines resilient computing
when a virtual machine crashes the
ability to recover from the crash or
deploy another machine for the same
purpose is very quick thus it can be
said that it is resilient geographic
distribution since the cloud works on
the internet it is possible for the
network or parts of the network to be
geographically in different locations
and still be able to communicate with
each other service orientation the cloud
model has been specially developed to
deliver a variety of services to an
organization or to your doorstep
advanced security if opting for a
service provider it is safe to assume
that security will be taken care of
considering the fact that the providers
investment in security will be far more
than that of a regular organization it
however bodes well to have those clauses
covered in your SLA and to verify the
factuality of those security measures
history of cloud computing here we are
going to explore the history of cloud
computing most of us think that this is
a brand new technology recently
introduced to the world and everyone's
trying to jump on the bandwagon but
nothing could be further from the truth
the concept itself was first introduced
in the 1960s by JCR Licklider who was
responsible for enabling the development
of ARPA n ET in 1969 his idea was of an
intergalactic computer network his
vision was to see everyone on the globe
to be interconnected and accessing
programs and data at any site from
anywhere it is a vision that sounds a
lot like what we are calling cloud
computing says Margaret Lewis product
marketing director at AMD other experts
attributed the cloud concept to computer
scientist john mccarthy who proposed the
idea of computation being delivered as
public utility similar to the service
bureaus which date back to the 60s since
then
cloud computing has evolved through a
number of phases which include grid and
utility computing application service
provision ASP and software as a Service
SaaS we will learn about technologies
like grid utility cluster and
virtualization that have helped us
evolve into cloud computing we will also
be studying the data center architecture
of grid utility and virtual machines the
development of cloud computing was timed
from the masses due to lack of bandwidth
however since the 90s the internet
bandwidth has been steadily increasing
thus making cloud computing more
available and beneficial to the people
one of the first milestones for cloud
computing was the arrival of
salesforce.com in 1999
it was simply based on the concept of
delivering enterprise applications
through a simple website then came
Amazon Web Services in 2002 which
provided a number of cloud-based
services then in 2006 came the elastic
cloud compute from Amazon offered as a
commercial web service again in 2009 web
2.0 hit its stride and companies like
Google Microsoft etc started offering
browser-based enterprise application
Google Apps how the masses benefit from
these services is up to them with a slew
of these services being offered by
multiple companies and the technology
still in its infancy there are a lot of
worries about legal issues security etc
we will be looking into these issues as
well and we'll be studying the Amazon
Web service AWS model as an example
history of cloud computing before the
evolution of cloud technology there were
four major technologies cloud is
essentially a repackaging of these four
technologies which translate into a much
more robust and useful model these four
technologies are one cluster computing
two grid computing three utility
computing for virtualization these
technologies have been around for
decades and have been in use
individually we will be looking deeper
into each of these technologies and we
will be able to understand that cloud
computing is nothing but a combination
of these four individually these
technologies have had varied success
ratios a few were very popular during
their time however due to their
limitations weren't able to sustain as
of today hardly anyone uses these at all
one cluster computing a cluster is a set
of loosely connected computers that work
together so they look like they are a
single system the computers in a cluster
are connected by land with each computer
running its own instance of an operating
system clusters evolved due to the
availability of low-cost micro
processors high-speed networks and
software for high-performance
distributed computing the necessity for
a cluster is to make a task or process
highly available take for instance a
task that requires a lot of
computational power depending on the
workload the workload will spike any
moment and drop at any moment it would
be a futile exercise to manually
introduce systems when required and take
them out when not so you form a cluster
to service this task with X number of
computers and depending upon the
workload that many numbers of computers
will operate to service the task as the
workload increases more computers in the
clusters will be made operational and
when not needed will be shoved down to
save resources
this is called load balancing this
ensures that the task is being serviced
properly with no undue stress given to a
particular server or set of servers
whenever a computer or a set of
computers get utilized more than a
particular set percentage immediately
more computers are brought into use and
the extra load is transferred to these
newly introduced computers thus workload
is maintained across this cluster as
soon as the workload decreases
unrequired computers are automatically
shut down or put into hibernate state to
grid computing grid computing is a
Federation of computer resources from
multiple administrative domains to reach
a common goal it is a distributed system
with non interactive workloads involving
a large number of files what
distinguishes it from cluster computing
is that they are more loosely coupled
have weathered the similar elements of
constituents and are geographically
dispersed a single grid is normally used
for a variety of different purposes they
are often constructed with the aid of
general-purpose Grid software libraries
known as middleware the best example of
grid computing is that of Seti search
for extraterrestrial intelligence this
organization uses telescopes and other
tools to scan the sky for radio
frequency sounds over a wide range these
scans are converted into files for
analysis these files are distributed to
individuals who voluntarily sign up with
the organization for analysis this is
done through a software installed on
volunteers computer this software scans
the files and reports the data back to
SETI once the file is complete the
software will download another file and
continue its analysis normally the
software uses the hardware resources of
the computer when they are not in use
but the user or our Idol so if we look
at this situation we have a lot of
computers from different administrative
domains with different components and
are geographically dispersed the main
task for these computers also varies as
per their users however this creates a
virtual supercomputer for SETI for their
file analysis amassing huge
computational strength three utility
computing utility computing is the
packaging of computer resources such as
computation storage and services as a
metered service this model gives you the
advantage of low or no initial cost to
create computer infrastructure instead
you rent all your requirements from a
provider IBM HP and Microsoft were early
leaders in the new field of utility
computing with their business units and
researchers working on the architecture
payment and development challenges of
the new computing model Google Amazon
and others started to take the lead in
2008 as they established their own
utility services for computing storage
and applications utility computing can
support grid computing which has the
characteristic of very large
computations or sudden peaks in demand
which has supported via a large number
of computers utility computing can be at
best compared to services like
electricity as a metered service gas as
a metered service etc which we use on a
daily basis this model is based on the
same principle of us getting computing
resources as a service as well thus we
will not have to invest in buying
computers instead we could rent the
resources required and upgrade and
downgrade as per our requirements this
same model has been implemented in cloud
computing as infrastructure as a service
for the
Chile's ation in computing
virtualization is the creation of a
virtual rather than actual version of
something such as a hardware platform
operating system storage device or
network resources we create a total
abstraction of the underlying physical
system and create a complete virtual
system in which the guest operating
systems can execute we can hence create
multiple instances of a virtual system
on a single computer and implement them
over the network
this also frees us from buying multiple
systems and creating them over a single
system saving us from multiple hardware
and maintenance costs virtualization is
an integral part of cloud computing if
we take out virtualization from the
cloud we are looking at nothing but grid
computing we shall look into this
concept later in our course the graph
shows us the trends of various computing
platforms over a period of time the time
period is from 2004 to 2008 and the
platform's being mapped are distributed
computing grid computing utility
computing and cloud computing as we can
see from the graph grid and distributed
computing shows the decline in
popularity and usage from well before
2004 and now are almost extinct however
legality systems and software's might
still use grid computing also a few
applications like SETI as we have
discussed earlier we can also see from
the graph that utility computing never
came into fashion it never actually
gained momentum and in India metered
computing resources were unheard of
until a couple of years ago even today
such resources are being used by huge
corporations and not by home users we
see cloud computing emerging on the
graph in 2007 however it's been around
since a lot more years if you remember
earlier we have discussed salesforce.com
coming into force in 1999 an AWS being
launched in 2002 however cloud computing
started gaining popularity around 2007
when the world woke up to cost-cutting
and reducing overheads
cloud computing offers hardware and
maintenance cost reduction on a large
scale and also offers ease of use cloud
computing has now gained momentum and
has surpassed all of the previous
technologies we have seen partly because
cloud computing is nothing but a
repackaging of these earlier
technologies grits are cloud evolution
we will now study how cloud computing
evolved over various technologies and
its path like we have seen earlier we
have had many platforms on which cloud
was developed we have grid computing
utility computing software as-a-service
SAS and then cloud computing let's go
through a small overview of what is grid
computing just a job our memory with
computing is a Federation of computer
resources from multiple administrative
domains to reach a common goal it is a
distributed system with non interactive
workloads involving a large number of
files we have also seen an example of
grid computing viz SETI we used for
computing for a task that requires huge
computational power for example research
etc this was made mainstream by Global
Alliance utility computing is where we
offer computer resources such as
computation storage and services as a
metered service this saves us huge money
on infrastructure investment and we only
pay rent for the services used this
service has never picked up until now
but it's still in its infancy stage this
has been in existence since the 60s but
was not used due to insufficient
bandwidth was rediscovered in the 90s
but again never really took off software
as a service SAS this is where provider
licenses an application to customers for
use as a service on demand this service
gained momentum since 2001
but again has been in existence since
the advent of the Internet
the most common example for this is your
web-based email you use your browser to
navigate to the website which gives us
an interface to interact with their
server and database after logging in we
see our email and attachments which are
in reality being stored on their server
so effectively we are using a web-based
software given to us by the service
provider to interact with their servers
and view our data stored with them other
examples of sass would be Google Maps
Picasso hotmail yahoo services Facebook
etc from this you will understand that
we have been using these services for
ages
cloud computing this is next-generation
internet computing this is where
anything and everything will be
accessible to anyone from anywhere
Internet is a must for cloud computing
without internet cloud will not work
then we say Internet is the core
essential of a cloud infrastructure for
it to even function let us look at how
cloud has evolved from various
technologies the underlying concept of a
cloud is of utility computing
if we remember utility computing is
giving computing resources as a metered
service this is known as IaaS
infrastructure as a service this
infrastructure can also give you many
virtual machines with varied hardware or
similar hardware for use in many or a
single track thus utilizing the concept
of grid computing we have already seen
SAS software as a service which is also
a concept in cloud computing we will be
going through these concepts in detail
later in the course
examples and studying Amazon Web
Services model cloud computing based
examples Amazon ec2 elastic cloud
compute s3 simple storage service both
these services can be classified under
infrastructure as a service is we will
be studying the Amazon Web Services AWS
model
giggles app engine the engine itself is
a platform as a service pass but the
applications hosted by the engine come
under software as a service SAS as they
are applications used on demand by the
consumer Microsoft Windows Azure
Microsoft SQL Services microsoft.net
services live services Microsoft
SharePoint services and Microsoft
Dynamics CRM services let us now look at
the services offered by AWS especially
the ec2 Amazon ec2 provides a number of
powerful features for building scalable
failure resistant enterprise class
applications including Amazon elastic
block store EBS offers persistent
storage for Amazon ec2 instances Amazon
EBS volumes provide of instance storage
that persist independently from the life
of an instance Amazon EBS volumes are
highly available highly reliable volumes
that can be leveraged as an Amazon ec2
instances boot partition or attached to
a running Amazon ec2 instance as a
standard block device when used as a
boot partition Amazon ec2 instances can
be stopped and subsequently restarted
enabling you to only pay for the storage
resources used while maintaining your
instances used while maintaining your
instance state Amazon EBS volumes offer
greatly improved Europe
over local Amazon ec2 instant stores as
Amazon EBS volumes are automatically
replicated on the back end in a single
availability zone for those wanting even
more durability Amazon EBS provides the
ability to create point in time
consistent snapshots of your volumes
that are then stored in Amazon s3 and
automatically replicated across multiple
availability zones these snapshots can
be used as the starting point for new
Amazon EBS volumes and can protect your
data for a long-term durability you can
also easily share these snapshots with
coworkers and other AWS developers see
Amazon elastic block store for more
details on this feature multiple
locations Amazon ec2 provides the
ability to place instances in multiple
locations Amazon ec2 locations are
composed of regions and availability
zones availability zones are distinct
locations that are engineered to be
insulated from failures in other
availability zones and provide
inexpensive low latency network
connectivity to other availability zones
in the same region by launching
instances in separate availability zones
you can protect your applications from
failure of a single location regions
consists of one or more availability
zones are geographically dispersed and
will be in separate geographic areas or
countries the Amazon ec2 service level
agreement commitment is 99.95%
availability for each Amazon ec2 region
Amazon ec2 is currently available in 8
regions US East North Virginia USD West
Oregon
u.s. west Northern California EU Island
asia-pacific Singapore asia-pacific
Tokyo South America South Paulo and AWS
gov cloud elastic IP addresses elastic
IP addresses are static IP addresses
designed for dynamic cloud computing an
elastic IP address is associated with
your account not a particular instance
and you control that address until you
choose to explicitly release it unlike
traditional static IP addresses
however elastic IP addresses allow you
to mask instance or availability zone
failures by programmatically remapping
your public IP addresses to any instance
in your account rather than waiting on a
data technician to reconfigure or
replace your hosts or waiting for the
dns to propagate to all of your
customers Amazon ec2 enables you to
engineer around problems with your
instance or software by quickly
remapping your elastic IP address to a
replacement instance Amazon virtual
private cloud Amazon VPC is a secure and
seamless bridge between a computer's
existing IT infrastructure and AWS cloud
Amazon VPC enables enterprises to
connect their existing infrastructure to
a set of isolated eight of us compute
resources via a virtual private network
VPN connection and to extend their
existing management capabilities such as
security services firewalls and
intrusion detection systems to include
their AWS resources Amazon CloudWatch
Amazon CloudWatch is a web service that
provides monitoring for eight of us
cloud resources and applications
starting with Amazon ec2 it provides you
with visibility into resource
utilization operational performance
and overall demand patterns including
metrics such as CPU utilization disk
reads and writes and network traffic you
can get statistics view graphs and set
alarms for your metric data to use
Amazon CloudWatch simply select the
Amazon ec2 instances that you would like
to monitor you can also supply your
business or application metric data
Amazon CloudWatch will be aggregating
and storing monitoring data that can be
accessed using web service API or
command line tools auto scaling auto
scaling allows you to automatically
scale your Amazon ec2 capacity up or
down according to conditions you define
with auto scaling you can ensure that
the number of Amazon ec2 instances
you're using scales up seamlessly during
demand spikes to maintain performance
and scales down automatically during
demand blows to minimize costs auto
scaling is particularly well-suited for
applications that experience hourly
daily or weekly variability in usage
auto scaling is enabled by Amazon
CloudWatch
and available at no additional charge
beyond Amazon CloudWatch feeds elastic
load balancing elastic load balancing
automatically distributes incoming
application traffic across multiple
Amazon ec2 instances it enables you to
achieve even greater fault tolerance in
your applications seamlessly providing
the amount of load balancing capacity
needed in response to incoming
application traffic elastic load
balancing detects unhealthy instances
within a pool and automatically reroute
traffic they're healthy instances until
the unhealthy instances have been
restored
you can enable elastic load balancing
within a single availability zone or
across multiple zones for even more
consistent application perform
Amazon CloudWatch can be used to capture
a specific elastic load balances
operational metrics such as request
count and request latency at no
additional cost
beyond elastic load balancing fees high
performance computing HPC clusters
customers with complex computational
work loads such as tightly coupled
parallel processes or with applications
sensitive to network performance can
achieve the same high compute and
network performance provided by customer
built infrastructure while benefiting
from the elasticity flexibility and cost
advantages of Amazon ec2 cluster compute
and cluster GPU instances have been
specifically engineered to provide high
performance network capability and can
be programmatically launched into
clusters allowing applications to get
the low latency network performance
required for tightly coupled no.2 node
Communication cluster compute and
cluster GPU instances also provides
significantly increased network
throughout making them well-suited for
customer applications that need to
perform network intensive operations
learn more about cluster compute and
cluster GPU instances as well as other
AWS services that can be used for HPC
applications VM import VM import enables
you to easily import virtual machine
images from your existing environment to
Amazon ec2 instances VM import allows
you to leverage your existing
investments in virtual machines that you
have built to meet your IT security
configuration management and compliance
requirements by seamlessly bringing
those virtual machines into Amazon ec2
as ready to use instances
offering is available at no additional
charge beyond standard usage charges for
Amazon ec2 and Amazon s3 AWS marketplace
is an online store that helps you find
buy and quickly deploy software that
runs on AWS you can use AWS marketplace
one-click deployment to quickly launch
pre-configured software and be charged
for what you use by the hour or month
AWS handles billing and payments and
software charges appear on your aww
Amazon Cloud users new york times NYT
needed to convert 15 million scanned
articles to PDF files it would have
taken a huge amount of time if they
would have decided to do it using their
own infrastructure let alone the time it
would have taken a lot of their hardware
and network resources were they to do it
in-house which would also have affected
their daily work what they did was
directly log into the Amazon Cloud using
their credit card they did not have to
contact Amazon for any requirements and
create an account they deployed 100
Linux computers using the hardware and
software resources offered by Amazon and
in a span of 24 hours transferred all
that data which totaled 240v to use PDFs
using ec2 and s3 it would have taken
months if they decided to utilize their
own resources thus not only was it
cost-effective for them it saved a lot
of time without creating a hindrance on
their existing network at the end of the
exercise they deleted the 100 Linux
machines they had created and thus only
had to pay for the resources consumed
for their entire exercise Nass tags
everyone knows Nass tax and the
voluminous theta they go through every
day they have to deal with hundreds and
thousands of script
on a daily basis and give ii ii updates
daily there are millions of files
showing price changes of entities over a
ten-minute segment storing all that data
on their hardware would require a
humongous investment in computer
resources also resulting in a lot of
maintenance costs so they use the s3
services offered by AWS for storage
integrated that storage in their
infrastructure and used s3 to host all
that data while being in sync at all
times they also created a lightweight
Adobe AIR application using which the
users were able to view the required
data these two examples show us to what
extent can a cloud infrastructure help
us a businessman no longer will have to
invest in infrastructure and maintain a
team of specialists to service it he can
now easily create everything on the
cloud let the provider worry about the
infrastructure and he can focus entirely
on his own business all he's doing here
is outsourcing his IT needs
IVM Google cloud example to a new IBM
Google initiative aims to provide
computer science students with a
complete suite of open-source based
development tools so they can gain the
advanced programming skills necessary to
innovate and address the challenges of
this computing model which uses many
computers networked together through
open standards and thereby drive the
internet next phase of growth the
companies will provide hardware software
and services to augment university
curricula and expand research horizons
while lowering the financial and
logistical barriers the academic
community to explore internet scale
computing IBM and Google have already
dedicated a large cluster of several
hundred computers for this program which
is planned to grow to a few thousand
servers over time using this virtual IT
lab students will learn how to develop
systems and write massively parallel
applications that take full advantage of
the distributed computing paradigm
rather than the conventional one server
one application model the architecture
used in this model will be open source
to keep costs under control
Linux hosts with Apache Hadoop file
system will be used with the Xen
virtualization these are existing
technologies and hence will be
compatible with almost all systems</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>