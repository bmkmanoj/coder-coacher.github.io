<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dockerizing An Application | Docker Tutorial For Beginners | DevOps Tutorial Video | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Dockerizing An Application | Docker Tutorial For Beginners | DevOps Tutorial Video | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dockerizing An Application | Docker Tutorial For Beginners | DevOps Tutorial Video | Simplilearn</b></h2><h5 class="post__date">2017-12-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rNsQGvt9v4k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi and welcome to this session on how to
docker is an angular application my name
is Mira user and in the next few minutes
I'll be working at your building and
docker image for an angular application
and then running the image locally on
your workstation as angular is a
front-end framework we'll be using a web
server called nginx to serve the
application HTML files after this
session you will understand three things
first what is docker and what is it used
for
you will understand the basics of docker
and what problem docker fixes next
you'll learn how to write the docker
file which you can think of as the
recipe docker uses to build your image
in this session we'll be using a
multi-stage docker file to build our
application third you will learn how to
use an engine X server to serve an
angular application inside a docker
container to follow through the examples
in this session you should have an
angular application ready and the latest
version of docker installed on your
workstation you can install docker by
following instructions on docker comm
first let's look at what docker is
docker is the most popular
containerization technology and it has
quickly become the de facto standard
when talking about containers container
izing an application means packaging it
and all its dependencies into a single
easily transportable and isolated
container this container will run in
exactly the same fashion regardless of
the computer it is run on by providing
this layer of consistency docker fixes
the traditional but it works on my
machine problem instead of distributing
just our application we're distributing
a full runtime environment along with
our application while not exactly
correct it might help you to think of a
docker container as a lightweight
virtual machine inside your computer
your computer can run multiple docker
containers at the same time stopping and
starting them individually as required a
common problem in software delivery is
dependency
management when one application is run
on multiple development machines and
multiple server environments a small
difference in the version of an external
library can change the functionality of
your application making it behave
differently on different environments
the beauty of docker is that if you
build your application into a container
image and transfer the same container
image to your colleagues computer you
can be sure that the application will
function identically on both computers
this is because the container includes
all dependencies for the application
inside it on the other hand a docker
container should not have any
dependencies to the host it's running on
apart from docker itself it's important
for you to understand what images and
containers are and what's the difference
between them for the purposes of this
session you can think of a docker image
as something that holds a file system
and some metadata in it the metadata
includes things like the name or tag of
the image and instructions for docker
like what command to run by default when
the image is started well the official
docker documentation is a bit vague
about the meaning of names and tags you
should learn the basics of it every
docker image has at least one tag and in
fact the same docker image can have
multiple tags pointing to it every image
tag has a name part and a version part
separated by a colon
for example the nginx image has multiple
tags representing different versions and
different flavors of the image there's a
special version tag called latest which
points the latest version of the image
all docker commands default to using the
latest version tag if no other version
tag is explicitly defined when you build
an image on your local machine or for
example a build server you can upload
the image into a docker registry when
you configure other machines they use
the same docker registry you can
download the same image to as many other
machines as you like
there is an official docker registry
called the docker hub and the model is
the same as for example github you can
store public images free of charge under
your account but you will need to pay to
host private images having your images
public is great for hosting open source
projects but for any commercial
proprietary applications you will most
likely want to opt for hosting the
images privately
if you don't want to use a third-party
service you can always run your own
docker registry my advice is that unless
you're planning on hosting a very large
number of images it's absolutely worth
the cost to spend a few dollars per
month on a house that docker registry
such as docker hub once your application
ecosystem starts to build around docker
the registry becomes a critical piece of
infrastructure and keeping it up and
running can prove to be a substantial
piece of work
when you build or download a docker
image to a computer and run it it
becomes a running container if you don't
tell docker otherwise docker will run a
pre-specified command inside the
container as far as this command is
concerned it is running inside a linux
server and it has access to other
commands and resources that are
available inside the same container you
can think of a container as a sandbox
where processes running inside the
container do not have access to the host
system or other containers unless you
explicitly specify otherwise note that
the complete container runs around a
single process created by the initial
command used to start the container if
this process terminates for any reason
the container will stop an example for
our container would be the nginx server
process when you start the container the
nginx server process starts in it and
when the nginx server process dies or is
killed the container will stop as well
it is possible to run multiple processes
inside a single container by for example
using an init system to spawn the
processes you need but for the purposes
of this session we'll use the concept of
just a single process per container
the basic workflow with docker is as
follows first you will build a docker
image either on your workstation or on a
continuous integration server second you
will push this docker image into a
docker registry to make it available for
other computers third you will most
likely want to run the image on a server
that is accessible via the internet so
clients can access whatever server
software is running inside your
container most often this is an HTTP
server and in this session we'll be
using the nginx
HTTP server
in software delivery environments that
embrace continuous delivery this
workflow is often part of a fully
automated pipeline a pipeline like this
can trigger from committing a piece of
code into a version control system and
to automatically deploy a new version of
your application into a testing or even
production environment whenever you
change your application code as we
discussed earlier a container is
isolated from the machine it's running
on as well as any other containers
running on the same machine unless you
explicitly define connections between
them this is out of the scope of this
session but I recommend you spend some
time after the session to learn about
docker networking and links between
containers in this session we'll be
running a single container that can
connect to the internet choose a host
machine and expose a single TCP port for
the nginx server process
building docker images is done with
docker files you can think of a docker
file as a text file containing a recipe
to tell docker how to build an image the
basic function of a docker file is to be
a list of instructions that
incrementally manipulate an existing
docker image the word existing here is
important while you can theoretically
build a docker image from scratch in
most use cases you will actually be
using another docker image as the base
for your new image many frameworks and
open source projects publish their own
docker images which can be used by
developers to build images on top of in
this session we'll be using a new docker
feature called multistage builds this
feature is available from docker version
17 point oh five forward and it allows
you to use one base image for building
our application and another base image
for serving it this works brilliantly
for our use case while angular is often
used as part of the full mean stack in
this session we're only concentrating on
angular I'm assuming you know angular
and how the angular command-line
interface works and that you have a
package.json in your application
directory if you're not quite there yet
check out the excellent getting started
guide on the angular website please note
that the functionality of angular varies
wildly between major versions however
the fundamental idea behind container
rising an angular application is the
same
our dockerfile is going to have two
stages first we'll need to build our
angular application using the ng build
command this will output the packaged
application in the disk directories
under our application route in the
second stage we will place the files
from the disk directory into a directory
that's served by a webserver and finally
run the webserver itself this is where
the multi-stage build feature comes in
handy as you remember a docker container
only has the software available which
we've explicitly added into it this
means that for the ng build command to
work we need to have the angular
command-line interface installed inside
our docker container
this is something we wouldn't want on
our final docker image because it's just
extra weight and on the other hand we
wouldn't well the web server inside the
image we're building our application in
because the angular CLI
is distributed as a node package we can
use the official node docker image as
the base of our first builder stage
because angular is a front-end framework
it doesn't come with a server-side
component other than the test server
shipped with the angular CLI
unfortunately this test server isn't fit
to be run on production so we'll want to
use something else to serve the angular
HTML files in this example we will use a
robust web server called nginx to serve
the angular application please note you
could just as well be using a server
application like Apache or lighty
instead I'm not going to be diving into
the specifics of configuring nginx nginx
does come with good documentation but
more importantly it also comes with an
official docker image we can use the
official nginx docker image as the base
for our angular application image the
good thing is that this makes our docker
file very short and simple let's have a
look this is the docker file and it's
divided in the two stages the way the
movie stage built works in docker is
that we're actually creating multiple
docker images with a single docker file
but we're only keeping the last image
we've defined this allows us to build
the application first and then copy the
resulting built artifacts into the next
Dugger image the idea behind this is
that our final docker image won't have
all the builds time dependencies in it
which makes the resulting image nice and
small in the first stage we'll use the
from directive to instruct docker that
we want to use the note 8 image as our
base image we're also using the S
builder keyword so we'll be able to
reference this stage in our second stage
later the next directive is the copy
directive this command is for copying
files and directories from our local
machine to the docker image
we're giving this directive two
arguments the location of our
application goat directory and the
target path within the docker container
in this example our application goat
sits in test app and we want to place it
in the directory test app inside the
container
the next directive is work dear
this tells docker that all following
commands should be run within the
specified directory which in this
example is the application directory
within the container
the next directives are the run
directives as you might guess these
directives instruct docker to run
commands inside the container first
we're running npm install to install alt
note package is defined in the package
type JSON file of our application then
we're running ng build to package our
angular application in the disk
directory now we'll get to the second
stage which is copying the contents of
the disk directory into a directory
that's served by default by nginx we're
using the from directive again this
tells docker to start building another
image we're defining the from image to
be an image called nginx which is the
official nginx docker image this image
is pre-configured to run the nginx
server and serve HTML files that are
stored in a predefined directory next
we'll use the copy directive but this
time with the from argument
this tells docker to not look for the
source path in our workstation but
instead the previous stage in this
docker file essentially we're copying
the contents of the disk directory in
the previous docker image into a
directory called user share nginx HTML
inside our new image the nginx docker
image will then serve the contents of
this directory
finally we're using a directive called
expose to tell docker that the port 80
has a server running in this image the
port 80 is the default HTTP port as you
might know
note that the expose directive alone
isn't enough to actually expose the port
when the image is run you can merely
consider this as a documentation feature
and we'll still need to explicitly map
port 80 from the container into a port
on the host machine this enables you to
connect to the mapped port on the host
machine which will ten-forward the
connection into the container to follow
this session for your angular
application create a file with these
contents and place it in the parent
directory of your application code
remember to customize the paths in the
file accordingly make sure the name of
the file is docker file written as one
word with the capital D
now that you've learned what the docker
file is and what one looks like for an
angular application let's learn how to
use it to containerize an example
angular application for this we'll be
using the docker command line interface
after you've installed docker on your
workstation you can use the docker
command line interface to build and run
images defines the command line shell on
a Mac click on the spotlight search icon
on the top right corner of your screen
and write terminal followed by enter
well the docker command line provides
many features in this session we'll
concentrate on to the docker built and
the docker run commands first let's
navigate the directory with our docker
file
the command will be using a stoker build
will be giving the command the - t
argument which defines the tack for the
image that will be built for this we'll
be using the tag test app the second
argument for the docker build command is
the built context this is where docker
expects to find the docker file will use
a single dot to denote the current
directory and press ENTER to start
building
you can see docker runs
every directive in the dockerfile as a
step out putting the directive and the
output of running it
finally you'll see that docker has
successfully built the image intact it
as test app latest as you might remember
from before since we didn't define the
version tagged the tech latest is used
automatically great work now let's run
the image we just built for this we'll
be using the docker run command we'll be
giving the docker run command a few
arguments first - I and - T to denote
that we want to run the image in an
interactive terminal that is running the
container attached to our current
terminal session so we can see the
output and interact with the process
we'll also be using the minus P argument
to map the port 8080 on our workstation
to port 80 inside the container as you
remember the nginx server is listening
on port 80 with this mapping anyone
connecting the port 8080 on our
workstation will get forwarded to port
80 inside the container finally the last
argument is the tag of the image we want
to run we're using test app colon latest
now that the container is running let's
use our web browser to navigate to port
8080
on our local machine and we can see our
angular application being served well
done you can see that nginx by default
outputs all the requests in the standard
output of the container after you've
finished testing press control C in your
terminal to terminate the nginx process
and test kill the container you were
running and that's how you docker is an
angular application before you go three
takeaways one using docker makes it easy
to ensure that your application runs
consistently
on different workstations and servers
secondly angular is a front-end
framework angular needs a separate web
server software for serving it nginx is
a fast and robust choice finally using
multistage docker files enables you to
streamline your build process and create
slim docker images thank you for
following and have a great day
hey once become an expert in cloud
computing then subscribe to simpler
Channel and click here to watch more
such videos turn it up and get certified
in cloud computing click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>