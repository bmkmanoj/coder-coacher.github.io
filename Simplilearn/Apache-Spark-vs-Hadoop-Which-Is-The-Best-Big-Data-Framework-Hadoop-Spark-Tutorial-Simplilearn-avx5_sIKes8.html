<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Spark vs Hadoop: Which Is The Best Big Data Framework | Hadoop &amp; Spark Tutorial | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Apache Spark vs Hadoop: Which Is The Best Big Data Framework | Hadoop &amp; Spark Tutorial | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Spark vs Hadoop: Which Is The Best Big Data Framework | Hadoop &amp; Spark Tutorial | Simplilearn</b></h2><h5 class="post__date">2016-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/avx5_sIKes8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning good afternoon good evening
depending on where you're joining us
from welcome to simply learn we are
honored to have you join us today for
spark versus Hadoop with your host Jamin
patel my name is Richard and after this
intro will be mostly in the background
helping Jamin so that everything goes
smooth and then I'll come back in for
the last 15 minutes of question and
answers and I'll be going through those
as they come in sparked versus Hadoop as
the two biggest players in big data
analysis who are they how are they
different and how are they the same what
are all these terms like our D D and our
DBMS mean from a career point what
studies and certifications will serve
the best moving you forward in your own
life and from a business standpoint
which one should be used for what spark
versus Hadoop if you do not know Damon
Patel Jamin is a hands-on performance
driven software professional with over
11 years of experience specialized in
object oriented design and analysis with
extensive experience in Java j2ee
technologies big data processing and
analytics using hadoop mapreduce machine
learning without our technologies spark
in a library streaming SQL graphics we
are lucky to have his experience today
and I hope you've brought lots of your
own questions for him we'll collect
those and answer those at the end of the
session you didn't come here to listen
to me I'm proud to introduce Jamin who
will be your host for spark versus
Hadoop go ahead and sit back relax have
a cup of coffee tea or maybe it's
evening where you're back where you're
at and enjoy a glass of merlot or wine
it's an honor for me to hand this over
to Jamin hello Jamin welcome to today's
webinar on spark versus Hadoop sound hi
there I'm here what the microphones
looking on there it's an honour for us
to have you thank you for being here
today Damon I greatly appreciate you
being here today thank you thank you
very shared and thanks for again joining
and hosting so along with me thank you
so hello everyone
chad has already talked nice words about
me so I'm really or not thanks which
aren't for those very warm welcome
introduction so today we are going to
talk about Hadoop versus spark right
this webinar is mainly focusing on
comparison between Hadoop and spark and
we are going to understand why are they
you know you know where they exist
coexist or do one replace one another
right there are a lot of such questions
that people would have and the basic
question that I'm hearing always is now
the spark is there what is the need of
Hadoop right or does Rayleigh spark is
able to completely replace or still you
need Hadoop
so we are going to actually focus on
webinar on understanding that and we're
going to understand some of the key
motivation or other key objectives why
spark was developed right so I'm going
to also walk you through the white paper
published by University of California
Berkeley they are the ones who have who
have pioneered sparks who are the ones
who actually develop spark and they have
mentioned it didn't out the reasons why
spark was needed right so we are going
to understand those reasons as well
right so to start with spark and Hadoop
let me first give you a very brief
comparison in a big data stack where do
they fit in right so that we can
actually compare them in a much more
easier manner right so first of all I
would like to help you understand that
these are the you know big data stack
for us and there are various components
available in big data stack so I would
love to mention couple of components
which would spark and Mister second okay
spark and I do provide so let me just
walk you through those components so we
have HDFS we have yarn we have MapReduce
right these three are the core
components of Hadoop when you install
Hadoop you get these three core
components right
these are part of her dog now we aren't
really going to spend a lot of time in
understanding what Hadoop provides but I
would still give you a brief idea about
what I do provide so Hadoop is a
framework which helps you to store big
data in an efficient and a distributed
manner so it is actually having a
component which is called Hadoop
distributed file system where it helps
you to store data in a distributed
manner and it also helps you to provide
the capability to process this data in a
parallel and a distributed manner so we
have two core components HDFS and
MapReduce from Hadoop which helps you to
do storage and processing of big data
Hadoop also provides one more layer
Kalyan which is doing cluster resource
management to provide better resource
management across the cluster so that
the jobs that you run on Hadoop cluster
get better resources and it gets gives
you better performance so that's about
how these are the three core components
that Hadoop provides ok now you have lot
of ecosystem components develop and I'm
not really going to discuss that right
now but still just to give you an idea
we have Pig we have hive we have hood
these are the components which are
developed on top of MapReduce so I would
call this as a MapReduce family ok I
will also talk about you know MapReduce
and spark in comparison so first of all
we really want to understand where spark
and MapReduce which together right so we
have MapReduce here and we have you know
let me just draw it in a and a color
which is which is probably a different
color so let me write it here spark
this is spark SQL this is for streaming
you have spark graphics and you have
Emily ok so this is a
spark family now if you'll notice here
from the diagram that I have fit in
anybody would easily understand that
spark is not a replacement of MapReduce
okay spark will replace one of the layer
which is a processing engine of Hadoop
which is we call it as MapReduce and it
has equivalent ecosystem components so
that it is going to replace these
components as well with these components
but underlying layer of Hadoop can still
be used along with spark so spark
provides just a processing engine okay
the resource management and the storage
layer can still be used or leverage from
Hadoop and that is why I said spark is
not a replacement of Hadoop
however to run spark you don't need her
dupe so spark is not dependent on Hadoop
spark and actually it's an independent
processing engine which you can install
on any distributed file system it could
be HDFS which is provided from Hadoop it
could be no sequel databases or it could
be any other local file system storage
spark also comes up with its own cluster
resource manager right which is
equivalent to yon well it is not exactly
equivalent to yon because it is not very
mature yon is very powerful and matured
resource manager where a spark has a
standalone or we call it as local
resource manager okay which will be
similar to yarn but it would not be used
in a production environment right so
generally people use yarn or there is
one more resource manager called missus
now you might be wondering that I am
actually you know bringing out lot of
terms here right and I don't really want
to discuss all these components here we
just want to focus and limit our
discussion between SPARC and MapReduce
so as I just briefly explain SPARC is
not a replacement for Hadoop SPARC is a
processing engine which is going to
replace and processing engine from
Hadoop which is MapReduce
okay and the primary advantage why spark
is going to replace a processing engine
of MapReduce which is basically because
path provides performance which is 10
times faster than MapReduce on the disk
and hundred times faster on network or
memory right so we are going to first
understand what was the need of
developing spark why was there a reason
why spark was developed why were why was
MapReduce not enough or capable solution
to process big data what were the
challenges with MapReduce once we
understand that we would be able to
appreciate spark much better right and
that's how we should start our journey
with spark or rather any technology
whenever you want to learn any new
technology the first basic question you
should ask is why is that needed what
kind of problem does it help me solve
which other technologies which are in
the same domain cannot solve once you
understand that that's it then you will
be able to appreciate it much better so
let's start our journey with that I'll
be parking this discussion aside just
now right now just assume that it is
going to be replacing MapReduce and it
has its own ecosystem components which
are going to replace these components of
MapReduce ok now let me start walking
you through some of the white paper
discussions and I would be spending some
time on explaining you that as well
which is you know which is a very good
paper which is detail out by the
University of California so they have
mentioned that what was the reason why
MapReduce was not efficient what are the
use cases where they thought that
MapReduce is definitely not giving you
better performance so let me explain
those use case and the first use case
which they'd eaten out was iterative
jobs or iterative analytics right and
the second one is interactive analytics
if we understand these two we will
understand why SPARC is needed okay so
let's start with that so let me explain
you these two you know
use-case first is I trade to analytics
okay now why do we need what is a
iterative analytics first of all we so
the the the the statement here is
MapReduce is not efficient to solve
problems which requires an intuitive
analytics and Spahr gives you better
performance here but before we
understand that let us understand what
is identify analytics so I trade you
analytics in a layman's definition is
running a same program on the same data
set again and again to improvise on the
findings that the program gets as an
output right so in a very layman's term
it's just about you are running the same
program on the same data set in a
nitrated manner now any rational person
would ask this question why do I need to
run the same program on the same data
set multiple times repeatedly can I not
get the output first type that is what
program is supposed to do all right so
that is what is a question that people
would have when we talk about right rate
of analytics and actually alternative
analytics is one of the major area in
machine learning lot of machine learning
algorithms are in fact developed and are
in fact working with iterative analytics
they are using additive analytics to
solve problems to build models so we
want to understand what exactly is AI
attentive analytics and I will be giving
you an examples through which we will be
understanding that right so let me spend
few minutes on explaining what is
exactly I do analytics I will be giving
you or sharing you the use case which I
have work on where I was working with
one of the company and we had developed
a platform where we were actually doing
iterative analytics for solving one very
interesting history so let me quickly
brief you about what we have done okay
so this company was actually getting lot
of financial transaction data of
billions of registered customers
from various banks of USA okay banks
like Bank of America Chase Citibank all
the big banks were their customers and
they were in all the users who are who
are registered for these banks who who
are using this Bank services their data
their daily transaction data we used to
get it in our databases okay why do we
get it how do we get it that's a
different story
let's assume we were running some
product which is a flagship product
through which we were getting those
transactions so we were getting all
these millions or millions of
transactions on a daily basis we used to
get about approximately 13 million
transactions okay just for a credit card
and a debit card transaction okay I'll
call credit or debit
so bank and car transactions is what we
used to get combining these two we used
to get about 13 million transactions for
approximately 5 million users ok so now
we were shipping this data you know we
were doing a lot of such we were
generating this data by the way in our
databases and I'm just talking about two
containers we had loan investment lot of
other containers through which were
generating data now what I'm trying to
highlight a point here is we were having
a lot of financial transaction data and
and and our team had come up with an
idea to generate analytics on top of
that and sell this analytic services to
the hedge fund companies who wants to
predict market trends for example these
companies are expert in finding out what
will be the Walmart stock price tomorrow
tomorrow morning what will be the
Walmart stock price but to predict that
they were actually running interesting
high-frequency trading algorithms and to
to make their algorithm
you know predict accurate price they
need to train their algorithm and for
that they need a data accurate data now
if I want to find out whether Walmart
stock is going to go up or down the best
way I can figure it out is how many
people are doing transactions at Walmart
right so
since this company had all the
transactions data of the users they were
actually getting all these millions of
transaction records and they wanted to
analyze this to find out how many of
them belong to Walmart on a daily basis
and then they will try to analyze this
on a
you know weekly or monthly basis and
find out that what is an overall sells
of Walmart on a periodic basis whether
it is going up or down and that would be
a very clear indicator for them to find
out whether the price of the stock of
Walmart is going to go up or down right
so it is very critical for them to find
out what is that which transaction
belongs to which merchant right they
were actually hitting on predicting the
prices for top 500 merchants in US and
they wanted to know how many people are
shopping at those 500 merchants right so
since we were getting all the
transactions data of different users see
whenever I buy let's say at Walmart or
if I shop at McDonald or JCPenney if I
shop $400
that's then my card statement will say
this is the transaction and I have show
up for shopped at JC Penney $400 right
now when you get this transaction me as
a user will easily find out because we
know that we have done the transaction
right but as a system he system will
just get to know that this is the
transition description and that
description will have unstructured text
which mentions JC Penney somewhere and
which mentions the dollars and all that
right now from that amount we need to
find out what is the right match so in
our data we never had a different column
which says this is the merchant name
generally banks don't give you a
separate column in your account
statement which says this is the right
merchant right I'm sure all of you who
are doing transactions do you know that
when you read your statements bank
statements you will know that there will
not be a separate column which says what
is the right much and whether it is
Walmart or JC Penney or whatever
now imagine that if I elect and I'm sure
you will all agree that different banks
will also have a different way to
represented merchants for example if I
am having a credit card or debit card of
Citibank and if I buy it let's say
Walmart 400 dollars and then next
I will use it's a Bank of America card
and I buy the same product
hundred-dollar at Walmart in my booth
that counts we'll have a statement but
one Bank will say you know $100 at W
Mart another Bank will say $100 at
Walmart third Bank will may say wml MLT
right so what I'm saying is different
banks will represent different all the
much each merchant in a different way
there is no standardization and that is
where it becomes a very difficult
problem to find out whether this
transaction belongs to the Walmart or
not so this was our problem statement
and hedge fund companies were really
interested in in knowing the right
merchants and they wanted to give us
this business so we had to do that and
hence we have used a machine learning
algorithms we have used a clustering
technique to solve this problem now I am
not really going to help you understand
how did we do that but the reason why I
am briefing this use case so that we
will understand what is a iterative
analytics okay so what we have done here
we have in fact I'll just quickly brief
you so we were told that we have to
categorize all the daily 30 million
transactions we have to find out the
right merchant for each transaction and
we have to in fact find out all the
transactions which belong to top 500
much and so they have given us the list
of top 500 merchants and and and they
were also telling us that if any
transaction which does not belong to any
of these 500 merchants then it is okay
if I say it is uncharacterized but if it
has a transaction of Walmart and if I
don't say it's a Walmart that's a big
business loss for them so I have to be
very accurate in saying that this
transaction belongs to Walmart and that
is why we had to use a machine learning
technique and generally in any machine
learning what normally you do is you
have huge data right which is called
training data which you supply to your
algorithm ml algorithm okay which will
learn the patterns and build the model
out of it after that you supply training
test data
to this algorithm an algorithm will use
this model to predict the result out of
it okay this result you will do
validation on and find out the accuracy
of your model this is the cycle that you
have to keep on repeating okay till you
perfect your model till you are
satisfied with the accuracy of your
model and generally people consider
above 95% is a good accuracy but it
varies depends on the business and the
domains now what I'm trying to highlight
a point here is in any machine learning
activity you have to train your model
should train your algorithm to build the
model and you have to perfect your model
you have to you know validate your model
once you're satisfied with the accuracy
you will you know deploy this model in
productions and algorithm we'll use that
model to predict any data item in in in
production okay so this was the activity
that we had to do we have used a
clustering algorithm to solve this
problem now as I said we had to cluster
all the transactions data into top 500
merchants so we had these clusters of
500 merchants I'm just giving a cluster
means just assume that they are
temporary files okay their files and we
have tagged each cluster as Walmart
JCPenney you know Amazon maybe this is
McDonald you know you can you can just
give some other name this could be let's
say Tommy knows whatever so there are
various merchants and we wanted to
actually find out the right model for
this now the clustering technique how
does it work so it will first find out
the centroid of each cluster it has to
fix that for that we have to train that
model and for that we had actually
supplied last one year of data every day
13 million into 365 so imagine that many
millions of billions of transaction data
we had to supply to this algorithm so
that algorithm will learn the pattern
and build the model out of it
so imagine so this transaction data we
had to convert it into a where
when I say vector on an XYZ plane you
can understand it as if it is a you know
on an XYZ plane right you can assume
that vector is a point let's say at this
point has XYZ coordinates similarly each
point has an X Y Z coordinate it's a
mathematical point now how do we convert
a normal transaction record or text
according to this mathematical point
that's a very big you know area in data
size right so we will not discuss that
right now but assume that you know we
have to work with data scientist to get
this once we have this then what
algorithm does here so again let me tell
you one thing we are not really
understanding how algorithm solve this
problem we are understanding what is
additive analytics and that is why I am
presenting this case study so here what
has algorithm done it has actually first
randomly assumed the centroid of each
cluster okay and then try to find out
the distance but then what it does after
assuming the centroid of each cluster it
will try to find the distance between
the centroid of the Walmart with each of
the point which is there on the plane XY
Z plane right and try to find out which
points are closer to this centroid so
the centroid will move towards that
right so imagine actually in this
diagram ideally the centroid will move
but let's assume that the points are you
know coming closer we'll just assume
that way similarly it has done it for
all the other you know questions and you
can see that the points it will try to
group the points which are closer to the
centroid of that cluster now this is
called one iteration all right this is
this completes first iteration now when
you complete first iteration after that
the algorithm will redo the same
activity again okay and it goes for the
second iteration now the question is why
does algorithm have to go for the second
iteration why can't it say that the
points that it has find out are the
perfect point for this cluster it has to
do the second iteration because the
first iteration remember we have assumed
the center
the centroids were not permanent were
not fixed we have assumed that so
anything which is based on assumptions
we can't really lie on that so we have
to perfect the centroids we have to fix
the centroid and that is why algorithm
will redo this activity it will
recompute the centroid of this points
again and based on that it will try to
you know find out the distance between
each points now you know my diagram
looks little weird for you that's fine
don't worry just try to understand the
concepts what I'm trying to highlight a
point is in the iterative analytics this
algorithm the clustering algorithm does
this iteration does this you know I mean
it has to perfect the centroids and for
that it has to rerun this iteration
again and again to finalize the centroid
and the question is now when does it
stop so it will stop when the centroids
are not moving from the previous
iteration right then it will say yes now
I have finalized the center that is
where it stops so this is call iterative
analytics right this is one of the
example of iterative analytics so that
all of you can easily understand what do
we mean by iterative analytics now most
of the machine learning algorithms are
right rative in nature and they would
need alternative analytics to solve any
problem and that is the major area in
any data size any editor analytics that
you do you will need those you know you
need to do IT team analytics and that is
where University of Berkeley they found
out that ok if we want to do a trading
analytics we have a MapReduce as one of
the you know a technique of Hadoop to
solve this problem right so let us
understand little bit on MapReduce now
I'm not really going to cover the
architecture of MapReduce in the
interest of time I'm just assuming that
you know little bit on that but let's
understand this way let's say I will
still give you a brief idea on that so
we have a MapReduce and MapReduce works
in three phases map shall fall short and
reduce okay so you have certain data in
the disk the map phase will read the
data from the disk process the data and
write the output into the disk again
then the shuffling sort read that data
from the disk does submerging and then
finally write the data into the disk the
radius will also read the data from the
disk does some processing and finally
write the output into the disk now you
may have a question what is there in Map
Reduce if you don't know MapReduce don't
worry about it just assume that it has
three phases and you have certain logic
that you need to define as a developer
in these two phase map and reduce and
the framework executes it accordingly
but ultimately it helps you to process
the data now imagine I want to do a
trade even or it takes the clustering
algorithm I want to implement it using
MapReduce in that case this is what
happens in the first cycle first
iteration this is actually first
iteration iteration one in that we are
going to the disk at every stage of
MapReduce right map surface water and
reduce now imagine if it is a iterative
analytics for each iteration we have to
go to the disk right you have map
shuffles not and reduce it goes to the
disk read the data it again goes to the
disk read the data goes to the disk read
the data
this is iteration two likewise we have
iteration three attention for and there
could be n number of iteration which
will do the same thing all right so
every stage of each iteration we are
going to do going to go to the disk in
MapReduce and do the operation so there
will be tremendous disk i/o at every
stage of MapReduce and an image in there
are n number of such iteration that
means there are so many stages which
will happen and every says if you go to
disk i/o
the overall performance of your program
is going to be very very slow and that
was the major concern that was the
bottleneck of MapReduce framework that
was the limitation the MapReduce
framework cannot help you get the
processing faster because united even
analytics you have to do multiple
iteration and MapReduce goes to the disk
at every stage this was the primary
reason why
very alarming reason why University of
Berkeley thought that we are not really
going to Map Reduce basically is not
really going to help us do the iterative
analytics in a much more faster manner
and we definitely need an alternative
solution in terms of some framework and
they named it as path to basically
process the data in a much more faster
manner and that's how spark came into
existence so this was the motivation of
why spark was developed so their primary
school of thought was instead of going
to the disk and processing the data at
every stage you just read the data into
the disk and then try to keep it in the
memory we call it as RAM try to keep all
the data and RAM and then the last
output will be stored in the disk every
iteration you just try to keep hold the
data into the memory okay now this is
where you are trying my design the
framework is designed in such a way that
it does lot of in memory processing and
hence it is actually going to do the
computation much much faster than
MapReduce and that is the primary
selling point of map I'm sorry a spark
over MapReduce as a distributed
processing engine okay now you may have
a question with this explanation that
what does it does it mean that spark can
hold all the data in memory and it would
not be practically possible because if
there is big data we will not be able to
hold all the data and memory throughout
the lifecycle of the program so that is
a true statement spark doesn't really be
able to hold data into the memory all
the time unless you know your cluster
has lot of memory so spa definitely try
to spill over to the disk in between
right when the data is not used for a
longer period based on the LRU policy
less reason to use policy so it is
trying to have it spark has a capability
to do intelligent memory management and
that's how it tries to free up the
memory when it is it is getting
acquiring full memory and that's how it
tries to make sure that all the data
which needs to current
processing should be in memory and the
older data should be removed which is
not needed so it was definitely
spillover to the disk with the ratio of
spilling over to the disk compared to
MapReduce is very low and that is why
SPARC is actually very efficient
framework
it also has concepts like caching lazy
evolution and lazy execution because of
and it executes data in the form of Dec
and that is why SPARC is actually
performing much faster and better than
MapReduce right now we are going to
understand detail architecture of spark
in the actual spark training and we
would really welcome all of you to join
the training to understand much better
about spot but this is a very brief
overview of why spark is needed as I
told you this is about iterative
analytics and there is also another
reason why spark was developed which was
related to interactive analytics as you
can see here and interactive analytics
means if some of you who already work on
Hadoop right who already have worked on
hive or Pig will know what do we mean by
interactive analytics it's about ad hoc
query analysis right normally data
analysts would like to query data to you
know find out some issues within the
data or find out the patterns within the
data they will try to query some tables
generally you know write some scripts to
get the the result of the data so that's
called ad-hoc query analysis and for
that also lot of time data analysts will
have was was actually facing an issue
that when they write a query to join
let's say multiple tables in hive I will
store you know big data in tables and if
you want to really join those tables
I will internally convert query into
MapReduce so again it is going to fall
back to use the use MapReduce engine to
do that and if you have written one
single high query which is an S in an
SQL format to join multiple tables
internally hive engine may convert that
into more than one MapReduce programs
it's called chaining of MapReduce jobs
so that means it is again very similar
to what we have understood here this is
job one this is job - this is job three
so
any job will be having MapReduce math
shuffles or reduce microphones not
reduce and that's why each job is going
to the disk many times to do the
processing and again there will be a
tremendous disk i/o so data analysts
were also not able to perform that hoc
query faster and that was another reason
why they wanted a framework which can
let them do a door query analysis much
faster and they don't have to wait for a
longer period to complete the operation
so these are the two major motivation
why spark was developed alright and
these are some of the slides which
actually backs up the claim that I have
just made which is you know as you can
see the we are reading the data from the
disk in the map and then we are writing
the output into the days then radio
surface also reads the data from the
disk and write the data into the days so
each stage here shuffle and what we are
assuming is part of the reducer only but
each stage passes through the hard
drives so if you have a right relative
jobs you can see every stage goes
through the disk so there is a
tremendous this guy you involved in that
and that's the primary reason why
MapReduce was not efficient in doing
item analytics okay so the primary
concern was there is lot of disk i/o can
we do something better and replace it
with the memory so if you can see here
there are two each iteration has a disk
each iteration goes through the disk and
now we will use memory instead of disk
between iterations so that we don't
really have to do this guy at every
iteration keep the data as much as
possible in memory to get better
performance and that was the primary
design change that spark brought in
compared to MapReduce okay so again I
repeat SPARC is a framework which helps
you to process your data much faster
than MapReduce it is a framework which
is a processing engine which can replace
MapReduce but that is not really going
to replace Hadoop's a lot of corporates
now are using Hadoop as a storage and
resource management layer and the SPARC
is a processing engine and that's how
they are trying to marry both these
frameworks to do the data
and that's a very current latest
combination of technologies to work on
big data in terms of storage and
processing okay so this is a this is the
discussion on you know why spark and
what is spark and white the difference
between spark and you know I do now
spark actually builds a lot of ecosystem
components as I have just mentioned here
so if you can see here this is the spark
ecosystem where you have you know spark
SQL you have spark streaming ml lib
graphics parka these are some of the
ecosystem components on top of the spark
core engine which is the master
you know component of spark which does
all the in-memory processing so you know
they have also come up with lot of
ecosystem component family where if a
person doesn't know how to write program
in spark he can actually write it in a
spark SQL which expects people to write
rate analysis logic in SQL query
internally that engine would convert
that into spark program and execute it
on the SD FS data all right similarly
there is a spark streaming component
which is another differentiating factor
between hadoop mapreduce framework and
spark hadoop mapreduce by the way is a
framework which gives you batch
analytics or best data processing
capability it is not helping you to do
real-time analytics right where a spark
with the spark streaming can give you
real-time analytics as well so spark
support both batch and real-time where
is Hadoop is adopt MapReduce is just
batch okay because as a framework it
cannot support real time whereas fog
does support real time as well as match
okay when I say do Diamond heretics I
can give you a quick use case you swipe
a credit card and I want to find out
whether it's a fraud or not that's a
credit card fraud detection or a debit
card fraud detection is a real time use
case real-time analytics use case all
right the stock price changes frequently
and you want to do some analytics on top
of that that's of real-time analytics
all right you you want to there are a
lot of other analytics interesting use
cases out there and you know
are built using these frameworks these
technologies so spark works in the
combination of Kafka to achieve the
real-time analytics however will not get
into that discussion right now now apart
from that we have a component called ml
lid it's a machine learning library of
spark so all the machine learning
algorithms one of the use case that I
explained called clustering is like that
there are many algorithms are developed
in ml lip which is basically these
algorithms are written in a spark way so
that they can be easily in executed on
the spark engine okay by the way one
thing I would like to highlight the
entire spark and all its ecosystem
components are developed using Scala int
a spark is written in a language called
Scala which is a functional and an
object-oriented programming language
very you can compare it with Java there
is no performance gain you you get when
you develop program on Scala so another
important you know a notion of thought
that developers would have if I use
Scala I will get better performance
that's a completely wrong statement if
you use Scala or if you use Java there
is no difference in performance because
both Scala and Java will internally be
compiled into a bytecode and which will
be executed on JVM okay so both will
need JVM and hence there is no
performance difference but definitely as
a developer you will love Scala more
than Java going forward even though if
you are ardent lover of Java you will
start falling in love with Scala I'm
actually using the terms like love and
all because this is something that I
have also experienced it's really really
really very easy language to pick up on
and to start working on that it's really
concise programming developers work is
tremendously reduced by Scala you can
write ten lines of code in Scala which
may be 200 lines in Java that's
the difference however Java 1.8 and 1.9
onwards Java is also making an attempt
to do lot of concise programming in Java
itself Java has also introduced lambda
expressions which is more like a
functional programming style so that's a
good news for Java developer however I'm
just giving you a brief idea on this
languages so entire SPARC is written in
Scala okay
however SPARC allows developers to write
spar programs in any of the three
language Scala Java or Python you can
use one of these three language to write
program in SPARC and execute it on the
spot okay now there is one more
programming language which you can use
which is called R it's R is amis you
know again a statistical analytical tool
and we can also call it as a programming
language because you write its language
itself and there are a lot of machine
learning libraries available in that but
R doesn't really do you know big data
processing it's not having a processing
capability of Big Data it does just a
small amount of data in memory and hence
if you want to leverage a big data
processing capability you can write your
analytics logic in R and then run it on
SPARC to leverage parallel processing
capability and for that there is a spark
R component developed so that data
scientist can write a algorithm in R and
integrate it with spark R to leverage
parallel processing benefit ok and they
also have graphics library for doing
some graph processing systems like for a
Facebook friends Network ending in
friends Network so this is what's path
provides alright and this is the
advantage of spark so this is what I in
fact wanted to touch base on in terms of
what is Park and why it is better and
you know what is the difference between
an open spot so this is where I want to
you know end my discussion on covering
the topics we just wanted to keep it at
at a level where all of you can easily
understand why SPARC is needed and then
if you really want to understand more
feel free to ask more questions shoot
out your questions to the email address
provided as well as we will welcome you
to join the training where
we'll have a great learning and a very
detailed learning on this technology
okay so will that with that note I would
open out for any Q&amp;amp;A if you have so any
questions you have please ask Richard if
you are there let's try to the questions
excellent and I've already been
collecting questions while you were on
there we a lot of people who are already
jumping in I'm gonna do two things one
I'm gonna take over your monitor here
for the next five to ten minutes as part
of the question is we have the the poll
that we always launch at the end okay
and I'll let you know when I give you
the screen back in case you have
something you want a more explain or
have some slides you want to show and
then I always take the first question in
you know with a group this large we do
have a lot of people back there people
are always asking is there a recording
of this session for later and the answer
is yes we do record all these sessions
if you go to simply learn calm and go
under support just send them a note and
request and they'll send you out a link
for the recording it does take up to 12
hours because they do double check them
just to make sure that everything came
through correctly so allow them a little
time there and then they'll send that
out to you also a quick hello to the
California team we have somebody from
California at the beginning say the
California team was saying hello I'm not
sure who that is and also dr. Vera glad
to have you here hello back for you from
the beginning on questions we have a
number of questions and let's go ahead
and start with one that was early
somebody who is just starting their
career into the Hadoop and the spark and
the Cassandra and the MongoDB system
what would be your suggestion for them
and we'll start with that like I said
the beginning and then I have some more
technical ones we'll jump into what
would be your suggestion to them as far
as certification and training that they
would want to go through specifically
Hadoop only Pig hive then to spark and
then - Cassandra MongoDB with a question
mark after it
all right yeah that's that's a that's a
very good question and in fact that is
very important question for everyone who
wants to get into this technologies so
you should definitely get into learning
the systems in this manner you don't
really I mean what I would really would
like to advise is don't just you know go
behind the technology names rather it is
Hadoop or spark or anything but try to
understand the fundamental concepts that
you need a distributed file system
understanding you need to know how data
gets distributed so you should be
familiar with and you should have a good
hands-on experience with how data gets
distributed in the system so that should
be one list one topic in your list which
is distributed file system then once you
store the data you should have one topic
in your list which says how do i process
this data in a distributed manner so you
should be learning one processing engine
that is another topic and the third
topic would be resource management and
you know then on top of that you can add
later on some of the high level
components which you have mentioned like
big hive and all which are obviously
needed to be used but then not a single
company can give you a guarantee that
you know we are having big and high both
or we are having been using big or very
high so don't ever you know develop your
skill a core skill in pig or hype rather
try to focus on these core components
like distributed systems distributed
processing engine the source management
capabilities once you understand once
you have them you know once you know the
these components very well then building
skills on top of this will be easier
right so if you if you know this you
will have a better chance of you know
getting good opportunities now coming
back to the distributed file system and
the processing engine currently in the
market as DFS which is from Hadoop or
true sequel databases which could be no
sir which should be Cassandra or MongoDB
you can pick up all of them or any of
them but be very careful you need to
first understand the difference between
has DFS and no sequel there is a lot of
difference you need to know when to use
what but I would suggest you understand
as DFS
and understand one of the no sequel
database thoroughly coming back to the
processing engine spark no doubt is the
hot cake right now in the market should
learn that right and to learn spark you
should also understand a basic of
MapReduce you should not ignore
MapReduce because it the way it
processes the data is using divide and
conquer and spark also uses that in fact
you will be surprised to know that you
can write MapReduce program in spark so
it's better if you know some of the
concepts of spark so that you can easily
appreciate those concepts in spark
matter right so you should know little
bit on sparkle sorry MapReduce as well
and then coming back to the resource
manager layer you should learn yon from
Hadoop or missus one of these two you
should have a very good understanding
that's it and you're set you get a good
job all the rest instead reading some
good money on that that's always
exciting that it's all about money a lot
of times it's also about enjoying your
work like I love playing with data and
myself we have a lot of ways it's hard
to search all the questions because
there's some that are very similar let
me see if I can group some of them
together let's go into this can you go a
little bit more as to how spark and
Hadoop exists together on the same
system how that they leverage each other
I'm just kind of like is it replacing it
is it are they in competition with each
other and one person asks do they exist
in the same project and if so when will
they do that can you go a little bit
more into that part I think this is
something which really need a lot of
time to you know explain but I would
still try to you know explain it quickly
so for example any you know Hadoop or
spark is actually a you know
master/slave architecture so you can see
here by the way can you just help me
share the screen hey if you want
anything I can't like close and open up
the poll and I'm supposed to keep it
open for like another a few more a
little bit longer here do you want to
some water in this question yeah right I
think it's a good idea
let's come back to this version later
okay so we'll come back to that a little
bit more and how the two function
together and separate from each other
and then one of the questions that came
in is also hard we're always an
interesting question because it's very
hard to pinpoint Hardware usually when I
when I have seen this question come up I
have to ask specifically have you had a
project where you've seen the difference
what is the difference in cost when you
set up a Hadoop file system and then you
want to do the spark either on it or
running on its own as far as expense and
hardware and setup right it's so
definitely a varied school of thought
that anybody would first of all have a
question when it comes to memory in
memory processing anybody would have a
question that does it really store all
the data in memory and if yes that that
means I need lot of memory that means my
cost is going to go high so do I need a
lot of memory in my do I need to upgrade
my cluster do I need to spend more money
well the answer is spark performance has
been tested out on the same resources
where hadoop mapreduce was working and
and in fact you know there is a company
called data breaks which has actually
set a world record in sorting terabytes
of number using the same cluster
resources right where they are they are
actually able to do it much faster in
spire than any other processing
framework on the same resources they did
not increase memory or CPU or anything
right so that set the benchmarking that
you don't really need to increase your
RAM or processing capability if you want
to run SPARC on the same cluster where
Hadoop is running right having said that
if you give more memory to spark spark
can leverage it better so it definitely
gives you better performance but it
doesn't mean that you need to have more
memory if you run SPARC I hope you gain
the clarity excellent thank you yes I I
know that I work with small businesses
so it's like the first question one of
the questions I always get asked is how
much does it cost which is a very hard
question to answer there's no there's no
easy answer for
that tapas had a question and I'll go
ahead and answer this one does cloud
declared era has the IDE for spark tapas
cloud era install usually by default
unless you change it include spark so
when you run a spark single note install
anything like that you'll have the our
cloud era installed for Hadoop you'll
have spark on it let's see is there a
difference between Hadoop and data
science where is a connection between
them that's from Benson so Hadoop and
data science how are those related
there's a big difference actually so
Hadoop is a technology and data science
is a concept right it's not a technology
it's actually it's a it's a it's a very
vast area which which has data
visualization data engineering machine
learning you know statistics and
probability everything falls under that
it has a major components inside it
right so data science is anything that
you want to do with data is actually
part of data science right it's not only
that you know those those mathematical
geeks who do that work they are only
doing the data science work in fact
anybody who does ETL work anybody who
does data analyst work anybody who tries
to play with data you know churn out
data to generate some reports every
activity is considered and data science
but data science has various streams and
one of the stream is called machine
learning that's where you know lot of
focus and you know that's where people
or corporate focus more because they
want to leverage interesting patterns
out of the data to remain competitive in
the business but apart from that there's
a data engineering which is one of the
big streams where you know we have
already data we want to process data
faster we want to store data faster so
that data engineering is one bucket
which is one branch of data science and
to achieve that we have Hadoop as one of
the technology which enables us SPARC is
another technology which enables to do
faster processing right so data
engineering has technologies which is
hot open spark and data engineering
falls under data science excellent
yes and I could Sikhs they always you
always see big data Hadoop and data
analytics is how it's almost always we
need to do searches but the data
analytics is separate that's excellent
well said from narender he says hats off
to you J MapReduce explains so simple
and straight so you had a little bit
extra point there from Narender and then
he also would see what see there with
you for just a moment here thanks let me
go ahead let me go ahead and close the
poll now and then hand the screen back
over to you we can go back to the
question on spark and Hadoop and how
they connect and fit together
there we go I'm gonna have me turn your
screen to you that seems to be one of
the big questions is this is it the
spirt versus Hadoop or is it spark in
conjunction with the doop how do those
fit together and connect right so this
is a small diagram that I'm drawing here
so normally in this cluster right
whether it is Hadoop or spark generally
corporates will not have separate
clusters they will install Hadoop and
spark together on the same cluster right
so you have a monster machines and now
there are various topologies and we're
not really going to discuss them but
there's a master/slave is a fundamental
architecture that these guys follows
these two frameworks follows okay so you
have you know these five processes which
are part of Hadoop name node data node
secondary name node which makes up the
SDF s which gives you the storage and
processing which in fact go gives you
the storage and reading of the blocks
that you store in HDFS basically any
data that you store in s DFS you have
these three process which will take care
of that when I say process they are
nothing but they are Java programs which
are part of Hadoop libraries okay
apart from that you have resource
manager and node manager these are two
more Java processes from Hadoop which
helps you to do yarn and MapReduce which
means it helps you to do resource
management and the processing of data
that is also one more process which is
called application master which is not a
daemon process which is
inch which is initiated only when it is
required to process any data however
right now assume that these are the five
processes which is running in the
cluster when you install Hadoop okay now
coming back to spark spark has both
master and worker these are the two
processes of spark there is spark
context as a driver which will be
initiated for every spark program so
when you install spark you will have
spark master right I don't have enough
space so sorry for my bad handwriting
there is a spark worker which is running
here so spark master and spark workers
are coexisting with the name node and
resource manager and data node a node
manager so you can see that name node is
a master and data node is a slave for
the HDFS the source manager is a master
and no man is it is a slave for Yan and
MapReduce spark master is a master and
worker is a slave and all masters will
run in master all slaves will run in
slave that's how you should understand
right now as I said you can there is you
know there are more master than the
machines you install for better high
availability of the master and meta
resource management
you also do Federation on the name nodes
for faster performance but as I said
there are different topology we need to
consider when we did detail out the
architecture but right now you just
assume this way so spark and Hadoop can
coexist together on the same cluster it
does coexist and this is how you should
visualize it excellent excellent
thank you let's see we have got a
question from Sam and you can correct me
if I give him the wrong information he
says what type of computer should be
used for learning this Sam you could
there's a lot of different options the
generally its installed on the Linux
operating system that doesn't mean you
have to go buy a Linux computer to do
that you can download VirtualBox and
both cloudera and Hortons have a version
of both Hadoop with SPARC already
bundled with it for what they call a
single node in
Staal it's definitely not big data but
you can actually use it and see how it
works in our classrooms in the developer
classrooms that simply learn we do have
you have access to a cloud lab where it
is a five node set up and you can
actually see how that functions and as
part of the cloud lab in our in our
learning sessions and then there's also
just recently or not recently sexually
been around for a while
there's also docker setups a little bit
more complicated but in some ways they
actually they actually have a little
less resources and strain on the
computer all those can run on a Mac they
can all run on a Windows when you log
into cloud lab you can log in you're
logging in through your browsers you my
mom there all the time and I have a
Windows PC and we have a number of
students who also have Apple machines I
also have a Linux computer that I've
actually installed the Hadoop on so all
of those can be used depending on what
level and what you have available
generally you want to have probably
twelve megabytes on the computer I have
eight on one of mine and it runs slow we
do have some people who run all the way
down in six doing the VirtualBox those
you want to run a cloud lab at that
point you want to be online someplace
else so yeah hopefully that got throughs
question answer about eight gigabytes
memory all right it's that's sufficient
to run VirtualBox but it's going to run
slow if you can you want to have twelve
gigabytes let's see I hope that answered
that do you have anything else on that
as far as what kind of computer or setup
they would have need as far as
practicing well I think they would they
would need you know any Windows B's or
Mac Mac OS base
laptop is fine but you know if they are
really planning to install a Hadoop
cluster in the local machines if they
take Apache distributions then the 4 GB
RAM is enough ok but if they take
clouded or Haughton works sandbox then
they need at least 8 GB RAM for Haughton
works I would prefer 16 GB for cloud era
8 GB is enough okay so that's the RAM
configuration in terms of CPU do
CPU should be enough for your machine so
that's the end the 64-bit operating
system so these are the three basic
hardware requirements that you should
have excellent yeah that's that's what
I've been working with the lab since
they do a lot of work in the background
and that's what I've had to give you
guys an idea if you're running a virtual
box with Cloudera on it too what you can
do on a computer run eight gigabytes ram
it takes about four minutes to load so
you can see why you really want the
twelve gigabytes ram if you can you know
if you go in the the dual-core processor
if you have a quad that's even better if
you before four core processor from
Vignesh the next kamar choosing a career
in SPARC better than Hadoop or Hadoop
alone is enough well learning SPARC
enhanced my career growth which has more
package in the current market well I
think that's a that's already straight
director question and probably why not
so well I would as I was explaining that
the industry is actually focusing
towards spark more and more in terms of
as a as an option of a distributed
processing engine so definitely if you
have a good skill on spark there is
definitely a very very valuable skill
right now in the market in terms of big
data processing so you should definitely
aim for spark Hadoop is also needed
because lot of companies have already
done lot of work in Hadoop so it's not
going to go away and as I said HDFS and
yon is still being used widely and
that's why you should have both
understanding if you have combination of
Hadoop and spark you know you can throw
us a party by getting a good salary like
I want the party don't forget to call us
and invite us right we have tea and I'm
not this one throws me for a loop so I'm
not sure if I understand the question
right but I'll just go ahead and read it
to you how would you say compare Marc
logic all he put it is all one word ma
RK l oh gee I see how WR does it fit in
the spark framework
or as MarkLogic is that a stone
standalone project which Big Data
technology is best suited where a lot of
crud CR UD capitalized operations are
involved okay so I think I would
probably not be able to answer it
clearly because even I am not really
very sure I don't have much idea about
mark magic but when it comes to the crud
operation I'm assuming that you know
he's talking about a lot of you know ETL
kind of operations if that is his
question then definitely Hadoop inspire
both are very suitable components where
you can load the data from the external
sources into Hadoop and then process it
or a spark and generate reports now for
reporting tools you need to use nab a
tool which could be tableau or QlikView
and integrate it with your you know
hadoop storage systems and for getting
the data into Hadoop you should have
tools like scoop or Kafka to get the
data in real time or in batch from
external sources it could be database or
it could be logs or it could be file
system it could be Twitter kind of
source into Hadoop so you know you have
to consider all the various big
deterring components to create this
pipeline which helps you to bring the
data from external source getting into
the HDFS process it or a spark and then
generate reports using bi2 that's how
companies are using it so if that is
your question I believe this is the
answer for that excellent yes that's
that's that's what I would come up with
that sounds really good so we have time
for maybe one or two more looking a
little bit more over just a quick
reminder exceda couple people asked
again yes you can get the recordings
afterward you do need to go to something
learn calm and just request them from
the support staff there and they'll
email you out the link it does take up
to 12 hours let's see we had a there's a
question on hive unfortunately I think
to really get in a difference of hive
and all the different queries in Hadoop
would take too long apologize for that
hive is specific to SQL or it's actually
called hql hive query language it's very
similar to SQL recording of session
what books would you reference for
learning it would be the top references
you would suggest for people in addition
to taking a class it simply learned for
them to go and and research and look a
little bit more in-depth as a reference
spag toward a budget or quasi that's the
best website you can actually get you
know a lot of materials and you know
online recordings from various sources
and I would really not discourage you to
go through them they are very useful but
then the best and in fact inspark Apache
has created the best documentation and
it's very detailed to the date latest
documents so you just go through the
different versions and read about
everything in fact they have also given
you the examples on every concept and
those examples are also mentioned let me
just you know give you a quick idea on
you know spark you know if I just want
to kind of give you an premiere of what
I'm really trying to tell you this is
what we are trying to say so you go to
the documentation in fact you should
just start from spark
hadji website and then here you know we
are basically trying to explain you that
you need to go to the documentation go
to the latest release and then it comes
to this programming right so you have
various components you can start with
any component whatever you want to
understand for example data friends so
if you go scroll down every detail
components are mentioned along with code
so you can see there is a code written
in Java or Scala or Python right so NR
as well so in fact it's a very good
tutorial for anybody to start and also
get a very advanced level of
understanding but for
and then you go to the source that is I
definitely reference that regularly yeah
get github github repository they have
actually provided a lot of source links
as well in the same you know a website
so you can go to them as well alright
and then from Minaj I'll see if I can
answer this question if I up for the
full training how long will I have
access to the labs after the training is
done etc Minaj it depends on which setup
you sign up for if you do the most basic
one if I remember correctly it's 90 days
afterward I'd have to look it up but
you'd want to go to the simply learn
site and just there's a pop-up and you
can actually request up from the little
chat window that comes up and some will
answer that question right away for you
it is more than long enough to go in
there and play with it for a while and
then once you have a handle of it on
there
definitely I actually know one of the
people we work with he walks into
working with us a job interview he'll
take his laptop in and open up his
VirtualBox and actually show them stuff
right then and there on his laptop so
you know having both is really important
being able to access the lab and having
it if you can if you have the resources
you know not everybody has a man to go
by a 12 gigabyte a fancy computer when
they start a new class but you do have
plenty of time on the labs and then
there's also some times we do extension
if you do the master's programs you'll
have even more time you have people who
sign up for like it there's like a I
think there's a year program you can
even sign up for that goes through the
whole year so it just depends on what
you sign up for I believe we're gonna
run out of time here
I want to thank narender he gave us a
definition of Mark logics I didn't know
what Mark logic was database to
integrate data silos so I'll probably
look that up I'll probably go google
that when we get offline here this I
have learned something new today
I believe that's let's see simply learn
placement was take one more question
does simpler give placement to their
students and with which companies do you
have ties from
I don't have as far I know I know people
there's different areas and I myself
have not had a job placement through
simply learn the companies I've looked
at that do that also charge 20 times
more so there's you know there's a
difference in price and there certainly
is a connection with the staff where
they do have you know sometimes you talk
to some of the teachers and I know
they'll reference you would help you
with what direction to go in and what
places to go to look for work so we do
have that depending on which teach here
have and they're really good about it
but I don't we don't have there's not
like a specific placement in the class
for that do you know anything like that
Jamin as far yeah even I have not heard
of motored so I don't think they have
but then yeah it's a good idea to you
know check with the simplement team to
get a confirmation on that correct they
never actually suggested them doing more
kind of stuff like that so and that's
the same thing that sanjay of yours is
the kind of the same question so they
don't have a specific placement
afterwards but I do know a lot of the
teachers but if you ask them they'll
start telling you where to go where to
post your stuff who to talk to in the
programming industry I'm finding in my
area it's so important to get out there
in fact I do I'll be doing a free lesson
in a month specifically on Map Reduce
and spark for the local companies and
businesses because they asked for it
that gets my name out there and then
they actually get to meet me before they
hire me and before I do work for them so
there's a lot of different ways to get
out there at your job placement
definitely you know never teach you
choose or whatever classroom you come
and ask them and they'll send you in the
direction to forums to post and things
like that I believe we are running out
of time here I'm gonna go ahead and wrap
this up Jamin if that's all right with
you unless you have anything last minute
thing you want to go ahead and ask or
mention or bring up I'm trying now I'm
saying excellent well thank everybody
for being here today and thank you Jamin
for being here
and we look forward to seeing everybody
in our classrooms thank you for sharing
thank you and thank you everyone thank
you for listening us patiently thank you
excellent hey take care never enjoy your
day today
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>