<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning Tutorial | Deep Learning TensorFlow | Deep Learning With Neural Networks | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Deep Learning Tutorial | Deep Learning TensorFlow | Deep Learning With Neural Networks | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning Tutorial | Deep Learning TensorFlow | Deep Learning With Neural Networks | Simplilearn</b></h2><h5 class="post__date">2018-04-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/skf35x1lNV4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to this tutorial on
deploying my name is Molly and in the
next about 1-1 hours I will take you to
what is deep learning into tensorflow
environment to show you an example of
deep learning
now there are several applications of
deep learning really very interesting
and innovative applications and one of
them is identifying the geographic
location based on a picture and how does
this work the way it works is pretty
much we train an artificial neural
network with millions of images which
are tagged their geolocation is tagged
and then when we feed a new picture it
will be able to identify the geolocation
of this new image for example you have
all these images especially but maybe
some significant monuments are all
significant locations and you train the
millions of such images and then when
you feed another image it need not be
exactly one of those that you have
claimed it can be completely different
that is the whole idea of training it
will be able to recognize for example
that this is a picture from Paris
because it is able to recognize the
Eiffel Tower so the way it works
internally if we have to look a little
bit under the hood yes these images are
nothing but this is digital information
in the form of pixels so each image
could be a certain size it can be doing
6 by 256 pixel kind of a resolution and
then each pixel is either having a
certain grade of color and all that is
fed into the neural network and it then
gets trained in and it's able to based
on these pixels pixel information it is
able to get trained and able to
recognize the features and extract the
features and thereby it is able to
identify these images and the location
of these images and then when you feed a
new image it kind of based on the
training it will be able to figure out
where
images from so that's the way a little
bit under the hood how it works so what
are we going to do in this tutorial we
will see what is deep learning and what
do we need for deep learning one of the
main components of deep learning is
neural network so we will see what is
neural network what is a perceptron and
how to implement logic gates like and or
nor so on using perceptrons the
different types of neural networks and
then applications of deep learning and
we will also see how neural networks
works so how do we do the training of
neural networks and at the end we will
end up with a small demo cord which will
take you through intensive law now in
order to implement deep learning code
there are multiple libraries or
development environments that are
available and tensorflow
is one of them so the focus at the end
of this would be on how to use
tensorflow to write a piece of code
using Python as a programming language
and we will take up a an example which
is a very common one which is like the
hollow world of deep learning the
handwriting number recognition which is
a M nest commonly known as M nest
database so we will take a look at M s
database and how we can train a neural
network to recognize handwritten numbers
so that's what you will see in this
particular video so let's get started
what is deep learning deep learning is
like a subset of what is known as a
high-level concept called artificial
intelligence you must be already
familiar must have heard about this term
artificial intelligence so artificial
intelligence is like the high-level
concept if you will and in order to
implement artificial intelligence
applications we use what is known as
machine learning and within machine
learning a subset of machine learning is
deep learning machine learning is a
little bit more generic concept and deep
learning is one type of machine learning
if you will and we will see little later
and maybe the following slides
little bit more in detail how deep
learning is different from traditional
machine learning but to start with we
can mention here that deep learning uses
one of the differentiators between deep
learning and traditional machine
learning is that deep learning uses
neural networks and we will talk about
what are neural networks and how we can
implement neural networks and so on and
so forth as a part of this tutorial so
little deeper into deep learning deep
learning primarily involves working with
complicated unstructured data compared
to traditional machine learning with
where we normally use structured data in
deep learning the data would be
primarily images or wise or maybe text
file so and it is a large amount of data
as well and deep learning can handle
complex operations it involves complex
operations and the other difference
between traditional machine learning and
deep learning is that the feature
extraction happens pretty much
automatically in traditional machine
learning feature engineering is done
manually the data scientist V data
scientists have to do feature
engineering feature extraction but in
deep learning that happens automatically
and of course deep learning for large
amounts of data complicated unstructured
data
deep learning gives very good
performance now as I mentioned one of
the secret sources of deep learning is
neural networks let's see what neural
networks is neural networks is based on
our biological neurons the whole concept
of deep learning and artificial
intelligence is based on human brain and
human brain consists of billions of tiny
stuff called neurons and this is how a
biological neuron looks and this is how
an artificial neuron look so neural
networks is like a simulation of our
human brain human brain has billions of
biological neurons and we are trying to
simulate the human brain using
artificial neurons this is how a
biological neuron looks it has ten dry
and the corresponding component with an
artificial neural network is or an
artificial neuron are the inputs they
receive the inputs through dendrites and
then there is the cell nucleus which is
basically the processing unit in a way
so in artificial neuron also there is a
piece which is an equivalent of this
cell nucleus and based on the weights
and biases we will see what exactly
weights and biases are as we move the
input gets processed and that results in
an output in a biological neuron the
output is sent through a synapse and in
an artificial neuron there is an
equivalent of that in the form of an
output and biological neurons are also
interconnected so there are billions of
neurons which are interconnected in the
same way artificial neurons are also
interconnected so this output of this
neuron will be fed as an input to
another neuron and so on now a neural
network one of the very basic units is a
perceptron so what is a perceptron a
perceptron can be considered as one of
the fundamental units of neural networks
it can consist at least one neuron but
sometimes it can be more than one you're
on but you can create a perceptron with
a single neuron and it can be used to
perform certain functions it can be used
as a basic binary classifier it can be
trained to do some basic binary
classification and this is how a basic
perceptron looks like and this is
nothing but a neuron you have inputs X 1
X 2 X 2 xn and there is a summation
function and then there is what is known
as an activation function and based on
this input what is known as the weighted
sum the activation function either gets
gives an output like a 0 or a 1 so we
say the neuron is either activated or
not so that's the way it works so you
get the inputs these inputs are each of
the inputs are multiplied by a weight
and there is a bias that gets added and
that whole thing is fed to an activation
function and then that results in an
output and
if the output is correct it is accepted
if it is wrong if there is an error then
that error is fed back and the neuron
then adjusts the weights and biases to
give a new output and so on and so forth
so that's what is known as the training
process of a neuron or a neural network
this is a concept called perceptron
learning so perceptron learning is again
one of the very basic learning processes
the way it works is somewhat like this
so you have all these inputs like x1 to
xn and each of these inputs is
multiplied by a weight and then that sum
this is the formula or the equation so
that's some WI X I Sigma of that which
is a sum of all these product of X and W
is added up and then a bias is added to
that the biases not dependent on the
input but or the input values but the
bias is common for one neuron however
the bias value keeps changing during the
training process once the training is
completed the values of these weights W
1 W 2 and so on and the value of the
bias gets fixed so that is basically the
whole training process and that is what
is known as the perceptron training so
the weights and biases keep changing
till you get the accurate output and the
summation is of course passed through
the activation function as you see here
this WI X I summation plus B is passed
through activation function and then the
neuron gets either fired or not and
based on that there will be an output
that output is compared with the actual
or expected value which is also known as
labeled information so this is the
process of supervised learning so the
output is already known and that is
compared and thereby we know if there is
an error or not and if there is an error
there it is fed back and the weights and
biases are updated accordingly till the
error is reduced to the minimum so this
iterative process is known as perceptron
learning
our perceptron learning rule and this
error needs to be minimized so till the
error is minimized
thus iteratively the weights and biases
keep changing and that is what is the
training process so the whole idea is to
update the weights and the bias of the
perceptron till the error is minimized
the error need not be zero that are may
not ever reach zero but the idea is to
keep changing these weights and vice so
that the error is minimum the minimum
possible that it can have so this whole
process is a nitrated process and this
is the eye creation continue until
either the error is zero which is
unlikely situation or that is the
minimum possible within these different
conditions now in 1943 two scientists
Warren McCulloch and waterbeds came up
with an experiment where they were able
to implement the logical functions like
and or nor using neurons and that was a
significant breakthrough in a sense so
they were able to come up with the most
common logical gates they were able to
implement some of the most common
logical gates which could take two
inputs like a and B and then give a
corresponding result so for example in
case of a NAND gate a and B and then the
output is a B in case of an or gate it
is a plus B and so on and so forth and
they were able to do this using a single
layer perceptron now most of these kids
it was possible to use single layer
perceptron except for XOR and we will
see why that is in a little bit so this
is how a NAND gate works the inputs a
and B the output should be fired or the
neuron should be fired only when both
the inputs are one so if you have 0 0
the output should be 0 for 0 1 it is
again 0 1 0 again 0 and 1 1 the output
should be 1 so how do we implement this
with the nura so it was found that by
changing the values of weights it is
possible to achieve this logic so for
example if we have equal weights like
point seven point seven and then if we
take the sum of the weighted product so
for example point seven into 0 and then
point seven into 0 will give you 0 and
so on and so forth and in the last case
when both the inputs are 1 you get a
value which is greater than 1 which is
the threshold so only in this case the
neuron gets activated and the output is
there is an output in all the other
cases there is no put because the
threshold value is 1 so this is
implementation of a NAND gate using a
single perceptron or a single neuron
similarly a nor gate in order to
implement a nor gate in case of a nor
gate the output will be one if either of
these inputs is one so for example 0 1
will result in 1 or rather in all the
cases it is 1 except for 0 0 so how do
we implement this using a perceptron
once again if you have a perceptron with
weights for example 1 point 2 now if you
see here if in the first case when both
are 0 the output is 0 in the second case
when it is 0 and 1 1 point 2 into 0 is 0
and then 1 point 2 into 1 is 1 and in
the second case similarly the output is
1 point 2 in this last case and both the
inputs are 1 the output is 2 point 4 so
during the training process these
weights will keep changing and then at
one point where the weights are equal to
W 1 is equal to 1 point 2 and W 2 is
equal to 1 point 2 the system learns
that it gives the correct output so that
is implementation of or gate using a
single neuron or a single layer
perceptron now xor gate this was one of
the challenging ones they try to
implement an XOR gate with a single
level perceptron but it was not possible
and therefore in order to implement an
XOR so this was like a road block in the
progress of neural network however
subsequently they realize that this can
be implemented and XOR gate can be
implemented using a multi-level person
drawn or MLP so in this case there are
two layers instead of a single and this
is how you can implement an XOR gate so
you will see that X 1 and X 2 are the
inputs and there is a hidden layer and
that's why it is denoted as h3 and h4
and then you take the output of that and
feed it to on the output or Phi and
provide a threshold here so we will see
here that this is the numerical
calculation so the weights are in this
case for x1 it is 20 and minus 20 and
once again 20 and minus 20 so these
inputs are fed into h3 and h4 so you
will see here for h3 the input is 0 1 1
1 and for h4 it is 1 0 1 1 and if you
now look at the output final output
where the threshold is taken as 1 if we
use a sigmoid with the threshold 1 you
will see that in these 2 cases it is 0
and in the last two cases it is 1 so
this is a implementation of X or in case
of X are only when one of the inputs is
1 you will get an output so that is what
we are seeing here if we have either
both the inputs are 1 or both the inputs
are 0 then the output should be 0 that
is what is an exclusive or gate so it is
exclusive because only one of the inputs
should be 1 and then only you will get
an output of 1 which is satisfied by
this condition so this is a special
implementation XOR gate is a special
implementation of perceptron now that we
got a good idea about perceptron let's
take a look at what is the neural
network so we have seen what is a
perceptron we have seen what is a neuron
so we will see what exactly is a neural
network so neural network is nothing but
a network of these neurons and they are
different types of neural networks there
are about 5 of them
these are artificial neural network
convolutional neural network the
recursive neural network or recurrent
neural network deep neural network and
deep belief Network so and each of these
types of neural networks have a special
you know they can solve special
of problems for example convolutional
neural networks are very good at
performing image processing and image
recognition and so on whereas RNN are
very good for speech recognition and
also text analysis and so on so each
type has some special characteristics
and they can they're good at performing
certain special kind of tasks what are
some of the applications of deep
learning deep learning is today used
extensively in gaming you must have
heard about alphago which is a game
created by a start-up called deep mind
which got acquired by google and alphago
is an AI which defeated the human voice
champion least at all in this game of go
so gaming is an area where deep learning
is being extensively used and a lot of
research happens in the area of gaming
as well in addition to that nowadays
there are neural networks or special
type called generative adversarial
networks which can be used for
synthesizing either images or music or
text and so on and they can be used to
compose music so the neural network can
be trained to compose a certain kind of
music and autonomous cars they must be
familiar with Google Google's
self-driving car and today a lot of
automotive companies are investing in
this space and deep learning is a core
component of this autonomous cars the
cars are trained to recognize for
example the road the lane markings on
the road signals any objects that are in
front any obstruction and so on and so
forth so all this involves deep learning
so that's another major application and
robots we have seen several robots
including so fire you may be familiar
with Sophia who was given a citizenship
by Saudi Arabia and there are several
such robots which are very human-like
and the underlying technology in many of
these robots is deep learning medical
diagnostics
and healthcare is another major area of
a deep learning is being used and within
healthcare diagnostics again there are
multiple areas where deep learning an
image recognition image processing can
be used for example for cancer detection
as you may be aware if cancer is
detected early on it can be cured and
one of the challenges is in the
availability of specialists who can
diagnose cancer using these diagnostic
images and various scans and and so on
and so forth so the idea is to train
neural network to perform some of these
activities so that the load on the
cancer specialist doctors or on collages
comes down and there is a lot of
research happening here and there are
already quite a few applications that
are claimed to be performing better than
human beings in this space can be lung
cancer it couldn't be breast cancer and
so on and so forth so healthcare is a
major area where deep learning is being
applied let's take a look at the inner
working of a neural network so how does
an artificial neural network let's say
identify can we train a neural network
to identify the shapes like squares and
circles and triangles
when these images are fed so this is how
it works
any image is nothing but it is a digital
information of the pixels so in this
particular case let's say this is an
image of 28 by 28 pixel and this is an
image of a square there is a certain way
in which the pixels are lit up and so
these pixels have a certain value maybe
from 0 to 256 and 0 indicates that it is
black cords it is dark and 256 indicates
it is completely it is white or later so
that is like an indication or a measure
of the how the pixels are later and so
this is an image is let's say consisting
of information of 784 pixels so all the
information what is inside this image
can be kind of compared
to do this 784 pixels the way each of
these pixels is lit up provides
information about what exactly is the
image so we can train your networks to
use that information and identify the
images so let's take a look how this
works so each neuron the value if it is
close to one that means it is white
whereas if it is close to zero that
means it is black now this is a an
animation of how this whole thing works
so these pixels one of the ways of doing
it is we can flatten this image and take
this complete 784 pixels and feed that
as input to our neural network the
neural network can consist of probably
several layers there can be a few hidden
layers and then there is an input layer
and an output layer now the input layer
take these 784 pixels as input the
values of each of these pixels and then
you get an output which can be of three
types or three classes one can be a
square a circle or a triangle now during
the training process there will be
initially obviously you feed this image
and it will probably say it's a circle
or it will say it's a triangle so as a
part of the training process we then
send that error back and the weights and
the biases of these neurons are adjusted
till it correctly identifies that this
is a square that is the whole training
mechanism that happens out here
now let's take a look at a circle same
way so you feed these 784 pixels there
is a certain pattern in which the pixels
are lit up and the neural network is
trained to identify that pattern during
the training process once again it would
probably initially identify it
incorrectly saying this is a square or a
triangle and then that error is fed back
and the weights and biases are existed
finally till it finally gets the image
correct so that is the training process
so now we will take a look at same way a
tract
now if you feed another image which is
consisting of triangles so this is the
training process now we have trained our
neural network to classify these images
into a triangle or a circle and a square
so now this neural network can identify
these three types of objects now if you
feed another image and it will be able
to identify whether it's a square or a
triangle or a circle now what is
important to be observed is that when
you feed a new image it is not necessary
that the image or the the triangle is
exactly in this position not the neural
network actually identifies the patterns
so even if the triangle is let's say
position here not exactly in the middle
but maybe at the corner or in the side
it would still identify that it is a
triangle and that is the whole idea
behind pattern recognition so how does
this training process of work this is a
quick view of how the training process
works so we have seen that a neuron
consists of inputs it receives inputs
and then there is a weighted sum which
is nothing but this X I WI summation of
that plus the bias and this is then fed
to the activation function and that in
turn gives us a output now during the
training process initially obviously
when you feed these images when you send
maybe a square it will identify it as a
triangle and when you maybe feed a
triangle it will identify as a square
and so on so that error information is
fed back and initially these weights can
be random maybe all of them have zero
values and then it will slowly keep
changing so the as a part of the
training process the values of these
weights w1 w2 up to WN keep changing in
such a way that towards the end of the
training process it should be able to
identify these images correctly so till
then the weights are adjusted and that
is known as the training process so and
these weights are numeric values could
be 0.5 0.25 0.35 and so on it could be
positive or it could be negative
the value that is coming here is the
pixel value as we have seen it can be
anything between 0 to 1 you can scale it
between 0 to 1 or 0 to 256 whichever way
zero being black and 256 being white and
then all the other colors in between so
that is the input so these are numerical
values this multiplication or the
product W ixi is a numerical value and
the bias is also a numerical value we
need to keep in mind that the bias is
fixed for a neuron it doesn't change
with the inputs whereas the weights are
1 per input so that is one important
point to be noted so but the bias also
keeps changing initially it will again
have a random value but as a part of the
training process the weights the values
of the weights W 1 W 2 W n and the value
of B will change and ultimately once the
training process is complete these
values are fixed for this particular
neuron W 1 W 2 up to W n and plus the
value of the B is also fixed for this
particular neuron and in this way there
will be multiple neurons and each there
may be multiple levels of neurons here
and that's the way the training process
who works so this is another example of
multi-layer so there are two hidden
layers in between and then you have the
input layer values coming from the input
layer then it goes through multiple
layers hidden layers and then there is
an output layer and as you can see there
are weights and biases for each of these
neurons in each layer and all of them
gets keeps changing during the training
process and at the end of the training
process all these weights have a certain
value and that is a trained model and
those values will be fixed once the
training is completed all right then
there is something known as activation
function neural networks consists of one
of the components in neural networks is
activation function and every neuron has
an activation function and there are
different types of activation functions
that are used it could be a Ray Lu it
could be sigmoid and so on and so forth
and the activation function is what
decides whether a neuron should be fired
or not so whether the output should be
zero or
is decided by the activation function
and the activation function in turn
takes the input which is the weighted
sum remember we talked about WI X I plus
B that weighted sum is fed as a input to
that duration function and then the
output can be either a 0 or a 1 and
there are different types of activation
functions which are covered in an
earlier video you might want to watch
alright so as a part of the training
process we feed the inputs the label
data or the training data and then it
gives an output which is the predicted
output by the network which we indicate
as Y hat and then there is a labeled
data because we for supervised learning
we already know what should be the
output so that is the actual output and
in the initial process before the
training is complete obviously there
will be error so that is measured by
what is known as a cost function so the
difference between the predicted output
and the actual output is the error and
the cost function can be defined in
different ways there are different types
of cost functions so in this case it is
like the average of the squares of the
error so and then all the errors are
added which can sometimes be called sum
of squares sum of square errors or SSC
and that is then fed as a feedback in
what is known as backward propagation or
back propagation and that helps and the
network adjusting the weights and biases
and so the weights and biases get
updated till this value the error value
or the cost function is minimum now
there is a optimization technique which
is used here called gradient descent
optimization and this algorithm works in
a way that the error which is the cost
function needs to be minimized so
there's a lot of mathematics that goes
behind us for example they find the
local minima on the global minima using
differentiation and so on and so forth
but the idea is this so as a training
process as that is the part of training
the whole idea is to bring down
the error which is like let's say this
is the function the cost function at
certain levels it is very high the cost
value of the cost function or the output
of the cost function is very high so the
weights have to be adjusted in such a
way and also the bias of course that the
cost function is minimized so there is
this optimization technique called
gradient descent that is used and this
is known as the learning rate now
gradient descent you need to specify
what should be the learning rate and the
learning rate should be optimal because
if you have a very high learning rate
then the optimization will not converge
because at some point it will cross over
to the side on the other hand if you
have very low learning rate then it
might take forever to convert so you
need to come up with the optimum value
of the learning rate and once that is
done using the gradient descent
optimization the error function is
reduced and that's like the end of the
training process all right so this is
another view of gradient descent so this
is how it looks this is your cost
function the output of the cost function
and that has to be minimized using
gradient descent algorithm and these are
like the parameters and weight could be
one of them so initially we start with
certain random values so cost will be
high and then the weights keep changing
and in such a way that the cost function
needs to come down and at some point it
may reach the minimum value and then it
may increase so that is where the
gradient descent algorithm decides that
ok it has reached a minimum value and it
will kind of try to stay here this is
known as the global minima now sometimes
these curves may not be just for
explanation purpose this has been drawn
in a nice way but sometimes these curves
can be pretty erratic there can be some
local minima here and then there is a
peak and then and so on so the whole
idea of gradient descent optimization is
to identify the global minima and to
find the weights and
at that particular point so that's what
is gradient descent and then this is
another example so you can have these
multiple local minima as you can see at
this point when it is coming down it may
appear like this is a minimum value but
then it is not this is actually the
global minimum value and the gradient
descent algorithm will make an effort to
reach this level and not get stuck at
this point so the algorithm is already
there and it knows how to identify this
global minimum and that's what it does
during the training process now in order
to implement deep learning there are
multiple platforms and languages that
are available but the most common
platform nowadays is tensorflow
and so that's the reason we have this
tutorial we created this tutorial for
tensorflow so we will take you through a
quick demo of how to write a tensor flow
code using Python and tensor flow is an
open source platform created by Google
so let's just take a look at the details
of pencil flow and so this is a library
Python library so you can use Python or
any other languages it's also supported
in other languages like Java R and so on
but Python is the most common language
that is used so it is a library for
developing deep learning applications
especially using neural networks and it
consists of primarily two parts if you
will so one is the tensors and then the
other is the graphs or the flow that's
the way the name that's the reason for
this kind of a name contents of flow so
what are tensor stencils are like
multi-dimensional arrays if you will
that's one way of looking at it so
usually you have a one-dimensional array
so first of all you can have what is
known as a scalar which means a number
and then you have a one dimensional
array something like this which means
this like a set of numbers so that is a
one dimension array then you can have a
two dimensional array which is like a
matrix and beyond that sometimes it gets
difficult so this is a three dimensional
array but tensorflow can handle many
more dimensions
so it can have multi-dimensional arrays
that is the strength of tensor flow and
which makes computation deep learning
computation much faster and that's the
reason why terms of flow is used for
developing deep learning applications so
tensor flow is the deep learning tool
and this is the way it works so the data
basically flows in the form of tensors
and the way the programming works as
well is that you first create a graph of
how to execute it and then you actually
execute that particular graph in the
form of what is known as a session we
will see this in the tensor flow code as
we move forward so all the data is
managed or manipulated in tensors and
then the processing happens using these
graphs there are certain terms called
like for example ranks of a tensor the
rank of a tensor is like a dimensional
dimensionality in a way so for example
if it is scalar so there is just a
number just one number the rank is
supposed to be 0 and then it can be a
one dimensional vector in which case the
rank is supposed to be 1 and then you
can have a two dimensional vector
typically like a matrix then in that
case we say the rank is 2 and then if it
is a three dimensional array then rank
is three and so on so it can have more
than three as well so it is possible
that you can store multi-dimensional
arrays in the form of tensors so what
are some of the properties of tensor
flow I think today it is one of the most
popular platform tensor flow is the most
popular deep learning platform or
library it is open source it's developed
by Google developed and maintained by
Google but it is open source one of the
most important things about tensor flow
is that it can run on CPUs as well as
GPUs GPU is graphical processing unit
just like CPU is sent to processing unit
now in earlier days GPU was used for
primarily for graphics and that's how
the name has come and one of the reasons
is that it cannot
generic activity is very efficiently
like CPU but it can perform iterative
actions or computations extremely fast
and much faster than a CPU so they are
really good for computational activities
and in deep learning there is a lot of
iterative computation that happens so in
the form of matrix multiplication and so
on so GPUs are very well suited for this
kind of computation and tensorflow
supports both GPU as well as CP and
there's a certain way of writing code
intensive flow we will see as we go into
the core and of course answer flow can
be used for traditional machine learning
as well but then that would be an
overkill but just for understanding it
may be a good idea to start writing code
for a normal machine learning use case
so that you get a hang of how tensorflow
code works and then you can move into
neural networks so that is just a
suggestion but if you are already
familiar with how terms of flow works
then probably here you can go straight
into the neural networks part so in this
tutorial we will take the use case of
recognizing handwritten digits this is
like a hollow world of deep learning and
this is a nice little Emnes database is
a nice little database that has images
of handwritten digits nicely formatted
because very often in deep learning and
neural networks we end up spending a lot
of time in preparing the data for
training and with M Ness database we can
avoid that you already have the data in
the right format which can be directly
used for training and amnesty also
offers a bunch of built-in utility
functions that we can straightaway use
and call those functions without
worrying about writing our own functions
and that's one of the reasons why M nest
database is very popular for training
purposes initially when people want to
learn about deep learning intensive flow
this is the database that is used and it
has a collection of 70,000
written digits and a large part of them
are for training then you have tests
just like in any machine learning
process and then you have validation and
all of them are labeled so you have the
images and their label and these images
they look somewhat like this so they are
handwritten images collected from a lot
of individuals people have these are
samples written by human beings they
have handwritten these numbers these
numbers going from 0 to 9 so people have
written these numbers and then the
images of those have been taken and
formatted in such a way that it is very
easy to handle so that is M Ness
database and the way we are going to
implement this in our tensorflow
is we will feed this data especially the
training data along with the label
information and the data is basically
these images are stored in the form of
the pixel information as we have seen in
one of the previous slides all the
images are nothing but these are big
cells so an image is nothing but an
arrangement of pixels and the value of
the pixel later it is lit up or it is
not or in somewhere in between that's
how the images are stored and that is
all they are fed into the neural network
and for training once the network is
trained when you provide a new image it
will be able to identify within a
certain error of course and for this we
will use one of the simpler neural
network configurations called softmax
and for simplicity what we will do is we
will flatten these pixels so instead of
taking them in a two dimensional
arrangement we just flatten them out so
for example it starts from here it is a
28 by 28 so there are 704 84 pixels so
pixel number one starts here it goes all
the way up to 28 then 29 starts here and
goes up to 56 and so on and the pixel
number 784 is here so we take all these
pixels flatten them out and feed them
like one single
into our new network and this is a what
is known as a soft max layer what it
does is once it is cleaned it will be
able to identify what digit this is so
there are in this output layer there are
ten neurons each signifying a digit and
at any given point of time when you feed
an image only one of these ten neurons
gets activated so for example if this is
trained properly and if you feed a
number nine like this then this
particular neuron gets activated so you
get an output from this neuron let me
just use a pen or a laser to show you
here okay so you're feeding a number
nine let's say this has been trained and
now if you're feeding a number nine this
will get activated now let's say you
fade one to the trained network then
this neuron will get activated if you
feed to this neuron will get activated
and so on I hope you get the idea so
this is one type of a neural network or
an activation function known as soft max
layer so that's what we will be using
here this is one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and we
will show you there directly but very
quickly this is how the code looks and
let me run you through briefly here and
then we will go into the jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using Python here
and that's why the syntax of the
languages Python and the first step is
to import the tensorflow library so and
we do this by using this line of code
saying importance of flow as DF DF is
just for convenience so you can name
give any name and once you do this TF is
tensorflow is available as an object
in the name of PF and then you can run
its methods and accesses its attributes
and so on and so forth and ms database
is actually an integral part of
tensorflow and that's again another
reason why we as a first step we always
use this example amidst database example
so you just simply import em nest
database as well using this line of code
and you slightly modify this so that the
labels are in this form at what is known
as one hot true which means that the
label information is stored like an
array and let me just use pen to show
what exactly it is so when you do this
one hot true what happens is each label
is stored in the form of an array of ten
digits and let's say the number is eight
okay so in this case all the remaining
values there will be a bunch of zeros so
this is like array at position 0 this is
at position 1 position 2 and so on and
so forth let's say this is position 7
then this is position 8 that will be one
because our input is 8 and again
position 9 will be 0 okay so one hot
encoding this one hot encoding true will
kind of load the data in such a way that
the labels are in such a way that only
one of the digits has a value of 1 and
that indicates so based on which digit
is 1 we know what is the label so in
this case the 8th position is 1
therefore we know this sample data the
value is 8 similarly if you have a 2
here let's say then the label
information will be somewhat like this
so you have your labels so you have this
as 0 the 0th position the first position
is also 0 the second position is 1
because this indicates number 2 and then
you have third as 0 and so on ok so that
is the significance of this one hot true
alright and then we can
check how the data is looking by
displaying the data and as I mentioned
earlier this is pretty much in the form
of digital form like numbers
so all these are like pixel values so
you will not really see an image in this
format but there is a way to visualize
that image have it show you in a bit and
this tells you how many images are there
in each sets of the training there are
55,000 images in training and in the
test set there are 10,000 and then
validation there are 5000 so altogether
there are 70,000 images all right so
let's move on and we can view the actual
image by using the mat plot clip library
and this is how you can view this is the
code for viewing the images and you can
view them in color or you can view them
in grayscale so the C map is what tells
in what way we want to view it and what
are the maximum values and the minimum
values of the pixel values so these are
the max and minimum values so of the
pixel values so maximum is one because
this is a scaled value so one means it
is white and zero means it is black and
in-between is it can be anywhere in
between black and white and the way to
train the model there is a certain way
in which you write your tensorflow code
and the first step is to create some
placeholders and then you create a model
in this case we will use the softmax
model one of the simplest ones and
placeholders are primarily to get the
data from outside into the neural
network so this is a very common
mechanism that is used and then of
course you will have variables which are
your you remember these are your weights
and biases so for in our case there are
10 neurons and each neuron actually has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs
this is the first neuron it has it
receives all the 784 this is the second
year on this also receives all the
Serenity so each of these inputs needs
to be multiplied with the weight and
that's what we are talking about here so
these are this is a matrix of 784 values
for each of the neurons and so it is
like a 10 by 784 matrix because there
are ten neurons and similarly there are
biases now remember I mentioned biases
only one putner odd so it is not one per
input unlike the weights so therefore
there are only ten biases because there
are only ten neurons in this case so
that is what we are creating a variable
for biases so this is something little
new intensive flow you will see unlike
our regular programming languages where
everything is a variable here the
variables can be of three different
types you have placeholders which are
primarily used for feeding data you have
variables which can change during the
course of computation and then a third
type which is not shown here are
constants so these are like fixed
numbers all right so in a regular
programming language you may have
everything as variables or at the most
variables and constants but in terms of
flow you have three different types
placeholders variables and constants and
then you create what is known as a graph
so tensorflow programming consists of
graphs and tensors as I mentioned
earlier so this can be considered
ultimately as a tensor and then the
graph tells how to execute the whole
implementation so that the execution is
stored in the form of a graph and in
this case what we are doing is we are
doing a multiplication TF you remember
the staff was created as a tensor flow
object here one more level one more so
TF is available here now tensorflow has
what is known as a matrix multiplication
or mat mul function so that is what is
being used here in this case so we are
using the matrix multiplication of
tensorflow
so that you multiply your input values X
with W right this is what we were doing
X W plus B you're just adding B and this
is in very similar to one of the earlier
slides where we saw Sigma X I WI so
that's what we are doing here matrix
multiplication is multiplying all the
input values with the corresponding
weights and then adding the bias so that
is the graph we created and then we need
to define what is our loss function and
what is our optimizer
so in this case we again use the tens of
flows api's so TF dot n n softmax
cross-entropy with two logits is the api
that we will use and reduce mean is what
is like the mechanism whereby which says
that you reduce the error and optimizer
for doing reduction of the error what
optimizer are we using so we are using
gradient descent optimizer we discussed
about this in a couple of slides earlier
and for that you need to specify the
learning rate you remember we saw that
there was a slide somewhat like this and
then you define what should be the
learning rate how fast you need to come
down that is the learning rate and this
again needs to be tested and tried and
to find out the optimum level of this
learning rate it shouldn't be very high
in which case it will not converge or
shouldn't be very low because it will in
that case it will take too very long so
you define the optimizer and then you
call the method minimize for that
optimizer and that will kick start the
training process and so far we've been
creating the graph and in order to
actually execute that graph we create
what is known as a session and then we
run that session and once the training
is completed we specify how many times
how many iterations we want it to run so
for example in this case we are saying
thousand steps so that is the exit
strategy in a way so you specify the
exit condition so training will run 4000
iterations and once that is done we can
then evaluate the model using some of
the techniques shown here so let
get into the court quickly and see how
it works
so this is our cloud environment now you
can install tensorflow on your local
machine as well I'm showing this demo on
our existing cloud but you can also
install tensorflow on your local machine
and there is a separate video on how to
setup your tensorflow environment you
can watch that if you want to install
your local environment or you can go for
other any cloud service like for example
Google cloud Amazon or cloud labs any of
these you can use and run
try the code ok so it is got started
we login
all right so this is our deep learning
tutorial code and this is our tensor
flow environment and so let's get
started the first we have seen a little
bit of a code walkthrough in the slides
as well now you will see the actual code
in action so the first thing we need to
do is import tensor flow and then we
will import the data and we need to
adjust the data in such a way that the
one-hot is encoding is set to true one
hot encoding right as I explained
earlier so in this case the label values
will be shown appropriately and if we
just check what is the type of the data
so you can see that this is a datasets
Python datasets and if we check the
number of images the way it looks this
is how it looks it is an array of type
float 32 similarly the number if you
want to see what is the number of
training images there are 55,000 and
there are test images 10,000 and then
validation images 5,000 now let's take a
quick look at the data itself
visualization so we will use matplotlib
for this and if we take a look at the
shape now shape gives us like the
dimension of the tensors all are in
order
erase if you will so in this case the
training data set if we sees the size of
the training data set using the method
shape it says there are 55,000 and
55,000
784 so remember the 784 is nothing but
the 28 by 28 28 into 28 so that is equal
to 784 so that's what it is showing now
we can take just one image and just see
what is the the first image and see what
is the shape so again size obviously it
is only 784 similarly you can look at
the image itself the data of the first
image itself so this is average it sure
so large part of it will probably be
zeroes because as you can imagine in the
image only certain areas are written
rest is black so that's why we will
mostly see zero say that it is black or
white but then there are these values
are so the values are actually they are
scaled so their values are between 0 &amp;amp; 1
ok so this is what you're seeing so
certain locations there are some values
and then other locations there are zeros
so that is how the data is stored and
loaded if we want to actually see what
is the value of the handwritten image if
you want to view it this is how you view
it so you create like do this reshape
and matplotlib has this feature to show
you these images so we will actually use
the function called I am show and then
if you pass this parameters
appropriately you will be able to see
the different images now I can change
the values in this position so which
image we are looking at right so we can
say
if I want to see what is there in maybe
five thousand right so five thousand has
three similarly you can just say five
what is in five five has eight what is
in fifty again eight so basically by the
way if you're wondering how I am
executing this code shift enter in case
you are not familiar with Jupiter
notebooks shift-enter is how you execute
each cell individual cell if you want to
execute the entire program you can go
here and say run all so that is how this
score gets executed and here again we
can check what is the maximum value and
what is the minimum value of this pixel
values as I mentioned this is it is
scaled so therefore it is between the
values like between 1 and 0 now this is
where we create our model the first
thing is to create the required
placeholders and variables and that's
what we are doing here as we have seen
in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by 10
cell values okay so one for the stennis
for each neuron there are ten neurons
and 784 is for the pixel values inputs
that are given which is 28 into 28 and
the biases as I mentioned one for each
neuron so there will be ten biases they
are stored in a variable by the name B
and this is the graph which is basically
the multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute I think
this code is executed then we define
what is our the Y value is basically the
label value so this is another
placeholder we had X as one placeholder
and white underscore true as a second
place holder and this will have values
in the form of our 10 digit 10 digit
erase and since we said one hot encoded
the position which has a 1 value
indicates what is the label for that
particular number all right then we have
cross-entropy which is nothing but the
loss loss function and we have the
optimizer we have chosen gradient
descent as our optimizer then the
training process itself so the training
process is nothing but to minimize the
cross entropy which is again nothing but
the loss function so we define all of
this in the form of a graph so the up to
here remember what we have done is we
have not exactly executed any tensorflow
code till now we are just preparing the
graph the execution plan that's how the
tensorflow
code works so the whole structure and
format of this code will be completely
different from how we normally do
programming so even with people with
programming experience may find this a
little difficult to understand it and it
needs quite a bit of practice so you may
want to view this video also maybe a
couple of times to understand this flow
because the way tensorflow programming
is done is slightly different from the
normal programming some of you who let's
say I have done maybe spark programming
to some extent will be able to easily
understand this but even in spark the
programming the code itself is pretty
straightforward behind the scenes the
execution happens slightly differently
but in tensorflow
even the code has to be written in a
completely different way so the code
doesn't get executed in the same way as
you have written so that that's
something you need to understand and a
little bit of practice is needed for
this so so far what we have done up to
here is creating the variables and
feeding the variables and rather not
feeding but setting up the variables and
the graph that's all defining maybe the
what kind of a network you want to use
for example we want to use a soft Max
and so on so you have created the
variables of to load the data loaded the
data viewed the data and prepared
everything but you have not yet executed
anything intensive flow now the next
step is the execution intensive law so
the first step for doing any execution
intensive flow is to initialize the
variables so anytime you have any
variables defined in your code you have
to run this piece of code always so you
need to basically create what is known
as a a node for initializing so this is
a node you still have not yet executing
anything here you just created a node
for the initialization so let us go
ahead and create that and here onwards
is where you will actually execute your
code intensive flow and in order to
execute the code what you will need is a
session tensorflow
session so TF dot session will give you
a session and there are a couple of
different ways in which you can do this
but one of the most common methods of
doing this is with what is known as a
wit loop so you have a width T F dot
session as says and with a colon here
and this is like a block starting of the
block and these indentations tell how
far this block goes and this session is
valid till this block gets executed so
that is the purpose of creating this
width block this is known as a wit block
so with TF dot session has says you say
says dot run in it now says dot run will
execute a node that is specified here so
for example here we are saying says dot
run says is basically an instance of the
session right so here we are saying T
dot session so an instance of the
session gets created and we are calling
that cess and then we run a node within
that one of the nodes in the graph so
one of the nodes here is in it so we say
run that particular node and that is
when the initialization of the variables
happens now what this does is if you
have any variables in your code in our
case we have W is a variable and B is a
variable so any variables that we
created you have to run this cord you
have to run the initialization of these
variables otherwise you will get an
error okay so that is the that's what
this is doing then we within this width
block we specify a for loop and we are
saying we want the system to iterate
four thousand steps and perform the
training that's what this for loop does
run training four thousand iterations
and what it is doing basically is it is
fetching the data or these images
remember there are about 50,000 images
but it cannot get all the images in one
shot because it will take up a lot of
memory and performance issues will be
there so this is a very common way of
performing deep learning training you
always do in batches so we have maybe
50,000 images but you always do it in
batches of 100 or maybe 500 depending on
the size of your system and so on and so
forth so in this case we are saying okay
get me 100 images at a time and get me
only the training images remember we use
only the training data for training
purpose and then we use test data for
test purpose you must be familiar with
machine learning so you must be aware of
this but in case you are not in machine
learning also not this is not specific
to deep learning but in machine learning
in general you have what is known as
training data set and test data set your
available data typically you will be
splitting into two parts and using the
training data set for training purposes
and then to see how well the model has
been trained you use the test data set
to check or test the validity or the
accuracy of the model so that's what we
are doing here and you observe here that
we are actually calling an amnesty
inhere so we are saying M NIST train dot
next patch right so this is the
advantage of using M Ness database
because they have provided some very
nice helper functions which are readily
available otherwise this activity itself
we would have had to write a piece of
code to fetch this data in batches that
itself is a lengthy exercise so we can
avoid all that if we are using M Ness
database and that's why we use this for
the initial learning phase okay so when
we say fetch what it will do is it will
fetch the images into X and the labels
into Y and then you use this batch of
hundred images and you run the cleaning
so says dot run basically what we are
doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously there
initially it will be wrong so all that
feedback is given back to the neural
network and thereby all the W's and B's
get updated till it reaches thousand
iterations in this case the exit
criteria is thousand but you can also
specify probably accuracy rate or
something like that further as an exit
criteria so here it is it just says that
ok this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neuron and that is run
4000 iterations and typically by the end
of these thousand iterations the model
would have learned to recognize these
handwritten images obviously it will not
be 100% accurate ok so once that is done
after so this happens 4000 iterations
once that is done you then test the
accuracy of this
models by using the test data set right
so this is what we are trying to do here
the code may appear a little complicated
because if you are seeing this for the
first time you need to understand the
various methods of tensorflow and so on
but it is basically comparing the output
with what has been what is actually
there that's all it is doing so you have
your test data and you are trying to
find out what is the actual value and
what is the predicted value and seeing
whether they are equal or not
TF dot equal and how many of them are
correct and so on and so forth and based
on that the accuracy is calculated as
well so this is the accuracy and that is
what we are trying to see how accurate
the model is in predicting these numbers
or these digits okay so let us run this
this entire thing is in one cell so we
will have to just run it in one shot it
may take a little while let us see and
not bad so it has finished the thousand
iterations and what we see here as an
output is the accuracy so we see that
the accuracy of this model is around 91%
okay now which is pretty good for such a
short exercise with in such a short time
we got 90% accuracy however in real life
this is probably not sufficient so there
are other ways in to increase the
accuracy we will see probably in some of
the later tutorials how to improve this
accuracy how to change maybe the hyper
parameters like number of neurons or
number of layers and so on and so forth
and so that this accuracy can be
increased beyond 90% so with that I
think we have come to the end of our
session here and thank you very much for
going through this video I hope this was
helpful thank you
hi there if you like this video
subscribe to the simple learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>