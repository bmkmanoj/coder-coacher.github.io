<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What is Big Data | What Is Hadoop and Big Data | Big Data Tutorial For Beginners | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="What is Big Data | What Is Hadoop and Big Data | Big Data Tutorial For Beginners | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What is Big Data | What Is Hadoop and Big Data | Big Data Tutorial For Beginners | Simplilearn</b></h2><h5 class="post__date">2015-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CKLzDWMsQGM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">simply learn your pace your place
hello and welcome to the introductory
lesson of the Big Data and Hadoop
developer course offered by simply learn
in this lesson you will be introduced to
the Big Data and Hadoop developer course
let us explore the objectives of this
course in the next screen by the end of
this course you will be able to describe
the fundamental concepts of Hadoop apply
programming skills in MapReduce utilize
big data analytic skills using pig and
hive explain the HBase data model and
its components describe how Sookie per
and scoop address the challenges
associated with distributed computing in
the next screen we will focus on the
course overview
this training course provides a detailed
introduction to the basics of Apache
Hadoop it will also enhance your
knowledge of the Hadoop ecosystem Hadoop
distributed file system Hadoop cluster
MapReduce Pig hive HBase zookeeper scoop
flume and so on in addition this course
will provide an overview of the
installation and configuration steps for
various Hadoop components you will learn
how to install and configure the
environment in the Ubuntu server
operating system finally you will learn
how to perform programming and analytics
using Java and other Hadoop ecosystem
components we will continue with the
course overview in the next screen
the course is aimed at professionals who
aspire to perform big data analytics
using Apache Hadoop analytics
professionals IT professionals ETL
developers project managers and testing
novices and experts benefit from doing
the course other aspirants and to
students who want to gain a thorough
understanding of the implementation of
Hadoop framework can also benefit from
this course now let us discuss the
prerequisites of this course knowledge
of any programming language is a basic
course prerequisite participants are
expected to have fundamental programming
skills to understand and implement
MapReduce operations using Java a
working knowledge of Java will be an
added advantage for this course though
it is not mandatory in the next screen
we will discuss how this course can add
value to participants
Hadoop professionals will be equipped
with Hadoop framework skills in the
fast-growing big data analytics industry
they will be able to process a huge
amount of data with a high level of
efficiency as the industry switches over
from Excel based analytics to real-time
analytics by equipping you with thorough
knowledge and the required skill set
this course helps you differentiate
yourself from other Big Data
professionals the demand for Hadoop
professionals will be high over the next
decade in all leading organizations
worldwide they will be able to derive
benefits from the emerging big data
industry once they are equipped with the
required knowledge and skills in the
next screen we will focus on the list of
lessons covered in this course there are
13 lessons covered in this course the
table lists the lesson numbers with
their corresponding names
hello and welcome to lesson one of the
big data and hadoop developer course
offered by simply learned this lesson
will provide an introduction to big data
further it will introduce Hadoop as a
Big Data technology let us explore the
objectives of this lesson in the next
screen
by the end of this lesson you will be
able to identify the need for big data
explain the concept of big data describe
the basics of Hadoop and explain its
benefits in the next screen we will
focus on the huge data explosion we
usually experience every day
over the last decade there has been an
unimaginable explosion of data in every
sector by an estimate over two point
five exabytes of data is generated every
day equivalent to 2.5 billion gigabytes
this huge volume of data originates from
various sources following are some of
these sources a typical large stock
exchange such as the New York Stock
Exchange captures more than one terabyte
of data every day there are around five
billion mobile phones in the world of
which 1.75 billion are estimated to be
smartphones YouTube users upload more
than 48 hours of video every minute
every second of HD video generates more
than 2,000 times as many bytes as
required to store a single page of text
large social networks like Twitter and
Facebook capture more than 10 terabytes
of data daily there are more than 30
million networked sensors in the world
in each of them sends in data
continuously in the next screen we will
discuss the types of data data can be
identified as one of three primary types
the first is structured data which can
be represented in a tabular format and
stored in relational databases like
MySQL Oracle database db2 and so on
the second is semi structured data which
does not have a formal data model though
it may have some semblance of a
structure for example XML files
represent a semi structured data the
third is unstructured data which does
not have a predefined data model text
files web logs and machine logs are
examples of unstructured data in the
next screen we will discuss the need for
Big Data
by an estimate around 90% of the world's
data has been created in the last two
years alone more over 80% of the data is
unstructured or available in widely
varying structures which are difficult
to analyze through the years of
developing IT systems we have observed
that structured formats like databases
have some limitations with respect to
handling large quantities of data it
also has been observed that it is
difficult to integrate information
distributed across multiple systems
further most business users do not know
what should be analyzed and discover
requirements only during the development
of IT systems as data has grown so if
data lakes within enterprises
potentially valuable data for varied
systems like ERP or enterprise resource
planning and SCM or supply chain
management is either dormant or
discarded it is often too expensive to
justify the integration of large volumes
of unstructured data information such as
Natural Resources has a short useful
life span and is best used in a limited
time span further information is best
exploited for business value of context
is added to it in the next screen we
will focus on data as the most valuable
resource data is fast becoming like oil
which used to be considered the most
valuable natural resource in terms of
monetary value in its raw form oil has
little value once processed and refined
it helps power the world the same holds
true for data or information in its raw
form data has little value once
processed and refined it helps power the
world that is why Clive Humby commented
on CNBC that data is the new oil in the
next screen we will define big data and
discuss its sources
big data is an all-encompassing term for
any collection of data sets so large and
complex that it becomes difficult to
process them using on hand data
management tools or traditional data
processing applications some of the
sources of Big Data are web logs sensor
networks social media internet text and
documents internet pages search index
data atmospheric science astronomy
biochemical and medical records
scientific research military
surveillance and photography archives in
the next screen we will focus on the
three characteristics of big data in
this screen we will discuss the
characteristics of big data technology
big data technology helps to respond to
the characteristics discussed in a
previous screen it helps to cost
efficiently process the growing volumes
of data for example as per IBM big data
technology has helped to turn the 12
terabytes of tweets created daily into
improved product sentiment analysis it
has converted 350 billion annual meter
readings to better predict power
consumption big data technology also
helps to respond to the increasing
velocity of data for example it is
scrutinized 5-million trade events
created daily to identify potential
frauds it has helped to analyze 500
million daily call detail records in
real time to predict customer churn
faster big data technology can
collectively analyze the widening
variety of data for example it is helped
to monitor hundreds of live video feeds
from surveillance cameras to target
points of interest for security agencies
it has also been able to exploit the 80%
data growth in images videos and
documents to improve customer
satisfaction according to Gartner big
data is high volume high velocity and
high variety information assets that
demand cost-effective innovative forms
of information processing for enhanced
insight and decision-making in the next
screen we will focus on the appeal of
big data technology
following are the reasons why Big Data
technology is popular and has been
attracting attention Big Data technology
helps to manage and process huge amounts
of data in a cost-efficient manner it
analyzes all available data in their
native forms which can be unstructured
structured or streaming it captures data
from fast happening events in real time
Big Data technology is able to handle
failure of isolated nodes and tasks
assigned to such nodes it can turn data
into actionable insights in the next
screen we will categorize the sources of
data
in this screen we will discuss the
traditional approach to develop IT
systems the following are the
requirements of the traditional IT
analytics approach and factors they are
challenged by the traditional IT
analytics approach requires the business
team to define questions before IT
development further the team needs to
define data sources and structures
beforehand the business team is usually
challenged if they have iterative and
volatile requirements or if data sources
keep changing the image illustrates the
traditional IT analytics approach in a
typical scenario of traditional IT
systems development the requirements are
defined followed by solution design and
build once the solution is implemented
queries are executed if there are new
requirements or queries the system is
redesigned and rebuilt in the next
screen we will discuss the approach
taken to build big data systems this
approach requires the business team to
define data sources and establish the
hypothesis here Big Data technology
should enable explorative analysis the
IT team should integrate data systems
and sources are required based on new
business questions and the hypothesis
the image illustrates how IT systems are
built with the help of Big Data
technology at the onset the initial data
sources are identified the IT team
creates a platform for creative
exploration of available data and
content the business team determines the
questions to ask and tests their
hypothesis any new question leads to the
addition of data sources and integration
without the need to redesign and rebuild
the platform in the next screen we will
discuss the capabilities of Big Data
technology
big data technology helps you to
understand and navigate big data sources
manage and store a huge volume of a
variety of data process data in
reasonable time ingest data at a high
speed analyze unstructured data and bear
faults and exceptions in the next screen
we will discuss the various use cases of
big data big data technology appeals to
various sectors and is used for a
variety of use cases as per cloud era
the technology has found uses in
industries such as automotive
communications consumer packaged goods
financial services education and
research high technology and industrial
manufacturing life sciences media and
entertainment online services and social
media healthcare oil and gas retail
travel and transportation utilities law
enforcement and defense in the next
screen we will discuss how to handle the
limitations of Big Data there are two
key challenges that need to be addressed
by Big Data technology these are
handling the system uptime and downtime
and combining data accumulated from all
systems to overcome the first challenge
Big Data technology uses commodity
Hardware for data storage and analysis
further it helps to maintain a copy of
the same data across clusters to
overcome the second challenge Big Data
technology analyzes data across
different machines and subsequently
merges the data in the next screen we
will introduce the concept of Hadoop
which helps to overcome these challenges
Hadoop helps to leverage the
opportunities provided by big data and
overcome the challenges it poses Hadoop
is an open source java based programming
framework that supports the processing
of large datasets in a distributed
computing environment Hadoop runs a
number of applications on distributed
systems with thousands of nodes
involving petabytes of data it has a
distributed file system called Hadoop
distributed file system or HDFS which
enables fast data transfer among the
nodes further it leverages a distributed
computation framework called MapReduce
in the next screen we will discuss the
origin and history of Hadoop
Hadoop originated from the nudge
open-source project on search engines
and works over distributed network nodes
in 2003 in 2004 Google released two
papers that provided insight into their
success the Google file system or GFS
and a MapReduce
simplify data processing on large
clusters the papers told the world how
Google performed a large-scale data
processing in July 2005 knotch
used GFS to perform MapReduce operations
in February 2006 nuch started a leucine
sub project which led to the era of
Hadoop in April 2007 Yahoo started using
Hadoop on a 1,000 node cluster in
January 2008 Apache took over Hadoop and
made it a top-level project in July 2008
a 4,000 node cluster with Hadoop was
tested by Apache the performance of that
cluster was surprisingly the fastest
when compared with other technologies
implemented that year in May 2009 a test
revealed that Hadoop successfully sorted
a petabyte of data in 17 hours Hadoop
reached version 1.0 in December 2011 it
is completely open source and written in
Java in the next screen we will discuss
how organizations are using Hadoop Yahoo
was the first company to design and use
Hadoop as a core part of their system
operations now Hadoop is a core part of
systems at internet companies like
Facebook LinkedIn Twitter and many
enterprise organizations such as JP
Morgan Chevron and so on the table
displays the names of organizations the
cluster specifications and the way they
have made use of Hadoop
let us summarize the topics covered in
this lesson big data is a term applied
to datasets that cannot be captured
managed and processed within a tolerable
elapsed and specify time frame by
commonly used software tools big data
relies on volume velocity and variety
with respect to processing data can be
divided into three types unstructured
data semi structured data and structured
data big data technology understands and
navigates big data sources analyzes
unstructured data and ingests data at a
high speed Hadoop is a free Java based
programming framework that supports the
processing of large datasets in a
distributed computing environment
hello and welcome to lesson 2 of the big
data and hadoop developer course offered
by simply learn this lesson will help
you get started with hadoop let us
explore the objectives of this lesson in
the next screen by the end of this
lesson you will be able to list the
steps to install VMware Player list the
steps to create a VM in VMware Player
explain how to use Oracle VirtualBox to
open a VM explain how to start an
existing Hadoop VM in the next screen we
will discuss vmware player
vmware player is a free software package
offered by vmware incorporated it is
used to create and manage virtual
machines during a hands-on session on
hadoop and its ecosystem components you
will be applying all theories in the vm
created within the vmware player the
vmware player can be downloaded for
personal use from the URL shown on
screen in the next screen we will
discuss the hardware requirements for
installing VMware Player
the basic hardware requirements for
working on vmware player are as follows
at least one gigahertz of processor
which can support intel VT or
virtualization technology one gigabyte
ram and 150 megabytes of hard disk space
to install the application however it is
recommended to have 2 gigahertz of
processor and 2 gigabytes ram for
optimum performance in the next screen
we will learn to install VMware Player
perform the steps shown on screen to
install VMware Player initialize the
installation process in the installation
wizard click Next to continue select the
destination folder and click Next
upgrade the product by selecting check
for product updates on startup click
Next continue and finish for the
subsequent screens double-click on the
desktop icon for VMware Player accept
the license agreement and click Next in
the subsequent screens we will see how
to perform the steps for the
installation of VMware Player in the
next screen we will initialize the
installation process
double-click the VMware Player
executable file for initializing the
installation process the window shown on
screen will be displayed click save file
to continue in the next screen we will
continue to discuss the steps for
installing VMware Player the
installation wizard gets activated and
displays the welcome screen for
installation of VMware Player click Next
to continue the installation process in
the next screen we will continue to
discuss the steps for installing VMware
Player
the destination folder window appears
giving users an option to either install
the software in the default location or
in a specific location accept the
default location and click Next to
continue the installation process in the
next screen we will continue to discuss
the steps for installing VMware Player
the check for product updates on startup
section will be displayed the checkbox
is selected by default
click Next to continue the installation
process in the next screen we will
continue to discuss the steps for
installing VMware Player
the wizard provides options to create
VMware shortcuts check the boxes and
click Next to continue the installation
process in the next screen we will
continue to discuss the steps for
installing VMware Player
the installation wizard states that the
installation process is yet to begin
click continue in the next screen we
will continue to discuss the steps for
installing VMware Player
the installation wizard displays the
setup wizard complete section click
finish to complete the installation
process in the next screen we will
continue to discuss the steps for
installing VMware Player
double-click on the desktop icon for
VMware Player accept the license
agreement and click Next in the next
screen we will continue to discuss the
steps for installing VMware Player
the window shown on screen is displayed
VMware Player is installed successfully
in the next screen we will list the
steps to create a VM in VMware Player
perform the steps shown on screen to
install VMware Player click create a new
virtual machine select I will install
the operating system later and click
Next select the appropriate guest
operating system and version choose VM
name and location name and click Next
specify the disk capacity by providing
the disk size select store virtual disk
as a single file and click the next
button once finalized click finish to
create VM in the subsequent screens we
will perform the steps to create a VM
install VMware Player in your system the
window shown on the screen will be
displayed click create a new virtual
machine in the next screen we will
continue to discuss the steps to create
a VM select I will install the operating
system later and click Next in the next
screen we will continue to discuss the
steps to create a VM
select the appropriate guest operating
system and version that need to be
installed in this virtual machine and
click Next in the next screen we will
continue to discuss the steps to create
a VM
choose the appropriate virtual machine
name and location to store the VM files
and click Next in the next screen we
will continue to discuss the steps to
create a VM
provide the maximum disk size in
gigabytes select store virtual disk as a
single file and click Next in the next
screen we will continue to discuss the
steps to create a VM
a screen that shows the summary of the
VM settings previously chosen is
displayed click finish in the next
screen we will learn to open a VM using
VMware Player the window shown on the
screen is displayed which means the VM
is created successfully in the VMware
Player click open a virtual machine to
open an existing virtual machine in the
next screen we will continue to discuss
how to open a VM using VMware Player
open the Hadoop sudo server VM by
selecting the vmx file provided by
simply learn and click open in the next
screen we will understand how to open a
VM using an Oracle VirtualBox
alternatively the oracle virtualbox can
be used to open virtual machines
generally apple macintosh users use this
free vm player since vmware fusion
player is usually available for trial or
limited usage download the latest
version of VirtualBox compatible with
your machine and firewall settings from
the URL shown on screen in the
subsequent screens we will perform the
steps to open a vm using VirtualBox open
Oracle VM VirtualBox manager click
machine tab and select new to create a
new virtual machine the create virtual
machine window is displayed enter the
name and target operating system and
click Next in the next screen we will
continue to discuss how to open a vm
using Oracle VirtualBox
allocate the desired memory size to the
virtual machine and click Next in the
next screen we will continue to discuss
how to open a VM using Oracle VirtualBox
select use an existing virtual hard
drive file and click create in the next
screen we will continue to discuss how
to open a VM using Oracle VirtualBox
that selected VM file is displayed in
the next screen we will discuss a
business scenario to understand the use
of vmware player
Olivia is the EVP of IT operations with
nutri worldwide incorporated Olivia
plans to work from home to do so she has
to access her system remotely from her
home computer her home computer has a
different operating system than the one
in her office she has just installed
VMware Player on her home computer and
wants to start working the demo in the
subsequent screen will illustrate how to
work on virtual machine VM though the
following demonstration shows how to
work on the VM for Windows platform
these steps can be applied to start a VM
player across all platforms in the next
screen we will view a demo to start the
Hadoop VM provided by simply learn
let us do a quick recap of the steps
performed
you
let us summarize the topics covered in
this lesson the vmware player executable
file is used to install the VMware
Player a VM can be created in the VMware
in five steps the Oracle VirtualBox can
be used to open virtual machines Hadoop
VM provided by simply learn is a
pre-configured VM with Hadoop installed
in Ubuntu server
12.04 OS
hello and welcome to lesson three of the
Big Data and Hadoop developer course
offered by simply learn this lesson
provides detailed information of the
Hadoop architecture let us explore the
objectives of this lesson in the next
screen by the end of this lesson you
will be able to describe the use of
Hadoop in commodity Hardware explain the
various configurations and services of
Hadoop differentiate between regular
file system and Hadoop distributed file
system HDFS explain HDFS architecture in
the next screen we will discuss some key
terms used in Hadoop architecture
an understanding of some key terms is
essential while discussing Hadoop
architecture the word commodity hardware
refers to normal PCs which can
successfully be used to make a cluster
the word cluster refers to multiple
systems interconnected in the same
network to logically work for one
process in distributed mode and nodes
are the commodity servers which are
interconnected through a network device
in the next screen we will discuss the
use of commodity Hardware in a Hadoop
cluster
Hadoop is a framework which is often
used in commodity hardware it supports
the concept of distributed architecture
the diagram represents the cluster of
nodes connected and installed with
Hadoop the maximum number of nodes that
can reside in a single rack is around 30
to 40 however it is not an exact count
you can have more number of nodes with
an adequate network speed normally the
uplink from rack to node is 3 to 4
gigabytes per second
the uplink from rack to Rack internally
is 1 gigabyte per second in the next
screen we will study Hadoop
configuration there are 5 core Hadoop
services name node data node job tracker
task tracker and a secondary name node
generally in fully distributed mode the
name node secondary name node and job
tracker are termed as master services
whereas the data node and task tracker
are termed as slave services in the next
screen we will study the core components
of Hadoop there are two major components
of Apache Hadoop they are Hadoop
distributed file system abbreviated as
HDFS and Hadoop MapReduce HDFS is used
to manage the storage aspects of big
data whereas MapReduce is responsible
for processing jobs in a distributed
environment in the next screen we will
discuss HDFS HDFS is used for storing
and retrieving unstructured data some of
the key features of Hadoop HDFS are as
follows HDFS provides a high throughput
access to data blocks when unstructured
data is uploaded on HDFS it is converted
into data blocks of fixed size the data
is chunked into blocks so that it is
compatible with the commodity hardware's
storage HDFS provides a limited
interface for managing the file system
it ensures that one can
form a scale up or scale down of
resources in the Hadoop cluster HDFS
creates multiple replicas of each data
block and stores them in multiple
systems throughout the cluster to enable
reliable and rapid data access in the
next screen we will discuss MapReduce
the MapReduce component of Hadoop is
responsible for processing jobs in
distributed mode some of the key
features of the Hadoop MapReduce
component are as follows it performs
distributed data processing using the
MapReduce programming paradigm it allows
you to possess a user-defined map phase
which is a parallel share noting
processing of input also it aggregates
of the output of the map phase which is
a user-defined
reduce phase after a map process in the
next screen we will differentiate a
regular file system from HDFS
a comparison between a regular file
system and HDFS is shown on screen in a
regular file system each block of data
is small usually about 51 bytes
however in HDFS each block is of a very
large size it is 64 megabytes by default
a regular file system provides access to
large data but this feature of the file
system may suffer from disk i/o problems
mainly due to multiple seek operations
HDFS can read huge data sequentially
after a single seek which makes it
unique since all these operations are
performed in distributed mode in the
next screen we will list the
characteristics of HDFS there are
certain basic characteristics of HDFS
that make it popular HDFS has high fault
tolerance an HDFS instance may consist
of thousands of server machines each
storing a part of the file systems data
since we have a huge number of
components and each component has a
non-trivial probability of failure there
is always some component that is
non-functional detection of faults and
quick automatic recovery is a core
architectural goal of HDFS HDFS provides
high throughput HDFS is designed to
store and scan millions of rows of data
and count or add some subsets of the
data this may take a few minutes or much
longer depending on the complexity
involved
traditionally HDFS has been designed to
support high throughput on large data
sets in batch style jobs HDFS is
suitable for applications with streaming
access to filesystem data these are not
general purpose applications that
typically run on general-purpose file
systems HDFS is designed more for batch
processing and interactive use by users
the emphasis is on high throughput of
data access rather than low latency of
data access
also HDFS is designed in a way such that
it can be built on commodity hardware
and heterogeneous platforms in the next
screen we will list the key features of
HDFS
the key features of HDFS are as follows
HDFS creates multiple replicas of each
data block and distributes them on
computers throughout a cluster to enable
reliable and rapid access of data HDFS
is the storage system for both the input
and output of MapReduce jobs the file
system can be accessed by using the HDFS
protocol to perform IO operation on the
file which is stored in HDFS block
storage metadata controls the physical
location of a block and its replication
within the cluster each block is
replicated to a small number of
physically separate machines for example
three machines however the count can be
modified by the administrator in the
next screen we will discuss HDFS
architecture view a typical HDFS setup
is shown on this screen this setup shows
the three essential services of Hadoop
named node data node and secondary name
node services the name node and the
secondary name node services constitute
the master service whereas the data node
service falls under the slave service
the master service is responsible for
accepting a job from clients and
ensuring that the data required for the
operation will be loaded and segregated
into chunks of data blocks HDFS exposes
a file system namespace and allows user
data to be stored in files a file is
split into one or more blocks stored and
replicated in data nodes the data blocks
are then distributed to the data node
systems within the cluster this ensures
that replicas of the data are maintained
in the next screen we will understand
the HDFS operation principle
HDFS components include different
servers like the name node server the
data node server and the secondary name
node server the name node server is the
core component of an HDFS cluster there
can be just one name node server in an
entire cluster it is responsible for
maintaining the file system namespace
the name node server is responsible for
managing the files and directories in
the file system tree information of the
data and the metadata is stored in the
namespace image and the edit log the
name node is aware of the data node
status that is it knows the data node in
which certain data is stored the name
node server is a critical single point
of failure if it fails the entire
cluster goes down however the name node
server can be partially restored by
using a secondary name node server data
node is a multiple instance server there
can be n number of data node servers
depending on the type of networking and
storage system that a set up has the
data node servers are responsible for
storing and maintaining the data blocks
which are provisioned by the name node
server on the basis of the type of job
submitted by the client the data node
server is responsible for storing and
retrieving the blocks when referred by
clients or the name node the data node
servers read write requests and perform
block creation deletion and replication
upon instruction from the name node
there can be only one secondary name
node server in a cluster it is
responsible for maintaining a backup of
the name node server but you cannot
treat the secondary name node server as
a disaster recovery server it will only
partially restore the name node server
in case of failure the secondary name
node server maintains the edit log and
namespace image information which is
synced with the name node server
sometimes the namespace images from the
name node server lag
find due to which we cannot totally rely
on the secondary name node server for
the recovery process in the next screen
we will discuss HDFS HDFS is a
master/slave architecture which consists
of a single name node and a data node
HDFS is also responsible for maintaining
the file system namespace and allowing
user data to be stored in files the file
is split during the map phase into
multiple blocks and these blocks are
stored in a set of data nodes ensuring
that the replication factor is
maintained the name node helps to
execute file system namespace operations
like opening closing and renaming of
files and directories which are present
in HDFS it also determines the linking
of blocks to data nodes data nodes are
responsible for performing read and
write requests in HDFS based on the
clients request data nodes also perform
block creation deletion and replication
on instruction from the name node the
name node and data node are pieces of
software designed to run on commodity
machines in the next screen we will
discuss the filesystem namespace of HDFS
HDFS exposes a file system namespace and
allows user data to be stored in files
HDFS has a hierarchical file system with
directories and files
it supports operations like create
remove move rename etc the name node
maintains the file system and records
any change to meta information in the
next screen we will understand name node
operation
the name node manages the file system
namespace allowing clients to work with
files in directory subtrees HDFS
supports a traditional hierarchical file
organization a user or an application
can create directories and store files
inside these directories the file system
namespace hierarchy is similar to most
other existing file systems one can
create and remove files move a file from
one directory to another or rename a
file the name node maintains the file
system namespace any change to the file
system namespace or its properties is
recorded by the name node the name node
maintains to persistent files a
transaction log called edit log and a
namespace image called
FS image the edit log records every
change that occurs in the file system
metadata for example creating a new file
the edit log is stored in the name nodes
local file system the entire file system
namespace including mapping of blocks to
files and file system properties is
stored in the FS image this is also
stored in the name nodes local file
system
however metadata loads into its memory
what blocks of data reside on which data
nodes at startup when new data nodes
join a cluster and periodically
thereafter when the name node starts up
it gets the edit log and FS image from
its local file system updates FS image
with edit log information and then
stores a copy of the FS image on the
file system as a checkpoint the metadata
size is limited to the RAM available on
the name node since a large number of
small files would require more metadata
than a small number of large files the
in-memory metadata management issue
explains why HDFS favors the latter over
the former if a name node runs out of
RAM it will crash and applications won't
be able to use HDFS until the
is online again in the next screen we
will study data block split each file is
split into one or more blocks stored and
replicated in data nodes data nodes
manage names and locations of file
blocks which are 64 megabytes each by
default for large files it is better to
increase the block size to 128 megabytes
doing so decreases pressure on the name
nodes memory on the contrary this
potentially decreases the amount of
parallelism that can be achieved as the
number of blocks per file decreases
since each map task operates on one
block if there are tasks lesser than
nodes in the cluster jobs will run
slower than they could the larger the
individual files involved or the more
files involved in the average MapReduce
job the lesser in issue this is in the
next screen we will understand the
benefits of the data block approach some
of the benefits of the data block
approach include simplified replication
fault tolerance and reliability and
shielding of users from storage
subsystem details in the next screen we
will understand the data block
replication architecture
in hdfs unstructured data is represented
in the form of data blocks the default
size of the blocks is 64 megabytes
however block size can be reset by the
user or the administrator HDFS performs
block replication on multiple data nodes
so that if any error exists on one data
node server the jobtracker service which
is present in the name node server will
resubmit the job to another data node
server block replication refers to the
creation of replicas of a block in
multiple data nodes usually the data is
split in the form of parts like part 0
part 1 etc as shown in the image the
image also represents the parts that are
stored in multiple data nodes in the
next screen we will discuss the
replication method each file is split
into a sequence of blocks all blocks in
the file except the last are of the same
size blocks are replicated for fault
tolerance block replication factor is
usually configured at the cluster level
but can also be configured at the file
level the name node receives a heartbeat
and a block report from each data node
in the cluster block report contains all
the blocks on a data node an application
can also specify the number of replicas
of the file needed that is the
replication factor of the file this
information is stored in the name node
in the next screen we will discuss a
suggested data replication topology in
terms of replication one of the
suggested replication topologies could
be having the first replicas placed on
the same node as the client the second
replicas is placed on a rack different
from the first rack the third replica is
placed on the same rack as the second
rack but on a different node in the next
screen we will understand a pictorial
representation of replication topology
on this chart we see a Hadoop cluster
with three racks represented by the dark
blue pillars each rack contains multiple
nodes represented by rectangular boxes
within each rack for example on the far
left you'll see a box labeled r1 and one
which represents node one on rack one
for simplicity each rack shown here has
eight nodes at the top you see the name
node is shown in yellow let's follow the
path of one particular data block block
b1 shown in purple on this chart b1 is
first written to node 4 on rack 1 a copy
is then written on a different rack rack
2 in this case in fact it's written to
node 5 on rack to the third and final
copy of the block is written to the same
rack as the second copy rack 2 but a
different node which is node 1 in this
example in the next screen we will
discuss various ways to access HDFS
HDFS provides various access mechanisms
we can use a Java API for applications
there is also a Python and C language
wrapper for non java applications we can
also utilize web GUI through an HTTP
browser an FS shell is available for
executing commands on HDFS in the next
screen we will discuss a business
scenario related to HDFS Olivia Tyler
the EVP of IT operations with nutri
Worldwide Incorporated has decided to
use HDFS for storing big data she will
be using an HDFS shell to store the data
in the Hadoop file system and execute
various commands on it the demo in the
subsequent screen illustrate how to
perform basic command line operations on
HDFS
let us do a quick recap of the steps
performed
you
let us summarize the topics covered in
this lesson Hadoop works on three
configurations namely standalone mode
pseudo distributed mode and fully
distributed mode the two core components
of Apache Hadoop are HDFS and MapReduce
the HDFS layer contains name node
secondary name node and a data node HDFS
can be used to handle big data sets with
its inherent features of fault tolerance
high throughout and streaming data
access
in the next lesson we will focus on
Hadoop deployment</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>