<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction To Apache Cassandra Certification Training | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Introduction To Apache Cassandra Certification Training | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction To Apache Cassandra Certification Training | Simplilearn</b></h2><h5 class="post__date">2016-04-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JlZxTmmRdDk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to the introductory
lesson of the Apache Cassandra course
offered by simply learned this lesson
will give you an overview of the course
its prerequisites and the value it will
offer you before you start please spend
some time and go through the key
features of simply learn offerings
displayed on the screen after completing
this course you will be able to describe
the need for big data and no sequel and
explain the fundamental concepts of
Cassandra you will also be able to
describe the architecture of cassandra
and demonstrate data model creation in
cassandra finally you will be able to
use Cassandra database interfaces and
demonstrate Cassandra database
configuration the Apache Cassandra
training course offered by simply learn
provides details on the fundamentals of
Big Data and no sequel databases the
course will also provide an overview of
Cassandra and its features further it
will enhance your knowledge on the
architecture and data model of Cassandra
and provide an overview of the
installation configuration and
monitoring of Cassandra finally the
course will enhance your knowledge on
the Hadoop ecosystem of products around
Cassandra the course is aimed at
professionals aspiring for a career in
no sequel databases and Cassandra
analytics professionals research
professionals IT developers testers and
project managers are the key
beneficiaries of this course other
aspirants and students who wish to gain
a thorough understanding of Apache
Cassandra can also benefit from this
course fundamental knowledge of any
programming language is a prerequisite
for the course participants are expected
to have basic understanding of any
database sequel and query language for
databases a working knowledge of Linux
or UNIX based systems is an added
advantage for this course although it is
not mandatory the Apache Cassandra
professionals will be able to
demonstrate their expertise in the fast
big data industry well placed to benefit
from the growing demand in no sequel
databases able to benefit from the
shortage of cassandra trained
professionals well-equipped to take
their organization towards big data
analytics using cassandra experienced in
tools used to process huge amounts of
data in the forefront of the Big Data
technology which is expected to be in
demand for the next 10 years there are
eight core lessons in this course apart
from the current lesson course overview
take a look at the course map displayed
on the screen in addition to the lessons
there are some demos provided in the
course to facilitate a better
understanding of the concepts
this concludes the course overview the
next lesson provides an overview to Big
Data and no sequel all the best hello
and welcome to the first lesson of the
Apache Cassandra course offered by
simply learned this lesson will provide
an overview of the Big Data and no
sequel database
the Apache Cassandra course by simply
learned is divided into eight lessons as
listed this is the first lesson overview
of the Big Data and no sequel database
after completing this lesson you will be
able to describe the three V's of Big
Data you will also be able to discuss
some use cases of Big Data further you
will be able to explain Apache Hadoop
and the concept of no sequel finally you
will be able to describe various types
of no sequel databases Big Data has
three main characteristics volume
velocity and variety volume denotes the
huge scaling of data ranging from
terabytes to zettabytes velocity
accounts for the streaming of data and
the movement of large volumes of data at
high speed variety refers to managing
the complexity of data in different
structures ranging from relational data
to blogs and raw text in addition there
are other V's of Big Data however
they're not as popular these are
veracity visualization and value
veracity refers to the truthfulness of
data visualization refers to the
presentation of data in a graphical
format and value refers to the derived
value of an organization from using Big
Data the term volume refers to data
volume which is the size of digital data
the impact of internet and social media
has resulted in the explosion of digital
data data has grown from gigabytes to
terabytes petabytes exabytes and
zettabytes as illustrated on the image
in 2008 the total data on the internet
was 8 exabytes it exploded to 150
exabytes by 2011
it reached 670 exabytes in 2013 it is
expected to exceed 7 zettabytes in the
next 10 years the table contains various
terms used to represent data sizes the
size among with the power and
description of each data term is given
it is recommended you spend some time to
go through the contents of the table for
better understanding the new terms added
to address big data sizes are exabyte
zettabyte and yottabytes typically the
term big data refers to data sizes in
terms of terabytes or more the term
velocity refers to the speed at which
the data grows people use devices that
create data constantly data is created
from different sources such as desktops
laptops mobile phones tablets and
sensors due to the increase in the
global customer base and transactions
and interactions with customers the data
created within an organization is
growing along with external data as
illustrated on the image there are many
contributors to this data growth such as
web online billing systems ERP
implementations machine data network
elements and social media growth in
revenues of organizations indicate
growth in data the term variety refers
to different types of data data includes
text images audio video XML and HTML
there are three types of data structure
data where the data is represented in a
tabular format for example my sequel
databases semi structured data where the
data does not have a formal data model
for example XML files unstructured data
where there is no predefined data model
everything is defined at runtime
for example text files digital data has
evolved over 30 years starting with
unstructured data initially the data was
created as playing
text documents next files were created
and data and spreadsheets increased the
usage of digital computers the
introduction of relational databases
revolutionized structured data as many
organizations used it to create large
amounts of structured data next the data
expanded to data warehouses and storage
area networks or sands to handle large
volumes of structured data then the
concept of metadata was introduced to
describe structured and semi-structured
data big data has the following features
it is extremely fragmented due to the
variety of data it does not provide
decisions directly however it can be
used to make decisions big data does not
include unstructured data only it also
includes structured data that extends
and complements unstructured data
however big data is not a substitute for
structured data since most of the
information on the Internet is available
to anyone it can be misused by
anti-social elements generally big data
is wide therefore you may have hundreds
of fields in each line of your data
it is also dynamic as gigabytes and
terabytes of data is created every day
big data can be both internal generated
within the organization and external
generated on social media every industry
has some use for big data some of the
big data use cases are as follows in the
retail sector big data is used
extensively for affinity detection and
performing market analysis credit card
companies can detect fraudulent
purchases quickly so they can alert
customers while giving loans banks
examine the private and public data of a
customer to minimize risk in medical
diagnostics doctors can diagnose a
patient's illness based on symptoms
instead of intuition digital marketers
need to process huge customer data to
find effective marketing channels
insurance companies use big data to
minimize insurance risks an individual's
driving data can be captured
automatically and sent to the insurance
companies to calculate premium for risky
drivers manufacturing units and oil rigs
have sensors that generate gigabits of
data every day which are analyzed to
reduce risk of equipment failures
advertisers use demographic data to
identify the target audience terabytes
and petabytes of data are analyzed in
the field of genetics to design new
models power grids analyze large hunts
of historical and weather forecasting
data to forecast power consumption with
the origin of big data analytics you can
use complete sets of data instead of
sample data to conduct an analysis as
represented on the image in traditional
analytics method analysts take a
representative data sample to perform
analysis and draw conclusions using big
data analytics the entire data set can
be used big data analytics help you find
associations in data predict future
outcomes and perform prescriptive
analysis the outcome of prescriptive
analysis will be a definitive answer not
a probable answer further using big data
for analysis also helps in taking
data-driven decisions instead of
decisions based on intuitions
it also helps organizations increase
their safety standards reduce
maintenance costs and prevent failures
traditional technology can be compared
with Big Data technology in the
following ways traditional technology
has a limit on scalability whereas Big
Data technology is highly scalable
traditional technology uses highly
parallel processors on a single machine
whereas Big Data technology uses
distributive processing with multiple
machines further in traditional
technology processors may be distributed
with data in one place however in Big
Data technology the day
is distributed to multiple machines
traditional technology depends on
high-end expensive hardware costing more
than $40,000 per terabyte on the other
hand Big Data technology leverages
commodity hardware which may cost less
than $5,000 per terabyte traditional
technology uses storage technologies
such as Sam
whereas Big Data technology uses
distributed data with data redundancy
Apache Hadoop is the most popular
framework for big data processing Hadoop
has two core components Hadoop
distributed file system or HDFS and
MapReduce Hadoop uses HDFS to distribute
the data into multiple machines it uses
MapReduce to distribute the process to
multiple machines further Hadoop
distributes the processing action to
where the data is instead of moving data
towards the processing the concept of
distribution performed by HDFS and
MapReduce is illustrated on the image
first HDFS divides the data into
multiple sets such as data 1 data 2 and
data 3 next these datasets are
distributed to multiple machines such as
CPU one CPU 2 and CPU 3 this action is
performed by MapReduce in the end the
processing is done by the CPUs of each
machine on the data assigned to that
machine HDFS is the storage component of
Hadoop which stores each file as blocks
with a default block size of 64
megabytes this is larger than the block
size on Windows which is 1k or 4k HDFS
is a write once read many form of file
system also called worm blocks are
replicated across nodes in the cluster
HDFS provides three default replication
copies the image illustrates this
concept with an example suppose you
store a 320 megabyte
file into HDFS it will be divided into
five blocks each of size 64 megabytes as
64 multiplied by five is 320 if there
are five nodes in the cluster each block
is replicated to make three copies to
result in a total of 15 blocks these
blocks are further distributed to the
five nodes so that no two replicas of
the same block are on the same node Map
Reduce is the processing framework of
Hadoop it provides highly fault tolerant
distributed processing of the data
distributed by HDFS MapReduce consists
of two types of tasks mappers and
reducers mappers are tasks that run in
parallel on different nodes of the
cluster to process the data blocks in
programming language mappers are called
key value pairs after completion of the
map tasks results are gathered and
aggregated by the reduced tasks of
MapReduce
reduce tasks consolidate and summarize
the results each mapper runs on the
machine on which the data block is
assigned data locality is preferred this
follows the principle of taking
processing to the data no sequel is the
common term used for all databases that
do not follow the traditional relational
database management system or our DBMS
principles in no sequel databases the
overhead of the acid principles is
reduced acid stands for atomicity
consistency isolation and durability
this is a set of properties that
guarantee the reliable processing of
database transactions these principles
are guaranteed by most our DBMS the
process of normalization is not
mandatory with big data it is difficult
to follow our DBMS principles and
normalization no sequel databases prefer
denormalized databases due to the
transactional database requirement of
our DBMS the relational databases are
not able to handle terabytes and
petabytes of data
a no sequel database is used to overcome
the limitations of transactional
databases the image represents people
uploading the data and reading the data
from a transactional database Eric
Brewer a computer scientist proposed the
Brewers consistency availability and
partition tolerance or cap principle in
1999 cap is the basis for many no sequel
databases in a distributed system
consistency means that all nodes in the
cluster viewed the same data at the same
time availability means that a response
is guaranteed for every request received
the response can be in terms of whether
a request was successful or it failed
partition tolerance means the system
continues to operate despite the ad-hoc
message loss or failure on part of the
system further Brewers stated that it is
not possible to guarantee all three
aspects simultaneously in a distributed
system therefore most no sequel
databases compromise on one of the three
aspects to provide better performance
there are four main types of no sequel
approaches graph database document
database key value stores and column
stores
let us discuss each of these one by one
first is the graph database a graph
database helps in representing graphical
data in the nodes and edges format some
of its features are as follows graph
databases can handle millions of nodes
and edges they can perform efficient
depth-first and breadth-first search is
on the graph data other graph searches
and traversal algorithms neo4j and flock
DB of the popular graph databases the
image depicts a graph with four nodes in
a neo4j database second is the document
database a document database helps in
storing and processing a huge number of
documents in the document database you
can store millions of documents and
process fields for example you can store
your employee details and their resumes
as documents and search for a potential
employee using fields like the phone
number MongoDB and couchdb are the
popular document databases the image
depicts the fields of a document stored
in MongoDB third is the key value stores
these store the data in key and value
formats where each piece of data is
identified by a key and has associated
values in the key value stores you can
store billions of records efficiently
and provide fast rights as well as
searches of data based on keys Cassandra
and Reedy's are popular key value stores
the image depicts the keys and values
stored in a Cassandra database you will
learn more about key value stores when
we discuss Cassandra in the upcoming
lessons last is the column stores these
are also called column oriented
databases column stores organized data
in groups of columns and are efficient
in data storage and retrieval based on
keys some of their features are HBase is
part of the Hadoop ecosystem that runs
on top of HDFS to store and process
terabytes and petabytes of data
efficiently column stores normally
maintain a version of the data along
with each value of data HBase and hyper
table are the most common column stores
the image illustrates how HBase stores
the data data is organized by column
families and each column family can have
one or more columns for each column
along with the data value a version
number indicating the time of data
update is also stored the column base
data is stored along with the key note
that no sequel databases cannot replace
general-purpose databases although they
provide better performance and
scalability they compromise on some
aspects
like ease-of-use and full sequel query
support let us summarize what we have
learned in this lesson
Big Data is mainly characterized by
variety velocity and volume with the
original big data analytics complete
sets of data can be used to conduct an
analysis instead of sample data Apache
Hadoop is the most popular framework for
big data processing it has two
components HDFS and MapReduce HDFS is
the storage component of Hadoop and
MapReduce is the processing framework of
Hadoop no sequel is the common term used
for all the databases that do not follow
traditional RDBMS principles it is based
on Brewers cap principle Brewers stated
that it is not possible to guarantee the
consistency availability and partition
tolerance aspect simultaneously in a
distributed system this concludes the
lesson on overview of Big Data and no
sequel the next lesson will provide an
introduction to Cassandra hello and
welcome to lesson 2 of the Apache
Cassandra course offered by simply learn
this lesson we'll provide an
introduction to Cassandra simply learns
Apache Cassandra course is divided into
eight lessons as listed on the screen
introduction to Cassandra is the second
lesson by the end of this lesson you
will be able to describe Cassandra and
its features described when Cassandra is
used demonstrate how to work with the
command-line interface of Cassandra list
the advantages and limitations of
Cassandra and demonstrate how to install
a VMware Player Cassandra is a no sequel
database that is highly scalable and big
data ready you learned in Lesson one
that a no sequel database is one that
can work with denormalized data
Cassandra is a distributed database that
is highly fault tolerant with no single
point of failure
further it is a high performance
database Cassandra derives its name from
Greek mythology Cassandra was the
daughter of King Priam of Troy and his
wife her Cuba Cassandra had the power to
predict the future but she also had a
curse that nobody would believe her
predictions true to its name
the Cassandra database holds a lot of
promise for the future but suffers from
many limitations as well Cassandra was
initially developed in 2008 at Facebook
as a combination of the BigTable
datastore used by Google and the Dynamo
datastore used by Amazon it was
developed by Avinash Lakshman the author
of Amazon dynamo and Prashant Malik it
was developed to solve the inbox search
problem of Facebook it has evolved a lot
since its inception in 2008 it started
with the concept of column families and
super column families but later evolved
as a key value store you can still get
messages from Cassandra about column
families version 1.0 was released in
2011 in 2014 Cassandra became an apache
open source project and the current
version 2.0 was released the following
are the main features of Cassandra
Cassandra is a key-value database data
is stored as tables and columns and
every table has a primary key further
cassandra has a limited sequel interface
in addition it provides very fast read
and writes a sample Cassandra query is
provided on screen select ticker comma
value from stocks where a ticker is
equal to XYZ order by ticker observe
that the syntax is similar to that of a
sequel in a relational database
management system Cassandra is used to
store a huge amount of information very
quickly for example when you are
processing telecom switched data or
stock market data a huge volume of data
is generated every minute Cassandra is
also useful when you want full indexed
search to get the data quickly and the
data needs to be sorted in a free to
and order full indexed searches searches
performed using a key another case where
Cassandra is useful is when you expect
an upsurge in data size cassandra
enables scaling by adding more nodes as
the data grows you can also use
Cassandra when you want a highly
fault-tolerant cluster with no single
point of failure
it is also preferred when you need high
performance for both data read and write
an example of a simple Cassandra program
is provided on screen in this example
you create a table insert a few records
and fetch data from the table you can
see that the Cassandra syntax is very
similar to the standard sequel syntax
the example creates the table called
stocks with two columns inserts three
rows into this table and selects data
from the table for a particular key
observe that Cassandra uses the primary
key of the table to fetch the data the
data is also stored in the primary key
order cassandra provides a command-line
interface that is similar to the linux
shell it can be invoked with CQ LSH if
that is in your path or using vin /cq
LSH from the cassandra base directory
you can use CQ l SH space hyphen H to
get help and arguments for the command
before starting CQ l sh you need to set
the all uppercase CQ LS h underscore
host parameter to the address of one of
the hosts where cassandra is running
within CQ l SH you can access the
history of commands with the up arrow
most commands have elaborate help
available and you can access it using
the help command within Cassandra you
can exit the shell using the exit
command note that you need to terminate
each command with a semicolon the image
shows how to run CQ l SH and also
depicts the sample output from CQ l SH
Cassandra has many advantages for
processing big data
it is highly fault tolerant with no
single point of failure
this means that if any node in the
cluster fails other nodes will take over
and
complete the work every node in the
cluster is identical as there are no
masters or slaves in Cassandra before
one machine cannot become the bottleneck
in the system further you can add a
machine to the cluster or remove a
machine from the cluster anytime without
downtime
Cassandra also provides very fast data
rights allowing real-time processing of
big data cassandra outperforms many
other no sequel databases in terms of
many performance benchmarks Cassandra is
not a general-purpose database due to
some limitations first it doesn't
provide aggregation of data with group
by some min or max like relational
databases any aggregation has to be pre
computed and stored second there are no
joins of tables so data has to be
denormalized before getting stored in
Cassandra third it doesn't support
additional search clauses or conditions
only keys or indexes can be used for a
search we will talk more about this
restriction later in the course lastly
there is no sorting provided on non key
fields in a later lesson you will learn
to install Cassandra on a Ubuntu Linux
system however if you need to work on an
operating system other than Linux you
can access the software provided by
VMware this software allows running one
operating system on another using a
virtual machine this is facilitated by
VMware Player for non-commercial use VM
player can be downloaded and used free
of cost from the VMware website simply
learn has created a virtual machine on
VMware Player this machine known as
Hadoop sudo server comes with the free
installed Ubuntu 12.04 LTS operating
system and Hadoop setup it can be opened
with the VMware Player and can be used
for installing Cassandra Hadoop sudo
server can be downloaded from the given
link putty is a popular free tool for
connecting to Linux systems from Windows
through a remote terminal it overcomes
some of the limitations of the VM
for example it allows moving the mouse
pointer with ease scrolling in the
window and copying and pasting text
Hoodie can be downloaded from the given
link let us summarize the topics covered
in this lesson Cassandra is a key-value
no sequel datastore Cassandra is a
highly fault-tolerant database Cassandra
was started in 2008 at Facebook and
became an apache project in 2014
cassandra supports tables columns and
simple sequel statements cassandra
provides fast reads and writes the
supporting real-time data processing
cassandra does not support aggregates or
joins if you need to work on an
operating system other than linux you
can assess the software provided by
VMware putty is a popular free tool for
connecting to Linux systems from Windows
through a remote terminal when SCP is a
popular tool for copying files between
Windows and Linux this demo will show
the steps to install Ubuntu virtual
machine or VM and connect with puddi
this demo will show the steps to install
Ubuntu virtual machine or VM and connect
with puddi
download the VMware Player from the
given link and double-click the VM -
Claire - 7.10 - 2 4 9 6 8 - 4 dot exe
file to start the installation
the installation wizard appears with the
welcome screen of the VMware Player
setup click the next button to continue
the installation process in the License
Agreement screen select the I accept the
terms in the license agreement radio
button after reading the full agreement
click the next button to continue the
installation process
in the destination folder screen choose
the default location and click the next
button to continue the installation
process in the software update screen
uncheck the check for product updates on
startup checkbox click the next button
to continue
in the user experience Improvement
Program screen uncheck the help improve
VMware Player checkbox click the next
button to continue in the shortcut
screen click the next button to continue
the installation process in the ready to
perform the requested operation screen
click the continue button the
installation process will start this
will take a few minutes to complete
you
in the setup wizard' complete screen
click the finish button to complete the
installation start the vmware player
in the welcome screen click the open of
virtual machine option to open an
existing VM download and unzip the
Hadoop pseudo server dot our AR file
provided by simply learn
the extraction process begins
open the vmware player again and click
the open of virtual machine option
browse and double-click the unzipped
Hadoop pseudo server file note that the
actual location may be different on your
system
open the Hadoop pseudo distributed
server vmx file
click the take ownership button
click the play virtual machine play
button to start the virtual machine
click the I copied it button
in the removable devices popup window
select the do not show this hint again
checkbox and click the ok' button in the
software updates window click the remind
me later button to continue
you
when you get the login prompt enter the
login ID as simply learn and password as
simply learn
you
note that you can use ctrl + alt keys to
switch between the VM screen and the
window screen after logging in enter
ifconfig in the command prompt to get
the network configuration of the system
write down the IP address of the system
as you will need it in the later part of
the demo download putty using the given
link install and configure putty to
connect to the IP address saved earlier
further download win SCP from the given
link winscp is a resourceful utility to
exchange files between the windows and
linux vm you can install and configure
winscp to connect to the IP address
saved earlier run puddi by
double-clicking the putty - 0.64 -
installer dot exe file in the security
warning window click the Run button the
Welcome wizard appears click the next
button to continue in the Select
destination location window click the
next button to continue in the select
Start menu folder window click the next
button in the select additional tasks
window click the next button to continue
finally in the read to install window
click the install button
in the setup wizard uncheck the view
readme.txt checkbox and click the finish
button
open buddy
in the footie configuration window enter
the hostname with the IP address saved
in the earlier part of this demo simply
learn at the rate 192.168.1 89.1 to 8
you
so let's simply learn from the given
options and click the load button
ensure the IP address is 192.168.1 89.1
to eight next click the Save button to
save the settings and the Open button to
open a connection to the VM
in the footie security alert window
click the yes button in the login prompt
enter the user ID as simply learned and
the password as simply learn
congratulations you have successfully
logged into the virtual machine from
puddi this VM will be used to set up
Cassandra also in the later lessons
note that multiple connections can be
opened on the VM using puddi this
concludes the process of setting up and
installing the VM let us summarize the
topics covered in this lesson Cassandra
is a key value no sequel datastore
Cassandra is a highly fault-tolerant
database Cassandra was started in 2008
at Facebook and became an Apache project
in 2014 Cassandra supports tables
columns and simple sequel statements
Cassandra provides fast reads and writes
the supporting real-time data processing
Cassandra does not support aggregates or
joins if you need to work on an
operating system other than Linux you
can assess the software provided by
VMware putty is a popular free tool for
connecting to Linux systems from Windows
through a remote terminal when SCP is a
popular tool for copying files between
Windows and Linux this concludes the
lesson introduction to Cassandra in the
next lesson we will introduce Cassandra
architecture hello and welcome to the
third lesson of the Apache Cassandra
course offered by simply learn this
lesson will provide an overview of the
Cassandra architecture the Apache
Cassandra course by simply learned is
divided into eight lessons as listed
this is the third lesson Cassandra
architecture after completing this
lesson you will be able to describe the
Cassandra architecture components of
Cassandra and the effects of Cassandra
architecture you will also be able to
explain the partitioning of data in
Cassandra Cassandra topology and various
failure scenarios handled by
andr Cassandra was designed to address
many architecture requirements the most
important requirement is to ensure there
is no single point of failure this means
that if there are 100 nodes in a cluster
and a node fails the cluster should
continue to operate this is in contrast
to Hadoop where the name node failure
can cripple the entire system another
requirement is to have massive
scalability so that a cluster can hold
hundreds or thousands of nodes it should
be possible to add a new node to the
cluster without stopping the cluster
further the architecture should be
highly distributed so that both
processing and data can be distributed
also high performance of read and write
of data is expected so that the system
can be used in real time cassandra is
designed such that it has no master or
slave nodes it has a ring type
architecture that is its nodes are
logically distributed mica ring data is
automatically distributed across all the
nodes similar to HDFS data is replicated
across the nodes for redundancy data is
kept in memory and written to the disk
in a lazy fashion hash values of the
keys are used to distribute the data
among nodes in the cluster a hash value
is a number that map's any given key to
a numeric value for example the string
ABC may be mapped to 101 and decimal
number to five point three four may be
mapped to two five seven a hash value is
generated using an algorithm so that the
same value of the key always gives the
same hash value in a ring architecture
each node is assigned a token value we
will learn more about token values later
in this lesson
let us continue discussing the Cassandra
architecture Cassandra architecture
supports multiple data centers data can
be replicated across data centers you
can keep three copies of data in one
data center and a fourth copy in the
remote data
Center for remote backup data reads
prefer a local data center to a remote
data center Cassandra architecture
enables transparent distribution of data
to nodes this means you can determine
the location of your data in the cluster
based on the data any node can accept
any request as there are no masters or
slaves if a node has the data it will
return the data else it will send the
request to the node that has the data
you can specify the number of replicas
of the data to achieve the required
level of redundancy for example if the
data is very critical
you may want to specify a replication
factor of 4 or 5 if the data is not
critical you may specify just two it
also provides tunable consistency that
is the level of consistency can be
specified as it trade-off with
performance transactions are always
written to a commit log on disk so that
they are durable the Cassandra write
process ensures fast writes first data
is written to a commit log on disk then
the data is sent to a responsible node
based on the hash value nodes write data
to an in-memory table called mem table
from the mem table data is written to an
SS table in memory a SS table stands for
sorted string table this has a
consolidated data of all of the updates
to the table from the SS table data is
updated to the actual table if the
responsible node is down data will be
written to another node identified as
temp node the temp node will hold the
data temporarily till the responsible
node comes alive the diagram depicts the
write process when data is written to
table a data is written to a commit more
on disk for persistence it is also
written to an in-memory
mem table
mm table data is written to SS table
which is used to update the actual table
the term Rack is usually used when
explaining network topology a rack is a
group of machines housed in the same
physical box each machine in the rack
has its own CPU memory and hard-disk
however the rack has no CPU memory or
hard disk of its own all machines in the
rack are connected to the network switch
of the rack and the racks network switch
is connected to the cluster all machines
on the rack have a common power supply
it is important to notice that a rack
can fail due to two reasons a network
switch failure or a power supply failure
if a rack fails none of the machines on
the rack can be accessed so it would
seem as though all the nodes on the rack
are down the Cassandra read process
ensures fast reads read happens across
all nodes in parallel if a node is down
data is read from the replicas of the
data priority for the replicas is
assigned on the basis of distance data
on the same nodes given first preference
and is considered data local data on the
same rack is given second preference and
is considered rack local data on the
same data center is given third
preference and is considered data center
local data in a different data center is
given the least preference data in the
memo and SS table is checked first so
that the data can be retrieved faster if
it is already in memory the diagram
represents a Cassandra cluster it has
two data centers data center one and
data center two data center one has two
racks while data center two has three
racks 15 nodes are distributed across
this cluster with nodes 1 to 4
on rack 1 nodes 5 to 7 on rack 2 and so
on the Cassandra read process is
illustrated with an example on this
screen the diagram explains the
Cassandra read process in a cluster with
two data centers five racks and 15 nodes
let us place data Row 1 in this cluster
data Row 1 is a row of data with 4
replicas the first copy is stored on
node 3 the second copy is stored unknown
5 and the third copy is stored unknown 7
all these nodes are in data center one
the fourth copy is stored on node 13 of
data center to give a client process
running on data node 7 once to access
data Row 1 node 7 will be given the
highest preference as the data is local
here the next preference is for node 5
where the data is rack local the next
preference is for node 3 where the data
is on a different rack but within the
same data center the least preference is
given to note 13 that is in a different
data center so the read processed
preference in this example is node 7
node 5 node 3 and node 13 in that order
Kassandra performs transparent
distribution of data by horizontally
partitioning the data a hash value is
calculated based on the primary key of
the data the hash value of the key is
mapped to a node in the cluster and the
first copy of the data is stored on that
node the distribution is transparent as
you can both calculate the hash value
and determine where a particular row
will be stored the diagram on the screen
depicts a four node cluster with token
values of 0 25 50 and
seventy-five for a given key a hash
value is generated in the range of 1 to
100 keys with hash values in the range 1
to 25 are stored on the first node 26 to
50 are stored on the second node 51 to
75 are stored on the third node and 76
to 100 are stored on the fourth node
please note that actual tokens and hash
values in Cassandra are 127 bit positive
integers replication refers to the
number of replicas that are maintained
for each row replication provides
redundancy of data for fault tolerance a
replication factor of 3 means that three
copies of data are maintained in the
system in this case even if two machines
are down you can access your data from
the third copy the default replication
factor is 1 a replication factor of 1
means that a single copy of the data is
maintained so if the node that has the
data fails you will lose the data
cassandra allows replication based on
nodes racks and data centers unlike HDFS
that allows replication based on only
nodes and racks replication across data
centers guarantees data availability
even when a data center is down network
topology refers to how the nodes racks
and data centers in a cluster are
organized you can specify a network
topology for your cluster in the
Cassandra - topology properties file
your data centers and racks can be
specified for each node in the cluster
the format is a list of lines with each
line specifying the IP address for
unknown nodes a default can be specified
you can also specify the host name of
the node instead of an IP address the
diagram depicts an example of a topology
configuration file this file shows the
topology defined for four nodes the node
with IP address 192.168.1.1 oh oh he's
mapped to datacenter dc1 and is present
on the RAC RAC 1 the node with IP
address 192.168.1.1 to and is present on
the RAC RAC 2 similarly the node with IP
address 102 Oh dot 1 1 4 dot 100 is
mapped to datacenter dc2 and RAC RAC 1
and the node with IP address 102 o dot
one 14.1 1 is mapped to datacenter DC 2
and RAC RAC 1 there is also a default
assignment of data center DC 1 and RAC
RAC 1 so that any unassigned notes will
get this data center and rack snitches
defined topology in Cassandra a snitch
defines a group of nodes into rax and
data centers two types of snitches are
most popular a simple snitch and a
property file snitch a simple snitch is
used for single data centers with no
racks a property file snitch is used for
multiple data centers with multiple
racks replication in cassandra is based
on the snitches cassandra uses a gossip
protocol to communicate with nodes in a
cluster it is an internal communication
mechanism similar to the heartbeat
protocol in Hadoop Cassandra uses the
gossip protocol to discover the location
of other nodes in the cluster and get
state information of other nodes in the
cluster the gossip process runs
periodically on each node and exchanges
state information with three other nodes
in the cluster eventually information is
propagated to all class
terr nodes even if there are 1,000 nodes
information is propagated to all the
nodes within a few seconds the image
depicts the gossip protocol process in
step 1
one node connects to three other nodes
in step 2 each of the three notes
connects to three other nodes
thus connecting to nine nodes in total
in step two so a total of 13 nodes are
connected in two steps seed nodes are
used to bootstrap the gossip protocol
they are specified in the configuration
file cassandra tamil seed nodes are used
for bootstrapping the gossip protocol
when a node is started or restarted they
are used to achieve a steady state where
each node is connected to every other
node but are not required during this
steady state the diagram depicts a
startup of a cluster with two seed nodes
initially there is no connection between
the nodes on startup two nodes connect
to two other nodes that are specified as
seed nodes once all the four nodes are
connected seed node information is no
longer required as steady state is
achieved the main configuration file in
Cassandra is the Cassandra dot y mo file
we will look at this file in more detail
in the lesson on installation right now
just remember that this file contains
the name of the cluster seed nodes for
this node topology file information and
data file location this file is located
in slash etc' slash Cassandra in some
installations and in slash etc' slash
Cassandra slash conf directory in others
virtual nodes in a Cassandra cluster are
also called
V nodes V nodes can be defined for each
physical node in the cluster each node
in the ring can hold multiple virtual
nodes by default each
known as 256 virtual nodes virtual nodes
help achieve finer granularity in the
partitioning of data and data gets
partitioned into each virtual node using
the hash value of the key on adding a
new node to the cluster the virtual
nodes on it get equal portions of the
existing data so there is no need to
separately balance the data by running a
balancer the image depicts a cluster
with for physical nodes each physical
node in the cluster has 4 virtual nodes
so there are 16 V nodes in the cluster
if 32 terabyte of data is stored on the
cluster each V node will get 2 terabyte
of data to store if another physical
node with 4 virtual nodes is added to
the cluster the data will be distributed
to 20 V nodes in total such that each V
node will now have 1.6 terabyte of data
the token generator is used in Kassandra
versions earlier than version 1.2 to
assign a token to each node in the
cluster in these versions there was no
concept of virtual nodes and only
physical nodes were considered for
distribution of data the token generator
tool is used to generate a token for
each node in the cluster based on the
data centers and number of nodes in each
data center a token in Cassandra is a
127 bit integer assigned to a node data
partitioning is done based on the token
of the nodes as described earlier in
this lesson starting from version 1.2 of
Cassandra V nodes are also assigned
tokens and this assignment is done
automatically so that the use of the
token generator tool is not required the
token generator is an interactive tool
which generates tokens for the topology
specified let us now look at an example
in which the token generator is run for
a cluster with two data centers
type token - generator on the command
line to run the tool a question is asked
next how many data centers will
participate in this cluster in the
example specified - as the number of
data centers and press Enter
next the question how many nodes are in
data center number one is asked type
five and press Enter the next question
is how many nodes are in data center
number two type 4 and press Enter
the tokens are calculated and displayed
on screen the example shows that token
numbers being generated for five nodes
in data center one and four nodes in
data center - the first node always has
the token value as zero these token
numbers will be copied to the Cassandra
gamal configuration file for each node
Cassandra is highly fault tolerant if a
node fails other nodes detect the node
failure request for data on that node is
routed to other nodes that have the
replica of that data rights are handled
by a temporary node until the node is
restarted any mem table or SS table data
that is lost is recovered from commit
log a node can be permanently removed
using the node tool utility when a disk
becomes corrupt Cassandra detects the
problem and takes corrective action the
data on the disk becomes inaccessible
and read of data from the node is not
possible this issue will be treated like
node failure for that portion of data
mem table and SS table will not be
affected as they are in memory tables
commit log has replicas and they will be
used for recovery
sometimes a rack could stop functioning
due to power failure or a network switch
problem as a consequence all the nodes
on the rack become inaccessible read of
data from the rack nodes is not possible
the reads will be routed to other
replicas of the data this will be
treated as if each node in the rack has
failed data center failure occurs when a
data center is shut down for maintenance
or when it fails due to natural
calamities when that happens all data in
the data center will become inaccessible
all reads have to be routed to other
data centers and the replica copies in
other data centers will be used though
the system will be operational clients
may notice slowdown due to network
latency this is because multiple data
centers are normally located at
physically different locations and
connected by a wide area network let us
summarize the topics covered in this
lesson cassandra has a ring type
architecture cassandra has no master
nodes and no single point of failure
cassandra supports network topology with
multiple data centers multiple racks and
nodes Cassandra read and write processes
ensure fast read and write of data
cassandra partitions the data in a
transparent way by using the hash value
of keys replication in Cassandra can be
done across data centers Cassandra uses
the gossip protocol for internode
communication Cassandra can handle node
disk rack or data center failures this
concludes the lesson Cassandra
architecture
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>