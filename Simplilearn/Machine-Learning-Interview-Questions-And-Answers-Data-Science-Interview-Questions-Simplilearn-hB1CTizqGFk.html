<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Interview Questions And Answers | Data Science Interview Questions | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Interview Questions And Answers | Data Science Interview Questions | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Interview Questions And Answers | Data Science Interview Questions | Simplilearn</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hB1CTizqGFk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone welcome to the session
I'm Mohan from simply learn and today
we'll talk about interview questions for
machine learning now this video will
probably help you when you're attending
interviews or machine learning positions
and attempt here is to probably
consolidate 30 most commonly asked
questions and to help you in answering
these questions we tried our best to
give you the best possible answers but
of course what is more important here is
rather than the theoretical knowledge
you need to kind of add to the answers
are supplementing your answers with your
own experience so the responses that we
put here are a bit more general in
nature so that there are some concepts
that you are not clear this video will
help you in kind of getting those
concepts cleared up as well but what is
more important is that you need to
supplement these responses with your own
practical experience okay so with that
let's get started so one of the first
questions that you may face is what are
the different types of machine learning
now what is the best way to respond to
this there are three types of machine
learning if you read any material you
will always be told there are three
types of machine learning but what is
important is you would probably very
matter of emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is a third
type which is reinforced mentally so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of key word that they
will be looking for which is labeled
imitator and so if you just say a stated
or historical data
the impact may not be so much you need
to emphasize on labeled data so what is
the label data basically let's say if
you are trying to do training
for classification you need to be aware
of for your existing data which class
each of the observations belong to right
so that is what is labeling so it is
nothing but a fancy name he must be
already well but just make it a point to
throw in that keyword labeled so that
will have the right impact okay so that
is what is supervised learning when you
have existing labeled data which you
then use to train your model that is
known as surprised learner and
unsupervised learning is when you don't
have this label data so you have data it
is not labeled so the system has to
figure out a way to do some analysis on
this so that is unsupervised learning
and you can then add a few things like
what are the ways of performing a
supervised learning and unsupervised
learning what are some of the techniques
so supervised learning P we perform what
we do regression and classification and
unsupervised learning we do clustering
and the string can be of different types
and only regression can be on different
types but you don't have to probably ill
operate so much they are asking for just
the different types you can just mention
these and just at a very high level
idiots but if they want you to elaborate
give examples that of course is nothing
there is a different question for that
let us see that later then the third so
we unsupervised and we have unsupervised
and then reinforcement you need to
provide a little bit of information that
is whether because it is sometimes a
little difficult to come up with a good
definition for reinforcement line so you
may have to little bit elaborate on how
the enforcement learning works so
reinforcement learning works in such a
way that it basically has two parts to
it one is the agent and environment and
the agent basically is working inside of
this environment and it is given a
target that it has to achieve and every
time it is moving in the direction of
the target so the agent basically has to
take some action and every time it takes
an action which is moving the agent
towards the target and towards a goal a
target is nothing but a cold
then it is rewarded and every time it is
going in a direction where it is away
from the goal then it is punished so
that is the way you can leave it
explained and this is used primarily or
it very very impactful or teaching the
system to learn games and so one
examples of this are basically used in
alphago you can throw that as an example
where alphago used reinforcement
learning to actually learn to play the
game of CO and finally it defeated Oh
what a champion this much of information
that would be good enough okay then
there could be a question on overfitting
so the question could be what is
overfitting and how can you avoid it so
what is overfitting let's first try to
understand the concept because sometimes
overfitting may be a little difficult
honest overfitting is a situation where
the model has kind of memorized the data
so this is an equivalent of memorizing
the data so we can draw an analogy so
that it becomes easy to explain this now
let's say you're teaching a child about
recognizing some fruits or something
like that okay and you're teaching this
child about recognizing let's say three
fruits apples oranges and pineapples so
this is a small child and for the first
time you're teaching the child to
recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you will take a basket of fruits
which consists of apples oranges and
pineapples and you take this basket to
this child and there may be let's say
hundreds of these fruits so you take
this basket to this child and keep
showing each of this root and then first
time obviously the child will not know
what it is so you show an apple and you
say hey this is Apple then you show
maybe an orange and say this is orange
and so on and then again you keep
repeating that until that basket is over
this is basically how training work in
machine learning also that's how
training works so till the basket is
completed maybe hundred fruits you keep
showing this child then
in the process what has happened the
child has pretty much memorized these so
even before you finish that basket right
by the time you're halfway through the
child has learned about recognizing the
Apple orange and find out now what will
happen after halfway through initially
you remember it made mistakes
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100% accurately it
will identify it will say the child will
say this is an apple an orange and if
you show a pineapple it will say this is
a pineapple so that means it has kind of
memorized this data now let's say you
bring another basket of fruits and it
will have a mix of maybe apples which
were already there in the previous set
but it will also have in addition to
Apple it will probably have a banana or
maybe another fruit like a jackfruit
right so this is an equivalent of your
test data set which the child has not
seen before some parts of it it probably
has seen like the apples it has seen but
this Anna and jackfruit it has not seen
so then what will happen if the first
round which is an equivalent of your
training leaders head towards the end it
has 100% it was telling you what the
fruits are and apple was accurately
recognized orange or I was accurately
recognised and pineapples were
accurately recognized right so that is
like a hundred percent accuracy but now
when you get another a fresh set which
were not out of the original one what
will happen all the apples maybe it will
be able to recognize correctly but all
the others like the jackfruit or the
banana will not be recognized by a child
so this is an analogy this is an
equivalent of overfitting so what has
happened during the training process it
is able to recognize or reach hundred
percent accuracy maybe very high
accuracy and we call that as very low
loss right so that is the technical term
so the loss is pretty much zero and
accuracy is pretty much hundred percent
whereas when you use testing there will
be a huge error which means the loss
will be pretty high and therefore the
accuracy will be also low okay this is
known as overfitting
this is basically a process where
training is done training processes
it goes very well almost reaching
hundred percent accuracy but when
testing it really drops down no how can
you avoid it so that is the extension of
this question there are multiple ways of
avoiding overfitting there are
techniques like what he called
regularization that is the most common
technique that is used for avoiding
overfitting and within regularization
there can be a few other subtypes like
dropout in case of neural networks and a
few other examples but I think if you
give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like questions
that will be more to stop you okay then
the next question is around the
methodology so when we are performing
machine learning training we split the
data into training and test right so
this question is around that so the
question is what is training satella set
in machine learning model and how is the
split done so the question can be so now
I should learning when we are trying to
train the model so we have a three step
process we train the model and then we
test the model and then once we are
satisfied with the test only then we
deployed so what happens in the train
and test is that you remember the
labeled data so let's say you have
thousand records with labeling
information now one way of doing it is
you use all the thousand records for
training and then may be right which
means that you have exposed all these
thousand records during the training
process and then you take a small set of
the same data and then you say okay I
will test it with this okay and then you
probably what will happen you may get
some good results all right but there is
a flaw there what is the flaw this is
very similar to human beings it is like
you are showing this model the entire
data as a part of training okay so
it has become familiar with the entire
data so when you are taking a part of
that again and you are saying that I
want to test it obviously you will get
good results so that is not a very
accurate way of testing so that is the
reason what we do is we have a label
data of this thousand records we set
aside before starting the training
process we set aside a portion of that
data and we call that test set and the
remaining we call as training set and we
use only this for training our model
know the training process remember is
not just about tossing one round of this
data set so let's say now your training
set has 800 records it is not just one
time you pass this 800 records what you
normally do is you actually as a part of
the training you may ask this data
through the model multiple time so this
thousand records may go through the
model maybe 10 15 20 times till the
training is perfect till the accuracy is
high till the errors are minimized
okay now so which is fine which means
that your that is what is known as the
model has seen your data and gets
familiar with your data and now when you
bring your test data what will happen is
this is like some new data because that
is where the real test is now you train
the model and now you are testing the
model with some data which is kind of
new that is like a situation let like a
realistic situation because when the
model is deployed that is what will
happen it will receive some new data not
the data that it is already seen right
so this is a realistic test so you put
some new data so this data which you're
set aside is for the model it is new and
if it is able to accurately predict the
values that means your training has
effect
okay the model got trained properly but
let's say while you're testing this with
this test data you are getting a lot of
errors that means you need to probably
either change your model or retrain with
more data and things like that now
coming back to the question of how do
you split this what should be the ratio
there is no fixed number again this is
like individual preferences some people
split it into 50 50 50 percent test and
50 percent training some people prefer
to have a larger amount
for training and a smaller amount for
tests so they can go by either 60/40 or
70/30 or some people even go with some
odd numbers like 65 35 or sixty three
point three three and that which is like
1/3 and 2/3 so there is no fixed rule
that it has to be something this ratio
has to be this you can go by your
individual preferences all right then
you may have questions around data
handling data manipulation what you call
data management
preparation so these are all some
questions around that area there is
again no one answer on single good
answer to this it really varies from
situation to situation and depending on
what exactly is the problem what kind of
data it is how critical it is what kind
of data is missing and what is the type
of corruption so the whole lot of things
this is a very general question and
differ you need to be little careful
about responding to this as well so
probably have to illustrate this again
if you have experience in doing this
kind of work in handling data you can
illustrate with example saying that I
was on one project where I received this
kind of data these were the columns
where data was not filled or these this
many rows where the data was missing
that would be in fact a perfect way to
respond to this question but if you
don't have that obviously you have to
provide some good answer I think it
really depends on what exactly the
situation and then there are multiple
ways of handling missing data or crap
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing and you have pretty much half of
your data having these missing values in
terms of number of rows okay that could
be one situation another situation could
be that you have records of eight are
missing but when you do some initial
calculation how many records are corrupt
or how many rows or observations as we
call it has this missing data let's
assume it is very minimal like a 10%
between these two cases how do so let's
assume that this is not a
mission-critical situation
and in order to fix this 10% of the data
the effort that is required is much
higher and obviously effort means also
time and money right so it is not so
mission-critical and it is okay to let's
say get rid of these records so
obviously one of the easiest ways of
handling the data part of missing data
is remove those records or remove those
observations from your analysis so that
is the easiest way to do but then the
downside is as I said in as in the first
case if it's a 50% of your data is like
that because some column of the other is
missing so it is not like every in every
place in every row the same column is
missing but you have in maybe 10% of the
records column one is missing another
10% column two is missing another 10%
column three is missing so on and so
forth so it adds up to maybe half of
your dataset so you cannot completely
remove half of your data set then the
whole purpose is lost okay so then how
do you hide then you need to come up
with ways of filling up this data with
some meaningful value and that is one
way of handling so when we say
meaningful value what is that meaningful
let's say for a particular column you
might want to take a mean value for that
column and fill wherever the data is
missing fill up with that mean my you so
that when you're doing the calculations
your analysis is not completely way off
so you have values which are not missing
first of all so your system will work
number two these values are not so
completely out of whack that your whole
analysis goes for a toss right there may
be situations where if the missing
values instead of putting mean may be a
good idea to fill it up with a minimum
value or with a zero so on with the
maximum value again as I said there are
so many possibilities so there is no
like one correct answer for this you
need to basically talk around this and
illustrate with your experience as I
said that would be the best otherwise
this is how you need to handle this
question okay so then the next question
can be how can you choose a classifier
based on a training set data size so
again this is one of those questions
where you probably do not have like one
size fits all
first of all you me not let's say decide
your classifier based on the training
set size maybe not the best way to
decide the type of the classifier and
even if you have to there are probably
some thumb rules which we can use but
then again every time so in my opinion
the best way to respond to this question
is you need to try out a few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just by if somebody defines a
problem to you and somebody even if
usually if they show the data to you or
tell you what is the data or even the
size of the data I don't think there is
a way to really say that yes this is the
classifier that will work here no that's
not the right way so you need to still
you know test it out get the data try
out a couple of classifiers and then
only you will be in a position to decide
which classifier to use you try out
multiple classifiers see which one gives
the best accuracy and only then you can
decide then you can have a question
around confusion matrix so the question
can be explained confusion matrix so
confusion matrix I think the best way to
explain it is by taking an example and
drawing like a small diagram otherwise
it can really become tricky so my
suggestion is to take a piece of pen and
paper and explain it by drawing a small
matrix and confusion matrix is about to
find out this is you especially in
classification learning process and when
you get the results and the model
predicts the results you compare it with
the actual value and try to find out
what is the accuracy so in this case
let's say this is an example of a
confusion matrix and it is a binary
matrix so you have the actual values
which is the labeled data and which is
so you have how many yes and how many no
so you have that efficient and you have
the predicted values how many yes and
how
so the total actual values the total yes
is 12 plus 113 and they are shown yeah
and the actual you knows are 9 + 3 12
okay so that is what this information
here is so this is about the actual and
this is about the predicted similarly
the predicted values there are yes are
12 plus 3 15 yeses and no r1 plus 9 n
loss okay so this is the way to look at
this confusion matrix and out of this
what is the meaning conveyed so there
are two or three things that needs to be
explained out right the first thing is
for a model to be accurate the values
across the diagonals should be high like
in this case right that is one number
two the total sum of these values is
equal to the total observations in the
test data set so in this case for
example you have 12 plus 3 15 plus 10 25
so that means we have 25 observations in
our test data set okay so these are the
two things you need to first explain
that the total sum in this matrix with
the numbers is equal to the size of the
test data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this an accurate order is
the model being acted if they're all
spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy so it is a
very simple mathematical calculation you
take some of the diagonals right so in
this case it is 9 plus 21 and divide it
by the total so in this case what will
it be let's be take a pen so your your
diagonal values is equal to if I say T
is equal to 12 plus 9 so that is 21
right
and the total data set is equal to like
we just calculated it is 25 so what is
your accuracy it is 21 by your accuracy
is equal to 21 by 24 and this turns out
to be about 85 percent right so this is
85 percent so that is
okay so this is the way you need to
explain draw diagram you can example
maybe it may be a good idea to be
prepared with an example so that it
becomes easy for you you don't have to
calculate those numbers on the fly right
so a couple of hints are that you take
some numbers which are which add up to
hundred that is always a good idea so
you don't have to really do this flex
calculations so the total value will be
100 and then diagonals I use a divide
once you find the diagonal edges that is
equal to your percentage okay all right
so the next question can be a related
question about false positive and false
negative so what is false positive and
what is false negative now once again
the best way to explain this is using a
piece of paper and pen otherwise it will
be pretty difficult to explain so we use
the same example of the confusion matrix
and we can explain that so a confusion
matrix looks somewhat like this and when
we just it looks more like this and we
continue with the previous example where
this is the actual value this is the
predicted value and in the actual value
we have 12 plus 1 13 yeses and 3 plus 9
12 knows and the predicted values there
are 12 plus say 15 yeses and 1 plus 9 10
nos okay now in this particular case
which is the false positive what is a
false positive first of all the second
word which is positive okay is referring
to the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive that is the way you should
understand this term false positive or
even false negative so false positive so
positive is what your system as
predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay now if you consider this row so
this is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actually
you is negative predictive value is
positive but the actual value is
negative so this is a false positive
right and here is a true positive so the
predicted value is positive and the
actual value is also positive okay I
hope this is making sense now let's take
a look at what is false negative false
negative so negative is the second term
that means that is the predicted value
that we need to look for so which are
the predicted negative values this row
corresponds to predicted negative values
all right so this row responds to
predicted negative values and what they
are asking for false so this is the row
for predicted negative values and the
actual value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is a way to look at
false positive and false negative same
way it can be true positive and two
negative as well so again positive the
second term you will need to use to
identify the predicted role right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positives
true in case of actual as yes right so
true positive is this one okay and then
in case of actual the negative we know
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative and nine is true
negative the actual value is also
negative and the predicted value is also
in it okay so that is the way you need
to explain this terms false positive
false negative and true positive truly
then you might have a question like what
are the steps involved in the machine
learning process or what are the three
steps in the process of developing
machine learning model right so it is
around methodology that is applied so
basically the way you can probably
answer in your own words but debate
model development of the machine
learning model what happens is like this
so first of all you try to understand
the problem and try to figure out
whether it is a classification problem
on a regression problem based on that
you select a few algorithms and then you
start the process of claiming these
models okay so you can either do that or
you can after due diligence you can
probably decide that there is one
particular algorithm that which is most
suitable usually it happens through
trial and error process but but at some
point you will decide that okay this is
the model we are going to use so in that
case we have the model algorithm the
model decided and then you need to do
the process of training the model and
testing the model and this is where if
it is supervised learning you split your
data the label data into training data
set and test data set and you use the
training data set to train your model
and then you use the test data set to
check the accuracy whether it is working
fine or not so you test the model before
you actually put it into production
right so once you test the model you're
satisfied it's working fine then you go
to the next level which is putting it
for production and then in production
obviously new data will come and the
inference circles so the model is
readily available and only thing that
happens is new data comes and the model
predicts devise regression
classification no so this can be a
nitrate of process so it is not a
straightforward process where you do the
training so the testing and then you
move it to production now so during the
training and test process there may be a
situation where because of either
overfitting or things like that the test
doesn't go through which means that you
need to put that back into the training
process so that can be a nitrate of
process not only that even if the
training in test goes through properly
and you deploy the model in production
there can be a situation that a data
that actually counts the real data that
comes but that this model is failing so
in which case you may have to once again
go back to the drawing board or
initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again
Tracy will deteriorate so that is again
a recursive process so once in a while
you need to keep checking whether the
model is working fine or not and if
required you need to tweak it and modify
it and so on and so forth so that this
is a continuous process of tweaking the
model and testing it and making sure it
is up to date then you might have
question around deep learning so because
deep learning is now associated with the
AI artificial intelligence and so on so
can be as simple as what is deep
learning so I think the best way to
respond to this could be deep learning
is a part of machine learning and then
obviously though the question would be
then what is the difference right so
deep learning you need to mention there
are two key parts that interviewer will
be looking for when you are defining
deep learning so first is of course if
learning is a subset of machine learning
so machine learning is still the bigger
let's say scope and deep learning is one
so then what exactly is the difference
deep learning is primarily when we are
implementing these our algorithms or
when we are using neural networks for
doing our training and classification
and regression right so when we use
neural network then it is considered as
a deep learning and the term deep comes
from the fact that you can have several
layers of neural networks and these are
called deep neural networks and
therefore that deep deep learning the
other difference between machine
learning and deep learning which they
together may be wanting to hear is that
in case of machine learning the feature
engineering is done manually what do we
mean by feature engineering basically
when we are trying to train our model we
have our training data right so we have
our training label data and this data
has several let's say if it is a regular
table it has several columns now each of
these columns actually has information
about a feature right so if we are
trying to predict height weight and so
on and so forth so these are all
features of human beings let's say we
have census data and we are all so those
are the features now that may be
probably 50 or 100
in some cases there may be 100 such
features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course you have
accuracy will probably get affected but
also there is a computational part so if
you have so many features and then you
have so much data it becomes very tricky
so in case of machine learning we
manually take care of identifying the
features that do not contribute to the
learning process and thereby we
eliminate those features and so on so
this is known as feature engineering and
in machine learning we do that anyway
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which do not use and
therefore feature engineering is also
done automatically so this is
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and it needs only a small amount of data
for training and then works well on
low-end system so you don't need large
machines and most features need to be
identified in advance and manually code
it so basically the feature engineering
part is done manually and the problem is
divided in two parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural networks
so here in deep learning we use neural
lines so that is the key differentiator
between machine learning and deep
learning and usually if learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines because it needs a lot
of computing power and the machine
learning features are the lot of
Engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handle end to end
so this is good comparison between
machine learning I'm learning in case
you have that kind of question then you
might get a question around the uses of
machine learning or some real-life
applications of machine learning in
modern business the question may be
worded in different ways but the meaning
is how exactly is machine learning used
or actually supervised to machine
learning it could be a very specific
question around supervised machine
learning so this is like give examples
of supervised machine learning use of
supervised machine learning in modern
business so that could be the next
question so there are quite a few
examples on quite a few use cases well
for supervised machine learning the very
common one is email spam detection so
you want to train your application or
your system to detect between spam and
non-spam so this is a very common
business application of a supervised
machine learning so how does this work
the way it works is that you obviously
have historical data above your emails
and they are categorized as and not spam
so that is what is the label information
and then you feed this information or
the all these emails as an input to your
model right and the model will then get
trained to detect which of the emails
are to detect which is spam and which is
not spam so that is the training process
and this is supervised machine learning
because you have label data you already
have emails which are tagged as spam or
not spam and then you use that to train
your model so this is one example now
there are a few industry specific
applications for supervised machine
learning one of the very common ones is
in healthcare Diagnostics in health care
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person
music or not whether a person has cancer
or not right so this is a very good
example of supervised machine learning
here the way it works is that existing
images it could be x-ray images it be
MRI or any of these images are available
and they are taxing that okay this x-ray
image is defective or the person has an
illness or it could be cancer
whichever illness right so they test
attacked as effective or clear or good
image and defective something like that
so we come up with a binary or it could
be multi class as well saying that this
is defective to ten percent this is
twenty four percent and so on but let's
keep it simple you can give an example
of just a binary classification that
would be good enough so you can say that
in healthcare diagnostics using image we
need to detect whether a person is ill
or whether a person is having cancer or
not so here the way it works is you feed
labeled images and you allow the models
to learn from that so that when new
image is fed it will be able to predict
whether this person is having that
illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business alright then
we can have a question like so we've
been talking about supervised
unsupervised then so there can be a
question around semi-supervised machine
learning so what is a supervised machine
I know semi-supervised learning as the
names such as it falls between
supervised learning and unsupervised
learning but for all practical purposes
it is considered as a part of supervised
learning and the reason this has come
into existence is that in supervised
learning you need labeled data so all
your data or training your model has to
be labeled now this is a big problem in
many industries or in many other many
situations getting the label data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of diagnostic images we can
just let's say take x-ray images now
there are actually million
of x-ray images available all over the
world but the problem is they are not
labeled so the images are there but
whether it is effective or whether it is
good and information is not available
along with that ID in a form that it can
be used by a machine which means that
somebody has to take a look at these
images and usually it should be like a
top top and then say that okay yes this
major screen and this image is cancerous
and so on and so forth now that is a
huge effort by itself so this is where
semi-supervised learning comes into play
so what happens is there is a large
amount of data may be a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we drain our model so I know this
is a little long whining explanation but
unfortunately there is no quick and easy
definition for semi-supervised machine
learnings is the only way properly to
explain this concept we may have another
question as what are unsupervised
machine learning techniques or what are
some of the techniques used for
performing unsupervised machine learning
so it can be worded in different ways so
how do we answer this so unsupervised
learning you can say that there are
types clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects or their characteristics can be
measured and if they have most of the
characteristics if they are similar then
they can be put together this is a story
then Association you can I think the
best way to explain Association is with
an example in case of Association you
try to find out how the items are linked
to each other so for example if somebody
bought maybe a laptop the person has
also purchased mouse so this is more
than a ecommerce scenario for example so
you can give this as an example so
people who are buying laptops are also
buying house so that means there is an
between laptops and mouse or maybe
people who are buying bread are also
buying butter so that is a Association
that can be created so this is
unsupervised learning techniques okay
all right then we have a fundamental
question what is the difference between
supervised and unsupervised machine
learning so machine learning these are
the two main types of machine learning
supervised done unsurfaced and in case
of supervised and again here probably
the key word that person may be wanting
to hear is labelled data now very often
people say we have historical data and
if we run it it is supervised and if we
don't harvest Oracle data yes but you
may have historical data but if it is
not labeled and you cannot use it for
supervised learning so it is it's very
key to understand that we put in that
keyword label so when we have labeled
data for training our model then we can
use supervised learning and if we do not
have label data and we use unsupervised
learning and there are different
algorithms available to perform both of
this types of trainings so they can be
another question a little bit more
theoretical and conceptual in nature
this is about inductive machine learning
deductive machine right so the question
can be what is the difference between
inductive machine learning and deductr
machine learning or somewhat the exact
phrase or exact question they can ask
for examples and things and but that
could be so let's first understand what
is inductive and deductive trading
inductive training is induced by
somebody and you can illustrate that
with the small example I think that
always have so whenever you are doing
some explanation try as much as possible
as I said to give examples from your
work experience or give some analogies
and that will also help a lot in
explaining as well and interviewer also
- so here we'll take an example or
rather we will use an analogy so
inductive training is when we use some
knowledge or the learning
we thought the person actually
experiencing okay what can be an example
so we can probably tell the person or
show a person of video that fire can
burn ving burn his finger or fire can
cause damage so what is happening here
this person has never probably seen a
fire or never seen anything getting
damaged by fire but just because he has
seen this video
he knows that okay fire is dangerous and
if fire can cause damage right so this
is inductive learning compared to that
what is deductive learning so here you
draw a conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video let's assume a person is
allowed to play with fire and then he
figures out that if he puts his finger
it's burning or you throw something into
the fire it burns so he is learning
through experience so this is known as
deductive learning okay so you can have
applications or models that can be
trained using inductive learning all
right I think probably that explanation
will be sufficient
the next question is are KNN and k-means
clustering similar to one another or are
the same right because the letter K is
kind of common between them so let us
take a little while to understand what
these two are one is KN in another risk
K means a cane and stands for K nearest
neighbors and K means of course is
clustering mechanism oh these two are
completely different except for the
letter K being common between them cane
and his completely different k-means
clustering is completely different k MN
is a classification process and
therefore it comes under supervised
learning
whereas k-means clustering is actually
unsupervised when you have KN n when you
want to implement KN n which is
basically K nearest neighbors the value
of K is a number so you can say K is
equal to three you want to implement KN
n with K is equal to three so which
means
that it performs the classification in
such a way that how does it perform the
classification so it will take three
nearest objects and that's why it's
called nearest neighbor so basically
based on the distance it will try to
find out its nearest objects that are
like three of the nearest objects and
then it will check whether the class
they belong to which class right so if
all three belong to one particular class
obviously this new object is also
classified as that particular class but
it is possible that they may be from two
or three different classes okay so let's
say they are from two classes and then
if they're from two classes now usually
you take our odd number your sign odd
number to say if there are three of them
and two of them belong to one class and
then one belongs to another class so
this new object is assigned to the class
to which the two of them belong now the
value of K is sometimes tricky whether
should you use these should you use five
should you use seven it can be tricky
because the ultimate classification can
also vary so it's possible that if you
are taking K as three the object is
probably in one particular class but if
you take K is equal to five
maybe the object will belong to a
different class because when you are
taking three of them probably two of
them belong to a class one and one
belong to class two whereas when you
take five of them it is possible that
only two of them belong to class one and
the three of them belong to class two so
which means that this object will be on
class two and so you see that so it is
the class allocation can vary depending
on the value of K now K means on the
other hand is a clustering process and
it is unsupervised where what it does is
the system will basically identify other
objects are how close the objects are
with respect to some of their features
okay and but the similarity of course is
the letter K and in case of K means also
we specify its value and it could be
three or five or seven there is no
technical limit as such but it can be
any number of clusters that you can
create okay so based on the value that
you provide the system will create that
many clusters of similar objects
so there is a similarity to that extent
that K is a number in both the cases but
actually these two are completely
different processes we have what is
known as naive Bayes classifier and
people often get confused thinking that
naive Bayes is the name of the person
who found this classifier or it will
develop this classifier which is not
100% true bass is the name of the person
ba ye s is the name of the person but
naive is not a name of the person right
so naive is basically an English word
and that has been added here because of
the nature of his particular classifier
I Bayes classifier is probability based
classifier and it makes some assumptions
that presence of one feature of a class
is not related to the presence of any
other feature of maybe other classes
right so which is not very strong or not
a very what you say accurate assumption
because these features can be related
and so on but even if we go with this
assumption this whole algorithm works
very well even with this assumption and
that is the good side of it but the term
comes from there so that is the
explanation that you can then there can
be question around reinforcement
learning it can be paraphrased in
multiple ways one could be can you
explain how a system can play a game of
chess using reinforcement learning or it
can be any game so the best way to
explain this is again to talk a little
bit about what reinforcement learning is
about and then elaborate on that to
explain the process so first of all
reinforcement learning has an
environment an agent and agent is
basically performing some actions in
order to achieve a certain goal and this
goals can be anything either it is
related to game and then the goal could
be that you have to score very high
score or it could be that your number of
lives should be as high as possible
don't lose life so this could be some of
them more advanced examples could be for
driving without emotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the
how to navigate through the roads and so
on and so forth that is also another
example now how does it work so if the
system is basically there is an agent in
the environment and every time the agent
takes a step or performs a task which is
taking it towards the goal the final
goal let's say to maximize the score or
to minimize the number of lives it is
rewarded and every time it takes a step
which goes against that goal and
contrary or the reverse direction it is
penalized okay so it is like a carrot
and a stick system now how do you use
this to create a game of chess or to
create a system to play a game of chess
now the way this works is and this could
probably go back to this alphago
examples where alphago defeated the
human champion so the way it works is in
reinforcement learning the system is
allowed for example in this case we are
talking about chess so we allow the
system to first of all watch playing a
game of chess so it could be with a
human being or it could be the system
itself there are computer games of chess
right so either this new learning system
has to watch that game or watch a human
being play the game because this is
reinforcement learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns
okay so reinforcement learning to a
large extent works so you need to create
a mechanism whereby your model will be
able to watch somebody playing the game
and then you allow the system also to
start playing the game so it pretty much
starts from scratch okay and as it moves
forward it it it's like right at the
beginning and the system really knows
nothing about the game of chess okay so
initially it is a clean slate it just
starts by observing how you're playing
so it will make some random moves and
keep losing
badly but then what happens is over a
period of time so you need to now allow
the system you need to play with the
system not just one two three four or
five times but hundreds of times
thousands of times maybe even hundreds
of thousands of times and that's exactly
how our NGO has done it played millions
of games between itself and the system
right so for the game of chess also you
need to do something like that you need
to allow the system to play chess and
and learn on its own over a period of
repetition so I think you can probably
explain it to this much to this extent
it should be sufficient now this is
another question which is again somewhat
similar but here the size is not coming
into picture so the question is how will
you know which machine learning
algorithm to choose or your
classification problem now this is not
only classification problem it could be
a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I am going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have SVM you have
logistic regression is it possible to
outright say yes for this particular
problem since you have explained us now
this is the exact algorithm that you can
use that is not possible okay
so you have to try out a bunch of
algorithms see which one gives us the
best performance best accuracy and then
decide to go with so in machine learning
a lot of it happens through trial and
error there is no real possibility that
anybody
can just by looking at the problem or
understanding the problem tell you that
okay in this particular situation this
is exactly the algorithm that you should
use then the questions may be around
application of machine learning and this
question is specifically around how
Amazon is able to recommend other things
to buy so this is a round recommendation
engine how does it work how does the
recommendation engine work so this is
basically the question is all about so
the recommendation engine again works
based on various inputs that are
provided obviously something like you
know Amazon on a website or e-commerce
site like Amazon collects a lot of data
around customer behavior who is
purchasing what and if somebody is
buying a particular thing they are also
buying something else so this kind of
Association right so this is the
unsupervised learning we talked about
they use this to associate and link or
relate items and that is one part of it
so they kind of build association
between items saying that somebody
buying this is also buying as that is
one part of it then they also profile
the users right based on their age
gender their geographic location they
will do some profiling and then when
somebody is logging in and when somebody
is shopping kind of the mapping of these
two things are done they try to identify
obviously if you have logged in then
they know who you are and your
information is available like for
example your age maybe your gender and
where you are located what you purchased
earlier right so all this is taken and
the recommendation engine basically uses
all this information and comes up with
recommendations for a particular user so
that is how recommendation engine
alright then the question can be
something very basic like when will you
go for classification versus regression
and when do you do classification
instead of regression or when do you use
classification instead of regression oh
yes so this is basically going back to
the understanding of the basics of
classification and regression so
classification is used when you
to identify or categorize things into
discreet classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression the use of course there will
be some keywords that they will be
looking for so it's just you need to
make sure you use those keywords one is
the discrete values
another is the continuous values so for
regression if we are trying to find some
continuous values you use regression
whereas if you are trying to find some
bisquit values you use classification
and then you need to illustrate what are
some of the examples classification is
like let's say there are images and you
need to put them into classes like cat
dog elephant Tiger something like that
so that is the classification or it can
be that is a multi-class classification
problem it could be binary
classification problem like for example
other a customer will buy or he will not
buy that is a classification binary
classification it can be in the weather
forecast area now weather forecast is
again combination of regression and
classification because on the one hand
you want to predict whether it's going
to rain or not that's a classification
problem it's a binary classification
right whether it's going to rain or not
however you also have to predict what is
going to be the temperature tomorrow
right now temperature is a continuous
value you can't answer the temperature
in a yes or no kind of a response right
so what will be the temperature tomorrow
so you need to give a number which can
be like 20 degrees 30 degrees right so
that is where you use regression one
more example is stock price prediction
so that is where again you will use
regression so these are the various
examples so you need to illustrate with
examples and make sure you include those
keywords like discrete and continuous so
the next question is more about a little
bit of a design related question to
understand your concepts and things like
that so it is how will you design a spam
filter so how do you basically design of
a spam filter so I think the main thing
here is he is looking at probably
understand
your concept in terms of what is the
algorithm you will use or what is your
understanding about difference between
classification and regression and things
like that I'd end up process of course
the matter to lossy and the process so
the best way to go about responding to
this is we say that okay this is a
classification problem because we want
to find out whether an email is a or not
spam so that we can apply the filter so
first thing is to identify what type of
a problem it is so we have identified
that it is a classification then the
second step may be to find out what kind
of algorithm to use now since this is a
binary classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support vector machines for
example SVA so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and then
you spread that into training and test
data sets you use your training data set
to train your model that are your
algorithm that you have used the model
actually so and you actually will have
three models let's say you are trying to
test out three algorithms so you will
obviously have three models so you need
to try all three models and test them
out and see which one gives the best
accuracy and then you decide that you
will go with that
okay so training in tests will be done
and then you zero in on one particular
model and then you say okay this is the
model will be use we will use and then
go ahead and implement that or put that
in production so that is the way you
design a plan the next question is about
random forests what is random forests so
this is a very straightforward question
however the response you need to be
again a little careful why we all know
what is random forests explaining this
can sometimes be so one thing is random
is kind of in one way it is an extension
of decision trees because it is
basically nothing but you have multiple
decision trees and trees will basically
you will use for doing if it is
classification mostly it is
classification you will use the trees
for classification and then you use
voting for finding that the final class
so that is the underlying but how will
you explain this how will you respond
yes so first thing obviously you will
say that random forest is one of the
algorithms and the more important thing
that you need to probably the
interviewer is is waiting to hear is
ensemble learner right so this is one
type of ensemble learn what is ensemble
than ensembles learner is like a
combination of algorithms so it is a
learner which consists of more than one
algorithm or more than one may be models
so in case of random forest algorithm is
the same but instead of using one
instance of it we use multiple instances
of it and we use in a way that is a
random forest is an ensemble and there
are other types of ensemble learners
where we have like we use different
algorithms itself so you have one may be
logistic regression and the decision
tree going to be other and so on and so
forth or there are other ways like for
example splitting the data in a certain
way and so on so it's all about ensemble
we will define random forests except I
think the interview will be happy to
hear this word ensemble and so then you
go and explain how the random forest
works so if the random forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work I'd say your random
forest consists of hundred kriesky
and each observation you pass through
this forest and each observation let's
say it is a classification problem
binary classification 0 or 1 and you
have hundred trees now if 90 trees say
that it is a zero and ten of the trees
say it is a 1 you take the majority you
may take a word and since 90 or them are
saying 0 you classify this as 0 then you
take the next observation and so on so
that is the way random forest works for
classification if it is the regression
problem it's somewhat similar but only
thing
instead of what what we will do is or
regression remember what happens you
actually calculate a value right so for
example you're using regression to
predictor temperature and you have 100
trees and each key obviously will
probably predict a different value of
the temperature they may be close to
each other but they may not be exactly
the same so these hundred trees so how
do you not find the actual value at the
output for the entire forest right so
you have outputs of individual trees
which are a part of this forest but then
you need to find the final output of the
forest Excel so how do you do that so in
case of regression you take like an
average or the mean of all the hundred
trees right so this is also a way of
reducing the error so maybe if you have
only one tree and if that one tree makes
a header it is basically 100 percent
wrong or not it was a try but if you
have on the other hand if you have a
bunch of piece you are basically
medicating that recusing okay so that is
the way random forest works so the next
question is considering the long list of
machine learning algorithms how will you
decide on which one to use so once again
here there is no way to outright say
that this is the algorithm that we will
use for a given data set this is a very
good question but then the response has
to be like again there will not be a one
size fits all so we need to first of all
you can probably shorten the list in
terms of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification element all
right so for example if it is a class
session you cannot use linear regression
algorithm or if it is the regression
problem you cannot use SVM or repeat now
you can use this VM but maybe a logistic
regression right so to that extent you
can probably shorten the list but still
you will not be able to 100% decide on
saying that this is the exact algorithm
that I am going to use so the way to go
about is you choose a few algorithms
based on what the problem
yes you try out your data you train some
models of these algorithms check which
one gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm okay
all right then they can be questions
around bias and variance so the question
can be what is bias and variance in
machine learning so you just need to
give out definition for each of these
for example bias and machine learning it
occurs when the predicted values are far
away from the actual line so that is the
bias okay and whereas they are all all
the values are probably they are far off
but they are very near to each other
though the predicted values are close to
each other right well they're far off
from the actual value but they are close
to each other you see the difference so
that is bias and then the other part is
your variance of variance is when the
predicted values are all over the place
and so the variance is high that means
it may be close to the target but it is
kind of very scattered so the points the
predicted values are not close to each
other right in case of bias the
predicted values are close to each other
but they are not close to the target
but here they may be close to the target
but they may not be close to each other
so they're a little bit more scattered
so that is what in case of some variance
okay then the next question is about
again related to bias and variance what
is the trade-off between bias and
variance yes I think this is an
interesting question because these two
are heading in different directions so
for example if you try to minimize the
bias variance will keep going high and
if you try to minimize the variance bias
will keep going high and there is no way
you can minimize both of them so you
need to have a trade-off saying that
okay this is the level at which I will
have my bias and this is the level at
which I will have variance so the
trade-off is that pretty much that you
decide what is the level you will
alright for your bias and what is the
level you will alright for variance and
a combination of these two in such a way
that your final results are not payoff
and having a trade-off will ensure
that the results are consistent right so
that is basically the output is
consistent and which means that they are
close to each other and they are also
accurate but that means they are as
close to the target as possible right so
if either of these is high then one of
them will go off the track define
precision and recall now again here I
think it would be best to draw a diagram
and take the confusion matrix and it is
very simple the definition is like a
formula your precision is true positive
by two positive plus false positive and
your recall is true positive by true
positive plus false negative okay so
that's you can just show it in a
mathematical way that's pretty much you
know that can be shown that's the
easiest way to define so the next
question can be about decision tree what
is decision tree pruning and why is it
so basically decision trees are really
simple to implement and understand but
one of the drawbacks of decision trees
is that it can become highly complicated
as it grows and the rules and conditions
can become very complicated and this can
also lead to overfitting
which is basically that during training
you will get hundred percent accuracy
but when you're doing testing you'll get
a lot of errors so that is the reason
pruning needs to be done so the purpose
or the reason for doing a decision tree
pruning is to reduce overfitting or to
cut all an orphan and what is decision
tree pruning it is basically that you
reduce the number of branches because as
you may be our tree consists of the root
node and then there are several internal
nodes and then you have the leaf nodes
now if there are too many of these
internal thoughts that is when you face
the problem of overfitting and pruning
is the process of reducing those
internal nodes all right
so the next question can be what is
logistic regression so basically
logistic regression is one of the
eggs used for performing classification
especially binary classification now
there is something special about
logistic regression and there are a
couple of things you need to be careful
about first of all the name is a little
confusing it is called logistic
regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that and they can also ask this
like a trick question right so that is
one part second thing is the term
logistic has nothing to do with the
usual logistics that we talked about but
it is derived from log so that the
mathematical derivation was log and
therefore the name logistic regression
so what is logistic regression and how
is it you so logistic regression is used
for binary classification and output of
logistic regression is either a 0 or a 1
and it varies so it's basically it
calculates a probability between 0 and 1
and we can set a threshold that can vary
typically it is 0.5 so any value above
0.5 is considered as 1 if the
probability is below 0.5 it is
considered as zero so that is the way we
calculate the probability of the system
calculates the probability and based on
the threshold x x a value of 0 or 1
which is like a binary classification so
you are okay then we have a question
around K nearest neighbor algorithm
so explain K nearest neighbor so first
of all what is the K nearest neighbor
algorithm this is a classification
algorithm so that is the first thing we
need to mention and we also need to
mention that the K is a number it is an
integer and this is variable and we can
define what the value of K should be it
can be 2 3 5 7 and usually it is an odd
number so that is something we need to
mention technically it can be even
number also but then typically it would
be odd number and we will see why that
is okay so based on that we need to
classify objects ok we need to classify
objects so again
it will be very helpful to draw a
diagram you know if you're explaining I
think that will be the best way so draw
some diagram like this and let's say we
have three clusters or three classes
existing and now you want to find for a
new item that has come you want to find
out which class this belongs to right so
you go about as the name suggests if you
go about finding the nearest neighbors
and the points which are closest to this
and how many of them you will find that
is what is defined by K now let's say
our initial value of K was 5 so you will
find the K the five nearest data points
so in this case as it is illustrated
these are the five nearest data points
but then all five do not belong to the
same class or cluster so there are a 1
belonging to this cluster 1 the second
one belonging to this cluster 2 3 of
them belonging to this third cluster so
how do you decide that's exactly the
reason we should as much as possible try
to assign our odd number so that it
becomes easier to assign us so in this
case you see that the majority actually
if there are multiple classes then you
go with the majority so since three of
these items belong to this class we
assign which is basically the in in this
case the green or the tennis or the
third cluster as I was talking about
right so we assign it to this third
class so in this case it is that's how
it is decided ok so K nearest neighbor
so first thing is to identify the number
of neighbors that are mentioned as K so
in this case it is K is equal to 5 so we
find the five nearest points and then
find out out of these 5 which class has
the maximum number in that kid and and
then the new data point is assigned to
that class k so that's pretty much how K
nearest neighbors work all right so that
brings us to the end of this module I
hope you enjoyed this interview
questions and hope you will be able to
crack your next interview if any of
these questions come up and in case you
have other questions that you would like
to be answered please feel free to put
them down below this video in the
comment section and
try to maybe compile all of them and
create a new video or don't I'm okay
with that I'd like to sign off thank you
very much have a good day
hi there if you like this video
subscribe to the simple learn YouTube
channel and click here to watch similar
videos sinnard up and get certified
click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>