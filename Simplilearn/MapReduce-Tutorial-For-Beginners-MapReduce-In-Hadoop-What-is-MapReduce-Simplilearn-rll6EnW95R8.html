<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MapReduce Tutorial For Beginners | MapReduce In Hadoop | What is MapReduce | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="MapReduce Tutorial For Beginners | MapReduce In Hadoop | What is MapReduce | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MapReduce Tutorial For Beginners | MapReduce In Hadoop | What is MapReduce | Simplilearn</b></h2><h5 class="post__date">2017-07-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rll6EnW95R8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">after completing this lesson you will be
able to define MapReduce explain the
characteristics of MapReduce understand
advanced MapReduce define scoop and its
uses analyze importing and exporting of
data from Hadoop using scoop understand
the basics of scoop
- why MapReduce prior to 2004 huge
amounts of data was stored on single
server if any program ran a query for
data stored on multiple servers logical
integration of search results and
analysis of data was a nightmare not to
mention the massive efforts and expenses
that were involved the threat of data
loss challenge of data backup and
reduced scalability resulted in the
issue snowballing into a crisis of sorts
to counter this Google introduced
MapReduce in December 2004 and the
analysis of data sets were done in less
than 10 minutes rather than eight to ten
days queries could run simultaneously on
multiple servers and now logically
integrate search results and analyze
data in real time the USPS of MapReduce
are its fault tolerance and scalability
what is MapReduce MapReduce is a
programming model that processes and
analyzes huge data sets logically into
separate clusters while map sorts the
data reduce segregates the data into
logical clusters thus removing bad data
and retaining necessary information new
tree is a well-known courier facility it
transports documents across the globe
when the ception staff receive a courier
they color coded based on the country to
which it has to be sent The Dispatch
staff then segregate the courier by the
tagged color code hence the reception
functions as map and the dispatched team
has reduced
the Map Reduce steps listed here
represent manual vote counting after an
election as an analogy step 1 each poll
booths ballot papers are counted by a
teller this is a pre MapReduce step
called input splitting step 2
tellers of all booths count the ballot
papers in parallel as multiple tellers
working on a single job the job
execution time will be faster this is
called the map method step 3 the ballot
count of each booth under the assembly
and Parliament seats position is found
and the total count for the candidates
is generated this is known as reduce
method
thus map and reduce help to execute the
job quicker than an individual counter
as you've seen in the previous screen
analogy of vote counting is an example
to understand the use of MapReduce the
key reason to perform mapping and
reducing is to speed up job execution of
a specific process this can be done by
splitting a process into a number of
tasks thus enabling parallelism if one
person counts all the ballot papers and
waits for others to finish the ballot
count it could take a month to receive
the election results when many people
count the ballots simultaneously the
results are obtained in one or two days
this is how MapReduce works in this
screen the MapReduce operation is
explained using a real time problem the
job is to perform a word count on the
given paragraph the sentence is this
quick brown fox jumps over a lazy dog a
dog is a man's best friend the MapReduce
process consists of inputting splitting
mapping shuffling and reducing phases
the input phase refers to providing data
for which the MapReduce process is to be
performed the sentence is used as the
input here the splitting phase refers to
converting a job submitted by the client
into a number of tasks
in this example the job is to split into
two tasks the mapping phase refers to
generating a key value pair for the
input since this example is about
counting words the sentence is split
into words using the substring method to
generate words from lines the mapping
phase will ensure that the words
generated are converted into keys and a
default value of 1 is allotted to each
key the shuffling phase refers to
sorting the data based on the keys as
shown on the screen the words are sorted
in ascending order the last phase is the
reducing phase in this phase the data is
reduced based on the repeated keys by
incrementing the value the word dog and
the letter a are repeated therefore the
reducer will delete the key and increase
the value depending on the number of
occurrences of this key this is how the
MapReduce operation is performed map
execution consists of five phases map
phase partition phase shuffle phase sort
phase and reduce phase in the map phase
the assigned input split is read from
HDFS where a split could be a file block
by default furthermore input is parsed
into records as key value pairs the map
function is applied to each record to
return zero or more new records these
intermediate outputs are stored in the
local file system as a file they are
sorted first by bucket number and then
by a key at the end of the map phase
information is sent to the master node
after its completion in the partition
phase each mapper must determine which
reducer will receive each output for any
key regardless of which map or instance
generated it the destination partition
is the same note that the number of
partitions will be equal to the number
of reducers in the shuffle phase input
data
fetched from all map tasks for the
portion corresponding to the reduced
tasks bucket in the sort phase a merge
sort of all map outputs occurs in a
single run in the reduce phase a
user-defined reduce function is applied
to the merged run the arguments are a
key and the corresponding list of values
the output is written to a file in HDFS
the mappers on each of the nodes are
assigned to each input split of blocks
based on the input format the record
reader reads the split as a key value
pair the map function is applied to each
record to return 0 or more new records
these intermediate outputs are stored in
the file system as a file thereafter the
partitioner assigns the records to a
reducer in the shuffling phase the
intermediate key value pairs are
exchanged by all nodes the key value
pairs are then sorted by applying the
key and reduce function the output is
stored in HDFS based on the specified
output format the essentials of each
MapReduce phase are shown on the screen
the job input is specified in key value
pairs each job consists of two stages
first a user-defined map function is
applied to each input record to produce
a list of intermediate key value pairs
second a user-defined reduced function
is called once for each distinct key in
the map output then the list of
intermediate values associated with that
key is passed the number of reduced
tasks can be defined by the users each
reduced task is assigned to a set of
record groups which are intermediate
records corresponding to a group of keys
for each group a user-defined reduced
function is applied to the recorded
values the reduced tasks read from every
map task
each read returns the record groups for
that reduced task
note that the reduce phase cannot start
until all mappers have finished
processing a job is a full MapReduce
program which typically causes multiple
map and reduce functions to be run in
parallel over the life of the program
many copies of map and reduce functions
are forked for parallel processing
across the input data set a task is a
map or reduce function executed on a
subset of data with this understanding
of job and task the application master
and node manager functions become easy
to comprehend the application master is
responsible for the execution of single
application or MapReduce job it divides
the job requests into tasks and assigns
those tasks to node managers running on
the slave node the node manager has a
number of dynamically created resource
containers the size of a container
depends on the amount of resources it
contains such as memory CPU disk and
network i/o it executes SMAP and reduced
tasks by launching these containers when
instructed by the MapReduce application
master the map process is an initial
step to process individual input records
in parallel the reduced process is all
about some mating the output with a
defined goal as coded in business logic
node manager keeps track of individual
map tasks and can run in parallel a map
job runs as part of a container
execution by node manager on a
particular data node the application
master keeps track of a MapReduce job
the flow diagram represents the Hadoop
job work interaction initially a Hadoop
MapReduce job is submitted by a client
in the form of an input file or a number
of input split of files containing data
MapReduce
master distributes the input split to
separate node managers MapReduce
application master coordinates with the
node managers MapReduce application
master resubmits the tasks to an
alternate node manager if the data node
fails resource manager gathers the final
output and informs the client of success
or failure status let's look at some
MapReduce characteristics MapReduce is
designed to handle very large-scale data
in the range of peda bites and exabytes
it works well on write once and read
many data also known as worm data
MapReduce allows parallelism the map and
reduce operations are performed by the
same processor operations are
provisioned near the data as data
locality is preferred commodity hardware
and storage is leveraged in MapReduce
the runtime takes care of splitting and
moving data for operations some of the
real-time uses of MapReduce are as
follows simple algorithms such as grep
text indexing and reverse indexing data
intensive computing such as sorting data
mining operations like Bayesian
classification search engine operations
like keyword indexing ad rendering and
page ranking Enterprise analytics
Gaussian analytics for locating
extraterrestrial objects in astronomy
Semantic Web and web 3.0 ensure that all
hadoop services are live and running
this can be verified in two steps
use the command JPS and then look for
all five services name node data node
node manager resource manager
small data consists of block sizes
lesser than 256 megabytes to upload
small and big data you will use the
following sources or int piece which is
from an ebook website weather data that
is updated on an hourly basis the sample
data set from these sources will be
provided and can be used to perform
MapReduce operations uploading small
data and big data the command to upload
any data big or small from the local
system to HDFS is Hadoop space FS space
copy from local space source file
address space destination file address
let's look at the steps to build a
MapReduce program determine if the data
can be made parallel and solved using
MapReduce for example you need to
analyze whether the data is right once
read many or worm in nature design and
implement a solution as mapper and
reducer classes compile the source code
with Hadoop core and package the code as
jar executable configure the application
job as to the number of mapper and
reducer tasks and to the number of input
and output streams load the data or use
it on previously available data then
launch and monitor the job study the
results the user or developer is
required to set up the framework with
the following parameters locations of
the job input in the distributed file
system locations of
output in the distributed file system
input format output format class
containing the map function class
containing the reduced function which is
optional if a job does not need a reduce
function there is no need to specify a
reducer class the framework will
partition the input schedule and execute
map tasks across the cluster if
requested it will sort the results of
the map task and it will execute the
reduce tasks with the map output the
final output will be moved to the output
directory and the job status will be
reported to the user the image shows the
set of classes under the user supply and
framework supply user supply refers to
the set of Java classes and methods
provided to a Java developer for
developing hadoop mapreduce applications
framework supply refers to defining the
workflow of a job which is followed by
all hadoop services as shown in the
image the user provides the input
location in the input format as required
by the program logic once the
resource manager accepts the input a
specific job is divided into tasks by
application master each task is then
assigned to an individual node manager
once the assignment is complete the node
manager will start the map task it
performs shuffling partitioning and
sorting for individual map outputs once
the sorting is complete reducers starts
the merging process this is also called
the reduced task the final step is
collecting the output which is performed
once all the individual tasks are
reduced this reduction is based on
programming logic the basic user or
developer responsibilities of MapReduce
are setting up the job specifying the
input location and ensuring the input is
in the expected format and location the
framework responsibilities of MapReduce
are as follows distributing jobs among
the application master and node manager
nodes of the cluster running the map
operation performing the shuffling and
sorting operations reducing phases
placing the output in the output
directory and informing the user of the
job completion status to install Eclipse
in the aboon to desktop 14.04 open
Ubuntu Software Center type Eclipse in
the search bar as shown on the screen
select Eclipse from the list next click
the install button to continue
once it Clips is installed in your
system create a new project and add the
essential jar files to run MapReduce
programs to create a new project click
the file menu select new project
alternatively press ctrl n to start the
wizard of a new project select Java
project from the list then click the
next button to continue
type the project name as word count and
click the next button to continue
include jar files from the Hadoop
framework to ensure the programs locate
the dependencies to one location under
the libraries tab click the add external
jars button to add the essential jar
files after adding the jar files click
the finish button to create the project
successfully setting up Eclipse
environment for MapReduce in this demo
you will learn to setup an Eclipse
environment to execute a MapReduce
program double-click the Eclipse exe
file to start the Eclipse environment
you will be asked for the workspace
where the program will be kept click ok
you
as you can see there are multiple
projects all
present and these projects have their
project files and other details you can
also refer to any of the old projects
for the purpose of this demo let's
create a new project
create a new project click file new job
project
give the name of the project let's say
simply learn
click Next
finish
it will create a simply learned folder
for you you can see that an SRC
directory and a JRE system library are
available for you now you need to import
a few jar files through which eclipse
will understand Hadoop program to import
jar files perform the following steps
right click simply learn folder navigate
to build path
select the configure build path
click add external jars
goto slash user slash library slash
Hadoop directory
you can see that there are multiple
available
you will need Hadoop common and Hadoop
core which you will find inside the
client 0.20 directory
you will notice Hadoop common and Hadoop
core jar file
select the files
common and Hadoop core
you
click apply and then okay you will
notice these jar files appearing in your
reference library directory now you need
to create a new class in this project
where you will write your first program
right-click the SRC folder
navigate to new
class give the name of the class file
which can be word count in this case
click finish
you
this has created a blank file word-count
Java this brings you to the end of the
demo in the
you have learned to set up an eclipse
environment create a project and the
steps to import jar files you have also
learned to write a program in a class
running MapReduce program in this demo
you will learn how to write a MapReduce
program in the Eclipse environment let's
import all the Hadoop libraries that are
required to execute this program in this
program you will calculate the number of
times each word occurs in a file that
resides in the HDFS directory you will
write two functions mapper and reducer
that will help you to achieve this task
you
in the main function you will have to
define the properties and configurations
to run a MapReduce program
after you finish writing the program
right click simply learn project
directory click export and select jar
file under Java type the name of the jar
file
provide any name of your choice
let's keep it word-count jar in this
case
finish since the
already exists you will have to
overwrite it so click yes
the file in the local directory you can
see the file word count jar in the
directory
in order to run this program let's type
a dupe jar and then the name of the jar
file word count jar that we have just
created
you
then type the name of the class file it
is inside the simply learned package dot
word count
you
now you need to provide input HDFS path
which is simply learn slash simply learn
txt
you
you also need to provide an output path
which can be in simply learn slash out
underscore simply learn
you
this out underscore simply learned will
be a new directory that will be created
in HDFS
you
now let's go to Hue click simply learn
text notice that the operation is done
on the simply learned text file let's
come out of it you will notice that
there is a new directory out underscore
simply learn which has just been created
this file will have all the counts of
the words
this brings you to the end of this demo
you have now learned to write a
MapReduce program and run it it is
important to check whether the machine
setup can perform MapReduce operations
to verify this use the example jar files
deployed by Hadoop this can be done by
running the command shown on the screen
before executing this command ensure
that the words txt file resides in the
data slash first location this image
shows the MapReduce version 2.7
architecture comprising yarn
a Duke Map Reduce uses data types to
work with user given mappers and
reducers the data is read from files
into mappers and emitted by mappers to
reducers process data is sent back by
the reducers data emitted by reducers
goes into output files at every step
data is stored in Java objects let us
now understand the writable datatypes in
advanced MapReduce in the Hadoop
environment all the input and output
objects across the network must obey the
writable interface which allows a dupe
to read and write in a serialized form
for transmission let's look at Hadoop
interfaces in some more detail the
interfaces in Hadoop are writable and
writable comparable as you've already
seen a writable interface allows a dupe
to read and write data in a serialized
form for transmission a writable
interface consists of two methods read
and write fields are shown here a
writable comparable interface extends
the writable interface so the data can
be used as a key and not as a value as
shown here the writable comparable
implements two methods compared to and
hashcode let's now examine various data
types in Hadoop and their functions the
first data type is text the function of
this data type is to store string data
the integer writable data type stores
integer data long writable as the name
suggests stores long data similarly
other data types are float writable for
storing float data and double writable
for storing double data there is also
boolean writable and byte writable data
types
no writable is a placeholder when a
value is not needed the illustration
here shows a sample data type you can
create on your own this data type will
need you to implement a writable
interface MapReduce
specify how its input is to be read by
defining an input format the table lists
some of the classes of input formats
provided by the Hadoop framework let's
look at each of them the first class is
key value text input format which is
used to create a single key value pair
per line text input format is used to
create a program considering a key as
the line number and the value as the
line itself n line input format is
similar to text input format except that
there are n number of lines that make an
input split multi file input format is
used to implement an input format that
aggregates multiple files into one split
for the class sequence file input format
to be implemented the input file must be
a Hadoop sequence file which contains
serialized key value pairs now
you have completed input formats and
MapReduce let us look into the classes
for MapReduce output format the first
class is the default output format text
output format it records as lines of
text each key value pair is separated by
a tab character this can be customized
using the MapReduce text output format
separator property the corresponding
input format is key value text input
format sequence file output format write
sequence files to save output this
represents a compact and compressed
version of normal data blocks sequence
file as binary output format is
responsible for writing key value pairs
that are in a raw binary format into a
sequential file container map file
output format writes map files as the
output the keys in the map file are
added in a specific order the reducer
then emits keys in the sorted order
multiple text output format writes data
to multiple files whose names are
derived from output keys and values
multiple sequence file output format
creates output in multiple files in a
compressed form distributed cache is a
Hadoop feature to cache files needed by
applications a distributed cache helps
boost efficiency when a map of reduced
tasks needs access to common data allows
a cluster node to read the imported
files from its local file system instead
of retrieving the files from other
cluster nodes allows both single files
and archives like zip and tar.gz copies
files only to slave nodes if there are
no slave nodes in the cluster then
distributed cache copies the files to
the master node allows access to
cast files from the mapper or reducer
applications to make sure that the
current working directory is added into
the application path allows referencing
of the cached files as though they are
present in the current working directory
here are the steps to use distributed
cache in Hadoop first set up the cache
by copying the requisite files to the
file system set up the applications job
config as shown use the cached files in
the mapper or reducer
joins our relational constructs that can
be used to combine relations in
MapReduce joins are applicable in
situations where you have two or more
data sets you want to combine a joint is
performed either in the map phase or in
the reduce phase by taking advantage of
the MapReduce or to merge
architecture the various join patterns
available in MapReduce are reduced side
join replicated join composite join and
Cartesian product a reduced side joint
is used for joining two or more large
data sets with the same foreign key with
any kind of join operation a replicated
join is a map side joint that works in
situations where one of the data sets is
small enough to cache a composite join
is a map side join use on very large
formatted input datasets sorted and
partitioned by a foreign key a Cartesian
product is a map side join where every
single record is paired up with another
data set a reduced side join works in
the following ways the mapper prepares
for joint operations
it takes each input record from every
data set and emits a foreign key record
pair the reducer performs a joint
operation where it collects the values
of each input group into temporary lists
the temporary lists are then iterated
over and the records from both sets are
joined a reduced side joint can be used
when multiple large data sets are being
joined by a foreign key when flexibility
is needed to execute any joint operation
when a large amount of network bandwidth
is available and when there is no
limitation on the size of the data sets
a sequel analogy of a reduced side joint
is given on the screen in the output of
a reduced side joint the number of part
files equals the number of reduced tasks
a replicated joint is a map only pattern
that works as follows it reads all files
from the distributed cache and stores
them into in-memory look-up tables the
mapper processes each record and joins
it with the data stored in memory there
is no data shuffled to the reduce phase
the mapper provides the final output
replicated joins should be used when all
data sets except for the largest one can
fit into the main memory of each map
task that is limited by Java Virtual
Machine or JVM heap size and when there
is a need for an inner join or a left
outer join with the large input data set
being the left part of the operation a
sequel analogy of a replicated joint is
given on the screen in the output of a
replicated join the number of part files
equals the number of map tasks a
composite joint is a map only pattern
working in the following ways all data
sets are divided into the same number of
partitions each partition of the data
set is sorted by a foreign key and all
the foreign keys reside in the
Associated partition of each data set 2
values are retrieved from the input
tuple associated with each data set
based on the foreign key and the output
to the file system composite joins
should be used when all data sets are
sufficiently large and when there is a
need for an inner joint or a full outer
join a sequel analogy of a composite
joint is displayed on the screen in the
output of a composite joint the number
of part files equals the number of map
tasks a Cartesian product is a map only
pattern that works in the following ways
data sets are split into multiple
partitions each partition is
said to one or more mappers for example
in the image shown here split a1 and a2
are fed to three mappers each a record
reader reads every record of input split
associated with the mapper the mapper
simply pairs every record of a data set
with every record of all other data sets
a Cartesian product should be used when
there is a need to analyze relationship
between all pairs of individual records
and when there are no constraints on the
execution time in the output of a
Cartesian product every possible tuple
combination from the input records is
represented scoop now that you have
learned about MapReduce let's talk about
scoop in Hadoop
scoop is an Apache Hadoop ecosystem
project it is a command-line interface
application for transferring data
between relational databases and Hadoop
it supports incremental loads of a
single table or a free-form sequel query
imports can also be used to populate
tables in hive or HBase exports can be
used to put data from Hadoop into a
relational database while companies
across industries are trying to move
from structured relational databases
like MySQL Teradata Netezza
and so on to Hadoop there were concerns
about the ease of transitioning existing
databases it was challenging to load
bulk data into Hadoop or access it from
MapReduce users had to consider data
consistency production system resource
consumption and data preparation data
transfers using scripts was both time
consuming and inefficient direct access
of data from external systems was also
complicated this was resolved with the
introduction of scoop scoop allows
smooth import and export of data from
structured databases along with Uzis
scoop helps in scheduling and automating
import and export tasks scoop in real
life can be used in online marketers
coupon comm uses scoop to exchange data
between Hadoop and IBM Netezza data
warehouse appliance the organization can
query its structured databases and
transfer the results in the Hadoop using
scoop the Apollo group an education
company also uses scoop to extract data
from databases as well as to inject the
results from Hadoop jobs back into
relational databases
Pachi Hadoop ecosystem project its
responsibility is to import or export
operations across relational databases
like MySQL MS sequel and Oracle to HDFS
let's discuss the various reasons for
using scoop sequel servers are deployed
worldwide a sequel server is the primary
way to accept the data from a user
nightly processing is being done on
sequel servers for years scoop allows
you to move data from traditional sequel
DB to Hadoop HDFS as Hadoop makes its
way into enterprises transferring the
data using automated scripts is
inefficient and time-consuming
hence scoop is used traditional DB has
reporting data visualization and other
enterprise built-in applications however
to handle large data you need an
ecosystem the need to bring the process
data from Hadoop HDFS to the
applications like database engine or Web
Services is satisfied by scoop scoop is
required when the database is imported
from a relational database our db2
Hadoop or vice-versa a relational
database or our DB refers to any data in
a structured format databases in minus
QL or Oracle are examples of our DB
while exporting databases from a
relational database to Hadoop users must
consider consistency of data consumption
of production system resources and
preparation of data for provisioning
downstream pipeline while importing the
data base from Hadoop to a relational
database users must keep in mind that
directly accessing data residing on
external systems within a MapReduce
framework complicates applications it
also exposes the production system to
excessive loads originating from cluster
nodes
hence scoop is required in both
scenarios let's look at the Venice
of using scoop it transfers data from
Hadoop to an R DB and vice-versa
it transforms data in Hadoop with the
help of MapReduce or hive without extra
coding it is used to import data from an
r DB such as sequel MySQL or Oracle into
the Hadoop distributed file system or
HDFS it exports data back to the r DB
following is a summary of scoop
processing it runs in a Hadoop cluster
it imports data from the R DB or no
sequel DB to Hadoop it has access to the
Hadoop core which helps in using mappers
to slice the incoming data into
unstructured formats and place the data
in HDFS it exports data back into our DB
ensuring that the schema of the data in
the database is maintained here is an
outline of the process scoop performs
the execution in three steps first the
data set being transferred is divided
into partitions next a map only job is
launched with individual mappers
responsible for transferring a slice of
the data set lastly each record of the
data is handled in a type safe manner as
scoop uses metadata to infer the
datatypes
my sequel basics
this demo you will learn how to work
with my sequel databases type my sequel
- you training - P training to connect
to my sequel databases in this command -
you denote username which is training
and - P denotes password in this case
the password is also training type show
databases if you want to view all the
databases present in my sequel you can
see that there is a simply learned
database available which will be used in
the demo type use simply learn to enter
the simply learned database now as you
can see you are inside the database type
show tables if you want to view all the
tables present in the simply learned
database
notice that there is an account table
present in the simply learned database
which will be used in further demos of
scoop
this brings you to the end of this demo
in this demo you have learned to connect
to a my sequel database view the tables
inside a database and enter a database
use the commands shown on the image to
import data present in MySQL database
using scoop where simply learn is the
database name and device is the table
name the process of the scoop import is
summarized on the screen
scoop introspects the database to gather
the necessary metadata for the data
being imported a map only Hadoop job is
submitted to the cluster by scoop the
map only job performs data transfer
using the metadata captured in step 1
the imported data is saved in a
directory on HDFS based on the table
being imported users can specify any
alternative directory where the file
should be populated by default these
fields contain comma delimited fields
with new lines separating the different
records users can also override the
format in which data is copied by
explicitly specifying the field
separator and recording Terminator
characters further users can easily
import data in Avro data format by
specifying the option as Avro data file
with the import command scoop supports
different data formats for importing
data it also provides several options
for tuning the import operation let's
discuss the process of importing data to
hive and HBase first scoop takes care of
populating the hive meta store with
appropriate metadata for the table and
also invokes the necessary commands to
load the table or partition next using
hive import scoop converts the data from
the native data types in the external
data store into the corresponding types
within hive further scoop automatically
chooses the native delimiter set used by
hive if the data being imported has a
newline or other hive delimiter
characters in it
scoop allows the removal of such
characters the data is then correctly
populated for consumption in hive lastly
after the import is completed the
xur can operate on table just like any
other table in hive now that you have
seen the process in importing data into
hive let's talk about the process
involved in importing data into HBase
when data is imported into HBase scoop
can populate the data in a particular
column family in an HBase table the
HBase table and the column family
settings are required to import a table
to HBase data imported to HBase is
converted using its string
representation and inserted as utf-8
bytes use the commands shown on the
screen to import data to HBase connect
to the database using the first command
specify the parameters such username
password and table name using the second
command create an HBase table with the
column family as specified in MySQL
using the third command now let's
discuss the process of exporting data
from hadoop using scoop use the commands
shown in the terminal to export data
from hadoop using scoop listing table of
my sequel DB through scoop in this demo
you have learned to view all the
commands through scoop and list the
table present in a database through
scoop type scoop help to view all the
supported commands in scoop
notice that there is a list tables API
that lists the available tables in a
database now let's see how you can do
that with a scoop command
type the command
to do that you will use the scoop list
Tables API - - connect then the
connection string to connect to my
sequel DB
in this demo we will use the simply
learn database
you can list the tables by providing the
database name which is simply learn - -
username which is training and - -
password which is training for the my
sequel DB
notice the result displayed on the
screen accounts
brings you to the end of this demo in
this demo you have learned to view all
the commands through scoop you have also
learned to list the table present in a
database through scoop we need to
perform the following steps to export
data from hadoop using scoop first
introspects the database for metadata
and transfer the data next transfer the
data from HDFS to the DB further scoop
divides the input data set into splits
it uses individual map tasks to push the
splits to the database each map task
performs this transfer over many
transactions to ensure optimal
throughput and minimal resource
utilization now we will look at the
various connectors using which we can
connect to scoop to different databases
the different types of scoop connectors
are generic JDBC default scoop and fast
path connectors the generic JDBC
connector can be used to connect to any
database that is accessible via JDBC the
default scoop connector is designed for
specific databases such as MySQL
PostgreSQL Oracle sequel server and db2
the fast path connector specializes in
using specific batch tools to transfer
data with high-throughput for example
MySQL and Postgres equal databases we
have learned that scoop divides the
tasks into four default mappers let me
explain you how parallelism helps scoop
in dividing tasks other than default
mappers by default scoop typically
imports data using for parallel tasks
called mappers increasing
number of tasks might improve import
speed but note that each task adds load
to your database server you can
influence the number of tasks using the
- M or num mappers option but scoop use
this only as a hint and might not honor
it in the following screenshot we set
parallelism to eight comments
commands are listed on the screen the
first command on the screen is to import
the data from the MySQL table scoop demo
to an HDFS directory note - m1 which
ensures that there is only one mapper
output in the second command note that
ID is greater than 2 which places a
condition on data to be imported you can
also specify a specific sequel query as
shown in the third command on the screen
where it says - II select start from
scoop underscore demo where ID equals 13
the third command on the screen shows an
export function please note the hyphens
and the double - before driver connect
username and password there are some
more commands listed on the screen for
your reference importing our DBMS table
to HDFS in this demo you will learn how
to import the table accounts from simply
learn db2 the default HDFS location type
HDFS DFS - RM - our accounts to delete
the accounts table from this directory
this will remove the accounts table from
the default directory if the table
exists
type the command now in order
to import the accounts table from my
sequel db2 HDFS you will have to type
the scoop import connection string for
my sequel DB you will have to provide
the simply learned database username for
my sequel password for my sequel - -
table and the name of the table that you
want to import to HDFS
you
type HDFS DFS - cat accounts to confirm
the creation of this table
you
as you can see it shows that a directory
named accounts has been created
type HDFS DFS
- LS accounts to view the content of the
accounts directory
you will notice that multiple part files
have been created you can view the files
by providing the whole file name
you
type HDFS DFS - cat accounts slash
asterisk
so let's view this zero zero zero file
you
notice the output displayed on screen
which is the
accounts table this brings you to the
end of this demo you have now learned to
import a table to a default location and
to remove a table from the default
directory you have also learned to
create a table confirm its creation and
view the contents of the created table
this command will list all tables in the
simply learned database in MySQL in this
screen let's look at scoops limitations
client-side architecture does impose
some limitations in scoop client must
have JDBC drivers installed for
connectivity with our DBMS client-side
architecture requires connectivity to
cluster from client users have to
specify username and password it's
difficult to integrate a CLI within
external application it is not supported
with no SQL DB because it is tightly
coupled with JDBC semantics scooped - is
the next-generation version of Hadoop
following are the advanced features when
compared to scooped client-server design
addresses limitations described earlier
API changes also simplify development of
other scooped connectors client requires
connectivity only to the scooped server
DB connections are configured on the
server by a system administrator and
users no longer need to possess database
credentials centralized audit trail
better resource management scoop server
is accessible via CLI REST API and web
UI scoop 2 is being actively developed
currently scoop 2 has fewer supported
features than scooped let us now
summarize what we've learned in this
lesson
MapReduce is a programming model that
simultaneously processes and analyzes
huge data sets logically in two separate
clusters MapReduce
execution consists of five phases map
phase partition phase shuffle phase sort
phase and reduce phase scoop an Apache
Hadoop ecosystem project is a
command-line interface application for
transferring data between relational
databases and Hadoop it is used to
import or export operations across
relational databases scoop import
processes contain three phases gather
metadata submit job to the cluster and
transfer data exporting processes of
data using scoop consists of
introspecting the database for metadata
and transferring the data from HDFS to
DB scoop 2 is the next version of the
dupe hey want to become an expert in Big
Data then subscribe to the simply
learned Channel and click here to watch
more such videos centered up and get
certified in Big Data click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>