<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Natural Language Processing (NLP) Tutorial | Data Science Tutorial | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Natural Language Processing (NLP) Tutorial | Data Science Tutorial | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Natural Language Processing (NLP) Tutorial | Data Science Tutorial | Simplilearn</b></h2><h5 class="post__date">2017-08-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6WpnxmmkYys" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">natural language processing or NLP is an
automated way to understand analyze
human languages and extract information
from such data by applying machine
learning algorithms the data content can
be text document image audio or video
sometimes it's also referred to as a
field of computer science or artificial
intelligence to extract the linguistics
information from the underlying data NLP
enables machines or computers to derive
meaning from human or natural language
input the world is now connected
globally due to the advancement of
technology and devices and this has
resulted in the high volume of digital
data across the world and has led to a
number of challenges in analyzing data
including analyzing the tons of data
that is generated in the form of text
image audios and videos identifying
approximately 6,500 languages and
dialects followed across the globe
applying quantitative analysis on huge
collections of data handling ambiguities
while interpreting data and extracting
information this is where natural
language processing proves useful one of
the main goals of natural language
processing
is to understand various languages
process them and extract information
from them in NLP full automation can be
easily achieved by modern software
libraries modules and packages these
software libraries and packages are
aware of diverse language and culture
and categorized data accordingly which
enables understanding cinemates better
with the help of these libraries we can
also build analytical models and
automate the natural language process
with minimum or no human interventions
NLP terminology now that you have
understood why
NLP is so important in recent times it's
time to make yourself comfortable with
the NLP terminologies one word
boundaries it determines where one word
ends and the other begins to
tokenization it's a technique to split
words phrases idioms etc present in a
document three stemming it's a process
to map words to their stem or root it's
very useful in finding synonyms and
extensively used in search engines for
tf-idf into numerical value which
represents how important a word is to a
document or corpus five semantic
analytics it's a technique in vectorial
semantics of analyzing relationships
between a set of documents in the terms
it contains by producing a set of
concepts related to the documents and
tons six disambiguation it's a technique
to determine the meaning and a sense of
words context versus intent seven topic
models it's a type of statistical model
for finding abstract topics which occur
in a collection of documents
now that you are familiar with the
various terminologies let's dive deeper
into the NLP approaches to analyze text
data these approaches can be
interrelated or they can be
independently applied depending on the
type of data to analyze basic text
processing it's a way to analyze text
and extract key words that sum up the
style or basic context of the text for
example if the content is religious or
fictional categorizing and tagging words
and this approach is about finding
lexical categories and automatically
tagging each word with its word class
for example tag a word using languages
such as Chinese Spanish etc words can
also be tagged as adjectives verbs nouns
and so on classify text with this
approach you can identify particular
features of language and use them to
classify it
for example classify the text as sports
politics or technology extract
information this approach is about
identifying the entities and
relationships in a text to extract
information in a structured way
for example date time money and a
direction can be used to establish
relationships with other words available
in the text analyze sentence structure
in this approach capture formal grammar
to describe the structure of a set of
sentences for example find a well-formed
or ill formed sentence structure build
feature based structure through this
approach we get an insight into
grammatical categories of the text
document for example detect features of
texts based on speech tag or some
grammar rules analyze the meaning
perform quantitative analysis of the
given set of data to extract the
information for example find entities in
the text and trying to establish a
relationship between them
in this demo we'll view how to install
in LP environment let's open the
anaconda prompt please ensure that your
system is connected to the Internet
let's go ahead and enter Conda install
scikit-learn it will install the
scikit-learn package enter Conda install
n LT k it will install the n LT k
package into your Python environment
now type in Python in anaconda prompt
command line type in import NLT kay here
to install the corpus collections and
models
we are interested in installing stop
words corpus select it and then click
download button it will install the stop
words corpus in your Python environment
you
now let's navigate through the other
models and all packages ensure that stop
words status is installed
now you can close the window and go back
to Jupiter notebook environment in this
demo you learned how to install the NLP
environment in this demo you'll learn
how to perform sentence analysis let's
start by importing the required library
class string from NLT que corpus
let's then import stop words which we've
installed earlier as part of the
environment setup
in the world of text analysis stop words
usually have little lexical meaning some
examples of stop words are I me myself
we our hours you yours and so on now
let's view the first 10 stop words
present in the stop words corpus of in
ltk library this is done by passing 0 to
10 index position in the parentheses
next let's create a test sequence to
analyze we'll create a variable test
sentence and pass the sentence this is
my first test string Wow we are doing
just fine
first we'll use string class punctuation
to remove all the punctuation present in
each message as they are also less
weighted for text analysis then we'll
remove the stop words
you
now we'll use basic Python syntax to
display the characters which don't have
punctuation as you can see we make use
of basic string class and it's
punctuation function
display the whole sentence with no
punctuation we just completed the first
step of the Senate's analysis process
now let's split the sentence into words
here we can see that the list contains a
lot of stop words let's remove the stop
words we'll use stop words corpus dot
words function with English as an
argument and basic Python word and split
function to clean each word
you
with this we are done with Senate
analysis and our sentences free of
punctuation and stop words following are
the major NLP libraries used in Python
in LT Kang
it's a python-based library for NLP and
widely used in the industry to build
programs to work with different human
languages scikit-learn
it's another powerful open-source Python
package and module for NLP it features
various algorithms and is designed for
operating with other Python libraries
like numpy and acai pie text blob this
library is used for processing text data
it provides simple api's for diving into
NLP spacing this is another library
which provides multiple useful views of
textual meaning and linguistic structure
now let's look at the scikit-learn
approach and its features
it's a powerful library with a set of
modules to process and analyze natural
language data such as texts and images
and extract information using machine
learning algorithms these are some of
the essential features of scikit-learn
approach built in modules it has built
in modules to load the data sets content
and categories that will discuss the
modules in detail in the upcoming screen
feature extraction it's a way to extract
information from data which can be text
or images psych it learns built-in
functions and methods help extract
features and attributes from text data
and image data for the purpose of
analysis model training in model
training we analyze the content based on
particular categories and then train
them according to a specific model model
training can involve supervised or
unsupervised models supervised models
and supervised models the goal is to
generate the data and find the right
answer in this model training the
outcome of new observation and dataset
is predicted unsupervised models in this
unsupervised models the response outcome
or the label of the data is not known
the main objective is to first
understand the structure of the data and
then identify the pattern in the data in
this type of model training we have to
find the predictors that behave in the
same fashion or have some familiarity
pipeline building and this is a
technique in scikit-learn approach to
streamline the NLP process in two stages
let's take a look at the various stages
of pipeline learning one vectorization
this is a general process of converting
a collection of text documents into a
numerical feature vector in
vectorization the documents are split
into words or tokens and each document
is assigned a number to transformation
and transformation the approach is to
extract features around the word of
interest also in this stage you find the
occurrence of each word in a document 3
model training and application this is
required for accurate predictions a
model is divided into training and test
datasets to optimize the overall process
for example in this stage different
algorithms are used to classify text
documents and find the sentiment
analysis performance optimization in
this stage we train the models to
optimize the overall process
grid search the model can have multiple
parameters and it's a powerful way to
search parameters affecting the outcome
for model training purposes it's one of
the most common ways to adjust the
extracted features
now that you have gone through the
scikit-learn approach let's learn about
its built-in modules for loading the
dataset contents and categories the
diagram shows you how a dataset is
loaded using methods and how the object
is structured in terms of the container
folder and its categories to load files
you have to use the main method
scikit-learn dot datasets dot load files
we can load text files with categories
as subfolder names the folder names are
used as supervise the signal label names
it doesn't try to extract features into
a numpy array or CyHi sparse matrix in
addition if load content parameter is
false it doesn't try to load the files
in memory to use text files in a
scikit-learn classification or
clustering algorithm you'll need to use
the SK learn feature extraction dot text
module to build a feature extraction
transformer that suits your problem a
data load object helps load the contents
of a data set shown here are the
attributes of a data load object bunch
it contains fields and can be accessed
as dict keys or an object target names
it's a list of requested categories data
it refers to an attribute in the memory
where files are loaded let's see how a
data set can be loaded using
scikit-learn there are several data sets
that can be loaded let's load the digits
data set to view the data using built-in
methods perform the following steps one
import the desired data set use the
syntax from SK learn data sets import
load digits to load the data set by
creating an object as we had discussed
earlier a data load object is used to
load contents three describe the data
set using DES see our function using
this function you can view all of the
information which describes the data set
next let's view the data set type use
the syntax type digits
data set as we can see the type of the
data set is bunch to view the data of
the digits data set apply the function
dot data the output displays the actual
data which is bound to the data set
let's view the target with the function
dot target the arrays displayed here are
the response data which is present in
the data set
next let's talk about feature extraction
another functionality of the
scikit-learn approach this is a
technique to convert the content into
numerical vectors to perform machine
learning the feature extraction
technique is used mostly in machine
learning while dealing with text or
image data there are two major types of
feature extraction techniques the text
feature extraction it's used to extract
features from text data
examples include large data sets or
documents image feature extraction it's
used to extract image features
examples include patch extraction or
connectivity graph of an image forming
contiguous patches also known as
hierarchical clustering bag-of-words now
let's learn about the bag of words which
is one of the most common text feature
extraction techniques it's used to
convert text data into numerical feature
vectors with the fixed size let's see
how it works by considering a document
where we need to extract text features
the steps to follow are assign a fixed
integer ID to each word by splitting
them into several words also known as
tokenizing then count the number of
occurrences of each word or token
finally store it as the value feature in
matrix format the matrix presentation on
the screen shows how multiple documents
and tokens are structured this screen
shows how the count vectorizer algorithm
works in scikit-learn
the goal is to convert a collection of
text documents to a matrix of token
counts there are several parameters
which affect the vectorization process
let's take a look at the components of
the signature input content the input
content can be a file name of the
sequence of strings which needs to be
vectorized encoding the default encoding
is utf-8 the encoding is to decode the
input strip accents it removes accent
present in the code tokenizer it
overrides the string tokenizer method
but the default value is
stop words it's a built-in stop words
list that is used max threshold it
indicates the terms or words that appear
more than a given threshold value and
should therefore be ignored min
threshold it indicates the terms or
words that appear less than a given
threshold value and should therefore be
ignored
in this demo you'll learn how to process
documents using bag-of-words
let's start by importing the required
library count vectorizer class
this class helps tokenize the document
it converts the text into vectors by
assigning numeric values to each word
let's create an object vectorizer by
instantiating the count vectorizer class
next we'll create three random documents
let's add some text to the documents
document one can have hi how are you
in document 2 we can add today is a very
very very pleasant day and we can have
some fun fun fun and in document 3 we
can say this was an amazing experience
now put all three documents together as
a list
you
next comes the interesting part which is
creating the bag of words for the list
of documents this is done by accessing
the fit method and fitting the documents
into the vectorized object
you
we'll move on to accessing the transform
method and transform the list of
documents
now let's print the bag of words and
look at the details you can see the
properties of vectorized object
as you can see the first number is the
tuple and the second number is the
frequency of words the tuple here
indicates the document number and
feature indices of each word which
belongs to the document feature indices
are generated from the transform method
this is a feature extraction process and
to extract the features you have to
refer to the indexes which is assigned
to any particular feature
next let's check for repeated words in
the vocabulary that is built with the
get function and then print it
you
finally check the bag of words type for
the documents
the output displays sparse matrix and it
falls under sigh hi in this demo you
learned how to use bag-of-words
technique and transformed documents
using the transform method
the next feature of scikit-learn is
model training an important task in
model training is to identify the right
model for the given dataset
scikit-learn provides a range of models
to choose from and also trains them by
using the extracted features from the
dataset the choice of model completely
depends on the type of data set the
models can be trained as a supervised
learning model or unsupervised learning
model supervised learning model for a
given data set will have observations
features and response in supervised
learning we make use of all three tasks
predict the outcome of new observations
and data sets understand which
predictors affect the response or
outcome and generalize the data and find
the right answer generalization is also
known as making predictions models can
be trained to classify the documents
based on the features and response for
example naive Bayes SVM linear
regression and KNN neighbors
unsupervised learning model in this type
of learning the response or the outcome
of the label of the data is not known
there is no definite or right answer in
this model so here we try to train the
model by following a few steps such as
understanding the structure of the data
first and identify the pattern in the
data finding predictors that behave in
the same fashion or have some
familiarity exploring the data set and
the goal is representation of the data
representation is also known as
extracting structure unsupervised model
can also be used to group documents by
applying clustering algorithms for
example clustering text documents using
k-means naive Bayes is a basic technique
for classification of text it assumes
that the probability of each attribute
belongs to a given class value and is
independent of all other attributes the
advantages of naive Bayes classifier it
performs the task with limited CPU and
memory therefore it's very efficient
it's fast as the model training time is
very less naive Bayes is heavily used in
cinnamon analysis email spam detection
of documents and language detection
multi nominal naivebayes is used when
multiple occurrences of the words matter
multi nominal distribution normally
requires integer feature counts let's
take a quick look at the site get multi
nominal NB model class there are three
major parameters in this class alpha
it's also known as a lat place transform
or smoothing parameter and is mainly
used for categorical data fit prior it
indicates whether to learn class prior
probabilities or not if false a uniform
prior will be used but as you notice the
default is true class prior the default
is none but if specified the prior class
is adjusted according to the data the
next feature of scikit-learn will be
looking into is grid search we use a
document classifier to predict the
category of a document to try predicting
the outcome of a new document we need to
extract the features a document
classifier can have many parameters and
the grid approach helps to search the
best parameters for model training and
predicting the outcome accurately when
we create a model it ends up with
multiple parameters it would be too
tedious to run through each parameter
and tweak it in such cases a grid search
useful the whole dataset can be divided
into multiple grids like a chessboard
and the search can be run on entire
grids or a selected combination of grids
so we use a grid search mechanism to
perform an exhaustive search on the best
parameters which affect the model
however this approach has some
constraint such as the whole grid search
process is subjected to availability of
the CPU cores of the system this is
owing to the fact that there can be
several parameters and running an
exhaustive search can impact the system
performance by taking more of the CPU
memory
so far we have learned that there are
three major steps for analyzing text
data one vectorizer this is a general
process of converting a collection of
text documents into a numerical feature
vector in vectorization the documents
are split into words or tokens and each
document is assigned a number to
Transformer during transformation you
find the occurrence of each word in a
document this process is primarily used
for feature extraction three classifier
or model training this is required for
accurate predictions a model uses
training and testing datasets to
optimize its overall performance for
example different algorithms are used to
classify text documents and find the
sentiment analysis this is where
scikit-learn becomes more productive by
creating the pipeline for the dataset a
pipeline is essentially a combination of
these three steps
so using scikit-learn pipeline can be
created and we can train the model using
a single command we just need to put
them all together in a single class and
create a pipeline to operate on a given
data set the main purpose of the
pipeline is to assemble several steps
that can be cross validated while
setting different printers
in this demo we'll learn how to apply
pipeline and grid search let's start by
importing the required library pandas
string print and time
you
next we'll get the spam data collection
with the help of the pandas library
here the backslash T indicates that data
in the spam dataset is tab separated or
tab delimited and set function takes it
as a parameter also message is a feature
which refers to the text present in the
data set
response is the label or the category of
the message
let's view the first 5 records of the
spam collection the head method is used
to view the topmost records and a data
set
let's import scikit-learn specific
libraries for text processing and
creating the pipeline and performing the
grid search
we will import counter vectorizer to
tokenize the document and convert the
text into numeric values also let's
import tf-idf transformer for tf-idf
transformation
now import SGD classifier from the
linear model this is to classify the
message in the data set
next import grid search from the SK
learn grid search class to perform the
grid search
finally import pipeline from pipeline
class
let's create a pipeline by instantiating
the pipeline class and passing vectorize
transformed and the model classifier in
his argument
you
the statement you see here helps create
the pipeline for the entire text
processing now let's define parameters
for grid search
we are going to use the tf-idf parameter
to weigh the terms present in the
message
you
next let's create an object grids search
and pass pipeline parameters jobs and
the verbose as arguments
you
the grid search consumes a lot of memory
and in jobs equals negative one
instructs the process to use all CPUs
while processing
we can't print the parameters and run
the grid search on the data set for both
the message and response after fitting
them into the grid search object using
the fit method
you
the grid search is a CPU intensive
process and it takes a little time since
it does an exhaustive search on the
entire data set on the past parameters
this brings us to the end of this demo
where you learned how to apply pipeline
and grid search mechanism hey want to
become an expert in Big Data then
subscribe to the simply learned Channel
and click here to watch more such videos
to nerd up and get certified in Big Data
click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>