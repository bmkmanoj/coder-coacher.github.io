<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AWS Architecture Tutorial | AWS Tutorial For Beginners | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="AWS Architecture Tutorial | AWS Tutorial For Beginners | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AWS Architecture Tutorial | AWS Tutorial For Beginners | Simplilearn</b></h2><h5 class="post__date">2017-08-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QbipcgIdSJc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in the topic how to design cloud
services we will take a look at the AWS
well architected framework and we'll
take a deeper look at the four pillars
which make up the framework and these
are security reliability performance
efficiency and cost optimization well
then take a look at where you can find
more information about designing cloud
services for Amazon and we will look at
the AWS QuickStart reference deployments
and where you can find out real-life
examples about how real businesses are
using Amazon Web Services
so let's start by looking at the AWS
well architected framework now this was
introduced because building cloud
infrastructure services is vastly
different to what you used to with
on-premise infrastructure so AWS created
this framework to help you understand
the pros and cons decisions you make
while building systems on AWS the AWS
framework documents are set the
foundation questions that allow you to
understand if a specific architecture
aligns well with the cloud best
practices and there's five principles it
helps you with the first of which is
stop guessing your capacity needs test
systems are production scale lower the
risk of architecture change automate to
make architecture experimentation easier
and allow for evolutionary architectures
and we'll take a look at each of these
in greater detail so firstly let's take
a look at stop guessing your capacity
needs now imagine you're designing a
brand new application for your business
all the infrastructure decisions you
will make are made using estimates which
are basically guesses
if you overestimate you might end up
buying too much hardware and have a
fleet of expensive idle resources
conversely you might underestimate and
have to deal with the performance
implications of limited capacity with
cloud computing these problems go away
as AWS helps you eliminate the guesswork
in your infrastructure capacity needs
you can use as much or as little
capacity as you need and scale up and
down automatically depending on your
demand
the cloud allows you to test systems of
production scale traditionally in a non
cloud environment is usually very
difficult to fully test new products or
services in development it is cost
prohibitive to have like the like
environments we just don't have the
resources available personally I've lost
count how many projects have worked on
when things have gone wrong as soon as
the products gone live due to
insufficient testing of the cloud you
can duplicate environments on demand and
when you've completed your testing just
shut them down and you only have to pay
for the time the test environment was up
and running so you can test fully before
you go live in production
AWS allows you to lower the risk of
architecture change as you can automate
the creation of test environments so you
can emulate your production
configurations and carry out testing
against comparable environments you can
also remove testing bottlenecks where
various teams are lining up to use your
test environments as with AWS you can
cheaply spin up as many test
environments as you need and ensure that
all the production changes have been
sufficiently tested
you can automate to make architecture
experimentation easier if you need to
make a change to production but not sure
what the impact will be well you can
automate the creation and replication of
your systems at low cost and low effort
to test these changes you can then audit
the impact and then if necessary you can
revert the changes and try again
WS you can allow the evolutionary
architectures in a traditional
environment architecture decisions are
often implemented as a static one-time
event you have to live with during the
lifetime of a system but with a cloud
the capability exists that automate and
test on-demand to lower the risk of
design changes so you can implement new
innovations and features easily what
this means is you aren't stuck with last
year's technology and you can implement
new technology as soon as it comes out
as mentioned earlier the world
architected framework is based on four
pillars security reliability performance
efficiency and cost optimization so in
the coming slides we're going to take a
further look at these four pillars
starting with security
and defined security as the ability to
protect information systems and assets
while delivering businesses value
through risk assessments and mitigation
strategies
this means it is you can apply security
at all layers so rather than just
running minimal security like firewalls
at the edge of your infrastructure you
can use firewalls and other security
controls on all of your resources for
example security groups on ec2 instances
allow you to define who and what can
access that specific resource rather
than just defining it at a global
infrastructure level you can enable
traceability so you can log in order all
changes to your environment you can
automate responses to security events so
you can monitor and automatically
trigger alerts depending on your needs
you can focus on sharing your system the
AWS shared responsibility model allows
you to focus on securing your
application data and operating systems
or AWS secures your infrastructure and
services and you can automate security
best practices AWS allows you to create
and save a custom baseline image of a
virtual server and then you can use that
image to automatically launch new
servers this way you can ensure that all
new resources adhere to your security
standards
he mentions the AWS shared
responsibility model now we take a look
at this in more depth in a later lesson
but what it means is that the AWS shared
responsibility model allows AWS
customers to focus on using services to
accomplish their security and compliance
goals because AWS physically secures the
infrastructure that supports your cloud
services if you look at this diagram you
can see it's split into two security
measures that the cloud service provider
implements and operates is the security
of the cloud and that's a responsibility
of AWS then the security measures that
the customer inputs and operates and
this is related to the security of
customer content and applications that
make use of the AWS services and this is
security in the cloud and this is a
responsibility of you
so let's take a look at security in the
cloud and this is composed of four areas
data protection privilege management
infrastructure protection and detective
controls
protection before architecting any
system practices that ensure security
should be in place with data protection
these are categorizing data based on
levels of sensitivity granting least
privilege or still allowing users to
perform their work and encryption to
protect your sensitive data if you're
storing customer credit card data
obviously this needs to be categorized
as sensitive and it needs to be
encrypted but log files from a daily
maintenance task wouldn't need this
level of security now there's a number
of tools AWS offers to assist you with
this AWS makes it easy to encrypt data
and manage keys and key rotation using I
am and key management service you can
perform detailed logging using cloud
trail and AWS offers resilient storage
systems like Amazon s3 which has 11/9
secure ability now other nines of
durability means that if you store
10,000 objects with Amazon s3 on average
you can expect to incur a loss of a
single object once every 10 million
years so in other words you're not going
to lose many files and finally AWS
offers versioning and lifecycle
management again with s3 so you can
protect to get a credential deletes or
file over the rights
now a central part of any information
security program is privilege management
so that you can ensure that only
authorized and authenticated users are
able to access your resources and that
they can only access them in a way that
is acceptable for example if a user
needs access to a resource don't just
give them admin or read/write
permissions by default because they
might actually only need read-only
access now to control your privilege
management you need to have a few things
in place the first of which is an access
control list and this is a document that
needs to be maintained which lists
access permissions attached to an object
so if you have some particular documents
you need to define what access is
allowed to it then you have role based
access controls and this defines the
permissions for a particular end-users
role or function so an administrator
would obviously need administrator
access to various resources but you know
a day a regular end user might only need
read-only access so you need to define
which roles have which permissions then
this password management and this
includes complexity requirements and
change intervals so you define how often
people need to change their passwords
and how complex they need to be now
Amazon helps you with all this with the
Identity and Access Management or I am
service and this is the primary service
used to control access to AWS services
and resources we cover I am in huge
detail in a later lesson but briefly it
allows you to apply granular policies
which assign permissions to users groups
roles or resources and you can also
control the password strength using this
as well as Federation which are existing
directory services such as Microsoft
Active Directory
we also need to protect your
infrastructure and to do this it's
recommended to have multiple layers of
Defense and also multi-factor
authentication to meet best practices
with industry or regulatory obligations
now Amazon allows you to do this by
implementing stateful or stateless
packet inspection EB using AWS native
technologies or by using partner
products and services available through
the AWS marketplace Amazon virtual cloud
VPC enforces boundary protection and
monitoring points of ingress and egress
and then this cloud trail and cloud
watch which provides comprehensive
logging monitoring and alerting
you should always have detective
controls in place so you can detect or
identify security breaches this needs to
happen so you can provide both a quality
support process and meet compliance
obligation and by a quality support
process I mean one in which when you
have a problem you're committed to
finding out the root cause and actually
resolving the issue rather than just
saying oh we think it was that let's
hope it doesn't happen again when I know
I've worked for some places like that
and it's certainly not quality support
process AWS helps you with your
detective controls by providing these
services AWS cloud trail is a service
that logs API tools so you can find out
the identity of callers the time of
calls source IP addresses and things
like that so you can find out who is
doing what and when Amazon CloudWatch is
a monitoring service for AWS resources
so you can log ec2 CPU usage or network
activity and you can use cloud watch
with many different services like RDS
EBS and more you can also set up alarms
so you can be notified when certain
things happen
AWS config is an inventory and
configuration history service that
provides information about the
configurations and changes in
infrastructure over time with Amazon s3
you can see to access all the ting so
you can find out who has been accessing
your s3 data and who when they did it
and when an Amazon Glacia you can use
the vault lock feature to preserve
mission-critical data with compliance
controls that are designed to be
auditable for long term retention
the next pillar is reliability and
Amazon defines this as the ability of a
system to recover from infrastructure or
service failures dynamically acquire
computing resources to meet demand and
mitigate disruptions such as
misconfigurations
or transient network issues with
reliability in the cloud there's a
number of things to look at firstly you
can test recovery procedures cloud
infrastructure means you can test how
your system fails and you can validate
your recovery strategy places I've
worked at in the past disaster recovery
was a big event it was only done
annually it took months of preparation
it involved working a whole weekend and
then some and was also incredibly
disruptive to the business imagine being
able to duplicate your production
environment and test failure and
failover whenever you want you can
automatically recover from failure with
the cloud monitoring a system for a key
performance indicator or KPI you can
automatically trigger automated recovery
processes where threshold is breached
you can scale horizontally to increase
aggregate system availability using
multiple small resources instead of one
large resource will reduce your single
points of failure and also you can stop
guessing capacity gone are the days of
using massive memory or not enough CPU
is a failure for a resource with the
cloud if you can scale as and when you
need to so you can't blame badly
estimated capacity with the reason for
reliability issues
now reliability in the cloud is composed
of three areas these are foundations
change management and failure management
to achieve reliability a system must
have a well-planned Foundation and
monitoring in place plus systems for
handling changes in demand or
requirements is also required in an
ideal world your system would be
designed to detect failure and
automatically heal itself before
architecting any system you should have
foundations in place so that your
reliability is not impacted you should
have sufficient network bandwidth to
your data center you should have
sufficient compute capacity your staff
should be trained in the areas they need
to be and you should have support
contracts with your vendors with AWS
network bandwidth and compute capacity
is already taken care of AWS also offers
a range of support contracts to help you
with issues and a range of training
courses for your staff change management
is a critical part being aware of how a
change affects a system allows you to
plan proactive and monitoring allows you
to quickly identify trends that could
lead to capacity issues or SLA breaches
with AWS you can monitor the behavior of
a system and automate the response to
KPIs for example you can add additional
servers as a system gains more users so
you can control who has permissions to
make system changes and audit the
history of these changes unfortunately
it's a given that failures will occur so
you need to have some failure management
in place it's good practice to
understand why things failed so you can
prevent it from happening again no one
wants to work for a company that when
things go wrong they just get it working
again with no understanding of why a key
to managing failure is frequent and
automated testing of systems to failure
and through recovery ideally you do this
on a regular schedule and also after
you've made significant system changes
now Amazon allows you to do this with a
few products firstly AWS CloudFormation
this allows you to launch temporary
copies of whole systems a very low cost
so you can use automated testing
verify your recovery processes you can
use cloud watch to set up automation to
react a monitoring data that indicates a
failure and you can store all your data
on Amazon s3 buckets for future use the
next pillar is performance efficiency
and Amazon defines this as the ability
to use computing resources efficiently
to meet system requirements and to
maintain the efficiency as demand
changes and technologies evolve so what
does this mean well performance
efficiency in the cloud means you can
democratize advanced technologies AWS
provides products such as no SQL or
media transcoding or even machine
learning as a service so rather than
your IT team having to learn how to host
and run these new technologies AWS does
it for you you can go global in minutes
AWS allows you to easily deploy your
systems in multiple regions around the
world with just a few clicks of your
mouse so this leads to lower latency for
your customers you can use serverless
architectures in the cloud which remove
the need for you to run and maintain
servers to carry out traditional
computer activities for example storage
services can act as static websites
which remove the need for web servers
and you can also use serverless
technologies such as lambda or elastic
beanstalk and you can also experiment
more often with virtual and automatable
resources you can quickly carry
comparative testing using different
types of instances storage or
configuration so performance efficiency
in the cloud is composed of four areas
compute storage database and something
called space-time trade-off and let's
take a look at each of these with the
cloud finding the optimal server
configuration for a product will vary
based on application design usage
patterns and configuration settings
traditional IT infrastructure requires
making estimates in advance which can
lead to incorrect server configurations
and lower performance efficiency AWS is
virtualized so you can quickly change
the ec2 server configuration to increase
its performance efficiency
it may be optimal to run serverless
computing for example AWS lamda allows
you to execute code without running an
instance an elastic beanstalk allows you
to run web applications in a similar
manner from an operational standpoint
you should have monitoring in place to
notify you of any degradation in
performance the optimal storage solution
for a particular system will vary based
on whether you need block file or object
storage the type of through code
required frequency of access and
availability and durability constraints
well architected systems use multiple
storage solutions and enable different
features to improve performance in AWS
Storage is virtualized and is available
in a number of different types this
makes it easier to match your storage
methods more closely with your needs and
also offers storage options that are not
easily achievable with on-premise
infrastructure for example Amazon s3 is
time for 1190 or ability and provides a
variety of storage glasses for your
different data categories like Glacia
for archived data EBS and EFS offer
numerous options for your ec2 instances
based on the level of performance you
require from your storage the optimal
database solution for a particular
system can vary based on requirements
for consistency availability partition
tolerance and latency many systems use
different database solutions for various
subsystems and enable different features
to improve performance selecting the
wrong database solution and features for
a system can lead to much lower
performance efficiency now for database
usage you need to go to monitor them
test them and also you need to have some
database platform knowledge but Amazon
makes this much easier for you as it
provides a suite of Amazon relational
database services which have fully
managed relational databases for example
Amazon DynamoDB
is a fully managed no SQL database that
provides single digit millisecond
latency at any scale Amazon redshift is
a managed petabyte scale data warehouse
that allows you to change the number or
type
note is your performance or capacity
needs change you can also make use of
aurora or am is where you install your
own databases from templates so what is
the space-time trade-off where you can
think of space as memory or storage and
you can think of time as the processing
time or the compute time to complete a
task so when you're architecting
solutions there's always a series of
trade-offs where it might be better to
have more memory of storage to reduce
the processing time or maybe you're
happy to sacrifice the processing time
in order to reduce your memory of
storage requirements AWS gives you the
option to maximize one or the other for
example you could launch your systems
globally so you're increasing space and
memory so that they're closer to the end
users in order to reduce latency or the
time but whatever resource you use you
need to have monitoring in place to
notify you of any degradation in
performance
and the final pillar of the well
architected framework is cost
optimization this is defined as the
ability to avoid or eliminate unneeded
cost or sub optimal resources AWS cloud
has a number of ways you can provide
cost optimization firstly you can
transparently attribute expenditure that
the cloud makes it easy to identify the
cost of a system an attribute IT cost of
individual business owners so those
owners will have an incentive to
optimize their resources and reduce
costs you can use managed services to
remove the operational burden of
maintaining servers for tasks such as
sending email or managing databases you
can create capital expense the operating
expense instead of investing heavily in
data centers and servers before you know
how you're going to use them with AWS
you pay only for the computing resources
you consume when you consume them also
AWS allows you to achieve higher
economies of scale because they can by
way more servers and hardware than you
could ever imagine so it's much cheaper
to use their hardware than by your own
you should also stop spending money on
data center operations AWS is the heavy
lifting of racking stacking empowering
servers so you can focus on your
customers and business projects rather
than on your IT infrastructure cost
optimization in the cloud is composed of
four areas match supply and demand
cost-effective resources expenditure
awareness and optimizing over time
matching supply to demand will deliver
the lowest costs for a system but you'll
need to be able to provide sufficient
extra capacity to cope with demand in
failures in AWS you can automatically
provision resources to match demand
auto-scaling time-based and event-driven
and cue based approaches allow you to
add and remove resources as needed
monitoring tools and regular
benchmarking can help you achieve much
greater utilization of your resources
using the appropriate instances and
resources for your system is key to cost
savings for example a reporting process
might take
hours to run on a smaller server but a
larger server that is twice as expensive
might better do it in just one hour both
jobs give you the same outcome but the
smaller server will occur more costs
over time but to be able to find out
these things you need to have monitoring
of expenditure in place and also regular
benchmarking AWS helps you with this AWS
trusted adviser goes through your
resources and tells you where you can
make savings you can use on-demand
instances you only pay for compute
capacity by the hour with no minimum
commitments required or you could use
reserved instances which allow you to
reserve capacity and offer savings of up
to 75% off on-demand pricing and with
spot instances you can bid on unused ec2
capacity at significant discounts you
can also take advantage of managed AWS
services such as RDS or dynamo DB which
can also lower your costs AWS and is
virtually unlimited on-demand capacity
requires a new way of thinking about
expenditures so you need to have a
different way of budgeting and you need
to understand your business units usage
to do this you can use cost allocation
tags to categorize and track your AWS
costs for AWS resources such as ec2 or
Amazon s3 an AWS generates a cost
allocation report with your usage and
cost aggregated by your tags so you can
set up tags for each of your business
categories and units this increased
visibility of cost makes it easier to
identify resources or projects that are
no longer generating value and should be
decommissioned billing alerts can be set
up to notify you of predicted overspend
and the AWS simple monthly calculator
allows you to calculate data transfer
costs and you can set up SNS alerts so
you can be notified when certain
resources or cost breaches occur
optimizing over time is an important
strategy to reduce your costs associated
with your cloud environment AWS is
always releasing new products and
services as such you need to reassess
your
things set up to see if it's the most
cost-effective for example rather than
running a database on an ec2 instance it
might be cheaper to run an Amazon RDS
database or lambda might be more cost
effective than ec2 but wait to find out
more information about these new
products and services from Amazon well
the first place to look for any new
architecture guide is the AWS
architecture Center this is a resource
that provides you with the guidance and
application architecture best practices
to build highly scalable and reliable
applications in the cloud here you'll
find the AWS reference architectures the
AWS reference architecture data sheets
provide architectural guidance to help
you build an application on the AWS
cloud you'll find data sheets that
include a description of how each
service is used plus visual
representations of the application
architecture for example web application
hosting or large-scale processing and
huge data sets or even building elastic
web front-ends for an e-commerce website
AWS white papers provide a comprehensive
list of technical AWS white papers that
cover all Amazon related topics such as
architecture security and economics
there are new white papers being
released all the time and written by
some of the most knowledgeable AWS
people around AWS Quick Start reference
deployments use AWS CloudFormation
templates to rapidly deploy a fully
functional environment for a variety of
enterprise software applications
supplementary deployment guides describe
the architecture and implementation in
detail in this course we'll deploy
things manually step by step so you can
see how it works but with cloud
formation templates you can do several
hours work with just a click of a button
examples of cloud formation templates
are SharePoint Rd gateway or even sequel
server on Windows Server failover
classes and finally case studies AWS
maintains a large list of case studies
and success stories from their clients
you can check to see how some of the
largest and most successful companies on
the planet use AWS for their business
you'll learn how the AWS well
architected framework is used in
planning and designing we'll take a look
at scaling and all the different
versions we'll look at the importance of
loose coupling how you can build
redundancy into your cloud
infrastructure how you can save money by
cost optimization will look at
automation and how it can make
management of an IT environment easier
and also we'll take a look at security
how you can secure your cloud resources
so let's start with scalability in the
past you would have to estimate the peak
load of your systems and purchase your
hardware accordingly so perhaps you were
setting up a new website and you're
estimating that you're going to get a
million people per month hitting your
website so you buy the hardware
accordingly but then you find out that
only 50,000 people are hitting it or 10
million people are hitting it so your
hardware is inappropriate for the work
cloud computing provides virtually
unlimited on-demand capacity so you can
scale whenever you need to to do this
your designs need to be able to
seamlessly take advantage of these
resources and there are two ways to
scale vertically and horizontally
vertical scaling means increasing the
specifications of an individual resource
for example increasing the memory or the
number of CPUs on the server with AWS
this is easily achieved with a restart
of your virtual server so that it
resizes to a larger or smaller instance
type the advantage of this approach is
that it's very easy and requires little
thought in the short term however this
type of scaling can eventually hit a
limit and sometimes be cost inefficient
especially if you scale up to some very
very large instances horizontal scaling
means increasing the number of resources
rather than the specifications of each
individual resource for example adding
additional web servers to help spread
the load of traffic hitting your
application this is a great way to build
applications that leverage the
elasticity of alcaman
however not all architectures can
distribute their workload to multiple
resources a stateless application is one
that needs no knowledge of previous
interaction and stores no session
information an example of this would be
a web server that provides the same web
page to any end user a stateless
application can scale horizontally as
any request from any end user can be
serviced by any of the available compute
resources you can add more resources as
required and then remove them when the
capacity is no longer available
stateless applications do not need to be
aware of their peers all that is
required is a way to distribute the
end-user sessions to them and there are
two ways to distribute load for multiple
nodes the first of these is the push
model a load balancer such as the AWS
elastic load balancer is a popular way
to distribute a work load across
multiple resources an alternative but
less recommended approach is to
implement a DNS round-robin using Amazon
route 53 where DNS responses return an
IP address from a list of valid hosts in
a round robin fashion this isn't easy to
set up an option but it can cause issues
if DNS records are cached the
alternative to the push model is the
pool model parts that need to be
performed can be stored as messages in a
queue and multiple compute resources can
pull and process the messages in a
distributed fashion examples of this are
big data processing scenarios where many
servers work to analyze data and return
results or media file conversion
processes where multiple servers convert
the files as they arrive
Amazon sqs or Amazon Kinesis are
services that can provide poor model
load balancing most applications need to
maintain some kind of state information
for example an automated multi-step
process will also need to track previous
activity to decide what its next action
should be you can make components in
your architecture stateless by not
storing anything on the local files
them and instead storing user or session
based information in the database right
dynamodb or MySQL or on shared storage
like Amazon s3 or EFS additionally
Amazon SWF can be used to sent released
or execution history and make your
workload stateless so the opposite of
stateless applications are state full
applications as it's not possible to
turn some layers of your architecture
into stateless components for example
databases which are stateful by
definition for applications that were
designed to run on a single server
providing multiple users a consistent
view of the database or application is
much simpler when your components are
not distributed distributing the load is
possible though through session affinity
this means that all transactions of a
session are bound to a specific compute
resource however this means that
existing sessions would not be able to
utilize newly introduced compute nodes
but also if a node is shut down or
becomes unavailable users bound to that
session will be disconnected
for situations when you need to process
large amounts of data it's always
beneficial to see the distributed
processing approach can be used
imagine a task that requires a huge
amount of data processing it gets to run
on a single computer esource
it would max out the resources and take
a long time to complete you can divide
the task into smaller fragments of work
then each of the tasks can be executed
across a larger set of compute resources
examples of this are the AWS elastic
MapReduce service which allows you to
run Hadoop workloads across multiple ec2
instances or Amazon Kinesis allows you
to run multiple shards of data on ec2 or
lambda resources for real-time
processing of streaming data the concept
of disposable resources is completely
new with cloud computing as cloud
computing completely changes the mindset
of an IT infrastructure environment
traditional environments involve
purchasing Hardware upfront and required
manual configuration of the software
network etc whereas with cloud computing
all infrastructure is temporary or
disposable you can launch new instances
when you need them and get rid of them
when you are done ultimately compute
resource initiation is a way to speed up
the creation of new environments AWS has
plenty of features to make new
environment creation and automated and
repeatable process and there's three
options available which we'll look at in
the coming slides bootstrapping
golden images and a hybrid approach
bootstrapping means you can take a newly
launched AWS resource with its default
configuration and then execute automated
scripts to install software or copy data
to bring that resource to a required
state so you could launch an ec2
instance and copy data on so that it
becomes a web server scripts can be
parameterized so that different
environments production or development
can be initiated easily with AWS
bootstrapping can be achieved with your
own scripts chef or puppet ops work
lifecycle events or cloud formation
and images mainly take snapshots of your
ec2 instances or RDS instances or even
EBS volumes and they can be used to
launch new instances these ec2 instances
can also be customized and saved as a
mis Amazon machine images and then you
can launch as many instances as you want
from this template if you have an
on-premise virtualized environment you
can also use AWS vm import/export to
create your own AWS AM is the hybrid
approach makes use of both bootstrapping
and golden images you can launch new
instances from a golden image but then
bootstrap to configure other actions
basically with AWS you are only
restricted by your own imagination or
coding skills AWS assets are
programmable so you can apply all these
techniques and principles that we've
just talked about to entire environments
and not just individual resources so you
can completely rethink the way you work
for IT infrastructure automation is a
big part of planning and designing cloud
infrastructure AWS allows you to reduce
the level of manual interaction in your
environment using automation you can
react to a variety of events without
having to do anything let's take a look
at some examples AWS elastic Beanstalk
developers can act with their code and
the Beanstalk service automatically
handles resource provisioning
auto-scaling load balancing and
monitoring ec2 auto recovery in some
situations you can use auto recovery to
automatically recover an ec2 instance if
it becomes impaired the recovered
instance is identical to the original
instance name IP address metadata etc
also scaling you can automatically add
resources to cope with demand spikes and
decrease again during quiet times cloud
watch alarms and events cloud works can
send SMS notifications to alert when a
particular event occurs for example CPU
usage is too high the notifications can
be used to perform follow-up actions
like run a lambda function opsworks
lifecycle events
you can continually update your
instances configuration to adapt to
environment changes for example if a new
database instance is added to your
environment then an event can
automatically trigger a chef repeat the
points existing applications for the new
server
loose coupling is an important concept
application should be designed so that
they're broken into smaller loosely
coupled components the desired outcome
is that a failure in one component
should not cause other components to
fail this is achieved through a variety
of measures firstly well-defined
interfaces by ensuring that all
components only interact with each other
through specific technology agnostic
interfaces for example restful api s
will result in being able to modify
resources without affecting other
components Amazon API gateway is a fully
managed service for a pis which you can
do this with loose coupling need
services to be able to interact with
each other without having any prior
knowledge of their existence for example
imagine two servers communicating on
hard-coded IP addresses then the
addition or modification of resources
will result in manual interaction to
update the configuration however loosely
coupled infrastructure is an essential
ingredient in making the most of cloud
computing elasticity the easiest way to
achieve this is to use an elastic load
balancer service to provide a stable
endpoint for all your services this way
they don't have to worry about each
other the elastic load balancer does
that for them rather than use
synchronous integration where server a
completes an action and passes it to
server B which then passes it to service
C asynchronous integration involves the
use of an intermediate storage area like
an S qsq this approach means that when a
server a completes its action it sends a
notification to sqs this way the compute
resources are decoupled and not directly
linked to each other this means you can
easily add or remove resources without
having to update the configuration of
the existing compute resources designing
applications to fail gracefully allows
other resources to continue a service
for about causing a complete outage
examples of this would be if your
primary website fails the route 53 DNS
failover feature points your users to a
back
site hosted on a static website on
Amazon s3 the idea is to continue to
provide a service even if it isn't the
full experience
traditional IT infrastructure involves
developing managing and operating
applications with a wide variety of
underlying technology components AWS
isn't just about ec2 instances there are
sweeter products to do everything
mentioned and a lot more besides to help
you lower your IT costs and allow you to
move faster as an organization for
example managed services AWS provides
many services that are fully managed
databases machine learning analytics
search email etc this lessens the burden
on your IT infrastructure department RDS
databases mean you don't have to worry
about backups fault tolerance for your
simple databases s3 allows you to store
as much data as you need without having
to concern yourself with capacity
management service architectures you
cannot reduce operational complexity of
your applications for the use of service
architectures tools such as the Internet
of Things
Vander s3 Beanstalk all allow you to run
your applications without paying for any
server infrastructure in a traditional
IT environment the type of database in
use is typically restricted by
constraints on licensing support
capabilities hardware availability and
things like that
AWS provides you with multiple managed
database server options that remove
these constraints meaning that you can
choose exactly the right database field
needs rather than making use of what is
available to you AWS offers fully
managed easy scalable services for
relational databases no SQL data
warehouses and search functions that
provide fully managed easily scalable
services that can create synchronously
replicated and buy instances in
different availability zones and they'll
also automatically failover
without the need for manual intervention
will cover databases in a whole lesson
later on in this course now we'll take a
look at redundancy introducing
redundancy will ensure your systems are
highly available so they can withstand
the failure of individual or multiple
components for example hard
disks servers or network links
introducing redundancy will remove
single points of failure from your
systems this is achieved
having multiple resources for the same
tasks in either standby or active mode
standby redundancy is often used for
stateful components like databases the
standby resource becomes the primary
resource quite failing over to it the
standby resource can be brought online
only when required to save cost or it
can already be running to speed up the
failover process and minimize the outage
active redundancy is where multiple
redundant compute resources share
requests and are able to absorb the loss
of one or more of the instances failing
examples of this would be multiple web
servers sitting behind a load balancer
automatic failure detection allows you
to react to an outage automatically
without the need for manual intervention
services like elastic load balancer and
route 53 that you configure health
checks so you can automatically route
traffic to healthy resources for example
signing up a health check to test for a
HTTP 200 response which means ok would
allow AWS to know if a node is healthy
or not services such as opsworks elastic
load balancer an ec2 auto recovery will
let you replace unhealthy resources with
brand-new healthy ones durable data
storage is an important part of planning
your cloud resources replicating the
data to other sites or resources will
protect its availability and integrity
there's three ways you can replicate
your data synchronously asynchronously
or quorum based
synchronous replication ensures that
your data has been durably stored on
both the primary and replication
locations any right operation will only
be acknowledged as complete when this
has taken place this should only be used
on low latency network connections where
absolutely necessary to try to ensure
maximum performance acing from this
replication decouples the primary node
from the replicas so that a write
operation doesn't have to wait for any
acknowledgment this can introduce a
replication lag but does not impact
performance and from personal experience
I've seen the difference between
synchronous and asynchronous replication
changing performance as much as a 100%
quorum based replication is a mix of
both synchronous and asynchronous
replication replication to multiple
nodes is controlled by defining the
minimum number of nodes that must
participate in a successful write
operation so if you have a cluster of 4
replicas you can say I need confirmation
from at least two of them before the
acknowledgement is sent back to the end
user a traditional data center failover
scenario involves failing over all your
resources to a secondary distant data
center because of the long distances
between the two data centers synchronous
replication is often impractical slow
and usually involves data loss and isn't
tested very often however it does
protect against low probability
scenarios such as a natural disaster so
if your main data center is in New York
and your secondary was in say Washington
if New York had a massive outage you
could fail over the Washington but it's
going to be slow and practical and take
some time a more likely scenario is a
shorter interruption in your data center
which isn't predicted to be too long in
this situation your choice is to sit it
out and wait or initiate the complex
failover procedure AWS data centers are
configured to provide multiple
availability zones in a region with low
latency network connectivity this means
you can replicate your data across data
centers in a synchronous manner and as a
result failover becomes much
also this concept is baked into many AWS
services such as RDS and s3 active
redundancy is great for disaster
recovery situations or balancing traffic
but what if all the requests to your
resources for harmful for example if a
particular request caused the bug and it
resulted in multiple instances crashing
a practice called shuffle sharding means
only sending some of the requests to
some of your resources this way if one
shard of resources is infected or down
the other shard of resources will be up
and running in the example shown here if
request 2 is bad then at least one set
of resources is still protected AWS
economies of scale offer organizations
huge opportunities to make cost savings
by making further use of AWS
capabilities there's even more scope to
create cost optimized cloud
architectures right sizing is one such
approach and start contrast at
traditional IT infrastructure cloud
computing means you can select the most
cost-effective resource and
configuration to fit your requirements
ec2 RDS redshift and elastic search give
you a wide variety of instant types to
choose from for a benchmarking you can
determine the optimal configuration for
your workload in some cases it might be
optimal to select many of the cheapest
instance type available in others it
might be more beneficial to select fewer
instances but of a larger instance type
AWS Storage offers the same level of
right sizing opportunities Amazon s3
offers many different storage classes to
suit your needs
EBS also comes in a variety of different
volume types all of which will be
covered in later lessons
AWS offers many elasticity options to
help you save money you can also scale
your ec2 workloads horizontally or
vertically you can turn off production
workloads when you don't need them and
where possible you can use lambda
compute workloads so you never have to
pay for idle resources
easy to on-demand instance pricing means
you only pay for what you use with no
long-term commitments however there are
two more ways to pay for receipt two
instances one of these is reserve
capacity by committing to a defined
period between 12 to 36 months you can
receive significantly discounted hourly
rates compared to on demand pricing AWS
trusted advisor or AWS ec2 usage reports
identify the resources that would
benefit from reserve capacity there is
technically no difference between
on-demand and reserved instances the
only difference is the way you pay for
it this concept also exists for redshift
RDS dynamodb and CloudFront ec2 spot
instances are ideal for workloads that
have flexible start and end times as you
are allowed to bid on spare ec2
computing capacity and spot instances
are often available at significant
discounts compared to on-demand pricing
your ec2 spot instance is launched when
your bid exceeds the current spot market
price and it will continue to run until
either you terminate it or the spot
market price exceeds your bid when the
latter happens your instance is
terminated and you'll not be charged for
the partial hour that it was running so
it's three strategies to use for spot
instances there's a one we've just
discussed which is bidding you could bid
much higher than the spot market price
for as long as the instant runs this way
even if the market price spikes
occasionally you'll still make a saving
overall the mixed strategy AWS
recommends designing applications that
use a mixture of reserved on demand and
spot instances this way you can combine
predictive capacity of opportunistic
access to cheaper additional compute
resources and then finally there's spot
blocks AWS allows you to bid for fixed
duration spot instances these have a
different hourly pricing but allow you
to define a duration requirement if your
bid is accepted your instance will run
until you terminate it or the duration
ends
now let's take a look at caching caching
data means storing previously calculated
data for future use so you don't have to
recalculate it again there's two
approaches application caching you can
design applications to store and
retrieve information from fast managed
in-memory caches this way an application
can look for results in the cache first
and if the data isn't there it can then
calculate it or retrieve the data and
store it in the cache for subsequent
requests this approach approves user
response times and reduces load on your
resources an example of this is Amazon
ElastiCache which is a service that
provides an in mem cache in the cloud
suitable for use with web services edge
caching uses Amazon CloudFront so you
can cache static content such as images
or video and dynamic content such as
live video around the world using edge
locations this way users can be served
the content that is closest to them and
resulting in low latency response times
the principle applies to both download
and upload with edge caching
and finally security it's important to
have security built into your plans and
designs for AWS infrastructure AWS
allows you to take most of the security
tools and techniques that you already
know and improve them by formalizing
security control design into your AWS
platform this simplifies the system for
administrators and auditors alike now
we're not going to cover too much here
because this is all covered in the I am
lesson but there's things like
defense-in-depth
which allows you to add multiple layers
of protection to your resources reduced
privileged access to only give people
the access they require security is a
code actually caps security requirements
in one script that you can use to
deploying new environments and real-time
auditing AWS has multiple services to
ensure you can test and alter your
environment in real time
AWS offers a wide and deep set of
functionality that allows you to
integrate your existing network security
data lifecycle management and resource
management capabilities with the cloud
AWS allows you to extend your existing
on-premise network configuration onto
your AWS virtual private clouds so the
your AWS resources will operate as if
they're part of your existing network
you can do this using Amazon virtual
private cloud VPC which is a
logistically isolated network in the
Amazon Cloud that gives you complete
control over your virtual networking
environment you can choose your own IP
range subnets route tables Network etc
then this direct connect rather than use
internet based connections between your
on-site resources and the Amazon Cloud
Direct Connect lets you establish a
dedicated network connection between the
two to provide lower costs and a higher
level of service you can use AWS to
reliably and cost-effectively backup and
secure your data AWS allows you to
replicate your data across geographical
regions manage the lifecycle of the data
or even synchronously replicate your
data to a local AWS data center AWS
Storage Gateway is a VM that you install
in your data center
and configure to be associated with your
AWS account then you can either use
Gateway cached which lets you use Amazon
s3 storage is your primary data storage
while retaining frequently accessed data
locally in your storage gateway or
gateway stored gateway stored volumes
that you stored a primary data locally
and asynchronously back that up to AWS
Amazon simple storage service s3 offers
a practically unlimited storage that's
available for backing up and archiving
your critical data and Amazon Glacia is
extremely low cost storage for
infrequently access data for which
recovery intervals of several hours are
suitable AWS security and access control
has easily managed using I am or
Microsoft Active Directory services AWS
Identity and Access Management is the
service that enables you to securely
control user access to all your AWS
services and resources and AWS directory
service is a managed service that allows
you to connect your AWS resources of an
existing on-premise Microsoft Active
Directory or you can set up a new
standalone directory in the AWS cloud
AWS security and access control is
easily managed using I am or Microsoft
Active Directory services AWS Identity
and Access Management is the service
that enables you to securely control
user access to all your AWS services and
resources an AWS directory service is a
managed service that allows you to
connect your AWS resources of an
existing on-premise Microsoft Active
Directory or you can set up a new
standalone directory in the AWS cloud
and the three flavors of Active
Directory as Microsoft ad which is a
fully blown managed Microsoft Active
Directory service that supports up to
50,000 users the simple ad which is a
standalone managed directory that's
available in two sizes smaller and large
small is for up to 500 users and large
supports up to 5,000 users all this ad
connector and this is a directory
gateway that allows you to proxy
directory
to your on-premise Microsoft Active
Directory and this also comes in two
sizes small and large a small ad
connector is designed for small
organizations of up to 500 users and a
large ad connector is designed for
larger organizations for up to 5,000
users
you can also use integrated resource and
deployment management tools in your
hybrid environment AWS provides
monitoring and management tools for
robust API s so you can easily integrate
your AWS resources of on-site tools and
most major software vendors already
support AWS like Microsoft VMware and
BMC pull such as AWS opsworks allow you
to deploy and operate applications in
the AWS cloud or in your own data center
you can use templates or build your own
tasks to specify an applications
architecture AWS code deploy automates
code deployments to instances in your
existing data center or in the AWS cloud
AWS code deploy simplifies the code
deployment process to your instances hey
once you become an expert in cloud
computing then subscribe to simpler
Channel and click here to watch more
such videos sooner it up and get
certified in cloud computing click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>