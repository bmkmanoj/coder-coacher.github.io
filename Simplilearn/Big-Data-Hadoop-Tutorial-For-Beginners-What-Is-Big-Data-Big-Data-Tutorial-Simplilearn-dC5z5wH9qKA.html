<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Hadoop Tutorial For Beginners | What Is Big Data? | Big Data Tutorial | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Big Data Hadoop Tutorial For Beginners | What Is Big Data? | Big Data Tutorial | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Hadoop Tutorial For Beginners | What Is Big Data? | Big Data Tutorial | Simplilearn</b></h2><h5 class="post__date">2017-12-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dC5z5wH9qKA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to the introductory
lesson of the Big Data and Hadoop
developer course offered by simply
learned in this lesson you'll get an
overview of the course just like the
cloud big data is a baffling tech term
to most people although it could be the
next big thing a buzzword of sorts not
many can give a definitive answer to
what it is if you happen to mention big
data you could be well subjected to
questions such as is it a tool a product
and is Big Data only for big businesses
and many more in the same vein but what
is Big Data it's just the huge amounts
of data both structured and unstructured
a business has to deal with on a daily
basis now that being said why is it
important to a business or for that
matter you to use this phenomenal amount
of data to its advantage a business must
fully grasp the possibilities of
analyzing big data and thereby making
informed decisions to stay ahead of the
curve there has to be a reason why many
analysts are calling Big Data a new
competitive advantage right let's look
at Gardner's three V's model to define
Big Data according to the American IT
research and advisory firm Gartner
incorporated big data is high volume
high velocity and/or high variety
information assets that demand
cost-effective innovative forms of
information processing that enable
enhanced insight decision-making and
process automation today the size volume
complexity variety and the rate of
growth velocity of the data that
organizations handle have reached such
unbelievable levels that traditional
processing and analytical tools fail
quite miserably are you still wondering
if you can come up with a rigorous
definition of Big Data yes then know
that you are not alone though people may
be divided on how best to define Big
Data few are likely to dispute that
these large pools of data are in too
to improve future projects but what is
certain is that understanding and
utilizing big data is a seriously
daunting task the sheer volume of data
that's captured and analyzed will pave
the way for a better economic
environment only if it's effectively
used to discern patterns that enable
better decision-making big data is not
just a buzzword or a fad it's something
that will undoubtedly touch businesses
and people's lives the world over for
people who think that this is just
another term that only the tech industry
seems to care about here are a few facts
about big data that will likely convince
them that big data is a revolution in
the making read on skeptics the data
volumes are exploding more data has been
created in the past two years than in
the entire previous history of the human
race if you burned all the data created
in just one day on two DVDs you could
stack them on top of each other and
reach the moon twice every minute we
send 204 million emails generate 18
million Facebook Likes and send 278
thousand tweets Google alone processes
on average over 40,000 search queries
per second making it over 3.5 billion in
a day around 100 hours of video are
uploaded on YouTube every minute and it
would take you around 15 years to watch
every video uploaded by users in just
one day today's data centers occupy an
area of land equal in size to almost six
thousand football fields by better
integrating big data analytics into
healthcare the industry could save 300
billion dollars a year that's equal to
reducing the healthcare costs of every
man woman and child by one thousand
dollars a year the image here gives a
very real picture of the evolution of
big data here the data size and
complexity have been mapped against
computing timeline in the early 1970s in
fact even before that the focus was on
generation and storage of data
mainframes were used for basic data
storage during the 1980s and 90s the
focus was on data utilization for which
relational databases and other data
intensive applications were brought into
the picture in the 2000s the spotlight
was on data driven technology for which
structured data unstructured data and
multimedia were used throughout this
evolution phase we saw an exponential
growth in data volume consider the
following case study in 2011 Netflix
made one of the biggest decisions it'll
ever possibly make it wasn't anything
material but rather it was about content
the u.s. headquartered company outbid
major cable networks like AMC and HBO to
win the rights for a US version of house
of cards getting them two seasons with
13 episodes each with a cost of four to
six million dollars per episode this two
season price tag was over 100 million
dollars Netflix had never before taken
such a big gamble on the content side so
why did Netflix make such a big bet and
how did big data analytics factored into
the decision let's find out now before
making the million dollar investment
Netflix knew a lot of users watched the
David Fincher directed movie the social
network from beginning to end the
British version of house of cards had
been well watched those who watched the
British version house of cards also
watched Kevin Spacey films and/or films
directed by David Fincher with the help
of big data analytics Netflix drew
conclusions from these three synergistic
factors before green lighting the
popular show combining these factors
obviously the popularity of political
thrillers makes it seem like an easy
decision for Netflix to make licensing
movies from Studios is expensive so
Netflix uses data which is generated
from its sixty nine point one seven
million worldwide streaming customers to
help pick the movies from the limited
ones to licence that the viewers will
most enjoy do you have data and use
to help you make decisions if not
Netflix provides a good case for why you
should so what are the predictions for
big data in the near future
market size according to the
International Data Corporation IDC the
big data and analytics market will reach
125 billion dollars worldwide in 2015
with growth at six times faster than IT
skills gap according to the McKinsey
Global Institute by 2018 the United
States alone could face a shortage of
140,000 to 190,000 people with big data
developer skills as well as 1.5 million
managers and analysts with the know how
to use the analysis of big data to make
effective decisions buying and selling
data will soon become the new bread and
butter for business across the globe big
data and hadoop certification training
from simply learn is designed to ensure
that you are job ready to take up an
assignment in big data this training not
only equips you with essential Hadoop
skills but also gives you the required
work experience in big data analytics
via implementation of real-life industry
projects that span three months after
completing this course you'll be able to
master the concepts of the Hadoop
framework and its deployment in a
cluster environment learn to write
complex MapReduce programs perform data
analytics using pig and hive acquire
in-depth understanding of the Hadoop
ecosystem including flume and Apache
easy workflow scheduler master advanced
concepts of Hadoop 2.7 including HBase
zookeeper and scoop get hands-on
experience in setting up different
configurations of the Hadoop cluster
work on real-life industry based
projects using Hadoop
the course flow is as follows first
internalize the domains which includes
introduction to Big Data and Hadoop
concepts and architecture of big data
tools installation and configuration of
Hadoop components such as MapReduce yarn
HDFS Pig hive a
space zookeeper scoop and flume and
Hadoop administration troubleshooting
and security the next step is to master
the components which includes Hadoop 2.7
Yarn MapReduce HDFS Pig hive HBase
zookeeper scoop and flume next you'll
learn to manage the following structured
unstructured and semi-structured data
Hadoop ecosystem and complex business
problems deployment of Hadoop cluster
and writing complex MapReduce programs
you'll then work on projects the
different projects in this course
include optimizing data storage using
HDFS in cloud environment using
MapReduce to sort and filter data using
pig and hive as replacement of Java
using HBase to process data in real time
using flume and a scoop to process
unstructured data and so on once these
steps are covered you are all ready to
take the last step which is to get
certified this course will facilitate
you to achieve Big Data Hadoop
development certification with Hadoop
2.7 after successfully completing a full
scale industry project and an objective
test to 45 questions this concludes the
course introduction the next lesson is
introduction to Big Data and Hadoop
wishing you all the best for this course
happy learning hi everyone and welcome
to Big Data Hadoop developer course
offered by simply learn now in this
lesson we'll be doing the introduction
to Big Data and Hadoop and we'll provide
an introduction to Big Data furthermore
it'll introduce Hadoop as a Big Data
technology after completing this lesson
you will be able to explain data
explosion and the need for Big Data
explain the concept of Big Data describe
the basics of Hadoop and describe the
history and milestones have had to
explain how to use oracle virtualbox to
open a vm over the last decade there's
been an incredible explosion of data in
just about every sector it's estimated
that over 2.5 exabyte or 2.5 billion
gigabytes of
is generated daily from various sources
that's a lot of data now here are some
of the sources of this big data
explosion a large stock exchange such as
the New York Stock Exchange generates
more than one terabyte of data daily
worldwide there are approximately five
billion mobile phones in use of these
nearly 1.75 billion are estimated to be
smartphones YouTube users upload more
than 48 hours of video every minute
every second of HD video generates bytes
2,000 times more than that required to
store a single page of text large social
networks like Twitter and Facebook
generate more than 10 terabytes of data
daily there are more than 30 million
networked sensors in the world and each
of them transmits data continuously
there are three primary types of data
the first is structured data which can
be represented in a tabular format and
stored in relational databases like
MySQL Oracle database and db2
the second one is semi structured data
which doesn't have a formal data model
though it may have some semblance of a
structure for example XML files
represent semi structured data now the
third is unstructured data which doesn't
have a predefined data model text files
web logs and machine logs are good
examples of unstructured data did you
know that according to an estimation
approximately 90% of the world's data
has been created in the last two years
it includes 10% of structured data and
80% of unstructured data that are
difficult to analyze the structured
formats such as databases have
limitations while handling large data
sets and there's difficulty in
integrating distributed information
furthermore most business users aren't
aware of the requirements during IT
system development potentially valuable
data for varied systems like enterprise
resource planning or ERP and supply
chain management or SCM are either
dormant or discarded forming a data
puddle within enterprises it's often to
accept
to justify the integration of large
volumes of unstructured data therefore
big data is needed to analyze and
integrate the enterprise's large data
sets irrespective of the data types big
data is a broad term for large or
complex data sets that are difficult to
be sorted using on hand data management
tools or traditional data processing
applications various sources of big data
are web logs social media sensor
networks internet text pages and
documents search index data atmospheric
science astronomy biochemical medical
records scientific research military
surveillance and photography archives
here are the characteristics of big data
variety encompasses managing the
complexity of data in different
structures ranging from relational data
to logs and raw text velocity accounts
for managing the streaming of data and
the movement of voluminous data at a
high speed volume denotes managing the
huge scaling of data ranging from
terabytes to zettabytes voluminous data
is useless without accuracy as incorrect
data causes concern in organizations
veracity manages accuracy of data and
its analyses especially an automated
decision-making process data veracity is
essential at the organization aims to be
information centric and here are more
characteristics of big data big data is
variable which means the meaning of the
data is constantly changing this makes
it relevant in sentiment analyses to
perform a proper sentiment analyses
algorithms need to understand the
context and decipher the exact meaning
of a word in that context visualization
makes voluminous data comprehensible
with correct analyses and visualizations
raw data can be used visualizations mean
complex graphs that can include
variables of data while still remaining
understandable and readable the value of
data is in analyses which turns data
into information followed by knowledge
based on data value organizations can
become
information-centric big data technology
helps to respond to the big data
characteristics now here are the three
characteristics of big data technology
firstly it helps to cost-effectively
process the growing volumes of data for
example per IBM big data technology has
helped to turn the 12 terabytes of
tweets created daily into improved
product sentiment analysis it has
converted 350 billion annual meter
readings to predict power consumption
better the second characteristic of Big
Data technology is that it helps to
respond to the increasing velocity of
data for example it's scrutinized
5-million trade events created daily to
identify potential frauds it's helped to
analyse 500 million daily call detail
records in real time to predict customer
churn faster the third characteristic of
Big Data technology is that it helps to
collectively analyze the widening
variety of data and handles failure of
isolated nodes and tasks assigned to
such nodes it can also turn data into
actionable insights for example it's
helped to monitor hundreds of live video
feeds from surveillance cameras to
target points of interest for security
agencies it has also been able to
exploit the 80% data growth in images
videos and documents to improve customer
satisfaction according to Gartner big
data is a high volume high velocity and
high variety information asset that
demands cost-effective innovative forms
of information processing for enhanced
insight and decision making Big Data
technology enables IT to leverage
multiple sources of data now the major
sources of data include application data
which has high volume and throughput and
it's structured machine data which has
high velocity is semi structured and
needs to be ingested at a high speed
social data which has high variety is
unstructured and requires you to
establish the veracity of the data
enterprise data which has variety is
available in different formats like PDF
spreadsheets and documents it's highly
unstructured and can be voluminous
here are the requirements of the
traditional IT analytics approach and
it's challenging factors the traditional
IT analytics approach requires the
business team to define questions before
IT development furthermore the team
needs to predefined the data sources and
structures the business teams usually
challenged if it has iterative and
volatile requirements or if data sources
keep changing let's now look at the
process involved in traditional IT
analytics approach in a scenario of
traditional IT systems development the
requirements are defined first this is
then followed by the solution design and
build once the solutions implemented
queries are executed if there are new
requirements or queries the system is
redesigned and rebuilt here are the
requirements and the challenging factors
that have to be overcome for using Big
Data technology as a platform for
discovery and exploration the Big Data
technology approach requires the
business team to define data sources and
establish a hypothesis big data
technology should enable explorative
analysis the IT team should integrate
data systems and sources as required
based on new business questions and the
hypotheses let's now look at the process
involved in Big Data technology approach
at the outset the initial data sources
are identified the IT team creates a
platform for creative exploration of
available data and content the business
team then determines the questions to
ask and tests their hypotheses any new
question leads to the addition and
integration of data sources without the
need to redesign and rebuild the
platform Big Data technology helps to
understand and navigate big data sources
manage and store a huge volume of
various data processed data in a
reasonable time ingest data at a high
speed analyze unstructured data and bear
faults and exceptions
okay now Big Data technology appeals to
various sectors and it's used for a
variety of use cases per cloud era the
technology has found uses in industries
such as automotive community
occassions consumer packaged goods
financial services education and
research high technology and industrial
manufacturing life sciences media and
entertainment online services in social
media healthcare oil and gas retail
travel and transportation utilities and
law enforcement in defense now there are
two key challenges to be addressed by
Big Data technology these are handling
the system uptime and downtime and
combining data accumulated from all
systems to overcome the first challenge
Big Data technology uses commodity
Hardware for data storage and analysis
plus it helps to maintain a copy of the
same data across clusters to overcome
the second challenge Big Data technology
analyzes data across different machines
and then merges that data Hadoop helps
to leverage the opportunities provided
by Big Data and overcome the challenges
it poses let's now look at the
definition of Hadoop and its requirement
Hadoop is an open source java based
programming framework that supports the
processing of large data sets in a
distributed computing environment it's
based on Google file system or GFS the
next question to be addressed is why
Hadoop is used Hadoop runs a number of
applications on distributed systems with
thousands of nodes involving petabytes
of data it has a distributed file system
called the Hadoop distributed file
system or HDFS which enables fast data
transfer among the nodes furthermore it
leverages a distributed computation
framework called MapReduce Hadoop
originated from the nuch open source
project on search engines and works over
distributed network nodes in 2003 in
2004 Google released two papers that
provided insight into their success the
Google file system or GFS and MapReduce
simplified data processing on large
clusters the papers told the world how
Google performed large-scale data
processing
in July 2005 nuts used GFS to perform
MapReduce operations in February 2006
nuch started a leucine sub project which
led to the era of Hadoop in April 2007
Yahoo started using Hadoop on a 1,000
node cluster in January 2008 Apache took
over Hadoop and made it a top-level
project in July 2008 a 4,000 node
cluster with Hadoop was tested by Apache
the performance of that cluster was
surprisingly the fastest when compared
to the other technologies implemented
that year in May 2009 tests revealed
that Hadoop successfully sorted a
petabyte of data in 17 hours Hadoop
reached version 1.0 in December 2011 it
is open source in written in Java on May
23rd 2012 Hadoop 2.0 point 0 was
released which delivers significant
features including yarn high
availability for HDFS HDFS Federation
HDFS snapshots NFS v3 access to data and
HDFS support for running Hadoop on
Microsoft Windows binary compatibility
for MapReduce applications built on
Hadoop 1 dot X and substantial
integration testing with the rest of the
projects in the ecosystem the current
version of Hadoop is 2.7 point one it
was released on the 6th of July 2015 it
is completely open source and written in
Java Yahoo was the first company to
design and use Hadoop as a core part of
their system operations now Hadoop is a
core part of systems at Internet
companies such as Facebook LinkedIn
Twitter and many enterprise
organizations such as JP Morgan and
Chevron vmware player is a free software
package offered by vmware incorporated
used to create and manage virtual
machines the vmware player can be
downloaded for personal use from the URL
mentioned here the image here shows you
where to access cloud lab from click lab
acts as
to get the screen similar to the image
here the basic hardware requirements for
working on VMware Player are as follows
one gigahertz processor to support Intel
VT or virtualization technology one
gigabyte of RAM and 150 megabyte hard
disk to install the application however
it is recommended that you have a two
gigahertz processor and four gigabytes
of RAM for optimum performance the
Oracle VirtualBox is used to open
virtual machines Apple Macintosh users
use the free VM player as the VMware
Fusion player is available only for
trial or limited usage download the
latest version of VirtualBox compatible
with your machine and firewall settings
from the URL mentioned here
now let's summarize the topics we talked
about in this lesson
big data is a term applied to datasets
that cannot be captured managed and
processed within a tolerable elapsed and
specified time frame by commonly used
software tools big data relies on volume
velocity and variety with respect to
processing data can be divided into
three types unstructured semi
unstructured and structured data big
data technology understands and
navigates big data sources analyzes
unstructured data and ingests data at a
high speed Hadoop is a free Java based
programming framework that supports the
processing of large datasets in a
distributed computing environment this
concludes introduction to Big Data and
Hadoop in the next lesson
we're gonna focus on Hadoop architecture
hey everyone and welcome to Big Data and
Hadoop developer course offered by
simply learn this lesson Hadoop
architecture will provide detailed
information regarding the Hadoop
architecture after completing this
lesson you'll be able to describe the
use of Hadoop and commodity hardware
explain the various configurations and
services of Hadoop differentiate between
a regular and a Hadoop distributed file
system explain HDFS architecture an
understanding of the key terms is
essential when discussing Hadoop
architecture the term commodity Hardware
refers to regular PCs which can
successfully be used to make a cluster
the word cluster refers to multiple
systems logically interconnected in the
same network for a distributed mode
process nodes are the commodity servers
which are interconnected through a
network device Hadoop is a framework
which is often used in commodity
hardware it supports the concept of
distributed architecture the diagram you
see here represents the cluster of nodes
connected and installed with Hadoop the
maximum number of nodes that can reside
in a single rack is approximately 30 to
40 however it's not an exact count you
can have more nodes if you have adequate
network speed normally the uplink former
act a node is 3 to 4 gigabits
second and the uplink from racked Iraq
internally is one gigabit per second
Hadoop supports three configuration
modes when implemented on commodity
hardware standalone pseudo distributed
and fully distributed mode click each
tab to know more
now in a standalone mode all Hadoop
services run in a single Java Virtual
Machine or JVM on a single machine
in a pseudo distributed mode each Hadoop
service runs in its own JVM but on a
single machine
and finally in a fully distributed mode
the Hadoop services run in individual
JVMs but these JVMs reside on different
commodity hardware in a single cluster
the core Hadoop services are named node
data node Resource Manager application
master node manager and secondary name
node generally in the fully distributed
mode the name node secondary name node
and application master are identified as
master services while the data node and
node manager are classified as slave
services there are two major components
of Apache Hadoop Hadoop distributed file
system or HDFS and Hadoop MapReduce HDFS
is used to manage the storage aspects of
Big Data and MapReduce is responsible
for processing jobs in a distributed
environment prior to 2011 storing and
retrieving petabytes or Zeta bytes of
data had three major issues cost speed
and reliability a traditional file
system approximately costs ten thousand
to fourteen thousand dollars per
terabyte searching and analyzing data
was both time-consuming and expensive
and if search components were saved in
different servers fetching data was a
probability things would break every day
in different way here HDFS or Hadoop
distributed file system comes into the
picture
it's major USPS r1 it is designed to run
on regular commodity Hardware that costs
around four thousand dollars per
terabyte since it is an open source
software it can be used with zero
licensing and support costs this cost
advantage lets organizations store and
process more data per dollar than
traditional systems in big data
deployments the storage cost often
determines viability of the system - it
can easily deliver more than two
gigabits of data per second per computer
- MapReduce this means large Hadoop
clusters can read or write more than a
terabyte of data per second 3a copies
data multiple times and distributes the
copies to individual nodes placing at
least one copy on a different server as
a result the data on nodes that crash
can be found within the cluster which
allows processing to continue while the
failure is resolved HDFS is
distributed file system that provides
great access to data across Hadoop
clusters like other Hadoop related
technologies HDFS is a key tool for
managing pools of big data and
supporting big data analytics
applications a college library was
gifted a massive collection of books by
a patron the books were very popular
titles the librarian decided to arrange
the books in a small rack and distribute
multiple copies of each book and other
racks so that students can find the
books easily similarly HDFS creates
multiple copies of a data block and
keeps them in separate systems for easy
access the table you see here compares a
regular file system with HDFS in a
regular file system each block of data
is small usually about 51 bytes
however in HDFS each block is of 64
megabyte by default a regular file
system provides access to large data but
this feature of the file system may
suffer from disk i/o problems mainly due
to multiple seek operations HDFS can
read large quantities of data
sequentially after a single seek this
makes it unique since all of these
operations are performed in distributed
mode the basic characteristics of HDFS
that make it popular our first HDFS has
a high fault tolerance an HDFS instance
may consist of thousands of server
machines each storing a part of the file
systems data there exists numerous
components and each has a probability of
failure and non functionality the core
architectural goals of HDFS are
detection of faults and quick automatic
recovery second HDFS provides high
throughput HDFS is designed to store and
scan millions of rows of data and
account or add some subsets of data the
time required in this process is
dependent on the involved complexities
third HDFS is suitable for applications
with large datasets
traditionally HDFS has been designed to
support high throughput of datasets in
batch style jobs fourth HDFS is suitable
for
applications with streaming access to
filesystem data they're not targeted for
general purpose applications HDFS is
designed more for batch processing
rather than for interactive use the
emphasis is on high throughput of data
access rather than low latency of data
access finally HDFS is designed in such
a way that it can be built on commodity
hardware and heterogeneous platforms the
key features of HDFS are as follows
it creates multiple replicas of each
data block and distributes them on
computers throughout a cluster to enable
reliable and rapid data access it's the
storage system for both input and output
of MapReduce jobs this file system can
be accessed by using the HDFS protocol
to perform input-output operation on the
file which is stored in HDFS block
storage metadata controls the physical
location of a block and its replication
within the cluster each block is
replicated to a few physically separate
machines however the count can be
modified by the administrator as we
discussed earlier HDFS architecture
consists of name node and secondary name
node services which constitute the
master services the data node service
falls under the slave service the master
service is responsible for accepting a
job from clients it has the task of
ensuring that the data required for the
operation is loaded and segregated into
chunks of data blocks HDFS exposes a
file system namespace and allows user
data to be stored in files a file is
split into one or more blocks stored in
replicated in data nodes the data blocks
are then distributed to the data node
systems within the cluster this ensures
that replicas of the data are maintained
prior to Hadoop 2.00 the name node was a
single point of failure or SPO F in an
HDFS cluster each cluster had a single
name node and if that machine or process
became unavailable the cluster as a
whole would be unavailable until the
name node was either
restarted or brought up on a separate
machine this impacted the total
availability of the HDFS cluster in two
major ways in the case of an unplanned
event such as a machine crash the
cluster would be unavailable until an
operator restarted the name node planned
maintenance events such as software or
hardware upgrades on the name node
machine would result in Windows of
cluster downtime the HDFS high
availability or H a feature addresses
these problems by providing the option
of running two redundant name nodes in
the same cluster in an active/passive
configuration with a hot standby now
this allows a fast failover to a new
name node in the case that a machine
crashes or a graceful administrator
initiated failover for the purpose of
planned maintenance in a typical H a
cluster two separate machines are
configured as name nodes and at any
instance one of the name nodes is in an
active state and the other is in a
standby state the active name node is
responsible for all client operations in
the cluster
while the standby is simply acting as a
slave maintaining enough state to
provide a fast failover if necessary
Hadoop supports two major
implementations one quorum based storage
and two shared storage using NFS quorum
based storage refers to the H a
implementation that uses quorum journal
manager or qjm for the standby node to
keep it state synchronized with the
active node in this implementation both
nodes communicate with a group of
separate daemons called journal nodes
when any namespace modification is
performed by the active node it durably
logs a record of the modification to a
majority of these journal nodes the
standby node is capable of reading the
edits from the journal nodes and is
constantly watching them for changes to
the edit log as the standby node sees
the edits it applies them to its own
namespace in the event of a failover the
standby will ensure that it has read all
the edits from the journal nodes before
promoting itself to the active State now
this ensure
that the namespace state is fully
synchronized before a failover occurs in
shared storage using NFS for the standby
node to keep its state synchronized with
the active node this implementation
requires both the nodes to have access
to a directory on a shared storage
device for example an NFS mount from an
NA s HDFS components include different
servers such as name node data node and
secondary name node server the name node
server is the core component of an HDFS
cluster there can be just one name node
server in an entire cluster it's
responsible for maintaining the file
system namespace the name node helps to
execute file system namespace operations
like opening closing and renaming of
files and directories which are present
in HDFS information of the data and the
metadata is stored in the namespace
image and the edit log it also
determines the linking of blocks to data
nodes furthermore the name node is aware
of the data node status that is it knows
the data node in which certain data is
stored it's also a critical single point
of failure if it fails the entire
cluster goes down however the name node
server can be partially restored by
using a secondary name node server the
data node is a multiple instance server
there can be n number of data node
servers depending on the type of
networking and storage system the data
node servers are responsible for storing
and maintaining the data blocks which
are provisioned by the name node server
on the basis of the type of job
submitted by the client it's also
responsible for storing and retrieving
the blocks when referred by clients or
the name node furthermore the data node
servers read write requests and perform
block creation deletion and replication
upon instruction from the name node
there can be only one secondary name
node server in a cluster it's
responsible for maintaining a backup of
the name node server however you cannot
treat the secondary name node server as
a disaster recovery
server it will partially restore the
name node server in case of failure the
secondary name node server maintains the
edit log and namespace image information
which is synced with the name node
server now at times the namespace images
from the name node server are not
updated so you cannot totally rely on
the secondary name node server for the
recovery process HDFS exposes a file
system namespace and allows user data to
be stored in files HDFS has a
hierarchical file system with
directories and files the name node
manages the file system namespace
allowing clients to work with files and
directories subtrees a file system
supports operations like create remove
move and rename the name node apart from
maintaining the file system namespace
records any changes to metadata
information the name node maintains to
persistent files a transaction log
called an edit log and a namespace image
called
an FS image the edit log records every
change that occurs in the file system
metadata such as creating a new file the
edit log is stored in the name nodes
local file system the entire file system
namespace including mapping of blocks
files and file system properties is
stored in FS image this is also stored
in the name nodes local file system
however metadata loads into its memory
the blocks of data that reside on a
specific data nodes at startup when the
new data nodes join a cluster metadata
then periodically loads at user-defined
or default intervals when the name node
starts up it retrieves the edit log and
FS image from its local file system it
then updates the FS image with edit log
information and stores a copy of the FS
image on the file system as a checkpoint
the metadata size is limited to the RAM
available on the name node a large
number of small files would require more
metadata than a small number of large
files
hence the in-memory metadata management
issue explains why HDFS favors a small
number of large files if a name node
out of RAM it'll crash and applications
won't be able to use HDFS until the name
node is online again each file is split
into one or more blocks stored and
replicated in data nodes data nodes
manage names and locations of file
blocks each file block is 64 megabytes
by default now for large files it's
better to increase the block size to 128
megabytes doing this decreases pressure
on the name nodes memory but this
potentially reduces the amount of
parallelism that can be achieved as the
number of blocks per file decreases each
map task operates on one block so if
tasks are lesser than nodes in the
cluster the jobs will run slowly
however this becomes a lesser issue when
the average MapReduce job involves more
files or larger individual files some
benefits of the data block approach
include simplified replication fault
tolerance and reliability and shielding
users from storage subsystem details in
HDFS unstructured data is represented in
the form of data blocks if there's a
default size for the data blocks it can
be reset by the user or the
administrator block replication refers
to the creation of replicas of a block
in multiple data nodes usually the data
is split in the form of parts such as
Part C row and part one HDFS performs
block replication on multiple data nodes
so that if any error exists on one data
node server the jobtracker service which
is present in the name node server will
resubmit the job to another data node
server each file is split into a
sequence of blocks except the last one
all blocks in the file are of the same
size now blocks are replicated for fault
tolerance the block replication factor
is usually configured at the cluster
level but it can also be configured at
the file level the name node receives a
heartbeat and a block report from each
data node in the cluster a block report
contains all of the blocks on a data
node an application can also specify the
number of replicas of the file needed
such as the rep
factor of the file this information is
stored in the name node one of the
suggested replication topologies could
be having the first replica placed on
the same node is the client the second
replica is placed on a rack different
from the first rack
the third replica is placed on the same
rack as the second but on a different
node on the chart shown here you can see
a Hadoop cluster with three racks
represented by dark blue pillars each
rack contains multiple nodes represented
by rectangular boxes for example you'll
see a box labeled r1 and one which
represents node one on rack one to make
it simple each rack shown here has eight
nodes at the top you'll see the name
node shown in yellow follow the path of
a single data block block b1 shown in
purple on this chart b1 is first written
to node four on rack 1a copies then
written on a different rack rack to more
specifically it's written to node five
on rack to the third and final copy of
the block is written to the same rack as
the second copy rack - but to a
different node node one HDFS provides
various access mechanisms a Java API can
be used for applications there's also a
Python and C language wrapper for non
java applications a web GUI can also be
utilized through an HTTP browser an FS
shell is available for executing
commands on HDFS Olivia Tyler is the EVP
of IT operations with nutria worldwide
Incorporated and she's decided to use
HDFS for storing big data she'll use
HDFS shell to store the data in a Hadoop
file system and she'll execute various
commands on it
okay time to summarize the topics we
covered in this lesson Hadoop works on
three configurations standalone pseudo
distributed and fully distributed mode
the two core components of Apache Hadoop
are HDFS and MapReduce the HDFS layer
contains name node secondary name node
and data node HDFS can be used to handle
big data sets with its inherent features
of fault tolerance high throughput and
streaming data access hey want to become
an expert in Big Data then subscribe to
the simply learned Channel and click
here to watch more such videos to nerd
up and get certified in Big Data click
here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>