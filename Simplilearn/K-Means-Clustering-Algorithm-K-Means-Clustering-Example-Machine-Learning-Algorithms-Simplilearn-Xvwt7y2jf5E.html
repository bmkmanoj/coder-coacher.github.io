<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>K Means Clustering Algorithm | K Means Clustering Example | Machine Learning Algorithms |Simplilearn | Coder Coacher - Coaching Coders</title><meta content="K Means Clustering Algorithm | K Means Clustering Example | Machine Learning Algorithms |Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>K Means Clustering Algorithm | K Means Clustering Example | Machine Learning Algorithms |Simplilearn</b></h2><h5 class="post__date">2018-03-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Xvwt7y2jf5E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to the session on
k-means clustering I'm Mohan Kumar from
simply learn so what is k-means
clustering k-means clustering is an
unsupervised learning algorithm in this
case you don't have labeled data
unlike in supervised learning so you
have a set of data and you want to group
them and as the name suggests you want
to put them into clusters which means
objects that are similar in nature
similar in characteristics need to be
put together so that's what k-means
clustering is all about the term K is
basically is a number so you need to
tell the system how many clusters you
need to perform so if K is equal to 2
there will be 2 clusters if K is equal
to 3 3 clusters and so on and so forth
that's what the K stands for and of
course there is a way of finding out
what is the best or optimum value of K
for a given data we will look at that so
that is k-means clustering so let's take
an example k-means clustering is used in
many many scenarios but let's take an
example of cricket the game of cricket
let's say you received data of a lot of
players from maybe all over the country
or all over the world and this data has
information about the runs scored by the
people ordered by the player and the
wickets taken by the player and based on
this information we need to cluster this
data into two clusters batsman and
bowlers so this is an interesting
example let's see how we can perform
this so we have the data which consists
of primarily two characteristics which
is the runs and the wickets
so the bowlers basically take the chips
and the batsman score runs there will be
of course a few bowlers who can score
some runs and similarly there will be
some batsman who will who would have
taken a few wickets
but with this information we want to
cluster those players into batsman and
so how does this work let's say this is
how the data is so there are information
there is information on the y-axis about
the run scored
and on the x-axis about the wickets
taken by the players so if we do a quick
plot this is how it would look and then
we do the clustering we need to have the
clusters like shown in the third diagram
out here we need to have a cluster which
consists of people who have scored high
runs which is basically the batsman and
then we need a cluster with people who
are taken a lot of wickets which is
typically the bowlers there may be a
certain amount of overlap but we will
not talk about it right now so it came
in scoring we will have here that means
K is equal to 2 and we will have two
clusters which is batsman and bowlers so
how does this work the way it works is
the first step name came in scoring is
the allocation of two centroids randomly
so two points are assigned as so called
centroids so in this case we want two
clusters which means K is equal to two
so two points have been randomly
assigned as centroids keep in mind these
points can be anywhere there are random
points they are not initially they are
not really the centroid centroid means
it's a central point of a given data set
but in this case when it starts off it's
not really the centroid ok so these
points though in our presentation here
we have shown them one point closer to
these data points and another closer to
these data points they can be assigned
randomly anywhere okay so that's the
first step the next step is to determine
the distance of each of the data points
from each of the randomly assigned
centroids so for example we take this
point and find the distance from this
centroid and the distance from this
centroid this point is taken and the
distance is gone from this centroid in
this and so on and so forth so for every
point the
distance is measured from both the
centroids and then whichever distance is
less that point is assigned to that
centroid so for example in this case
visually it is very obvious that all
these data points are assigned to the
centroid and all these data points are
assigned to this centroid and that's
what is represented here in blue color
and in this yellow color the next step
is to actually determine the central
point or the actual centroid for these
two clusters so we have this one initial
cluster this one initial cluster but as
you can see these points are not really
the centroid centroid means it should be
the central position of this data set
central position of this data set so
that is what needs to be determined as
the next step so the central point on
the actual centroid is determined and
the original randomly allocated centroid
is the reposition to the actual centroid
of this new clusters and this process is
actually repeated now what might happen
is some of these points may get
reallocated in our example that is not
happening probably but it may so happen
that the distance is found between each
of these data points once again with
these centroids and if there is if it is
required some points may be reallocated
we will see that in a later example but
for now we will keep it simple so this
process is continued till the centroid
repositioning stops and that is our
final
cluster so this is our so after I
Croatian we come to this position this
situation where the centroid doesn't
need anymore repositioning and that
means our algorithm has converged
convergence has occurred and we have the
cluster two clusters we have the
clusters with a centroid so this process
is repeated the process of calculating
the distance and repositioning the
centroid is repeated till the
repositioning stops which means that the
algorithm has converged
we have the final cluster with the data
points and the centroids so this is what
you're going to learn from the session
we will talk about the types of
clustering what is k-means clustering
application of k-means clustering
k-means clustering is done using
distance measure so we will talk about
the common distance measures and then we
will talk about how k-means clustering
works and go into the details of k-means
clustering algorithm and then we will
end with the demo and a use case for
k-means clustering so let's begin first
of all what are the types of clustering
there are primarily two categories of
clustering hierarchical clustering and
then partitioning clustering and each of
these categories are further subdivided
into agglomerative and divisive
clustering and k-means and fuzzy c means
clustering let's take a quick look at
what each of these types of clustering
are in hierarchical clustering the
clusters have a tree like structure and
hierarchical clustering is further
divided into agglomerative and divisive
the glue Moretti of clustering is a
bottom-up approach we begin with each
element as a separate cluster and merge
them into successively larger clusters
so for example we have ABCDE F we start
by combining BMC on one cluster D and E
form one more then we combine de and F
one more bigger cluster and then add BC
to that and then finally a to it
compared to that divisive clustering or
divisive clustering is the top-down
approach we begin with the whole set and
proceed to divide it into successively
smaller clusters so we have ABCDE F we
first take that as a single cluster and
then break it down into a b c d e and f
then we have partitioning clustering
split into two subtypes k-means
clustering and fuzzy c means in k-means
clustering the objects are divided into
the number of clusters mentioned by the
number k that's where the K comes from
so if we say K is equal to 2 the objects
are divided into two clusters c1 and c2
and the way it is done is the features
or characteristics are compared and all
objects having similar characteristics
are clubbed together so that's how
k-means clustering is done we will see
it in more detail
as we move forward and fuzzy Siemens is
very similar to k-means in the sense
that it clubs objects that have similar
characteristics together but while in
k-means clustering two objects cannot
belong to or any object a single object
cannot belong to two different clusters
in C means objects can belong to more
than one cluster so that is the primary
difference between k-means and fuzzy c
means so what are some of the
applications of k-means clustering
k-means clustering is used in a variety
of examples or variety of business cases
in real life starting from academic
performance diagnostic system search
engines and wireless sensor networks and
many more so let us take a little deeper
look at each of these examples academic
performance so based on the scores of
the students students are categorized
into a b c and so on clustering forms a
backbone of search engines when a search
is performed the search results need to
be grouped together the search engines
very often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so plus ringing
specially k-means clustering uses
distance measure so let's take a look at
what is distance measure so while these
are the different types of clustering in
this video we will focus on k-means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measures
there is Euclidean distance there is
Manhattan distance then we have squared
Euclidean distance measure and cosine
distance measure these are some of the
distance measures supported by K
means clustering let's take a look at
each of these what is Euclidean distance
measure this is nothing but the distance
between two points so we have learnt in
highschool how to find the distance
between two points this is a little
sophisticated formula for that but we
know a simpler one is square root of y 2
minus y 1 whole square plus X 2 minus X
1 whole square so this is an extension
of this formula so that is the Euclidean
distance between two points what is the
squared Euclidean distance measure it's
nothing but the square of the Euclidean
distance as the name suggests so instead
of taking the square root we leave the
square as it is and then we have
Manhattan distance measure in case of
Manhattan distance it is the sum of the
distances across the x-axis and the
y-axis and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
k-means now let's go and check how
exactly k-means clustering works okay so
this is how k-means clustering works
this is like a flowchart of the whole
process there is a starting point and
then we specify the number of clusters
that we want now there are a couple of
ways of doing this we can do by trial
and error so we specify a certain number
maybe K is equal to 3 or 4 or 5 to start
with and then as we progress we keep
changing until we get the best clusters
or there is a technique called elbow
technique whereby we can determine the
value of K what should be the best value
of K how many clusters should be formed
so once we have the value of K we
specify that and then the system will
assign that many centroids so it picks
and Emily that to start with randomly
that many points that are considered to
be the centroids of these clusters and
then it measures the distance of each of
the data points wrong
these centroids and assigns those points
to the corresponding centroid from which
the distances minimum so each data point
will be assigned to the centroid which
is closest to it and thereby we have K
number of initial clusters however this
is not the final clusters the next step
it does is for the new groups for the
clusters that have been formed it
calculates the main position thereby
calculates the new centroid position the
position of the centroid moves compared
to the randomly allocated one so it's an
iterative process once again the
distance of each point is measured from
this new centroid point and if required
the data points are relocated to the new
centroids and the mean position or the
new centroid is calculated once again if
the centroid moves then the iteration
continues which means the convergence
has not happened the clustering has not
converged so as long as there is a
movement of the centroid this iteration
keeps happening
but once the centroid stops moving which
means that the cluster has converged or
the clustering process has converged
that will be the end result so now we
have the final position of the centroid
and the data points are allocated
accordingly to the closest centroid I
know it's a little difficult to
understand from this simple flow chart
so let's do a little bit of
visualization and see if we can explain
it better let's take an example if we
have a data set for a grocery shop so
let's say we have a data set for a
grocery shop and now we want to find out
how many clusters this has to be spread
across so how do we find the optimum
number of clusters there is a technique
called the elbe method so when this
Buster's are formed there is a parameter
called written sum of squares and the
lower this value is the better the
cluster is that means all these points
are very close to each other so we use
this within sum of squares as a measure
to find the optimum number of clusters
that can be formed for a given data set
so we create clusters or we let the
system create clusters of a variety of
numbers maybe above ten ten clusters and
for each value of K the within SS is the
measure and the value of K which has the
least amount of within SS or WS s that's
taken as the optimum value of K so this
is the diagonal metric representation so
we have on the y-axis the written sum of
squares or WS s and on the x-axis we
have the number of clusters so as you
can imagine if you have K is equal to
one which means all the data points are
in a single cluster the witnesses value
will be very high because they are
probably scattered all over the moment
you split it into two there will be a
drastic fall in the Biton SS value
that's what is represented here but then
as the value of K increases the decrease
the rate of decrease will not be so high
it will continue to decrease but
probably the rate of decrease will not
be high so that gives us an idea so from
here we get an idea for example the
optimum value of K should be either two
or three or at the most four but beyond
that increasing the number of clusters
is not dramatically changing the value
in WS s because that pretty much gets
stabilized okay
now that we have got the value of K and
let's assume that these are our delivery
points the next step is basically to
assign two centroids randomly so let's
say C 1 and C 2 are the centroid
assigned randomly now the distance of
each location from the centroid is
measured and each point is assigned to
the centroid which is closest to it so
for example these points are very
obvious that these are closes to c1
whereas this point is far away from c2
so these points will be assigned which
are close to c1 will be assigned to c1
and these points or locations which are
close to C 2 will be assigned to c2 and
then so this is the how the initial
grouping is done this is part of c1 and
this is part of c2 then the next step is
to calculate the actual centroid of this
data because remember c1 and c2 are not
the centroids there be randomly assigned
points and only thing that has been done
was the data points which are closest to
them have been assigned but now in this
step the actual centroid will be
calculated which may be for each of
these data sets somewhere in the middle
so that's like the mean point that will
be calculated and the centroid will
actually be positioned or repositioned
there same with c2 so the new centroid
for this group is c2 this new position
and semen is in this new position once
again the distance of each of the data
points is calculated from these
centroids now remember it's not
necessary that the distance still
remains the or each of these data points
still remain in the same group by
recalculating the distance it may be
possible that some points get
reallocated like so see this so this
point earlier was closer to C 2 because
C 2 was here
but after recalculating repositioning it
is observed that this is closer to see
what that C 2 so this is the new
grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is a repetative
process I creative process and if the
centroid
doesn't change once the centroid stops
changing that means
the algorithm has converged and this is
our final cluster with this as the
centroid c1 and c2 as the centroids
these data points as a part of each
cluster so I hope this helps in
understanding the whole process I Dre do
process of k-means clustering so let's
take a look at the k-means clustering
algorithm let's say we have X 1 X 2 X 3
n number of points as our inputs and we
want to split this into K clusters or if
we want to create K clusters so the
first step is to randomly pick K points
and call them centroids they are not
real centroids because centroid is
supposed to be a center point but they
are just called centroids and we
calculate the distance of each and every
input point from each of the centroids
so the distance of X 1 from C 1 from C 2
C 3 each of the distances we calculate
and then find out which distance is the
lowest and assign X 1 to that particular
random centroid repeat that process for
X 2 calculate its distance from each of
the centroid C 1 C 2 C 3 to CK and find
which is the lowest distance and assign
X 2 to that particular centroid same
with X 3 and so on so that is the first
round of assignment that is done now we
have K groups because there are we have
assigned the value of K so there are K
centroids and so there are K groups all
these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
have to calculate the actual centroids
for each of these groups which is like
the main position which means that the
position of the randomly selected
centroids will now change and they will
be the main positions of this newly
formed K groups and once that is done we
once again
this process of calculating the distance
right so this is what we are doing as a
part of step 4 we repeat step two and
three so we again calculate the distance
of x1 from the centroid c1 c2 c3 and
then see which is the lowest value and
assign X 1 to that calculate the
distance of X 2 from c1 c2 c3 or
whatever up to CK and find whichever is
the lowest distance and assign x2 to
that cetera and so on in this process
there may be some reassignment x1 probe
was probably assigned to cluster c2 and
after doing this calculation maybe now
x1 is assigned to c1 so that kind of
reallocation may happen so we repeat the
steps 2 &amp;amp; 3 till the position of the
centroids don't change or stop changing
and that's when we have convergence so
let's take a detailed look at it at each
of these steps so we randomly pick K
cluster centers we call them centroids
because they are not initially they are
not really the centroids so we let us
name them c1 c2 up to CK and then step 2
we assign each data point to the closest
Center so what we do we calculate the
distance of each x value from each C
value so the distance between X 1 c1
distance between x1 c2 x1 c3 and then we
find which is the lowest value and
that's the minimum value we find and
assign x1 to that particular centroid
then we go next to x2
find the distance of x2 from c1 x2 from
c2 x2 from c3 and so on up to CK and
then assign it to the point or to the
centroid which has the lowest value and
so on so that is step number two in step
number three we now find the actual
centroid for each group so what has
happened as a part of step number two we
now have all the points all the data
points grouped into K groups
because we wanted to create K clusters
right so we have K groups each one may
be having a certain number of input
values and they need not be equally
distributed by the way based on the
distance we will have K groups but
remember the initial values of the c1c2
were not really the centroids of these
groups right we assign them randomly so
now in step 3 we actually calculate the
centroid of each group which means the
original point which we thought was the
centroid will shift to the new position
which is the actual centroid for each of
these groups okay and we again calculate
the distance so we go back to step 2
which is what we calculate again the
distance of each of these points from
the newly positioned centroids and if
required we reassign these points to the
new centroids so as I said earlier there
may be a reallocation so we now have a
new set or a new group we still have K
groups but the number of items and the
actual assignment may be different from
what was in step 2 here okay so that
might change then we perform step 3 once
again
to find the new centroid of this new
group so we have again a new set of
clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of k-means clustering
we will actually
see some live demos and bitin or doc
using pattern notebook but before that
let's find out what's the problem that
we are trying to solve the problem
statement is let's say Walmart wants to
open a chain of stores across the state
of Florida and it wants to find the
optimal store locations now the issue
here is if they open too many stores
close to each other
obviously they will not make profit but
if they if the stores are too far apart
then they do not have enough sales so
how do they optimize this now for an
organization like Walmart which is an
ecommerce joint they already have the
addresses of their customers in their
database so they can actually use this
information or this data and use k-means
clustering to find the optimal location
now before we go into the Python
notebook and show you the live code I
wanted to take you through very quickly
a summary of the code in the slides and
then we will go into them by the
notebook so in this block we are
basically importing all the required
libraries like numpy matplotlib and so
on and we are loading the data that is
available in the form of let's say the
addresses for simplicity sake we will
just take them as some data points then
the next thing we do is quickly do a
scatter plot to see how they are related
to each other with respect to each other
so in the scatter plot we see that there
are a few distinct groups already being
formed so you can actually get an idea
about how the cluster would look and how
many clusters what is the optimal number
of clusters and then starts the actual
k-means clustering process so we will
assign each of these points to the
centroids and then check whether they're
the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids
then go through this iterative process
till the whole process converges and
finally we get an output like this so we
have four distinct clusters and which is
if we can say that this is how the
population is probably distributed
across Florida State and these centroids
are like the location where the store
should be the optimum location where the
store should be so that's the way we
determine the best locations for the
store and that's how we can help
Wawa find the best locations for the
stores in Florida so now let's take this
into Python notebook let's see how this
looks when we are learning running the
code the life all right so this is the
code for k-means clustering in jupiter
notebook we have a few examples here
which we will demonstrate how k-means
clustering is used and even there is a
small implementation of k-means
clustering as well okay so let's get
started okay so this block is basically
importing the various libraries that are
required like my prod lab numpy and so
on and so forth which would be used as a
part of the code then we are going and
creating blobs which are similar to
clusters now this is a very neat feature
which is available in scikit-learn make
blobs is a nice feature which creates
clusters of datasets so that's a
wonderful functionality that is readily
available for us to create some test
data kind of thing okay so that's
exactly what we are doing here we are
using make blobs and we can specify how
many clusters we want so centers we are
mentioning here so it will go ahead and
so we just mentioned four so it will go
ahead and create some test data for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make lobster II provides now from
here onwards we will basically
run the standard k-means functionality
that is readily available so we really
don't have to implement k-means itself
the k-means functionality of the
function is readily available you just
need to feed the data and create the
clusters so this is the code for that
the import k-means and then we create an
instance of k-means and we specify the
value of k this n underscore clusters is
the value of K remember k-means it came
in scale is basically the number of
clusters that you want to create and it
is a integer value so this is where we
are specifying that so we have K is
equal to 4 and so that instance is
created we take that instance and as
with any other machine learning
functionality if it is what we use the
function or the method rather fit is
what we use to train the model here
there is no real training kind of thing
but that's the call ok so we are calling
fit and what we are doing here we are
just passing the data so X has this
values the data that has been created
right so that is what we are passing
here and this will go ahead and create
the clusters and then we are using
after doing fit we run the product which
basically assigns for each of these
observations which cluster it belongs to
all right so it will name the clusters
maybe this is cluster ones two three and
so on or actually start from zero
cluster zero one two and three maybe and
then for each of the observations it
will assign based on which cluster it
belongs to it will assign a value so
that is stored in Y underscore k-means
when we call predict that is what it
does and we can take a quick look at
these wireless call came in solving the
cluster numbers that have been assigned
for each observation so this is the
cluster number assigned for observation
one maybe this is for observation two
observation three and so on so we have
how many about I think three hundred
samples right so all the three hundred
samples there are three hundred values
here each of them the cluster number is
given and the cluster number goes from
zero to three so there are four clusters
so the numbers go from zero one two
three okay now so this was a quick
example of generating some dummy data
and then clustering that okay and this
can be applied if you have proper data
you can just load it up into X for
example here and then run okay so this
is the central part of the k-means
clustering program example so you
basically create an instance and you
mention how many clusters you want by
specifying this parameter and underscore
clusters and that is also the value of K
and then pass the data to get the values
now the next section of this code is
implementation of k-means now this is
kind of a rough implementation of the
k-means algorithm so we will just walk
you through I will walk you through the
code at each step what it is doing and
then we will see a couple of more
examples of how K means clustering can
be used in maybe some real life examples
real life use cases all right so in this
case here what we are doing is basically
implement
k-means clustering and there is a
function or a library calculates for a
given two pairs of points it will
calculate the distance between them and
see which one is the closest and so on
so this is like this is pretty much like
what came in to us and so it calculates
the distance of each point or each data
set from predefined centroid and then
based on whichever is the lowest this
particular data point is assigned to
that percent right so that is basically
available as a standard function and we
will be using that here so as explained
in the slides the first step that is
done in case of k-means clustering is to
randomly assign some centroids so as a
first step we randomly allocate a couple
of centroids which we call here we are
calling as centers and then we put this
in a loop we take it through a nitrated
process for each of the data points we
first find out using this function
pairwise distance arc men for each of
the points we find out which fun which
center or which randomly selected
centroid is the closest and accordingly
we assign that data or the data point to
that particular centroid or cluster and
at once that is done for all the data
points we calculate the new centroid by
finding out the mean position because
the center position right so we
calculate the new centroid and then we
check if the new centroid is the
coordinates or the position is the same
as the previous centroid the positions
we will compare and if it is the same
that means the process has converged so
remember we do this process till the
centroids or that centroid doesn't move
anymore
right so the centroid gets relocated
each time this reallocation is done so
the moment it doesn't change anymore the
position of the centroid doesn't change
anymore
we know that converge
has occurred so till then so you see
here this is like an infinite loop while
true is an infinite loop it only breaks
when the Centers are the same the new
center and the old center positions are
the same and once that is done we return
the center Center labels now of course
as I explained this is not a very
sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in the
change will be very minor so in that
case also with that is actually
convergence right for example the change
is point zero zero zero one we can
consider that as convergence otherwise
what will happen as this will either
take forever or it will be never ending
so that's a small flaw here so that is
something additional checks may have to
be added here but again as mentioned
this is not the most sophisticated
implementation this is like a
implementation of the k-means clustering
okay so if we execute this code this is
what we get as the output so this is the
definition of this particular function
and then we call that fine underscore
clusters and we pass our data X and the
number of clusters which is four and if
we run that and plot it this is the
output that we get so this is of course
each cluster is represented by a
different color so we have a cluster in
green color yellow color and so on and
so forth
and these big points here these are the
centroids is the final position of the
centroids and as you can see visually
also this appears like a kind of a
center of all these points here right
similarly this is like the center of all
these points here and so on so this is
the example or this is an example of an
implementation of k-means clustering and
next we will move on to see a couple of
examples of how k-means clustering is
used in maybe some real-life first areas
or use cases in the next example or demo
we are going to see how we can use
k-means clustering to perform color
compression
we'll take a couple of images so there
will be two examples and we will try to
use k-means clustering to compress the
colors this is a common situation in
image processing when you have an image
with millions of colors but then you
cannot render it on some devices which
may not have enough memory so that is
the scenario where red something like
this can be used so before again we go
into the Python notebook let's take a
look at quickly the the code as usual we
import the libraries and then we import
the image and then we will flatten it so
three shaping is basically we have the
image information is stored in the form
of pixels and if the images like for
example 427 by 640 and it has three
colors so that's the overall dimension
of the of the initial image we just
reshape it and then feed this to our
algorithm and this will then create
clusters of only sixteen clusters so
this the colors there are millions of
colors and now we need to bring it down
to 16 colors so Vizio's k is equal to 16
and this is our when we visualize this
is how it looks there are these are all
about 16 million possible colors the
input color space has 16 million
possible colors and we just sub compress
it to 16 colors so this is how it would
look when we compress it to 16 colors
and this is how the original image looks
and after compression to 16 colors this
is a the new image looks as you can see
there is not a lot of information that
has been lost though the image quality
is definitely reduced a little bit so
this is an example which we are going to
now see in Python let's go into the
Python and once again as always we will
import some libraries and load the
image called flower dot jpg okay so let
me load that and this is how it looks
this is the original image which has I
think 16 million colors and this is the
shape of this image which is basically
what is the shape is nothing but the
overall size right so this is for 27
pixel by 640 pixel and then there are
three layers which is this three
basically is for RGB which is red green
blue so color image will have that and
so that is the shape of this now what we
need to do is data let's take a look at
how data is looking so let me just
create a new cell and show you what is
in data basically we have captured this
information
so data is what let me just show you
here
all right so let's take a look at China
what are the values in China and if we
see here this is how the data is stored
this is nothing but the pixel values
okay so this is like a matrix and each
one has about four for this for 27 by
640 Peck pixels all right so this is how
it looks now
the issue here is these values are large
the numbers are a lot so we need to
normalize them to between 0 and 1 right
so that's why we will basically create
one more variable which is data which
will contain the values between 0 and 1
and the way to do that as divided by 255
so we divide China by 255 and we get a
new values in data so let's just run
this piece of code and this is the
shapes we now have also yeah what we
have done is we changed using reshape we
converted into the three-dimensional
into a two dimensional data set and let
us also take a look at how let me just
insert probably a cell here and take a
look at how data is looking right so
this is our data is looking and now you
see this is the values are between 0 &amp;amp; 1
and so if you earlier noticed in case of
China the values where large numbers now
everything is between 0 &amp;amp; 1 this is one
of the things we need to do alright so
after that the next thing that we need
to do is to visualize this and we can
take random set of maybe 10,000 points
and plot it and check and see how this
looks
let us just plot this
so this is how the original the color
with the pixel distribution is these are
two plots one is red against green and
another is red against blue and this is
the original distribution of the color
so then what we will do is we will use
k-means clustering to create just
sixteen clusters for the various colors
and then apply that to the image now
what will happen is since the data is
large because there are millions of
colors using regular k-means may be a
little time consuming so there is
another version of k-means which is
called mini batch came in so we will use
that which is which processes in the
overall concept remains the same but
this basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of corn and also visualize this so that
we can see that there are is this is how
the 16 colors would look so this is red
against green and this is red against
blue there is quite a bit of similarity
between this original color schema and
the new one all right so it doesn't look
very very completely different or
anything like that
now we apply this the newly created
colors to the image and we can take a
look how this is looking now we can
compare both the images so this is our
original image and this is our new image
so as you can see there is not a lot of
information that has been lost it pretty
much looks like the original image yes
we can see that for example here there
is a little bit it appears a little
aylesh compared to this one right
because we kind of took off some of the
finer details of the color but overall
the high level information has been
maintained at the same time the main
advantage is that now this can be this
is an image which can be rendered on a
device which may not be that very
sophisticated now let's take one more
example with a different image in the
second example we will take an image of
the sample
in China and we repeat the same process
this is a high-definition color image
with millions of colors and also
three-dimensional now we will reduce
that to 16 colors using k-means
clustering and we do the same process
like before we reshape it and then we
cluster the colors to 16 and then we
render the image once again and we will
see that the color the quality of the
image slightly deteriorates as you can
see here this has much finer details in
this which are probably missing here but
then that's the compromise because there
are some devices which may not be able
to handle this kind of high-density
images so let's run this code in Python
on work all right so let's apply the
same technique for another picture which
is even more intricate and has probably
much complicated color schema so this is
the image now once again we can take a
look at the shape which is 427 by 640 by
3 and this is a new data would look
somewhat like this compared to the
flower image so we have some new values
here and we will also bring this as you
can see the numbers are much big so we
will much bigger so we will now have to
scale them down to values between 0 and
1 and that is done by dividing by 255
let's go ahead and do that I'd reshape
it okay so we get a 2 dimensional matrix
and we will then as a next step we will
go ahead and visualize this how it looks
does the 16 colors and this is basically
how it would look 16 million colors and
now we can create the clusters out of
this the 16 k-means clusters we will
create so this is how the distribution
of the pixels would look with 16 colors
then we go ahead and apply this I
visualize how it is looking for with the
new just the 16 color so once again as
you can see this looks much richer in
color but at the same time and this
probably doesn't have as we can see it
doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered or now slightly our
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done using
k-means clustering and we have also seen
in the previous examples of how to
implement k-means the corn - roughly how
to implement k-means clustering and we
use some sample data using blob to just
execute the k-means clustering alright
so with that let's move on so let's
summarize what we have learnt in this
video we started with an example of how
we can apply k-means clustering taking
the cricut example then we understood
what are the types of clustering two
major categories hierarchical clustering
and partition and clustering which in
turn had two sub categories
agglomerative and divisive and then
k-means and fuzzy c then we understood
the distance measures what are the
different types of distance measures
supported by k-means clustering and we
focused on k-means clustering we talked
about its applications and how exactly
the process flow works for the k-means
clustering and then finally we ended up
with a demo and a couple of use cases
all right so with that we come to the
end of this video thank you very much
for watching if you have any more
queries or questions please feel free to
visit us at www.simpleandsensational.com
hi there if you like this video
subscribe to the simple learn YouTube
channel and click here to watch similar</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>