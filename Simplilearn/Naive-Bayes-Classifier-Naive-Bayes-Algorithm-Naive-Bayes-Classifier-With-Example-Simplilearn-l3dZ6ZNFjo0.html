<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Naive Bayes Classifier | Naive Bayes Algorithm | Naive Bayes Classifier With Example | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Naive Bayes Classifier | Naive Bayes Algorithm | Naive Bayes Classifier With Example | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Naive Bayes Classifier | Naive Bayes Algorithm | Naive Bayes Classifier With Example | Simplilearn</b></h2><h5 class="post__date">2018-04-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l3dZ6ZNFjo0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">introducing naive Bayes classifier have
you ever wondered how your male provider
implements spam filtering or how online
news channels perform news text
classification or how companies perform
sentimental analysis of their audience
on social media all of this and more is
done through a machine learning
algorithm called naive Bayes classifier
welcome to naive Bayes tutorial my name
is Richard Kirchner I'm with the simply
learned team that's WWMT learned comm
get certified get ahead what's in it for
you
we'll start with what is naive Bayes a
basic overview of how it works we'll get
into naive Bayes and machine learning
where it fits in with our other machine
learning tools why do we need naive
Bayes and understanding naive Bayes
classifier a much more in depth of how
the math works in the background finally
we'll get into the advantages of the
nave Bayes classifier in the machine
learning setup and then we'll roll up
our sleeves and do my favorite part
we'll actually do some Python coding and
do some text classification using the
Navy base what is naive Bayes let's
start with a basic introduction to the
Bayes theorem named after Thomas Bayes
from the 1700s who first coined this in
the Western literature naive Bayes
classifier works on the principle of
conditional probability as given by the
Bayes theorem before we move ahead let
us go through some of the simple
concepts in the probability that we will
be using let us consider the following
example of tossing two coins here we
have two quarters and if we look at all
the different possibilities of what they
can come up as we get that they could
come up as head heads come up as head
tell tell head and tell tell when doing
the math on probability we usually
denote probability as a P a capital P so
the probability of getting two heads
equals 1/4 you can see on our data set
we have two heads and it's occurs once
out of the four possibilities and then
the probability of at least one tail
occurs three quarters of the time you'll
see on three of the coin tosses we have
tails in them and out of four that's
three-fourths and in the probability of
the second coin being head given the
first coin is tail is 1/2 and the
probability of getting two heads given
the first coin is a head is one half
will demonstrate that in just a minute
show you how that math works now when
we're doing it with two coins it's easy
to see then when you have something more
complex you can see where these
properties formulas really come in and
work so the Bayes theorem gives us the
conditional probability of an event a
given another event B has occurred in
this case the first coin toss will be B
and the second coin toss a this could be
confusing because we've actually
reversed the order of them and go from B
to a instead of a to B you'll see this a
lot when you work in probabilities the
reason is we're looking for event A we
want to know what that is so we're going
to label that a since that's our focus
and then given another event B has
occurred in the Bayes theorem as you can
see on the left the probability of a
occurring given B has occurred equals
the probability of B occurring given a
has occurred times the probability of a
over the probability of B this simple
formula can be moved around just like
any algebra formula and we could do the
probability of a after a given B times
probability of B equals the probability
of B given a tions probability of a you
can easily move that around and multiply
it and divide it out let us apply Bayes
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example H H out of
4 1/4 and we have three with tails in
them giving us 3/4 or 3/4 75% the second
condition the second set three and four
we're gonna explore a little bit more in
detail now we stick to a simple example
with two coins because you can easily
understand the math the probability of
throwing a tail doesn't matter what
comes before it and the same with the
heads so still going to be 50% or 1/2
but when that come when that probability
gets more complicated let's say you have
a d6 dice or some other instance then
this formula really comes in handy but
let's stick to the simple example for
now in this sample space let a be the
event that the second coin is head and B
be the event that the first coin is
tails again we reversed it because we
want to know what the second events
going to be so we're gonna be focusing
on a and we write that out as a
probability of a given B and we know
this from our formula that that equals
probability of B given a time's the
probability of a over the probability of
B and when we plug that in we plug in
the probability of the first coin being
tails given the second coin is heads and
the probability of the second coin being
heads given the first coin being over
the probability of the first coin being
tails when we plug that data in and we
have the probability of the first coin
being tails given the second coin is
heads times the probability of the
second coin being heads over the
probability of the first coin being
tails you can see it's a simple formula
to calculate we have 1/2 times 1/2 over
1/2 or 1/2 equals 0.5 or 1/4 so the
Bayes theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial
understanding naive Bayes and machine
learning like with any of our other
machine learning tools it's important to
understand where are the naive Bayes
fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning
there's also reward system this falls
under the supervised learning and then
under the supervised learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes use let's look at some of the use
scenarios for it as a classifier we use
in face recognition is this Cindy or is
it not Cindy or whoever or I might be
used to identify parts of the face that
they then feed into another part of the
face recognition program this is the eye
this is the nose this is the mouth
weather prediction is it gonna be rainy
or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease or other
ailments and news classification you
look at the Google News and it says well
is this political or is this world news
or a lot of that's all done with a naive
bayes understanding naive Bayes
classifier now we already went through a
basic understanding with the coins in
the two heads and two tails and
head-tail-tail heads etc we're gonna do
just a quick
you on that remind you that the naive
Bayes classifier is based on the Bayes
theorem which gives a conditional
probability an event event a given event
B and that's where the probability of a
given B equals the probability of B
given a times probability of a over
probability of B remember this is now to
break functions we can move these
different entities around we can
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easily come up with
a new formula for the probability of B
to me staring at these algebraic
functions kind of gives me a slight
headache it's a lot better to see if we
can actually understand how this data
fits together in a table and let's go
ahead and start applying it to some
actual data so you can see what that
looks like so we're gonna start with the
shopping demo problem statement and
remember we're gonna solve this first in
table form so you can see what the math
looks like and they were gonna solve it
in Python you didn't here we wanted to
predict whether the person will purchase
a product are they gonna buy or don't
buy very important if you're running a
business you want to know how to
maximize your profits or at least
maximize the purchase of the people
coming into your store and we're gonna
look at a specific combination of
different variables in this case we're
gonna look at the day the discount and
the free delivery and you can see here
under the day we want to know whether
it's on the weekday you know somebody's
working they come in after work or maybe
they don't work weekend you can see the
bright colors coming down they're
celebrating not being in work or holiday
and did we offer a discount that day yes
or no did we offer free delivery that
day yes or no and from this we want to
know whether the person's gonna buy
based on these traits so we can maximize
them and find out the best system for
getting somebody to come in and purchase
our goods and products from our store
now having a nice visual is great but we
do need to dig into the data so let's go
ahead and take a look at the data set we
have a small sample data set of 30 rows
we're showing you the first 15 of those
roads for this demo now the actual data
file you can request just type in below
under the comments on the YouTube video
and we'll send you some more information
and send you that file as you can see
here the file is very simple columns and
rows we have the day the discount the
free delivery and did the person
purchase or not and then we have under
the day whether it was a weekday a
holiday
was at the weekend this is a pretty
simple set of data and long before
computers people used to look at this
data and calculate this all by hand
so let's go ahead and walk through this
and see what that looks like when we put
that into tables also note in today's
world we're not usually looking at three
different variables in 30 rows nowadays
cuz we look like data so much we're
usually looking at 27:30 variables
across hundreds of rows the first thing
we want to do is we're gonna take this
data and based on the data set
continuing our three inputs day discount
and free delivery we're gonna go ahead
and populate that to frequency tables
for each attribute so we want to know if
they had a discount how many people buy
and did not buy did they have a discount
yes or no do we have a free delivery yes
or no on those days how many people made
it purchased how many people didn't and
the same with the three days of the week
was it a weekday a weekend a holiday and
did they buy yes or no as we dig in
deeper to this table for our Bayes
theorem let the event by BA now remember
we looked at the coins I said we really
want to know what the outcome is did the
person buy or not and that's usually
event a is what you're looking for and
the independent variables discount free
delivery and day BB so we'll call that
probability of B now let us calculate
the likelihood table for one of the
variables let's start with day which
includes weekday weekend and holiday and
let us start by summing all of our rows
so we have the weekday row and out of
the weekdays there's nine plus two so
it's 11 weekdays there's eight weekend
days and 11 holidays well it's a lot of
holidays and then we want to sum up the
total number of days so we're looking at
a total of 30 days let's start pulling
some information from our chart and see
where that takes us and when we fill in
the chart on the right you can see that
9 out of 24 purchases are made on the
weekday 7 out of 24 purchases on the
weekend and 8 out of 24 purchases on a
holiday and out of all the people who
come in 24 out of 30 purchase you can
also see how many people do not purchase
on the week dates 2 out of 6 didn't
purchase and so on and so on we can also
look at the totals and you'll see on the
right we put together some of the
formulas the probability of making a
purchase on the weekend comes out 11 out
of 30
so out of the 30 people who came into
the store throughout the weekend weekday
and holiday eleven of those purchases
were made on the weekday and then you
can also see the probability of them not
making a purchase and this is done for
doesn't matter which day of the week so
we call that probability of no buy would
be 6 over 30 or 0.2 so there's a 20%
chance that they're not gonna make a
purchase no matter what day of the week
it is and finally we look at the
probability of B if a in this case we're
gonna look at the probability of the
weekday and not buying true of the no
buyers we're done out of the weekend out
of the 6 people who did not make
purchases so when we look at that
probability of the weekday without a
purchase is gonna be 0.33 or 33% let's
take a look at this at different
probabilities and based on this
likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37 percent 0.367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20% and the probability of a weekday no
purchase is roughly 2 out of 6 so 2 out
of 6 of our no purchases were made on
the weekday and then finally we take our
P of a B if you luck we've kept the
symbols up there we got P of probability
of B probability of a probability of B
if a we should remember that the
probability of a if B is equal to the
first 1 times the probability of no
purpose over the probability of the
weekday so we could calculate it both
off the table we created we can also
calculate this by the formula and we get
the point 3 6 7 which equals or 0.33
times 0.2 over 0.36 7 which equals 0.179
or roughly 17 to 18% and that would be
the probability of no purchase done on
the weekday and this is important
because we can look at this and say as
the probability of buying on the weekday
is more than the probability of not
buying on the weekday we can conclude
the customers will most likely buy the
product on a weekday now we've kept our
chart simple and we're only looking at
one
aspect so you should be able to look at
the table and come up with the same
information or the same conclusion that
should be kind of intuitive at this
point next we can take the same set up
we have the frequency tables of all
three independent variables now we can
construct the likelihood tables for all
three of the variables we're working
with we can take our day like we did
before we have weekday weekend and
holiday we filled in this table and then
we can come in and also do that for the
discount yes or no did they buy yes or
no and we fill in that full table so now
we have our probabilities for a discount
and whether the discount leads to a
purchase or not and the probability for
free delivery does that lead to a
purchase or not and this is where starts
getting really exciting let us use these
three likelihood tables to calculate
whether a customer will purchase a
product on a specific combination of day
discount and free delivery or not
purchase here let us take a combination
of these factors day equals holiday
discount equals yes free delivery equals
yes let's dig deeper into the math and
actually see what this looks like and
we're gonna start with looking for the
probability of them not purchasing on
the following combinations of days we
were actually looking for the
probability of a equal no buy no
purchase and our probability of B we're
gonna set equal to is it a holiday did
they get a discount yes
and was it a free delivery yes before we
go further let's look at the original
equation the probability of a if B
equals the probability of B given the
condition a and the probability times
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is B C and D or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that in
just second in the formula times the
full probability of a over the full
probability of B so here we are back to
this and we're gonna have let a equal no
purchase and we're looking for the
probability of B on the condition a
we're a says for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is a day equal a holiday
those are our three variables of the
probability of a of B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day true out of six were no
purchase on a free delivery day and
three out of six were a no purchase on a
holiday those are our three
probabilities of a if B multiply it out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the six out of 30 we divide that out
by the probability of each category over
the total number so we get the 20 out of
30 had a discount 23 out of 30 had a yes
for free delivery and 11 out of 30 were
on a holiday we plug all those numbers
in we get to point one seven eight so in
our probability math we have a point one
seven eight if it's a no-buy for a
holiday a discount and a free delivery
let's turn that around and see what that
looks like if we have a purchase I
promise this is the last page of math
before we dig into the Python script so
here we're calculating the probability
of the purchase using the same math we
did to find out if they didn't buy now
we want to know if they did buy and
again we're gonna go buy the day equals
a holiday discount equals yes free
delivery equals yes and let a equal buy
now right about now you might be asking
why are we doing both calculations why
why would we want to know the no buys
and buys for the same day that going in
well we're going to show you that in
just a moment but we have to have both
of those pieces of information so that
we can figure it out as a percentage as
opposed to a probability equation and
we'll
to that normalization here in just a
moment let's go ahead and walk through
this calculation and as you can see here
the probability of a on the condition of
B B being all three categories did we
have a discount with a purchase do we
have a free delivery with a purchase and
did we is a day equal to holiday and
when we plug this all into that formula
and multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times a day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our point nine
eight six so what are we going to do
with these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.9 86 we have a
probability of no purchase equals point
one seven eight so finally we have a
conditional probability to purchase on
this day let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals 0.9 86 86 plus point 1 7 8 and
that equals the 1 point 1 6 4 if we
divide each probability by the sum we
get the percentage and so the likelihood
of a purchase is eighty four point seven
one percent and the likelihood of no
purchase is fifteen point two nine
percent and given these three different
variables so as if it's on a holiday if
it's with a discount and has free
delivery then there's an eighty four
point seven one percent chance that the
customers gonna come in and make a
purchase array they purchased our stuff
we're making money if your we're owning
a shop that's like is the bottom line is
you want to make some money so she keep
your shop open and have a living now I
promised you that we were going to be
finishing up the math here with a few
pages so we're going to move on we're
going to
two steps the first step is I want you
to understand why you want under why you
want to use the naivebayes what are the
advantages of naivebayes
and then once we understand those
advantages we're just look at that
briefly then we're gonna dive in and do
some Python coding advantages of naive
Bayes classifier so let's take a look at
the six advantages of the naive Bayes
classifier and we're gonna walk around
this lovely will looks like an origami
folded paper the first one is very
simple and easy to implement certainly
you could walk through the tables and do
this by hand you're gonna be a little
careful because the notations can get
confusing you have all these different
probabilities and I certainly mess those
up as I put them on you know is it up on
the top or the bottom got to really pay
close attention to that when you put it
into Python it's really nice because you
don't have to worry about any of that
you let the Python handle that the
Python module but understanding it you
can put it on a table and you can easily
see how it works and it's a simple
algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see you just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
cards referrals spam filters is because
there's no time delay as it has to go
through and figure out a neural network
or one of the other many setups where
you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as a naive Bayes and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on data on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing Census and studying
of people and habits where they might
have one study that covers one aspect
another one that over
laughs and because a to overlap they can
then predict the unknowns for the group
that they haven't done the second study
on or vice versa so it's very powerful
in that it is not sensitive to the
irrelevant features and in fact you can
use it to help predict features that
aren't even in there so now we're down
to my favorite part we're gonna roll up
our sleeves and do some actual
programming we're gonna do the use case
text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
Python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're gonna do a Python code doing text
classification very popular for doing
the naive Bayes so we're gonna use our
new tool to perform a text
classification of news headlines and
classify news into different topics for
a news website as you can see here we
have a nice image of the Google News and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're gonna use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're up my
favorite part we're actually gonna write
some Python script roll up our sleeves
and we're gonna start by doing our
imports these are very basic imports
including our news group and we'll take
a quick glance at the target names then
we're gonna go ahead and start training
our data set and putting it together
we'll put together a nice graphic it's
always good to have a graph to show
what's going on
and once we've trained it and we've
shown you a graph of what's going on
then we're gonna explore how to use it
and see what that looks like now I'm
gonna open up my favorite editor or
inline editor for Python you don't have
to use this you can use whatever your
editor that you like whatever interface
IDE you want this just happens to be the
Anaconda Jupiter notebook and I'm gonna
paste that first piece of code in here
so we can walk through it let's make it
a little bigger on the screen so you
have a nice view of what's going on and
we're using Python 3 in this case 3.5 so
this would work in any of your 3x if you
have it set up quickly should also work
in a lot of the 2x you just have to make
sure all of the versions of the modules
match your
Python version and in here you'll notice
the first line is your percentage mat
plot library in line now three of these
lines of code are all about plotting the
graph this one lets the notebook notes
and is in line set up do we want the
graphs to show up on this page without
it in a notebook like this which is an
explorer interface it won't show up now
a lot of ID's don't require that a lot
of them like on if I'm working on one of
my other setups it just has a pop up and
the graph pops up on there so you have a
that setup also but for this we want the
mat plot library in line and then we're
gonna import numpy as NP that's number
Python which has a lot of different
formulas in it that we use for both of
our SK learned module and we also use it
for any of the upper math functions in
Python and it's very common to see that
as NP numpy as NP the next two lines are
all about our graphing embrace that
three of these are about graphing well
we need our mat plot library pie plot as
p LT and you'll see that p LT is a very
common set up as is the SNS and just
like the NP and we're going to import
Seabourn as SNS and we're gonna do the
SNS set now Seabourn sits on top of pi
plot and it just makes a really nice
heat map that's really good for heat
maps and if you're not familiar with
heat maps that just means we give it a
color scale the term comes from the
brighter red it is the hotter it is in
some form of data and you can set it to
whatever you want and we'll see that
later on so those you'll see that those
three lines of code here are just
importing the graph function so we can
graph it and as a data science test you
always want to graph your data and have
some kind of visual it's really hard
just to shove numbers in front of people
and they look at it and it doesn't mean
anything
and then from the SK learned data sets
we're gonna import the fetch 20 news
groups very common one for analyzing
tokenizing words and setting them up and
exploring how the words work and how do
you categorize different things when
you're dealing with documents and then
we set our data equal to fetch 20 news
groups so our data variable will have
the data in it and we're gonna go ahead
and just print the target names data
target names and let's see what that
looks like and you'll see here we have
alt atheism
graphics cop OS ms windows miscellaneous
and it goes all the way down to talk
politics top miscellaneous talk religion
miscellaneous these are the categories
they've already assigned to this news
group and it's called fetch twenty
because you'll see there's I believe
there's twenty different topics in here
or twenty different categories
as we scroll down now we've gone through
the twenty different categories and
we're gonna go ahead and start defining
all the categories and set up our data
so we're actually getting here gonna go
ahead and get it get the data all set up
and take a look at our data and let's
move this over to our Jupiter notebook
and let's see what this code does first
we're gonna set our categories now if
you noticed up here I could have just as
easily set this equal to data dot target
underscore names because it's the same
thing but we want to kind of spell it
out for you so you can see the different
categories it kind of makes it more
visual so you can see what your data is
looking like in the background once
we've created the categories we're gonna
open up a train set so this training set
of data is gonna go into fetched twenty
newsgroups and it's a subset in there
called train and categories equals
categories so we're pulling out those
categories that match and then if you
have a train set you should also the
testing set we have test equals fetch
twenty newsgroups sub set equals test
and categories equals categories let's
go down one size so it all fits on my
screen there we go and just so we can
really see what's going on let's see
what happens when we print out one part
of that data so it creates train an
under train at krei strain data and
we're just gonna look at data piece
number five and let's go ahead and run
that and see what that looks like and
you can see when I print train dot data
number five under train it prints out
one of the articles this is article
number five you can go through and read
it in there and we can also go in here
and change this to test which should
look identical because it's splitting
the data up into different groups train
and test and we'll see test number five
is a different article that's another
article in here and maybe you're curious
and you want to see just how many
articles are in here we could do links
of train dot data and if we run that
you'll see that the training data has
eleven thousand three hundred and
fourteen articles so we're not gonna go
through all those articles that's a lot
of articles but we can't look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the second amendment IDs
vtt line fifty-eight liens 58 in article
etc you can scroll all the way down and
see all the different parts to there now
we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you wait this if you look down here
we have different words and maybe the
word from well from is probably on all
the articles so it's not gonna have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not
so trying to figure out which category
fits in based on these words is where
the challenge comes in now that we
viewed our data we're gonna dive in and
do the actual predictions this is the
actual naivebayes
and we're gonna throw another model at
you or another module at you here in
just a second we can't go into too much
detail but until specifically working
with words and text and what they call
tokenizing those words so let's take
this code and let's skip on over to our
jupiter notebook and walk through it and
here we are in or did you put a notebook
let's paste that in there and I can run
this code right off the bat it's not
actually gonna display anything yet but
it has a lot going on in here
so the top we have the print module from
the earlier one I didn't know why that
was in there so we're gonna start by
importing our necessary packages and
from the SK learn features extraction
dot text we're gonna import tf-idf
vectorizer
I told you we got through a module at
you we can't go too much into the math
behind this or how it works you can look
it up the notation for the math is
usually TF dot IDF and that's just a way
of weighing the words and it weighs the
words based on how many times are used
in a document how many times or how many
documents are used in and it's a well
used formula it's been around for a
while it's a little confusing to put
this in here but let's let her know that
it just goes in there and weights the
different words in the document for us
that way we don't have to wait and if
you put a weight on it if you remember I
was talking about that up here earlier
if these are all emails they probably
all have the word from in them from
probably has a very
wait it has very little value and
telling you what this documents about
same with words like in an article in
articles in Kostov on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the articles doing well how do
you figure out all those weights in the
different articles that's what this
module does that's what the tf-idf
vectorizer is going to do for us and
then we're gonna import our SK learn I
bays and that's our multinomial in b
multinomial naive bayes pretty easy to
understand that where that comes from
and then finally we have this guy learn
pipeline import make pipeline now the
make pipeline is just a cool piece of
code because we're going to take the
information we get from the tf-idf
vectorizer and we're going to pump that
into the multinomial in B so a pipeline
is just a way of organizing how things
flow it's used commonly you probably
already a guess what it is if you've
done any businesses they talk about the
sales pipeline if you're on a work crew
or project manager you have your
pipeline of information that's going
through or your projects and what has to
be done in what order
that's all this pipeline is we're going
to take the TFI to vectorizer and then
we're gonna push that into the
multinomial in B now we've designated
that as the variable model we have our
pipeline model and we're going to take
that model and this is just so elegant
this is done in just a couple lines of
code model dot fit and we're gonna fit
the data and first the train data and
then the Train target now the train data
has the different articles in it you can
see the one we were just looking at and
the Train dot target is what category
they already categorized that that
particular article ass and what's
happening here is the train data is
going into the TF idvr so when you have
one of these articles it goes in there
it weights all the words in there so
there's thousands of words with
different weights on them I remember
once running a model on this and I
literally had two point four million
tokens go into this so when you're
dealing like large document bases you
can have a huge number
of different words it then takes those
words gives them a weight and then based
on that weight based on the words and
the weights and then puts that into the
multinomial in B and once we go into our
naivebayes we want to put the train
target in there so the train data that's
been mapped to the tfiid vectorizer
is now going through the multinomial in
B and then we're telling it well these
are the answers these are the answers to
the different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're gonna set that equal to
model dot predict most of the SK learned
use the term dot predict to let us know
that we've now trained the model and now
we want to get some answers and we're
gonna put our test data in there because
our test data is the stuff we held off
to the side we didn't train it on there
and we don't know what's gonna come up
out of it and we just want to find out
how good our labels are do they match
what they should be now I've already
read this through there's no actual
output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion matrix and
a heat map so the confusion matrix which
is confusing just by its very name it's
basically gonna ask how confused is our
answer to get it correct or did it miss
some things in there or have some missed
labels and then we're gonna put that on
a heat map so I have some nice colors to
look at to see how that plots out let's
go ahead and take this code and see how
that take a walk through it and see what
that looks like so back to our Jupiter
notebook I'm gonna put the code in there
and let's go ahead and run that code
take it just a moment and remember we
had the in line that way my graph shows
up on the in line here and let's walk
through the code and then we'll look at
this and see what that means so make it
a little bit bigger there we go no
reason not to use the whole screen too
big so we have here from SK learn
metrics import confusion matrix
and that's just gonna generate a set of
data that says I the prediction was such
the actual truth was either agreed with
it or is something different and it's
gonna add up those numbers so we can
take a look and just see how well it
worked and we're going to set a variable
Matt equal to confusion matrix and we
have our test target our test data that
was not part of the training very
important in data science we always keep
our test data separate in other ways
it's not a valid model if we can't
properly test it with new data and this
is the labels we created from that test
data these are the ones that we predict
it's going to be so we go in and we
create our SN heat map the SNS is our
Seabourn which sits on top of the pie
plot so we create an SNS dot heat map we
take our confusion matrix and it's going
to be met dot t do we have other
variables that go into the SNS heat map
we're not gonna go into detail that all
the variables mean the annotation equals
true that's what tells it to put the
numbers here so you have the 166 the 1
the 0 0 0 1 format D and C bar equals
false have to do with the format if you
take those out you'll see that some
things disappear and then the X tick
labels and the white ick labels those
are our target names and you can see
right here that's the alt atheism comp
graphics comp Oh SMS windows
miscellaneous and then finally we have
our PLT dot X label remember the SNS or
the Seabourn sits on top of our mat plot
library our PLT and so we want to just
tell that X label equals a true is is
true the labels are true and then the
while Abel is prediction label so we say
a true this is what it actually is and
the prediction is what we predicted and
let's look at this graph because that's
probably a little confusing the way we
rattled through it and what I'm gonna do
is I'm gonna go ahead and flip back to
the slides cuz they have a black
background they put in there that helps
it shine a little bit better so you can
see the graph a little bit easier so in
reading this graph but we want to look
at is how the color scheme has come out
and you'll see a line right down the
middle diagonally from upper left to
bottom right what that is is if you look
at the labels we have our predicted
label on the left and our true label on
the right those are the numbers where
the prediction and the true come
together and this is what you want to
see is we want to see those lit up
that's what that heat map does as you
can see that it did a good job of
finding those data and you'll notice
that there's a couple of red spots on
there where a mist you know it's a
little confused we talked about talk
religion miscellaneous versus talk
politics miscellaneous social religion
Christian versus alt atheism it
mislabeled some of those and those are
very similar topic so you could
understand why it might miss label them
but overall I did a pretty good job if
we're gonna create these models we want
to go ahead and be able to use them so
let's see what that looks like to do
this let's go ahead and create a
definition a function to run and we're
gonna call this function let me just
expand that just a notch here there we
go I like mine in big letters predict
categories we want to predict the
category we're gonna send it s a string
and then we're sending it train equals
train we have our training model and
then we had our pipeline model equals
model this way we don't have to resend
these variables each time the definition
knows that because I said train equals
train and I put the equal for model and
then we're gonna set the prediction
equal to the model dot predict s so it's
going to send whatever string we send to
it it's gonna push that string through
the pipeline the model pipeline it's
gonna go through and tokenize it and put
it through the tf-idf convert that into
numbers and waits for all the different
documents and words and then I'll put
that through our naive Bayes and from it
we'll go ahead and get our prediction
we're gonna predict what value it is and
so we're going to return train dot
target names predict of zero and
remember that the Train target names
that's just categories I could have just
as easily put categories in there dot
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from I don't know
what the actual numbers are but let's
say 0 equals alt atheism so we're gonna
convert that 0 to the word or 1 maybe it
equals comp graphics so we're going to
convert number 1 into comp graphics
that's all that is and then we got to go
ahead and
and then we need to go ahead and run
this so I load that up and then once I
run that we can start doing some
predictions let me go ahead and type in
predict category and this is to predict
category Jesus Christ and it comes back
and says it's social religion Christian
that's pretty good now note I didn't put
print on this one of the nice things
about the Jupiter notebook editor and a
lot of inline editors is if you just put
the name of the variable out as
returning the variable train target
underscore names it'll automatically
print that for you in your own IDE you
might have to put in print let's see
where else we can take this and maybe
you're a space science buff so how about
sending load to International Space
Station and if we run that we get
science space or maybe you're a
automobile buff and let's do a they were
gonna tell me audience better than BMW
but I'm gonna do BMW is better than an
audio so maybe you're a car buff and we
run that and you'll see it says
recreational I'm assuming that's with re
C stands for autos so I did a pretty
good job labeling that one how about if
we have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous so when we take
our definition our function and we run
all these things through kudos we made
it we were able to correctly classify
texts into different groups based on
which category they belong to using the
naive Bayes classifier now we did throw
in the pipeline the TF idea vectorizer
we threw in the graphs those are all
things that you don't necessarily have
to know to understand them the naive
Bayes set up or classifier but they're
important to know one of the main uses
for the naive Bayes is with the tf-idf
tokenizer vectorizer where tokenize is a
word and as labels and we use the
pipeline because you need to push all
that data through and it makes it really
easy and fast you don't have to know
those understand naive Bayes but they
certainly help for understanding the
industry and data science
we can see their categorize err or naive
Bayes classifier we were able to predict
the category religion space motorcycles
autos politics and properly classify all
these different things we pushed into
our prediction in our train model let's
go ahead and wrap it up and let's just
go through what we covered I would give
you an introduction to naive Bayes and
how it's used to perform basic
classification as a classifier we went
through the basic formula the
probability of a given B and the
probability of B given a the basics of
the naive Bayes we touched a little bit
on some of the different uses for the
naive Bayes we also went over the
advantages of it and where it really
shines especially when we talk about
real time processing naive Bayes is very
fast we talked about the shopping demo
remember that if you want to try that on
your own
send a note down below let us know and
we'll get you that data set and we also
went through the Python my favorite part
the text classification so we learned
all kinds of things in there and walking
through a real-life scenario where you'd
use the naive Bayes at I want to thank
you for joining us today if you have any
questions don't forget to type them down
below happy learning and we look forward
to hearing from you
hi there if you liked this video
subscribe to the Simple learn YouTube
channel and click here to watch similar
videos Dinard up and get certified click
here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>