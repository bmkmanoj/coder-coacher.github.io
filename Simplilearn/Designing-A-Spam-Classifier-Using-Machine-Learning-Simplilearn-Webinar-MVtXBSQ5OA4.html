<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Designing A Spam Classifier Using Machine Learning | Simplilearn Webinar | Coder Coacher - Coaching Coders</title><meta content="Designing A Spam Classifier Using Machine Learning | Simplilearn Webinar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Designing A Spam Classifier Using Machine Learning | Simplilearn Webinar</b></h2><h5 class="post__date">2016-10-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MVtXBSQ5OA4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone good afternoon or
good evening depending on where you are
in the world thank you so much for
joining us for this webinar today with
SIA drizzly designing a spam classifier
using machine learning now today you're
going to learn how to design a spam
classifier with machine learning api's
you'll discover the magic of classifiers
and supervised learning for
classification and labeling of data sets
we'll talk a little bit about naive
Bayes classifier for inferring and
classifying languages based on word
occurrence and spam filtering it's going
to be a little bit technical but it's
going to be fascinating and if anybody
is the person to do it
it is Syed and if you don't know about
him let me please introduce a little bit
about him he is an IT engineering
manager with 14 years of experience in
software development technology
management IT strategy tech consulting
and product development he has been a
big data and hadoop consultant with best
in town analytics and work with both
Deloitte Consulting Mumbai and Tata
Consultancy Services among many many
others so if anybody can teach you a
little bit about machine learning here
in just 40 or 45 minutes that this is
the man to do it but before we get
started I want to make sure everybody is
hearing us ok and I want to make sure
that you have found the questions tool
in your dashboard so this is an
opportunity for you to say hello and let
us know if you would on a scale of 1 to
4 how familiar you are with machine
learning go ahead and type in if you're
brand new to machine learning type in a
1 and if you are working in the field
and familiar with it type of 4 and
anywhere in between and that way we know
you're hearing us fantastic I see
raveloe Shukla I'm cush all saying hello
looks like we've got some beginners in
machine learning here and if anybody has
any first-hand experience go ahead and
put a 4 in there tube that gives us an
understanding of who all we're talking
with today and how we should present our
materials good to know that you found
that questions module if during the
presentation you
do have questions and you would like our
our faculty member here to circle back
around and give you additional
information or answer a question
specific to your situation type it in
the question module and at the end of
the presentation we will loop back
around and get to as many questions as
we possibly can
hi Daniela shalash I see so many people
in there saying hello I'm so happy
you're here and thank you for letting us
know where you're at with machine
learning if you want to share what you
learn on social if you hear something
super awesome and brilliant that you
with your audience on linkedin or on
twitter or what-have-you please feel
free to go ahead and do so and you can
use the hashtag SL webinars that's
pretty much everything you want to hear
from me you didn't come here to hear me
talk I want to turn things over to my
friend say add and he will take you
through the magic of machine learning so
go ahead and grab your coffee or your
tea and lean back take a few notes and
sir it's a pleasure to have you back
thank you for taking care of us today
thank you so much chip and thank you so
much for a wonderful introduction as
always it's a pleasure to hear you
talking as always thank you so much
thanks a lot hello and welcome to all of
you why this is so I get here say it
this way hopefully you guys are able to
hear my voice and see my screen for
those of you who have already attended a
couple of my sessions or my batches for
that mantle would know that I use this
sheet called as much each sheet or the
TOC sheet to introduce any topic I keep
presenting them over here and keep
coming back to this read and keep
marking them done so just as a kind of
you know demonstration of that chip just
now gave my introduction and give me one
second
yep she gave my introduction I'm just
marking it as as done now before I start
with our presentation my friends let me
give you a quick idea about the
structure of the presentation what I'm
going to do is that I'm going to first
of all introduce you a little bit to
bigger than Hadoop because that is where
all the game all the story starts
that is what is the basic premise of you
know how machine learning has been
tackled with these days if possible I'm
going to also do a quick you know Hadoop
sample run program sample run and and
kind of you know give me an idea about
that and then I'm going to slowly slowly
move on to the areas where Hadoop fails
and a newer spa a newer is framework
like spark you know take the cake they
come into the picture and kind of you
know just win the show and then I'm
gonna tell you a bit about you know what
a bit about what spark is it's a little
long topic you know course of its own
and as you would you know understand
it'll give you brief idea about that the
reason for me to give you the idea about
Hadoop and spark both is that they form
the very core the very basic of how
machine learning library is didn't spark
works on top of it so that's a reason
why it becomes important for me to kind
of you know understand that in this
short little session it's it's you know
for me - for you - for me to make
complete justice with the topic and for
you to get even a slight bit of
understanding what it is all about I
think this is the structure which has
been making sense for me pretty well
from master blow presentations anyway so
that's why we are a quick introduction
to the big data right what does Big Data
the first question the first and the
most important and the perennial
question what is Big Data
I keep telling this in all my
presentations that you know big data is
the data with sure you know your normal
computer cannot hold it's beyond the
capability of your normal computer to
hold write a one terabyte let laptop
anything which is bigger than that a 4
terabyte desktop anything which is huge
than that your single machine cannot
hold it
that is big it up you're talking about
volume here you're talking about volume
here that is the data big data is the
premise of everything at this point of
time my friends it's a premise for
everything for analytics for machine
learning for streaming so graph
processing you name it it it all in
encompasses it nice so if this is so
you know omnipresent it is so important
right how's it is it easy to deal with
it it's it's difficult to deal with it
how is it to deal with it well basically
when you deal with it obviously the
first thing that you would want to do is
you would want to store this data now
that is where the first very very
interesting challenge becomes right
store data it's a huge data can you
store it on your one on your single
machine no you cannot
it's beyond the capability of the
storing on a single machine you just
have a gamut of machine working together
a collection of machines working
together does it sound like distributed
computing oh yes oh yes it does sound
like distributed computing I am indeed
talking about distributed computing and
these all technologies are about
distributed computing right later on
you'll see that what Hadoop and spark in
machine learning adds to this concept of
distributed computer learning is it
makes it or to meet it it makes it
automated so anyways more on that a
little later storage is a problem
storage is a challenge what is the other
problem processing is the problem
processing is a challenge right
processing is a problem processing a
challenge if you've been able to store
the data processing becomes challenge so
these are the two yeah you know concept
that I wanted to you know data which is
very buggy big yes I want to talk about
quickly on my I'll mark this also is
done right this also is done quick
introduction to Hadoop then what does he
do
Hadoop is the framework which provides
automated distributed computing of big
data storage and analysis both right
storage analyst both over here I have
given the definition of it I will go
into it in much detail usually in my BDS
course I go into it in lot of detail
I won't go into it in too much details
right what it does is that it's an open
source review which provides it
automated distributed computing
environment right automated is the
watchword here right open source
framework is the watchword here and
these two talent challenge is a storage
is made by something called as HDFS in
Hadoop
Anoop distributed file system provides
the mechanism to store big data right
and MapReduce in Hadoop provides the
mechanisms to process thing here
Zhao what the problems solved by Hadoop
both the challenges solved by Hadoop so
if me I may call it Hadoop is the
framework which is married to Big Data
it provides all the problems which are
imposed in front of you
challenges which are thrown in front of
you by big data very quickly very
quickly hopefully it gives you an idea
about you know what it is ah those of
you who would have been aware of you
know what what Hadoop is would know that
you know this is how a typical
installation of a new food looks like
right so you want to take the version of
Hadoop it will be version you can check
it like this nice two point seven point
two if you want to kind of you know so
quickly delete some files from your HDFS
you can do something like this right
yeah you can do something like this
right you can do something like this and
also if you want to run a let's say a
program in do something like this right
so this one of
Hadoop jobs running now when the job is
running let me tell you one of the
challenges which people have faced with
Hadoop over a period of time 9 the
telling is that people have faced is
tired Hadoop has two components called
as na and deduce now this mapper when it
runs it gives an intermediate output it
dumps the intermediate output on the
machine on which the method is running
let me tell you if you have little bit
of idea about I do you would know that
you know having tens and thousands of
mappers in a job is a common phenomena
it's a common it's it's its usual to
have tens and thousands of mappers
now imagine each one of these mappers
dumping the output on the individual
machines 30 that they are running on
imagine that right that is where it
becomes performance inefficient that is
where it becomes performance inefficient
so quickly I have done the sample done
if I do I'm not getting into details of
Hadoop right now because I want to kind
of you know quickly move on to machine
learning but anyway so this is what it
is this is where Hadoop fails to live up
the promise it dumps everything all the
mappers dump their individual output on
the machine on which the mapper is
running this is very very performance
inefficient very very performance
efficient so there was a need of
creating a faster more performance
efficient framework and that is where
incomes spark this path does not do
intermediate output on the on the
machine on which their programming
you're interning it usually chains and
caches the information it is a concept
called as transformation right and a
concept called as action transformation
keeps on storing the data and the
results of your of your of your of your
processing on the in the memory itself
and then where the time comes to give
the final output on HDFS or local it
dumps it nice so it's you know 10 times
much more performance efficient
processing framework so we'll see a
formal definition of it so I'm just
marking this also over here quick
introduction to that let us see a formal
definition of par then in that case what
is spark a spark is a fast Apache spark
the fast and general purpose plus think
of the cluster computing environment
right it has been improved upon Hadoop
and MapReduce processing part of Hadoop
in leaps and bounds
it provides a high level API in three
languages not one Java Scala Python and
I in fact for right and optimizing user
support general execution graph it's
optimized way beyond MapReduce right and
it also supports a rich set of
higher-level to look at it very very
carefully my friends this is where I'm
going to be introducing machine learning
to you it also supports a rich set of
high-level tools including spark SQL for
SQL structured data processing machine
learning em live for machine learning a
package called as MD for machine
learning which I'm going to run through
quickly freeze over here right graphics
for graph processing and spark streaming
for streaming streaming API why don't
you show you quickly go and show you a
graphic over here there you are night
spark is arranged like this look at the
middle portion spark or night look at
the upper portion spark SQL spot
streaming M live graphics and then below
it needs it utilizes certain schedulers
me sauce standard ruler yarn which is
Hadoop scheduler right which is Hadoop
shallow scheduler so it it it takes care
of it it makes use of that as well
nice excellent how does spark work
this is hot spot work similar to your
java api similar to your Hadoop api
spark like Hadoop has got a job job or
job context object in it
this park has got a spire context right
like Hadoop has got yarn Spock has got
cluster manager like Hadoop has got
worker nodes like Hadoop has got your
slave nodes spark has got worker nodes
like Hadoop of word mapper and reducer
spark has got executed right like Hadoop
has got yon child spark has got tasks
but the things which are additional over
here is the tasks over here does not
dump the output on the mapper and cache
is something which keeps on caching the
data as your tasks are running and then
finally when the jobs are done it dumps
the output right so that is how it
differs and that is how it differs right
but this quick introduction to spark of
this fire to you my friends right now
spike works on a very basic concept
called as oddities he says resilient
data structures right so an oddity in
spark just kind of you know give you a
formal you know introduction to it and
then you know maybe load an oddity and
show you what transformation in actions
are right on my shell an oddity in spark
is simulated simply an immutable
distributed collection of objects night
each our daily is split into multiple
partitions so these are you know these
these these these these portions that
you see over here on work on node 1 work
on new node 2 are node n this is where
the moment you load the data the loader
go the the data goes and sits over there
right the data goes and sits over there
I mean like multiple partition which may
be computed on different nodes of the
cluster there you are
different nodes of the cluster right
what the notebook the notebook a node
worker node over here Wow very good
right our duties can be
guess what of any type written in any
language Python Java Scala are right
just like HDFS loads data in Hadoop
rdd's load data and spark right they're
following two types of Arden is
transform it is an actions night
transformation construct new or did you
from looking this one let me give you an
example
why don't I go ahead and show you a
spark shell while I'm talking over here
there you are I have a spawn stall over
here let me start us action which is in
Scala for you
okay and what I'm gonna do with that I'm
going to load some data for you
there is one day delay on HDFS Hadoop
data word count test right which I can
show you from here I can say that I do
FS - LS not a task at slash data slash
right this data has got seven lines so I
come over here and load this data as an
RDD right so this is the transformation
this is a transformation when I have
loaded the data using s0 takes back SC
is by the way a context spark context
now what I can do is that I can say test
dot count count the number of lines what
will do is it is going to count the
number of lines long is equal to 7 and
there's a lot of other interesting
things I can do over here transformation
will result in storing a data set in two
different different executed note action
like count will process that data very
simple isn't it two lines of code then
50 60 70 lines of MapReduce code
straightaway nicer that is
transformation and action for you let me
just come here and see what all I have
completed so this is done this is done
this is done nice house pot works what I
can do is that I can show you a post
from my blog which basically tells you
the similarities and differences between
Hadouken spa try and understand these
similarities and differences very very
you know carefully because that is going
to be make your premise for learning
machine dirty right so if you see this
post here and I would suggest you go
through all my post actually not only
this one similarities and differences
between loop and SPARC architecture June
6 right so what you can do is you can go
through
I'll just pass it on to everyone through
my chat window and later on we can pass
it on to everyone else you can
understand the similarities and
differences where does it differ
whatever just whatever I have just now
five minutes back said it's all
documented now so you can go through
that as well right so how spark works
from the block you can have a look in
time right it is exactly what I like you
know I said over here this have told you
just now this of told you just now
it basically should help us move on to
machine learning give me one second
just one second just losing my jacket so
what is machine learning what is machine
learning it's good understand machine
definition of formal definition of
machine learning in the typical way I
usually explain things to people right
oh well yeah riding in this mission
right in front of you right so what is
she learning
machine learning is a method of teaching
computers to make and improve
predictions or behaviors based on some
data over a period of time right the
data depends entirely on the problem
right it could be reading from robot
sensors as it learns to walk and then
improving the precision on that work
right or correct output of a program for
certain input or letting a small toy car
know itself how to drive itself in a
much more accurate way there are
obtained number of examples which I can
give you right drop T no more examples
which I can give you there are there are
machine learning can be done in two ways
one is supervised the other one is
unsupervised a supervised machine
learning uses algorithm supervised
algorithm uses label data with both
input and output are provided to the
algorithm right so you
the input you know the output example is
a spam detection or credit card for
detection night or class or classifying
your email basically right so something
like you know you would have seen your
email right you would have seen your
Gmail for example right
you shoulda seen your Gmail now in Gmail
you can basically decide write your
Gmail basically decides which mail to go
into which of your labels where they
should go to social label or you should
go to promotions label or come to your
main inbox or for that matter any level
that you can create or otherwise for
that matter consider to consider it as a
you know it consider it as a spam and
deliver it into then status time folder
it is learning in which computers is not
needed to be explained explicitly
programmed yes over a period of time it
learns with data and imagine if you put
a white big get out of it how much
learning it can do right and who do
being the solution for big data a piece
for the storage so big data Hadoop small
machine learning that's how they are
connected to each other Ravi kala right
that's how they are connected to each
other's that's how in short little time
I'm kind of you know presenting these
three four concepts together to you
right
so classification is one example Gmail
the other one is unsupervised night
unsupervised algorithm do not have
output in advance these algorithms are
left to make sense of the data without
tables example is Google News right
Google News does not have any label you
know that you know what just create the
categories top stories business right
top stories business and then sports
Technology night things suggested for
you right new suggested for you and then
it does the trick fine
it shows the relevant news in the
relevant section yeah
that's a clustering based based on
machine yeah this is another cordless
collaborative filtering maybe we'd have
not you know kind of you know I
mentioned over here I'm not mentioning
over here because the long topic and
we've got less time so that is what is
the brief background of the machine
turning right what I'm going to do is
that I'm going to now quickly show you
some steps in how to do that with spark
right and then I'll run an example for
you quickly to show you how you write
the code for it just to get you a give
you a whiff of how things work right so
these and by the way this sheet my TOC
sheet is something that you can get from
the support team because I believe it
has important information over here
which you can use so you can get it from
my team and you can talk to support team
and they'll be able to provide it to you
night let me go through these steps
first of all introduce these are steps
to you and then show you a graphic and
then move on to an example code which
will implement these concepts and then I
will run it for you on the scholarship
sounds like a plan right okay so um the
first thing that you do is you create an
RDD of your input data right now we know
what the RTD is and let's say we put the
input data on HDFS
so clearly already DB from the input
data that is the first step the second
step is run one of your omlet feature
extraction algorithm right there is a
concept called as feature ization in
machine learning what what it does is is
it picks up the words it picks up the
words from your input and then assign it
to certain numerical values right it is
is a concept similar if you working in
Hadoop it's a concept similar to hash
modulo partitioner in Hadoop right what
it does is that it picks up a value find
out the ASCII key divides it by whatever
number that you have defined right and
then comes up with a comes up with a
remainder depending upon the remainder
you create categories right
depending on the categories you do
feature ization and then you say that
okay this is this is this is this
particularly this is the other
particular category okay so run one of
your emblem feature extraction algorithm
to convert text applied into numerical
value in fact leash who you is right to
give you an idea of how it works right
let's say Ted and Carol like movies B
and C they're three movies a B and C
they're in Carol like movie B and C so
what the way you have come to this
conclusion is that you have taken our
training data if use an algorithm you
come up with the model right so you come
to know that B and C is something which
is liked by Ted and Carol in comes Bob
who likes movie B so with this model you
can predict whether Bob is going to
likes see you or not and over a period
of time similar predictions you came to
so it's about making the training data
providing an algorithm to it coming up
with a model and then using this model
supplying new data coming up with
predictions right so create a model
supply new data come up with predictions
that's how it works that's how it works
hopefully it makes sense to you now
right so let me just come back come out
of here and say that run run run of your
Emily feature extraction algorithm to
convert take supplies to numerical value
they should give you back an RDD of
numerical vectors it's a it's a what you
call Spartan vector it kind of figure
represents digit in numbers and
numerical values in that form this
process is called a feature ization or
model creation or feature extraction
right you use you use algorithms like
hashing TF or idiot there are certain
algorithms to do this feature ization
already available in spark there are
certain algorithms which are already
available for you you don't have to
create anything nice
second step feature addition step create
labels right main email socials email
promotions email spam email create a
label spam are normal and they're
labeled pointed assets right
so label point data sets what it does is
that it matches up it matches up your
labels to these numerical values right
it matches up these labels to the
numerical values now you see the picture
right you had the data you converted the
data into vectors you connected the
vectors to the label value and using the
label value in the first step you create
the tracking data in the fifth step you
create the model and in the sixth step
you test that model using in the fifth
step you do this create the model the
first point algorithm model training it
algorithm model in the fifth step in the
sixth step you evaluate the model by
giving predictions right by providing
predictions as simple as that
easy peasy like and the best part about
SPARC is that spark already has inbuilt
you know in build you know in build your
future eyes Asian models as well as
these classification algorithms so you
when you create a model right over a
period of time you would have library
has developed certain classification
algorithms you supply the data to that
classification you got it like
logistical regression nice in a new
value at that model right so two things
part does for you which is 80% of the
work regionalization and classification
you just supplies later and sit back and
enjoy make use of it right so
again load the data in the oddities do
the feature ization create the label
create the label label point where your
your labels get multiple vectors night
create the training data right and using
the training data and using the
classification algorithm create a model
and then evaluate the model come up with
predictions that's how you create
classifiers in spark yeah that's how you
say create + r/n spark now you can as I
said you can have a copy of this sheet
from the supporting you know for your
reference let me see what completed over
here right this is done why don't I go
ahead and run the example figure now no
to get a complete idea of what it looks
like amazing it isn't and sounds also
simple so a spam classifier right and
this file also you can get from the
supporting night ah those of you worked
in Java
excuse me those of you working Java
Scala you know that you know what
reporters right my special over here
based in put all these values so my
libraries are important look at this
feature ization look at this
classification right it's all in my own
time now it's all in my own time it's
all in my own time right then I have
this data over here which I put on my on
my HDFS this is the span data right cut
I upload it right so RDD creative
remember the first step create RDD right
from here create the oddity
pretty ugly for this man and for this
pan as well as for the normal values the
normal values kara yeah but it unloaded
created oddities now the second step
declare your your hash tagging value
which will basically help you convert
defrag the detects inside this pan not
tht n test dot txt into features of our
duties of vector right so enter here I
created a hashing you know tag value and
divided created the divisor has ten
thousand that means it is going to find
out it is going to find out the the
ASCII value of each and every word
inside your span dot txt and test dot
txt in the next step right and divided
by ten thousand and whatever the
remainder it is going to categorize
according to that remainder right said
the documented all this over your create
create a hash tagging instead to mount
email text two vectors up and throw the
features amazing now look at the next
step I'm still doing feature ization
picking up individual here I'll just
declared D the algorithm right now do
the splitting each email is split into
words each word is mapped to one feature
using hash modulo by ten so here we'll
get an oddity of vectors between one
minus one minus n then for both normal
span files right let's do it for
complete the feature ization for span
five i'm using a method call as map over
here right let's use the same thing for
the normal files easy peasy
isn't it alright now the third step
labels data point oops the label data
point so go back here it's mime
classifier label data point I'm going to
create positive data points it will take
spam that means it's a spam and positive
right and I'm going to take negative
data points accident I'm going to create
training data for you to get your
training data and then I'll create the
model right I'll unionize both the
positive and the negative so my training
data is ready now to model as simple as
that using spark API and training it on
the model I have the model ready now the
model ready now right then at the model
ready I'm going to supply the new data
to it right I'm going to supply the
generator and c4 check for predictions
so I am supplying oops I am supplying a
spam data oh my god get cheap stuff by
sending money to blah blah blah
sounds like spam and then negative test
that means proper man hi dad I started
studying spark the other day right faced
when I get into the complete course I
you know you know get into the details
of it as to you know what it means and
all that here I'm kind of you know just
skimming through the surface of the
concept and then you can see the
positive set a positive step it will
give you one that means spam was spam
and the normal was normal right so
you basically passed on the value passed
on a value which was spam so it said
okay 1.0 it's a span you passed on a
value which is normal value it said okay
fine this is normal right and it told
you whether it's it's a spam or a normal
value right so that's how that's how
machine learning works in inspark
in fact what it also do with that I
usually pass on this information this
link to all my students for you to go
through go through machine learning in
detail so what I suggest is that if you
guys get a chance apart from going
through my blog go through this link as
well how to finalize the value for the
divisor usually you look at the size you
know it's like you know GB 2gb to 10,000
so if you got a GB of data in fact a
terabyte to 10,000 right so terabyte of
data one each for each of the
classification have 10,000 then in
becomes two terabyte make it make it you
know penny thousand and that's that
that's basically the route of rule of
thumb I know to come on alright why I
can do is let me just pass on this link
to you so that go through this you know
it basically tells you you know
different different types of and it
basically give you an example of a
machine learning of third type of
machine learning which is collaborative
filtering using decision tree right and
I saw another course today morning which
actually had a very interesting case
study on how to do you know do movie
prediction based on the data of 20
million users where who have you know
given them you movie preferences in the
past but I cannot share it to you right
now because it's I think it's from one
of the other vendors similar paid
players but but you know this is one
thing which I can you know kind of
suggest to you
so that would be pretty much it I would
say I just had it over do now do
you know for us to go through the
question on sessions and we can take it
from there
Jay you live with me I am sir absolutely
thank you so much for that really
in-depth and for me personally not being
I I would be a 1 on the scale of 1 to 4
familiarity with machine learning so
that was some very in-depth stuff for me
and I really appreciate the granularity
of what you what you present it so thank
you we do have some questions lined up
before we do that though I'm going to
ask everyone to indulge me for just one
moment while I take over the screen and
launch this quick little poll for the
folks that have joined us here and the
reason that I do that is this is a
public facing webinar that means of
course it's open to anyone to join us
and sit in with our subject matter
experts like say it and to pardon me
sorry I got I got distracted there for
just a moment and so it's available for
anyone to to join us here whether you're
already a member of simply learn or
whether you have never heard of us
before outside of the context of these
webinars we do many many of these
webinars every month and we share a
variety of different topics of learning
across data science and project
management and Six Sigma and digital
marketing and so on and so on covering
both very timely topics as well as more
strategic kind of evergreen overviews
depending on what it is that's going on
in that industry at that time but our
core service is more than just the
webinars we are a business and
technology training provider and as
that's what we do is we have over 400
web-based training courses across a wide
spectrum of all of these various skill
sets and these courses are accredited by
more than 40 third-party certifying
bodies like prints 2:00 p.m.
CP the open group exit and so forth so
we design our training specifically to
prepare you for the professional
certification exams that are delivered
by these organizations and we base our
training on the very latest body of
knowledge that those organizations
prescribed all of these courses are
designed and presented by our faculty of
very experienced professional trainers
who are working in the field and when
you join these training programs you get
the structured courses you get all of
the information you get video lessons in
some cases you get live virtual
classroom training depending on the sort
of course that it is and the the access
that you decide to pursue you get
hands-on practical experience lab
exercises simulation exams in certain
cases you'll get access to additional
platforms and software to help you run
experiments and practice projects to get
that hands-on experience that you need
beyond simply the training and you also
get access to our moderated forums where
you can interact directly with our
faculty and with other students so if
you want to know more about taking
courses that can help prepare you for
these certification exams by these
parties and in this case with regard to
machine learning if you want to learn
more about the courses we have on
machine learning and on Big Data just
take the appropriate box we'll have
someone get in touch with you there's no
obligation but we can show you around
give you an idea of what the training
platform looks like what courses are
available to you and help you make the
right decision to reach the goals that
you have set for yourself for your
career and for your business back to
being able to see your screen
I hope go to webinars a little slow
sometimes there we go now we're back to
seeing your screen again sir so we do
have some questions queued up I'll start
with the most common question which is
can I get the slide deck or did you
record the webinar and the answer is yes
we always record all the webinars though
they will go through a production
process we'll clean them up and most of
them make it into the archive at the
simply learned site where you can access
them and you should also keep an eye on
our YouTube channel as well then we have
other questions and
you know you can see them there in the
question level too sir Mallika is asking
about octave as a programming language
if you are familiar with that and its
relevance to using octave which is a
Linux friendly programming language for
machine learning reliable and relevant
no not not not really nowadays
Parabolica machine learning has been
primarily done in either mouth which is
an apache top-level project on spark
that is what I've seen it I have a
couple of you know clients over here and
Sidney whom I am working here you know
with primarily it's it's it's it's
either sparkers street spark or machine
library or with mahout not with octave
that would be my simple answer I suppose
we like those very digital answers like
that
Mei is asking I'm learning are currently
what do you suggest as my next step on
the learning curve so as we are
preparing for our careers and enhancing
our skills with working on our right now
what would be the next best thing for
your career goal
ma I am assuming that if your learning
are you have a statistic background so
let me talk about your background you
might be having a statistical background
or you have might be having interest in
background you might be having
experience as a business analyst
somewhere and also you would might you
know wanting to you know intend to
become a data scientist if that is your
goal you know I am assuming a couple of
things over here if that is your goal
then the next thing that you should be
doing is you should be learning a
software called establish write a
software called stem you get an idea
about that because R and W go hand in
hand with each other W is much more
sophisticated and also look at one of
the areas where you can see how you can
integrate our with Hadoop and tableau
with Hadoop r with SPARC and tableau
with our tableau with spar with Hadoop
right there are certain epi is called as
our Hadoop
which helps you do that when you do that
right and I've seen that most of the
data scientists they kind of you know
keep doing this doing doing a lot of
work I have been doing one of the
classroom trainings and this gentleman
who was an IIT pass up at a Madras and I
teach an eye and he was he has you know
working in SAS and he wanted he was on
this kind of learning curve and he
wanted to become an analyst a kind of
you know this is analytics analytics you
know wanting to have become a data
scientist in future so that could be
your recommended path if I may is
following up letting you know this says
I am in analytics right now using
tableau so integration with Big Data
technologies like like Hadoop and like
spark right integration with that and
and whenever I say spark all the
libraries of spark right so graphics
your your your M latest your spark your
SQL right all of them
how are and tableau interacts integrates
with them I'm sure you could tell you
there are two full courses and simply
none which will help you do that
but that should be a target and my
assumption is right when I say that
you're moving towards a data scientist
kind of profile which is where you need
it yeah I'll be sharing a couple of
links towards the end to just pointing
people at some appropriate course
choices over at the simply learn site so
we'll keep an eye on your chat screen
because I'll share a few links towards
the very end
fantastic so sure others asking can we
get the data from HDFS local mode to run
a program and spark that's a very
technical question that I don't
understand that also I'm hoping it makes
sense to you
look there's nothing call it I DFS local
board HDFS always works in distributed
mode so I share it I would suggest you
go through the notes once ya get one
basic notes from somebody in the room
somebody in the supporting you might
only want something called as I give the
local know that I've ever heard you can
have HDFS only in distributed mold my
friend and yes from the different
distributed more to indefinitely load
data into this part as I did over here
on my spark shell the the data when I
loaded over here let me tell you let me
tell you let me tell you let me tell you
quickly yeah this is HDFS it DFS's HDFS
distributive Hadoop distributed file
system it's a distributed files and
there's nothing local about it sir
Arthur's mentioning specifically
standalone mode not a node so I think
you got confused over yeah might might
this hdf this over here is standalone
sure right and what I'm doing over here
is unloading data from standalone so
this is a different will be in
standalone and and and and and local
nine local means on your local host will
this right so I'll tell you what let me
just show it to you why don't I show it
to you tell us - okay so I say card test
dot txt this is my test or txt on local
right and when I say Hadoop FS - cat
slash test dot txt this is my ID FS
HDFS can never be local your local can
always be local see I'm saying
it's like watching Mozart on a piano or
keyboard so I guess I get so humbled
when I watch somebody like yourself I
love being doing this so much no wonder
I'm doing this all the time in question
to people let's say Sharma has a very
specific question about cloud era sure I
uses cloud era central 6.4 quick start
running we'll read it let me see
so you're saying I use cloud era using a
point for quick start I ran back
reducing high in order to be a machine
running should I use our high for our
Hadoop you should use our Hadoop in
order to use our for machine learning
you know to use our for machine learning
you should to use our Hadoop because
that has got better a place to integrate
with with with cloud era our Hadoop is a
visit like you know is a is a
encompassing like you know
all-encompassing API Shaima okay thank
you for distilling that question down
for me
I'm a has a great question here that I
really I like this aspect of where the
opportunities are and how to prepare
yourself for a career in a given region
that you're interested in living in I'm
a says that I am more interested in
staying in India so from that
perspective what is the scope of these
technologies in India as a region and
the opportunities the opportunities to
to leverage them into your career in a
meaningful way our enterprise and
companies looking for this technology do
you see growth in India oh sure
oh very much very much I tell you what I
don't know whether you know about this
or not but three million organizations
in Bangalore in India right now who are
not playing who are not paying their
employees in in INR anymore they are
playing them in dollars or a foreign
see that should give you an idea about
how the like you know how what is the
scope of the industry these technologies
in India right now Bangalore is one
place where I have seen I am based out
of Bangor I have seen such wonderful and
phenomenal and creative ideas people are
coming up with as far as you know
starting new initiatives are concerned
right so and I've also seen that there
you know there are a lot of my friends
sort of my associate is all of my
colleagues and lot of my even the
students who are leaving places like us
leaving places like you know Europe and
coming back to it back back to India and
making base in cities like Hyderabad and
Bangalore and because that is where a
lot of action is happening in these
technologies at this point of time I
mean if you have any intention
inclination to work in a startup company
where they would be working in some like
an a very very innovative idea you know
here's a place to be right now that's
what I feel and this is I'm sending you
on the basis of all my observation
because I keep yeah I keep I keep you
know being in touch being being in touch
with these kind of people am i saying
how do you connect to these company
paying in USD oh that's a trade secret
I'm kidding
actually what happens is I'm a once you
establish yourself you're good and let's
say you have some experience you you've
done your you know clouded a
certification you have this you know
couple of certifications or simply not
and only what nowadays the world is such
that you don't have to go and find
companies you don't have to go and find
organizations they find you they find
you in Lindon they find you on your
social on your networking profile and
they get in touch with you that has
always worked for me there's no reason
why it should not work for you that's a
great segue into another question that
comes up somewhat often you mentioned
LinkedIn do you have any recommendations
for presenting yourself in the best
light with regard to
you know this machine learning and big
data and so forth on LinkedIn do you
have any insight into how companies are
using LinkedIn to vet prospective
contractors or employees and besides
posting our certifications on our
LinkedIn profile what other things can
we do to our profile to make ourselves
look more attractive I tell you what tip
and this is what I believe and I think
in one of my other webinars also I
mentioned this see what people need now
is a lot of people in that bless you who
have a lot of conceptual knowledge
what what what people need now is the
surety the assurance that you know how
much practical hands-on experience you
have so I'll give a very quick idea and
this always worked for me
let me just open this for you tip chip
and and all of you all for watching
walks
okay what you can do is have a mighty
note set up my instruction structions of
their money on my blog have a mighty
note set up do a multi node set up on
your machine this is my multi node
setter you know Hadoop master is there
and OOP
secondary is there there are slaves
there and things like that right have a
multi node set up and in this multi node
set up implement some two three use
cases it's a Big Data implement some use
cases create a YouTube YouTube video of
that right and post it on your profile
it nice say that I have a multi node
cluster where I have implemented these
five use cases and I have got ten more
which if provided if needed I can
provide it to me I have one such video
on my LinkedIn profile to get a Q you
can have a look at that when the when
when the prospective employers employers
come to your Linden and see such
hands-on work from your side you know
they know that the guy is not kidding he
really means business
so that is one
the way that I can give you which is
always well for me which is which I
think what about for everyone I think
that's that's really brilliant actually
you know to make the video I mean not
just to have it prepared on your screen
for when you're in an interview or just
or talking with somebody but to go ahead
and make that video and have it
associated with your profile so that
people can see how you've mastered
certain aspects of this project
development is that's that's terrific I
love that couple of quick questions from
provoke again what time do we have 27 so
we've got a few more minutes here and
Smita first I'm sorry was was first in
line here I've done the Hadoop developer
course from simply learned what do you
think I should go to next to pursue my
career in Duke oh do you as much
hands-on as as you can as I was you know
suggesting in my previous answer as much
hands-on as much hands-on take crazy
datasets implement use case put it on
your LinkedIn profile create a video put
it on your LinkedIn profile have your
own blog write do as many you know 30 40
use cases implement 30 40 use cases on a
multi node cluster right and project
yourself like that to the prospective
employer see technologies will keep
coming right there are if you go to
Hadoop website right now there are some
25 components right they will keep
coming they will keep changing right and
that is something that you will have to
keep on adapting to but that's fine the
expectation is a lot more in the
industry right now I'm sure you are
asking this question from the
perspective of how to make yourself more
saleable in the job market and on that
perspective I can only suggest that give
you know project yourself as a hands-on
person tonight some two or three use
cases using these these combination of
technologies right you know you are now
that you have done this course from
simply learns or something which is
commonly have a big high wedge base you
know MapReduce the scoop flume and then
taejun jaguar and and then you know some
a bit of what you call Kafka in it a bit
of spark in it you know get up you know
get a flavor of third you know that kind
of you know
spicy stuff if you can present on your
website and make yourself more you know
so so in one short answer is your
technologies and components you will
anyway learn but do more of hands-on in
terms of how you can implement use cases
that is available we are seeing for an
employer to see that practical
application right so they know that you
know how to do these things with your
hands and not just from book learning
and I think this addresses to provola
cos follow-up questions here says I'm a
BTech graduate can I come to machine
learning without any experience would
having a certificate be enough to get a
job and I think the answer to that and
you'll correct me if I'm wrong if I'm
wrong but the certificate all by itself
may not be enough to set you head and
shoulders above your competition in the
job market anymore right you need to
show these hands-on examples I think me
and you are on the same page on that
terrific I hope that helps provoke and
we are at nine well my time nine twenty
nine we are at the half hour in any case
so I think we've used up our whole hour
I hope we got to everybody's questions I
think we did I'm gonna share a few more
links here into the chat module so
everybody can take a look there real
quick what I've done is I've shared some
links directly to some of the simply
learn courses on machine learning
certification and on Big Data cloud
platform machine learning fundamentals
I've also given you some other resources
there some articles and ebooks on the
simply learned site as well as again I
have Reshard
say ads blog link and also the mapper
com
Apache spark learning tutorial link and
lastly a link to the other upcoming
webinars and resources organized by
specialties so if you also want to
follow up on let's say project
management or digital marketing you'll
find all of our specialties organized
on that last link and you'll find them
articles and eBooks and white papers and
upcoming webinars and all sorts of
interesting things available to you to
continue your learning and with that
said sir I believe we have filled our
hour so it was fun hanging out here with
you again thank you once more for
sharing your wisdom with everybody sad
welcome it's always a pleasure to speak
to you chip and it's always a pleasure
to talk to all these students who come
and are these very interesting questions
absolutely thank you everybody and with
that said enjoy the rest of your morning
or your afternoon or your evening
wherever you might be in the world and
until we see you here next time take
care of yourselves bye bye thank you bye
what bliss
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>