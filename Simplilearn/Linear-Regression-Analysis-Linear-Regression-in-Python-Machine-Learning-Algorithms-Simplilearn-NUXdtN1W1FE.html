<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Linear Regression Analysis | Linear Regression in Python | Machine Learning Algorithms | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Linear Regression Analysis | Linear Regression in Python | Machine Learning Algorithms | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Linear Regression Analysis | Linear Regression in Python | Machine Learning Algorithms | Simplilearn</b></h2><h5 class="post__date">2018-03-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NUXdtN1W1FE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to linear regression my name is
Richard Kirchner and with simply learned
let's look at an example of a common use
for linear regression profit estimation
of a company if I was going to invest in
a company I would like to know how much
money I could expect to make so we'll
take a look at a venture capitalist firm
and try to understand which companies
they should invest in so we'll take the
idea that we need to decide the
companies to invest in
we need to predict the profit the
company makes and we're going to do it
based on the company's expenses and even
just a specific expense in this case we
have our company we have the different
expenses so we have our R&amp;amp;D which is
your research and development we have
our marketing we might have the location
we might have what kind of
administration is going through based on
all this different information we would
like to calculate the profit now in
actuality there's usually about 23 to 27
different markers that they look at if
they're a heavy duty investor we're only
going to take a look at one basic one
we're gonna come in and for simplicity
let's consider a single variable R&amp;amp;D and
find out which companies to invest in
based on that so we take our R&amp;amp;D and
we're plotting the profit based on the
R&amp;amp;D expenditure how much money they put
into the research and development and
then we look at the profit that goes
with that we can predict a line to
estimate the profit so we draw a line
right through the data when you look at
that you can see how much they invest in
the R&amp;amp;D is it good markers - how much
profit they're gonna have we can also
note that company's spending more on R&amp;amp;D
make good profits so let's invest in the
ones that spend a higher rate in their
R&amp;amp;D what's in it for you first we'll
have an introduction to machine learning
followed by machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with linear
regression let's look at another example
of machine learning based on the amount
of rain
how much would be the crop yield so we
here we have our crops we have our
rainfall and we want to know how much
we're gonna get from our crops this year
so we're gonna reduce two variables
independent and dependent the
independent variable is a variable whose
value does not change by the effect of
other variables and is used to
manipulate the dependent variable it is
often denoted as X in our example
rainfall is the independent variable
this is a wonderful example because you
can easily see that we can't control the
rain but the rain does control the crop
so we talk about the independent
variable controlling the dependent
variable let's define dependent variable
as a variable whose value change when
there is any manipulation the values of
the independent variables it is often
denoted as Y and you can see here our
crop yield is dependent variable and it
is dependent on the amount of rainfall
received now that we've taken a look at
a real-life example let's go a little
bit into the theory and some definitions
on machine learning and see how that
fits together with linear regression
numerical and categorical values let's
take our data coming in and this is kind
of random data from any kind of project
we want to divide it up into numerical
and categorical so numerical is numbers
age salary height we're categorical
would be a description the color a dog's
breed gender categorical is limited to
very specific items we're numerical is a
range of information now that you've
seen the difference between numerical
and categorical data let's take a look
at some different machine learning
definitions we look at a different
machine learning algorithms we can
divide them into three areas supervised
unsupervised
reinforcement we're only gonna look at
supervised today unsupervised means we
don't have the answers we're just
grouping things reinforcement is where
we give positive and negative feedback
to our algorithm to program it and it
doesn't have the information until after
the fact but today we're just looking at
supervised because that's where linear
regression fits in in supervised data we
have our data already there and our
answers for a group and then we use that
to program our model and come up with an
answer the two most common
uses for that is through the regression
and classification now we're doing
linear regression so we're just gonna
focus on the regression side and in the
regression we have simple linear
regression we have multiple linear
regression and we have polynomial linear
regression now on these three simple
linear regression is examples we've
looked at so far where we have a lot of
data and we draw a straight line through
it multiple linear regression means we
have multiple variables remember where
we had the rainfall and the crops we
might add additional variables in there
like how much food do we give our crops
when do we harvest them those would be
additional information add into our
model and that's why B multiple linear
regression and finally we have a
polynomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess what there's gonna go up or
down or say buy today housing sales to
estimate the number of house as a
builder would sell and what price in the
coming months score predictions cricket
fever to predict the number of runs a
player with score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we
get since linear regression is based on
drawing a line through data we're gonna
jump back and take a look at some
Euclidean geometry the simplest form of
a simple linear regression equation with
one dependent and one independent
variable is represented by y equals M
times X plus C and if you look at our
model here we plotted two points on here
x1 and y1 x2 and y2 y being the
dependent variable remember that from
before and X being the independent
variable so Y depends on whatever X is M
in this case is the slope of the line
where M equals the difference in the y2
minus y1 and x2 minus x1 and finally we
have C which is the coefficient of the
line or where happens to cross the 0
axis let's go back and look at an
example we used earlier linear
regression we're going to go back to
plotting the amount of crop yield based
on the amount of rainfall and here we
have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y-axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the green dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive in to the math and the formulas
that go behind this and what's going on
behind the scenes I want you to note
that when we get into the case study and
we actually apply some Python script
that this math you're gonna see here is
already done automatically for you you
don't have to have it memorized it is
however good to have an idea what's
going on so if people reference the
different terms you'll know what they're
talking about let's consider a sample
dataset with 5 rows
and find out how to draw the regression
line we're only gonna do five rows
because if we did like the rainfall with
hundreds of points of data that would be
very hard to see what's going on with
the mathematics so we'll go ahead and
create our own two sets of data and we
have our independent variable X and our
dependent variable Y and when X was one
we got y equals two when X was two Y was
four and so on and so on
if we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is what the means is of each of the
data coming in the X and the y the means
doesn't mean anything other than the
average so we add up all the numbers and
divide by the total so 1 plus 2 Plus 3
plus 4 plus 5 over 5 equals 3 and the
same for y we get 4 if we go ahead and
plot the means on the graph we'll see we
get 3 comma 4 which draws a nice line
down the middle a good estimate here
we're gonna dig deeper into the math
behind the regression line now remember
you for I said you don't have to have
all these formulas memorized or fully
understand them even though we're gonna
go into a little more detail of how it
works and if you're not a math whiz and
you don't know if you've never seen the
Sigma character before which looks a
little bit like an e it's opened up that
just means summation that's all that is
so when you see the Sigma character it
just means we're adding everything in
that row and for computers this is great
because as a programmer you can easily
iterate through each of the XY points
and create all the information you need
so in the top half you can see where we
broken that down into pieces and as it
goes through the first two points it
computes the squared value of X the
squared value of y and x times y and
then it takes all of X and adds them up
all of Y adds them up all of x squared
adds them up and so on and so on and you
can see we have the sum of equal to 15
the sum is equal to 20 all the way up to
x times y where the sum equals 66 this
all comes from our formula for
calculating a straight line where y
equals the slope times X plus a
coefficient C so we go down below and
we're
compute more like the averages of these
and we'll get explain exactly what that
is in just a minute and where that
information comes from is called the
square means error but we'll go into
that in detail in a few minutes all you
need to do is look at the formula and
see how we've gone about computing it
line by line instead of trying to have a
huge set of numbers pushed into it and
down here you'll see where the slope M
equals and then the top part if you read
through the brackets you have the number
of data points times the sum of x times
y which we computed one line at a time
there and that's just a 66 and take all
that and you subtract it from the sum of
x times the sum of Y and those have both
been computed so you have 15 times 20
and on the bottom we have the number of
lines times the sum of x squared
usually computed as 86 for the sum -
I'll take all that and subtract this sum
of x squared and we end up as we come
across with our formula you can plug in
all those numbers which is very easy to
do in the computer you don't have to do
the math on a piece of paper or
calculator and you'll get a slope of
point six and you'll get your C
coefficient if you continue to follow
through that formula you'll see it comes
out as equal to two point two continuing
deeper into what's going behind the
scenes let's find out the predicted
values of Y for corresponding values of
X using the linear equation where M
equals point six and C equals two point
two we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y equals
point six x where x equals one plus two
point two equals two point eight so on
and so on and here the blue points
represent the actual Y values and the
brown points represent the predicted Y
values based on the model we created the
distance between the actual and
predicted values is no one as residuals
or errors the best fit line should have
the least sum of squares of these errors
also known as a square if we put these
into a nice chart where you can see X
and you can see why what we actual
values were and you can see why I
predicted you can easily see where we
take Y minus y predict it and we get an
answer what is the difference
between those two and if we square that
why - why prediction squared we can then
sum those squared values that's where we
get the point six four plus the point
three six plus one all the way down
until we have a summation equals two
point four so the sum of squared errors
for this regression line is two point
four we check this error for each line
and conclude the best fit line having
the least square value in a nice
graphical representation we can see here
where we keep moving this line through
the data points to make sure the best
fit line has the least squared distance
between the data points and the
regression line now we only looked at
the most commonly used formula for
minimizing the distance there are lots
of ways to minimize the distance between
the line and the data points like sum of
squared errors sum of absolute errors
root mean square error etc what you want
to take away from this is whatever
formula is being used you can easily
using a computer programming and
iterating through the data calculate the
different parts of it that way these
complicated formulas you see with the
different summations and absolute values
are easily computed one piece at a time
up until this point we've only been
looking at two values x and y well in
the real world is very rare that you
only have two values when you're
figuring out a solution so let's move on
to the next topic multiple linear
regression let's take a brief look and
what happens when you have multiple
inputs so on multiple linear regression
we have well we'll start with the simple
linear regression where we had y equals
M plus X plus C and we're trying to find
the value of y now with multiple linear
regression we have multiple variables
coming in so instead of having just X we
have x1 x2 x3 and instead of having just
one slope each variable has its own
slope attached to it as you can see here
we have M 1 M 2 M 3 and we still just
have the single coefficient so when
you're dealing with multiple linear
regression you basically take your
single linear regression and you spread
it out so you have y equals M 1 times X
1 plus M 2 times X 2 so on all the way
to M to the N X to the N and then
you had your coefficient on their
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R&amp;amp;D trying to figure out its profit
we're gonna start looking at the
expenditure of the company we're gonna
go back to that we're gonna predict his
profit but instead of predicting it just
on the R&amp;amp;D we're gonna look at other
factors like administration costs
marketing costs and so on and from there
we're gonna see if we can figure out
what the profit of that company is going
to be to start our coding we're gonna
begin by importing some basic libraries
and we're gonna be looking through the
data before we do any kind of linear
regression we're gonna take a look at
the data to see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with import in the
basic libraries now I'm gonna be doing
this in anaconda Jupiter notebook a very
popular IDE I enjoy it's such a visual
to look at it's so easy to use just any
ID for python will work just fine for
this so break out your favorite Python
IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries were
importing first we're going to import
numpy as in P and then I want you to
skip one line and look at import pandas
as PD
these are very common tools that you
need with most of your linear regression
the numpy which stands for number Python
is usually denoted as in P and you have
to almost have that for your SK learn
toolbox you always import that right off
the beginning pandas although you don't
have to have it for your SK learn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
I'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import mat plot
library pie plot as p LT and then
Seabourn as SN s Seaborn works with the
mat plot library so you have to always
import map
library and then Seaborn sits on top of
it and we'll take a look at what that
looks like you could use any of your own
plotting libraries you want there's all
kinds of ways to look at the data these
are just very common ones and the
Seabourn is so easy to use it just looks
beautiful it's a nice representation
that you can actually take and show
somebody and the final line is the amber
signed map plot library in line that is
only because I'm doing an inline IDE my
interface and the Anaconda Jupiter
notebook requires I put that in there or
you're not gonna see the graph when it
comes up let's go ahead and run this
it's not going to be that interesting so
we're just setting up variables in fact
it's not going to do anything that we
can see but it is importing these
different libraries and setup
the next step is load the data set and
extract independent and dependent
variables now here in the slide you'll
see companies equals PD read CSV and it
has a long line there with the file at
the end 1000 companies CSV you're gonna
have to change this to fit whatever
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and supply
you with that file so you can try this
coding yourself so we're gonna add this
code in here and we're gonna see that I
have companies equals PT reader
underscore CSV and I've changed this
path to match my computer C colon slash
simply learned slash 1000 underscore
company's dot CSV and then below there
we're gonna set the x equals two
companies under the eye location and
because this is companies as a PD
dataset I can use this nice notation
that says take every row that's what the
colon
the first colon is comma except for the
last column that's what the second part
is where we have a colon minus one and
we want the values set into there so X
is no longer a data set of pandas data
set but we can easily extract the data
from our pandas data set with this
notation and then Y we're gonna set
equal to the last row
well the question is gonna be what are
we actually looking at so let's go ahead
and take a look at that and we're gonna
look at the companies dot head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the pandas
when I hit run you'll see it breaks it
out into a nice setup this is what pan
is one of the things pandas is really
good about cuz it looks just like an
Excel spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
zero one two three four and then it
shows your different columns R&amp;amp;D spend
administration marketing spend state
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data
so here I've opened it up as a text
editor and you can see at the top we
have our nd spend comma administration
comma marketing spin comma state comma
profit Kerry's returned I don't know
about you but I'd go crazy trying to
read files like this that's why we use
the pandas you could also open this up
in an excel and it would separate it
since it is a comma separated variable
file but we don't want to look at this
one we want to look at something we can
read rather easily so let's flip back
and take a look at that top part the
first five row now as nice as this
format is where I can see the data to me
it doesn't mean a whole lot maybe you're
an expert in business and investments
and you understand what one hundred and
sixty five thousand three hundred and
forty nine dollars and twenty cents
compared to the administration cost of a
hundred and thirty six thousand eight
hundred ninety seven dollars and eighty
cents so on so on helps to create the
profit of one hundred ninety two
thousand two hundred sixty one and
eighty three cents that makes no sense
to me whatsoever no pun intended so
let's flip back here and take a look at
our next set of code where we're gonna
graph it so we can get a better
understanding of our data and what it
mean so at this point we're gonna use a
single line of code to get a lot of
information so we can see where we're
going with this let's go ahead and paste
that into our notebook and see what we
got going and so we have the
visualization and again we're using SNS
which is pandas as you can see we
imported the mat plot library pipe lot
is PLT which then the Seabourn uses and
we imported the Seabourn as SNS and then
that final line of code helps us show
this in our in line coding without this
it wouldn't display
you could display it to a file and other
means and that's the mat plot library in
line with the amber sign at the
beginning so here we come down to the
single line of code
Seabourn is great because it actually
recognizes the Panda data frame so I can
just take the companies dot core for
coordinates and I can put that right
into the Seabourn and when we run this
we get this beautiful plot and let's
just take a look at what this plot means
if you look at this plot on mine the
colors are probably little bit more
purplish and blue than the original one
we have the columns and the rows we have
R&amp;amp;D spending we have an administration
we have marketing spending and profit
and if you cross index any two of these
since we're interested in profit if you
cross index profit with profit it's
gonna show up if you look at the scale
on the right way up in the dart why
because those are the same data they
have an exact correspondence so R&amp;amp;D
spending is gonna be the same as R&amp;amp;D
spending and the same thing with
administration costs are right down the
middle you get this dark row or dark
diagonal row that shows that this is the
highest corresponding data that's
exactly the same and as it becomes
lighter there's less connections between
the data so we can see with profit
obviously profit is the same as profit
and next it has a very high correlation
with R&amp;amp;D spending which we looked at
earlier and it has a slightly less
connection to marketing spending and
even less to how much money we put into
the administration so now that we have a
nice look at the data let's go ahead and
dig in and create some actual useful
linear regression models so that we can
predict values and have a better profit
now that we've taken a look at the
visualization of this data we're gonna
move on to the next step instead of just
having a pretty picture we need to
generate some hard data some hard values
so let's see what that looks like we're
going to set up our linear regression
model in two steps the first one is we
need to prepare some of our data so it
fits correctly and let's go ahead and
paste this code into our jupiter
notebook and or bring in anis we're
gonna bring in the SK learn
pre-processing we're gonna import the
label encoder and the one hot encoder to
use the label encoder we're going to
create a variable called label encoder
and set it equal to capital L label
capital e
coder this creates a class that we can
reuse for transferring the labels back
and forth now about now yes what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the company's head
and we printed the top five rows of data
we have our columns going across
we have column 0 which is R&amp;amp;D spending
column 1 which is administration column
2 which is marketing spending and column
3 estate and you'll see under state we
have New York California Florida now to
do a linear regression model it doesn't
know how to process New York it knows
how to process a number so the first
thing we're gonna do is we're gonna
change that New York California and
Florida and we're gonna change those two
numbers that's what this line of code
does here x equals and then it has the
colon comma 3 in brackets the first part
the colon comma means that we're gonna
look at all the different rows so we're
gonna keep them all together but the
only row we're going to edit is the
third row and in there we're gonna take
the label coder and we're gonna fit and
transform the X also the third row so
we're gonna take that third row we're
gonna set it equal to a transformation
and that transformation basically tells
it that instead of having a New York it
has a 0 or a 1 or a 2 and then finally
we need to do a one hot encoder which
equals one hot in order categorical
features equals three and then we take
the X and we go ahead and do that equal
to one hot encoder fit transform x2
array there's final transformation preps
our data for us so it's completely set
the way we need it is just a row of
numbers even though it's not in here
let's go ahead and print X and just take
a look what this data is doing you'll
see you have an array of arrays and then
each array is a row of numbers and if I
go ahead and just do row 0 you'll see I
have a nice organized row of numbers
that the computer now understands will
go ahead and take this out there because
it doesn't mean a whole lot to us it's
just a row of numbers next on setting up
our data we have a voiding dummy
variable trap this is very important why
because the computers automatically
transformed our header into the setup
and it's automatically transformed with
all these different variables
so when we
did the encoder the encoder created true
columns and what we need to do is just
have the one because it has both the
variable in the name that's what this
piece of code does here let's go ahead
and paste this in here and we have x
equals x colon comma 1 colon all this is
doing is removing that one extra column
we put in there when we did our one hot
encoder and our label encoding let's go
ahead and run that and now we get to
create our linear regression model and
let's see what that looks like here and
we're going to do that in two steps the
first step is going to be in splitting
the data now whenever we create a
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and string KITT down a size or two so it
all fits on one line so from the SK
learn model selection we're gonna import
train test split and you'll see that
we've created four completely different
variables we have capital X train
capital X test smaller case Y train
smart case Y test that is the standard
way that they usually reference these
when we're doing different models
usually see that a capital X and you see
the train and the test and the lowercase
Y what this is is X's are data going in
that's our rnd spin or administration or
marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity that's what we're going
to shoot for in this tutorial the next
part train test split we take X and we
take Y we've already created those X has
the columns with the data in it and Y
has a column with profit in it and then
we're going to set the test size equals
point two that basically means 20
percent so 20 percent of the rows are
going to be tested we're going to put
them off to the side so since we're
using a thousand lines of data that
means that 200 of those lines
we're going to hold off to the side to
test for later and then the random state
equals 0 we're going to
randomize which ones it picks to hold
off to the side we'll go ahead and run
this it's not overly exciting since
setting up our variables but the next
step is the next step we actually create
our linear regression model now that we
got to the linear regression model we
get that next piece of the puzzle let's
go ahead and put that code in there and
walk through it so here we go we're
going to paste it in there and let's go
ahead and since this is a shorter line
of code let's zoom up there so we can
get a good luck and we have from the SK
learn the linear underscore model we're
gonna import linear regression now I
don't know if you recall from earlier
when we were doing all the math let's go
ahead and flip back there and take a
look at that do you remember this where
we had this long formula on the bottom
and we were doing all this summarization
and then we also looked at setting it up
with the different lines and then we
also looked all the way down to multiple
linear regression where we're adding all
those formulas together all of that is
wrapped up in this one section so what's
going on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor dot fit in this case we do
X train and Y train because we're using
the training data X being the data in
and why being profit what we're looking
at and this does all that math for us so
within one click in one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals true fit intercept equals true in
jobs equal 1 normalize equals false it's
just giving you some general information
on what's going on with that regressor
model now that we've created our linear
regression model let's go ahead and use
it and if you remember we kept a bunch
of data aside so we're gonna do a Y
predict variable and we're gonna put in
the X test and let's see what that looks
like
scroll up a little bit paste that in
here predicting the test set results so
here we have Y predict equals regressor
dot predict X test going in and this
gives us Y predict now because I'm in
Jupiter in line I can just put the
variable up there and when I hit the run
by
you'll print that array out I could have
just as easily done print why predict so
if you're in a different IDE that's not
an inline setup like the Jupiter
notebook you can do it this way print
why predict and you'll see that for the
200 different test variables we kept off
to the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're gonna take just
a short detail here and calculating the
coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're gonna take a
short detour here and we're gonna be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us we can simply just
print regressor dot coefficient
underscore when i run this you'll see
our coefficients here and if we can do
the regressor coefficient we can also do
the regressor intercept let's run that
and take a look at that this all came
from the multiple regression model and
we'll flip over so you can remember
where this is going into where it's
coming you can see the formula down here
where y equals m1 times x1 plus m2 times
x2 and so on and so on plus see the
coefficient so these variables fit right
into this formula y equals slope one
times
column one variable plus slope 2 times
column two variable all the way to the M
into the N and X to the n plus C the
coefficient or in this case you have
minus 8 point 8 9 ^ 2 etc etc at times
the first column in the second column
and the third column and then our
intercept is the minus one zero three
zero zero nine point boy it's kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're gonna
divide this information we have to make
sure it's a valid model that this model
works and understand how good it works
so calculating the r-squared value
that's what we're gonna use to predict
how good our prediction is and let's
take a look what that looks like in code
and so we're going to use this
from SK learned metrics we're gonna
import our to score
that's the r-squared value we're looking
at the error so in the r2 score we take
our Y test versus our Y predict Y test
is the actual values we're testing that
was the one that was given to us so we
know our true the Y predict of those two
hundred values is what we think it was
true and when we go ahead and run this
we see we get a point nine three five
two
that's the r2 score now it's not exactly
a straight percentage so it's not saying
it's ninety-three percent correct but
you do want that in the upper 90s oh and
higher shows that this is a very valid
prediction based on the r2 score and if
r-squared value of 0.9 1 or 92 as we got
on our model could remember it does have
a random generation involved this proves
the model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model let's take a look at what we went
over today and take a look at our key
takeaways first we have an introduction
to machine learning we talked about some
general set up and predicting crops and
weather we saw this that numbers our age
salary so on then we have categorical
color when we did our actual regression
model we saw that we had numbers which
was dollar amounts and we had a location
which was Florida and New York which was
categorical that we had to convert next
we have application of a linear
regression model we had somebody showing
there some different applications you
could use it for
we had our use case implementation of
linear regression where we dug in deep
showed how those are set up we had our
multiple linear regression model so you
can see the math behind it and finally
prediction using the regression line so
we showed you how to predict things on a
regression line setup and the actual
scripting and code that concludes our
demo today I want to thank you for
joining the simply learned team remember
if you have any questions on the video
or you wish to request a copy of the CSV
file we use to generate this linear
regression feel free to comment down
below and we'll get back to you as soon
as we can thank you very much
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>