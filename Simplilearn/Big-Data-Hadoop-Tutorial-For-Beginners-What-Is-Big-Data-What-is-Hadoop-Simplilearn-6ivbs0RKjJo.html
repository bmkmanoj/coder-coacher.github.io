<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Hadoop Tutorial For Beginners | What Is Big Data? | What is Hadoop? | Simplilearn | Coder Coacher - Coaching Coders</title><meta content="Big Data Hadoop Tutorial For Beginners | What Is Big Data? | What is Hadoop? | Simplilearn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Simplilearn/">Simplilearn</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Hadoop Tutorial For Beginners | What Is Big Data? | What is Hadoop? | Simplilearn</b></h2><h5 class="post__date">2014-10-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6ivbs0RKjJo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so before I begin the session let me
introduce myself everyone I'm Jamie in
battle I I hail from India I am actually
having ten years of experience in
software industry for past three years I
have been working in Big Data and Hadoop
technology and I have developed an
analytics platform where we have used
Hadoop machine learning our and related
technologies extensively where I got my
hands dirty and and you know learnt a
lot in this space apart from that I also
possess interest in organizing the
conferences with people with like-minded
people who want to share the knowledge
in the same space or who are doing some
innovative work in the same space so we
do lot of open conferences in this area
and and and with that I also have a
passion of sharing my knowledge with the
people and that is one of the reason why
we are meeting here so I also do lot of
corporate trainings and I take up
corporate trainings in head open big
data space so today's webinar session is
scheduled for one hour and we will be
covering so I will be just walking you
through the agenda topics where that
would be confidentially thing okay just
kidding but I will I will let you know
in the end okay all right so I will be
just walking you through the agenda and
course topics that we would cover today
as part of this webinar session right so
let us start discussing the agenda so
the agenda that we have designed for
this webinar we have designed it by
keeping in mind that anyone who's very
new in this technology anyone who want
to learn and any any such technologies
for the first time right what are the
basic questions that comes into their
mind the first thing is what is that
technology and
I that is important right so that's how
we will also address this this big
buzzwords we will understand what is big
data and why big data is such a big
buzzword or why industries are spending
huge money in or doing a lot of
investment in big data space and solving
a lot of problems in that right so that
once we understand the importance of big
day a town and and by doing so we will
also cover common big data customer
scenarios we understand how different
companies or different industries are
using big data to solve their business
problems once we do that then we get
into the technologies which which other
technologies which will help us work
with big data and how Hadoop has become
the most effective technology to work
with big data so that's where we
understand the motivation for Hadoop
now in the interest of time we would not
be cover all the technical differences
between Hadoop and the traditional
systems but we would try to touch base
on some of the key difference
differences which are very important and
which would really help us understand
why Hadoop is the most effective new
technology which can help us to work
with big data okay so that's how we have
organized our seminar what I am trying
to make sure that the objective for me
in this seminar is I want to make sure
that after at the end of this seminar or
this webinar I would say everybody who's
attending this webinar should be very
comfortable with the bot and why part of
Big Data and Hadoop
right so let us try to have that
objective in mind and we will begin our
journey with Big Data space free yeah
one more thing feel free to ask me any
questions any time you have and I would
also tell that at the end of the session
we will spend about 5 to 10 minutes
where we address questions from the
audience if anyone has any specific
questions feel free to ask me of that ok
all right so let us start with big data
so what is Big Data
as the name suggests right for us thing
it comes to my mind is lots and lots of
data
right it's about terabytes and petabytes
of data but is this enough
is this really a big data if I have a
huge amount of data can i term it as a
big data so what is it really so before
I go defining it formally or before I go
giving a formal definition about big
data I would like you to you know go
through some of this use cases that I
have captured here I'll be explaining
the use cases and by doing so we will
understand what are the different
sources of big data and how they are
using that okay so the first diagram
that I have shown here is an airline
industry which says the fact says that a
line jet collects about 10 terabytes of
data of sensor data every 30 minutes of
flying time which is which is huge right
so I'm sure many of you are aware of
that there is a there is something
called black box system in an airline
and then jet and that that has all the
sensors attached to it which which help
us to collect all the data which all the
event data which happens in and around
during the flying time and that could
log it into the system which which which
basically generates such a huge amount
of volume now these data can be used by
a nline industries to solve various use
cases right and some of the common use
cases that there's always improving the
performance of a flight how do they
improve how do they make sure that their
flight or the rail line jet gives a
cost-effective performance right that is
the biggest use case for them to sustain
in the business and that is where they
are actually analyzing these logs to
make sure how do they improve the
performance of a flight and make it more
cost-effective
another example or a use case is you
know controlling the air traffic that is
also you know an area where lot of
airlines would actually earlier days
they used to have a control system
centralized control systems and people
used to monitor that
but these days with the data that they
do real-time analytics from the data
which gets generated out of different
airlines and based on that real-time
analytics they actually find whether
there is any traffic or not so let me
give you a very similar example to this
use case I'm sure everybody uses this
smartphone these days and I'm sure
everybody uses GPS so when you go to a
particular let's say when you are
traveling from a location a to location
B right and you want to know which route
you're traveling so obviously you will
use GPS for that
now when you start navigation in your
smart phones if you notice that on that
route it would show you different
different indications and one of the
indications on the route could be I mean
it will show the indication based on
different colors it and one of the you
know fail rate indication is showing it
in a red color if there is any traffic
dude in that route right it will show it
in a red color now if you think how does
it actually see because the navigation
does not work on the Internet it
actually works through satellite so how
does it collect this data how does it
identify that there is a you know
traffic so they solve this problem by
since most of the people they are
assuming that they are having this GPS
enabled and they are moving in the same
direction
all these GPS devices are emitting the
signals and if all the devices which are
emitting the signals which are which are
moving in the same direction and moving
slowly then there will be too many
dotted points on that area and that's
where they identify that okay there is
slow moving traffic here or that could
be a traffic because a lot of devices
in this area and that is one way they
actually predict that okay this is the
point where there could be a traffic so
that similar to that even I learn in
industry is try to solve this problem by
analyzed
is Italy gets accumulated to about
terabytes of data per day so stock
exchange and it's about terabytes of
data every day now there are stock there
are actually companies available in the
market now what they do is they
specializes in predicting the stock
values upfront even before the stock
market opens before the you know stock
opens in that market it actually
predicts how this stock is going to
perform today or at this particular day
or this particular time and they do that
by writing something called
high-frequency trading algorithms I'm
sure many of you are aware of that there
is something called high-frequency
trading algorithm which is a very famous
jargon these days with respect to stock
trading and stock trading markets and
not of investors are using this
technology to you know get more returns
now these algorithms to perform better
they need huge amount of data as an
input so they actually collect they tie
up with such stock exchanges they
collect this data from the stock
exchanges and then based on that they
start predicting the value or a trend
for a particular stock of a company so
that's where you know big data is used
and again the underlying technology that
all of these different industries are
using is Hadoop so we are we are anyway
yet to come to the point where what is
Hadoop and how they do that alright so
let me show you another example which is
a facebook famous Facebook where you
know I think this has become the
integral part of most of our life where
we all use Facebook at least once in a
day right to obviously make sure that we
get socially connected with our friends
and families so I am NOT going to
explain what is Facebook and all but
what I'm trying to emphasize here is
there are some states that is published
by Facebook which says every 20 minutes
on Facebook lot of activities happens
links shared events invites written
request accepted photos uploaded
this sank Facebook has about billion
users or I would say more than that
across the globe and those users will do
such activities every 20 minutes on
Facebook and in turn it will accumulate
to this many number this is the amount
of data which gets generated by doing so
by performing so many activities by such
a massive number of users right and that
accumulates the huge data so what I'm
trying to cover here is you know
Facebook has a very I'm sure everyone
knows that the Facebook has a revenue
model as advertisement they have many
other revenue streams but advertisement
is one of that premium at a revenue
model so let me just spend few minutes
on explaining how they do do-do-do this
advertisement so by the way we all use
televisions at our home right so when we
watch any particular channel or a
program a person who's sitting in USA
watching a particular program and the
person who's sitting in you know India
watching a particular program maybe at
the same time will taste livestream they
may both will get to see the same
advertisement which comes during that
program right now the person who's in
India might not be might be interested
in viewing that but the person who's in
USA might not be interested in doing
that but even in that case both of them
have to see that if they are watching
that program that is called mass
advertisement which means a user is not
allowed or basically advertisement is
not being shown based on the users
interest right
but Facebook has changed the way
advertisement industry works what they
have done is they have done something
called targeted advertisement so with
the analytics with the help of
analytical technology analytics
technologies and Hadoop as an underlying
framework they were collecting such a
massive amount of data from different
users and
every user is registered on Facebook so
Facebook knows about that user when I
say you know if this book knows about
that user which means Facebook knows
about the details of that user what that
user do what activities that user
performs what his or her likes and
dislikes everything Facebook can analyze
by analyzing the data that is there with
for for that particular user and by
doing so Facebook can actually find out
what is that person's likes and dislikes
and based on that they will select what
advertisement we should show to this
particular user on his particular wall
page right so that is called targeted
advertisement and by doing so for
example if someone who's interested in
sports and if he is doing a lot of
sports activities he's uploading out of
sports videos and you know sports photos
and images and he's joining a lot of
sports events and sports groups then
Facebook over a period of time learn by
itself that okay this particular user
has an interest towards sports
activities so if there is an event which
is going to happen let's say a football
FIFA World Cup is going to happen in a
particular City where this user belongs
then Facebook can just show that that
okay this is the event that is happening
and probably to show a lot of
advertisement related to that so chances
of that user going and clicking those
page and finally ending up doing a
transition would be much more higher
than a person who's never interested in
sports right and that answer Facebook
finds it by analyzing this data using
Hadoop as underlying technology right
and you can realize that it is very
important for Facebook to be very
accurate in identifying this analysis
they need to find out what is the users
likes and dislike they should be very
accurate about it then only they can
actually materialize this revenue right
through advertisements so that is one
such important use case where Facebook
uses big data heavily and also Hadoop as
underlying technology okay
another similar example is is what I
wanted to cover is actually Twitter
example so very similar I don't want to
spend much time here but they do they
also have a very similar model where
they have set of users who are doing lot
of tweets on a particular topic and
based on that they do sentiment analysis
of a user's and they build a set of
users who are interested to do some
activity now there is a saying that you
know someone wants to do a startup I
would not say saying but lot of state
says that if someone wants to open a
startup or they want to market that new
product twitter is definitely one of the
social media where they should advertise
their product why because Twitter can
get them the right set of users who are
interested to buy this product right
they are readily available for them and
that's where Twitter scales and thats
where Twitter makes its marks by
analyzing this technologies so that is
one such use case where Twitter also
uses Big Data and Hadoop as an
underlying technology this is the slide
which you know explains you how on such
a data is exploding so this is just a
trend about the data that is being
growing if you can see till late 1990s
the data volume was very less but over a
period of time if you see that there is
a hockey stick growth in this cow which
means the curve is actually growing in
an exponential manner manner but a lot
of lots and lots of data is getting
added from only 2000 onwards and the
reason for that is Internet is being the
primary driver
everybody has smartphones tablets
laptops and different gadgets through
which they are connected to internet
they are everyday connected to internet
they do lot of activities on the across
the globe or all the users doing lot of
activities on internet and in turn
basically it is contributing to adding
lot of
across the globe right so there is a
growth in this area during this period
and lot of companies have seen
opportunity in this space to do
something and that is where this big
data and analytics train has begun and
lot of companies thought that let us do
something here something meaningful here
let us try to find patterns in data let
us try and try to find meaning within
the data to solve some business problems
and remain competent or ii-in
competitive advantage over competitors
and that is where
I'll just say big data is physically
threebees you have volume velocity
pardon my rating that's very bad and
variety okay so big data is categorized
into three different buckets volume
variety and velocity volume as we all
know it's basically huge volume of data
huge amount of data it's what terabytes
and petabytes of data but can we say
that if I have huge amount of data it's
a big data the answer is no we need to
actually check more parameters to that
so that is another parameter called
velocity what do we mean by velocity
velocity means let me first give you an
example so let's say I will I will say
that I have about one terabyte of data
that gets generated in my system after
every one year if I say that then so
that is basically one scenario now let's
say another scenario is I have one
terabyte of data which gets generated
into my system every other day okay so
in first case I am getting one terabyte
of data in 1e here and the second case I
am getting one terabyte of data in every
day now if you can see the difference
the second use case we have the
frequency of getting so much amount of
data is very high that is called
velocity the rate at which the data
comes to your system or data gets
generated into your system is called
velocity okay and the third dimension is
called variety variety is nothing but
what are the different types of data
which get generated from different types
of sources so by the way any data in the
world can be categorized into three
different buckets structure data um
data and semi structured data structure
data means all the data that we store in
relational database systems like Oracle
my sequels equals over all this
different are the VMS systems that we
store our data in the form of table rows
and columns is called structure data
right then what is unstructured data
unstructured data the example that I
could give you is you know in facebook
let's say we upload a lot of photos
images video for his empathy songs
audio/video files all this these are the
example of unstructured data they are
basically plain raw stream of bytes you
don't have any structure attached to it
I cannot say that get me the eyes of a
person from this image there is no
proper structure to it where I can
directly retire the value of our eye
right I mean I have to pass that image
so basically it's it's it's just mites
and it just pixels so it's actually
unstructured data so that's one example
of unstructured data another example
could be the log files so I'm sure you
have worked on lot of enterprise
applications which are deployed in the
production environment in different
companies and these applications will
generate lots and lots of logs every day
now the logs that we write in those
files will not follow any particular
structure right so that is also
categorized as unstructured data and our
third type of data is called semi
structured data the best example I could
give you in semi structured data is XML
files if you have worked with XML files
XML files are are basically the files
which which has a parent-child
relationship between the tank so you
have to first define the tags within the
tag you define the value the tags or I
would say the key is that you define
will have some sort of relationship and
that's why it there is some structure to
it but the value that you put within
each tag or each
he can be anything you can put any form
of value and that is why there is some
amount of instruction eight-hour
uncertainty is also there and that is
where on the whole it is it is termed as
semi structured data another example for
that could be JSON files JSON file is
also termed as semi structured data so
just to summarize if I have huge volume
of data if it gets if if I get huge
volume of data at a very fast pace in my
system and also that data can be of
structured unstructured or semi
structured type then I can say I have a
big data so that is a formal definition
of big data okay so we understood what
is big data and we also understood some
applications of big data so till this
point are we clear does anyone has any
questions till this point
okay so looks like we are fine we are
good nobody has any questions as of now
so I'm just moving forward please do
stop me if you have any doubts okay
now we have understood what is Big Data
now we we we want to understand why Big
Data is very important and what are the
common big data customer scenarios where
companies are using big data to solve
that business problem once right so some
of the examples I have already covered
now I'm just adding few more I have
actually taken examples from different
industry sectors one of the industry
sectors that I want to talk about is
wave any telling industries by the way
the the use case that I've mentioned
here in the interest of time you would
not be able to cover all the use cases
but in actual training we do that right
now I'll just cover a few of them which
are important and we will just
understand how they solve this problems
so they've been detailing industries or
you know ecommerce companies like eBay
Amazon all these companies you know how
they use big data so one of the best
example that I could give you is there
is a use case called recommendation
engines so let me tell you a scenario
let's say if someone logs into this
ecommerce company or websites let's say
someone logs into Amazon or Ebay
and she has done some shoppings on that
now she's been doing that over a period
of time and then let's say after a few
days again she logs into eBay and you
know she would see that on the one part
of the page she will see that these are
the products which are recommended for
her right
so she will see some sort of comparisons
of the products which which is very
similar to what she has done all the way
what transactions
is already done with this company right
that is called recommendation engine so
what this company does is they actually
analyze all your historical transaction
data and then based on that they form a
opinion or they basically try to find
out the relationship between different
products and therefore they try to
answer this question that what are your
likes and dislikes and what are the
products or product categories or price
range that you would be interested in
buying and accordingly these campaigns
will show you the recommendations there
are algorithms machine learning
algorithms which are used extensively to
process such big data on a Hadoop
cluster and give you this answer that is
about technology part of it but this is
how they saw the use case now they are
claiming there is there is a state which
says that you know by doing such by
using this technology and solving these
problems they were able to the heat
ratio of this is about 60 to 70% which
means if let's say a person who has
never thought of buying let's say today
I have not made up my mind to do any
shopping but I'm just casually log into
Amazon and browsing some products but
the way Amazon recommends the products I
would be tempted to go in
in Big Data and Hadoop another use case
have already talked about which is
targeted advertisement now let me take
one more use case from another different
industry sector which is telecom
industry sector so in telecom industry
sector there is a very good use case
which is called customer churn
prevention let's say there are you know
I let me take two different telecom
providers telecom service providers
telecom service providers a and B okay
I'm not naming them I just say a and B
these two companies who are a very
famous let's say very giant telecom
providers in a particular area now there
may be a situation where some users who
are using the services provided by a are
not happy with them and they are
migrating to the services or they're
trying to opt services provided by
telecom service provider B or complete B
right now in this case whenever people
or users who are you know leaving the
services provided by a the they will get
a call from you know a company as
representatives and you know they need
to understand why are you leaving us
what is the reason and then customers
have to tell that I am not satisfied
these are those ones for reasons and all
that now once they do that they will log
this all these reasons and these
companies will analyze this data all the
reasons and they find out what are the
top let's say I'll just give a number
that what are the top 10 reasons why my
customers majority of my customers are
leaving me if you can see for any
business to be successful it is very
important that their customers stays
with them and they don't leave them
right so even for this use case it is
very important for this telecom
companies that you know their customers
stay with them if they are leaving them
they need to really find out the reasons
one of the topmost reasons why they are
leaving us and if once they know that by
analyzing
data they can acutally find out that
okay let us try to come up with better
competitive offers the reasons could be
that you know they are not providing
good services in the area where customer
stays or you know the competitors are
providing better competitive offers as a
cheaper rate or it could be any other
reason right so by analyzing this data
they are coming up with better offers to
mitigate those reasons and make sure
that the in future customers will not
leave because of that that is one way of
preventing your customers from leaving
you and making sure they stay with you
to make sure your business is successful
so that is called customer churn
prevention and to solve this use case
and mostly telecom companies are using
Big Data technology and and Hadoop a the
underlying technologies for that okay so
let me take another use case from
another I would not see industry but a
government right government sector now
I'll take one example one project that
is being completed or recently by Indian
government so people who are from India
would know that there is something
called a dark art now this for people
who don't know what is another card it
is it is like social security number in
u.s. it's a unique identity for every
individual citizen in India so that was
an initiative which was taken by
government and that has been implemented
to make sure that every individual
citizen of India will have the you know
unique ID this entire project is
implemented by Hadoop as an underlying
technology cut open HBase they have used
this 2 technologies extensively to
implement this entire project and by the
way this project is called world's
biggest biometric database obviously
because of the populations of of India
so that is why it is called baguettes my
metric database but to handle such a
massive load they have used Hadoop as an
underlying technology so that is the key
point another use case is welfare scheme
where you know government what they do
is and that is to with US government or
any other country government that what
they do is they analyze let's say they
want to come up with a welfare screen
for a particular area how do they come
to know or how do they decide which 12l
scream should I come come up for this
area and for what video and you know
what are the conditions to that so to
answer that question and to come up with
a better welfare scheme they analyze the
GDP growth of the populations of that
that area and based on that they analyze
this data and come up with an answer
that okay fine
this area needs more you know education
Zoar needs more infrastructure of
services or facilities and stuff like
that so that is also one area where the
government uses such technologies
another industry sector is healthcare
and life science you know there are you
see any use cases this companies add up
using you know a Big Data technologies
but one of the famous use case is drug
safety there is also a use case called
gene sequencing and I'm sure many of you
are aware of that gene sequencing is
nothing but it's called DNA profiling
which is basically used mostly in crime
related you cases where if you want to
predict whether this the person who's
involved in the crime is actually the
the suspect who's involved in the crime
is actually the person who has committed
the crime so to answer those questions
they do lot of such image processing
image matching and gene sequencing on
that there is also another use case
called drug safety
so specifically pharmaceutical companies
who want to launch a new drug in the
market what they do is they basically
you know take up this drug the launch of
drug in the market they tie up with big
hospitals now these hospitals doctors
will start prescribing these dogs to the
patient's right and and obviously all
the hospitals will have the data about
how this drug has performed over six
months now after six months this these
pharmaceutical companies will collect
the data from these hospitals and
analyze this data to find out how that
drug has performed and how it whether it
has been able to cure the diseases what
is the percentage ratio what what was
the effectiveness it has provided and
based on that by analyzing this data
they find out whether to improve the
drug by adding or removing particular
you know instances or substances from
this drug or should we you know come up
with a better version of it or should we
just drop this drug because it has never
cured any disease it was not performing
well
so all these answers they are trying to
come up with by analyzing this data and
again by by two to analyze such a
massive amount of data they are using
Hadoop is on the line technology okay
there is one more industry sector and I
am sure it is favorite for many of you
which is banks and financial services
this is again one such area where big
data is heavily used so what they do is
they actually you know try to kind of
come up with I mean they they use big
data to solve many use cases but I would
just take one use case which is credit
scoring and analysis so in around 2008
per year right during that time frame
I'm sure everyone knows that the rules
economic countdown ton and lot of people
have become defaulters to the loan that
they have taken and then bank could not
recover the money from that and that was
one of the driver of the economy
downturn indecision during that period
and an even that time the banks used to
have credit scoring checks and
everything but what they did not have
was analytics part of it so after that
recession what they did is they improve
that system to do analytics and do more
analysis on the data so what they're
doing is now let's say if a person goes
to a bank for taking a loan the bank can
actually get all his transaction data
and financial data
whichever bank this user Apple has an
account to the bank can waive because
these days all the banks are centralized
in the bank and actually
by themselves by analyzing all the
financial transaction data that you that
of the of the user right and that way
they are making sure that they will not
run into the risk my landing loan to
these people so that is also one such
area where big data technology since
having used okay so that's what I wanted
to cover as part of different big data
customer scenarios and and the objective
of covering these use cases is to just
will make us realize that how different
industries are using big data and why
big data is so much important such a big
buzzword why companies are spending so
much money and doing a lot of investment
in this space okay so till now what we
understood is what is Big Data and why
it is very important right why it has
become such a big buzzword even this
this slide I have captured just to you
know emphasize this this fact more that
there are consulting amazed like me in Z
and Gartner who are also saying the same
thing that use use Hadoop to gain a
competitive advantage over risk ever
percenter prizes
so definitely Hadoop is the key
technology to work with big data and
everybody all these companies are also
saying the same thing these are some of
the Hadoop users I've just captured some
name but definitely the list is huge
you have Google link bean Amazon eBay
rec space Yahoo Facebook IBM and n a lot
more users so I am just captured few of
them few big names okay alright so since
we understood the basic concepts of what
is Big Data and why it is important let
us try to understand what are the
technologies that can help us work with
big data and why Hadoop is the most
effective technology to work with big
data
okay as I said in the interest of time
we would not be able to cover in detail
technical disk
you know architecture and our
differences about that but I will just
try to
give you some key differences here so
before that let me just quickly brief
you about initially so traditional
large-scale competitions was basically
traditionally the competition has been
processor mom where you had relatively
small amount of data and significant
amount of complex processing was needed
so primary push was to increase the
computing power of a single machine if
you can see this particular slide you
will realize that initially the data was
very less and that's why the primary
push was to increase the processing
power or a CPU because they were they
don't really have a problem of storing
this data right they have a problem of
executing the logic faster and that's
why they were increasing the process of
power but over a period of time when
this curve has been growing this
particular space has actually you know
taken I mean has has grown a lot I mean
lot of data has been growing and that is
where they started realizing the
importance of storing this data in an
efficient manner and that is where
distributed system evolved right so a
lot of traditional systems like MPI and
PBM were there so since I have
introduced a word called distributed
file system let me just spend a few
minutes to explain what is that I'm sure
many people know about that but I just
want to explain it so that people
understand the basics of it
so why distributed file system so let me
take let me give you two scenarios one
is let's say you have one machine where
you are storing a one terabyte of data
let's say assume that that machine has
one terabyte storage capacity and that
machine will have four i/o channel any
channel has 100 Mbps speed if you want
to read that one terabyte data from that
single machine how much time does it
take so let us not do some math and go
into that direction let us assume that
okay it takes 45 minutes
right now I have another scenario where
I have ten machines I am trying to
distribute this one terabyte of data
into ten different machines each machine
has the same configuration for i/o
channel each channel has 100 Mbps speed
now in this case when I want to read
them parallely all the wonder of my data
obviously the time it takes is just four
and a half minute it is one tenth of the
time which earlier it has taken right
that is the fundamental reason or
motivation why distributed file systems
are evolved right I'm sure there are
many more technical differences but if
someone wants to understand this concept
this is the fundamental objective or I
would say motivation why DFS is evolved
okay now once we understand why DFS let
us understand what is DFS right so let
me give you a scenario let's say we have
a file and I want to store this file
into DFS okay
so I'll tell you one thing distributed
file system is not a physical file
system it is just a virtual file system
okay now that means let me explain that
what I'm saying so let's say I have four
different physical machines okay one is
it Chicago one is in Austin one is in
Denver these four machines are connected
in a network these four machines belongs
to one company right so these four
machines form one particular cluster
let's assume that way now I have one
file which has about one terabyte of
beta that file name is let's have fun
okay when I store this file in a
distributed file system by the way what
is DFS so DFS is just nothing but when
these systems are connected over a
network all of these systems all of this
machines will have some set of programs
running on them these programs
are called district DFS okay so I'm just
kind of you know putting a dotted line
around this machines so you can just
assume that it's a virtual layer which
is provided by DFS but basically there
are set of programs running here here
here and here these four machines will
have set of programs running these
programs are nothing but they are the
ones who form this distributed file
system okay but for an end user they
think that this is the physical storage
so let's say I want to store this file
into the DFS so what happens is this
file assumed that is divided into four
different parts this file is of one
terabyte size this file is divided into
you
you
you
Srishti yeah so you can just check with
a simply learn team for that they will
have the answer of a bet they'll give
you more details on that sorry everyone
just give me two more minutes
I'll just coordinate with simply none
team unfortunately looks like the
presenter control has been revoked so I
just need to check with them
hey can you make me the present of this
yeah looks like they're actually revoke
that so people are waiting here
you
you
you
you
okay meanwhile I got one more question
from Anthony so there will be a PDF of
presented himself by the way Anthony we
actually use these for you know our
marketing as slides and webinar so we
would not be able to share that but
still I would
you
you
you
you
you
hello can I quickly get them
confirmation from everyone am i already
well
Valente they will give you the clarity
on that okay all right great so first of
all sorry guys sorry for this hiccup for
five minutes
somehow there was a technical problem
and we couldn't speak or connect for a
few minutes again we are back so yeah I
was just explaining you what is the FS
and this could leave covered that so
basically what I was telling is you have
different parts of the file stored in
different machines and for user he has
two good let's say a data in a
particular location let's say user slash
have fun he has told that file which is
divided into this four parts right ABCD
and the final name is have fun now for
user he think that I'm storing
everything in this path but when he
stores the file internally these
programs which forms the DFS are
actually making sure that this file is
divided into different parts and
physically stored in different machines
who may be located geographically
differently right now what they do is
when you when a user say cat or let's
say they want to view the file then give
the same path user slash have fun when
they do that all the programs which are
running as part of distributed file
system will take up all these parts of
the file right and that will be
basically presented as one big file to
the user right so distributed file
system programs will abstract the
complexity of how data internally stores
in different machines - from the user as
well as how it is retrieved from them
right
so for user he thinks as if like he is
storing all the data into the single
machine but internally it is not
internally it is stored in a different
machine so that is what is called
distributed file system
distributed file system is just a set of
programs which gives you the virtual
file system it's not a physical file
system okay so we just understood this
distributed file system because we
wanted to understand something called
Hadoop which which is also using a
distributed file system as underlying
storage mechanism to store the data
right so we were understanding the
motivation for Hadoop
so basically Hadoop follows cap
principle you know consistency
availability and partition tolerance so
before I go into the details and I think
in the interest of time you would not be
able to cover all the things in detail
but I'll just you know spend few minutes
on covering what is the difference
between how to open the traditional
system so there is a key difference and
we will understand that so let's say
there is a so by the way every
distributed file system is a master
slave architecture okay so let's say
there is a master and this this these
guys are slave nodes okay so in
traditional distributed system when you
have a file which is one terabyte of
size okay and which is divided into
different parts a b c and d and if these
parts are stored here a b c d this by
the way machines are connected through
network it's a master slave architecture
it's a distributed file system so i will
first explain a scenario of traditional
system how does it work is let's say if
you want to write a distributed program
to process this particular file of 1
terabyte size you submit you as a
developer submit this program to master
node right now what master node does is
it will get all the parts of this file
ABCD all these four parts will be
transferred over a network right to the
master node I'd say all the parts will
be transferred like that so basically
you are transferring all these four
parts which accumulates amounts to one
terabyte of data to the master node
right and and that's where the
processing happens so the disadvantage
here is you are occupying so much
network bandwidth by transferring such a
heavy load one terabyte of data over
over a network right and at the same
time you are not achieving the
parallelization you are not achieving
the parallel processing because all the
data you are gathering in master node
and then you are processing so the
problem with addition systems were they
were sending data to the nodes where
computation programs are running right
now what Hadoop has done is they have
actually done it completely reversely
so
what they are doing is instead of just
do this so instead of you know again
there is a file for parts are there so
instead of sending data to the
computation nodes how do will send the
programs to the data nodes all these
nodes where the data is stored this
program will be divided and sent across
it will be like p1 is here we do is here
p3 is here and p4 is here now you may
wonder that how come a program can be
divided so don't worry about that that
is the whole beauty of MapReduce program
and in our actual training you will
understand how does it work but the
right now consider that the program is
divided into multiple sub programs and
they will be parallely executed on
individual part of the file right so now
data is not transfer over a network the
program is sent to the node where core
data is stored so that way you are
avoiding this major overhead and you are
not occupying occupying so much network
bandwidth and at the same time you are
actually achieving parallel processing
once this programs processed the data
they will generate partial output for
all this parts right once this output is
ready by the way this is not the final
output this is just output generated by
processing this individual part of the
file but we want to process this entire
file at one shot so that's where there
is one more process which will be
triggered which will collect all these
output write it and then send it to one
of the node so this output will be
combined together send it across to one
process which will aggregate everything
and do some aggregation operation on
that and generate the final output so
this is how MapReduce model works this
is how Hadoop processing works and that
is one of the major advantage what
Hadoop provides in
terms of processing the data and that is
where it scales very high compared to
traditional systems okay so that is what
exactly I wanted to cover as part of the
motivation of Hadoop
so and apart from that it follows care
principle but since it's already time I
would not get to get into the details of
that and I'll just quickly brief about
the Hadoop is 3 so Hadoop has you know
evolved from about 2005 2006 frame
timeframe so google has published a
paper on GFS and MapReduce GFS is
nothing but Google file system and
Hadoop distributed file system which is
called as DFS is built on top of this
idea GFS there is a guy called up
cutting who has implemented this concept
in his project called
nut and then that cutting was hired by
Yahoo both of them together started a
project and they gave that project and
name is Hadoop and from that they have
made that as an open source and and from
that point onwards companies like
Facebook Yahoo cloud era all these are
regular contributors to the Hadoop
right so that is just a journey of
Hadoop how Hadoop has evolved right but
if someone really wants to understand
what is Haro if I will just give your
definition and if you remember this way
you will never forget what is hard ok
Hadoop is a framework which allows you
to do two most important thing remember
we talked about Big Data right so now
let's say if someone has a big data and
they want to process this big
and Hadoop is a framework which gives
you the solution for these two important
challenges or problems Hadoop gives you
two core components one is called SDF S
which allows you to do storage of data
in a distributed and an efficient manner
and another component is called
MapReduce which allows you to write
distributed programs in an efficient
manner so that they can get executed in
an efficient manner and they can give
you faster performance right so Hadoop
distributed file system plus MapReduce
is equal to core Hadoop that is the
definition of hello if you remember this
way you will never forget what is Hadoop
right and all the other keywords that
you would have heard about like high
peak HBase no sequel all these other
keywords are all ecosystem components of
hano so in our the actual training we
will explain them as well all right so
we have understood what is Big Data why
Big Data is important and we have also
understood why Hadoop is as chosen as an
efficient technology to work with big
data and we introduced Hadoop as well so
that's what we wanted to cover as part
of today's session right now so there
are some key characteristic of Hadoop
which says it is reliable it is
economical scalable and flexible and
that is one of the reason why it is
chosen as one of the efficient
technology so so basically all these
terminologies we will explain it in
detail during our actual life training
sessions okay all right and that's about
simple on so these are the slides which
talks about what simply land does and
how they provide that training across
the globe it is definitely one of the
leading trading provider companies
across the globe it does provide
training in various areas and all these
areas are listed here right so feel free
to visit the website and check more
details about that so that's it guys
that's what I wanted to cover as part of
two
in our session I hope you all have
enjoyed this so I'll just spend few more
minutes to take any questions if you
have let me know if anyone has any
specific questions
so oh if you can you know contact simply
learn they will give you more details on
that please please connect with simply
Ellen team and they will give you all
the details about the prizes as well
so please contact this particular email
ID and that you you all should contact
the similarity you will get all your
queries answered there this is the email
ID you should connect to and he should
get you all the details in terms of one
of the courses available and one of the
prizes for big data what are the
upcoming batches of Big Data and Hadoop
any any more questions
taursus which generates big data and how
they are using big data we covered the
scenarios of Ellen industries we covered
Facebook example Twitter example right
we cover New York Stock Exchange example
and and after that we formally define
what is big data so you need to remember
three B's volume variety and velocity
which which makes a big data after that
we started with why big data is
important so we have covered common with
data customer scenarios where we
understood you know different industry
sectors and how they are using big data
to solve their business problems we have
taken and use cases from retail
e-commerce domain from telecom sectors
from government from finance sectors
from healthcare industries right and
once we understood the importance of it
we started understanding what is what
are the different additional systems
available which can help us work with
big data and that's where we understood
what is distributed file system and why
it is important once we understood that
then we also understood the difference
between Hadoop and traditional
distributed system in terms of how they
processes data and that's where we
understood that Hadoop takes programs to
the nodes where data is stored and
that's where it actually achieves the
parallel processing and it gives you
faster performance in terms of
processing their big data right and
after that we understood you know the
other some characteristic of Hadoop and
we introduced shadow and then we
formally define what is Hadoop so Hadoop
gives you two capabilities one is
storing the big data in a distributed
manner and then processing the big data
in a distributed manner so that's what
Hadoop is all about so that's what we
have covered as part of today's webinar
sessions
oh I see okay well I apologize for that
seems to be something wrong with the
time zone so you should definitely send
your feedback to the Similan team as I
said it would be I would not be the
person who can confirm on that so all
your queries related to course materials
and training should divert to the email
ad which I have shared in this session
they'll be able to help you with that
okay so I hope all of you have enjoyed
this session and I hope you found it a
good useful of time right so you can get
the contact details from the simply
known website
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>