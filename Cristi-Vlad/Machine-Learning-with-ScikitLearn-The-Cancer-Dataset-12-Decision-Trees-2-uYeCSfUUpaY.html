<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - The Cancer Dataset - 12 - Decision Trees 2 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - The Cancer Dataset - 12 - Decision Trees 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - The Cancer Dataset - 12 - Decision Trees 2</b></h2><h5 class="post__date">2017-04-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uYeCSfUUpaY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in the previous video of this machine
learning series we started building a
decision tree classifier and we trained
it on the cancer data set and
scikit-learn however as you may remember
we got an accuracy on the training
subset of 100% which means that our
classifier is overfitting I've told you
that this happens primarily because the
decision tree was unrestricted and went
all the way down through the branching
until it perfectly fit the training data
so how do we deal with this situation we
apply restrictions we want to limit the
depth of the decision tree which in turn
is going to limit its complexity now in
the context of decision trees there is
this technique called pruning so we can
apply pruning to prevent overfitting
there are two types of pruning pre and
post with pre pruning we stop the
creation of the tree at an earlier stage
we basically don't let it develop
completely we could limit the maximum
depth of the tree or the maximum number
of leaves or we could require or specify
a minimum number of points for a node so
that it can keep splitting so these are
the parameters with which we can play
for pre pruning now on the other hand
with post pruning we allow the tree to
get built and only there after we remove
irrelevant notes or notes that contain
little information for our specific case
here we're gonna do some we're gonna
basically apply pre pruning and we're
gonna modify or set the maximum depth of
the tree to four thus limiting the
number of questions that can be asked
as we built it top down now enough with
the theory so once again we're gonna
build the tree I don't have to rewrite
all the code from above or do the
imports because I already ran those
cells so tree is decision tree
classifier and we're gonna set the
maximum depth to four and we're gonna
keep the random state to zero then we're
gonna fit it on the training data and
we're gonna print its accuracy I mean
let me just copy these I don't have to
write them okay
now let's run this shift-enter
okay now that looks better so we've
reduced we've reduced the accuracy on
the training subset thus reducing
overfitting and it seems we've also
increased the accuracy on the test
subset which is good so what we've
basically learn here is how to modify
the parameters of the decision tree
classifier to improve its prediction
accuracy on the cancer data set now to
better understand how it all works under
the hood we're gonna do some
visualization of this decision tree in
the next video so if you enjoyed this
one please hit that like button and
subscribe thank you for watching and
I'll see you in the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>