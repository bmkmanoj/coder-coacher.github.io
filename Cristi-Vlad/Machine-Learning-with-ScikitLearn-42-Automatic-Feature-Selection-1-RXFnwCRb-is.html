<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - 42 - Automatic Feature Selection - 1 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - 42 - Automatic Feature Selection - 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - 42 - Automatic Feature Selection - 1</b></h2><h5 class="post__date">2017-11-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RXFnwCRb-is" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to another machine learning
tutorial today we are going to talk
about automatic feature selection so
what do I mean by automatic feature
selection in the last few tutorials in
this series we learned about ways to
increase the dimensionality of our data
such as that we looked into what hot
encoding and how when used it creates
additional features in our data set but
higher dimensionality may not always be
a good choice as it adds more complexity
to the problem and it might also
increase the risk of overfitting and
that's one of the reasons why reducing
the number of features in high
dimensional data can be a good practice
as a reminder we are following the
machine learning textbook that's linked
in the description which for this
purpose discusses three strategies for
feature selection and these are
univariate statistics model-based
selection and iterative selection and we
start with the first one univariate
statistics so what this method basically
does is to determine if there's a
statistically significant relationship
between each feature and the output or
the target this is also known as an ova
or analysis of varieties so after the
relationships have been computed the
features that have the highest
confidence with the target are those
that are selected now the univariate
keyword simply means that each feature
is considered in isolation or
individually in determining its
relationship with the target okay so how
do we do this in scikit-learn now
without getting into too many details
or to mentioned methods select gay best
which allows you to limit your selection
to a K number of features and a select
percentile which as you might guess
allows you to limit your selection to a
certain percentage of the original
features now as per the book we're going
to use the breast cancer data set and
we're going to add some noise to the
data before applying the selection
method so jump on to the code import
numpy SNP and from SK learn data sets
we're gonna import load breast cancer
from SK learn model selection we're
gonna import train tests split to split
our data and from SK learn feature
selection we're gonna import select
percentile okay now to instantiate our
data set so cancer equals load breast
cancer if it were that easy in real life
when you have to do a lot of cleaning
and pre-processing to the raw data
anyway now to the noise so let's say RNG
equals NP random random state 42 and
noise
equals RNG normal distribution size the
science is gonna be the length of our
cancer data so the features so the
length of the features and 50 so we're
going to have an additional of 15 noise
features now let's add them to the data
so let's say X with noise equals NPH
stack so we're gonna stack cancer data
and noise now let's split the data so as
always X train X test white train Y test
equals train test split X with noise
cancer target a random state of 0 and a
test size of 0.5 now we are going to
apply the Select percentile so let's
call it select equals select percentile
and percentile is gonna be 50 which
means it will select 50% of the features
okay and then we're gonna fit it on to
the data
so select outfit X train y train and
then we're going to transform the
training set so let's say X train
selected as select transform X train now
let's see how it did by printing the
shape of both X train and extreme
selected so print
X train dot shape is format X train
shape and for the one with the Select
percentile applied to applied to it so
print X train selected dot shape is
format X train selected dot shape right
now let's run this entire thing so the
initial number of features was 80 so 30
from the original data set and the
additional 50 noise features that we
generated and after the transformation
they've been reduced by 50% or 240 now
we'll stop here for now and in the next
tutorial we're going to look at exactly
which features were selected and we're
also going to see if this type of
feature selection can improve the
performance of an algorithm because
we're gonna apply logistic regression to
both of these data the original and the
selected and see how they compare
alright so please hit the like button
and subscribe if you enjoyed this and
you want to follow along with the series
thank you for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>