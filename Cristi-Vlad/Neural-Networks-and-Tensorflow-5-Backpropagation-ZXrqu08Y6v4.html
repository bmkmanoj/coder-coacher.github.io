<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks and Tensorflow - 5 - Backpropagation | Coder Coacher - Coaching Coders</title><meta content="Neural Networks and Tensorflow - 5 - Backpropagation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks and Tensorflow - 5 - Backpropagation</b></h2><h5 class="post__date">2018-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZXrqu08Y6v4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">as we've learned in the previous
tutorial gradient descent is central to
deep learning and understanding it at
least at its most basic level is a
necessity
now back propagation is another central
aspect of deep learning and this is what
we're gonna delve into today the purpose
of this is not to go deep into the
mathematics of it which is really
beautiful but to have an intuitive
understanding of backdrop now there are
many well done papers blog posts and
video tutorials that go into extensive
detail on the math behind backdrop so
I'd recommend you use your preferred
search engine if you're interested about
it I will also link some of my favorites
in the description of this video so in
short when we do a forward pass through
a neural network with inputs randomly
initiated weights and a bias we get the
output we compare our output so our
prediction the y hat with the real
output and that's how we get the error
or how far is the network from the real
value of the output now doing a backward
pass through the network or doing back
propagation
we're actually propagating the error
backwards for the purpose of updating
the weights and by updating the weights
we will be closer to the real value of
the output thereby we will be reducing
the error and increasing or improving
the performance of the neural network
now in doing the calculations it is
important to mention that we're
consistently applying the chain rule and
the power rule now for these images I'd
like to thank Sebastian Raska who wrote
the amazing book Python machine learning
all right now a more
formal definition of backdrop is that it
calculates the gradient of the error
function with respect to the weights so
it's a backward propagation because we
first compute the gradient of the final
layer of weights and move on to the
hidden layer to ultimately compute the
gradient of the first layer of weights
now doing so is really important for the
efficiency of the neural network because
if we could compute the gradient of each
layer separately or if we would compute
the e a gradient of each layer
separately the process would be far less
efficient now like I said we won't go
into the mathematical details but I
linked a few great resources in the
description of this video if you're
interested so to reiterate in forward
passing through the network we compute
activations and log it's so Z and a and
reach a predicted output using randomly
initiated weights and the bias term then
we do back propagation using radio
descent to optimize the cost function or
to decrease the error of the network now
decreasing the error will reduce the gap
between the predicted value and the
actual value of the output so with back
propagation we will be updating the
weights to reduce the error and improve
the performance of the neural network I
have to say once again that having a
basic understanding of forward
propagation and how to compute
activations for neurons having a basic
understanding of gradient descent and
back propagation is central to becoming
literate in deep learning alright now
that we summarily went through the
basics next we'll be starting to slowly
introduce tensor flow and then gradually
getting into more calm
problems as always if you enjoyed this
video and you'd like to see more of
these tutorials coming please hit the
thumbs up and subscribe now I also
appreciate if you consider becoming one
of my patrons on patreon to support the
work for these educational videos that
being said thank you for watching and
I'll see you in the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>