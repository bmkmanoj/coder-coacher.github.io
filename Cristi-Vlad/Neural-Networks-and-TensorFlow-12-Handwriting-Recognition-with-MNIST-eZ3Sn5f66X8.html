<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks and TensorFlow - 12 - Handwriting Recognition with MNIST | Coder Coacher - Coaching Coders</title><meta content="Neural Networks and TensorFlow - 12 - Handwriting Recognition with MNIST - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks and TensorFlow - 12 - Handwriting Recognition with MNIST</b></h2><h5 class="post__date">2018-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eZ3Sn5f66X8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in this lesson we're doing softmax
regression on DMS data set if you didn't
know amnesties one of the reference
datasets or databases that's so very
often used in machine learning or deep
learning projects to train different
models now this data set contains
handwritten digits as you can see in
this image and what we're actually gonna
do is to build and train a model in
tensorflow
so that it recognizes digits like these
ones now like I said we're going to do
softmax regression which we've discussed
in an earlier video from a theoretical
standpoint now I need to mention that we
have the amnesty preloaded in tensorflow
so we don't have to do the burdensome
pre-processing the data which is
probably the I mean it happens in the
majority of deep learning projects and
it can take a great deal of time and
mental effort alright so another thing
to mention is that our inputs are going
to be these images of handwritten
characters and we will be unrolling them
or the pixel values at each position and
we're actually going to unroll them into
a very long factor now this is a
procedure that's different compared to
what we do in convolutional neural
networks when we make use of spatial
information but we'll talk more about
that when we reach that point so anyway
let's begin but before that let me give
once again credit to Tom Pope and his
textbook which is linked in the
description for this example all right
so we're gonna import Dancer flow as DF
and from tensor flow like I said we're
gonna preload our dataset so from tensor
flow examples tutorials
amnesty we're gonna import the input
data okay let's run this and let's make
some space here right now our data
directory is gonna be let's call it data
and our number of iterations or epochs
it's gonna be 1000 which I guess is
decent to train this data set and we're
gonna use mini-batch science so mini
batch size so we're gonna train the
model in batches of 100 samples each
time okay now let's also run this let's
say data and now we're gonna instantiate
our data so it's going to be input data
we're gonna read datasets from data
directory and we're actually gonna want
how to encode them so we're gonna set
the one hard to true
we usually one hunting coded data for
more efficient computation and I talked
about this procedure in one of the
videos from the machine learning series
okay now let's let's actually do a place
holder for our inputs so place holder
which is gonna be of float 32 and it's
gonna have a shape of none so this is
actually where our mini batches are
going to go and 784 now why 784 because
the images are 28 by 28 pixels okay so
this is our input and now let's add the
weights so the weights are going to be a
TF variable of zeros so we're going to
instantiate we're going to initialize
the weights to zeros and they're going
to look like 700
eighty-four and 10 so 10 is the number
of possible outputs or labels which is
going to be the digits from 0 to 9 which
are 10 in number
okay now our true outputs are going to
be TF placeholder float32 data type
these are gonna be of shaped none and 10
so 10 possible labels and our predicted
outputs are going to be a matrix
multiplication and you should probably
notice of X and the weights so notice
that we don't use the bias here in this
simple example okay now let us define
our loss function so let's say loss
equals TF reduce mean and we're going to
use a soft max regression with log it's
so soft max cross-entropy with long it's
DF and and soft max cross-entropy with
long it's where our law gates are going
to be Y predicted so our predicted
labels and our labels are going to be
our true labels okay now our optimizer
is gonna be a gradient descent optimizer
optimizer so TF trained gradient descent
optimizer and the learning rate is gonna
be 0.5 and the purpose of optimization
is to minimize the loss okay and what
we're gonna do here is to apply a
correct mask or a correction mask for a
correct prediction which is going to be
a vector of boolean values for what we
want to predict so we'll say let's say
correct brad is DF equal DF art max so
it's gonna take the maximum between Y
Fred and 1 and T f dot art max it's
gonna be is gonna take the maximum
between white 2 and 1 and then the
accuracy is going to be TF reduce mean
DF cast our correct Brad and the data
type float 32 okay now let us create and
execute the session so as you already
know with DF session as SAS first we're
happy we're gonna have to initialize
we'll have to run a global variables
initializer so we could have just
defined an init here which would have
taken the value of TF and it global very
oh do you have global variables
initializer but we can just do SAS run
DF global variables initializer okay and
then for each step in range of our
iterations so in a range of num ITER's
will have a batch of axes and a batch of
wise which are gonna be data train and
this is actually so next batch this is
like a generator mini batch size and
we'll run our optimizer and we're it'll
gonna take values so the fit dictionary
is gonna be so x
it's gonna take values so our
placeholder over here is gonna take
values from batch access and our y2 is
gonna take values from batch quiet okay
and that's that's it now in the same
session
let's also actually do some testing on
work actually going to take data from
the test set so we can just say test or
testing SAS run and we're gonna run the
accuracy and this time our feet
dictionary X is gonna take values from
data test images okay and our white shoe
is gonna take the labels from data test
labels okay and let's finally print
print the output or the accuracy after
the training so we can just say print
accuracy and we're gonna have four
floating points and the format testing
we're going to multiply it by 100
because the accuracy is gonna be between
zero and one and we actually want it in
percents so percent times 100 okay and
that should be it I guess
let's run this now so first it downloads
and extracts the needed data set and
additional files and then it runs the
training see how quickly it finished so
it actually went through
one thousand iterations or epochs in in
a matter of seconds and with good
performance I guess we can say that of
course we could increase this
performance through hyper parameter
tuning but we'll talk about that in a
separate lesson okay so that's it for
now if you found this video helpful
please hit the like button and subscribe
thank you for watching and I'll see you
in the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>