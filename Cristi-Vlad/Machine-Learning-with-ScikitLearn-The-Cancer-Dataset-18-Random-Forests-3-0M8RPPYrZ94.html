<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - The Cancer Dataset - 18 - Random Forests 3 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - The Cancer Dataset - 18 - Random Forests 3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - The Cancer Dataset - 18 - Random Forests 3</b></h2><h5 class="post__date">2017-05-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0M8RPPYrZ94" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in this last year on random forests
we're going to briefly look over the
strong and weak points of this machine
learning algorithm as well as some of
the parameters we can modify to
optimizes performance now let's start
with the strengths a random forests are
some of the most used machine learning
algorithms they are powerful and they
don't necessarily to provide good
performance they share most of the
benefits of decision trees one of them
being that they don't require scaling of
the data now they are better than a
single decision trees in many projects
because they make up for the
deficiencies of single trees you choose
a single decision tree if you want to
show a visualization of the
decision-making process like for
illustration purposes to non-experts
this is least likely possible with
random forests because of their
complexity now a little bit on
parameters if your computer laptop or
device has multiple cores you can adjust
the parameter and jobs which will
specify the number of cores the
algorithm will use in the training
process now according to Muller and
guide O's machine learning book and
there's a link for that in the
description using two cores will double
the speed at the random forests convert
to using only one core and if you want
to use all the cores of your device you
would have to specify the end jobs to be
equal to minus one now another parameter
to adjust is n estimators which will
specify the number of trees
the forest will use more more trees
which translates to higher end
estimators is always better
and it tends to reduce the overfitting
of the algorithm but you also have to
consider the training time and memory
allocation because more trees and
higher-end estimators drain more
computer resources so you really have to
try different values and observe how it
works now similar to single decision
trees you can apply pre pruning by
specifying max depth of the so the
maximum depth of the tree then another
parameter is max features which
determines the randomness of each tree
so a smaller value reduces overfitting
now according to Muller and guide of
this book it's okay to go with the
default values which are these so the
square root of ten features if your task
is classification and the logarithm of n
features if your task is regression now
there are other parameters of random
forests that you can tune but I'd say
these are probably the most important
that we should talk about now in terms
of weak points random forests may may
not work well with data that is sparse
and very high dimensional like and text
processing for example in which case
you'd better go with linear models now
in general random forests can have good
performance even on big and large data
sets especially if you realize you're
training over many CPUs by using the and
jobs parameter but like I said they may
need more time and more memory for the
training process compared to other
models okay so this is it for random
forests and for this video next we're
going to start looking at neural
networks and how they are implemented in
scikit-learn
as always if you enjoyed this video
please give me a thumbs up and subscribe
thanks for watching and I'll see you
soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>