<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - The Cancer Dataset - 23 - Neural Networks 5 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - The Cancer Dataset - 23 - Neural Networks 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - The Cancer Dataset - 23 - Neural Networks 5</b></h2><h5 class="post__date">2017-05-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nbDBkkCG51M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in this video we're going to look at the
learning process of our neural network
so we're actually going to find out how
it learns to make decisions remember
that we did something similar for
decision trees and random forests by
looking at the importance that each
feature bears in the decision-making
process now in this case it's not going
to be as easy okay what we're going to
do is to inspect the weight of our model
specifically we're looking at the
weights learned between the input and
the hidden layer but we'll use method
Lib and we'll create a figure so first
we'll go here and at the top stand
import Matlock lead pipe lot as PLT and
we're going to do the knot plot live in
line magic man so that we can have our
plots displayed within the notebook and
not in a separate window and let's run
this okay and then we'll do the plotting
so we'll plot a figure with a fixed size
of 20 by 5 and then we'll do plot M show
the coefficients of our multi-layer
perceptron with no interpolation and
with a color map of green blue GNB you
then will
- so PLT over here and over here PLT why
ticks are going to be in range of 30 so
the number of our features and cancer
that feature names and then our X label
is going to show the columns in weight
matrix and the Y label is going to show
our input feature and finally we're
going to display the color bar aside
okay now let's run this with shift enter
now at a glance this might look like
very complicated but let's try to break
it down so the rows in the plot are the
30 input features of our data set as
they are denoted over here and the
columns will present the number of
hidden units 100 in this case we've
chosen the color map green blue so gnb
you as our heat map with darker or blue
associated with more positive values and
lighter or greenish or lighter green
associated with more negative values
like it's shown here in the color bar
now as suggested in mueller's is machine
learning book which I linked to in the
description we could infer that features
with very small weights across the
hidden units may be less important so
please try and look at the overall
picture here and as you can see or as
you can observe there is a pattern
over here so between smoothness error
and fractal dimension error these seem
like they don't play such an important
role in the decision-making and
similarly for mean smoothness over here
and mean compactness these two now this
type of coloring of weights for these
features could also mean that we didn't
represent them in a useful way to our
network so make sure to be careful with
the interpretation anyway to understand
this type of visualization better we
could experiment with less complex
datasets and I encourage you to do that
on your own okay
we'll conclude neural networks in
scikit-learn for this data set in the
next video when we'll review the strong
and weak point of this type of
classifier alright as so as always if
you enjoyed this video please give me a
thumbs up and subscribe thank you for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>