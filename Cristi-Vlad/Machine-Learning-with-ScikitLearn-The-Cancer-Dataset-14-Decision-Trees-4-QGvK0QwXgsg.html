<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - The Cancer Dataset - 14 - Decision Trees 4 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - The Cancer Dataset - 14 - Decision Trees 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - The Cancer Dataset - 14 - Decision Trees 4</b></h2><h5 class="post__date">2017-04-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QGvK0QwXgsg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the fourth video on decision
trees and the fourteenth in the series
of machine learning pétanque you learn
on the cancer data set in the previous
video we created a graphical
representation of the tree to provide
some insight on how the decisions are
made as the tree has been trained or fed
on to our data set to further understand
how decisions are made we can use a
property of the decision tree which is
called feature importances and as you
may guess it will show us how much
weight each feature carries for
decision-making we can call this
parameter and when we do it each feature
is attributed a number between zero and
one zero meaning that the feature is not
used for decision for the decision
process while one meaning that it
perfectly predicts the target so let's
print feature importances format our
tree feature importances shift-enter
to run the cell
let's check the type of the of the
feature importances
so having the names of each feature as
well as the graphical representation of
them would be even better than just
viewing them as a numpy array so this is
basically a numpy array telling us the
importance each feature carries but we
don't know exactly I mean we could look
into the names if we say print cancer
feature names and we could just try to
associate one with the other but this is
not the most elegant way to do it so
we'll do it in another way
first let's import numpy over here so
we're gonna import numpy as and be run
the cell of course and then we'll go
back over here so we'll do some
visualization first let's say the number
of features is cancer data shape 1 which
is the same as saying that n features
equals 30 now let's do some plotting so
we're gonna use a bar H and the range is
and features tree feature importances
and we're gonna align to the center
okay now the white ticks are gonna be in
number a range and features
and cancer feature names our X label is
gonna display feature importance and our
Y label is gonna say feature now let's
show this blot okay now here we can see
that the worst radius carries a lot of
weight in decision-making and if we look
at the tree over here we can see that
it's at the top or the root of the tree
then we have the worst concave points
texture error and so on in terms of
their importance now one should be
careful in interpreting these
importances worst radius may be
important as a feature but as pointed
out by Andrea's Muller and Sarah Kaito
in their machine learning book it might
not tell us that a higher radius really
indicates that the sample is malignant
or benign okay now and the next and
final video on decision trees will
review some of the strong and the weak
points of decision trees if you consider
them using in your machine learning
projects if you found this video helpful
please give me a thumbs up and subscribe
see you in the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>