<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - The Cancer Dataset - 22 - Neural Networks 4 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - The Cancer Dataset - 22 - Neural Networks 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - The Cancer Dataset - 22 - Neural Networks 4</b></h2><h5 class="post__date">2017-05-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xcWEO9Vl1ec" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the 22nd video in this
machine learning series and the fourth
for neural networks now after scaling
the cancer data set and retraining the
neural network our performance got much
better compared to the one on the
unskilled data we finished the last
video noting that there is still a
discrepancy or gap between the training
subset and test subset scores of the
classifier
there are many parameters that can be
modified for neural networks inside
headline and so far we only modified the
maxpider or the max the duration number
from 200 which is the default to 1,000
we've kept the rest of them with their
default value okay so let me show you
let me call our classifier shift enter
to run it now a side of the max idle
which was set to 1000 the rest of these
are with their default values and as we
can see there are many ways to control
the complexity of annual Network now the
default parameters work good in general
but if we have solid knowledge of neural
networks and how they work in
second-round we can modify or tune them
to improve the performance of the
algorithm we can play with the solver
for the optimization of weights
switching between l-bfgs atom which is
the default and SGD stochastic gradient
descent we can also change the number of
hidden layers the default is 100 the
activation function we can switch
between gray Lu which is rectified
linear unit 10 H tangent hyperbolic is a
logistic or others
we can also change the penalty the
learning rate the regularization or
alpha parameters and so on in fact let
me show you how you can quickly look at
these parameters their options and the
short documentation for each within the
notebook so once again I'll type MLP and
then Shift + tab and then click on the
plus sign let me make some space here
okay so MLP Shift + tab and then the
plus sign okay well there you go this is
a convenient way to access the
documentation now this aside let's get
back to our gap so in this case we want
to decrease the discrepancy between the
performance on the test subset and
training subset let's see what happens
when we strengthen the regularization by
increasing the Alpha from very low so
right now it is 0.0001 and we'll
increase it to 1 for convenience we'll
try to run the entire process again so
MLT will instantiate the new classifier
MLP classifier will keep the next adder
to 1000 and we'll set the alpha to 1 and
the random state to 42 then we'll fit it
onto our scale the data white train and
then print the performance so again I'll
just copy paste these and all right
just enter to run the the cell X train
it's not X trained so extreme scale rate
so as you can see with being able to
decrease the gap compared to this one
hence we've been able to improve the
performance of the algorithm so we
simply strengthen the regularization of
the model through the alpha parameter
now I'd encourage you to play with the
rest of the parameters and also look
further into the documentation to see
how each contributes to the performance
of the neural network okay and the next
video we'll do some visualization and
we'll try to understand how our neural
network learns through the training
process now please give this video a
thumbs up and subscribe if you like it
thanks for watching and I'll see you in
the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>