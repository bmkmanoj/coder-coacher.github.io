<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks and TensorFlow - 9 - Loss Function, MSE, Cross Entropy | Coder Coacher - Coaching Coders</title><meta content="Neural Networks and TensorFlow - 9 - Loss Function, MSE, Cross Entropy - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks and TensorFlow - 9 - Loss Function, MSE, Cross Entropy</b></h2><h5 class="post__date">2018-02-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MXWHDVGCYb0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is the purpose of your typical
machine learning or deep learning
project let's say for example that we're
doing supervised learning our purpose is
to have a model that learns how features
associate with labels thus during the
training process the objective is to
optimize this model by tuning its
parameters and that's what we're going
to start discussing today
now we'll do a regression model for
which f of X I'm is a linear combination
of the input data the weights and a bias
or intercept and our target as f of X I
plus some noise okay now like we've seen
previously we're gonna define our
variables so we'll do this just for
exemplification purposes so let's say X
Y so inputs output W and B and our X is
going to be ATF place holder of float32
data type and the shape is gonna be none
and three ok now our labels so Y - it's
gonna be TF placeholder the same data
type and shape is gonna be none now our
weights are gonna be ATF variable of 0 0
0 and D types of data type is going to
be TF float32
and we're just gonna name it W and our
bias or intercept is going to be TF
variable so we're going to initialize
the bias with 0 the data type is going
to be TF float32 and
name it B and what's left now is to
define this model which is a
multivariate linear regression so let's
say output Y predicted is gonna be TF
Matt mall of WTF transpose X plus B so
this is a matrix multiplication of the
transposed inputs and the weights to
which we add the bias and I'd like to
give credit to Tom Hope whose book is
linked in the description for this
example alright now here comes the very
important part so to evaluate the
performance of the model we need some
kind of measure we need a good measure
for that and this measure needs to tell
us the gap or the distance between the
predicted and the true values and this
is also known as a loss function so let
me repeat that to make it clear the loss
function will tell us how far our model
is from the truth
or how big is the distance between the
predicted and real values therefore the
ultimate purpose of the training process
is to minimize this gap or to bring our
model close to the truth so in other
words we're looking to minimize the loss
and we usually do that by tuning the
parameters or by adjusting the
parameters the weights and the bias here
now for the sake of following the
textbook will be discussing the mean
squared error as the measure for the
loss function so ms e in Swart averages
the squared distances between the real
white true and the predicted Y pred
values so this is how it looks like not
very beautiful I'd say
and in tensorflow this is gonna be look
like the following so let's say Ms II
mean squared error we'll just call it
loss TF reduce mean TF square white Chu
- why
Pratt so first we have an element-wise
subtraction between the vectors then we
compute the square of the result and
then we average these square the
differences okay so this is mean squared
error one of the measures for the loss
function now another commonly used loss
as cross entropy which is often used for
categorical values so this is cross
entropy in short this basically reduces
to the negative logarithm of the
probability placed by the classifier or
the algorithm on the correct label okay
I know it's a mouthful so how is cross
entropy gonna look like in intensive
flow so let's say cross entropy and
we'll just call it loss it's gonna be TF
and n sigmoid cross entropy with log
it's another mouthful our labels are
gonna be wide true and our lockets are
gonna be why Brad and then we have two
TF reduce mean the loss so we could have
simply written everything in one line
but I guess this makes it more clear so
cross-entropy measures how similar two
distributions are in this case P and Q P
is the true or our true labels and Q is
the predicted or our white Brad
so as cross entropy the loss function
decreases P and Q are going to be closer
or more similar to one another okay so
we have mean squared error and cross
entropy we should stop here because I
think we've covered a lot of ground in
one lesson and these concepts may be a
bit harder to digest when they are not
put in a specific context but don't
worry we'll get into programming later
first we have to appropriately
understand these critical concepts okay
so as always please give this video a
thumbs up and subscribe if you enjoyed
it if you want to support these
educational videos you can do that by
following the patreon link in the
description thank you for watching and
I'll see you in the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>