<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - The Cancer Dataset - 26 - SVMs 2 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - The Cancer Dataset - 26 - SVMs 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - The Cancer Dataset - 26 - SVMs 2</b></h2><h5 class="post__date">2017-06-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/404knXpDaPM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in this video we're going to talk about
one of the most important features or I
would say strong points when it comes to
support vector machines and that is the
kernel trick we're not going to go into
the mathematics of it even though I'd
like to do that and maybe I'll do it in
a future video however there are some
tutorials on YouTube that explain the
math behind svms so you might want to
check that out if you're interested here
I'll try to present the kernel trick
conceptually and we kind of don't have
to know the details to implement it in
most machine learning libraries and in
scikit-learn however that doesn't mean
that it's not good to know
anyhow proceed shall we so we can easily
deal with linear problems using or
applying a linear SVM classifier or
regressor but that may not be the case
when we have nonlinear data or data that
is not separable by a line so let's look
at a very simplified example let me
execute this cell
so here we have just one feature and the
data is not linearly separable if we end
another feature say X 2 which is X 1 to
the power of 2 then the data becomes
linearly separable as you can see and in
this case we've just used something
called polynomial features to make the
data separable by a line we can use this
on very low complexity data or for
smaller data sets and we can apply a low
polynomial degree but when the data
becomes more complex and higher
dimensional that's when we can use a
kernel or the kernel trick which
basically works along the same lines so
with the kernel trick it is like adding
multiple polynomial features and even on
polynomials a very high degree without
actually having to add them and in this
way are kind of avoiding the
computational complexity or burden that
comes along with adding multiple
features to your data that's why it's
called the trick so think of it like
this the kernel trick twists or works
the space in which your data is and by
warping this space the algorithm is able
to find a linear separation of the data
now the linear separation is within the
work space and that linear separation
becomes nonlinear when transposed or
applied in the non worked space or in
the original space now to put it in
other words
using kernels allows you to apply some
sort of transformation to your data to
change your original input space to a
higher dimensional space in which you're
able to find a linear separation for
your data and then you take that to your
original space where it becomes
nonlinear but visible so something like
this once again this trick is very
praised within the community because it
saves computational complexity and
burden and the mathematics of it is just
beautiful it it has to do with
calculating the dot product of the
transform vectors without having to deal
with the transformation itself but I
think this is enough for the level of
detail in which I want to go into now in
the description of this video you'll
find another similar short explanation
of the kernel trick by sebastian front
of Udacity there are several kernels
used in machine learning so for example
we have the linear kernel the polynomial
kernel the radial basis function or the
RBF kernel which is also known as the
Gaussian RBF there's also a sigmoid
kernel and so on but the - like most
used are the polynomial and the Gaussian
kernel now in the next video we're going
to use an RBF kernel with an SVM and
we're going to apply that to our cancer
data set
so if you enjoyed this video please hit
that like button and subscribe thank you
for watching and I'll see you in the
next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>