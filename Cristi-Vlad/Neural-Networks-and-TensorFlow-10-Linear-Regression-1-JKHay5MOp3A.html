<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks and TensorFlow   10   Linear Regression   1 | Coder Coacher - Coaching Coders</title><meta content="Neural Networks and TensorFlow   10   Linear Regression   1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks and TensorFlow   10   Linear Regression   1</b></h2><h5 class="post__date">2018-03-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JKHay5MOp3A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">remember from a previous lesson that
we've discussed the gradient descent
optimizer which we used to minimize the
last function so gradient descent will
update the parameters the weights and
the bias in this case so that the loss
decreases over time now there are
several other optimizers that you can
use but we'll stick to gradient descent
for now as it is widely applicable now
gradient descent can be done in more
than one ways like for example if we
determine the gradient for the entire
data set we call it batch gradient
descent but if we determine the gradient
on multiple subsets or many batches of
the data set we'll call it stochastic
gradient descent now in this case we'll
also have to figure out how large our
mini batch has to be and this might be
tricky because different batch sizes may
yield different results and different
computational costs but this is not the
subject of this lesson okay so how does
gradient descent look in tensor flow
well it is as simply as so optimizer TF
you're just calling gradient descent
optimizer and specifying the learning
rate or how big the step towards the
negative gradient it is and then we just
we just minimize the loss so we just
call optimizer minimize loss once once
we have these define we'll run them
within or through a set up run okay now
let's see this in practice let's do a
linear regression for example on some
synthetic data the data will have some
properties and the purpose of our model
is to recover these properties through
optimization so let's import numpy as NP
and then generate some data
say X datum and PE so I have to once
again thank Tom hope for providing
inspiration for this example and his
textbook is linked in the description of
this video so Ann P random rent and 2003
so we have 2,000 samples each having
three features now we'll have our
weights and bias so our real weights and
our real bias so W real is gonna be lets
say 0.4 0.6 and 0.2 and our be real so
be real is gonna be minus 0.3 and then
we're going to generate some Gaussian
noise which we we're going to add it to
each sample so let's say noise equals NP
the same random random one two thousand
times 0.1 and our output is gonna be Y
data a matrix multiplication and we're
gonna use the method from numpy so NP
Matt mall W real between the weights and
our transposed data to which we're gonna
add the real bias plus B real plus the
noise okay now we'll be creating our
computational graph let me just make
some space here so we'll be creating the
computational graph with weights and
biased initialize to 0 and when we run
the graph the purpose its purpose is to
bring to bring these weights and biases
and we'll do ten iterations after which
we'll print the output at every 5
iteration now in most projects there are
better ways to optimize
the weights or to initialize the weights
and baez not with zero but we'll touch
upon that in later lessons okay so let's
say number of iterations is gonna be ten
then we're gonna create a graph let's
say G is gonna be TF graph so we have
tensorflow imported I think it's
imported some somewhere over here import
tensorflow and and also not by I guess
this is redundant but anyway so TF graph
and we're gonna create a list for our
parameters so let's say WB from weights
and bias it's gonna be a list and then
with G as default G as default we're
gonna create a place holder so actually
we're gonna create placeholders for the
input data and for the true for the true
output so X is gonna be TF placeholder
the data type is gonna be float 32 and
the shape is gonna be none and 3 so we
can add as many samples as we want as
long as they have three three features
here so this is this actually it's gonna
be able to receive data from X data here
but we'll see that okay so then we're
gonna actually have the placeholder for
Y true okay um so TF placeholder the
same float 32 and the shape in this case
is gonna be simply in none so no shape
okay so we'll be using name scopes here
to better understand how operations
group together so we'll say with
d/f name scope so the first name scope
is gonna be inference as scope w is
gonna be atf variable so like i've said
we initialize the weights with zero so
it's gonna be 0 0 0 the data type is
gonna be so d time it's gonna be float
30 so it's gonna be TF actually DF float
32 and we'll simply gonna name it W from
weights and then our bias is gonna be a
TF variable of 0 D type DF float 32 and
the name is gonna be be okay and now our
predicted output so we have our y2 and
then we're gonna have Y Pratt it's gonna
be now we're actually calling the Matt
mall from tensorflow so TF matrix
multiplication between W the weights and
the transpose TF transpose of our X of
course to which we add the bias okay now
we're gonna do another name scope for
our last function so with TF name scope
let's say a loss as scope of course and
we're gonna use a we're gonna use the
method that we've discussed in the
previous video so the mean squared error
this one I can simply just copy paste
this so DF reduce mean
d/f reduce means you have square white
room - white bread so actually this is
an element-wise subtraction - which then
we square the result F and after which
we use the reduce mean over the result
okay so now that we have the lost
function defined we're gonna create
another named scope for the training so
the purpose is to minimize the loss so
with TF names code training as scoped so
we'll have to define a learning rate and
we'll just use 0.5 learning rate and
whenever you're actually doing a project
a deep learning or machine learning
project involving learning rate you'll
actually tweak the learning rate by
trying different learning rates to see
how they influence the results as well
as the computational costs of the
process so the learning rate 0.5 then
let's define the optimizer so optimizer
is gonna be TF trained and we're gonna
use gradient descent optimizer using our
learning rate and the purpose is to
Train so train the optimizer minimize
minimize the loss function ok now all we
have to do is to initialize all
variables so let's say in it
DF global variables initializer and then
create a session after which we run it
so as we've seen previously with TF
session
as sess first we run the initializer
success run Annette Annette after which
we'll just say for step in range of the
number of our iterations so num errors
will run the training process so we'll
just say SAS run train and now we
actually have to feed in values from our
X data to the place holder here so we'll
just say so this is gonna be a
dictionary so X our place holder is
gonna take data from X data and why true
is gonna take values from so white
through white data okay
and now we'll have to a plant actually
append and display the output at every
five up five iterations or epochs so
we'll just say if if step modulus five
equals equals zero
we're gonna print the step of course and
then says run W and B okay and we'll
actually have to append
SAS run our weights and bias so W and B
of course and finally we're gonna print
what happens after ten iterations or
tank epochs so let's say print 10
sass run WB ok I know it's quite a lot
of code in one setting so let's see if I
actually made any mistake here
so now we'll actually run this and see
how quickly it finishes so hit shift
enter okay so it actually finishes in an
instant and we we can see that the
predicted Waits and the bias or very
close to the real ones so remember that
we the real ones are 0.4 0.6 0.2 so
after 10 iterations we have 0.40 point
59 and 0.1 hundred and 99 which is
really close to 2 2 and minus zero point
24 so the be real didn't actually
converge that close but it's actually
really close and it might get closer
after even more iterations okay so this
is actually how a simple model is
implemented in tensorflow next we'll
look at a different example and instead
of reduce mean we'll use so instead of
mean squared error
we'll use cross entropy for our loss
function
all right so please give this lesson a
like and subscribe if you enjoyed it
thank you for watching and I'll see you
in the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>