<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks and Tensorflow - 4 - Gradient Descent | Coder Coacher - Coaching Coders</title><meta content="Neural Networks and Tensorflow - 4 - Gradient Descent - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks and Tensorflow - 4 - Gradient Descent</b></h2><h5 class="post__date">2018-01-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J0uc3GWq4_k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's try and get the bigger picture
that's mostly sought after in many deep
learning projects so we often work with
labeled datasets and these datasets are
made of hundreds thousands or even
millions of samples and each sample is
characterized by multiple features and
one or more labels we said that for each
input that goes into our algorithm or
our neural network we will multiply it
by a weight and add a bias term which is
a constant now when we train the neural
network on our data set its goal is to
find the best weights to efficiently
learn the features with their
corresponding labels but what are these
initial weights well very often we
randomly initialize these weights so
when the neural network does the first
computation with these randomly
initiated weights it might be very close
or very far from the solution but how
does it know what the goal is
and how does it know to work its way
towards the optimal values for these
weights well here is where we will use
optimization algorithms and one of the
most popular being gradient descent now
this algorithm is used to find or to
determine the minimum of a function so
let's take a step back consider that our
random initiated weights have an error
value associated with them we will be
using gradient descent in an effort to
reduce that error making our weight
toward the minimum of the function so to
find the local minimum of the function
the optimizer takes small steps which
are proportional to the negative of the
gradient of the function at its current
point but what is this gradient so in
mathematics a gradient is basically a
generalization of the derivative now the
difference between gradient and
derivative is that the derivative is a
function of a variable while the
gradient is a function of multiple
variables less abstractly but the
gradient is a slope now gradient descent
that means to go down the slope so think
of gradient descent the following way
assume you are on a mountaintop and
you're skiing gradient descent in this
case would be the path that takes you to
the bottom the fastest now this may seem
simple but the mathematical details
complicated a lot but luckily today one
might not need to know all these details
if they're working with higher level
libraries like tensor flow for example
so how do you know that you're skiing in
the right direction or the fastest path
to the bottom well you take small steps
as pointed out by Aaron Golgi and AI
researcher and I'm gonna quote here you
need a some kind of heuristic to find a
correct way one of the best choice and
the simplest one is to check the decline
rate by single step for each possible
direction and chose the most steepest
one and if gold so how many steps back
and forth and how big are these steps
going to be well this is where we will
establish a learning rate for our
algorithm a high learning rate will
allow you to take bigger steps but you
might overshoot the local minimum now on
the other hand a lower learning rate
might be better in many situations but
its drawback is that it can be
time-consuming
so to efficiently optimize the algorithm
one might need to do a little tweaking
of the learning rate so here as courtesy
of andrew inc we have a small learning
rate and in this case gradient descent
is slow well here we have a higher
learning rate and as gradient descent
can overshoot the minimum
so how would gradient descent work step
by step from a higher-level perspective
okay so with our randomly initiated
weights we first evaluate the gradient
using its formula at the current
position now this will help us find the
direction of the steepest descent then
we'll take a step into that direction
which theoretically makes us closer to
the minimum or brings us closer to the
minimum then we'll reassess the
direction of the steepest descent at the
new position by determining the gradient
at this position and then taking a step
in this new direction and we'll be
repeating these steps until eventually
reaching the point of the minimum error
now there are different ways to do
gradients and some of them are batch
gradient descent mini-batch gradient
descent and stochastic gradient is said
or so stochastic gradient descent or SGD
and some of the most used optimizations
can be for example momentum aramez prop
atom Atta grad and so on and each of
these deserve an entire chapter to
discuss them in detail which is not the
point of this video so to reiterate we
use gradient descent in deep learning
problems to find the minimum of a
function now gradient descent is central
to deep learning so we should have at
least a basic understanding of it okay
so we'll be applying gradient descent
optimizers and dancer flow when we get
to the coding part of the East tutorial
alright that's it for now I hope this
video was helpful before leaving please
hit the like button and subscribe thank
you for watching and I'll see you in the
next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>