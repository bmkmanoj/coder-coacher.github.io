<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Scikit-Learn - 34 - Uncertainty Estimation in MultiClass Datasets | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Scikit-Learn - 34 - Uncertainty Estimation in MultiClass Datasets - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Scikit-Learn - 34 - Uncertainty Estimation in MultiClass Datasets</b></h2><h5 class="post__date">2017-07-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gIJHI5jSChM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in this final video on uncertainty
estimation as I could learn we're going
to look at how to approach multi-class
classification remember in the first two
tutorials for uncertainty estimation
we've looked at methods like the
decision function and predict proba for
binary classification that is we
classify tumors into malignant or benign
now what happens if we have more than
two classes well probably the easiest
and most convenient way to look into
this is through the iris dataset which
similar to the cancer data set is
preloaded and pre-processed in
scikit-learn now the flowers in this
data set are divided into three
categories
so Tosa virginica and versicolor so we
have three classes here now let's do
uncertainty estimation for that and
we're starting a new notebook since
we're dealing with another data set so
first things first from SK learn data
sets import load iris which is our data
set I don't think we're going to need
numpy MATLAB or other libraries for now
then from SK learn model selection we're
going to import train tests split now
we're going to create an instance of our
iris data set so iris a load iris then
split it but first let me show you the
classes so iris dot target names and
we're going to run this it's going to
take a second to load so we have setosa
versicolor and virginica as I said
okay so now let's split the data so X
train X dust white rain why test is
train test split iris data iris target
and we're going to have a random state
I keep capitalizing them of 42 which is
for reproducibility all right now we're
actually going to follow the machine
learning book of Andrea's Miller which
is kind of linked in the description and
we're going to train a gradient boosting
classifier we've not used this type of
classifier so far but it's a kind of the
same thing with training the rest of
them as I could learn so let's import
the classifier first which is an SK
learn ensemble import gradient boosting
classifier and now let's instantiate it
so first let's run this and now let's
instantiate it so givi our tea is gray
and boosting with a learning rate of
zero zero one and a random state of zero
and now let's train it or fit it to our
data so g PRT fit onto our training data
X train why train all right now let's
run this shift enter
okay so first let's print the decision
function print the decision function for
the three class iris data set we're
going to have two new lines here
okay now format our classifier
this is a function on our ex test let's
say the first twenty or better the first
ten samples of the test subset now
shift-enter to run it so gradient
boosting classifier I didn't I think I
didn't so G BRT decision function the
decision function okay run all right so
remember higher values for this decision
function for each sample represent a
Horizonte that the sample is in the
respected class while lower values means
otherwise now I won't go into details
because I've already discussed this in
the decision function video which I
think it was the 32nd in this tutorial
series
now as for predicting probabilities
let's print the predicted probabilities
probabilities for the samples in the
iris data set we're going to have two
new lines format make sure not to mess
this up GVR t predict remember we're
using predict proba on our x test the
first ten samples shift-enter right now
as you may remember for predicting
probabilities summing up the values on
each row will lead up to one hundred
percent or one and the class with the
highest value will be the one that's
predicted so for example for this first
well the class that's predicted is the
second one for this sample it's the
first class for this sample it's the
third class as it has the highest value
and so on and so forth okay and I think
that's just about it for a multi-class
classification not much of a difference
compared to binary classification but I
guess it was worth going through for the
sake of example fine all right
this concludes our short incursion into
uncertainty estimation which I could
learn next we're probably going to start
discussing pre-processing so make sure
to subscribe if you're interested as
always if you found this video helpful
please hit that like button thank you
for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>