<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks and TensorFlow - 11 - Logistic Regression | Coder Coacher - Coaching Coders</title><meta content="Neural Networks and TensorFlow - 11 - Logistic Regression - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Cristi-Vlad/">Cristi Vlad</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks and TensorFlow - 11 - Logistic Regression</b></h2><h5 class="post__date">2018-03-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LgNpIPYstw0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in the previous lesson we went through
an example of linear regression with
tensorflow let's now look at logistic
regression here will have a linear
component as being the product between
the weights and the transposed inputs
plus the bias something similar to two
linear regression
but what's going to be different is that
we'll take this linear component through
a logistic function and we'll use the
sigmoid now I've discussed about the
sigmoid function in earlier videos in
this series
okay so let's define this function so
you can see how to do it if you want to
build a model from scratch
however for ease of implementation there
is a method in tensorflow
which is D F dot sigmoid which you can
directly call and uses the sigmoid
function anyway all right so let's
define our sigmoid which is gonna take X
and it's gonna return 1 over 1 + and P
exp which is basically the exponential
of minus X ok and now similar to what we
did in the previous tutorial we're going
to generate some data so let's say X
data equals MP random rent and so I have
to make sure I'm not gonna make the same
mistake like I did in the previous one
and this time we're gonna take 20,000
samples with three features okay and now
our real weights are gonna be a list 0.4
0.6 0.2 I guess these are exactly the
same as the previous ones yes and real
bias so be real you have to be real it's
gonna be minus 0.3 okay and now
or linear function W B and the a matrix
multiplication from mat by numpy excuse
me W real X data transpose and of course
to which we're gonna add the bias okay
and same as we did previously we're
gonna have the output before the noise
so let's call it Y data before noise
equals and here we're gonna use here
we're actually gonna use the same way
that we just defined so we'll call it
sigmoid of WB okay so we are applying
the sigmoid function and then and then
we're actually gonna do the noise so for
the noise we're going to randomly draw
from this white data before noise so
we'll just simply call it white data and
it's gonna be random binomial one and
white data before noise okay so one here
denotes the number of trials
now let's specify the number of
iterations create the graph and then
execute it
so we'll say num let me just execute
this one and continue in the next cell
okay so this time we're going to go
through 50 iterations and we're gonna
instantiate the graph so G is DF graph
and similar to what we did for linear
regression we're gonna use a list for
the weights and the bias so it's going
to be an empty list and then we're gonna
just do
with G as default and here we we're
actually defining the placeholders for X
so our inputs DF placeholder is gonna be
of data type float 32 and the shape is
gonna be none so this means that we can
have as many samples as we want as long
as they have three features and Y true
so our true output is going to be TF
placeholder the same TF float 32 and the
shape in this case is gonna be equal to
none okay so we have the placeholders
for the inputs and for the output and
we're actually gonna use name scopes
like we did previously so this is like a
percent similar to linear regression
with a few exceptions so we'll just say
with TF name scope inference as scope of
course we're gonna have our we're gonna
initialize our weights so we'll say TF
variable and we'll gonna initialize the
weights with zero and like I've told you
there are best ways to initialize the
ways but for the sake of this example
we're gonna initialize them with zero
okay now the data type is gonna be TF
float 32 and we're just gonna name it
weights
now the bias is going to be also a TF
variable and we're gonna initialize it
with zero the data type is gonna be TF
float 32 and the name is gonna be B and
now we'll do the predicted output Y
prime so we have Y true and y brand
so predicted output is gonna be a matrix
multiplication this time we're taking it
from tensorflow with TF
transpose the transpose of X to which we
add the bias okay now another name scope
for the last function and this time
we're gonna use a different loss
function
we're gonna use a sigmoid cross-entropy
with log its so let's say with TF name
scope loss ESCO we're just gonna say
loss and this is gonna be a long word D
F and n sigmoid of course we can just
hit tab and fill it automatically but
let's not do that or let's do it
so cross-entropy with log it's the
labels labels are gonna be our true
output and the log it's are going to be
our predicted output okay and then we'll
just gonna apply the reduce mean over it
of course we could just say here TF
reduce mean to all this mumbo-jumbo but
to for better visualization which do do
it sequentially so we're gonna TF reduce
mean our loss that we just defined over
here okay and now we're gonna go through
the training so another name scope for
training with gradient descent scope and
we'll have the same learning rate of 0.5
then we'll actually define our optimizer
is gonna be TF trained gradient descent
optimizer using our learning rate and
the training so trained will use our
optimizer to minimize the loss alright
now let us initialize these variables so
we'll say in it d/f global variables
initializer okay and finally let's
create and run our session so we'll say
with DF session as SAS first we're going
to run D init to initialize our
variables so let's run in it and then
for each step in range of in range of
what in range of num errors so num
errors will go necessar on the training
process this over here and we'll have a
dictionary in which X with the place
which is a placeholder is going to take
data from X datum and y2 so the true
predictions are gonna take values from Y
data okay and then we'll append and
display the output at every five
iterations so if a step modulus five
equals equals zero will print the step
so we'll print a step at which we are at
and then the sass run wmb and then we're
going to append to this empty list so WB
which is an empty list we're going to
append the sass run W and B and when
we'll reach the fiftieth step we want to
see the final output so we'll just say
here print when we are at the fiftieth
step we'll say
sass run WB this is to actually see
to see the out the final output
basically so says run WB okay so quite a
lot of code in here let's hope
everything is written correctly okay now
shift enter to run this and we'll
actually gonna see how the training
process goes and it actually finishes
really quickly so shift enter
okay so TF reduce mean loss what's wrong
in here invalid syntax loss equals TF
reduce me okay all right so the initial
so our real ones were 0.4 0.6 0.2 and
minus 0.3 so the way it's 0.4 0.6 0.2 so
and the bias minus 0.3 so as we can see
it took a couple of more iterations to
converge compared to what we did for
linear regression but it finally
converts actually really well okay so
this is logistic regression in
tensorflow next we're going to look at
the M nest data set or how to classify
handwritten digits and for that we're
going to use the softmax regression
which we've talked a little bit about in
previous lessons okay so if you enjoyed
this video please give me a thumbs up
and subscribe and if you want to support
these educational videos please follow
the patreon link in the description
thank you for watching and I'll see you
in the next one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>