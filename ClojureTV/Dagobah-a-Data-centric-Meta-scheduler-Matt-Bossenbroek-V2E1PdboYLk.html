<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dagobah, a Data centric Meta scheduler - Matt Bossenbroek | Coder Coacher - Coaching Coders</title><meta content="Dagobah, a Data centric Meta scheduler - Matt Bossenbroek - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dagobah, a Data centric Meta scheduler - Matt Bossenbroek</b></h2><h5 class="post__date">2015-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/V2E1PdboYLk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I work at Netflix and at Netflix we have
a lot of data pipelines big data
pipelines that do anything from ETL to
building machinery lots of different
things and the one thing that we found
more than anything is that there's no
one solution that that fixes all of the
problems nothing nothing that does
everything everybody has their own
personal preferences some developers
might like one thing I prefer pigpen but
it can't do everything
SPARC is the new hotness everybody seems
to love that these days same with docker
sometimes you want more abstractions
sometimes you want the the MapReduce
thing sometimes you want less like with
the docker image you just want give me a
machine to do stuff you know the Hadoop
variants you know they're they're old
but they do their job and they they can
handle a lot of data and there's new
exciting stuff like onyx we heard about
earlier so what we had at Netflix was a
lot of pipelines built using different
hybrid technologies and they are all
kind of duct-taped together you might
have a bash script or something to glue
these together it's impossible to reason
about so we built something instead of
trying to compete with these and do all
of the things we've we built something
to help put these together in a sane way
and that's what dag about is the dag is
for directed acyclic graph so talk a
little bit about the motivation we
wanted data-centric pipelines a lot of
the workflow tools that are out there
today are more about execute a series of
steps they don't really know that
they're producing data we have data
we're producing data let's see if we can
get that into the mix a little bit more
we want predictability if I run it
multiple times I want the same thing I
want the same result I want to know
where that result came from so if I have
a wonky piece of data that I produced a
week ago and also don't like crap what
happened there you know I want to know
what the input data to that was what
code ran on that and importantly I want
structural sharing and this one is
because a lot of times we have a big
pipeline I just want to change one part
of it I don't have to rerun the whole
thing obviously we want fine-grained
computations we want basically a bunch
of pure functions data in data I don't
make stuff easy to test I guess I said
earlier we want to use a bunch of
different technologies different
resources and be able to stitch those
all together one thing that I'm not a
fan of is going into UI and clicking a
thousand times to modify something or
making systematic changes via a UI so
one of the design things for this is we
wanted it to be you change something and
check it in and that communicates with
an API we'll get into that a little bit
more later and most importantly we need
resilience resilience resilience
resilience stuff fails all the time
stuff fails daily we need to be able to
pick up where we left off and reason
about that there's often delays we don't
want everything to fall apart while
we're waiting for for data and we want
to be able to restart our workflows in a
sane way so before looking specifically
at d'Agata let's look at a traditional
workflow and what those are good for and
that'll help to drive why dagger but a
little bit different so traditional
workflow you have a start you do some
things this would be good for like
deploying a cluster into the cloud so
you start it might be like a trigger or
your build finished or something like
that and then what we'll bake an image
and then we'll deploy it and scale it up
might run some tests and then scale down
the old cluster and then maybe send the
user an email or something like that
traditional workflows are going to be
good at like branching out in this
direction so you might have something in
here that says all right in this stage
I'm going to look at the current state
of the world and based on the current
state of the world I'm going to take
this path or that path so they branch
out in this direction really well you
can make decisions and you you can
further that branching so we go out to
here and say
now I'm going to make another decision
so in this case you start at the same
place but you might have multiple
different outcomes from this workflow
where it gets a little bit sticky is if
you start adding more starts like what
does this what does this mean now you
could kind of say you know we'll run
both of these first and when they're
both done then we do the second second
layer then continue on yeah that makes
sense what about this what do you do
when you have something like that like
do I run the third one optimistically in
case that I took that branch and you
know like it's difficult to reason about
stuff like that in a regular workflow
setup but you can still cram a data
workflow into that and they often look
like this because we have a lot of
different pieces of data and a data
workloads general you're going to take a
piece of data here and a piece of data
here maybe process it and then smash
them together and create some other
result and then you're gonna take some
more data and smash it together and
create another result and so that's kind
of where we ended up before is you have
a bunch of different starting points and
kind of one result that the model or
whatever data you want to produce at the
end we can still kind of Reason our way
through how we would run such a thing
you know you can slice these off and say
we'll run all the top ones first you
know that that makes sense
then once those are done we can run the
next one and so on and so forth if
you're paying attention you could
probably notice you could probably start
running this one on the left here after
the top two are done without waiting for
the one on the right
not all workflow managers actually even
do that they don't take advantage of the
async nature of these things and knowing
that those two steps are decoupled Pig
is a good example of this internally Pig
if it as multiple stages will block on
the the right one before continues but
even if we can run these things you know
we can reason about this and we can we
can school we can build a data workflow
using these tools where it really starts
to get sticky is when you change one of
these things
what happens when I change this node so
there's a couple of different variations
of this so let's start with you know I
have this workflow and I want to try
something different for that one so do I
take and do I copy the whole workflow
and then just change that now I have
another named copy of this and I have to
run that is that rerunning that is that
going to rerun the whole left-hand side
when I rerun this will that result
overwrite an existing production data do
I have to worry about stepping on other
stuff there and so it kind of gets
sticky there you have a couple of
different problems now say I'm I'm done
iterating on this I actually want to
commit it so I have my workflow do I
just go into my workflow system and
change that one does it remember that it
used to be something else and that
yesterday it was a different node that
produced that data and today it's it's a
new node that's producing that same data
can it even reason about that these
nodes are producing data that they're
not just mutating my environment or
doing something else the other thing
that we don't really need in a data
workflows a lot of branching or
conditionals you don't generally want to
say like alright I'm it to this stage
now let me look at my environment and if
it says this I'm going to use this data
and if it doesn't I'm gonna use that
data you know yeah it's unpredictable
how do you know what you're gonna get if
you choose your input data at random
there's the aspect of when to run these
things a lot of workflows you just hit
go it doesn't really have a connotation
that we just need to wait for the data
to be available at the beginning say we
run this workflow in and you know the
step on the right fails do we have to
rebuild everything that we already built
on the left do you have any
checkpointing in there anything like
that even more so like say I had my old
version in my new version and I want to
have a workflow step that compares those
two versions how do you even express
that you know you get into these weird
things like you can't even try that so
we decided to take some of the closure
concepts that we know and love and apply
them to this problem and that's what we
come up with tangela so to start with
daga
we start with a single node so this is a
node in our dag so what's a node node
can be a computation so right now we
support pigpen obviously we support
medic at it's a thing that we have it's
basically like the metadata over s3 day
that kind of looks like a hive table we
had this thing at Netflix called Titan
that runs docker images you can run a
Python script via the Titan we were on
spark Oh
everybody loves spark in the machine
learning area and we can run presto
queries just like a faster version of
hive queries you really you could put
anything into here
you got a blob of information it says go
run this and you just have to ship it
off somewhere and write the key
difference in d'Agata is that each of
the nodes has an address so we've
separated the actual computation from
the name of what we're building we'll
get into the addressing a little bit
more later but for now you can see it's
just just a vector so we have some key
words and some symbols in there this one
says I'm going to build a search model
for a given date and the way that you
ask daga bow to do something is you
issue a request for it so you say I want
the search model for this given date and
it's gonna line it up and it's going to
do some matching based on the static
parts of that pattern and says well I
found this node they can build that
pattern so I can build that for you so
it's gonna go off and it's gonna run
that node which will produce data all of
our stuff lives in s3 for right now but
there's nothing that says that it has to
and then we end up with this data
pointer it's d4 data so we have this
data pointer that points to the location
of us the actual physical data in s3
where our job wrote it and dagga will
tell you here's a unique location -
right - it points back to the node that
wrote it so it says this is the code
that we know built this data and it
points back to the address and it says
you know this this was the address of
this date and we remember this but if we
just have a single node that's not
terribly interesting we don't we don't
have a
workflow yet so the way that we do DAGs
essentially is each of these nodes can
specify I need this input data so I need
some play data so this might be views of
people watch something on Netflix they
might say here the things that people
search for on Netflix so we're going to
pass the data in that we got from our
user we're gonna issue new requests
within the system and say dagger would
build this data for me first so it goes
up the graph we're gonna find some more
nodes that can satisfy those requests
find the the nodes build the data it's
gonna build them at the same time say
one of them finishes first and now we
have a data pointer points to s3 points
to the address points to the node now
the other one finished and now we have
another piece of data in there now we
can run our new node zero that has new
dependencies the important distinction
here is that the data pointer for a node
0 now points to the ancestor data from
node 1 and node 2 it knows the exact
piece of data that was used to build
node 0 so you if you go and you're like
why was my model all wonky I want to
find out you can actually go back
through the the lineage of the data and
find the input data find the code that
was run and then reproduce it so that's
kind of the overall model of d'Agata
it's a little bit different than a
workflow because each individual step is
kind of issuing a new request but that's
what it looks like so we have these node
definitions let's look in the addresses
let's look a little bit closer at that
of course you're defining them in Eden
you know this is this is closure so why
not you have a map that defines what a
node looks like and we're going to give
it an output address so this is the
address that we showed in the last slide
and this is the logical address for this
node is a search model I had another
parameter in here called model type in
data so this means that the the keywords
used here are the static parts of the
address the symbols used are the
variable parts so this note is parameter
izybelle
by model type and data you
they build the model foo for data and
today or build the model bar for data in
tomorrow there's a physical address and
this is mapping our different request
parameters into a template that can be
used to generate an s3 path or some
other sort of physical storage path this
is what daga was going to use to figure
out where to tell the node the actual
code that's running where to write the
actual data we have dependencies so this
is where I'm saying
you know I want to go get this other
data first so give me play data for that
date and give me search queries for that
day and just give them friendly names so
when I'm in my in my code I can just
refer to him by those names and this is
the code to run so in this case we're
going to run a pigpen script the script
fun trick function is the entry point
into our code and then executive says
where to run it Jeannie is a service
that we have at Netflix that executes
Hadoop jobs of any sort you can
basically just say Jeannie go run this
for me and it'll tell you when it's done
lastly we have tags tags are applied by
the build process you don't actually
specify this when you're writing one of
these but it ends up looking like this
once it gets built and what this does we
have it integrated with we use stash but
any sort of get repo would work so we
have a build number and then a branch
and then the SHA from the commit not
just the abbreviated one but those are
hard to fit on slides so now I have this
node in the system what do I do with it
so we issue a request so the requests
are also in Eden so I would give this
blob of Eden to the tag of a service and
say go build me this I want the data
search model through to 2015
11:16 it's today so what this is going
to do is find that path line it up and
then save foo is my model type that
dates my date can also specify you know
email me when it's done
generally useful thing to know when
something's done building I can also
tell it different things about how I
want this to be built
so I can say you know I want the the
code to come from the branch master so I
can say for this address the code has to
come from the branch master I don't want
somebody's feature branch getting in my
way or or anything like that I can also
say you know for all the play data that
we use use build 42 exactly exactly that
one so you can specify different things
about the ancestor data as well that you
know that this will come into play when
building this data so now let's dig into
the node definition a little bit deeper
there's more stuff we can do than just
just running a single thing so we can
add secondary outputs these are useful
for you know my main thing that I'm
building is a search model but along the
way I might have records that I say oh
yeah this this doesn't look good let's
throw this off to the side and deal with
it later so I can kind of have these
side outputs if you will and what this
is doing is it's defining another
address for the side output so I could
have another node that's hooked up to do
validation on that and say you know if
this error data that I wrote out becomes
too big yeah halt the whole process stop
everything you know we don't want to
continue there can add virtual
parameters into the output and this is
saying something like I have a model I'm
going to build it for every country out
there but the outputs partitioned by
country so if you know a specific
country ID you can ask for it and that
becomes an addressable piece of data so
I might want to have this node produce a
model for all countries in the world but
I only have validation that runs in the
US so give me the u.s. chunk of that
data so that I can run validation on
that you can add in different kind of
default parameters so these are almost
as if I had them in the address it's
just another parameter that I can pass
in and say here's I want this threshold
the number of days in history to use but
they have default values if the
requesting user wants to override these
they could ask for a threshold of 0.9
five or whatever other value
want and run a different thing and that
would be a different address but by
default it comes baked in the last thing
in the output is a match type and this
is kind of where we get a sort of
branching sort of in here so you can
have two nodes that have the same
logical address say they both build
different models but this one builds
models foo and bar and this one builds
model baths so you can have two
different sets of codes and dagga is
gonna gonna do that dispatch based on
what's asked for if you ask for this
model type it'll give you this note if
it gives you the other one give you at
the other and this is one of the
greatest things in here we don't have
another DSL for how to define your
dependencies it's just closure so you
can drop in whatever arbitrary piece of
closure code you want you can import
stuff you can require stuff and compute
what you want as your dependencies and
this works when the node is requested so
when a user comes in and says build me
this model for this day we pass those
parameters into this and at that point
we figure out what the dependencies are
and you can have all sorts of
expressions to build huge cool branchy
graphs that look really nice and terse
in here so I have my node definition now
now what do I do with it so say I have
some code these are files here so next
to my code I put the node definitions
just an Eden file that sits there you
can have many of the different even or
nodes Eden files if you want to break up
your node definitions to kind of go
along with the different pieces of code
that you have but at the root of your
project you should have a dagger but not
eaten file that's just going to tell it
where all the nodes are what artifacts
need to be built like do I need to build
a jar do I need to build a docker image
and say for starters we have this on our
laptop you know it's a laptop because it
says laptop
otherwise you might not know that that's
a laptop so the laptop builds the builds
your code it builds these artifacts and
we have tooling around this and it's
going to shunt them off to some safe
storage for these things be at s3 for
jars or we'll take a docker image and
throw it up in the docker registry and
then from our laptop we're going to say
hey d'Agata I have these new notes and
it's going to give it the node
definitions and those know definitions
will refer to where those artifacts live
at this point dagwon knows everything it
needs to to actually run these these
jobs and execute these queries so you
could ask it and say you know go build
me one of these nodes dag of is going to
farm the actual work off to other
systems to run a pigpen job or run the
stocker image on mesas cluster somewhere
under the hood dagger b is storing all
of this information in de Tomic we use
cassandra under that I'll talk more
about this later it's just interesting
that that's where we put it and because
you're you're probably a sane person you
don't keep all of your code only on your
laptop I actually put it in to get
somewhere you know have some sort of
repo and so now we've checked in our
code we check in these nodes in the the
dag Avedon file along with our code so
now our code has all this information we
can set up an automated build job so we
use Jenkins to say you know anytime I
change something and get go ahead and
build this push the artifacts out there
and tell dag above where it is so this
means I can create a feature branch and
make commits to it and every time I make
a commit it triggers a job that builds
everything puts the artifacts in the
right place tells dag of where it is and
then I can just from my laptop say hey
go run this job you can even use this
for long-lived branches so we have some
a be tests that we're running where each
a/b test is just a different branch of
master and you can really manage stuff
like that nicely so what are some of the
cool things that come out of modeling
the world this way oops say I have a
note here
in these next couple of slides I'm going
to gloss over the way that we use a
request to actually trigger an ode it's
just going to say the little blue box is
a request in the way of getting to the
note so here we're going to issue a
request that it that triggers this node
and that node triggers some other nodes
and so we have our graph built up so say
we issue this request and this is what
we deem that we need to build and then
we go through and we build the data
again D for data and then so we go down
to the next stage and we start building
some more data and oops that one failed
yeah so at this point a lot of
traditional workflow systems you have to
start all over but we don't we have a
check point of view we know we built
this state already so when we show you
the request again it just goes up to the
parts that we need it finds the existing
data it can continue to build it happily
just building the last two stages and
once we're done all of this lives
forever in Dagobah we have knowledge of
that data so this allows us to do some
other cool things like structural
sharing so say I have this node and I'm
going to issue a request for it and
that's going to go out and builds a
bigger and bigger graph and so it starts
building data then lo and behold
somebody comes in and asks for node one
so do I have to start building it over
no of course not it's the same thing so
we can use that existing in-flight
request and attach to that in a sense
and so when this is built then node one
is satisfied we can tell that user your
data has been produced everything it's
good in the world and continue on moving
down the chain to build node zero so now
we have all of that in the system so now
we can add in a new node say I want to
do something different but kind of
similar so I add a new node seven I
issue a request for it that goes up here
I'm using the same inputs as node zero I
just need node one and two again lo and
behold I have that data so I can just
use that again
and now I can use that and just
immediately compute node seven so now
we've shared the data from the earlier
computations so now where it gets really
cool is say I found a bug in node six
and I need to fix it and I have a
version two of node six but I want to
see you know what's the difference after
my bug fix right so I had this new node
to the graph you can see there's V two
up at the top of there and now I can
issue a Request and say give me nodes
ero but build it with node 6v to alter
my dependency up in the graph so that's
going to go and issue the requests and
on one side it's going to find data
because node one has nothing to do with
node six so that satisfies the request
because there's no restriction on the
date of it no to the data it was built
there was using node six V one so that's
not going to work so we go up the graph
do the same thing node five works then
we go off to node six v2 issue that
request we build that data and now we
can build a different version of node
two using a different input and this is
an important distinction here is that
dagger one knows the difference between
these two different versions of node to
it they're both using the same from node
two code they both have similarly named
inputs but they're they're different
actual addresses within the system we
can distinguish between these two things
and reason about them differently so
then similarly we come down we can do
the same thing for node 0 it's using
this new version of node 2 and now we
satisfied the request now we have two
pieces of data so if I had another node
that could take two copies of node 0 and
compare them we could attach that to the
bottom of this and the data would be
there ready and waiting for it and so
now basically what we've built is is a
big persistent data structure that's
just lazily evaluated and you know as we
ask it to do stuff it's going to build
stuff and keep it there forever and
remember where it is and the memory is
important because say I have a big
complex graph you know I can't even tell
where these lines go anymore and so I
have this note
xv1 and we're like now that was really
bad I need to just cut it out of the
system and excise anything that came
from it you know fruit of the poisoned
tree so let's let's kill node 61 but
then we can ripple that change down
through and say all right anything that
was built from that has to go to
anything that come at it that marked
it's bad and similarly so forth you you
work your way down and that way if
somebody comes along looking for one of
these other pieces of data what it's
going to do is now rebuild it and it'll
rebuild the whole chain that's required
but using version two of node six so you
get the correct data there
so persistent data structures are great
but we live in the real world so
sometimes you know stuff goes wrong but
you want to say okay you know just do it
anyway you know fudge it so we have a
concept called gates and this would sit
kind of next to a node or has nodes as
inputs and outputs so we have a gate so
we have an input comes from another
another node or data address you know
and the idea with the gate is you have
some validation that would run on the
output of the node and say you know this
looks okay or it doesn't look okay and
if it doesn't look okay we kick it out
to the user we send you an email and say
hey this is this looks bad but it's your
call
and then the user gonna respond and say
yeah I think that's okay go ahead and
allow it through or no stop it if you
know you just it kills the whole process
it says yes it basically just copies the
input through to the output I have a
concept of a fallback
so this says you know if today's model
looks bad then just immediately go to
yesterday's model if I say no
because I still want to have data
therefore the graph purposes another
thing that we're working on haven't
built quite yet is thing we're calling
actions here we have a simpler node has
a model and then a date that goes along
with it so say I request the model for
today and then I request the model for
tomorrow and then the next day and these
things are building up because say
there's a data is not available yet for
the 16th and then the seventeenth relies
on that - in the 18th they all rely in
the last couple of days of history so
that you get these requests stacking up
you know and dagger was just going to
patiently wait until the data is
available and then start building them
so then you're running this problem say
the data all shows up at once it starts
building all of these things that's
great you know you get the concurrency
but in the real world we might have a
cache that we need to update and
actually take these things and put them
into a cache and change them mutable
stored at the bottom of our graph so the
the idea of an action is to serialize
these and make sure that they go in
order so we get xvi goes to the action
since yeah alright we finally have data
on let's put in the cache
go ahead the 18th this is okay that's
greater than the 16th let's put it in
the cache and the 17th comes along the
17th is less than the 18th
it says no can't do that and it blocks
it the way that we do this is we just
define a comparator over the different
model parameters so this is something
the user would specify 19th comes along
we can publish that and all its get in
the world this also enforces that only
one of these is happening at once so
even if the 17th comes in slightly
before the 18th we don't want them both
writing the cache at the same time so
under the hood I mentioned earlier
wheezed atomic you don't need to
understand anything on this slide it was
just more of a cool graphic than
anything but the way that we modeled
this is kind of roughly analogous to
like a Ricci algorithm so we have the
information that dagger burn knows about
the world and then you have different
user inputs the user input might be add
this node add this request cancel this
request or update the status of this
then each of those is translated into de
Tomic datums so I take a note I
translate it into the the atomic
representation of that it might be a
status update as like use an entity
reference and then change the status
yeah and then I take my existing data
and I say all right add this new
transaction data and give me a new what
if database and then I have a bunch of
these rules that say alright if this in
this then that so an example of these
rules we call them elaboration rules
might be if I have a request and I have
a node and it doesn't have a job running
go ahead and create the job if there's a
job with no active attempts then go
ahead and create the attempt English one
of those would be a decoupled rule so
what we do is we just recursively apply
and apply and apply all of these rules
until they give us nothing more until
we've bottomed out in the string then we
have this big set of datums and say
alright take this whole thing commit the
whole thing and then we run some queries
at the end and say all right based on
all that we know now about the world
what jobs do we need to start what users
do we need to notify that the jobs are
done etc etc you can kind of see the
different the
different state transition diagrams so
like if I add data then I need to do
this rule and after that I can do this
query so that limits the number of
different things that we need to
evaluate on each pass makes it more
efficient there are things that I love
about des Tomic like it's it's awesome
that we can express each of these rules
as a function that takes a database
value and does a query on that in the
end the function and then returns the
transaction data so it's awesome that we
can write queries as Edin unfortunately
we can't store the data as eating you
have to manually serialize that so there
are still a couple of rough edges around
it one thing that we love is that you
can do the go forward in time things so
you can say I have this database now
what if this and gives me a new database
if I screwed up something last week I
can say you know go back in time and
what was the the database value at that
time unfortunately they don't work
together so you can't go back in time
and then go forward in time but I'm sure
they're working on all these things all
in all it works great for us because
we're modeling a persistent graph
structure and a data storage that is a
persistent graph structure so if they
play well together one of the nice
things to about this is because after
everything that we do we store it back
to des Tomic you can literally unplug
d'Agata and walk away for days and then
plug it back in and it'll resume running
exactly the same jobs and the same
workflows there's no state in the actual
database service whatsoever so we do
actually use this in production at
Netflix not for all of Netflix yet but
we do use it for a lot of things right
now we're processing about 200 user
requests every day so it sounds small
but this is actually 200 data pipelines
that it's running every day and so these
data pipelines might spend hours or days
or long times in the end it ends up
running about a thousand jobs every day
we have 13,000 no def
missions with about 200 unique addresses
in the system and what this means is
that people iterate they iterate a lot
so you have the same note and then all
my different copies of it while I was
trying stuff out and all my different
copies of it can share data with the
existing production system and as a
result of this right now we have about
60,000 pieces of data that it knows in
the system we've been running it maybe
six months at this rate so it's it
secreting very quickly but it seems to
be holding up I'm sure someone will ask
are we going to open-source this yes we
do have plans to open source this make
it public when it's done yeah that's
what I've got we are hiring if you're
interested in working at Netflix and
building cool stuff like this</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>