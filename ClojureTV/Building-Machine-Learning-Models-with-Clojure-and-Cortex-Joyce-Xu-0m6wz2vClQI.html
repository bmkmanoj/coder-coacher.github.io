<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Machine Learning Models with Clojure and Cortex - Joyce Xu | Coder Coacher - Coaching Coders</title><meta content="Building Machine Learning Models with Clojure and Cortex - Joyce Xu - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Machine Learning Models with Clojure and Cortex - Joyce Xu</b></h2><h5 class="post__date">2017-10-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0m6wz2vClQI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so I'm gonna get started thank
you everyone for showing up this
afternoon on this half of the
presentations going on
looks like machine learning is enough of
a buzzword that even for as rich Hickey
put it this morning cranky old
programmers it's enough to draw you in
which is really great for me I came from
a Python background and machine learning
actually so I've worked in Python and
machine learning for two years before
moving over to closure to do machine
learning and just based on my own
experiences there's a pretty large area
of intersection between functional
programming and pah machine learning
that's been left largely unexplored and
this intersection is particularly
interesting between machine learning and
closure which is why I want to give this
talk on building machine learning models
with closure and cortex which is a
machine learning library built for
closure so this talk is largely going to
be split into three parts we're going to
start off by talking about what machine
learning is other than the buzzword that
got you all into this room we're gonna
talk about why we want to use closure
specifically and why I said that the
intersection between machine learning
and closure is especially promising
moving forward and finally we're going
to talk about how to use cortex which is
the library that we can use to do
machine learning in closure and at the
very end as long as we have time I want
to walk through how to build this baby
classification model in cortex that can
hopefully show off how you can perform
machine learning in practice and also
why some closure techniques fit fit in
so neatly with machine learning in
practice so to get started I want to
talk about machine learning some people
in this room may already know a lot
about machine learning and know exactly
what it is for the rest of you you've
probably heard a lot about machine
learning and what it does but not too
many details about how it works in
practice so you probably know that it's
what powers self-driving cars you know
it powers Google search you know what
powers Amazon recommendation engines you
know it powers Apple Siri you know it
powers all of bc5
in Silicon Valley and if any of you are
interested in any of those particularly
the latter you've probably gone on
Google and searched around for what
machine learning is which means you've
probably landed on this page which is
perfectly legitimate it's my go-to site
as well so everything I'm that I'm going
to talk about with machine learning and
neural nets I'm gonna draw from this
page so the first thing we want to do is
establish a little bit of context around
what machine learning is and according
to Wikipedia it's an application of
artificial intelligence so that gets us
to our first point the current
definition of artificial intelligence is
pretty broad there's a large scope of
what can and cannot count us artificial
intelligence but probably the most
useful definition for us is the
challenge of trying to get computers to
mimic human behavior and carry out tasks
that are traditionally associated with
human level intelligence or cognition
and this can mean a wide variety of
things but before the advance of machine
learning techniques and before machine
learning started taking over the world
most of artificial intelligence took
place in the form of expert systems so
this happened throughout the 80s and 90s
even a lot of the 2000s and how expert
systems achieved artificial intelligence
was by essentially teaching computers
how to think like humans we would
explicitly program in details of how the
computer was supposed to reach its
conclusions by teaching it as if we had
expert knowledge in the field and we
were giving the expert system the same
knowledge so if I was working at a
quantitative trading firm I might have a
rule that says ok if Apple shares ever
drop down to $150 let's sell a thousand
shares so if we combine a bunch of these
rules that human experts have come up
with domain experts have come up with
and hard-coded into the system we can
achieve the system that can mimic human
behavior pretty well and we can achieve
a system that can mimic intelligence
pretty well and this has been the
dominant paradigm ever saw up until the
2010s I'd say there are only a couple
problems with the idea of X
systems or rule-based systems the first
one is that in a lot of cases humans
don't actually really know how to think
like stock markets are a perfect example
of this like we want to achieve
intelligence here but humans don't
actually know exactly how to reach it so
we can't hard code the instructions in
the second problem is that sometimes
humans know humans know the result but
we can't explain how we've achieved the
results and this becomes particularly
apparent in the fields of natural
language processing and computer vision
where we can very recognize a cat but we
can't explain to each other why we can
recognize a cat so if that's a situation
that's going on it's very difficult for
us to hard-code this into an expert
system we can't have a set of rules that
teach the computer exactly how and when
to recognize something as a cat or a dog
or a human face in a conference room so
when we can't teach the computer how we
have to let the computer come to the
conclusion itself so in machine learning
instead of programming in the
instructions for how a computer reaches
the results we just feed the computer
the results we want it to see so we say
that we feed the computer images of cats
and tell it that it's a cat or we tell
the image we tell the computer that on
some day we Apple shares drop to $150
and we sold a thousand of them and this
is what happens and we let the computer
itself come up with the reasoning
process come up with the how you reach
the conclusion so that's a contrast
between expert systems and machine
learning machine learning takes in data
and from that data it learns an
association from them within this fear
of machine learning we have things that
fall under deep learning and things that
fall under not deep learning of not deep
learning are things you probably haven't
heard of as much you have support vector
machines extra boost has become fairly
popular recently that's a gradient
boosting machine you have things like
random forests logistic regression naive
Bayes a bunch of statistical models that
work really well in practice and in many
cases aren't actually irrelevant now
with the advance of deep learning there
are many use cases for all of these for
which deep learning is an overkill for
improv
not the best choice for but these
accomplished essentially those same
things through the same techniques for
all of these models in machine learning
you're feeding and data and it's at
least in supervised machine learning
you're getting a prediction or a Brazil
out of them deep learning is a specific
sub fear of machine learning that has
been achieving a lot of the most
advanced results that we've seen so far
in artificial intelligence and that's
what we're going to be focusing on today
and deep learning just consists of these
things called neural nets which are very
very very loosely based on human brains
that's how they were initially inspired
nowadays they've diverged pretty far
from the analogy of a human brain but
that's still one way we can understand
what neural nets do but before we get
into that now that we've established the
context for machine learning I want to
talk about a little bit about what it
looks like in practice so Wikipedia
tells us that machine learning involves
a bunch of algorithms that can learn
from and make predictions on data which
means that in practice when you fit this
into your software space or your
technology stack this happens in two
distinct steps learning stuff and the
predicting step in practice the learning
stuff looks something like this you have
an algorithm that you've written up or
that some library has written up and you
feed in a set of data and labels so
let's say we're trying to predict
housing prices in Baltimore we can have
the label be at the price of the house
the final price of how where it's sold
and the data can consist of a bunch of
information about the house the square
footage the location how many bedrooms
how many bathrooms etc etc and you know
all of these all of this information
could potentially be useful but you may
not know exactly how it relates together
to get you your final house price but
you can feed these in separately to the
algorithm which can then generate a
model through what right now to us is in
black box black box magic but you get a
model out the other end that has learned
to associate the data with the labels so
in the next prediction step you can use
this model that the algorithm has
generated to feed a new unseen data so
say a new house rolls out onto the
market we can feed it into the model in
the
can make a fairly accurate prediction on
what the final housing prices and that's
because through this algorithm we've
learned an association or a mapping
between the data and the label the data
and what we're trying to predict and
when it comes back to machine learning
and neural nets in particular it's most
helpful for us to think about this
mapping as a function so think about the
housing price as a function of the data
that we've been passed in if we think
about it in this light then the entire
goal of machine learning is to learn the
function learn the function f that map's
whatever input data you have to the
prediction that you want to generate now
the reason your own nets are so powerful
is because there's this thing called the
universal function approximation theorem
which essentially says that a neural net
with a single hidden layer and we'll get
into what that means it just a little
bit theoretically has the power to
approximate any arbitrary function there
is in the real space which means it can
approximate something super simple just
a one-to-one mapping between x and y a
linear level just a single line it can
approximate more complicated
relationships between these two
variables and it can approximate
functions of any dimensional space you
want so this is three dimensions I can't
display a four dimensional image up
there because it doesn't exist as an
image but it can do four dimensions five
dimensions and in machine learning and
with deep learning in particular it
tends to get up to hundreds of
dimensions or thousands of dimensions
and the idea is that in theory these
neural nets can approximate any function
mapping between however many dimensions
of input data you have and the output
that you're trying to predict given
enough examples and given enough of a
training data set to learn from your
neural net can make accurate predictions
for anything in the future and the way
it does this and I don't want to get too
deep into the weeds here but the way
neural nets make these predictions is
through a series of multiplications
additions and nonlinear transformations
so this is a perceptron it's a single
unit of a neural net and the output is
generated after passing a set of inputs
so we have three inputs here X 1 X 2 and
X
you multiply each one by some weight
value you add them all together and you
pass it through a nonlinear what we call
activation function in this case sigmoid
which just squashes the number onto a
scale of 0 to 1 essentially and that's
how you get your output Y in a machine
learning and in neural nets the whole
goal of learning is to learn the proper
way parameters here so w1 w2 and w3 in
other words what values do we multiply
with the square footage of a house or
the number of bathrooms or the location
what values multiplied by all of these
and added together and pass through
another activation function you can get
me the correct prediction at the very
end in this case it looks fairly simple
because there's only three valleys to
predict in neural nets as you size up
because this is just one unit you can
get a whole combination of all of these
this is still a very small neural net it
has three inputs in a single hidden
layer and you can see that with each
node in the neural net it's the same
perceptron as we've seen earlier it has
a bunch of inputs coming in and one
output going out it just happens to be
fully connected among every node to
every other layer and we do the same
exact process of transformation in all
of these except we learn maybe a
different set of weights that are
associated with each of these nodes so
once we have this this is our basic
outline of what a neural net is it's
supposed to model the neurons in our
brains and the way our thoughts flow
through our brains to my very very
limited understanding of neuroscience
this isn't accurate whatsoever but it's
a very useful analogy for how computers
quote-unquote think so this is machine
learning deep learning is a fat neural
net basically once you have more than
one hidden layer in a neural net it
becomes a deep neural net and then you
have quote-unquote deep learning so you
can add an arbitrary number of hidden
layers in between and you can also get
different types of layers in between so
these are all fully connected linear
layers you can have things like
convolutional layers which are useful
for image processing you can have
recurrent neural Nets which are useful
for natural language processing
but they all have the same general
structure you have a series of nodes and
input values that flow through these
nodes and a series of weights that are
associated with each node that you learn
in order to make your prediction so
that's what machine learning is you have
a bunch of data and you want to teach
the computer how to reach a conclusion
from that data and the computer is
responsible for learning that itself
through this process so I now want to
talk a little bit about why we are
introducing closure into the
conversation of machine learning at
closure cons it's a pretty simple reason
you guys are all closure is this is like
the only way I can convince you to do
machine learning outside of closure this
is definitely not the case I'm sure none
of us are fans of reinventing the wheel
and it happens that machine learnings
wheel has been invented many many times
over outside of closure we already have
a suite of honestly very good libraries
that do machine learning in a variety of
other languages so tensorflow is written
by and powered by and used by Google I
believe torch is used by face off
Facebook and a bunch of these other
libraries are being used and supported
by a wide variety of other researchers
and industries out there the vast
majority of these are written and used
in Python we have a couple we have one
in Java and some of them support C++
none of them work in closure but this
does serve over 99% I'd estimate of the
machine learning community and if these
already work and have such good support
why do we want to use closure turns out
there's a decent intersection between
functional programming and machine
learning that people haven't really
thought about too much and the first
aspect of this is idea of functional
function composition which plays out
very well in both theory and in practice
so in theory if we look at neural nets
again and we look at the perceptron
which I mentioned before as a function
transformation between the three
dimensions of the input to your single
output Y and we call this F 1 then we
when we string these together into a
neural net well we actually have is a
series of these transformations where we
pass in our in
to our first set of notes which perform
one function on it then we take the
output of that and pass it forward as
the input to the second hidden layer and
that performs a second transformation on
it so when we string these together it's
a composition of functions from input to
output
it's a pipeline and if you're missing
the closure syntax a little bit you can
think of this very informally as a
threading procedure you can think of the
prediction you make at the end as the
output of this function composition
where you passed in the input to however
many other layers and however many other
function transformations you want and
you can view machine learning especially
deep learning as a whole in this case as
the process of optimizing the set of
composed functions so we have a bunch of
hidden variables hidden parameters that
we're searching for and we're looking
for the optimal set of weights and
parameters that would give us the
transformation and give us the output
value that we are looking for in the
data so in theory neural nets match up
very well with function composition
which is a core principle of functional
programming in practice the entire
machine learning pipeline now regardless
of where whether or not you're using
neural nets at all is also performed in
many cases as a set of composed
functions because when it comes to
machine learning in practice neural nets
and whatever algorithm you're using is
only a very small part of it you end up
having to load the model configuration
and load the data and do a whole bunch
of transformations on the data you have
to clean the data if there are any
inconsistencies or missing values you
have to extract features if your raw
values aren't good enough you have to
shuffle the data around you possibly
have to augment the data you have to do
a whole series of transformations to get
the data into a form that's appropriate
for your machine learning model to learn
from and when you have this pipeline of
transformations what you have as a
pipeline of functions that you're
composing together to get your final
output of the data and this data is also
passed into the algorithm which is then
processed in a feedback loop which again
is a pipeline that goes back and forth
so you can view this entire process
again as a series of composed functions
where you're fetching the data
wherever you're storing the data first
and then you're doing whatever
transformations necessary in this case
cleaning it encoding it into some other
form augmenting it and then running the
model and evaluating the results all of
this is one pipeline in one set of
composed functions even outside of
closure it is most clean and elegant and
effective to write this pipeline in a
functional manner so if we have both our
neural net being shaped by function
composition and also our machine
learning pipeline shaped by function
composition and it makes sense to do
machine learning in practice
functionally now we want to talk about
why closure specifically is so good for
machine learning so so far I've been
talking about why functional programming
in general is useful for machine
learning and why there's an intersection
there closure comes in when it comes to
data processing and I think rich Hickey
talked about this in the morning very
effectively he talked about how in
systems engineering or any type of
software engineering you have a core bit
of data or logic and a whole bunch of
information processing surrounding it
and it's the same thing with machine
learning where in fact the most
time-consuming part of the process isn't
the building the model building the
algorithm itself but actually the ETL or
extraction transfer transformation and
loading of the data so the data
processing it takes the most time is the
most route work involved with machine
learning and the in terms of the process
so closure shines when it comes to a
data processing one example of this is
the fact that it provides things like
lazy sequences machine learning models
tend to because they perform iteratively
they expect the data to come in in batch
form you have batches of data that
update the machine learning model one
step at a time and then you need to
repeat the entire data set many many
times over to get a model that can
stable out and reach an equilibrium in
terms of how well it performs so closure
by providing things like lazy sequences
you can very effectively augment data
sets to become effectively infinite
without having to load all of it into
the memory and realize it all at once
so this is an example of some code
that's rhibs straight out of the cortex
library actually and I just want to
display a
because it's so simple and
straightforward you're taking in a
finite data set which is stored as a
sequence of maps so that's the map seek
and then you're performing a series of
function transformations over it you're
shuffling the data you're repeating at
an infinite amount of times you're
concatenating everything together and
you partition it into whatever size the
model expects it as and through this you
get a lazy sequence that's effectively
infinite of the data set that you're
expecting so the model can process it
and learn from it as many times as it
needs so that's one thing closure as
lazy sequences closure also has really
powerful built-in data structures and
I'm not going to talk too much about
this I'll talk a little more about this
when we get to the specifics of cortex
and how it's been implemented but
essentially we can do everything with
the built-in data structures that end up
giving us neural nets that are highly
transparent and highly customizable when
we want to work with them so because
closure is so good at the information
processing part and because closure was
specifically designed to handle that
problem of having so much information
processing to do in any type of software
engineering when it comes to machine
learning and the fact that the most
time-consuming part of machine learning
is in data processing closure is a very
good tool in a very good way of carrying
all of this out so I don't personally
believe any tech talk is complete
without a meme I don't know about all of
you because according to rich Hickey I'm
in a room with a bunch of cranky old
programmers so I don't know if you'll
appreciate this humor but this is how it
actually happens in practice this is
deep learning and practice where a lot
of people glorify it and think it's a
lot more than it actually is
but in practice especially if you're
using Python you are from piano
importing everything so all of your
functionality everything I talked about
for how you build neural Nets that's
already built in to some library for you
and you're just calling it and using it
fortunately for us that's a fairly good
model because as end-users we don't
actually have to deal with the
nitty-gritty parts of anything and
that's why for us if we want to work in
closure we don't import anything from
piano
we import it straight from cortex so
cortex is an open source machine
learning library it was it's been around
for maybe one or two years possibly more
it's been developed by my old company
think topic and I had the opportunity to
work a little bit on cortex over the
summer and see why it works and over the
first couple of weeks I was working with
it I'll be honest I was still very
skeptical of why it existed in the first
place like I said I don't like
reinventing the wheel I thought this was
reinventing the wheel I wasn't in love
with closure yet like all of you are so
I was learning a new language and trying
to be convinced that it was necessary
for machine learning but because of all
of the things I talked about earlier
where I learned about how good closure
is with data processing and how good the
intersection between functional
programming and machine learning is in
general I got a lot more interested in
the development of cortex and there are
a couple interesting parts that make us
stand apart from a lot of the other
libraries so at its core it's an open
source machine learning toolkit you can
use it in the same way the meme used
piano you can just import it and use a
lot of its functionalities to build
machine learning algorithms in very
little code it's built on top of a
back-end that consists of a compute
library so the compute library is a
generalized framework that's designed to
run to build and run algorithms
efficiently on both the CPU and the GPU
it happens that machine learning trains
machine learning algorithms train a lot
faster on the GPU so we actually have a
GPU compute library that contains the
cuda implementations of all of these
compute out of thems and implements all
of that so you don't have to worry about
it so I'm not going to go too deep into
the details of a compute library just
because you will almost never interact
with it the really nice thing about the
compute is it abstracts away almost all
of the nitty-gritty low-level
computation related details of the
operations are performing away from you
and that leaves us with the
functionality of cortex which is built
on top of the compute library and the
core design functionality of the core
design principle behind cortex was to
implement as much of the high-level
details
details about neural nets as possible in
pure closure and the reason why that's
so good is because you end up getting
neural nets the entirety of the neural
net and the entirety of the traversal
through the neural nets represented in
pure closure data structures such as
maps and this gives us a lot of
transparency a lot of control and a lot
of customization power into working with
the neural nets because in other
libraries if they're implementing this
in no lower-level C++ or other things
that are shielded away from us end-user
you can't go in and control the exact
details of the neural nets you can't go
in and see quite exactly what the values
of everything you're looking at at any
given step you're looking at that easily
you kind of just have to trust that the
algorithm works in cortex you can trust
that the algorithm works but you also
can very easily access all of this
information and see exactly what you're
building and I'll show you what I mean
in just a little bit the way cortex is
designed in terms of the neural net is
very similar to a lot of these other
libraries so for example tensorflow
which is arguably the largest machine
learning library out there right now
implements its neural nets in the form
of a directed acyclic graph so you have
your input coming in you're performing a
transformation you're performing a
matrix multiplication with it with the
weight made weight matrix and then
you're passing that forward you're doing
an ADD operation you're doing a
nonlinear function transformation so
forth and so forth
until you get your final output cortex
is designed the exact same way we call
all of the layer transformations nodes
we call the direction of the data flow
edges we the parameters that we're
holding and the flow of the data and all
the memory that we're allocating for
that data the buffers and then we call
all of our inputs and outputs the
streams so closure represents all of
these in the form of a map and if you
build a neural network enclosure and you
print it you're going to get something
that looks almost exactly like this you
have a compute graph so that's your
computation graph and within this map
you have four keys you have your nodes
your edges your buffers and your street
and one nice thing about cortex is for
every single element within any of these
four categories it goes in and
automatically associates a unique ID to
it so for your input under nodes you
automatically get the ID of input one
and you can see a map of all of its
properties so the dimension of the input
that's flowing in you automatically get
the first linear layer in all of its
properties and when you get to things
like buffers it automatically allocates
space for linear layers because it knows
that when you have a fully connected
layer of neurons there are weight
matrices associated with it
there are bias vectors associated with
it so you get not only the names of
these but also the random initialization
or as you go on in time the not so
random initialization of these
parameters and you can go in and see the
exact values that you're working with
so in cortex at any given step if you
need to debug your neural net or need to
learn more about it you can go in and
look at all of this you can figure out
what's going wrong or going right and
with these unique IDs you can very
easily update information and any part
of this you can very easily associate
new properties get rid of old properties
come up with your own specific loss
functions for example come up with
various little tweaks that you want to
introduce the neural net and cortex will
respond accordingly because this is
exactly what it works off of and you
don't get that type of low-level control
in any other library so if this is how
cortex neural nets look I want to now
walk through the basic steps of how you
would build up a neural net in cortex so
there are a couple basic steps to
building a model the first one is
loading and processing the data that's
what we were talking about earlier with
ETL this tends to be the most time
consuming step until you get into like
actually tuning the model and trying to
improve the model and then that's your
most time consuming step by far but the
first step is just loading and preparing
your data your second step in cortex is
defining a very minimal description of
what your neural net will look like and
that's just a description of what layers
you want in what
and your third step is to build a full
network from that minimal description
that's just a single function call and
training the model on the data that
you've loaded so with these three steps
you can actually train and evaluate your
own neural net on any data set of your
choosing as an example I chose a credit
card fraud data set which was
interesting to me for a couple reasons
one of them is that this is actually how
your credit card companies work and how
Amazon works and Walmart works they use
machine learning algorithms to evaluate
the purchase that you're making against
all of your other past purchases or past
usages to try to determine if this was
fraud or not in almost all the cases
it's not fraud but in the 0.1% of times
that it is fraud it tends to be that
it's a pretty big purchase and it tends
to be something that you want to catch
in a model so if we go back to the
diagram that we had at the beginning we
can think of the past users card usage
as the data that you're feeding in so
how often you use the card for example
how much did you usually buy how much
money do you spend how often do you shop
all these details about the user and
then we have a label so now we have a
data set of past purchases and confirmed
frauds or not fraud fraudulent instances
so you have your data and your labeled
there you pass it into some algorithm in
cortex and you end up generating a
neural net that has learned the proper
weights to associate with the data in
order to give you your proper prediction
then when you want to actually use it in
practice all you have to do is feed in
the card usage this model and it can
make a prediction on whether or not the
purchase that was just made was
fraudulent or not so first step of this
is data loading and pre-processing one
thing I want to emphasize is this is the
bare minimum of what you have to do like
I talked about it tends to be that in
real world machine learning it's never
this simple because there's a lot of
middle steps to actually transforming
and cleaning the data this kind of
assumes that the data has been handed to
you in the perfect form for a machine
learning model which
ever happens in real life but the nice
part is that like I talked about earlier
whatever transformations you have to do
to the model don't get too much more
complicated you just write up the
functions and you string them together
as some form of composition of the data
functions you pass the data through them
and you get the data out put it in the
form that you want for the neural net so
the key parts this minimal example you
want to identify obviously where your
resources so now we're assuming that the
credit card data is being held in some
CSV file somewhere with all of the
information that you need you would load
this file into memory of course and then
you would separate out the data from the
labels so this is one area where you're
assuming that the last column in your
data set so the drop last is the label
so when you're grabbing your data you're
grabbing everything but the label and
you're converting everything into a
number so the neural net can process it
for the label you're doing something
very similar you're grabbing only the
last column you're mapping everything
into a number and then you're calling a
cortex function that transforms this
class into a one hot vector a one hog
vector is something that is used to
represent the output of some
classification task essentially so when
you have three different possible
classes you take whatever class it is
and transform it into a
three-dimensional vector where each
column corresponds to the existence of
that class so if it's the first class
you get 1 0 0 if it's the last class you
get 0 0 1 and the only reason we do this
is because the model outputs predictions
in this way it outputs the probabilities
for every single class so when we
generate the labels we also want it in a
matching form and finally we associate
the data and the labels the
corresponding ones that we've actually
grabbed so cortex actually expects its
data sets in a form of sequence of maps
where each map corresponds to one
training instance in your data so for
example you can have data and a set of
numbers of each number corresponds to
some variable in your data set and you
come have a label of two classes so in
the first third and fourth example it's
one class in the second example it's the
other class
so through these steps here you can
generate a data set that looks something
like this and in this case you're done
with loading your data this is all
cortex expects from you with the data
when you want to build up your model
description it's also very
straightforward the first step is simply
writing a definition of the network as a
series of layers and this is the layers
of the neural net in the order that you
want them in so in this case we can see
we have an input we have a linear layer
which is just a fully connected layer
that uses the rectified linear unit
activation function we have a dropout we
have some more linear layers in the end
of maps to something that looks like
this where you have your data coming in
you have another layer corresponding to
the second blue column you see here and
so forth and so forth in a drop out
layer in the middle which is just
another transformation you can have that
essentially ignore certain parameters
during training to make the model more
robust and more generalizable in the
future so this is one particular
architecture of a neural network you can
train you can tweak the numbers here or
tweak the layers that you want to see
here to correspond to whatever neural
net that you're trying to build in
practice so creating the model itself is
just a series of definitions for the
layers that you want to see the final
step is training the model that you've
just described on the data that you've
just loaded so with every neural net
there are a set of hyper parameters
associated with it regarding say the
size of the batches that you want to
train on how many times you want to
Train it so forth and so forth so you
can specify all of these parameters and
then in your actual train function you
first build the full network from the
description that you just had so this
function transforms the very minimal
description we saw here into the
full-blown computational graph that we
had earlier and that was the graph that
contained the nodes the edges the
buffers and the streams so once you call
this function here and you print the
network you can actually see that output
and you're going to see the weight
matrices initialize with some random
numbers in it you can then split up your
train data set
you can split up your train dataset into
a train portion and a testing function a
testing portion and with your train data
set you can make it infinite so this is
a function that's almost identical to
the one I showed earlier that's just
infinite data set it just so happens
that because this is a classification
task you can also balance it by the
class so you can have an equal number of
positive examples as negative examples
and then you can call one single
function to do all of the rest of the
work for you so in the experiment
library we have a train on function
train network and you just pass in all
of these variables that you've created
earlier so the network the train data
set and the testing data set and you
pass in all of your hyper parameters and
this function will not only train your
neural net but it will actually evaluate
it on the testing set for you and print
out your output for how well the model
is doing at each step of the training so
in theory at this point you can actually
get up to a fully working model and you
can see exactly how well it's performing
on the data set that you're testing on
through these steps here the loading of
the data and the description of the
network and the training you've gotten
to a point where you have a fully
functioning neural net that you can
theoretically deploy in a practice in
production obviously there are a few
more steps that are involved if you want
to make this neural network well on the
particular credit card data set I was
using just these steps left me with a
very very very bad model so there are
some extra steps that are involved and
this gets more into a machine learning
theory and practice then we want to go
into today but you can do feature
selection so you can pick out features
that are more useful than others you can
tune the hyper parameters of the model
or just tune the model to use a
different architecture altogether and
you can do data augmentation and
generate artificial new data based on
the data you already have and train on
the data augmented data set but with the
combination of all of these set steps
you can get up to a very solid working
neural net model in this case with the
credit card data set we achieve fairly
good results with an f1 score of 0.8 for
that's pretty decent for the credit card
data set because it's extremely
unbalanced it has like 0.1 person
positive examples and this is the
process that you can use to train on any
arbitrary data set essentially so using
these combinations of steps and using
any of these additional model tuning
steps after building the initial model
you can get up to a working model and
that's how cortex works in general so if
we want to look at the big takeaways we
can see again that the neural nets and
machine learning pipelines are just a
composition of functions we can view
neural nets as an optimization of the
composed functions and we can view
machine learning as a pipeline of these
composed functions so we can see that
machine learning and functional
programming intersect together very well
and we can also see that closure is very
good at handling information processing
and data processing and for these
reasons closure is very good and very
applicable for machine learning and the
machine learning space closure on cortex
provides you with a very high-level
interface for designing and training
your neural nets which not only gives
you simple initialization processes but
also gives you a very high degree of
control over how to design your neural
net and control the exact outputs and
inputs and weight matrices inside which
gives you the full spectrum of using it
for research purposes to using it for
production purposes for using it to
learn machine learning in general so if
you're interested in machine learning at
all or if you're interested in closure
at all give cortex a try and if you have
any more specific questions on cortex
you can feel free to come talk to me
after the talk or we have some folks
that think topic here who I'm sure would
be very happy to talk to you as well
so thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>