<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Core.Async in Use - Timothy Baldridge | Coder Coacher - Coaching Coders</title><meta content="Core.Async in Use - Timothy Baldridge - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Core.Async in Use - Timothy Baldridge</b></h2><h5 class="post__date">2017-03-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/096pIlA3GDo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome as Alex mentioned I'm Tim
Baldrige I'm a consultant with cognate
ektu work on their consulting team and
today we're going to talk a little bit
about core async now I've titled this
talk quarry sink in use we could also
call it quarry sink from the trenches or
core a sink where do we go from here if
we wanted to start the day off with a
good pun but the fact is that claw
ratings actually been out for about
three and a half years about four years
ago we started work on it and in that
time it seemed rapid adoption by the
community I would I was going to stand
up and say it's the quickest adopted
language are a library of the closer
ecosystem and then spec came out and
that's not true anymore I think spec has
overtaken it but but still we've seen
rapid adoption by core racing as a
consultant in the closure space I seen a
lot of good core async code and a lot of
bad a colloquy resync oh and I want to
talk about that a little bit today some
patterns I've seen and some ways of
maybe getting rid of the bad patterns
there's been a lot of talks about core
async by people that are here today
about how to use core racing but not a
lot about when to use or perhaps when
not to you for library but I want to
make a point here first of all and that
is this is not a judgment call on core
async or any other library I believe
very firmly that core is the code is a
moral that is to say coding of itself is
not good or evil it's how it interacts
with people that may make it more
desirable or less desirable and that
libraries themselves are collection of
value propositions and we should
recognize and evaluate the trade-offs in
every library what this means is is that
a library of code promises to give you
some value in exchange for a trade-off
now that trade-off may be as simple as
you now have a new dependency that you
have to track right you have a new
dependency that someone may update and
you have to figure out if those updates
fit into your code or not but there's
some overhead now that's the lowest
level of trade-offs at a much higher
level you may run into situations where
a library asks you to adopt a new model
of programming a new
a turn a new way of thinking and you
have to evaluate if that cost is worth
the value that the library gives you and
that is going to depend on the project
and the task at hand so today we're
going to go through some analysis of
some styles of chorusing programming
I've seen in the wild and a lot of these
patterns you'll probably recognize I've
seen them on lots of client projects
that don't worry all the code I've
actually written myself so we can
contain anyone but then we're going to
discuss some of the pain points we see
of using core async in these in these
models and they're going to look at some
new patterns that you've probably heard
of maybe even seen but that help ease
the pain so here's our first example
this is a bit of code that takes data
from an input channel and it batches it
up into a different size and puts it on
an output channel right so we have an
input channel here we have a max size
we're going to read a small batch of
data from the input channel we're going
to concatenate accumulator and if the
new size is above our max size we're
going to output it on the output Channel
seems pretty simple I mean if you've
done Cori I think you've probably
written something like this but I've
found a lot of problems with this code
first of all we have I Oh everywhere now
this is something if you're using the
core async library you may not catch
right at first but these operations here
the put in the take RI oh they will
return a different value depending on
the state of your entire system it's
non-deterministic right and as a
functional programmer I don't like that
buried in the middle of the logic of my
program I want my functions to be pure
and I want to try to push that non
determinism to the edges of the
application second of all this may seem
really weird but the reason I originally
had to look at this code was because the
performance of concat degraded over time
and the reason for this is really weird
but it's essentially this if I take two
collections and I can catenate them
together
what can cat will do is build a third
collection on the top that first gives
you the items in the first collection
and in the second so this looks
completely fine at first but in
production the problem we
is that this max size was a thousand
items and the batches we were getting in
were one or two items of piece so over
time we would build a stack of caen cat
sequences that was 500 elements deep and
then when you try to get an element you
had to go up and down that entire stack
of 500 items and since this was running
enclosure scripts the browser basically
crawled will halt
another problem is is that count on that
sequence is o n time so it has to walk
each item in the sequence and one at a
time and that that was also killing
performance and I put here a note that I
came to closure to avoid thinking about
this so you may be tempted at this point
to say whoa wait a minute
sometimes count is o and sometimes it's
a 1 what's wrong here that's not the
point
the point I'm making is that I am forced
because of the way this function is
written to think in an imperative style
and I have to keep all the stuff in my
mind about how can cat works how count
works and all the sort of things I'm
working at such a low level that I have
to keep too much in my head and it's
easy to make these sort of mistakes I
want I came to closure in an attempt to
find a language that was more
declarative that allowed me to look at
code at a higher level so I didn't have
to worry about the specifics of how to
rebadge items that seems like a pretty
simple thing how much we batch these
items in a different size there's got to
be a better way to do that another
problem with this code is that because
I'm constructing this channel here at
the top there is no way to pass
additional arguments to the channel
there's no way to give it a different
sized buffer or any other parameters I
am telling the user what sort of output
channel they are going to get and they
did have to live with it or modify this
code another problem is there's no error
handling here at all this may work fine
for days until someone decides to put an
integer in this input Channel and then
the whole thing falls apart and I may
get an error somewhere on a console but
you know maybe I only catch that into
log and the whole thing just stops
working and I got to figure out why so
it'd be great if we had a better way to
do error handling now there's one last
long
this code as it is written here will
work kind of it'll compile it will read
some data but it won't ever actually
output data and can anyone catch the
really weird flaw that I left in here
because when I wrote this it was here
and I didn't catch it slows proofreading
it later anyone see the last problem
yeah exactly I'm creating an output
channel and I'm not actually doing
anything with it once again I am at such
a low level here that I have to remember
all of these things now I said it will
compile and it will it'll actually try
to run because this go block returns a
channel so you know some funny side note
here type checking ain't going to help
you here because I said this thing
returns a channel it returns the channel
it just all works I there are probably
some types checkers that could catch
this but they it would take some work
okay yeah as I mentioned I didn't turn
the return the right channel so some
thought this code is highly imperative
and side effect we have side effects
anywhere there's barely anything
functional about it and I keep thinking
why do I have to think it's such a low
level isn't there something that's more
declarative we had there is closure
recently of year or so ago added
transducers and they allow us to think
in this higher level abstraction so this
that entire block of code we looked at
before can basically be reduced down to
this we take a bunch of chunks of data
and we can cap them together so now we
have a stream of elements and then we
repartition them in the correct size now
the semantics are slightly different but
we'll gloss over that today we could we
could write code in with transducers
that would do it the same exactly the
same way as the other code but we see
here that we've kind of removed all the
whole thought about looping we have to
think about looping anymore we don't
have to think about the proper way to
count or to repartition data in fact
what's interesting about this is that if
in the future partition all was
redesigned by Alex and the rest of the
closure team to be much faster we would
just get that benefit on
matically and that's what I was looking
for when I came to closure I was looking
for this ability to think it's such a
high level that somebody else writing a
library or writing the closure compiler
libraries could optimize them and this
actually does happen it happened I
remember when transients were introduced
a lot of these functions like that set
in - were redesigned to use transients
and people reported on the mailing let's
say my code got 50% faster or even
faster than that and all they did was
update the closure runtime so if I am
dealing with such a low level of loops
and an imperative programming it doesn't
give as much of an opportunity to get
those sort of benefits so the other so
what i'm doing here is i'm building a
transducer and i'm piping from the input
to the output
and so when Corey's think we have these
pipeline functions which I do recommend
if you're doing Cori synced code at all
I recommend and you'll understand
transducers I recommend stopping what
you're doing and looking at transducers
because they really do help a lot and
it's a pretty low entry point to just
you know kind of understand how to use
them and start using them with with
pipeline and the like so it great we can
use pipeline to some variants of
pipeline for different situations that
work great we can also collapse this
even further and just put the transducer
right on the channel itself and there's
there's nothing wrong with that approach
heater but why put it in the channel and
we don't always need to use a some
channel so we could just make something
that makes a batching transducer and
we've compressed all that code we saw
before - just two lines so that's great
the second type of code that I see a lot
of is this so we have an asynchronous
function called get value it takes a key
and a callback right this is this is
normal JavaScript some stuff right yet
set timeout that sort of thing you give
it a callback and so we say you know
what it'd be nice to use core async with
this let's wrap it so what we're going
to do is we're going to create this
function called value for we create a
channel and our callback just puts the
value onto
channel simply the problem is because
this returns a channel if we want to sum
the values in that we have a bunch of
values we want to get a Grunch of keys
and we want to get the values when we
want to sum those we end up having to
loop and this thing itself now also is
asynchronous so we wrap that in a go
block and then finally down at the end
we pass a bunch of values in and
actually I have a typo there but will
disregard that for now
we passed a bunch of values in and we
get the result now what are the problems
with this example well first of all and
most VMs async code pollutes the return
type and this is true in almost every
language you will encounter c-sharp
Python JavaScript closure they all do it
one form or another some lengths some
languages like C sharp will throw it in
your face a little bit more if you have
an asynchronous operation your function
no longer returns an int it now returns
a type of it all right that's our task a
task event Python and JavaScript use
promises but it's the same sort of thing
if you think about how what they
actually turn data from the function is
it's it's changed from what you
originally bought on every one of the
cases what we're interrupting with
channels here we have another problem
with errors what happens if we get an
error in this value for we're going to
return an error here okay now we have to
put an if in here to make sure that we
don't add an error to our accumulator
that's that sort of thing it is isn't
going to work
now we can do exceptions as values but
like I said it requires us to check
every value in the channel and once
again we have IO everywhere and this
once again there's nothing functional
about this code every one of these
functions has IO of one type or another
so my conclusion at this point when I
was writing this talk that we need
better solutions than abstractions what
are some ways that we have already that
we see in the wild for dealing with this
asynchronicity so let's talk about some
pattern
this person I have one weird trick that
will reduce the complexity of your code
especially when it comes to a
synchronicity just don't do a sync code
I mean it is simple right now I mean
this I'm being I'm being a little bit
ridiculous here but I think it's an
important point and that is there's a
certain amount of cargo cult scene and
so we say feature loss that comes along
with a synchronicity in this day every
database wants to be a sync and so we
want all our services to be a synced
because everybody wants a service that
handles 10,000 users or concurrently or
something um and you know hey that batch
process that runs in the middle of the
night on a server that no one really
cares about does it really need to open
10,000 connections to the database so it
can work as fast as possible if you have
a batch process like that and it can run
in an acceptable amount of time and be
straight line completely deterministic
one or two threads write it that way
right whenever you add a synchronicity
into your code it adds non-determinism
and it increases the amount of errors
you could have in the code and so it's
possible just don't do it there's
nothing wrong with that someone unless
you need it obviously right I'm not
going to stand up here and say never do
async code that's not true
so let's look at the patterns this is an
interesting pattern I came across
recently we have a function and you'll
see a function like this all the time in
AWS run and that is we say get a page we
give it an index of a page of data that
we want and after a certain amount of
time it'll call our callback so we're
just simulating that here we're going to
say fetch page index we're going to
sleep for 100 milliseconds and then
after a while we're going to call our
callback with five integers starting at
that index right so if you walk through
the index you'll get all the integers
starting from zero working out now we
could just wrap this in a go block and
just start putting these pages on the
channel right let's fetch the first page
put it on a channel second page put on
the channel just kind of lazily spool
this on to a channel and that would work
but there's some problems with that the
other option is to write a function like
this and my comment got clipped
so we'll just ignore that but we're I'm
not going to really look at what this
code does a whole lot and more just
describe how we would use it this
session is being recorded you can look
at this code later on I mean my slides
will be online if you want to actually
try this out because it does work but
but the user interfaces this we have a
pages function and we hand it a
transducer and a reducing function and
then we hand it one call back at the end
so our transducer here says you're going
to get pages of integers can capitals
all together into a long stream of
integers filter out all the ones that
are greater than 10 and then take ten of
those so what we've done is we've turned
this sequence of pages into a stream of
data that combined with our reducing
function which I can't highlight here
but our reducing function cons will take
all those and put those in a vector and
then that final vector the final result
gets handed to one callback and this is
the result that we get we say we get
fetching page 0 1 2 3 4 and our result
now there's some interesting things
about this first of all it works on any
language there is nothing here in this
code a cipher of transducers which you
can implement normals every language
nothing is required that's unique to the
closure ecosystem so you could do this
in JavaScript or Ruby your job I even it
removes the implied buffer of one per
process allowing for more control over
the number of pages requested and that's
what I was going to talk about if we
simply just take the series of pages and
spool them on to a channel with a go
block the problem is there's a really
weird problem with CSP which is the the
theory behind Cori sync which basically
states whenever you introduce a go block
into a chain and increases the buffer
size of the overall chain of processes
by one because that that process is
going to take a value from one channel
hold it in its locals and then try to
put it on another Channel and when it's
holding that value in its local space
that's another buffer in fact if you
read the CSP literature they even do
crazy stuff like say we're not even
going to talk about buffers at all if we
want
buffer we just chained enough processes
together to get a buffer of the
appropriate size I mean you know leave
it to Matt Matt mathematicians to come
up with an elegant but yet inefficient
way of doing something so anyway we
could expand that sort of system to
handle errors as values right we could
say hey if anything doesn't work in this
pipeline any of our transducers fail we
get an error here in the result instead
of the the actual result and maybe we
get even more information about what
failed with an exception and the
transducer that our networks fail so
what we've done here is we've kind of
compacted the number of places that we
need to think about errors into one
place and that is this callback we could
expand our example to prefetch pages so
that we get the data faster and if we
wanted we could use Cory's ink with this
but I think that last point is the
interesting thing here and that is
because we are not using any predefined
concurrency model aside from callbacks
this is a great model for library
writers to use if you're writing a
database library and you're going to
return results give your users an
interface like this that is an
asynchronous response give your users an
interface like this because they can use
whatever library they want they want to
use guava promises they can do that they
want to use core async that's fine in
closure script maybe you don't want to
use any of that you want to just have a
lightweight application and so you may
just want to use callbacks for that one
or two places where you need to do an
asynchronous call another model I want
to talk about is one that Oh next use
and I think there's some interesting
properties about this so this is a
component in home next and a lot of
these things can be done in other
frameworks but I think it's best
exemplified here this is a autocomplete
and we don't really need to know a whole
lot about how it works except we're
going to point out three things here we
have a render function that's going to
return a Dom for our component and then
we also define what the input parameters
the user configured parameters are for
this component and then we have a third
function that says query that says I'm
going to need some data
and here's the data I'm going to need
and if that is generated by these
parameters so right so you say give me a
parameter I'm going to tell you what
data I need from the server once you get
that data from the server call my render
function that entire process I talked
about right there has a synchronicity in
it but we don't see that in the
component this component is completely
synchronous because the framework has
assured us that it will never call our
render function unless it has the proper
data in fact if someone comes along
later and modifies a parameter so we're
now searching for a new substring to do
an auto complete on the framework we'll
go in a call our query function again
and then sometime in the future we'll
get more data from the server and call
our render function so how do we dealt
with a synchronicity here we've just
removed it from the user interface and I
think that's very important and by user
interface here I mean the code that the
UI writers have to deal with on a daily
basis I think this is a good thing to
think about when dealing with
asynchronous code is is there a way that
you can wrap up or contain the non
determinism of a synchronicity so that
users writing business logic writing the
UI whatever don't actually have to think
about the mechanics of how the data
comes from the server and the delays and
all that sort of stuff so like I said
there's no async code to be seen at all
it's handled by the framework the
required data is explicit we are
actually telling the framework exactly
what data we need this allows for bulk
loading there there is the opportunity
here for the framework to gather all the
data needed by all components and do one
database hit to get that data this code
is also very testable we know what data
we need we know we can test what the
functions return given certain
parameters we can also combine this with
spec we could attach closer to closure
specs to the query or the params and we
could allow spec to just generatively
test hard
user interface alright I know I'm doing
kind of a whirlwind tour here but I only
have two more to go through the next one
is the reified stack because no closure
conference would be complete without
reifying something or other right we
talked about this a fair amount in
closure but the idea of a reification is
the idea of taking something that is
virtual and making it concrete something
that is intangible and something and
turning into something tangible and in
this case we're talking about the call
stack now on the JVM you can't actually
manipulate the call stack in fact most
languages you're not allowed to do that
but we can simulate it and this is what
pedestal interceptors do to get rid of a
lot of the problems with asynchronicity
so if you've never heard of them before
pedestal which is an HTTP library that
cognate egg make chains has this concept
of interceptors and what interceptors
are is basically a record with in this
case where we're simplifying the model
but with three hooks and enter and exit
and a non error and what you can think
of this is is that if you have a call
stack so a bunch of functions calling
into each other
we have sliced that call stack up a
little bit so that the on enter is the
code executed on the way up the call
stack the exit is the code executed on
the way out and if an error occurs
anywhere along that chain we're going to
call the on errors going back down the
stack so here's an example stack it's
just the vector for an HTTP pipeline
very simple pipeline we're going to
parse the parameter string into hash map
hash map of sorts we are going to on the
way out and code the body is JSON and
then at the end we have our handlers
that's going to create the body of our
HTML response now each one of these
functions enter/exit
on air so forth take a context modify
the context and return a new context so
this is the body of our parse params
enter function
on the way up the stack we're going to
take our params dring split it out and
associate that into our context on the
way out we're going to get the body and
we're going to encode that as JSON and
put that back into the context so and
then our handler is actually going to
create the body and we're going to talk
about the fact that we have a
asynchronous go block here in a little
bit so basically what happens with the
Interceptor runner is that it's going to
walk down this set of vectors and it's
going to call on enter on all of them
and on the way back it's going to call
on all on exit and it's basically just
going to thread this context variable
through all of those and what's
interesting about this well first of all
if we store this pipeline itself inside
the context which pedestal does we can
modify the context while we've modified
the context and therefore the stack as
it's running so this is a weird
situation where a little pedestal
actually uses this with its router so
the URL router actually does is it
examines the URL and based upon the
whatever matches the URL it appends new
interceptors onto the chain of
interceptors to be run so that's a cool
feature um non async interceptors are
kept non async so what I was going to
describe here is that these interceptors
are synchronous and so they're kept
synchronous there's nothing they
synchronize about them but what the
pipeline runner does is it whenever
executing one of these on enter on exit
functions if it gets back a channel it
doesn't say that channel is the new
context so it takes it goes it take from
that channel and some time later when it
gets a value out of the channel that's
the new context so specific interceptors
that need to be asynchronous can be
asynchronous but most of the time if you
don't need to deal with an asynchronous
interceptor you don't we've removed the
asynchronicity
from the daily code that a user would
write if you don't need it don't use it
in addition we can execute these stacks
multiple times they are immutable so in
the case of an
response or WebSockets you can actually
execute the on exit part multiple times
on and that's that's kind of a cool
feature and you can do this sort of
thing with a message pipeline as well
there's nothing that says that you
couldn't have it be taking messages from
a rabbitmq RabbitMQ queue and if the
whole pipeline completes correctly you
would act the message otherwise you can
Mack it so that's great all right one
last pattern and then we'll kind of talk
about the pattern of patterns and all of
this I'm kind of grouping in this case
data flow and FRP together I know
they're different functional reactive
programming is slightly different than
traditional data flow but splint that at
the right way and they're kind of the
same the idea with both of these is that
you have nodes connected via
communication channels right so you can
have a node and you're going to have
connections between the dough just think
of a aggress each node takes and emits
from one or more inputs or outputs and
then each node consists of a function
that computes outputs based upon the
inputs so let's look at an example of
this we could define a filter node right
the filter node has one input called in
and it's this filter function and the
filter function says okay we're going to
take a state which is the state of this
node and we're going to take a message
and if that message matches our
predicate we're going to emit the
message so all it's doing is filtering
everything that doesn't match we could
build a more complex node that splits
the output so based upon what message
you get you're going to split to
multiple outputs we can also write one
that does something like distinct it
only emits messages that are distinct
and it hasn't seen before now what's
interesting at this point is if you look
at this code wait a minute we already
have distinct we already have filter we
already have map why are we writing
these again right that's true and that's
really what's powerful about transducers
so we can actually leverage transducers
for this this is that first filter node
redone as a transducer
I won't walk through this code but
basically we could convert a any old
transducer to something that would work
in this manner so what we have now we
build this out is a giant hash map of
names nodes and each node has multiple
inputs and outputs and then we could
also link those nodes and say the input
of this dul turn ode goes to this other
node right but all of that so far
is just describing in data the
connection between functions and it
would look something like this so we
could add a split node we could add a
node that's squares values that it gets
and we can combine them together using
connections and then ingest some data
when we get some output I don't actually
have here the whole bit of connecting
and how you would run this graph because
it is it is fairly straightforward but
let's think about this design first of
all the nodes are all easily testable we
know their inputs what they take they
have input ports and we know that they
will put data on output ports in certain
conditions we could put spec on that we
can put predicates or tests on that
fairly easily they're all functionally
pure each one of those nodes is going
take a look at that real quick this
filter node takes an old state and
returns a new state this is basically a
reduction over messages right it takes a
version of this of itself and then
transforms itself in some way the
connections between all these nodes are
explicit which means that once you get
that graph you can do what I've done a
couple times and attach grasses to it
and admit a nice visual diagram of the
whole thing there's all of you bugging
there's a whole sort of things you could
do with this sort of stuff you can give
real-time feedback as to where all the
messages are in the graph and if you
want to use use core async to make this
highly parallel and concurrent okay
so uh it's interesting that's why I was
thinking about this I think what about
actors I it's been a while that Cory
suits been out and I haven't really ever
talked about actors so let's let's do a
rant I mean a discussion about actors
actors look a lot like that data flow
graph except for a few key differences
all the connections between actors are
implicit you never know what actor can
talk to any other actor in your system
in the actor model most actor systems
also allow for sending messages inside
user code there are some immutable actor
systems out there and if you're going to
use an actor system I recommend thinking
about that thinking is my code
functionally pure are my actors
functionally pure you can make them that
way a lot of frameworks don't build them
that way
same thing with opaque state you could
build an actor system that looks a lot
like that data flow system that says
tell me what messages to send and what
your new state is based upon an input
message but once again most systems
aren't built that way and one of the
biggest problems I have with actors is
that they only have one input there's
only ever one message box for an actor
and it just it isn't compelling compared
to a data flow system so what's the
pattern of patterns here right we've
looked at a lot of different patterns
what are the key things that I'm talking
about first of all keep your user space
pure try to keep a synchronicity channel
operations non-determinism
out of your business logic out of the
code that you're going to be working
with on a daily basis try to get the
asynchronous code right once put a lot
of tests about it think about it once
and then just not leave it don't let it
bit rot but make it so you don't have to
worry about it every single day make the
dependencies between your modules
explicit the more explicit you are about
the connections between the components
in your systems the more leverage you
have you can go and add closer spec
specs to them for generative testing you
can have auto documentation you could
it is trivial in a system like that that
data flow graph we had to simply spit
out a list of all the nodes that are
connected to a given node the system can
introspect itself and that's that's very
powerful and you can leverage all that
for a lot easier testing like I said
with speck but the key thing I want to
want to say today is use chorusing to
enable cleaner extractions not to an end
of itself every one of the patterns I
talked about do not use quarry sink as a
core piece of enabling technology it's a
tool you may use claw resync at some of
that you don't have to if you came to me
today and said hey Tim I got this new
model program and what we're going to do
is every single function is going to
have an atom in it and we're going to
forget locals we're just going to put
all the data in that atom and then we're
going to pass that atom from one
function to another I would say you are
crazy why would you build something that
way you have not have mutation in every
function that you have but yet we do
that for some strange reason with Cory's
think I don't know I mean I'm pointing
to myself here as well I think the idea
of a new library that was a powerful
score I think is has caused us to kind
of forget why we use a functional
language in the first place and that is
to have a mute ability to have
determinism to be able to deal with
powerful abstractions and somehow we've
lost that over the past three and a half
years so that's that's my take away
point think about how to use core async
as an enabler and not as the answer to
every single problem we encounter thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>