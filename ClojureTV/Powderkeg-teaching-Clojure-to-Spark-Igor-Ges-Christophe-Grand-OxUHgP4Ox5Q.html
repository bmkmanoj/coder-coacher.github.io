<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Powderkeg: teaching Clojure to Spark - Igor Ges, Christophe Grand | Coder Coacher - Coaching Coders</title><meta content="Powderkeg: teaching Clojure to Spark - Igor Ges, Christophe Grand - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Powderkeg: teaching Clojure to Spark - Igor Ges, Christophe Grand</b></h2><h5 class="post__date">2016-12-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OxUHgP4Ox5Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone oh this works good
so I'm Igor this is Christoph and we're
going to talk about powder keg so let's
start was introducing where the powder
keg came from hospital Corporation of
America is well huge so we have about
what hundreds of facilities covering 20
states basically the biggest healthcare
provider in the US and with all of this
size comes a lot of data so a couple
years ago a unit was created in HCA with
the purpose of basically sciencing all
this data and there's any data
scientists will tell you 90% of the work
is basically data cleaning and a little
bit of THD mess on top of it so with all
the data and all the janitorial work
we ended up gravitating towards some
sort of big data solution and then ended
up with spark now for those who don't
know much about spark many years ago and
Hadoop platform appeared it turned out
that hadoop mapreduce was a little bit
slow so there became a big competition
for finding the next big performance
solution and by providing a in-memory
computation platform SPARC succeeded in
winning hearts and minds the key API for
SPARC is an RDD the resilient data set
now the data set part is simple that's
your data the distributed part means
that basically all your data is spread
all over the nodes on your cluster and
hopefully computed locally there and the
resilient part comes from the fact that
the information about the
transformations you are performing on
your data is stored within the RDD and
in any case of failure
everything can be recomputed so in
practice it works like this we have two
types of operations that are performed
in rdd's the first one is a
transformation so something like map of
filter they comprise zr d d and then you
have something like an action called in
this case for example a group by what
happens in practice
for the transformations they performed
locally so in this case we're looking at
a cluster of three nodes and map and
filter everything happens on each node
the data is not passed around and
everything is nice and quick but as soon
as you get to enact to an actual action
for example group by in this case the
RTD is realized so the calculations are
performed and the new then the data is
shuffled and the new or DD is created
the shuffling part is actually quite
expensive and a lot of sparkly Foreman's
problems will probably end up from you
not very carefully arranging your
operations in such a way that you have
as few actions as possible or at least
grouping most of your transformations
together so if you try to use spark
enclosure before you probably know that
there are some excellent libraries out
there we have Flambeau and sparkling
both sort of started from somewhere by
climate corporation and we as both of
them whose some success but overall the
underlying java api was little kludgy
and it kind of translated to making work
not particularly comfortable so what I
mean by that is in plan closure if you
try to square some numbers we don't
really need to do much we'll start with
the collections through a transducer on
it and we're done now as soon as we move
to spark it gets kind of weird because
first of all we have a lot more code
second if you notice they have redefined
some core closure functions like
function or map and on top of all that
they also have a problem that you have
to alt all of your code so you have to
compile all of this to run now if you
step back for a second what SPARC really
is despite the of the Java API is a way
to describe operations on data and in
closure we have this thing that is
supposed to describe the essence of a
transformation on data regardless of the
inputs and outputs which happens to be
transducers so can we run transducers on
spark can we do it on such a way that we
don't have to alt everything and can we
do it without trying to rewrite most of
the closure and spark as it so happens
we could so
first off we'll explain how we get there
so we made a proof of concept and the
proof of concept was being able to run
this the last line where we just define
an additive function which takes an
input producer and create the
corresponding oddity and so when you do
that you don't have to learn about the
semantics of a new map new map function
and you collect a new filter and so on
which yes we use did not let you about
the closure core API and suddenly it was
rather easy to get it working while we
were running spark in local mode local
mode is failed spark
basically it's spark running with your
application inside a single JVM so you
share everything it's not disputed at
all most of the difficulties are not
there and this is why with Flambeau
sparking you can have some record
experience but you have to stay in local
mode you can add the repair experience
against a real cluster and this is
really what we are after
because we didn't like all the bad
surprises we which occur when you
migrate from the local mode to the
cluster and we also wanted to to enable
people to do some exploit exploratory
work with the real data set on the
cluster so to get stuff to the cluster
you need to send classes and that's why
you need to alt usually but something
gave gave us all the deserts color repel
for spark and the scalpel works against
of a cluster so we looked into odd out
the scalpel
managed to send its classes to to the
closer but sadly it's very specific to -
skele
we can't reuse that for foreclosure
because it uses some semi protected
stuff so it wasn't a good option
luckily there's something a in spark
which is pretty useful is that you can
extend the class pass of a written
application you can add new stuff on the
class pass and you do that by providing
a job and adding it to to the
application while it runs so now the the
goal was to get all classes that were
defined dynamically at the wrapper
package them into a job and send a jar a
particular social workers but to get the
bike out of the classes we have to ask
for the bank card to the closure dynamic
class rudder and there's no facility in
the closure dynamic class order to get
to the by card so one option would have
been to foreclosure so as to expose this
facility into the class order but we
weren't clean about closure on
maintaining or on the fork even is if
it's alpha designed offline so we will
switch over to Java Java agent Jareau
agent is a process which connects with
JVM and get full access to all the
bytecode and can read it and even modify
it so we were looking to writing an
agent it turns out that you don't have
to have a separate process JVM can beat
own agent so we created the over bubbles
named after the self debris snake it's
just a very simple agent which connects
to the JVM and expose the the proper
full instrumentation object to the
wrapper so once you get her all on this
object you'll have full access to any
class you can patch how you can do
anything and so now we can get all the
or the backyard of any class packaged
them into a single jar and send it
across the network and boom now this
work under cluster birth sadly we were
so focused on getting the classes over
to the repressor that we forgot about
about species involved so while the
first one works this didn't work the
first one the transducer is the only
thing which which gets transferred to
the worker and the transducer has no
dependency on anything but Prosecco and
Prosecco is always available because
it's it's the reading of Prosecco is
triggered by the static initialization
of Tresor classes themselves and so now
we have to find a way to transfer the
state of the nemi species and the walls
to the to the cluster so we went for
very naive approach we work through all
the namespaces and inside each namespace
we look at each law we captured the
method the metadata the value and we
build a big big map representing the
whole environment there are some nerves
will control that there is a black list
to not send some name species and you
can opt out of the transfer by putting
some metadata on the walls but generally
the idea is just to create a big map
reverse big immutable representation of
the ruling government send it to the
worker and of the worker recreated on
the other end and now this works but we
need to do this each time that we want
to send something to spark so the the
first approach would have been to give
the twinam is paid the to namespace
snapshots and send only the thief bad
thing thinking a bit more about that we
realized that since we have uber was
always already connected
we can just add instrumentation to
closure along the wall so as to be able
to be notified whenever is create create
or modify so we don't have to to
traverse the internal ever over and over
and over we just keep a log of the
modification to the wall and we send
this modification log to the to the
cluster so now demo are the basics give
the keyboard to ego okay everybody's
sort of Caesars yes
bigger okay so we're in our regular
raffle let's start by importing tag or
powder keg
okay so what happens here as you can see
it starts our boroughs and instruments
itself
collects all the Java classes and plugs
into closure bar so at this point let's
connect an actual cluster now I'm going
to start a cluster locally with three
nodes
let's Commission mock no okay as you can
see it is now sending all of the jars
all the classes back to them in jars and
sends them over so let's start by
creating nor didi
now this is fairly simple we'll take a
closer closer collection and make a
Nardini out of it and what happens here
is as you could see it again ship the
local context to the cluster in the jar
and it is in fact a regular Java RDD so
we can use a Java API is on it like this
for example and voila
that works
but this is a little convoluted and we
continue to do this in a little more
closer way so let's use em - and now
we're gonna use our first reducer on it
so it's to take and we're operating on
the same LED and the red works or you
could even just do in two and there are
DT because our digits are reducible so
you can choose reduce our into n them
so with that working let's try something
more complicated for example the example
we were looking at a second ago so again
we start with M - to be able to actually
see the output we create the RTD and
we're going to apply a transducer
directly to it here so technology allows
you to apply transducers directly so
let's do the square example
and there we go
that works as well but the example that
actually was given as trouble is the one
whose local namespaces so let's redefine
multiplication
and instead use our new newly defined
multiplication here
sorry
which you multiply you just pass when I
get home
there you go that's better now here's
something more interesting if for
example we have redefined it here it
still picks up the change to it and can
then we can see that and now the plane
already is just one of the available
rdd's there are in fact other types of
oddities and they have their own
challenges okay so much of the time to
do something useful with oddity in spark
you are not going to stay with plain
oddities of items you are going to use
key value oddities keep your key value
oddities are just a disease which open
to all the pears but not any kind of
pearls it must be a skeletal to and so
it means that yogurt then would have to
deal with specialized accessor function
and constructors to to get to the value
of the temporal and to create temples
and this is something that we didn't
want to have to do we didn't want to
expose any kind of hospitals spark or
scale as possible and so when you have
when you have a map entry or twice and
vector it's going to be automatically
converted into a total tool and when you
have an idea of Table two is going to be
converted on the fly to to map entries
before being passed to the closure card
this all this conversion may seem may
seem a bit inefficient but most of the
time is not even going to occur but
first to to explain how this works we
have to make a detour to another library
so xforms is a library that I started
before working with pork
in truth the language of xforms is
whether old I always had this pet
function of mine reduce by which is like
the offspring of group by and reduce he
instead of good group by always produce
a vector and sometimes you don't want to
club group by do the big vector and then
post process the vectors we use it so
often I had the need to have a reduced
by so as to be able to perform the
grouping and the reduction in one step
in one pass over the data and when
transducers were introduced to closure I
thought you might've okay I'm going to
to create transfused by Kino or trusses
by and working a bit more with strongest
I realized that I was really missing
fortin's user because I use and abuse
for in my card and when I wanted to
convert some sequence C code using for
mm I have no option but we write that
with map card and song and it was
painful
so I said studied X forms for for and
for what was originally transduced by so
for for is really just like the regular
for except that the first correction
expression is replaced by place order
person and so to rewrite a regular for
to a transition for you just replace the
first the first correction by a percent
and put the collection below but it's
rather simple and transduce by i never
watch it because then i realized that
with some sort of a specific to do
something better than transfers by it
was possible to define
I heard the producer Becky Becky is a
transistor which takes a nosotros user
as its argument the simplest example of
bikie is an advanced but well it's just
a function which which apply another
function to each value of a map and we
turn the map of the transform of the
transform values so the only thing that
you will do is specify by key map F and
so the job of by key is to take the pair
to take it apart pass the value to the
nested producer get the transform value
back pair it back with the key and they
meet the key so it allows to express
marvels in one as a one-liner and what
is interesting is that Mikey spawns
several concurrent transactions so you
have several transitions going on one
for each key so for example if you have
a stateful source also like in two then
you are going to perform Lancel
transaction for each key and this gives
the ability to redefine book by by just
saying that you are going to group to
consider a cave and provide you the key
so you get the key you get the original
value you pass the rational value to the
transistor which is in - which is going
to accumulate them into a vector and so
you you are several accumulation going
on
one for each key and at the end all the
accumulation are completed emitted back
to the emissive back with the result
tagged with the original key and you get
this definition of goodbye and similarly
you can
different frequencies pretty easily or
some balance and by is typically the
kind of stuff that we'd have written
with wedges by before and using by key
you can easily wide some cut perform
some some well herbs or what kind of
aggregation and an IV to be performed in
one single pass of the data and since
xforms was ready a lot about dealing
with key value pairs
it took some inspiration from widows
Cavey we just heavy rose expects a
reducing function which takes three
arguments instead of two it expects the
kin value to be passed as discrete
argument to the reducing function
instead of as a pair and so a January
8th in xforms I other KVL fable protocol
which allows function to declare that
the they are ready to take three
arguments most of the eggs from
processors would know how to deal with
KVL federal function and produce also
caviar effable reducing function so when
you create a transaction on pairs with X
forms most of the time you don't you
don't even allocate a single pair and
this is how by key this is our in in
cake we managed to avoid the corrosion
form and to Table two because at the
start we take the skeletal to be
splitted in two and pass it the discrete
argument all along the way of the
processing and only at the end we
recreate a single table - so we don't
allocate much spells we even in some
cases allocate less person scale
and so there's an electrician in kg
which is Mikey Mikey just a special
special version of oddity which is going
to ensure that the data is partition
because when you use oddity the data is
segmented into several parts but it's
very sequential it's not meaningful when
you when I say that Becky and show that
it's partition is ensures that pairs
that share the same key are collocated
on the same node and so that's the Java
ba of by key and so Becky is a take as
an input an oddity our collection which
is which already as pairs need or it can
take a collection and oddity of random
item and keep them under under fly this
is what is done on in the second example
so Becky he's going to Becky may accept
a transducer to and the transistor is
going to be executed for each key like
4x by key the same logic as with the by
P of X forms so here the first example
is just would take a range of 100 item
we group them in odd and event and we
sum them and so this is going to create
an oddity should perform the design on
each node then shuffle the the past
sessions and end the same at the end but
if you followed closely you notice that
we perform the this descent twice once
before the shuffle and once after the
shuffle
and it's not always that easy to perform
exactly the same computation on the two
sides of the defense and so what what we
have the opportunity to to specify
different transistor to use before and
after the shuffle so let's take a look
on how all of this works okay we're back
in our rap and let's start with
something simple let's create an RDD out
of a map so we created the regular RDD
now what was what we're talking about
with regards to partitioning is this as
you can see it says there is no
partition what this means is that this
is default partitioning and it hasn't
actually reallocated the data according
with the columns to the key so if we do
check by key and run it against the or
DD you can see that the practitioner Act
is now actually assigned and the data
has been shuffled so let's look at more
functionality of by key
so we start by creating an RDD again of
100 things and we're going to try to key
them a modulo 7 so what this is going to
do is take all of these hundred numbers
and assign them a key value and thus
created in your DD which we can look
into there you go now this is the basic
functionality let's take a look at what
happens with transformations
so we start with a little smaller or DD
and we are going to key it with odds so
separating into odd and even numbers
then we're going to use xforms to reduce
and sum them as long as going for tax
forms of course
okay let's try this again
now notice that here we're putting that
into an actual map and there is a
calculation now this is a happy path
we're summing things on both sides
our first before shuffling and after
actually works better because you can
get the Samsun each individual node and
you just send me up the results but if
we do something like this you're going
to be in trouble so let's take a look at
this example we start with a map or DD
and we're just going to increment them
and as you can see we have a bit of a
problem because we started with 1/2 the
increments should have produced 2 &amp;amp; 3
between it up was 4 &amp;amp; 3 so what happened
is that we incremented at first before
shuffling and then we incremented at the
game now to fix that there are a couple
of ways of doing it first of all you can
say that we only would like to perform
this calculation either before or after
and both of those work or we could
actually shuffle the RDD before applying
the calculation so you can do this by
pretty shuffling it there are also a
couple more knobs on katpa own tag by
key which you can explore later in the
API now we can't actually use all of
these options together in some
meaningful ways but before that what
kind of spark demo will this be without
the favorite word count so this is the
word count Escala has picked up from the
spark home page this is the exact same
reamed reimplementation of it in keg so
as you can see we start out by assigning
a spark context to a file this the spark
context is available in keg and then
we're doing going to map cat it
splitting up the words in each
individual line then we're assigning one
to each word to give it some area and
then we reduce it and sum it up now the
one problem with this at work the you
think would have would is that we're
signing one to every word for some
reason instead of actually summing them
up in progression so with the advanced
functionality of bikie we can in fact
avoid this allocation of tuple two's
that are going to just run away by first
King
by identity so it's about so it's the
same thing we create for each individual
word it's assigned the key but instead
we're using one of the xform student
reducers that will allow to count them
as they occur as they are assigned only
on each of the partitions and then when
they are aggregated back after the
shuffle we just sum them up so let's see
if we can get that to work so it's the
same code I'm just using collected works
of Shakespeare and we have created an RD
out of it
now we're going to use a transducer
again to take a look at the results and
there you go there are some other api's
provided as well they're not covered by
standard closure functions so we have
for example join where he can perform
inner join outer join will have joined
right joint by assigning the default
values this is also grouped by and a few
other things available in the powder keg
so what's next for part again well we're
developing streams and there's some work
completed but there's still some things
to sort out with windowing of the actual
streams also data frames were introduced
in spark and we haven't looked at those
so if anybody's interested of starting
working those that will be very much
appreciated and we have not migrated try
to migrated to spark 2.0 yet but the
most interesting application is to take
herb rose and try to use the same
infrastructure to be able to execute
closure code in other places like for
example Amazon lambdas so hopefully
we'll be able to spin-off that into a
separate library and then people can
pick it up and start working from there
so if you were excited if you'd like to
play with it it is available on github
if see data lab powder-keg and one more
shameless plug if you're interested in
music city and you like closure then
come talk to any of us and would be
hopefully happy to offer you a position
working for HTE thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>