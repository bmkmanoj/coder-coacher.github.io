<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure Compilation, Backwards - Nicola Mometto | Coder Coacher - Coaching Coders</title><meta content="Clojure Compilation, Backwards - Nicola Mometto - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clojure Compilation, Backwards - Nicola Mometto</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2SGFeegEt9E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everybody so I'm Nicola mo meto
you might know me by my nickname brown
stop
I've written a bunch of libraries which
you might have heard of called tools
analyzer or tools reader but today I'm
going to talk to you about a new library
that have been working on called tools
decompiler which as the name suggests is
a decompiler foreclosure but first a
quick overview of how computation
actually works in closure so we have the
simple steps of reading which is taking
a stream of text and turning it to code
as data then we have a step which is
macro expansion which takes this code
and completely macro spends it we have
an analyzer that takes that macro
expanded code and turns it into an AST
and usually in the implementation the
macro expansion and the analysis are
done in lockstep but there is just
usually a performance optimization so we
can think of them as separate phases and
finally we have an emitter that goes to
a SD to actual byte code which each
avian execute so those are the basic
steps and keep them in mind because for
the compiling which is essentially gonna
run this pipeline in Reverse a quick
overview of the class file structure so
everything that runs on the JVM is has
its classified structure a class file
has three different aspects to it it's
got a constant pool where the JVM stores
stuff like string contests or integers
or references to classes and methods
it's got a list of fields of static and
instance fields and finally it's got a
section which represent the method
bodies of of this class and so this
those method bodies have
some quite interesting properties and
they're very rich in what they contain
and they have a lot of debug information
which helpfully the closure compiler
omits such as the local variable table
which gives you a set of local names to
offset in the byte code which the JVM
uses for printing useful the debug
information in stack traces we have an
exception table which tells us the
bytecode offsets where an exception
handler will be active and we have a
line number table which tells us where
which bytecode offsets map to which code
lining columns and finally we have the
proper byte code so with that in mind
the architecture of the D compiler is
this we have a bit more faces than than
what I previously described but it's
pretty similar so we have initially a
bytecode loader which takes a byte
buffer or a battery and returns that
represents byte code and returns this
same byte code in a symbolic form so
that we can manipulate it as if it was
data we have done an analyzer that takes
this data structure representing the
byte code and turns it into a ASD which
is also data we have a sweetener which
is a phase that takes a Sdn returns ASD
but does some shoring over that ASD then
we have a compiler that takes this ast
and turns it back into a closure code we
have a compactor that takes this code
and tries to reduce the amount of code
that we have to undo macro expansion for
example and undo inlining and ffunny we
have a pig printer that that takes this
code and just formats it and makes it a
bit more readable so let's go into all
these pieces in a bit more detail
the bytecode loader is implemented using
bcl which is an Apache library which is
what similar to ASM only it's got a more
horrific ABI but it's got a much better
support for actually parsing bytecode so
what you but code loader does is it
starts parsing and despite code I need
and it parses the class fields first so
that we can have a representation of
every data field you know in our class
and then we start parsing all the
methods and building in memory of those
debug tables that we're going to use so
we build the exception table we look at
variable table and we build this jump
table which is going to be a mapping
from bytecode offset to positional
elements in this bytecode vector in this
symbolic factor which we're going to use
to map jump instructions when when we're
gonna do compile and then we're just
gonna build this symbolic bytecode
vector and just gonna represent the
bytecode as a data structure and this is
what what what a class for just
returning a string hello looks like and
if if you've ever used a library called
no disassemble
you're gonna see that it's quite similar
it's not exactly the same just because
for the combination purposes we needed a
bit more information but it's close
enough then we have the analyzer that
takes this data structure and tries to
turn this into from bytecode to nasc
representing closure code and so the
first thing that the analyzer does is it
needs to figure out what type of class
we're actually trying to decompile so
the closure compiler emits three
different types of class files for
actually so if and and we use some
heuristics to figure out which class
file we're looking at so if the
superclass of this glass wall is a
or Reza's fan then we know that we were
decompiling a closure function if the
class fall implements I type or I record
then we know that we're comparing a def
type or a DeFranco expression and if the
class fall failing that if the class
file implements I object and the name
contains refi then that's the only way
that we have to know that we're trying
to decompile a refi expression and
finally if all this fails and the name
of the class file finishes with dash
dash in it then we know that the class
file that we return to decompile is a
namespace initializer that contains all
the mapping in that namespace to draw
the functions that are defined after
this dispatch the analyzer does some
analysis over the methods and it needs
to ignore methods that we don't care
about that are just emitted for JVM
running purposes such as abstract
methods or bridge methods we can
initialize static fields by parsing the
static initializer and then we collect
all the useful methods through to the
compiled so in the case of a function
we're gonna either collect invoke
methods or invoke trim or inbox static
methods depending on which one the
function actually implements in the case
of learning spaces in initializer we're
gonna collect the load method which is
what the closure compiler emits and so
on and then we're gonna start processing
each method and we're gonna start
decomposing it and to do this we're
going to use a stack machine interpreter
which rather than then executing proper
instruction just builds ast nodes and it
does so by by using a bunch of virtual
stacks so it operates on a virtual stack
and rather than then properly executing
the instruction it just pushes ast nodes
into this stack and it just operates on
that and we have a PC to track which
point in the device what we're we're at
and so this processing of instruction is
a bit
different come in in the case of
different instruction times
so for stack operations like swap or dub
we execute them as if it was a real
virtual machine so we take the value
that's on the top of the second we just
replace it for branch operations like
instance or for or I think we we know
that that this means that we're at the
starting point of a conditional
expression closure like an if or a case
statement so that's that's gonna help us
know that we're gonna have to produce a
nice you know representing that if you
encounter lookup variable operations
like a load or a store then then we know
that we're at a point where we're
starting
electrical block which might be a let
expression or a loop expression and
we're gonna have to do some analysis to
figure out which one it is and so on for
math operation and then every other
invoke operation we're just gonna create
the appropriate ast node so those are
examples of methods that that process
there's instructions so we see for the
dupe bytecode we just pick the value of
the virtual stack and just update the
stack are in the same value on top of
the of the stack for the operation that
does get static which Maps you
anesthetic access field we retrieve the
target class and the target name out of
the pool element and we just generate
this map op static field which represent
this static field access and we just
push that onto the the vector sack
another example is this issac processing
instruction and so in this case we all
that we do is we collect the offsets in
the bytecode that that map to the true
branch and the false branch and the test
case is going to be the value on top of
the sack and after we've collected all
this we just
pass this information to a processive
function that that just takes the
starting and ending offsets of the 10
and s branches and decompose those
branches as if they were normal control
for operation and just builds a an
appropriate ast node and so this is what
the AST kind of looks like for that same
expression that I shown you before and
if you if you've ever looked at tools
analyzer or the closest get compiler
you're gonna see that that PSD format is
exactly the same and this is going to be
important later the sweetener is a phase
that that essentially add syntax sugar
over the ast and it overlaps
significantly with a later phase that
that I'm gonna describe which is the
compactor the weight in that they differ
is that these Whitner acts over the ast
while the compactor acts over code and
so if you have to do some sugar in that
needs to know type in for example
electrical contacts info Tangier I have
to do it at the SC level the compiler is
a really simple phase that takes ASE and
outputs compile push code and it's
essentially the same code that to
general users and that curry sync uses
as well and this is really really
important because by reusing the same
code where we're building on top of a
code base that had been proven correct
for for a couple of years and a very
interesting phase now it's the macro
compactor which needs to do we need to
undo macro expansion and undoing lining
and if you know a bit about macro
special you're gonna know that that it's
technically impossible to undo it
because it's it's a problem equal to
solving the housing problem so what we
do instead rather than trying to solve
an impossible to solve problem as we
chose we used some heuristics and
pattern matching to
200 guessing in a best-case way and so
when I was trying to implement this at
first I felt well we already have key
bit that does something like this it
recognizes patterns and tries to suggest
more abstract representation of them and
I have implemented the first pass using
qubit and core logic and it worked
beautifully the problem is that it took
15 seconds to micro compact simple to
expression so it wasn't really viable
and on top of this by using core logic
we will lose the ordering guarantees
because it's a relational enzyme and it
turns out that we actually need ordering
current is because micro expression is
defined you know in a specific order and
we need to undo it in the exactly the
reverse order so the second pass of this
compactor was written with core match
and it was really really fast
I've written a DSL over match because if
you've ever tried to use core match to
pattern match over structural data it's
not very pleasant so every Chanin DSL
that kind of looked like structural
matching the problem with using core
matches that we lose the guarantees of
unification of unification but for the
use case that we need it turns out that
we can simply use guards to enforce the
quality guarantees that we need and so
this is what the DSL kind of looks like
and this is the compactor for end the
end macro and what this is doing is
saying if my expression starts with a
lat and a tea symbol that map's to X
initialize value and if the children
expression is an if statement in which
those terms appear and if T is a symbol
that that starts with ends underscore
underscore then this is a good match for
and expression and compact is gonna
replace is let if expression with the
expression so this is a simple
expression this is a simple example but
this is powerful enough to compact
almost every macro in enclosed car after
the compactor we have a pretty printer
which just takes care of removing some
boilerplate that the closure papillary
means that that's not part of the
original source code and it does stuff
like anything namespaces and using
addresses rather than than long
namespaces and it uses fit to printer
print which is really really fast and
really good and so at this point after
writing all this I had working the
compiler and so if if you've ever
written a compiler or an interpreter a
sub hosting compiler then you know that
the first thing that you're going to do
is point at itself and so I did and I
got this but this happened not while
trying to decompile the it was the
compiler it happened while trying to
compile it and as it turns out the
compact macro that I brought was causing
so much code to be generated that it was
the file name was too long for the file
system to handle and so what I did
initially was to trying to work around
this by patching the compiler dot Java
to make it so that that the file name
got truncated and it worked and by it
worked I mean it nothing worked for
three days and I fix hundreds of bucks
and then it worked and the problem was
that one I noticed that my legs were
frying because one thread when one CPU
was stuck at a hundred percent usage and
so I didn't know what what what's going
on and I knew that my code wasn't
responsible for this so I looked at the
dependencies that were using and it
wasn't busy off it wasn't core match and
it wasn't fit either so I turned to the
one remaining thing which was the
closure compiler and after hours of
debugging in turns out that the compiler
wasn't at fault either
so this is a picture of me at 3:00 a.m.
that my flatmate took and I had no idea
what was going on I see and it took me a
couple of days and I finally noticed
that that this only happened while
trying to decompose EDD compiler itself
or while trying to compile very large
programs if I was decompiling simple
it's a simple functions then it worked
fine and so I got this idea it kind of
felt like it must be something that that
happens while some instruction gets
executed multiple times and so I thought
well maybe it's the JVM itself that
that's causing this
and so I I i took thread dumps of the
JVM processes and yeah it was the c1
hotspot compiler that got stuck in a in
a busy loop and this happened because
the way that that core match omits in
the way core match compiles a master
expression it uses a lot of exception to
to backtrack and in a way that that that
apparently the JVM that puts the JVM
compiler in an exponential loop and so I
was at a point where this compactor was
causing me to have to patch compiler and
causing this bug in not spot itself so I
decided that I needed to fix this in a
proper way and what really helped me was
that core match is a beautiful library
and it's implemented the proper compiler
so it's got a separate front end and a
separate back end and so I decided that
a way to fix this would be to just
change how the pattern matches is
emitted as closure code and so I brought
a new back-end for core match that that
uses
continuations rather than an exception
for backtracking and this fix all the
issues because now we I didn't have
nested like hundreds of nested functions
I just had a flat list of continuations
and the EJB amounts what wasn't wasn't
unhappy about it anymore and it turns
out that that when when you're doing
matching over a massive data structure
than this is also way faster than 10
using exceptions and so yeah I was
really happy
I had a working prototype and I'm gonna
show you now what that looks like so
here I have I'm just gonna import some
stuff and I have this fool definition
which is I have no idea what it does
just wrote it to be as complex as as
possible and so this has a lot of
complexity structuring a lot of a number
of very complex macros that and I'll
show you what the macro expansion
actually looks like so for that eight
lines of code and this is what what the
compiler sees and this is what I'm gonna
try to make look similar to the original
expression so I'm gonna evaluate this
this function and I'm gonna invoke this
decompile classes metal on the de
compiler
and there we go
so I'm gonna intent this a bit so you
can look at it and as you can see this
is almost exactly the same code that
that we have the original
and you can tell that I'm not
because F and G have extra appearances
and so this is really cool for me but
what are some practical consequences of
having this episode so an idea that I
had because of the property that that
the decompiler goes to an ASC that's the
same format as tools analyzer I thought
why can't I just decompile to ASD and
then pass that ASC to to turn a ledger
and do some phases over that ASEM rican
pallets so that that's what I did this
is a proof of concept of a spy bank
macro that takes any memory function and
adds debug information to it without
ever looking at the proper source code
so I'm gonna just define this helper
function and we have this make inspector
know which is taking a form and I mean
it ast node and just in reliance it's
handwritten ast node that builds a debug
information no I'm not going to describe
what it does you can look at it later
and then we have this spy Bank function
that takes a var and directs it gets the
class name decompose this function until
ast runs this tooth analyzer pass over
this ASC and what this does is it looks
at the current node and it if the
current node is a lat binding then it
replaces the initialization of that in
its binding we have the ast node that
I've just shown you before otherwise it
just keeps traversing the tree and after
this is done we just take the ast and we
compile it back to closure we evaluated
and we rebind the original part to this
new version of the function and so let's
let's take for example this bar function
which has to let bindings
and invoke spirits and invoke bar and so
what what this is join is injecting code
without ever looking at this source code
and this might seem trivial but but it's
it's a probably it's a really
interesting way of doing meta
programming so you a few ideas that I
had for this approach would be to like
having a compiled function making it go
from synchronous to a synchronous
without ever touching the code just
sticking a bar and saying hey I want
this function to be a sync now well
let's just decompile it and rewrite the
EST node and recompile it on the fly or
stuff like if how many times have you
had stuck trace
you got anonymous function can be cast
to whatever and you have no idea what
what that anonymous function does yeah
what if you could just click on that
anonymous function I get this source
code how good would that be how good of
an experience would that be so this
works quite well there are some known
limitations so the closure compiler does
some managing on the names and we have
no way of knowing what what your
original names were so if you have a - P
we're going to compare it as a
underscore B because we have no way of
knowing whether the original name was a
and a square B or a - P
this means that there are some cases
where symbol might collide and we have
no way of saltiness at the moment we
don't emit type hints at all but it's
something that I'm looking to join if
the original source code contains
explicit casts or implicit cast and then
it's possible that the compiled code
will either contain extra casts that are
still technically correct but that it's
going to be a bit more code than than in
the original source code there's some
there's a few macros in closure code
that that I haven't written a parser for
but that's just a matter of doing it I
just got tired of writing parsers and
there's unknown but in which so closure
the closure compiler we happily emit
byte code that's technically wrong just
in ways that that the JVM will never
reach so the JVM doesn't complain about
it because it's unreachable the compiler
doesn't know that when an instruction is
unreachable so if that's the case it
might barf it will only happen in very
very at like hard to find edge cases
I've only had it up and once
some future work I want to try to sugar
syntax quote expression so if you've
ever looked at what's in that scrotum
it's it's a massive amount of apply and
lease and Concord expressions which at
the moment are just in lines as if they
are and I want to write a parser that
that figures out whether this amount of
code is a syntax what expression and
printed easily support for macros like
four and ducek is there but I don't
support when and let in foreign ducek
because it turns out that it's really
really hard to to parse that and NS is
not compacted because the output code is
readable enough but it's nothing that
I'm looking I'm looking to do in the
picture and another interesting thing
thing to do maybe it's reuse the line
table to guide proper be printing which
is something that I'm not and so I
wanted to chop and search this and I
wanted to have answers this today but I
thought I'm gonna have some fun with you
and so I'm gonna open source this this
evening but in the meantime you can go
to this URL and instead and what you're
gonna find there is a jar file with DD
compiler a or two compiles which you can
use to decompile yourself and look at
your code
so that that's all I had for you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>