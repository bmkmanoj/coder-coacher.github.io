<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure Versus the Botnets - Marshall Bockrath Vandegrift | Coder Coacher - Coaching Coders</title><meta content="Clojure Versus the Botnets - Marshall Bockrath Vandegrift - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clojure Versus the Botnets - Marshall Bockrath Vandegrift</b></h2><h5 class="post__date">2015-11-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5lL1xYHDCSk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright my name is Marshall and I will
be talking about our experience at
damballa using closure to find botnets
and I will get my slides working maybe
maybe one day one of these buttons how
do computers work okay that worked so
I've been thinking a lot recently about
what makes a language practical and
there's a lot of like intrinsic and
extrinsic aspects of a language which
can make it practical but this kind of
necessary subset is that the language is
productive and by productive I'm
speaking specifically about getting
things to production so taking something
from an idea like the soft and building
software that can turn that idea all the
way through into something that delivers
value at appropriate scale with
appropriate maintenance costs for
whatever length of time you need this
piece of software to do the thing that's
supposed to do so we found closure to be
an incredibly productive language and
for some very specific applications for
these specific set of reasons being JVM
hosted highly interactive and value
oriented so other languages have one or
sometimes two of these but closure is
fairly unique in having all three of
these to an incredibly deep level and we
found that the combination of these
three things working together results in
something that is far more productive
than the contribution of each one
individually so dumb Bala you probably
haven't heard of us but what we do is we
make a appliance and network clients
like a physical box that sits in
customers networks and monitors their
network traffic what we're looking for
is signs of malware activity on
individual computers in their network
when we see malware active when it's
communicating up to its commander
control servers or doing something else
malware does we let them know the
customer is not the malware
so at ambala on the rd team we love
closure we've been using it for about
four years now there isn't that much
closure like in the appliance itself but
it's used extensively and almost
exclusively on our back-end server
infrastructure we use it for our malware
analysis pipeline and for like
integrating microservices but there's
one place in particular where we've been
using it for the longest and that's in
data analysis problems and that's where
this kind of Triforce of properties it
really comes in and really what's the
term likely makes closure the most
productive environment that I'm aware of
so this was an alternative title for the
slide Bobby kind of prefigured my joke
here a little bit but the we can call
closure the practical submersible
submersible factory for your data lake
and if you haven't heard this term data
lake i know i heard some people laughing
when it was mentioned earlier but here's
the wiki the wiktionary definition so a
massive easily accessible data
repository for storing big data designed
to retain all attributes especially so
when you do not yet know what the scope
of data or its use will be like this is
my favorite buzzword because it's so
funny like here's my attempt at an
honest definition so it's a Hadoop file
system filled with messy a
semi-structured data that no one's
figured out what to do with but you've
got to keep it so you just shove it all
in HDFS so when someone says that they
have a data lake architecture they're
kind of admitting that you know they
have a data architecture in the same way
that a small child cleans their room by
just shoving everything in the closet
yeah so it's ambala we have a data like
architecture so we've got all these like
random data sets that have weird names
that were created you know with random
ad-hoc data formats for specific
projects that no longer exists but its
data that is potentially useful and we
keep it and I think that actually we can
justify it and you probably if you're
doing this can probably justify it too
if your core business follows this kind
of pipeline what we need to do
ultimately is predict the price
of malware on customers networks so that
means we need to figure out how to
transition from raw data to meaningfully
predictive features and then translate
those features into predictions about
the presence of malware and then that
first part like this this kind of
process from data to features like
that's the hard part and that's the part
where you don't want to throw anything
away it might potentially end up being
predictive this parts easy like this
part like if this part where the hard
part and I would argue that everyone
would be programming in J and not in
Java like in J like you can write an
algorithm like and the entire thing like
in the length of code it takes you to
make a joke about however both Java it's
right but the point is that the hard
part the part that you need to iterate
on in the part that you need to actually
like need your language to support being
productive in is having this iteration
of getting from your raw data to things
that are actually have predictive value
so kind of my my takeaway here is that
when we have this sort of problem where
we need to start with something that's
like an unknown predictive value data
and turn it into something where we can
actually get new information out of it
is that we prefer having real programs
over simple queries right we want the
power of a full programming language
which you know here so we prefer a
closure filter too simple where clauses
so back to this so I'm going to
contextualize this by talking about a
specific project that I've spent a lot
of time on recently at done Bella that I
named penumbra so what penumbra does is
that we attempt to classify computers
based upon their aggregate DNS behavior
so the collection of domains that they
resolve not simply individual signals
the best example of this that or the EES
example of this is talking about click
fraud malware so click fraud malware
will get commands of sets of sites to
generate traffic against and these will
be updated periodically and these are
sites that humans don't visit very often
but a human might visit one individually
but if we see a computer visiting a
whole bunch we can be pretty confident
that that's actually malware and not
human just randomly visiting one of
these websites so in order to do that we
need to apply that process I was showing
earlier and that's where the properties
that I was just talking about start to
come into play so I mostly want to talk
about their combination but this one JVM
hosted pneus is so important on its own
i'm going to call it out so the data
that we have this that we're pumping
into our data lake you know it's not the
biggest data but we're getting about
half a terabyte a day and then of that
data about a quarter terre by today is
the data set that i'm using for this
project where we have data sharing
arrangements with our isp customers and
get anonymized passive DNS data now a
quarter terabyte is not huge but if you
want to do temple cross temporal
analysis where you're looking at a
couple weeks at a time it gets pretty
big and to handle that you're kind of
going to have to use Hadoop like they're
just really isn't any other publicly
available solution that allows you to
have heterogeneous jobs running in a you
know ad-hoc fashion on that scale of
data and Hadoop of course is a JVM
platform tool so that means you need a
JVM language to interoperate with it
well now that's only partially true
right obviously the spark people have
put a ton of work into making Python
work well with spark but they had to put
a ton of work into it by just having a
JVM language to start with then you get
to part of making actual you know adding
business value far more efficiently and
then we're not just using Hadoop right
so here's this like logo of projects
that are involved in penumbra we're
using a bro for data we're doing some
initial analysis using hyper log log
implementation from stream web we're
using some tiny bits of mahou not any of
the machine learning and then we're
using the lib linear report of Java
sorry the Java port of linear to build
the models for each individual malware
campaign and what's fascinating about
this in my opinion and the way that that
closure implements its JVM hosted pneus
is that these are the libraries where we
have significant integration wrappers
right so
for avro we have the a burcad library
which allows us to directly produce
closure data from a bro data and that's
highly useful for Hadoop we use parkour
but then there's lots of other libraries
out there that lets us you know right
our closure code and then have it run
very directly as Hadoop jobs but for all
the other stuff on this slide like you
you don't need those details right you
can just call the Java API and that is
incredibly powerful like opens up the
range of options you have in your
ecosystem incredibly so so kind of my
takeaway here is that you should prefer
integrating to rapping like that's been
our experience the time we don't spend
writing superficial wrapper libraries is
time that we can spend instead having a
better understanding of the libraries
that we are using and applying them to
the problems we actually want to solve
okay so the next important thing is then
starting to look at these intersections
so the intersection of JVM hosted pneus
and interactivity so the key place this
comes into play in the kind of
applications I was talking about and in
the development of penumbra isn't asking
the question what does my data actually
look like right like even if you're
using a system with schemas like we are
with a bro you still end up like you
might stare at the avarice game you like
it okay well what does that mean when
I've got this union of these three types
and then I can allow a string here and
you know you you just don't have a clear
picture or rather you can have it even
clearer picture than you could have
otherwise by getting to look at the data
and actually pulling it into your rebel
where you can see it so this is an
example using park or distributed
sequences but you can do the same thing
with with spark or any of the other
libraries or any database in fact that
you're using right you've got your data
and then you can pull it into your rebel
and you can see what it looks like
because you are writing this application
that runs in the JVM that will use
whatever data access method you'll use
your rebel is just another JVM
application you're just putting them in
the same jvm no problems just pull it in
see what your data actually looks like
so this gives you the confidence that
when you're writing applications like
you you kind of have this idea that you
know you
data will be what you expect it to be so
the takeaway from here is you know don't
you know try to avoid doing things that
prevent you from integrating data
sources with the rebel we had some
experiences with some earlier libraries
before we wrote parkour and the sparks
case enclosure still seems to be a
little bit of this where you know
there'll be disclaimers like this
doesn't work on the rebel and that's not
necessary and it impedes this
development so I think the yeah the
takeaway here is to think about this
case and you know make sure that data
can just be pulled right into the rebel
so the second part is the combination of
the repple interactivity and the value
oriented philosophy now x value
orientation here this is this part this
one's going to be the like the like kind
of obvious one for most people in the
closure community but the valley
oriented part we tend to write our
functions are we time to try out
programs in terms of pure functions
operating on immutable values so when
we're talking about this pipeline like
this is where we pulled the data into
the repple but now we want to iterate we
want to try to find these actually
predictive features so we're
transforming our data and even if we
know what it looks like initially like
what does it look like after we
transform it well with the repple we
have this very obvious way where we can
actually just try these transformations
out and see what the result is and then
because we're working with pure
functions on immutable values we can
hoist what we've done in the ripple put
it in a function and then it does still
does the same thing right so that gives
us confidence that we know that when we
perform a transformation on data that we
know what looks like we know what the
result will be and that lets us iterate
on this process of figuring out our
predictive predictive features far more
quickly so the takeaway here is that we
prefer values estate right and this one
is again this is the guinea this is
obvious in the clutter community but you
know there's certainly ways that state
can sneak in some of our early
experience with with closure you know we
did crazy things with dynamic variables
and you still see it occasionally with
libraries that interact with databases
well there will be some like global
peace
state that needs to be you know properly
configured I think component has cleaned
a lot of that up but you know anything
that impedes this ability to have
transforms right in the repple slows
down this process and you know is easily
avoided all right so the last part is
when we can take the value orientation
these pure functions and combine them
with the hosted pneus of the JVM so the
place where this comes nearest to my
heart is transformations that we want to
run in a MapReduce fashion so MapReduce
is actually pretty straightforward to
understand you know if I'm sure how many
people have been doing MapReduce a lot I
produce okay so actually a fair number
so this this will be completely
necessary but the basic idea is that we
we have an operation we have some
transformation and we want to split it
or what all we need to do to make it
work with MapReduce is kind of split
into two operations one where we
previously just had we're transforming
our square into a circle now we
transform a square into a circle but
then we also have to have another
operation that takes some arbitrary
number of circles and produces another
circle so then all we do for MapReduce
is we run a whole bunch of the first
operation in parallel and then we run
another set of the operations with all
those output circles so this is pretty
straightforward the problem like like I
think almost all the complexity comes in
when you start looking at it from the
kingdom of nouns Java object oriented
perspective where then these are what
your operations look like and oh my
slides are getting cut off on the side
so you end up with this like collection
of like like templates where it's like
all the transformation is bolted
together with like the mechanism right
so that you have this like intermingling
of parallelism and it's a MapReduce and
what are my types and it's just kind of
all Guam together so enclosure we can
focus on the transformations right we
can just have pure functions operating
on values and then have those operate in
the JVM cleanly now systems like spark
you know put the pure
functions in the forefront but then they
almost have the opposite problem where
if you're working with spark with partly
thumb with Python or with Scala you know
you'll have this pure functions here
where you can only use pure functions
and like if you read the spark
introductory documentation you know they
make very certain to call out like hey
be careful like if you try to have this
mutable object sitting off here and then
you've got this pure function like it's
not going to work you know because your
job is going to be distributed and all
this code will be running on the cluster
and with with closure if everything is
already written as pure functions you
don't even need to think about that
stuff just works right you wouldn't even
think to be writing this mutability in
terms of an object in the first place so
the takeaway here is prefer interfaces
that work in terms of functions and not
interface or interfaces that require
things that aren't functions Michael
dragoness made the same point with on
extent but I think it applies to a wide
variety of data processing applications
as you know especially if you're
integrating with a JVM system which you
know in its underlying Java
implementation or scala implementation
like wants something that's an object
that embraces mutability instead of like
having some way of like having a deaf
random thing that you bolt into that
like find a way where you can feed a
pure function in and have the pure
function do all the work and the result
will work in the repple and will also be
a far cleaner more closure ish
experience all right so I've kind of
zipped through my slides so my
concluding conclusion so this is the the
Triforce of things that makes closure an
incredibly productive language for data
processing applications and the lessons
that we can kind of take away from the
experience we've had at closure or the
experience with closure that we've had
it done Bala is that we want to you know
work on ap is and interfaces that
support these properties and the
interaction between them so we prefer
things that work in the repple to things
that require ahead of time compilation
we prefer about used to state and we
prefer things that are functions to
things that are functions so that is
that any questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>