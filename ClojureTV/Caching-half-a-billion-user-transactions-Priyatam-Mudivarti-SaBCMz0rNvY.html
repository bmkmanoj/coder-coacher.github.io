<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Caching half a billion user transactions - Priyatam Mudivarti | Coder Coacher - Coaching Coders</title><meta content="Caching half a billion user transactions - Priyatam Mudivarti - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Caching half a billion user transactions - Priyatam Mudivarti</b></h2><h5 class="post__date">2016-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SaBCMz0rNvY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Preetam I work at the
level money team it's been a fairly new
new gig for me about seven or eight
months it's a great team so I've had a
lot of fun working with them and this
talk will be about caching at the
application layer it's not something I
have worked extensively for a while so
we'll see we'll see how how I could
share some of that with you guys
so level money app let me just set this
out so level money is a is a platform
and an app it hasn't been around for
that long time but it has been out for
like more than two years so that's
that's not a long time I would say it
has been bought by Capital One and like
most most apps that you would know we
have a we have a classic Android web and
iOS steams and the fun part here is that
most of it involves in viewing user
transactions and what user transactions
are are essentially when you make a
purchase on a credit card or a debit
card you have a you have a row right
when you link to your bank account you
have that single row that says this is
what happened so if you combine a
million we have a million download so
there are a lot of users if you combine
all of them and if you look at a
synthetic view of them you actually have
millions and millions of transactions so
the key question here is we're doing all
of that it we're doing querying and
filtering and sorting and quit all of
that at the application layer how
complex can that be so that will be the
premise of the talk some of the stuff
you might be interested in now is this
whole app is actually a single
monolithic and a single source 32
thousand lines of closure code and I
would say this has been my biggest
production code base we do have about
170 clusters easy to close easy to
instances so there's a lot of code that
is running but the best part is yet to
come
one of the things I've learned by doing
by understanding how cache systems work
both in the libraries and as well
application is that there are two
fundamental concepts that is we're
either looking at spatial space-based
locality or temporal locality and I
believe that most of the issues we find
an application cache kind of falls down
into temporal locality what I mean by
that is if you access a data X in a time
delta there is a good chance if you
design your application cache well that
you are going to access the same Delta
again very soon within the same Delta P
that same cache segment and I think if
you see if you look at it this way it is
important to understand that what you're
looking at is not compressing or
optimizing the space because memory is
cheap right storage is cheaper memory is
cheap what we're optimizing is the
concurrent axis what happens between t1
2 p.m. I think that is key and therefore
I would like to highlight the three
things I've learned in most caches we we
get the three basic concepts I think we
should be all familiar with this right
but I have a question to ask does anyone
know which comes first if you were to
design caching and if you have these
three concepts which comes first I guess
I cheated I gave you the answer right so
I think invalidation mainly comes first
maybe I could be wrong but let's see so
what is invalidation in a nutshell I
like to explain things for a
six-year-old because I think I'm a
six-year-old sometimes so invalidation
is essentially you have concurrent
access either same user are trying to
access the multiple like this set of
information multiple times or multiple
users are racks trying to access that so
the question is how do you notify how do
you how do you tell the other person the
other cache requests that it is no
longer valid
that's essentially invalidation
so that's no longer valid right and it's
not as easy it is actually a hard
problem eviction on the other hand is
asking a question that comes after that
the fact of invalidation which is how
long do I exist I in this case is a cash
segment so I don't know why this guy is
excited but but it seems like most
algorithms in my in my research I've
seen that most algorithms focus on
evictions correct if you if you look up
cash algorithms
you'll get L are you starting little are
you all the way to lives but most of
these algorithms are focusing on
optimizing the space are optimizing the
memory so like most closure engineers
I've been doing it for a Bandhan closure
for a little less than three years and I
always ask the question how do you
decoupled things and it looks like most
caching solutions and I'm talking about
application level at the library level
we couple eviction and invalidation
right if you look at cold hard cash
there is an effect function look at any
Java library any Java Enterprise caching
do you know name your favorite language
you couple both why are they couple
guesses
I think he has he has a point
you one is dependent the other but but
my point here is that oops my point here
is eviction seems to be if you forget
the fact that memory is limited
imagine memory is unlimited eviction
cannot exist without invalidation I did
put a star because that is actually not
true but if you think of it this way
invalidation has the property an
eviction is the aftereffect correct so
why not decoupled so this is the sort of
insight we had and I'm sure there is
there's a lot of people have done this
in different layers but but I think our
premise is that if we decouple I think
we might have a little more control and
and by that I mean you're going back to
that question what time the question is
not what time the function is called if
you remember the previous slide t1 to TN
it is what is the relative time to the
invite invalidation request it boils
down to time and there goes my third one
is it seems like naming is what couples
them both so why not put the time in the
cache name itself in the cache key name
itself so this is how our cache key name
looks like we actually have a cache name
a UID which is our user ID we have a
cache version which we will see in a bit
and the notion of logical timestamp now
if you've seen the from the first slide
I did start that temporal locality is
where the focus is but we don't want to
keep capturing every instant of every
user request because that is a really
hard way to solve time problem in a
concurrent access instead we will have a
logical time stamp which is essentially
a counter anatomic counter to saying
that if X number of requests happen if
there is an invalidation the counter is
bumped it's a really simple concept we
have seen this in many many applications
an atomic clock essentially instead our
atomic clock is logical and the other
key insight is that our keys we don't
know the keys yet we have to know the
logical time stamp to get the keys so
the keys essentially are immutable what
that means is that although you get
invalidation
sort of free because each key is now
mapped to a generation that and when
every time there's an invalidation that
happens that generation is no longer
valid therefore our key cache keys are
essentially you know providing as a
small snapshot of the caches so going
back to that it seems like our focus
should be on invalidation and naming
gives us that gives us that access and
let's see how that works in our case we
have three three essential caches which
is local remote and generational and I
want to talk about generational first
because that's what I introduced a
generation so a level cache in level
cache this is how it looks like it's
actually a just simple map so we start
with a name we start with a version and
we start with dependencies and in the
dependencies you will see that the cache
is dependent on other caches this is
just like any any concept you've seen
where you know services are dependent on
other services or even components are
dependent on other components same
concept except you actually focus on the
cache query itself right each function
that is surface as a cache is now having
these segments so the key here is the
compute function the compute function is
responsible for taking it's an opaque
function it returns let's say a tea some
value that can get serialize and
deserialize that's all it is so if we
have a definition of our cache we have
dependencies and we'll see how these die
together
have you seen as you've seen in the
beginning a user's transaction is
essentially our entire focus of the app
right
we surface up the users you know
transaction history we do some
predictions we a weed we have some data
science algorithms that run everything
goes through the transaction so we're
essentially caching all of these
segments and the dependency actually
does not is not that fun because it ends
up being a dag and if you if you've seen
systems where services
are dependent of other services you can
apply the same logic here right so in
our case you can look at this balance
projection over here that actually
depends on other other cash segments so
how do you compute a cash that is at the
bottom of the bag and that and then
other caches have to be computed before
that it is a tough problem right so
let's see how that works
turns out we didn't want to solve when
we started off this we didn't want to
solve the cache dag automatically so we
actually hand coded that order it's an
ordered list at this point there are no
branches so as you can see the entire
list is listed one by one so if you have
to invalidate full funds as you can see
over here we just invalidate the rest
it's a fairly simple idea but when we
have time we would probably make this
more automated use something like
Stewart's here as component which does
dependency is much much better and I'll
see at the end how that that might work
the dependencies are essentially
hierarchy - these are some points that
there are left in the slide just in case
you wanted to know at the end but the
most important thing is that we don't we
have a compile time check that sees that
these dependencies are don't end up in
circular order we'll see that in a bit
as well and that's that's well it's a
single function we can validate that if
our caches are you know perhaps not
written well the dependencies we
actually have a validation thrown at the
end at the beginning of the app but this
is my favorite part which is the
generational cache so as I said there's
a core cash logic that is pure closure
has no dependencies on anything else
including our own apps apps ApS in that
core cache there is an immutable key
that is tied to how values in the cache
are produced and there's dynamodb we're
just using that the store our logical
time stamp you could replace that with
anything else any data store it is
important to store the cache key because
it has to be atomic and persistent it's
the only way we can actually see the
generational cache
maybe it prevents race conditions I
don't know but you might fool with we
might want to think like that the next
step is that the logical time stamps
start at one and they get atomically
incremented as i just mentioned and we
have several api boxes that actually
query these caches and of course there's
a mesh will see the memcache part in a
bit we also have queues that
refill the cash every in our case we
have every 24 hours we also have some
properties which some services that
check if the caches fill every three
hours but at this point there is no
external storage this was what we
started off two and half years ago we
actually used that dynamo as our cache
storage engine that was a bad idea
costed us two I think close to $10,000 I
want and nobody wants that right so so
you will see in a bit why how we move
away from dynamo and actually use
memcache but as you can see most of the
cash logic was pure closure there was no
dependency on even dynamo for that
matter we were just storing it as a data
storage as far as race conditions as I
mentioned if you just invalidate the
cache we actually increment the logical
time stamp so that gets that is how we
invalidate our cache the old generation
will still have access to it but that is
such a small time span then it that it
doesn't matter for us so next step is
memcache so as you can see there has to
be a delegation because because we
didn't want to tie the storage provider
to our cash logic so we used sort cirrus
component which kind of played well in
this case because we can now delegate
all the storage storage access patterns
inside that along with metrics anything
that you don't want to deal with at your
core logic so therefore and there's also
serialization that is involved
with memcache now we use protocol
buffers and g-sib that and there are two
questions that come up when you're doing
this which is when should I add and when
is it proved there are no right answers
I would say that it depends again the
worst answer that I hear and and
unfortunately I have the same answer is
it depends because you have we have to
keep calibrating our API boxes right if
if it is getting too full then you might
want to check is it because of queue
service because typically your your back
by a queue service the one thing that we
don't want to do is keep refilling the
caches we do what I didn't want to I did
mention this earlier that we have API
boxes but the one thing that we do is
that we pre fill every users cache on
load what that means is when an app is
when someone logs in every data segment
they need for the transaction is
actually pre computed this is important
because that's the fastest way to
actually load stuff but at the same time
that the trick here is to not load
everything at the same time because if
CPU cycles are going to be busy so
there's always a trade-off balance there
the thing with memcache is that there it
isn't the memcache issue itself it
depends on your the data payloads we
actually have beta pillars that are 1 MB
imagine a user having last three years
you know transaction history from their
bank accounts and if you actually
compress all of them it still is a
bottle but 700 KB to 1 MB and gzipping
that itself takes a lot of time we're
talking about all these that's right so
this is somewhere we we hit we saw the
performance hit but I think nipi was
faster it still wasn't that good that's
when we went into the second level I
didn't mention there was a generational
cache local cache and remote cache what
we saw was remote cache and because we
had dependencies and we were computing
again and again and see utilizing again
and again for each dependency it made
sense to cache something locally because
we don't want to compute the same thing
over and over within the same computer
cycle so core cache is fairly simple it
is
close your can trip and it has a lot of
good implementations the best part is
that spy cache which is the library we
use from memcache already has a core
cache picked in so this is how the core
cached protocol is what we will see here
is just a hit and a miss we are not
using the others as well as lookup but I
hope that makes sense in the next couple
slides because I think a lot of it came
together at that point for us at least
so as you can see now the component is
getting busier we had the core logic
logic over here and we introduced the
memcache which is separating the storage
provider now the component is also this
deciding should I look at local should I
look at remote and that is the couple
from the application logic which is
doing invalidation and recalculation and
it's a plain right through and read
through so before
I want to go to the next slide have you
heard of all about delays raise your
hand if you're familiar with the lace in
closure okay that's good this is based
on a blog and we were kind of unsure
whether to know buy more memcache
clusters or you know spend like a couple
of weeks on like tuning and then we
found this blog I would say that a lot
of it came together at this blog I
totally recommend this even if you're
not doing cache this is an excellent
example of how closure actually provides
small set of tools and not necessarily a
package solution but when you find the
right hit it actually is a sweet spot
and in our case delay was what was what
made the recomputation more efficient
and if you look at delay even if you
haven't heard of it this is it it is
literally a Java this is a Java
implementation in the closure of delay
and delay looks at it has it has a
synchronized method for D reffing and
that's pretty much about it and it is
only it is guaranteed to run only once
and any time any other person who dear
effort gets the cached value so in other
words it is
it is touted as a better memorize and
you have to read the blog to kind of
slowly go through the entire delay
versus feature versus promise the way I
understand it as a six-year-old usually
is that delay is a smaller subset future
adds on top of delay and promise is
basically delivering a value to a future
whereas delay is you're delivering the
value but you're not computing it at
that time in other words if you have a
computation that is expensive and you
want it to be synchronized and you want
to actually execute it at a later point
of time delay is actually a good fit so
we have two functions so after after all
of this we looked at how can we do our
local cache to memcache look up because
the whole problem boils down to the fact
that we don't want to go to memcache if
we have something in local cache and in
local cache we have a big tag that means
there are a lot of dependencies whose
computations have to be traversed and
each with an each traversal we want to
make sure that that computation is not
hitting again and again so this is how
it looks like so query cache is the API
that is exposed to all the rest of the
services the first step is we get the
logical time stamp and we get the
compute key over here and we have a
recursive call to compute all the
dependencies and what that what I mean
by that is a cache may have other
dependencies that may have other
dependencies and the values that are
computed if you go to bottom to up have
must be passed to everyone who's
computing it because otherwise you'll be
hitting the same value again again right
and the next step is we delay the
recalculation and the recalculation is
essentially a single function that
computes what is the value of this cache
but we don't want to calculate it at
that point we wanted to live so you put
it in a delay and then third step is
that is what we call gen cache and in
gen cache we're saying no cache
component instant is an atom that holds
a three keys you will
in the next slide gen cash essentially
is swapping the components of the atoms
map with this function called query
through and it's passing the delayed
value which is essentially saying hey
take this recalculate computation if
there's a delayed value so we're passing
delayed values which is which is one of
the insights we had that you can
actually pass a delayed value and then
we give a serialize and deserialize
functions to that and we'd return the
result we dere after result of this I
hope that this will make sense in the
next slide so the query through which is
getting the component which is the atom
it has a memcache and a local cache it
did have the base key and the value
delay was the delayed recalculation
right so the first step it does is it
will it will say hey okay someone is
asking me check this cache name is a
there or not the first step we will do
obviously is to look at local cache
right so we'll do a core cache look up
which is the primitive end core cache
protocol but then it will put it into
the cache hit saying that it is it is
calculated and we always put the result
in the result key now that atom has
three keys local cache memory and result
the one who is be restless will always
look at the result if it's not found
then we look at the memcache and return
a delay of the memcache value now this
gets a little heavy here because we're
talking about a delay that is that is
being passed and over here we are
delaying a memcache value which is
essentially looking up if that value
exists in memcache or not if it is not
then we miss we do a cache miss because
at this point it's not in local it is
not in memcache then we call the core
care saying okay this miss now you do a
value delay raise your hands if you if
this does not make sense because I'm
going to raise my
when I saw this it was like okay this is
one of those moments in Claudette land
where I'm frustrated or probably Lisbon
I am completely frustrated I don't get
it but then I get it and then I walk
away and I will say when I got it is
that this is the value delayed e'rything
now if you remember the previous slide I
am passing the delayed value the delayed
recalculation value to the query through
correct and over here I am dealing that
delayed value which is wrapped in a
delay this is the beauty of delay you
can pass a delay and dear effort into in
a wrapper that is a delayed right hold
on to this in a bit
so right now if it is now that we found
it and we found we didn't find it in
memcache we hit it right we get the
value and then we deduct the value but
again notice this is a delay
we're debriefing the value and then I
merge it and I put the result in
memcache delay now if you've noticed in
the past slide we're returning to the
API provider now this is the single
function that is called by all the api's
it gets the result and the result is
always going to be the value that is
computed either from local cache or
memcache which means that the API does
not care where it came from it might
come from local cache it might come from
memcache and if there are 10 concurrent
users actually requesting this the first
one is only going to calculate that the
nine other requests will get the same
cache value and this was to me that was
the best part now we saw the issue that
there might be hundreds of requests Val
requesting the cache value and if it is
in local cache it is guaranteed that
even if we have a complex dag it is only
within that computation it's only going
to compute once before this it was
computing every single time and that is
not fun it's like taking 20 seconds even
though it is hitting the same for the
same request and I thought that was
really cool
because we did not do any locks here
there is no primitives other than this
thing
delay and I thought that was that was
really good I would say I probably
understand 99% of it it would be sad if
I said I understood everything this is a
lot still that I'm learning but the best
way I've found how to understand this
how does and delay and delay our
computation is like how we box things
and this is an abstract notion of how I
understood is that you can wrap the
recompute delay and then keep passing it
into memcache delay and the final result
and you're essentially delaying that
value and finally the result is being
bereft I call that delay the
recalculation so in in summary what we
have is cash logic which is pure pure
closure we have a cash component that is
decoupling to look up through the local
cash provider or the remote cash
provider the local cash provider is
using delays so that the computation of
in the entire dag is only done once and
it also making sure that the concurrent
axis is in a to its best there is no
guarantees yet is there might be still
some race condition but it at least
tries night.the 99.99% I would say and
then we still use Dynamo or any external
storage provider for the get get logical
user timestamp and going back to the
first slide this kind of makes sense
because now we have a physical cache and
now we have many logical caches and if
you have you know if this is you're
running the service in about a dozen API
boxes each API box has its own local
cache right and the key here is that we
don't we want the local cache to be
really really fast because what's the
point of having a local cache we don't
see utilize our deserializing local
cache but we do see utilize in the
memcache part which which takes a little
bit of fit but but that's ok so yeah
this is the summary of what what we have
at this point it is not perfect but it
gave gives us that nice separation of
local to remote and not we don't have to
worry about hitting memcache
time there are some gorgeous especially
the queues can be backed up and
sometimes for filing could help there
are some other interesting facts as I
mentioned we have several API boxes
which means there's obviously a load
balancer in front and this is how it
looks like so there are many requests
coming into the load balancer and now
they go to individual API boxes and each
box that will have a local cache now
that is the fastest part it usually is
about a couple milliseconds that is
really fast but here's a problem can
someone see a problem here especially
with the load balancer hitting the local
cache in terms of yes so he said that
coherency will the same will the client
get the same local cache when they make
the request that is a problem
so so our load balancer is stateless we
did not put sticky sessions yet and
we're still looking into it but the
consensus is that making the load
balancer smart is probably not a good
idea
but there is an issue where if the same
user requests makes multiple requests it
will hit in the other API box and they
may not have a local cache the same
local the same hit they had in the
previous request so in that case it will
go to memcache and in memcache is
anywhere from 80 to 100 X hip in this
case it happens to be you know 100
milliseconds the best-case scenario is
like less than 50 milliseconds but it
but if you take the network load it
might actually bump up and if it's not
fun in memcache then you have to go to
dynamo and that's really expensive but
that's something we're we're ok to live
with we haven't discovered a better
solution yet but that's the known issue
I always watch your dashboards as I
especially look at the Vixens it just
looks like this which means it's busy
and that's that's decent sign and I
didn't mention the component part the
component is a pretty busy guy just like
all the machinery one of the things
you might wonder how do you know which
how many hits or miss you have and and
the nice part about component is is that
you can wrap all your calls with these
loggers and timers and I find it really
useful pattern and we do that a lot and
this is a matrix drop visit library it's
it's a pure Java one we just wrap it in
foil and it's pretty good there are
other fun parts about performance that
cache caches doesn't have to be with
with those two things that I mentioned
it also boils down to things like do you
know about sequences I don't know I'm
still learning and the fun part about
these is that you actually end up
looking at the core closure of data
structures and some of the fun parts
looks like this does anyone know or do
you know what is wrong with this
function if I pin tell you what the
context of who's calling this function
is this is our like part of our
production code and it's a it's it's
literally filtering with a bunch of
conditions and cards is a sequence so
this is a sequence and it's filtering
that sequence we all write this kind of
code right we pass collections along we
pass sequences along and we filter them
and the issue with this without knowing
the context is that it can be slow and
in our case it was actually really
really slow because we were calling this
from a reducer and sequences are not
like if you have 5,000 elements in that
in which we had for a user we have to
sort them and actually sort them in
reverse order and so we started using
our C so we actually factored out this
function instead of returning cards we
actually had a function that reversed
the cards ended in our sake that was
like close to like 30 times faster so
there are little things like this that
you will know and I would urge to you
know as to profile we use a your kit
which is which is a pretty solid
profiler and yeah it's it's it's
surprising to see that you know we use
maps and sequences all the time but when
it when it you know
boils down to the basics like they are
not always the best solution closure has
a lot of lower level primitives and I
alerted you two guys to like look into
that because sometimes it is worth and
like in our case it was worth it
there is a we're still refactoring this
code hopefully in we would like to open
source it and my favorite part is that
the entire caching logic is abstracted
away in a single function and this is
another part of the excitement I have is
like when I look at things like other my
paths have done some caching solutions
in the Java land and in some Python land
and I've I found the abstractions not
like very limiting I had to look at a
lot of functions to tea to understand
how caching works and even though this
was a long convoluted way to get into
hey this is what we got at the end of it
it is just a single function and it
reads like this we take you know UID and
a timestamp in a timezone and we get the
cache name everything else is hidden
from the api's from the quote from our
queues and I like this I kind of liked
how the reason a cache without knowing
how it is implemented
so in a nutshell it is surprisingly
spare I would say six hundred lines of
code for the entire caching is literally
three files it started out with one file
and then we split into three files
because we split into memcache and local
cache as well and one protocol and then
the the nice thing is that we're now
able to look at cache as its own living
entity the cache has a name it has a
version the version essentially what the
version means is that if if you change
the definition of the cache compute we
just mom the version which means that
the cache logic could be updated over
time and then the old cache still will
be valid and the new cat the new guys
with the key will be looking at the new
one so there will be couple serialized
deserialize and compute and I think that
is interesting
seen other libraries do that but going
through that on your own I think it's
fun I think the part of a lot of fun is
that no we have a solid team and
aggregor who was one of the first
engineers on the team he wrote the first
version and then I came in we did some
refactoring on it with memcache and then
we had another engineer who did a lot of
performance Gregory so though at the end
of you know if you look at three years
the whole codebase moved but we didn't
add a lot we actually reflected a bit
and were sort of in a happy place and
the lesson I have for myself and I hope
it is useful is that you can you can do
a lot of these things when you build
something homegrown based on the data
patterns that you understand in your
cache and will quite pleased with that
and the best part the best part is that
so I was working on a sprint where we
were had to do a lot of predictions on
the transactions as well as having a
completely new interface it was all done
and we were about to release we had a
couple days same story you've all heard
it
and then he was slow and so we look back
into how can we make this feature called
cards faster and it turned out we had to
go back and add one map entry and one
function and when we saw that that is
when we kind of like took a break and
you know went in you know went into the
video game room and then said okay it's
worth it and having those little moments
I think is helpful because you kind of
see the whole thing coming together at
the end so there are a lot of things
that that are not perfect especially the
dag as you've seen in the first part the
dag is literally an ordered list it
would be really nice to have a library
that generates that to have dependency
explicitly mentioned if you've seen
Stewart Sierra's talk yesterday he does
mention about component I think a lot of
goodness could be used from that library
we actually are using it but not for
dependency one good idea is to take our
cache map and actually use each of those
map as a cache component so instead of a
big giant map you know
honey a couple dozen entries that we
have we can use each entry as a cash
component and have cash dependent on
each other so that's a nice idea we also
can do some work on the optimizing the
load cache and maybe async computation I
haven't thought about it but we're
working on an async library so maybe we
could use that libraries primitives in
our compute function the key here is
that anything could be swapped out
because we decouple compute serialize
and deserialize is possible to swap out
things and I think that's that's a nice
property to have so that's the team I
would like to thank a lot of the work
goes to Gregor he did the original code
so when I first came in I saw like hey
there's like twenty five twenty thousand
lines of code who's who wrote this and
he was like okay that's me and that's
when I knew that okay this is I have a
lot of work ahead and that's the fun
part of being enclosure teams because
you look around and then they're just
like three or four people in that team
and to be part of that team is
empowering a thing that you learn a lot
and I've learned a lot for sure and
Gregory Sizemore he he helped with the
performance but I think when I first
looked at the delay blog I it wasn't
quite apparent for me then he sat down
and he actually implemented the first
part and then I looked into what he'd
done it totally made sense so big thanks
to him as well and of course the rest of
the team but there's one person I want
to thank eyes his name is noodles and I
know it's it's actually very serious the
the thing with if you worked in a splint
this is nothing to do with caching but
if you work in teams that are really
small and especially in the team that
has that bigger codebase it is important
for the tech lead to drive a direction
have a set of qualities right I think we
all agree to that you can't just check
in code and closure because closure has
a sense of style and you know there's
there's a lot of careful careful Ness
then you have to take care it's not like
other languages closure has to be a
little more like now I don't use the
word handheld but it has to be taken
with care
so I learned a lot with github reviews
PR requests and I would like to thank
noodles because noodles is the best
worst PR reviewer in the world and yeah
it's a big thanks because because
without noodles doesn't that is Gregor's
cat by the way I don't think I would
have learned that much and I urge all of
us hopefully that if we're working on
teams that are small and with different
skill levels Pyrrhic pull requests are
actually really useful it's really
useful on a final note I want to end
with this I've been doing closure for
about I would say two and half years
though I wrote my first hello world
about three years ago and I quit my job
after that because I thought because I
thought okay if I can do a hello world
and and a simple rest server in about
you know half an hour I probably should
like quit my job so a lot happened then
and and last year I went to Japan I was
in Tokyo and if you've been to Tokyo you
know a lot of good things in Tokyo there
is one great thing of many things is
that the subway system right there
subway system is is an excellent example
of a complex system that is simple
efficient and precise but I'm not
talking about the subway system I was
actually sitting there with my camera
and then I saw this lady holding this
little box with the cat and I remember
at that point that my writing teacher
back in Miami FAA's fiction that is said
that you might be working in a novel for
like three years five years ten years
and you will be rewriting because the
process of creativity is to revisit and
revisit and it is frustrating you will
bang your head I actually banged my head
for six months and closure before I
finally thought I should take it
seriously and the best part is once you
kind of get it I think we all share that
right I think I don't think we will be
here without that once you'd get it
there is this moment of silence and
that's what I found is that that's it
and I find that once in you know three
days maybe three months and that's what
I'm after
I do want to share this because I'm
still in fine I'm still trying to find
them and it's worth it because closure
gives you that small small cherishes I
would say for me delay was one for sure
and and a few other things like local
cash when you find that it's what I
think a language is worth learning for
that so I just want to end that because
hey otherwise I would be back in my Dark
Ages I will not mention the language but
we know where that is
well so thank you
so think I have about five minutes if
you have any questions oh it's just like
one file we just stopped because we have
it back with the protocol we just have
to switch our data storage provider so
his question was what happens if you
switch memcache with Redis I didn't want
to focus on why memcache why Redis
memcache is just simple for us but we
use memcache as a pure data storage
provider that gives you eviction a solid
eviction for free and we didn't want to
deal with eviction as I mentioned my
first well so it is simply a matter of
switching and writing the gets inputs
the other question I guess not all right
thank you again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>