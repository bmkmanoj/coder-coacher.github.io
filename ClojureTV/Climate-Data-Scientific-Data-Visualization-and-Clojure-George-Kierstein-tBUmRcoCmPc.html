<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Climate Data, Scientific Data Visualization and Clojure - George Kierstein | Coder Coacher - Coaching Coders</title><meta content="Climate Data, Scientific Data Visualization and Clojure - George Kierstein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Climate Data, Scientific Data Visualization and Clojure - George Kierstein</b></h2><h5 class="post__date">2015-11-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tBUmRcoCmPc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is George Kirsten and I work for
I'm government contractor for global
science and technologies that works for
the National centers for environmental
information all like lots of acronyms
but that is the former National Climatic
Data Center and the four most like
location for pretty much most of the
world's climate data and I'm gonna start
off warning you that there's not an
outline because that would miss be
misleading as if I had some linear
progression because I'm going to be
making gross
broad generalizations that are likely to
piss people off or at least be
controversial
instead it's kind of going to be more
like this I'm gonna start at a very high
abstract level with a completely
seemingly unrelated topic kind of drill
down through data visualization and
contrast it with what we call Big Data
now which is typically industry and she
sort of then contrast it with the
science and it's visualization problems
and then we're gonna look at climate
data and then we're gonna demo the work
that my co-worker here egg Davis and I
have done to visualize multivariate
climate data with very different types
of datasets so we all know this in fact
that was my best attempt and that's
supposed to be a dove sad but you know
obviously there are some images and some
ways of visually communicating that are
so profound that they really do change
people's minds and hopefully their
behavior we'll see about that last one
they long run with respect to climate
change so I'm gonna go through what
brains have to do with it in part
because back after the first tech bubble
popped I did what most people did which
was going back to grad
school and I did that in computational
neuroscience which has a lot of brains
in it so both and the point I'm really
trying to make here is that our neural
architecture has left us with some
subtle cognitive biases and let's be
honest there are some not so subtle
cognitive biases as well they're in play
here
so it's arguable that at least half of
our brain is devoted to or involved in
visual processing it's not going to go
through any parts but that kind of gives
you a gestalt overview of what people
think the visual parts of your brain are
which is quite a lot now basically
although we have this seamless
perception of the world or at least
seemingly so we in fact completely
manufacture it all the time it's a
complete lie we have a tiny this is a
ISO density map of the retinal ganglion
cells in our eye and this shows you the
out the shape of kind of power retinas
laid out which is circularly symmetric
and it kind of falls off pretty rapidly
actually inside your brain it's a log
rhythmic and pretty much all of your
vision comes from that time point in the
center which of course is why if we have
to look at all over the place
and this is what it looks like if you do
a reconstruction of what your very first
part of your brain sort of after it's
represented in cortex you can see that
the only real definition is in our eye
and it falls off pretty rapidly from
there but that's just the beginning of
the level of complexity that goes on
when your brain tries to give you the
seamless perception of the world because
that there is the retina with a lot of
blood vessels all over the place and
they're obstructing the actual like
retinal ganglion cells so that nice map
is not actually a nice map because it's
covered with veins that block certain
cells and it's not like you sort of have
a static eye what this is is a picture
of something called
gods unconsciously to you when you look
at anything and you take it in your eye
in the time course of 20 milliseconds or
less actually traces a pattern like this
to pick up details so that you can
translate it into a seamless view of a
larger scene then your eye could ever
take in at once and it just keeps
getting more complicated from there this
is a picture of the dorsal and ventral
streams of where kind of visual
processing starts to carve out and
bifurcate so it really has left us
though with an impressive array of tools
that we kind of unconsciously leverage
all the time and that is really handy
because we use it primarily and the
sense of data visualization you know we
have comprehension of the world and we
want to visualize it and we've always
been curious and try to formalize it
some of these are from art some of these
are from you know architecture it's
arguable whether or not the actual thing
we're looking at here is revealing a
fundamental principle of nature or just
the types of patterns that are very well
attuned to our neural architecture for
vision for example that that picture on
the bottom right with the golden spiral
ratio is perfectly sort of suited to our
particular kind of like fall-off because
the real center of action is the one
right where there's the cluster and
you're looking at it but at the same
time you still get the sense of motion
all in one Gestalt view right
immediately because you don't really
have to look at it all to notice that
the far sort of left is got something
going on but the real Center is where
you're really interested so there's lots
of things like that in the brain and
it's pretty interesting that we
ostensibly have these huge cognitive
biases so and we've always try to
leverage them this is an awesome map
perhaps one of the most famous best
visualizations ever and it is a map of
Napoleon's invasion
Russia in nineteen or 1812 by Jacques
Menard and it's beautiful and expressive
and looks really modern in some a lot of
sense senses so post the Industrial
Revolution we pretty much have only
gotten more and more data and it pretty
much looks like that's just going to be
the way it is and we need tools to
interpret this complexity so luckily for
us computers came along and they were
very popular but unfortunately like the
PowerPoint slide here that I think I got
off of some random 10 worst PowerPoint
slide sort of website it is really not
very helpful and in this era that that
sort of idea and it was coupled with
success accessibility of these tools
kind of led to a confusion around what
even they good visualizations should
look like and how people should really
use them I soom this was an improvement
of some kind perhaps not a very
well-executed one though and of course I
couldn't help myself
this of course is Edward Tufte and along
with many others in the 50s and 60s he
really started to see the possibility
that the modern computing you know and
all of these other sort of tools that
give us way more power and
expressiveness than we've ever had
before
could be formalized using a lot of
techniques and I'm just gonna say I'm a
huge fan now this is the first time I
really encountered this idea of
quantitative information and data
visualization and it left a huge impact
it really did leave a huge impact for a
lot of people and he laid out pretty
much many of the fundamental principles
that we ultimately use today and I'm not
gonna get too technical or too nitpicky
because I'm sure that I will make
massive mistakes because it's an
enormous discipline now and lots of
subfields like this you know here we
have a nice nicely thought out you know
diagram that demonstrates some of the
downsides and upsides of how to do
things how to do layout and these come
from places of you know cognitive
neuroscience influences like attention
studies and so forth and we're starting
to make some significant progress in how
to formalize really understanding so the
most effective ways to visually
communicate with the tools we have like
that and we could go on all day and I
would love to show lots of sexy data vis
you know but we don't have time so I'm
gonna move on to Big Data or mining
analytics for fun and profit which
really kind of makes sense I'm gonna try
to do my best movie voice in a world run
by computer
all right well it makes sense that we
would me really need to start to
leverage these because we have more data
and that any one human can just sort of
look at in fact it's come a global
effort to even understand the volumes of
data we're producing and eventually if
not already the volumes of data that
computers are producing for each other
and our expectations now of true like
instantaneous communication and so forth
have kind of left us no choice but to
become really good at this particular
style of communication and in my opinion
it's probably the best chance we have to
really tackle some of these hard
problems like climate change so this is
probably the most contentious and
hilarious sort of point a part of the
talk because I'm gonna make broad
generalizations which are potentially
inaccurate and or a little bit like you
know controversial we can debate about
it outside so on the developer end of
the world for data visualization and I'm
pointing this out as a contrast to the
situation that climate data
visualization is going to have we can
see that in some sense we've all we've
had this nice path starting from the 50s
and 60s with networked computers where
computer scientists have generated data
but in a pretty systematic way and it's
a kind of interesting to point out that
ARPANET was in 1969 now as a point of
contrast the invention of the Telegraph
1835 arguably the first sort of you know
weather measurement and prediction that
would be considered modern was 1861 and
by 1901 nation-states had begun oops
yeah nation-states had begun to actually
systematically try to put this you know
into our cousin keeps keep records of it
and it worked at least
paper so developers also love
interoperability whether or not we
achieve it that's always a contentious
point but at least we try way more than
I think scientists do when they handle
their data and the tools we want we
don't want to you know rewrite the wheel
every time we encounter something and I
think developers as a community really
do wanna share like there's just no
value in kind of hoarding your little
you know nugget of gold when all of us
could make use from it so and there's
plenty of other things that could be
thrown in here but I think that kind of
covers the point and we all know it's
not entirely accurate certainly not that
easy
now what about data vision science I
argue it's dragons all the way down why
because of physics for instance we start
off with instrumentation they're often
incredibly complex you know there's a
tiny guy down there it's just for scale
that's the Large Hadron Collider they
often measure meta you know aspects of a
phenomena not even the phenomena
directly itself which leads you even
more complicated to instrumentation to
validate something or not they also have
to be calibrated because they're
sensitive and very very you know
expensive and complicated machines
these are diagrams of two satellites
they're platforms for earth science
observation and they have their own
problems with calibration because you
can't just go hey some sensor seems to
be weird why don't you pop out there and
figure out what's going on you know or
at least not very easily and in this
case the calibration aspect for
temperature is this weird blackbody box
it says dark as can be humanly possibly
made and it aims its sensor into that to
calibrate how off it is and they
fluctuate all the time even if they're
not gone or down because there's all
kinds of particles and things that throw
off their ability to do their job in
space
not to mention these datasets are really
difficult to merge because they're so
different from each other they have an
entirely different sort of perspective
history and so forth which makes it an
open challenge still for how to do a
systematically effective job and making
sure that you can tie them in together
and scientists this is very diplomatic
but they tend to be insular and they
stick with what they know and more and
so whether they were yeah oops lost
track oh so consequently this is a
pretty common visualization this is
modern in a way it's really precise
because it demonstrates sort of the time
course dynamics of training up a neural
network and you know it doesn't look too
bad but unless you really understand
what you're looking at the methodology
the neural network architecture and all
the rest of it it is really difficult to
parse what their point really is on the
other hand this is a gorgeous image made
the cover of Nature Neuroscience in 2000
and is still basically one of the most
famous neuroscience images ever made and
I'm almost tempted not to tell you what
it is because you have this intrinsic
feeling because of its elegance the
symmetry to kind of not care but I will
anyway so your brain is basically a
millimeter thick kind of gelatinous
sheet about a metre Wow
you know meter on a side that's kind of
squished and crumpled up into your head
so when you see the picture you know the
early picture with all those folds
that's because it's just basically the
crumpled of your brain fitting into your
skull now in the very early stages of
vision which is what I focused on in my
work there are these little cells that
are tuned to respond to stimuli at
certain angles that's all they like
they're just angle receptors basically
and they get all excited when they see
an angle but since you're on a sheet you
kind of have a weird tiling problem
right if you're gonna have lots of them
how do you do the de loop you know
when you get back to this loop where do
you put it to in order to actually lay
it down on a map and it's not really
entirely clear exactly how your brain
decided to do this on the right it's
like a little that's the millimeter of
your little chunk of cortex and they've
you know plotted out these nice
left-right columns but the receptors
live in these nice vertical orientation
columns and you can kind of see the
orientation on the side of the right and
on the left you see how they figured out
how they took that picture which is kind
of creepy so but it really involves
a lens and a camera and some light the
problem with that image is kind of
profound so my adviser Eric Schwartz he
came from a high particle physics
background and was very concerned about
the lack of rigor in most of
Neurosciences research because of the
biology driven kind of history of where
cognitive neuroscience came from so he
wanted to make sure I knew that you
could introduce weird symmetries by
blurring things that you know this
wasn't in fact a systemic you know error
that basically made that research not
say anything useful because it could
just be a systematic bias on the left is
the Monte Carlo simulation they did of
the optical imaging system and on the
right they just sort of wanted to make
sure that they're in a model the third
Monte Carlo model produced the same
kinds of results from the original paper
and it looks like a pretty good fit and
this is a animation from John Paul
Minnie myko lab partner who demonstrated
that as you adjust the lens he's kind of
running through the focus depth of the
lens you can see that the little
pinwheels in the symmetry all just move
around like there is no stability within
that system with that particular
recording device and it's really
remarkable that today like even today I
found a paper that is kind of still
interested in you
these these orientation columns that
didn't even acknowledge that this
systemic bias could even exist they just
sort of blithely assumed that the
original paper looks so great that they
should just go with that and continue to
do research which is a real problem in
science because over time like these
timescales of adoption of real results
just don't percolate out due to the kind
of ivory tower effect and that is in
real contrast to how we tend to
publicize and want to get on top of real
problems that are producing systemic
bias and causing all of our stuff to go
off the rails and in short the worst
part is that scientists are rarely
developers I've seen some incredible
code in fact in the building a co-worker
of mine has removed I think 345 goat
cheese to date from this one piece of
code that was written 25 years ago I
think started you know to by one person
who was then passed it off more than
more than once and in fact I worked on a
project not too long ago with scientists
who I was interested he's you know
retiring but he had never even heard of
the concept of version control he was
into it once I explained what its use
was but he was like he's like that
sounds like a great idea
you know that's pretty typical and how
about climate data then to me in a real
sense it's only really become relevant
to us until like very very recently and
that's had some real side effects this
is a quote from one of my favorite
thinkers of planetary change Frank
Herbert and I think that's really quite
true and pithy at the same time so
climate data I feel is unique and some
really interesting and important ways
which is these are the longest
continuous data sets humanity have ever
collected they go back at this point
somewhere the oldest ones are at least
150 years old and they have a
distinction perhaps a little bit
different from you know know in business
analytics
and business than big data where the
provenance is scrupulously monitored and
it's a very intrinsic part of their
process not only because if you have
data going back 150 years it's not like
you can walk down the building and ask
someone why you know they've made a
choice and it didn't work out too well
because they're dead and of course it's
a global effort and in a in a way that
is really truly a global effort pretty
much an every nation state that has the
money or the capability has satellites
in orbit doing this work and they're all
happy to shove it at NCU DC or nzei
pardon me we just changed so it's still
having a cognitive dissonance about that
but it really is this global effort you
know we get data in from Indian
satellites Chinese satellites and
everybody's happy to you know be a
community on this particular sort of
topic of how we do good job which
satellites go where you know so we have
global coverage it's a real cooperative
kind of thing because basically it needs
to be as a scale of the problem and
these generational scale impacts are
something kind of new I think to how
people even perceive data because
basically the brain part of the talk is
your vision is far from the only place
where you have these blind spots that
are due to your cognitive you know
architecture shall we say like so and
that's a real problem but I'm gonna
segue a bit into you a bit of history of
the building because I think it's pretty
interesting this is on the top where the
all of the data climate data for the
world used to be stored it's a lovely
old building in Asheville North Carolina
that you know had an incredible amount
of paper data punch card data microfiche
data and it was started basically in
NuLu yeah my notes say just cuz I want
to be specific this started out in New
Orleans in nineteen
thirty-four let's try a transfer to
Asheville and 51 so by then there you
know was millions of paper records and
so forth and in 1993 of course they
moved out of the building which is a
shame because man it's beautiful inside
I got to say but at this point the NOAA
and the NCI Institution archives
basically 99% of all NOAA data and I
love these statistics
they've got 320 million paper records
2.5 million microfiche records and then
petabytes and petabytes of data so let's
see and there's all the same issues that
you struggle with in science itself like
for instance this is a picture of one of
the instruments that our data set you
know generated data from and it had a
weird problem in fact egg was part of
this story although it's great anecdote
at one point the crn people who is the
climate reference network that's
actually responsible for these
instruments they found some really some
weird glitch in precip data in one
station in Florida so lead scientists
being an instrumentation specialist
doubled down and trying to figure out
what went wrong with the instrument and
you know many other attempts were tried
and try to think through failure modes
and so forth and it certainly was not
just a sensor failure because there's
three weight sensors on there and
they're all in agreement so they sent
somebody down there and when they did
they discovered that tree frogs had
decided that that bucket was the best
place for them to live and of course
they'd hop in and out of it occasionally
introducing weird biases into the data
that would otherwise be really difficult
to debug from afar and not to mention
along with the generational time scale
of these types of datasets which really
are our first attempt at understanding
how to pass data down through the
generations in a way
it makes sense and it's sustainable and
useful those generational effects have
caused all kinds of problems that we see
commonly but don't really hurt quite as
bad like someone decides to change the
header and your data you know imagine
that happening over lifetimes and you
know we typically only keep jobs a few
years so forth and so on you know that's
a little less true within the scientific
community but it still happens pretty
frequently the like not built here
syndrome come in you like look at the
insanity that you got left here just
eager to fix it but you're passing that
on to someone else and they're passing
it on to someone else ad infinitum
and it's hardly been happening for you
know at least 70 years at this point and
it's not pretty not to mention that the
data changes you know these instruments
become more sophisticated every year you
put a new satellite up there they've
improved the sensors they've added a
whole bunch of other kinds of sensors
and so you can't always easily compare
the the data that was 50 years ago
missing most of what the new data has
and again these datasets many of them
are manufactured from code and they have
their own idiosyncratic choices about
what to do and sometimes even the same
nominal concept like temperature is
thought through in a completely
different way between two different
datasets so you almost have no like easy
way to figure out how to merge them some
of them are do some of that is due to
the just the physicality of the
measurement device like a lot of
satellites have a five-kilometer little
pixel and then the new ones have a
one-kilometer little pixel and you want
to merge those two to try to understand
something specific how it's not clear
and I want to be a little fair you know
because I'm painting a pretty picture of
how sort of really a difficult and
frustrating it can be to actually work
on these datasets but it's not like they
don't know and it is constant source of
problems that they really know they have
you actually address but you know we
don't live in the type of country that
fully supports this notion and backs it
with the kind of funding and respect
that I think it deserves and that means
that this is a pretty typical
visualization of climate data now it's
not bad in a sense but you can't really
tell what the error bars are and worse
it's one it's one dimensional and it's
probably just one section of one data
set and there are hundreds of these data
sets that all have facets of you know
the climate problem that are looking at
it in different angles and that is not
great when you're trying to express this
to someone who is not a climatologist
I'm not a climatologist I barely
understand a lot of the datasets that I
work on and that's not helpful when you
want to make sure that everyone else in
the world can work on this problem -
it's just so multi-dimensional that egg
and I thought how hard could it be to do
a better job of merging two very
different datasets well or in the best L
voice for me so this is our data we have
two wildly different data sets one of
which with completely different
instrumentation underneath them the one
on the left is from the climate
reference network which is you know I'm
trying to address a really great data
set because they are the first one -
from the ground up choose to try to make
this a beautifully designed pristine
50-year period data set their goal is to
make this clean and continuous and well
thought through so that 50 years from
now you know you will not have any of
these fractured problems and you know a
pope fully it will continue but that is
their goal and that's a laudable one but
it's sad that that's the first time that
you know that's becoming a conscious
thing to try to do now their ground
stations so the little dots you see are
where they're located geographically and
the region here we're looking at is
the region of data that the
visualization is going to be confined to
so on the right we've got this thing
called sweetie hale we're going to look
at hale data because hale data is one of
the most damaging and important and
types of data you know types of weather
effects to understand and it's typically
still derived from radar data which goes
back a long time
I think radar was built in 1939 and it
is still the great like the main
workhorse of how we detect a lot of
weather patterns regionally that is a
mesocyclone
and this sweetie actually stands for the
severe weather data inventory one thing
that I think most people don't
understand because it's difficult to
communicate this is that most of the
data sets that people think you know
encounter when they think of climate
data are actually generated out of an
algorithm that someone wrote there's raw
weather radar data underneath there but
and that kind of gives you sort of the
raw picture but then after an algorithm
is run it does things to try to detect
hail by looking at reflections in you
know the clouds in a mesocyclone type of
weather pattern and ironically it's so
detailed that they just decided that you
know they take a quarter and then they
decide to sort of say oh well of course
you know they're hail circular so when
they give you a measurement size for how
big it might be they're saying like you
have about a quarter size of a quarter
because there's a great language yet to
also describe the types of you know real
complexity of these weather phenomena
themselves still and basically then our
approach started off with a hand-rolled
framework which was quickly abandoned
for luminous it provided a lot of extra
affordances that we really wanted to
avoid having to hand roll herself and it
really removed a lot of the comply
unnecessary complexity for us trying to
get a date of is going and we picked
react because react is awesome and I
can't imagine that people don't want to
use it but we went with reagent at first
and been quite happy with it but after
this morning's talk I think oh next is
definitely gonna get checked out for a
lot of great reasons and we used this
awesome JavaScript library called math
box and it's amazingly well written
we're actually on the first version he
came out another one and just blows away
the first version decided to work with
that and it's basically a lara berry
that targets WebGL so it's running super
efficiently on your GPU with 3GS and T
query if you have more questions about
that eggs gonna have to answer them but
it gave us a really nice way to have a
good visualization tool that mapped into
the three dimensional multivariate kind
of space that we were looking at that's
closely you know sort of fitted to the
type of data that we're going to be
using so here is just math box running
random data so it's really nice and you
can move it around if it'll let me oh
there we go
and it has other affordances but you
know you kind of get to the point I'm
gonna show you a little bit of the
wrapper that we needed to use in order
to be able to get this - well hold on
let me go back because there was a
little a couple extra points and
unfortunately they're on my wait a
second anyway there were a lot of
challenges trying to integrate a
stateful a really stateful library into
a nice sort of state less you know
framework and concept so egg here wrote
this part of code which is an attempt to
write a wrapper to make it more stateful
and there was another pain point here
where once that happened we still had to
you know avoid a lot of problems that
are combining reagent with this as well
where it's got you know an impedance
mismatch because it's watching for
certain things rat
our atoms and how do we had to create an
adapter layer to be able to keep these
things all playing nice and to make sure
that the math box internals and the
reagent sort of internals did not really
have to know about each other to kind of
keep it nicely separated and it's nicely
sort of condensed and expresses so you
don't see a lot of the problems we chose
to keep an instance around and you know
basically it's pretty nicely updated
with the reagent atom underneath it so
here is CRN data basically these are six
stations in that region that I showed
you on the map earlier they're lat/long
you know organized so it allows you to
see some diurnal patterns this is about
a week's worth of data of temperature
for each station and you can kind of
tell you know here's the day you know as
the week rolls on if we let it play long
enough you'll actually see seasons you
can clearly see the sort of seasons
segue and so forth and basically since
we are aiming to represent more than one
data set we really needed to leverage
something like this in order to kind of
do go go okay fine I'll use my mouse in
order to merge them so this is pale data
being visualized with the CRN data so
thus witty data set basically is a bunch
of severe weather events they keep track
of what happened there's it's a little
bit misleading in detail because there's
probabilities associated with it because
it's a calculated algorithm so they're
basically saying with a certain
probability we are pretty sure that hail
fell right here at this time and that's
about all they can do of course
so what you'll also notice this really
interesting I think when you have
multivariate kind of data like this is
that you can start to see you know
suggestive regional patterns you know
there's one isolated storm here you know
that's pretty far away from these other
you and then you get these clusters over
time and of course you will watch
although it when I not playing it that
long because looking at just the other
things done yes yeah yeah
this is lat long division it's not to
some particular like scale at this point
though and the Hale blobs don't indicate
sort of what exactly the probability of
these events were we just went with
somebody said that it was very likely
that it was likely at all that Hale
happened at this time but it does give
you a way to sort of check for you know
it gives you more information about
either one of these because you could
plot the Hale thing you could plot all
the Hale data alone and you could plot
the you know seer and temperatures alone
but it doesn't give you a lot of ability
to say oh you know visually and kind of
have an intuition that you know these
this particular type of storm in this
particular area or this particular
weather pattern was quite regional or
this one was quite broad and it is a I
think a better type of approach in
general and we didn't get done with all
of the polish that we would like to but
I think it does it you know lend itself
to having a slightly better
understanding of how these two types of
datasets that are both very highly
provenance to mean certain things to
certain people like might be able to be
merged without you know dealing with the
nitty-gritty of the fact that they don't
really they never were intended to play
nice together yet this
complex 3d visualization for a long time
right what he said yeah so thank you and
if there are any questions anybody
question</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>