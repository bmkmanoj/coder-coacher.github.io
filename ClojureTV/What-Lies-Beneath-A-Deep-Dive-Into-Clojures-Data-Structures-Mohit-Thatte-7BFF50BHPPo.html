<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Lies Beneath - A Deep Dive Into Clojure's Data Structures - Mohit Thatte | Coder Coacher - Coaching Coders</title><meta content="What Lies Beneath - A Deep Dive Into Clojure's Data Structures - Mohit Thatte - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Lies Beneath - A Deep Dive Into Clojure's Data Structures - Mohit Thatte</b></h2><h5 class="post__date">2015-07-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7BFF50BHPPo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning euro closure my name is
Mohit I'm here from India and I get to
start three minutes early I I don't know
what the ten minute break was about
anyway so I'm going to talk about what
lies beneath it's a deep dive into
closures data structures i'ma hit that
they on twitter and past Fri on github
and slack okay so for some motivation
when I come into work every day what do
I do
and what do most of us do right we set
up our nice standing desks and MacBooks
and screens and everything and then we
start coding away and when we're coding
what are we doing we're writing features
and we're fixing bugs then we're writing
features then we're fixing bugs so we're
essentially at least I am essentially
working at a really high level of
abstraction I'm working at a business
application level right so I'm sitting
on this big Tower of abstraction where I
write programs that use maps I I depend
on an interface and then beneath that
there is an implementation and beneath
that there is a platform and a machine
and it's Turtles all the way down so
what I wanted to do today is dig a
little deeper and figure out how these
things actually work what's what's the
magic that actually makes it work so I
came up with a quote any sufficiently
advanced data structure is
indistinguishable from magic
with apologies to Arthur Clarke so the
idea is that I keep hearing about the
amazing immutable data structures
enclosure there's H a.m. T's and and
bitmap vector tries and so forth so how
do they actually work that's the idea
I'm sure all of you agree with this
anyone who does not okay
but this also so immutability is a good
thing performance is necessary because
we want to deploy your code in
production we want it to be fast these
two things are in tension you can have
one or the other but typically it's
difficult to get both so what I want us
to do is to change perspective to look
at things from the other side
and by other side I mean try to think of
it as a data structure designer as a
language implementer how would you
tackle these problems this picture is
called Earthrise it was taken by the
astronauts on Apollo 8 when they were
orbiting the moon so this is what the
earth looks like if you're sitting on
the moon so Chris Okazaki says that
functional programming stricture against
destructive updates is like taking away
our master chefs knives and coming from
him that's a very valuable statement
because he is the master chef pretty
much in the world of functional data
structures so he's saying it's really
difficult let's talk about the challenge
what does a data structure designer have
to deal with right so essentially you
are given an abstract data type what is
an abstract data type in abstract data
type is a name so Q and it's a bunch of
operations and invariants
so given this your challenge as a data
structure designer is to come up with an
implementation for this now one thing I
wanted to point out as an aside is that
most languages that we commonly use make
it really easy to give something a name
and to give something an interface but
it's actually pretty hard to to specify
invariants in the language so I could
say that any implementation of a Q must
have these operations but how do I
assert in my language that it must be
first in first out or last in first out
depending on what it is so I'm really
excited to see languages like Idris
which apparently let you do that in the
type system anyway that was in a side so
the challenge as a data structure
designer is we want a correct
implementation we want an immutable
implementation and we want a performant
implementation and x marks the spot so
as a data structure designer I have to
do all these things in order to get it
right
okay along comes Chris Okazaki
and he says challenge accepted so way
back in 1996 Okazaki basically was doing
his PhD and he did a literature review
and he found that a lot of algorithm and
data structures text books they say that
you can use any language you want as
long as it's imperative because all the
pseudocode that you see has assignments
and destructive updates so he basically
quotes Henry Ford and says that Henry
Ford once said that a customer can buy
any Ford that he wants as long as it's
black so okazaki decided to change that
and he wrote his thesis about functional
data structures and this book is a
really nice read some key ideas from
their structural sharing bootstrapping
and hybrid structures so hybrid
structures came a little later but yeah
structural sharing is this idea that
when you want to mutate a data structure
what do you do so one very simple thing
to do would be to copy the whole thing
and make the mutation in the copy but
that would not be very space efficient
so what you do is you only copy the
things that you need to copy and you
share structure with the original one
okay this is how it would look if you
did the structural sharing for a linear
structure for a tree structure you get
even more sharing because essentially
you only have to copy the path from the
root down to the leaf that you're
modifying right so as you can see in
this in this picture the entire left
side of the tree from C onwards is
shared so the more branching you have
the more sharing you have essentially
and and that makes for pretty efficient
persistent data structures so as a data
structure designer again life is all
about sharing the second idea is a
structural decomposition so it's
essentially saying that you can build
bigger and bigger data structures using
smaller components so
basically if I already have a list data
structure can I build a map using that
can I build a set using that I can and
yeah there are some other ideas there
but I'm going to skip those the third is
hybrid structures so hybrid structures
are essentially saying that let's take
some properties from this data structure
some properties from this one and make a
better one that does something more ok
so these are the key ideas ok so that's
enough background let's dive in what
data structures do we have in closure so
we have lists which we use primarily for
macros vectors for sequential random
access maps for structured data and sets
for sets so what I'm gonna focus on
today is essentially vectors and maps
and the reason for this is that lists
are implemented as consoles and that's a
very basic concept in pretty much every
Lisp out there and sets are literally
maps of the key map to itself so if we
know how maps work we know sex work I'm
going to start with maps when I first
got into closure map always brought up
this picture in my head anyway so what's
the interface so we have pretty much
these basic operations you get something
out you put something in you remove
something and you can merge maps
together right what makes a good map so
typically the map implementations are
really lookup focused right so how
quickly can I find something in the map
right so the idea is that you you want
constant time operations independent of
the number of keys so as your map grows
larger you don't want things to slow
down you want efficient space
utilization even with mutation so you
want to take advantage of structural
sharing as much as possible and in a
dynamic language like closure you want
to support arbitrary objects as keys and
arbitrary objects as values so ideas how
do we go about implementing this
now we are all data structure designers
and we have the ADT in front of us and
we have to do all of the things that are
required so how do we do it okay here's
the simple idea let's just stick the key
value pairs in an array right what that
would look like is something like this
and then when I wanted to look up a key
I would just jump through the array
until I found the key or not now this is
a really simple idea to implement also
you can just do a copy on write and in
it all works but the time complexity is
pretty terrible as your array becomes
large your program is going to just die
space efficiency not so much because as
we saw the amount of structural sharing
possible with linear structures is not
not so much can we use objects as keys
yes
so how do we do better please to the
rescue like the Lord of the Rings and
everything else trees always to the
rescue interesting aside since we were
in Barcelona since we're in Catalunya
Ramone UE is a philosopher from the 13th
century he actually created this tree of
science
so he actually categorized all the
sciences into trees and it's called the
arbol de ciencia so I just wanted to say
that I think humans have been thinking
with trees a categorization and breaking
things down since forever
it's the oldest idea in the book so
binary search trees right how do we can
be implemented map that is backed by a
binary search tree yes we can so let's
put key values at every node and compare
the key if it's smaller go down the left
if its larger go down the right this is
a again a very basic idea but binary
search trees can regenerate into lists
depending on the order of insertion so
if you put everything in sorted order
you're just going to degenerate down to
a list again and this is no different
than the array that we saw before so the
worst case complexity would be order n
space efficiency possibly if it's
actually a tree then we would be able to
share stuff but if it be generates then
no and objects s keys yes
so the next question how do we keep our
trees balanced again a topic of a lot of
research over the years we use balanced
binary search trees and there are a
bunch of these there's AVL tree splay
trees red black trees closure actually
uses red black trees always balanced
100% money back guaranteed created by
givers and Sedgwick in 1978 they have
some invariants so an interesting piece
of trivia here the red and black colors
are completely arbitrary they could have
been any two colors but the printer only
had red and black so they chose red and
black and unfortunately when it went to
print the red didn't even show up so
yeah four invariants every node is
colored either red or black the root is
black a red node cannot have a red child
and every path from root to an empty
node must contain the same number of
black nodes now it's this third and
fourth invariant that really give this
tree balance and there are some really
beautiful mathematical proofs of this
there's professor tim roughgarden who's
done a course on Coursera I believe he
has an amazing explanation of why this
tree actually balances itself so I'd
encourage you to go and look at that if
you're interested okazaki in his book
gives this piece of SML code which looks
like complete magic but essentially it's
the four cases for rebalancing a tree if
it does get unbalanced right so the idea
of a red black tree being always
balanced is that if you insert something
and it breaks one of the invariants then
you must rearrange the tree so that it
does not break the invariants anymore
and that's what gives you the balance so
I find this piece of code really
beautiful because it corresponds very
well to the geometry of the tree but
some might say it's completely
unreadable so red black tree makes for a
pretty good map you get log in search
time you get space efficiency because
it's a balanced tree so you are going to
get a lot of sharing and you can use
objects as ease and closure sorted Maps
actually backed by red black tree so if
you actually use a sorted map
constructor you're going to get a red
black tree implementing it but there are
still some constraints the primary one
being that the keys must be comparable
and not all things are comparable
secondly keys are actually compared at
every node and this can be expensive so
when we do Big O notation we often say
things like equality comparison is
constant time but that's not necessarily
true if you have humongous objects it
might not be so how do we do better we
use this idea try the word try coming
from retrieval so is it pronounced tri
or tree I don't know I'm just going to
call it try so that we know the
difference between the two so the idea
here is that you do not compare the
entire search term at every level what
you have instead is you have nodes and
you have branches going out of that node
for alphabets or symbols in your search
term right so if I want to search for
the word PAP in this tri what I'll do is
at level 0 I'll take the first symbol it
matches an outgoing edge so I'll go down
to the next level then I'll take the
second symbol it matches an outgoing
edge then I go to the next level it
matches and finally I find an end of
word leaf node so I know that the word
ta P actually exists in this try
there are optimizations on this there is
a really nice paper called the world's
fastest Scrabble solving program they
came up with a data structure called dog
directed acyclic word graph which was a
compressed try pretty cool data
structure so essentially it's a finite
state machine right if you remember your
discrete structures that you have a set
of symbols you have a set of nodes and
you have a set of edges going out of the
node and you essentially have this
function that tells you I'm at a node
I'm looking at a symbol where do I go
next
that that's the real real challenge and
implementing a try
so let's go to implementations how are
tries actually implemented the first
idea here is lookup tables you associate
each symbol with an offset so if you had
a through Z then a would be 0 Z would be
25 and then you just have look ups with
pre-allocated arrays ok this image is
actually from fill Bagwell's paper fast
and space-efficient try search is really
nice read I highly recommend it so if
you're looking for the word a DD ad what
you do is you start with level 0 since a
has offset 0 you go to that node that
points you to the next level then D has
offset 3 so you go there and then D has
offset 3 so you go there and so you just
go down levels until you find an end of
word and that's how it works
clearly we have a problem though that we
need to pre allocate these lookup tables
for the length of the symbol table right
so we need 26 pointers pre-allocated so
it's not a great map time complexity is
actually really good because the
branching factor is equal to the length
of the alphabet so if you have 25
symbols you have 25 possible branches
and so you're going to reach a leaf
pretty quickly but not space efficient
and can't use objects as ease so the
next question is how do we avoid null
nodes here's an interesting idea so
here's the first hybrid structure what
if we combine the idea of a binary
search tree and a try it's called a
ternary search tree proposed by Bentley
and Sedgwick in 98 so the idea is it
looks something like this you take the
first symbol and you go and search in a
binary search tree and that will send
you down if the symbol is there it will
actually send you down to the next level
if it's not there you'll just reach a
null node and then you rinse and repeat
so by combining a binary search tree and
a try you get a pretty efficient
structure which is a ternary search tree
time complexity roughly log n again
space efficiency yes because we
eliminated the null
notes objects as keys not yet because in
that binary search tree you you're still
using one symbol at a time so so we need
to have a fixed alphabet set so we
improved on null nodes but can we do
better than log to the base 2 again fill
Bagwell comes in and says challenge
accepted so two papers here really cool
of the first one is where he gives all
this evolution of how price came about
and the second one is called ideal hash
keys so the first idea is the array
mapped try
how do array map tries work so array map
tries say that we use bitmaps bitmaps to
the rescue as they are in so many data
structures so use a bitmap to determine
the presence or absence of a symbol so
let's say we had 16 symbols 0 through 15
right if you had a bitmap like this and
I've made it go from right to left
because that's how it is actually in
closure so in the 5th position or in the
index 5 we have a 1 that means that if
you're looking at the 5th symbol or the
6th symbol in the alphabet it actually
is in the bitmap if it's a 0 it's not in
the bitmap right fairly simple idea so
how do we actually answer this question
in code does the symbol with offset 6
exist we need two bitwise and this thing
with a mask
what mask do I need to use all zeros
except for the one which I care about so
if I bitwise and these two things I'm
going to get either a 0 or a 1 if it's a
0 there's nothing there if it's a 1
there's something there fair enough
how do I create the mask I take one and
I shift it over by the offset right so
this left shift operator just shifts
things so if I started with a 1 1 would
be in the rightmost position and then I
just shifted it over to whatever
position I wanted I got a mask a bitwise
handed them then either I got a 0 or a 1
if I get a 1 there's an array that lives
alongside the bitmap
that only contains entries for the ones
it's not pre-allocated okay so now our
next question is where in this dynamic
array should I look so this is a really
cool idea it says just use the ones as
tally marks so if you're looking from
right to left the first tally mark is at
the 0th position the second tally mark
is at the first position and so on
something like this just think of going
from right to left and that's really the
the key idea in the array map try right
so now we have to figure out how to go
from this bitmap to an offset in that
dynamic array because we don't want to
scan that array either again the idea is
where is the entry for six I know that
six exists and and I when I say six I
mean the offset six I know that it
exists because the bit is set but where
is it
so what you want to do is you want to
count the tally marks to the right of
the offset so you want to count how many
bits are set in that part and then that
gives you the offset in the dynamic
array okay everyone with me so far so
how do I create a mask to do this ideas
okay it's really easy you have zeros all
the way till six and then once all the
way after right so that's going to give
you the only the part that we are
interested in like this now how do I
come up with this number it looks fairly
arbitrary but two's complement
arithmetic to the rescue so we had a
number right we took one and we shifted
it over six times if you just subtract
one from that you're going to get this
so that's all you really need to do you
already had a mask you just do mask
minus one and you get the second mask
and then you get another bitmap and all
you have to do is count the number of
set bits
in this bitmap right so that gives us
that pink circle and how many bits are
set that's going to be our offset into
the dynamic array how do we count set
bits
well on most CPUs it's actually a
primitive Java gives us an
implementation and integer which fill
back well complains about saying that
they should just delegate to the
architecture it will make our maps go
much faster but this is what we do so
you do integer dot bit count and bitmap
and mask okay what happens when I insert
a new map entry so let's say this green
one which wasn't there before
has now come in what we're going to do
is we're going to move things over and
make space for it so it's kind of like
insert insertion sort if you remember
that but essentially we have to maintain
that invariant that the offset position
in the dynamic array is always going to
be the number of tally marks to the
right so if I put a new set bit then I
have to ensure that it's going into the
right place in the dynamic array okay so
this is a pretty decent map the only
thing that we're missing is that we
don't support arbitrary objects as easy
why because we need to know our symbols
beforehand right that's the size of our
bitmap so how do we fix that problem
any ideas this so you take an arbitrary
object do we have a way of converting an
arbitrary object into an integer we do
it's called hashing so you use a good
hash function to generate an integer key
from any object right integers are on
the JVM at least 32-bit numbers then you
divide the 32-bit integer into symbols
so you define your symbol table
beforehand and you say that it's going
to be 5 bit symbols right so you have
symbols from 0 to 31 think of it as I am
representing this number in base
- if you remember your conversions from
decimal to binary or hexadecimal this is
base 30 - this is what this number looks
like in base 32 and now what I can do is
I can take each part of the number as a
symbol and go walk down in array map try
make sense now this is interesting if I
have T bits per symbol
then I have to raise 2 T symbols right
so if I have 5 bits I have 32 if I have
6 bits I have 64 and so on so there's a
lot of times this question asked that
why are closures trees 32 wide and fill
bag well actually answered that in one
of his talks this is a terrible
screenshot from YouTube but it it's
actually a trade-off between in between
search and update so as as you basically
make your trees wider your search is
going to become faster because you're
going to have lesser levels but your
updates actually become slower so
basically he just chose 32 because it's
a nice trade-off between the two so 32
wide trees means 5 bits how do you
actually compute the symbols you compute
them by shifting and masking right so if
I want to find out what's my n 2 digit
right so I'm starting from the right so
0 1 2 3 what I need to do is shift the
thing over by 5 into N and mask with one
F 1 F is this special number with the
least significant 5 bits set and
everything else is 0 right so if I do a
bitwise and off anything with 1 F I'm
always going to get the last 5 bits
right now if I want to get these bits
then what do I need to do I need to
shift by 10 and then do the end right so
this is the piece of code that does the
magic at each level of the H am T you
have a shift parameter so at the zeroth
level you shift by 0 at the first level
you shift by 5 at the second level you
shift by 10 and you keep ending that
with 1
and you keep getting the symbol with
which to do the lookups best comment
ever persistent hashmap line number 19
rich has these four lines rich is
actually credited with the first
persistent rendition of the hash area
map try so this is how he did it
path copying for persistence which just
means structural shearing which we saw
already hash collision leaves versus
extended hashing this is when two things
have the exact same hash value what
happens you just put them in a node way
down there which is just an array map
note polymorphism versus conditional so
no if-else is instead we have
polymorphism inside the persistent hash
map those will not go into the details
of everything the node polymorphism is
interesting so at the very top of the H
amt you have these array nodes array
nodes are just 32 byte arrays that send
you straight down
there's no bitmaps or anything so when
your tree becomes big enough you start
off with array nodes then you go to the
next level in the next level around the
leaf levels you get these bitmap index
nodes which is what we just saw with the
bitmaps and the dynamic arrays right and
then you have hash collision nodes for
things that collide so as an example
let's say I have a humungous map with 1
million keys and and a million values
and I want to get something out of it so
what do I do first I convert that number
into its base 32 representation and then
I just walk down the amt right so at the
level 0 I have 28 as my offset so that's
where I go and I ask the array node
where should I go next the array node
could have an entry or not if it does
not have an entry then the key does not
exist if it does have an entry then it's
going to send me down to the next node
so it's just a recursive call it just
says next node dot find with Shift + 5
so as you can see our shift has now
increased and that lets us take the next
digit and that sends us to the next node
and that sends us to the next node and
we finally find what we need to find so
this is a really good map great time
complexity like you have a billion keys
you can still reach the leaf in six
great space efficiency and objects s
keys also keys are actually only
compared once because you only compare
the key when you reach that dynamic
array where the key actually lives
before that it's those five bit things
that you are comparing bit juggling
gives you great performance right those
are really constant time operations and
six hops to a leaf why not just use a
regular hash table right so root
resizing is one of the common problems
when use a regular hash table you need
to pre allocate the number of buckets
and then you do modulo that and then if
your number of keys increases too much
then you have to resize it and so forth
and it's not amenable to structural
shearing okay I talked about searching
for things but what about updates so
updates essentially search for the thing
and then they clone the leaf node and
the path to the root exactly like the
photo that I showed other the picture
that I showed in structural sharing so
you just want to copy the path from the
root down to the leaf everything else is
shared okay let's talk about vectors so
the intuition is very similar its array
nodes all the way the same array nodes
that we saw in H AMT you break the index
into digits and you walk down the
different levels in the array node right
so if this is my index the only
difference in vectors is that you start
with a shift of 15 so you start with how
high your tree is and then you reduce
your shift so it's instead of shift plus
five with shift minus five that's the
only real difference otherwise it's the
same idea pretty much so
function called array for and you that
takes an index and it just tells you
that you're going to find your number in
this array and then you just go and find
it there right so it's a very similar
idea there's one very neat optimization
that the root holds a pointer to the
tail of the vector and this is why cons
on a vector is really fast because you
you can just go directly to the tail and
insert something directly into it and
just clone the tail and the root and you
get a new vector right so these are the
structures that we already have in our
language but they are not perfect in all
ways right so we want to use the right
tools for the job
hash maps for example don't merge
efficiently so if you want to do a lot
of merging consider using data in trap
which Zack Delmon has published recently
it's based on Okazaki's work again and
some pretty cool performance numbers
there where you get an almost 10x
improvement with sorted entries of
course there is a constraint that your
keys must be integers but if you do a
lot of merging with integer keys then
this would be a really great choice
vectors don't concat efficiently and
they do not sub vector XI so if you do
subject a lot subject is just a view on
the original vector so it's going to
hold on to the head and it's going to
hold on to the indices right so it
doesn't let the JVM garbage collect the
original vector and you could
potentially have memory issues are RB
vector is an interesting idea again
based on Bagwell's work michael has an
implementation co dot RB vector
benchmarks I couldn't find but yeah so
if you have a lot of vector catenation
and sub vector so splicing this is a
really good idea to look at there is a
really epic sounding data structure
called seat rice which Michael is going
to talk about tomorrow at zero at 50 so
be there I know I will and I have no
idea how this actually works I'm going
to just skip that
we stand on the shoulders of giants
there has been amazing research done in
data structures since forever
right the first paper on trys came out
in 1959 and I'm sorry I'm going to
mispronounce a lot of European names
here so Brandeis and fredkin Fred Kim
came in 1960 so Brandeis was trying to
create a Fortran compiler with variable
length words the words were going on
disk disk was really slow memory was
really small so what do you do you
create an index so that was the first
motivation for a try then it's actually
not clear who came up with this idea of
binary search trees wikipedia has for
inventors there so I've put all their
names in 62 we have adults in bielski
and Landis coming up with AVL trees so
that was the first balanced binary tree
then you have red black trees splay
trees 96 Okazaki writes his thesis on
functional data structures so you get
functional variants of all those things
we get turn research peas in 98 am T's
and H AMT is around 2000 and then rich
actually implements persistent H a.m.
T's in 2007 so that's actually the first
implementation of course is tha empty
now we have our RB vectors and int maps
also so that's great here's the reading
list in case you are interested in
looking at this further so the papers
that Bagwell has written are actually is
really readable which means that you
have to read them about five times
instead of the average ten Okazaki's
book is also really readable the first
three chapters like most good technical
books the world's fastest Scrabble
program that's a nice paper and Brandeis
has has a nice paper to John Nicolas has
a really nice blog poly mafia where he
explains how a lot of these data
structures work with really nice
pictures and so I highly recommend
reading that if you are interested in
this topic
questions I'm going to deflect to the
experts in the room but that's my talk
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>