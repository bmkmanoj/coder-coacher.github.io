<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modeling the World Probabilistically using Bayesian Networks in Clojure - Chas Emerick | Coder Coacher - Coaching Coders</title><meta content="Modeling the World Probabilistically using Bayesian Networks in Clojure - Chas Emerick - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modeling the World Probabilistically using Bayesian Networks in Clojure - Chas Emerick</b></h2><h5 class="post__date">2013-01-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xoSFcSqo1jQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so Chaz Emmerich for those of you who
don't know him is a infamous in the
closure community for just being a
really mean guy I mean you you do not
want to mess with the guy standing next
to me which is why he has now twice
organized fundraisers to bring people to
closure Concha couldn't otherwise do so
last year he brought Anthony Grimes to
the cons for the first time and she saw
Anthony's talk yesterday I think you can
agree that was a runaway success and
this year he managed to top even that by
bringing Ambrose from halfway around the
world to come to closure con so I don't
know what's next the moon anyway here he
is talking about by Asian networks so
just to house the sound just to clarify
I fundamentally didn't do anything
except heard people around I want to
thank everybody who's ever donated or
helped Anthony or Ambrose in any way
possible and everybody that I've ever
known in the closure community has been
fantastic and I certainly wouldn't be
doing many things that I'm doing now
without all of you guys so I appreciate
everything so I talking a little bit
about Bayesian networks who I am I I'm
Chaz Emmerich I've been using closure
for a long time or a long time in the in
the continuum of when closures been
around been a contributor here and there
and I do way too many other stuff too
many other things including putting the
final touches on a closure book for
O'Reilly so we're breaking the o'reilly
lisp curse finally and talking you a
little bit about Bayesian networks first
a little bit about why bayesian networks
many some of you may be taking the
online courses for machine learning or
artificial intelligence and whether you
have or not you've probably heard all
sorts of different tools and techniques
and methodologies for handling various
problems that aren't Amina balu
straightforward implementations in
Beijing networks are one I chose to
focus on them because of a couple of
reasons that we can get into in a little
bit of details that there's a lot of
modeling options
so certainly when I was in college I did
a lot of work with neural networks and
other black boxes like that and the
problem with them is that if you get a
result that you are unhappy with or you
would like to get an explanation of a
result generally that's not possible you
open up a neural network and it's just a
bunch of weights and you have no idea
where those came from and you have no
other recourse but to go retrain over an
updated refresh data set and and and
going back to the to the modeling point
those networks are built automatically
there's no connection between like
internal neural network nodes and things
that have real semantic value to you out
in your world and so Beijing networks
help resolve that on the point about
them not being black boxes you can
actually do root cause analysis and
determine you know how did you arrive at
this answer what part of my data set
contributed to providing this answer and
that can be very helpful in debugging
scenarios as well as just modeling in
general and because of their granularity
they can interact with other systems at
any level you don't have to go whole hog
and choose to build your entire
application on bayesian networks you can
integrate things like logic programming
into them at different levels and also
they're optimized abul so although I
haven't touched this particular corner
of it you can compile Bayesian networks
down to circuits essentially and at some
point I'd like to actually admit closure
code that you then load and then you're
not paying the penalty for having that
that Beijing Network reified up into a
model that you have to interpret
essentially so a couple of disclaimers
I'm a practitioner which is which is a
short way of saying that I have no idea
what I'm talking about fundamentally I
am a software developer with various
problems that I have to solve and most
of them i try to do Burke very
straightforwardly and I keep on bumping
into things that need different
treatments thus Beijing networks and so
that's all as a long way of saying
that I've need to just bootstrap my way
from nothing not having had a lot of
formal training in mathematics or
statistics or anything and so I have to
give a little bit of credit to some
materials that I would not have been
able to do anything in this domain
without in particular the Blue Book
darwish modeling and reasoning of Asian
networks from some from my perspective
where I'm coming from a place where I
don't know the formalisms that well it's
a very easy ramp compared to some of the
other literature and of course the two
books from Judea pearl that really go
into the into the epistemological basis
for why Bayesian networks have the fs
the efficacy that they do and the nice
correspondence that they have with how
we reason about the world and how we can
help our programs reason about the world
effect effectively so you hear about
machine machine learning you hear about
artificial intelligence and it all
sounds a little disconnected from
reality especially when you look at
sample problem sets and things like that
and so I just wanted to touch on a
couple of domains that are applicable so
anytime you're working with incomplete
representations of the world you have
crummy data you have noisy data you have
unreliable data you have data that you
cannot depend upon you need to have some
mechanism to help cope with that and get
you out the other side with decisions to
the questions that you ask of it and
examples of these include industrial
process control with various sensor
systems decision systems of all kinds in
terms of modeling business relationships
and business processes prediction of all
sorts and also classification of all
sorts one of the most well-known
examples of the use of Beijing inference
is in email classification for spam
filtering Paul Graham is probably the
most well known instance of someone
talking about that topic so there's a
lot of very very practical domains
just to really bring it down to brass
tacks what I do is a lot of document
analysis so this is a sec filing that
there's nothing particularly special
about it it's a bunch of financial
holdings and shares and current value
and for the fixed income stuff they got
principal amount and all that nothing
special if all the documents I had to
work with look like this then I wouldn't
be talking about Beijing networks I'd be
having a beer right now unfortunately
this same data that has a structure this
data came out of a database somewhere
and the only way you can get access to
it and access to a lot of data like it
is by pulling it out of documents like
this that have no structure to them and
what people end up doing is they have
farms of people manually keying in data
like this because it comes in forms like
that and like that same data same
fundamental structure underlying it and
like that it's particularly nice win
instead of having graphical tables they
use a ski dashes and pipes and things
like that that's that's good so so so
what I've been what I do especially in
the past couple of years is working on
ways to systematically extract
structured data out of documents like
this where you have a particular
document type where it especially the
domain expert can characterize the
structure of that data and what you'd
like to have is something you could call
it a an extraction plan that takes a
corpus of these documents and produces a
CSV file or an XML file so that's what I
use this for you might be able to find
some applicability in your domains but
what I think we'll talk about for right
now is a simple example just so that
people can understand how things were
that where the moving parts are so you
have a bunch of variable States in your
world or your data set and this is a
canonical example pulled from Judea
pearls original materials whether the
ground is wet whether it's raining
what season is it and whether the
sprinkler on your lawn is running and
there are some dependencies between
these states out in the world and you
can imagine collecting data about those
states and wanting to reason about if
you know some subset of that of those
states at a particular point in time you
might want to ask questions about what's
the likely state of those variables that
I don't have information for so feature
selection is a very deep and complicated
field in and of itself it's very
domain-specific the features that I use
when doing document analysis are going
to be different than when you extract
features when trying to do handwriting
recognition when you're trying to do
email spam classification or whatever
else you're applying these techniques to
and to bring it down to a more concrete
level these random variables you can
sort of think of as closure identities
so if you think of out in the world
there's an atom for each one of these
that has a particular value in it at a
point in time and you can take
observations of them and they change
over time so the next time you go and
look you see oh it's spring now I wonder
what the likelihood is that my lawn is
wet
so I had a hard time understanding
Beijing inference for a while not that I
claim to understand it still but one
thing that helped me was thinking about
these random variables in terms of a
truth table and so you can take I've
alighted out season and just using
Springs so that we're always working
with random variables that are discrete
and boolean so that simplifies things to
a certain degree you can have an
enumeration of all possible states of
your world if this is your entire world
there's going to be 16 different
potential states of that world of all
different permutations of those random
variables and then you can describe what
the different relations are between them
using various logical relations and
things again if this was enough to solve
my problems I would have stopped there
and used regular programming but
sometimes logic isn't enough that's me
most days anyway so I i talked about
truth tables because they look to me and
feel to me similar to in concept anyway
joint probability distributions that's a
hell of a thing to try and take in but
what this is really talking about is we
still have we still have in this case
because we have four variables that are
all boolean we have 16 different worlds
that are possible and each of those
random variables has a probability
associated with it based upon our prior
observations of them those are called
your prior probabilities and each world
has a particular joint probability which
is notated on the right and so for any
particular combination or permutation of
states for these random variables you
have you have a known prior probability
of that world being true and so if you
know that if you know that it's if that
the sprinkler is running then you can
eliminate a large number of potential
worlds and that changes the posterior
probability of the other worlds when you
are performing classification or
inference and so on right so the nice
thing about this is that given x and y
any random variables a model like this
allows you to say how likely is Z but
also given a or b you can ask it what
additional data would most would most
likely help me understand the likely
state of C so there's a there's a
concept of conditional dependencies so
if you have two variables that are
independent and you're trying to find
out the state of a third finding the
independent variable state isn't going
to help you find the state of the third
at all you want to find one that has a
conditional dependency on that third one
right so we need to learn about a
probability for a little while well I'm
going to take about ten minutes this is
your reaction panic and sorrow but I'm
kidding I'm not actually going to do
that I'm I'm not any kind of authority
on bayesian statistics and there are
much better sources for that information
but just to take a look at sort of the
punchline of Bayesian inference place
within the context of the model we're
looking at if we want to know the
probability that our lawn is wet and we
know from observation that it is spring
to determine that we find from our prior
probabilities from our prior
observations what was the probability
that it is it was spring when we saw
that the lawn was wet and then we
factored that back using the singular
probabilities for whether the lawn was
wet and divide that all by the by the
probability that it was spring and that
brings the scales especially if you're
using continuous variables back up to
what you had dropped into your model to
begin with that's all I'm going to say
about details
so joint probability distributions don't
scale you notice for four random
variables we had 16 worlds this is a
problem because the joint probability
distribution the size of that table is
the the number of potential states and
your random variables to the power of
however many you have that's bad so we
have 16 worlds in our example model and
you know 20 random variables that's a
pretty small world and you get into some
serious trouble if you have three
discrete States that's 3.48 six times
ten to the ninth it's bad so you can't
use joint probability distributions
that's the that's the model you want to
have in your head for conceptualizing
the problem at a higher level but we
need a better representation of that
joint probability so going back to our
features those look suspiciously like
nodes and when you connect them using
some method using edges in this case to
indicate causality so if you are
relatively aware of how seasons and
weather patterns and the effects of
sprinklers on lawns you can develop a
model that indicates that the
probability of it raining or the
sprinkler being on is dependent upon
which season it is and the probability
that your lawn is wet is dependent upon
the probability of it raining and or
your sprinkler being on and so this is a
this is an example of a manually curated
model you can find this same model
through various search mechanisms if you
have a bunch of observations that
indicate the coincidence or not of these
random variables and the arrows can mean
causality and in certain subtle ways
them indicating causality can allow you
to guarantee certain semantics about the
network and certain performance
characteristics about
inference so the effect of this on our
runtimes is that instead of joint
probability distributions where
everything is in a massive table that we
can't possibly work with each level in
the Bayesian network only needs to have
what are called conditional probability
tables the subset of the joint
probability distribution that is
relevant at the local level so the
season that should be that should be 0
point 0 to 5 and 0 point 0 75 that's my
bad but each each level in the network
has its own conditional probability
table that is owned that only needs to
represent the probability of that node
given its parents probabilities and so
at each level you have much more
manageable sizes of conditional
probability tables down to this fully
populated one here where the wet note
only needs to have three random
variables represented and this allows us
to do the same work as having a full
joint probability table joint
probability distribution available to us
in log storage you know I don't know the
details of that but it's log it's good
i'm not going to i'm not going to try
and prove it it's it works so you can
exhale now because you pass through the
valley of the shadow of probability and
you're probably asking where the hell is
the closure
so raposo is a closure library for
Beijing inference and modeling that I've
been working on off and on for probably
about six months or so it's a
reimplement ation / extraction from an
existing application that's currently
doing that document analysis stuff that
i was talking about i had originally
hoped to implement it on its own off to
the side so i could slice it off an open
source and hopefully get help with
things like that but bad things happened
and it's all blocked up so now I'm real
menteng re-implementing it from scratch
supporting both continuous and discrete
random variables a big difference with
raposo versus any other Asian Network
Beijing inference toolkits in Java of
which there are a couple very large and
well-respected ones is that raposo
provides a closure idiomatic treatment
of data all the other frameworks that I
looked at as reliable and sophisticated
as they were they were clearly built by
statisticians not programmers and so for
example you had to load your data into
their own concrete data classes and
things like that and if you have any
large amount of data and want to have
good performance that's not going to
work and also even if I end up using one
of those frameworks down the road I
wanted to be able to know what's going
on under the covers and so I've been
using it as a learning vehicle here's a
here's an example of some closure data
that raposo can consume in the first
example they are all discrete boolean
values in a sequence each each set
indicates presence of that condition so
in the in the first set you have it was
raining and the ground was wet and the
second it was raining and in this
particular model you don't know whether
the ground was wet or we just didn't
have our wet ground sensor working that
day and then for continuous variables
you can provide maps of identified ran
variables and either discrete values or
continuous values now I have having been
a Java programmer for years and years
and years before coming to closure and a
scholar programmer for a little period
in between I have a body of legacy Java
code that I've been dragging kicking and
screaming for that long period of time
and so I need to be able to bring that
data into raposo and so it defines a
protocol called observed that has two
functions features which it just
provides a sequence of the random
variables that that piece of observed
data provides and then value which
allows you to access it and so the sets
and maps with enclosure end up just
being a special case of these that
there's a default implementation of so
if you have sets and maps you load them
straight in but if you have other if you
have other data that is coming from
external sources or other libraries you
can provide a mapping so that we can get
to that data from raposo but even beyond
that your implementation of value here
could perform some kind of
discretization perhaps or other feature
identification on the fly
when we want to model the world we have
this create model function and we
provide to it a representation of a
graph data structure again think back to
that those four nodes with the edges
that connect them that's that's
represented pretty trivial trivially
using a map and sets keyed by their
identifiers and from this model that
we've defined the that that we've
defined we can load data into it the
model that's returned by create model is
a closure collection it's not a vector
it's not a list it just implements
closures I persistent collection
interface so you can conjugate it into
it you can use into and all the usual
things and in the background it will
either at the at each step it will
update the prior probabilities so that
you could for example hold a model in
again an atom and have data streaming in
updating it and then if you want to
perform inferences you just grab a
snapshot of the model do your work and
let it drop on the floor if you don't
have a hand curated model or if you have
data for which there's no way for you to
develop one then currently raposo uses
Monte Carlo to discover the the
structure of the random variables that
you provide within your data set this is
a bit of a hack at the moment i'm not a
huge I'm not great at the Monte Carlo at
the moment it's not been my focus it
works for me but Patch's welcome when it
when it hits github and you can help me
improve that part of thing so what does
it look like to query this thing how do
we do real work we've built a model
we've loaded in our data let's see what
it can do we can ask it what's the
probability that the ground is wet given
that we've observed that it's raining by
that much there's no units associate
with this at the moment unfortunately
and that our sprinkler is on to some
extent
and we see that we have a eighty-four
percent chance that it's wet these these
particular probabilities are just based
off of a dummied up set of data that was
vaguely vaguely likely that i loaded in
again what's the probability that the
ground is wet given that the sprinkler
is not on we can't really say it's
saying fifty-fifty based on the model
that we provided that the the wetness of
the ground is a result of it raining or
the sprinkler being on right now it
doesn't know the state of the state of
the rain so it's not providing any
information if we knew that it was
springtime or not then it would provide
a better answer one way or the other and
then conversely what's the likelihood
that it is raining given that the
sprinklers not on and the ground is wet
we have a very high probability of
seeing that it's raining so the current
state of raposo is that it's working
it's working with test data it's not
raposo itself isn't in production it's
that piece that I'm using as a model to
re-implement from that some production
they need to get this thing on github
that's happening fairly imminently after
the cons and I need to again get the get
the mechanisms for network learning and
generation currently Monte Carlo exposed
with hooks and its own protocols and
things like that so people can implement
their own and replace the bad one that i
have i'd really like to pursue
optimization of it you can there are
very well researched ways of compiling
down a bayesian network to a circuit as
i said and like to get that pushed out
into disclosure code that is is more
analogous to a compilation than it had
been an interpretation also right now if
you just load data instead of providing
a hand curated model it retains all the
random variables it finds within your
data but if none of them are coincident
if
there are random variables that are
entirely independent of anything that
you'd like to perform inference or
classification over they still hang
around we still do calculations over
them so we need to do elision of those
so that we don't pay that price and I
already talked about compilation because
it has a because the structure of the
network is open these things are random
variables there the random variables and
the values that get put in them are
black boxes the source of that data can
be anything it could be a sensor outside
it could be data from a document or it
could be the result of a deterministic
result from core logic for example so in
particular in my use cases where I know
or my customers know that these
particular conditions hold for a type of
document being able to specify that and
not pay the price of running this type
of Beijing inference and instead use a
logic program to use as an input into
the inference would be quite the quite
the optimization thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>