<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Yet &amp; Datomic Immutable Facts Mutated Our Stack - Milton Reder | Coder Coacher - Coaching Coders</title><meta content="Yet &amp; Datomic Immutable Facts Mutated Our Stack - Milton Reder - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Yet &amp; Datomic Immutable Facts Mutated Our Stack - Milton Reder</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gcJmNYj4Mec" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hey everyone welcome everyone to
Baltimore
my name is Milt reader from yet
analytics and I'm gonna be giving an
experience report today about using day
Tomic in our systems with a focus on how
day Tomic has actually changed a lot of
the way that we do things that you know
we can't actually use day Tomic directly
in and a lot of the good ideas that are
inside of it so let's see alright yet
analytics what do we do you know like
anyone doing a talk like this I have to
explain what my startup does so I went
to the marketing website
and I got this it's too long we just
don't have time to read it so I'm gonna
distill it down data goes in insight
comes out I can't explain that so a
little bit about the meat of what we do
so we consume usually hopefully
structured event data in JSON
these are semantic statements about
experience and activity usually in the
context of learning doing simulations
serious games stuff like that it's
called the experience API this is the
API that we implement comes from the
government as an open governance process
now which is nice there's a lot of rdf
DNA in this standard you may recognize
it kind of like a subject object
predicate we have these little JSON
blobs they're not actually in json-ld or
anything like that but they have a lot
of things that refer out to vocabularies
that are actually rdf so you know we
take that data and then we do analytics
on it it's kind of an overloaded term
rich would get a dictionary I'm not
going to do that I'm going to just sort
of speak from the heart about what I
think it is so we D structure and
schematize data we do various
calculations on it we do a little bit of
machine learning some time series
analysis some classification correlation
that sort of stuff and we offer products
with ad hoc and persistent query
so like alerting things like that
and a real job right we want to distill
meaning and insight get stuff out of
data we also visualize it we make pretty
graphs they look like this
and this and that
lots of dashboards day Tomic is actually
informed the way that like we think
about visualizing our data too but
that's a totally different talk won't go
into it so a little bit of background on
the choice to use closure we didn't
always do this kind of stuff we used to
be an EdTech company before our pivot
about three and a half years ago we made
professional development software for
teachers so teaching teachers to teach
about teaching our stack involved a
little bit of pain I mean it worked
reasonably well but we had a giant Ruby
code base we had an even giant or single
page app in CoffeeScript and backbone
there was a lot of like two-way binding
and all this fun stuff objects have an
object's ORM kind of nameless dread it
has a name also not the topic of this
talk
the object-relational impedance mismatch
you know in that time though and this is
relevant later on our most interesting
stuff that we did wasn't actually in
that Ruby or in that CoffeeScript it was
way down in the database in like pl/sql
and it was materialized views of our
data to get around a lot of the sharp
edges right of doing like highly
relational social stuff in a tabular
database but you may see how this segues
in today Tomic but you know as far as
the choice of the language we'd use
closure for a toy project back when we
were in ed tech company and it generated
fake ed tech startup names with Markov
chains so here's when it came up with
chalk to our horror a lot of the
companies that we generated are actually
now real ed tech companies
it's kind of this figure pretty heavily
in us getting out of that space so let's
say here you know so after that kind of
joke thing right we really like this
language we really love the ecosystem
the libraries the ability to use
anything you know in Java
the community is amazing it's only
getting more amazing took us far less
code to do the same stuff that we were
just writing circles around ourselves to
do in Ruby or maybe we were just not
that good at Ruby I'm not really sure we
never looked back so around three years
ago this beard was a soul patch and it
was we decided we were gonna pivot what
the company did from doing a social
learning app to being a learning
analytics platform and we had some vague
idea what analytics meant at that point
we really loved closure
we were we were pretty sure about that
choice and we really loved having a
greenfield project to work on right like
nothing's holding us back we're
uninhibited but what about a database so
our choice for DES Tomic the API
specification that we work with has some
kind of strange operational requirements
right so it's primarily for you know
historical analysis of learning data and
some of these properties were cool right
like these little statement triples are
supposed to be immutable they're not
supposed to change that sounds fairly
good right for closure and maybe
diatomic it had a synchronous API you
know it was initially developed in the
sort of the heyday of rest api's so you
know you when you send one of these
statements you expect to get some
confirmation back and some idea of
consistency it also does conflict
validation right so these are things
that you really need like transactional
systems for it also had all these
requirements for doing pretty
complicated queries on this stuff you
know the provenance of statements
relations between them and their
references and then this one really
weird part of the API
you know mandated that you're able to
return all of the historical changes to
an ephemeral value like somebody
somebody's name something good isn't a
totally identifying attribute and here
they are asking you to return this
entire history over time right this is
something that is uniquely suited to the
way de Tomic works so the competition in
the space this was all very new but
somehow we actually did have competition
they were mainly using document DBS to
store these statement kind of blobs they
were getting transactional properties by
using a sequel as like an index and they
were using sort of Cavey stores for just
random other parts of the spec we
watched the day Tomic videos that was
sort of our initial introduction to it
and you know of course blew our minds
you know at that time watching those
things would what do that for anyone I
think so this was transactional it was
immutable there was time travel right so
we could satisfy these sort of stranger
requirements and it was super idiomatic
closure we had heard that term used a
few times and figured we'd start saying
it ourselves we had like immediate
excellent results with this we had an
MVP and like under a month so the
specification defines this thing called
a learning record store which is what's
actually stores and lets you retrieve
and do certain queries on these
statements we got that done really
really quickly it had awesome properties
right because of the way we were D
structuring these things in today Tomic
we got like really really really high
levels of domain compression because
once these sort of referential things
are already in our system these
statements really boil down to like a
triple of references to other things
maybe an ID maybe a timestamp so this
was actually like a really good fit for
des Tomic and that was immediately born
out the first time we backed up our
system we thought it was messed up
because it was like a 30 Meg backup for
a couple hundred thousand of these
statements it didn't make sense to us
even though we had you know defined the
schema that let that happen
that was a pretty amazing moment and the
front-of-house was like really happy you
know we actually like delivered on
something really quickly after making a
lot of technical choices that you
probably couldn't make in a larger
business or you know might be really
hard and that's it that's the end of my
talk you know happily ever after it's
great we've been using the atomic for
three years this is great no of course
not
so like meeting a specification is a
relatively easy thing to do you know if
that specification is well thought-out
making a product though is hard right
making a platform is really hard you
know this is a word that gets used a lot
there's a lot of work there and we are
not a large team you know for the
majority of this this was a two-person
development team we're up to five people
now which is great and we're only
growing so you know we've lots of things
to do right we've got to make UIs and
dashboards we've got to do some actual
like machine learning on the stuff
that's in this system we need to
integrate with other systems so you know
X API fairly new specification in the
last ten years so you know there's a lot
of data integration that has to happen
from old legacy systems it's preceded by
this thing called SCORM which there is a
sort of profile you can use to translate
them but there's a lot of sort of glue
that has to happen there aside from just
having a conformant thing and we were
sad right like after initial experience
of using des Tomic we wanted to like
take it absolutely everywhere with us
and have these kinds of properties in
different edges of our system and we
were initially kind of sad we didn't
think we could do that but something was
going on behind the scenes right the
atomic didn't let us off that easily
there was something else there like
during our hammock time that was
operating on our brains and the sort of
the if you take one thing from this talk
it's that you can't use or even read
about des Tomic without it starting to
change the way that you think about and
approach problems and that's really what
this talk is about
so diatomic it's a collection of good
ideas if I try to
list them all here that would be like
eight slides the big one I'm going to
talk about in this talk is the concept
of event sourcing which is probably old
hat to a lot of the people here to
generalist hackers like us it was not
old hat and there's a difference right
between like learning about something
hearing about it on a talk and like a
deep sort of understanding of it and
like how what its properties are people
use the word grok a lot for stuff like
that so also logical queries and rules
data log is insanely powerful rules are
amazing we're talk about that a little
bit schema is data and the data being
eaten right that's a huge thing that's
not just like we're enclosure we like
Eden Eden has properties that just make
it really excellent to sort of use as
your lingua franca in systems like this
there's other things like seekers is
amazing this command query
responsibility segregation this like
lets us immediately tick a box on every
time we have to do a security audit
we're like oh yeah you know those two
things are separate you know this is
this whole section we can skip it that's
really powerful you know when you're a
small team you don't have a lot of time
just having those things right off the
bat is incredible so good ideas right
there infectious bad ideas are - but
that's a different talk you know day
Tomic what we found was it was actually
teaching us things that we didn't know
were lessons right even after using it
for a while and they would jump out at
strange points we're gonna talk about a
few of these today so it could jump out
at you at a hackathon at a football game
or during the re-engineering of
enterprise data architecture that one is
maybe a little on the nose you might
expect to start thinking about day Tomic
when you're doing that
all right so our first little vignette
here
it was the baltimore hackathon in 2014 a
little aside i don't really like the
term hackathon and i think it's you know
got a lot of baggage now i think we
should start calling them co Dios and so
if you want to go back to wherever you
live and start doing that I'd appreciate
it
lookingglass cyber solutions a great
Baltimore company here put up a
challenge at this hackathon to implement
a game of checkers in a browser using
closure script core async and home they
sort of had a little template that you
could start developing off of and you
had to make the game logic and you know
the big sort of point in this challenge
right was to utilize events sourcing
where possible and 300 bucks which is
you know some pretty serious beer money
for you know people at a very small
startup so events sourcing you know we
had maybe heard this like in passing but
like they gave us an article about it
it's a great article it was totally over
our heads we did not understand it you
know but there was this hint right it's
like oh maybe you want to add something
like data script to you know replay
state back and forth you know and have
have all these these cool properties and
we had actually heard of data script we
knew what that was and our our you know
response right to this challenge was oh
crap you know we can do this we know
what closure script is we you know sort
of have an idea what data script is so
for those of you who haven't heard of it
data script mirrors most of the day
Tomic api right so it's you know you run
pretty much the same functions there's a
few little differences here and there it
happens in constant time whereas des
Tomic is you know always recording your
entire history but it's a really
excellent way to get control over your
state and browsers and do some really
powerful stuff so the state of the world
in things like data script and atomic
write is built up from a stream of teeny
little atomic facts de Tom's and this
atomicity right you can expand it to
cover a set of facts which would be a
transaction so we knew this right we
knew how this worked but this term event
sourcing was still a little bit lost on
us and after a sort of looking through
this we sort of realized you know we
actually did know what if a sourcing was
we had been using it you know for like a
year at the core of our systems
and you know that's kind of creepy right
to find out that you've been learning
something when you know you thought you
were just working away on you know
whatever your your product is the other
thing we ended up leveraging in this
hackathon a lot were rules in data log
we knew what they were from day Tomic
but we didn't really have a deep
understanding of them I don't think you
know we thought they were just sort of
this tool to cut down on repetition and
sort of refactor out queries to be
shorter they're obviously they're a lot
more than that right rules are like an
isolated pure expression of your
business logic they're you know really
sort of at the heart of what data log
does they're composable right and in the
ways you can compose them it used to be
the only way for instance to get a union
you know in in day Tomic was to actually
compose multiple rules together with the
same sort of invocation and they can be
recursive so all of the cool sort of
tree structures and graphs and things
that day Tomic lets you model can be
really kind of effortlessly traversed
with rules there's a little chunk of
some of our rules for the game of
checkers there on the side you know our
decision to use them right it was naive
we like we have to make checkers and
checkers has rules and data script has
rules so we'll make the rules be rules
this was a naive decision right but it
turned out to be a great decision
because you know if you're doing the
rules engine for a game right you do it
imperative Lee all of these things get
really complicated there's lots of
checking and branching and stuff you
have to do but when you express it
declaratively all of that kind of melts
away and you have this really nice
expression of what's going on so you
know long story short we won we had a
pretty good game of checkers you could
fast forward time you know back time up
and play it back and redo parts of the
game that kind of stuff it didn't have
an AI but this was just a hackathon
project we won the challenge no one else
entered
you know I think maybe if it happened
again today with all of the improvements
in CL Jas tooling that are out there and
you know they'll sort of lower barriers
I think we probably would have had
competitors but we didn't and we blogged
about it so you can actually go and see
all the little pieces of this and the
source code at our blog so okay
hackathons over all right everyone back
to work after maybe a day of rest we had
a lot on our plate at that point right
so we had this API we were you know
doing the specification but we needed
dashboards we needed like a thing right
that someone could actually go to we
need a value adds to this we loved the
way you know the closure script wrappers
around react we're working but you know
reagent right had pretty few opinions
about state outside of a single rat Tom
um did have some opinions but we hadn't
really figured out what Rea if I did yet
and so we were we were a little freaked
out about that but we had a firm idea
right of what this idea of event
sourcing like really meant what it was
and that informed what we were doing in
browsers and as you know as any set of
dashboards does right it started to
balloon but we weren't getting those
same kind of headaches that we had and
you know in our backbone system as it
got larger it really didn't get harder
and whenever you want to implement a new
feature right there was really a clear
path to it as time went on the closure
community just gave us some really
awesome tools to help chief among those
is reframe it's amazing and magical and
we absolutely love it use it all the
time um next also you know I don't have
to tell anyone that that's that's like a
pretty world shattering set of ideas
it's really great and you know even
outside in the larger JavaScript
community right you got things like the
you know flux pattern and then Redux and
you know these things sort of propagate
and good ideas do a lot of the time
went out right so all of these things
like languages libraries frameworks
databases tools companies you know they
come and go but good ideas stick around
or ideas in general like Lisp right
where we're still programming in Lisp
and it's 2017 that's not because Lisp is
like cool or Lisp is is you know just a
really fun language to work in it's both
of those things right it's because
there's really good ideas at the core of
it all right little site here so
diatomic you know is obviously it's not
free its proprietary and this is there's
been a lot of hand-wringing hemming
hawing Fudd about this kind of stuff
I've been guilty of it you know it's
it's something that initially is like oh
wow you know it assumes something like
this is open-source I think this is a
little bit misguided right day Tomic
doesn't really hide its best ideas at
all you can go through the day Tomic
documentation and come out of it with a
better grasp on a lot of these concepts
than you went in with the best ideas in
day Tomic yeah they're on the sleeve
they're out there for everyone to see
and you know the people who make
diatomic talk about them all the time
these things are not secret sauce
they're not hidden from you so another
vignette here this is a couple years
later this is in fall of 2016 so
diatomic is still running at the core of
our product where you know it's still
changing us inside we sort of had some
idea that this was happening at this
point we had a very large front-end but
it was as I said it was you know kept in
line with sort of solid event sourcing
concepts we had all of our business
logic was boiled down into these rules
you know we kept moving imperative stuff
into just data log rules that really
worked well for us and we're starting to
look you know maybe outside of the
experience API or outside of those safe
paths for bringing in this kind of data
for analytics
not all this is event data right some of
its tabular some of its in strange
places so time you know for for some ETL
you know this talk is really taking a
turn here you know it's just Oh God ETL
what we were asked to do is aggregate
and explore data about the entire world
around a football team a college
football team and it's sort of exposed
this in a data workbook that could
accompany a talk on a set of biometric
sensors that were being released and the
team was using these sensors so this
would be it would include you know team
player records yardage I actually don't
know like before I didn't know anything
about football going into this project
so if I say something here that is
uninformed about football please correct
me later as I said there was some IOT
data sensors from wearables that the
players were using there were
evaluations from coaches and there were
lots of medical evaluations college
athletes are tested all of the time for
you know things like hydration you know
urine samples happen several times a day
during practice season injuries
obviously are looked really closely at
dietary regimen is very strictly
controlled there's lots of environmental
factors the heat the humidity our data
set actually had the phase of the Moon
on game days which we never found any
interest in correlation with but we
thought it was kind of cool and there
was some definite challenges right so
it's really diverse sources of data
formats shapes some of them you know
when it was merciful they were api's
that we could just go pull stuff from
sometimes we had to go scrape the web
you know for things like game scores or
you know player aggregate performance
over time things like that and there
were spreadsheets the coach has a way he
likes to do them so yeah yeah here's
some acknowledgement here you know the
idea that every spreadsheet right is
just going to be like a CSV and it's
just
the table that's a very quaint idea
right artisanal data you know like
really interesting stuff can have
multiple tables on a single sheet it
really can just map the mind of the
person who entered it and you know
there's been some great talks at
previous colleges about sort of dealing
with that stuff it was something we had
to deal with here
there are also some maybe more sciency
problems sparsity of data and reporting
lag so a lot of this was self-reported
by coaches the players had to upload
their IOT data every night which was not
great because they had to actually do it
you know again that artisanal data the
sort of natural schema of the data we
were processing right would change
around a whole lot and we had to account
for that in our stuff also we were web
scraping and those of you who web scrape
know that it has a pretty short
half-life no matter what kind of stuff
you try to do so on the surface this is
not really maybe an amazing fit for DES
Tomic you know it's all over the place
it's not really an operational system
it's just doing all this weird ETL of
stuff but I get together but there are a
few things that kind of made us pause
there the amount of data pretty small
it's mostly tables time bound events you
know there's nothing that's too weird
like the stuff on web pages was just
tables and we could go grab any of that
historical data at any time the best
thing right for using des Tomic is we
got to define the modeling ourselves we
got to model sort of what the you know
people and places and things and that
study looked like in the data model so
the plan we came up with was doing like
an ephemeral ETL in memory for query we
use data script to prototype right
around that time des tommix licensing
model got more awesome and so we
actually switches it to a de Tomic in
memory partway through I know that's
just a development convenience but we
actually loved it for this project we
used gorilla repo to expose the
workbooks which was just an awesome
workflow I can't say enough good things
about that
so we use data log to query and we use
some stuff from an Cantor and core
matrix for statistical functions so the
extract in ETL how did that work closure
was really the MVP here and its
libraries right so Hickory and core
match let you do web scraping in a way
that's super fun and anti fragile a word
we love to use dr. was great for
wrapping Apache poi and letting us sort
of parse those artisanal spreadsheets
and we used Eden as the lingua franca
that was a natural choice for us you
know while these things were sort of in
transit or gonna be transacted in
alright for the actual transform part of
ETL this is where des tommix like entity
attribute value modeling totally awesome
we really there was nothing in the study
that we really had a problem expressing
and there was a much shorter path in
most cases from whatever the natural
format was of the data to the natural or
to the representation in memory that we
had which was a real time-saver and as
things in the study like concerns and
stuff like that shifted under us it was
really easy to change the schema right
it's ephemeral it's all just in memory
rerun it and you've got a new thing got
a little bit of our schema here on the
left that's the depth of snow in feet it
never went above zero or de tomate enum
on the right for the player's position
you know modeling stuff was really
natural and we could get a really rich
model of what was going on okay so load
yeah de tommix transactions totally
awesome you know by defining uniqueness
this is the same thing we did with the
experience API by defining uniqueness at
key points in our schema you can
effectively get item potent transactions
on your stuff so you can keep
transacting the data and you only get
novelty that's really nice right when
you're sort of doing all this on the fly
in memory you know it just makes the
workflow a lot more facile you don't
really have to worry about you know
tearing things down putting them back up
and you can just do this in real time
and a repo which is really fun and all
this really is right this is
just more events sourcing its event
sourcing you know in a different context
last part of it was querying so with
ephemeral sort of ETL for a scientific
thing like this it should be a
four-letter acronym right because
there's there's no reason to make this
ephemeral data and load it in if you're
not actually gonna be able to query it
so data log was awesome for this right
you can't get sparse results that's huge
when you're doing statistics and a
little bit you know ml which I'll talk
about a little bit later on you know we
found ourselves like if we were using in
cantor data sets writing all these like
densify the data functions that that
just wasn't a thing we could also embed
stats functions as aggregates they Tomic
already has some really awesome
statistical aggregates we added some
more and that was really great and of
course the rules you know we kept the
sort of size the queries down and made
them really pretty consistent by sort of
defining core rules on what we queried
so the results of the study weren't
great there was some methodological
problems sort of there when we came into
it there was the compliance issues I
talked about we did get one result that
we thought was kind of funny it was that
players who you know sort of did what
they were asked and submitted their
sensor data every night performs
significantly better during games they
had you know better yardage higher
scores we're not going to venture a real
guest to this other than maybe elite
players are pressured to comply with
studies more I don't really know you
know we could ask a lot of interesting
questions but the basis data left a lot
to be desired there was no fault of you
know the tools that we were using and
what do we get out of it as a company we
got a really robust like set of tools
for modeling and exploring data in
contexts like this doesn't work for huge
data right but for a limited set of
stuff even really diverse stuff these
tools were like natural and like really
fun to use also as far as event sourcing
goes you know before we were talking
about operational event sourcing so
using it in a front-end application this
is doing it in a analytical context
right taking a stream of data that you
want to analyze and building up
interesting state out of it this is a
really really really powerful idea all
right so that project was over back to
work
and people on our data science staff of
which I see one back there who was
giving a talk tomorrow
that's Wilhoit it was really natural for
them to use a lot of the same stuff to
start pulling out datasets for training
machine learning models doing some
neural network stuff you know this idea
right of doing science with tools that
aren't just familiar they're like your
your tools of preference was really it
was really powerful and fun meanwhile
the experience API actually started to
get some traction demand grew you know
we started this you know in a very
speculative way it turned out that that
gamble paid off this data started coming
in you know there were customers there
was data and there was lots of data so I
want to talk for a second here about a
sort of more general problem and I know
this is something that people in this
room have some knowledge of and you know
encountered sort of since the you know
90s or 2000s very large organizations
enterprises government organizations
they've been retaining as much data as
possible right they've been like just
keep keep everything we possibly can
there's data warehouses and lakes and
Mart's and ponds and boutiques and
they're doing this right so we can do
big data stuff at some point down the
line and this sort of led us to an
understanding right because this was the
same in our domain of sort of learning
data there was decades of LMS data from
stuff like blackboard Human Resources
systems you know lots of historical data
that's applicable here there's this idea
of the F Drive go back to more of this
artisanal data right a giant shared
thing you wouldn't believe the companies
that still have like a giant share drive
or a lot of their most important core
business information is and there's also
of course real-time data right that's
coming in off of the web in all sorts of
places it's a really a right scale
problem and what we realized was that in
a lot of cases you know people didn't
just need a sort of a store and an
analytics platform for a sort of narrow
category of learning events in a lot of
cases what we could provide was really
kind of re-engineering their their data
architecture at least on the analytical
end to start utilizing all of this data
that they had scooped up and collected
because they felt like they should you
know they needed more they needed a
platform right and as we said before
products hard platforms like really hard
this was a problem right because we were
de Tomic up and down and this is it says
right in the docs you know de Tomic
isn't a good fit for a right scale
system and a little side here we also
realized that actually the specification
that we were working under x api wasn't
a great fit in a lot of ways - right
it's synchronous it's transactional it's
got conflict validation you know if I
send an event to Google Analytics that
conflicts with another event it doesn't
matter right it's all just you know
going into a big queue so the
specification had some baggage that
really meant that as far as the
specification being what it currently is
day Tomic was gonna continue to be a
great fit for us but when we scale
things up and we bring in lots of other
data we weren't gonna try to like break
day tommix back with that in a way like
what we learned from day Tomic led us to
understand that we should work in the
governance process for the specification
that we implement to make it into
something that day Tomic isn't a good
fit for anymore when's the last time
your database like put you into an
existential crisis like that in a good
way right like this this kind of
learning is something you don't really
get from
the tools that you use and we were sad
you know we we absolutely loved working
with day Tomic and it was totally great
and yeah it was it was tough so I'm
gonna talk a little bit about what we're
building now and the talk is almost over
we are moving away from this idea of
sync synchronicity a single right point
we are leveraging Kafka and especially
streams of the core of our systems we're
doing some stuff with a really crazy
Apache project called knife I for doing
really nice durable integrations with
other systems and you know anyone who's
worked with distributed stuff right the
more tools you add that are distributed
your system diagrams just gets stranger
and stranger and stranger which is why
you need to bring some some order to
them Bobbe Calderwood has a great talk
on this the commander talked yeah so
there's a lot to love in tools like this
in Kafka especially Kafka streams it's
really fun to write the stuff in closure
we now know what Rea fi does which makes
that way easier as we mentioned before
we didn't use to you can bring your own
serialization and state stores right
that's a really powerful idea that
doesn't have opinions about that stuff
it's Avro if you're using sort of
confluence platform but you know for a
lot of things you can just plug in your
own I'm sure there's people in this room
who have written an Eden serializer
deserialize ER for a Kafka system takes
two seconds really easy and you have
these nice declarative topologies of the
way that your stream processing works
again this is feels like a very closure
II idea there's also my slide is frozen
okay you have your choice of log
compaction semantics you can do some
really powerful things with that and the
K table abstraction looking at a stream
write as a table this is just event
sourcing and the way it's implemented
this pretty elegant and we got this
feeling right we're looking at this
stuff going oh well we can't use the
atomic anymore but this stuff really
does kind of seem familiar as time went
on
Kafka got more updates and this feeling
really intensified right so in the last
update you've got things like on a on a
topic right you can tell a producer a
table idempotence just start doing it I
think that's a really funny config
variable I snarfed my drink when I first
saw that there are distributed
transactions and in the kafka streams
library you can set this flag you know
exactly once processing that's a really
big deal
there was a talk great talk it's strange
loop a couple of weeks ago about it was
called listen up I'll only say this once
about how they implemented that it's
really fascinating I recommend everyone
go go watch that so again we're moving
sort of backwards in time here or moving
back to these models that we learned
with day Tomic and i think in our case
that was because day Tomic taught us
these properties and these semantics to
look for in a system and how to use them
you know to get leverage over data state
and time and in that way you know
diatomic will always be with us it'll be
in our hearts but remember remember
those pluggable state stores that I
mentioned remember those weird system
diagrams and the complication you know
that you get the complexion you get from
distributed systems
remember the joys that we had doing this
ephemeral event sourced ETL and query
and our really early success back when
we were using Ruby right with expressing
things with this pattern of materialized
views the martin Clubman talk on turning
the database inside out is required
viewing I'm sure a lot of you have seen
it go watch that day Tomic isn't going
anywhere for us it's going everywhere
you know we can used atomic in a more
ephemeral way right to consume and
transact in today Tomic streams of data
coming out of Kafka that lets us have
these great queries insightful
expressive ways of asking questions of
our data
it lets us through the state store
functions we can add intelligence and we
can control like the way you know
joins work in really complex ways inside
of our extreme topology by just using de
Tomic right for a materialized view of
the world it's a great fit obviously for
operational and business data it like
imagine that using de Tomic as it was
intended you know all of our users all
that stuff all of our customer data
that's a really natural fit for de Tomic
we're already doing that it works great
that's never gonna stop and a big thing
and I think you're gonna probably see
diatomic use more in this way is the you
know managing that configuration and
that complexity of distributed systems
diatomics actually a really great fit
right for keeping configuration across a
huge sort of space of stuff you know
we've been using a lot of zookeeper and
exhibitor and things like that
which gets you a lot of the way there
but there's some rough edges to that
stuff
I think de tama can actually in a lot of
contexts replace some of that stuff in a
really interesting way I think you're
gonna see people using it like that a
lot more so the family's back together
and we've got a very bright view of the
future I just want to take a second here
so I started this journey as employee
number two at yet analytics employee
number one was Jason Lewis who was my
mentor he introduced me to closure he
was the one who did the final sign-off
on the initial decision to use closure
and des Tomic he's really he's just
taught me so much and he passed away
last year
wherever he is I know he'd be really
like proud to see his company and what
he built being shown off at conch so I
just wanted to give a little shout out
to Jason and that's it
I'm happy to take any questions or if
you want to ask me stuff at lunch that's
fine too</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>