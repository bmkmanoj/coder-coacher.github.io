<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure for Lisp Programmers Part 1 - Rich Hickey | Coder Coacher - Coaching Coders</title><meta content="Clojure for Lisp Programmers Part 1 - Rich Hickey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clojure for Lisp Programmers Part 1 - Rich Hickey</b></h2><h5 class="post__date">2012-12-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cPNkH-7PRTk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm here to talk about closure which
I've done several times and never oh yet
once at the European Common Lisp
workshop for an audience of lispers but
usually not so it's always it's always
nice to talk to a crowd of people who
already know lists because then I can
talk about more advanced things and more
interesting things I have to explain as
six questions and things like that but I
want to make sure that that presumption
is correct is everybody here know some
dialect of Lisp yes anyone not okay
so what I'm not going to cater to you so
you're just going to have to like hang
on and amongst the people who know Lisp
familier of the Common Lisp persuasion
and scheme okay I won't make any scheme
jokes though does this work oh yeah look
at that so I have a lot to talk about so
it's nice it's a lisp group because I
can also presume that you're all very
smart and I can talk fast so first night
I set this up what was I trying to do in
building closure well I wanted to have a
lisp okay I found Lisp a long time ago
and decided this is what I want to be
able to do in particular I want closure
to be a functional programming language
there's a lot of reasons for that one of
which is has to do with robustness of
programs and the other of which has to
do with concurrency which we'll talk a
lot about I want to be a language to the
sports concurrency and what I do I
haven't written a non concurrent program
in almost 20 years
do broadcast automation software and
things like that and there's always more
than one thread or process closure also
is designed to run on the JVM being on
the JVM is not an implementation detail
it's an objective interact
interoperating with java is an objective
and an important part of closure it's
not just the something under the hood
which brings me to my next question how
many people know Java oh good okay I
will make fun of Java
but I hope after this talk is done that
you have an appreciation of the
difference between the JVM and Java it's
okay to hate Java and like the JVM
that's that's where I'm at
also closure is open source because I
think that's mandatory for adoption for
language so why list this is not an
audience in which I have to justify why
Lisp right we all like Lisp is dynamic
in particular for me it was great
because it's got a small core you know
the same you implement these seven
primitives and just macros and
everything else takes you from there
which means I have a shot at getting it
correct
unlike languages you know let's have all
that syntax and then therefore very
complicated compilers and parcels and
everything else we all like less syntax
this is still an advantage of Lisp code
as data and the resulting macro system
and syntactic construction also I saw in
closure an opportunity to help along the
ever never-ending battle of having
people not block at the parenthesis
before they see the rest of the value so
one of the things I want to do and one
of the questions I expect to face in
this room is you know we have these two
great lists they're standardized to some
degree why yet another Lisp and I think
this is an important and valid question
which I'm going to answer tonight I
think any new dialect of Lisp should be
asked this question and should have a
good answer so some of the reasons why I
couldn't use common lisp or scheme for
what i wanted to do was they they're
hard to change I don't know how you
change countless scheme just recently
changed a little bit because I want a
language that's functional I want the
core data structures to be immutable
okay that's not something you can
retrofit on either of those two lists
there are no concurrency specs for those
languages as it exists that's not to say
that implementations don't provide
concurrency semantics for the stuff
that's already in the language or
facilities for dealing with threads but
it's not in the language it's not
something you can presume it's lots of
love ports and there are already good
lists on the JVM Kawa
there since they're all good so if you
have as your objective I have all this
code and I want to move it on the JVM
for some reason that's a solid problem
there's no reason for me to take it on
the other thing has to do with languages
and platforms and I'll talk more about
that but basically lisps like any
language of their day are kind of their
own platforms and that's also true
languages like Python and Ruby
so why the JVM well it's coming around
that just like you would say I went this
language and I want to target this
operating system I want to be able you
know I'll port it to these operating
systems as targets virtual machines are
targets in fact they are the new targets
they are the new platforms just like we
would say well this is a platform you
know UNIX is a platform for businesses
these VMs or platforms because they
abstract away platforms or what we used
to call platforms the hardware the
instruction set the CPUs no one wants to
deal with that no one wants to do with
the OS level stuff anymore so we've now
got another layer of abstraction which
are these virtual machines the two big
ones are net and Java and they provide a
lot of facilities more than just how to
access the file system including type
systems what's interesting and
surprising a little bit is that the type
systems in these VMs are dynamic they
have dynamic enforcement dynamic
creation of types they're very dynamic
even though the languages that typically
target them Java and c-sharp are not
particularly dynamic the big story of
course is this huge sense of libraries
you get to tap into all this code that
other people wrote they also provide a
bunch of high-level services like
garbage collection and memory management
resource management meaning that garbage
collection is no longer a language
problem garbage collection is a platform
problem which is good and also I'll talk
more in detail about this later but they
you know they offer bytecode in
particular in the JVM just-in-time
compilation that is extremely
sophisticated you know runtime analysis
and of your running program to find best
paths optimize short call paths these
things will take apart your code make a
fast path compiled a special fast path
all kinds of things that you absolutely
cannot do with a static compiler the
compilation technology on the JVM is
state-of-the-art so old-school was well
you know we didn't have very great tools
we're all writing our languages and or
starting or bootstrapping with Si and
every language sort of solve the same
set of problems defining its own
platform so a garbage collector it's on
byte code it's on type systems and
facilities for libraries now we can look
to these VMs to provide that stuff for
us and in particular they can provide it
in a way that's somewhat independent of
language I would pretend that the JVM or
the.net runtime the CLR are really
independent of language especially the
CLR they talk so much about it but
really you know any language can charge
this platform as long as it looks like
c-sharp but it ends up that if you said
well if I want to have an abstract
machine I could have like a little stack
machine it did almost nothing or I could
have a stack machine that had a sort of
a primitive object system well that's
what these virtual machines have that
the primitive object systems that are
supported by C sharpen and Java and a
big thing for me is closures designed to
let me continue to work for a living and
I know a lot of people in this room or
some people in this room are extremely
lucky and can write common lists for
money great but most people can and
platforms are dictated by clients just
like I say I have this Sun box and your
program has to run on it they also now
say I have this JVM infrastructure and
your software has to run on it because
my guys know how to install it secure it
scale it and you have to target it if
you don't your language is just like
your language didn't support
OS 10 if your programs just couldn't run
there and they have real investments and
they're not stupid this is a real
requirement in a lot of commercial
software so again as I said before I'd
like to hopefully get you two distinct
between Java and the JVM unfortunately
son called them both Java for the
longest period of time was like Java the
ecosystem and Java the language which
made it difficult to split them apart
but it's really always been the case
there are hundreds of hundreds of
languages that target the JVM and now
that the JVM is open-source
you're seeing even more adoption or more
targeting more libraries more
infrastructure any language always had
to have an interrupt layer with
something typically that was C it ends
up that that's pretty insufficient
because lots of the great new software
and lots of the great new infrastructure
is not being written in C the old stuff
was but how the new stuff isn't so some
ability to call or consume Java is
critical and it's it's very difficult to
do with the bridge I've written two
bridges for common lisp you may know of
J fly or foil I wrote them I tried very
hard to make that work but it's not a
satisfying experience so if we look at
closure as a language in the JVM as a
platform
why functional programming languages
well I think there are a couple of
benefits before you even get to
concurrency they are certainly easy to
reason about and easier to test now
everybody will say Lisp invented
functional programming we've been doing
functional programming from the
beginning it's certainly possible to
adopt a functional programming style in
lists that already exist and that's both
true and not true it's true in that yes
if you want to use lists only and you
promise each other never to modify them
you can take a functional approach but
otherwise you can and I'll talk more in
detail about why that's the case
when you get to concurrency however you
need to take a much more serious view of
what it means to be functional what it
means to be immutable because it's going
to be the case that we simply cannot go
into this multi-core environment and
keep scribbling on the same memory from
different threads
there are too many dynamic functional
languages are langostino make functional
language enclosures one but most of the
functional languages like Haskell and
though couple this functional
programming and by that I mean
programming without side effects
with these ornate and very complex type
systems because that's where the hot
research is but they don't need to go
together although people will dispute
that in particular it's my opinion
having tried to get people to do it for
a pretty long time that functional by
convention is not good enough both
because people mess it up and because
it's not subject to the optimization
that's available when things really
really are immutable something that's
genuinely immutable and provably
immutable by the optimizers is a
different thing from something where two
people have said in a conversation at
lunch I promise not to change that all
right why focus on concurrency well
we're we've long since I mean spent a
while now we are not going to have our
single threaded programs have their
performance improved by time passing and
Moore's law kicking it that is over
we've hit this plateau it is not going
up if you have a program right now today
and it's only capable of running on one
CPU it will never ever be faster okay it
wasn't always the case used to be wrote
a program you said man by the time we
finish this it'll be fast enough and
that's not the case so you really have
to get a grip on this but trying to do
this with threads with the conventional
mechanisms and Java has locks and
everybody has locks and a lot of the
languages need a lot of it enhancements
to Lisp that support multi-threading
really have the same kind of crusty lock
based mechanisms for dealing with
concurrency they don't work they're very
very hard to use they don't scale and
even if you happen to get it right in
one moment in time in maintenance
it breaks program in a functional way
helps of course anything that's really
immutable you don't have to worry about
multiple threads can access it
there's no sweat nothing bad can happen
but it ends up that most real programs
that do real work that people will pay
for are not functions you know the
things that a lot of the functional
programming
is right are really functions a compiler
is sort of a big function right it takes
the source code as an input calculates
output spits it on the desk theorem
provers sort of a function a broadcast
automation system is not a function I
work on the election system that runs on
election night I can tell you that's not
a function it's all we could do to make
it function but it's certainly not a
function so these kinds of you know real
programs are more like models of
processes you go to business they have
this process most things have to run
indefinitely
so you need something that's going to
appear to change over time that multiple
parts of the program can access we
typically would call that state and it's
a reality so closure couples this
immutable approach to programming with
some mechanisms for doing safe State and
in particular I think it's critical that
the language provides both support and
enforcement for that that it's not a
convention where a locks are or
convention closure is not
object-oriented and there are many
reasons most of which have to do with
the fact I think we have to dig
ourselves out of this rut we've gotten
into with object-oriented programming
because we just haven't done anything
novel with it in decades
but in particular for this problem space
it's a disaster
okay object-oriented programming as
normally implemented is inherently
imperative it's about creating this
object and banging on it for multiple
threads there's encapsulation but it
doesn't that's orthogonal to the problem
of concurrency so this has got to change
also I think the traditional style
object-oriented programming where you
have methods inside a inside a class has
been proven not as flexible as common
list generic functions which show we
should bring this polymorphism out of
this box and apply it to a multiple
arguments for instance and I go even
further and say why do we base
polymorphism on types alone you know in
the real world we don't categorize
everything by a
as fundamental at birth type you know
nobody's born a taxi driver or painter
or tall but you know these type systems
work that way you shall be tall and
we'll make every decision about you
based upon your being tall whereas most
systems and most reality supports some
sort of polymorphism based upon value or
current current state or relationships
to other things so we need to sort of
pull this stuff apart and the
traditional object orientation is just
put us in this rut where we came and see
the difference between these things so
this is the question I want to answer in
addition to others tonight
why is closure different the first thing
that's different about closure is it has
more first-class data structures so list
you know if you interpreted list as list
processing you know this is there's
definitely a primary data structure
which is the list and everybody who uses
lists especially common list knows it
that ain't enough right it's not a data
structure that scales well it's not
suitable for a lot of application
dominions of course Common Lisp has
other data structures that has vectors
and it has hash tables but they're in a
first class okay there's no reader
support they don't print well they just
are not as nice as lists so closure
enhances list by making print read
representations for more data structures
and I'll show you the details of that
another big difference with closure is
that the common and core algorithms are
defined in terms of abstractions as
we'll see a lot of the librarian lisps
is based around a concrete data
representation you know the two Sal
Khan's that is really bad of course no
one's blaming McCarthy for doing that 50
years ago it was brilliant but today we
can do better and we have to do better
closure is different again because it
embraces a host it is symbiotic with the
platform which means it's going to get a
lot of leverage
that platform and other people have done
we're threat aware and as you'll see I
am not constrained by
backwards-compatibility I am NOT going
to compile your own code it's not the
objective of closure and that frees me
up - for instance we use words which if
they are to mean what they mean and
common lists been schemed forever you
will never have a list with different
semantics
you just can't own the words sorry okay
so I'm going to break this down you know
we're sort of branching like a tree
we'll talk about closure as a list we'll
look at the data structures I'll talk
about abstraction and building
algorithms on top of abstractions
instead of concrete data structures
we'll dig into how the immutability
works in closure and then I'll look at
some grab-bag of Lisp features so you'll
see how closure differs from Lisp or
Common Lisp or scheme and then we'll dig
down into the concurrency mechanic
mechanisms in closure and finally look
at the JVM both how you ensure operator
it and how is it as a as a host platform
so closure meets all the objectives of
being a lisp it's dynamic code is data
which is a primary characteristic of
list is the same enclosure it's the same
compilation model it's common list I'm a
sort of a common list guy I'm not a
scheme guy so I like the reader and you
know the properties of the reader being
well-defined as a small core you have a
repple it it's lifted the notion of
lists - thus abstraction called
sequences and we'll talk a lot more
about that and it has macros and the
other goodies you expect from a list if
you use closure you would not be at all
confused about being less as a list
these are the details it's a it's a list
one its lexically scoped it's a list one
however it has common list style you
know sort of full full function macros
do whatever you want in there and it has
dynamic variables which behave a lot
like dynamic variables in common
us except they have threading semantics
it's a case sensitive language that's
really gotta go if you want to
interoperate with XML and sort of
anything you have to be case sensitive
it has name spaces instead of packages
and I might have some time to talk about
that in detail but in particular it's
it's important to note that closures
reader is side-effect free which is a
big benefit closure is 100 percent
compiled there's no interpreter it loads
code it compiles it right away to JVM
bytecode from there there's a lot more
compilation that goes on which I didn't
have to write as I said before this very
sophisticated optimizing compilers
inside the jits inside Java VMs
particular hotspot closure does not have
tail call optimization not because I'm
against it I'm all for it it's something
I had to trade off in being on the JVM
however I was just at the jvm languages
summit and tail call optimization every
language designer who came up there said
to alcohol opposition tail call
optimization they've heard it and
hopefully it will come in a future VM
hopefully soon a lot of the names are
different as I said before the names
can't mean the same thing forever and
ever and ever
there's only a few good names so these
are the primitives of closure fun is
like lambda if death is defined let loop
and recur I'll talk about a little bit
later do is block operation new dot our
Java things Java Interop things try and
grow I have to do with exceptions
they're set quote and bar so I might
have time to talk about but this should
be familiar in terms of their operation
if not the names so the atomic data
types are the things you would expect
that has closure has good math with
integers that don't truncate or wrap
there are doubles there are big decimal
literals which are important for people
who deal with money
there are ratios strings or double
quotes characters are preceded by a
slash
a backslash there are symbols the thing
about symbols is they are not places
with values stuck on them symbols and
bars are different enclosures are two
separate things so you can consider
symbols and closure to be more simple
names there's no in turning of the
symbol with associated values key words
start with the colon the colon doesn't
participate in the package system like
it would in common lisp so there's no
implication there other than the typical
thing your expected key words which is
that they evaluate to themselves there
are boolean literals true and false they
unify with Java boolean and there is nil
which unifies with Java null so Chavanel
Anila the same thing I'll have a whole
slide on nil so I'll save the arguments
and whatever for that and there are
regex literals but there really aren't
any more atomic literals than these so I
said earlier closure has more first
class data structures right everybody
knows you you couldn't write a
performant program with lists okay it's
just going to be a pig you know yes you
can use a list and pretend you have an
associative thing but it's not that's
not good enough the other ones a second
class they don't have reprint support in
particular though they're not very lispy
a lisp hash table is not lispy at all
okay you can't recur on it you can't
build it up incremental e you can't
unroll the stack and just pop off values
and use it like you do a list that you
don't use hash tables like you do lists
and in common list right and same thing
with vectors and one of the reasons why
is because they're they're destructive
right you add something to a hash table
you've trashed it you can't undo that
trashing which means that you do nice
things with lists right you make you
know lists of a lists as you guys you go
down you know if you're writing a little
evaluator or compiler right you'll have
a
survey lists and you know you can just
unroll that and like pop it right back
off right because it's all
non-destructive you can't do that same
job with hash tables enclosure you can
because closures data structures are
structurally recursive you could say
lists of a lists for an environment that
is really not so great but I'd much
rather have a stack of modifications to
a non-destructive true performance
associative data structure and that's
what closure gives you in particular
compared to the languages du jour Python
Ruby whatever the lack of a first-class
associative data structure is really a
bad blight on unless people look at that
and like I can't work with us because
it's something you really need so
closure hazard so what have we got we
have lists okay
we have lists we have lists they're
singly linked they're exactly like what
you know I've heard rumours closure
doesn't have calm cells closure does
have consoles but closure doesn't have
is a giant beautiful library that's
married to consoles that's what it
doesn't have
it does have consonants closure has
vectors it has maps both hashed and
sorted by maps I mean the associative
data structures hash tables a hash map
is a hash table it has sets again
first-class sets both hashed and sorted
they all have read print support they're
all structurally recursive in addition
they all support a uniform add operation
which is cons another SJ pun but it ends
up being an extremely powerful thing you
can write algorithms that manipulate
lists vectors maps or sets uniformly add
things to them look for things map
functions across them and you could just
substitute a map for set for a list for
a vector not change the rescue code at
all okay so if you have this presumption
that you have these data types more data
types means more complexity for my
macros and I have all this gook because
now I have this heterogeneity to my data
structure set that's not true because
you have homogeneity in your algorithm
set and I'll show you more about that
later
and conscious one of the keys to that
this is what they look like this look
like what you're familiar with okay this
is parenthesize they're heterogeneous
you can stick anything you want in there
they grow at the front every time I say
grow change and remove there's quotes
around it okay because none of these
things actually change so you know just
like a list in any of the lists you know
adding something to the list isn't
change the list it's a new list right
that logic applies to all the data
structures enclosure vectors support
fast indexed access like you expect they
also grow at the end automatically so
you can cons onto a vector it's going to
grow at the tail maps are in curly
braces it's just key value key value
commas or whitespace they make people
coming from other languages feel more
comfortable and so they can use them we
don't care if you don't want to use them
you don't have to so some comments are
just ignored and again it's key value
key value and the keys and the values
can be anything sets or in curly braces
preceded by the half sign and that's
just going to be a set of unique things
what's the support fast lookup in
detection of the things in there and
they have as I said before both of these
sets of maps have hashed versions and
sorted versions with different
performance characteristics and sorting
characteristics and then all of this
stuff nests so this is just some code
uses the data structure literals and you
can see what's going on so you can just
you can just put the literals in your
code you get versions of that of course
as in all this lisps lists if I default
get evaluated so if you want a list you
have to say let's store
but the nice thing is we have this
uniform interface we can add to any
collection something and it will do the
right thing for whatever kind of
collection it is so we can add key value
pairs to maps we can add to the
beginning of vector of the beginning of
lists and the end of echoes making these
changes doesn't really change anything
or see we print we get to change
versions and unchanged versions if at
any point somebody's questions nobody
what is that program to you this program
just shows you what and that does
nothing it creates a vector a map and a
list it adds something to each of those
and it's returning a vector of the
changed versions and the original
versions Oh the brackets are it's a
literal literal vector right so as a
vector of the answers and the originals
then we get a vector of the answers
change vector change map change list and
the originals unchanged original vector
map from west yes everywhere everywhere
have at it
stick them anywhere you want just
whatever makes you feel good now there
are functions for all these things
they're named functions for all of them
so you can map you know vector and get
yes the bracket is part of the let
syntax you've seen some schemes start to
advocate using square brackets for lists
that are not calls right and in them
like guilty or whatever they treat them
as lists that same convention of using
square brackets when things are not
calls or operators happens in closure so
these things these are not false so
people don't get confused because when
they see friends everywhere they're like
this is a call is this data I can't get
my head around it so that convention was
a good one except in closure I don't
want to waste square brackets on another
kind of list so I have real vectors
that's a vector when this is read this
is a list with the symbol a vector with
a symbol of Ector symbol map symbol
lists vector etc etc this is real code
as data you read the sentence fine
this is a hash - oh that's the
destructuring stuff I'll show you that
later
ignored ignored is sealed ignored
ignored to the end the line coming so
reader nothing is produced by the reader
for that it's a cool reader though for
instance it captures all the line
members and associate some of the data
structures as metadata I'll talk about
metadata later okay so this is a big
thing I want to talk about and I want to
kind of beat up on the existing list not
because they made the choice but it's a
choice that's getting moldy okay which
is you have so many great algorithms
defined in terms of concrete data types
right that's really a mistake today
right it limits how general your code
can be it limits the extensibility of
your language how many people said oh I
wish this was a generic function about
some library function okay so yeah you
do because the generic functions are are
the abstraction mechanism in Common Lisp
and they weren't used a lot in the
standard library because of the way the
history was again I'm not blaming the
people who wrote Lisp Common Lisp and
scheme for doing what they did it's just
today starting from scratch that would
be not a good idea because there are so
many efficient mechanisms for
implementing abstractions that are fast
that are super optimized it ends upon
the JVM that mechanism is interfaces in
virtual function calls so under the hood
implementing this stuff intimately these
abstractions in terms of that mechanism
means it's super fast because some
people say well we can't make that thing
in common list but generic function
because we just can't take the hit no
hit these things I mean these optimizers
can make them all go away and inline
virtual functions pretty much everywhere
so this happened a lot in closure but I
want to take as a as a an example case
the concept getting rid of the console
getting it out of the library code so
the console is just brimming with
details right it's so sad see the list
book sitting with pair of mailboxes with
arrows between the boxes what a horrible
way to start
right because because there's a great
abstraction that's that's in there
that's inside Kahn's is right we don't
care that it has two slots well we don't
care about card cutter at all we don't
care about the contents and in
particular we don't want to know that at
all I don't want to promise that you're
going to give me back something that I
can change that's terrible
so if we were to lift the interaction
pattern you have with these things
there's only two things right get me the
first thing and get me the rest two
functions two functions that have
nothing to do with pairs with consoles
with boxes or arrows right this is an
abstraction in there which is there's
this sequence potentially now what if
there's no sequence what do we have well
the common Lister's will say you have
nil right and and I do too okay
but nil means nothing in closure it
means you have nothing it does not mean
some magic name that also means the
empty list okay because this empty
vectors and this empty all kinds of
things
li'l means you don't have something so
either you have it sequence or you have
nothing no if you have a sequence it's
going to support these two functions
first it's going to do what return the
first thing in the sequence that's good
not surprising rest is going to do what
it's going to return another sequence
okay again no change this is not an
iterator right this is the abstraction
inside consoles right rest says get me
the rest which is going to be either
what a sequence or no that's it that's
the abstraction that's inside there
what's what
No well at this point I've stopped
talking about lists right I have nothing
to do with this in fact the next thing
I'm going to say is there's no reason
you can't build an exemplar of this
abstraction on top of vectors right I
can make something that implements first
and rest on vectors I can implement
something that makes first implements
first and rest on maps
I can implement something that provides
first and rest on anything and that's
what happens all the data structures and
closures support a function called seek
which gets you something that implements
this interface on that data structure
now you've got the separation of
concerns sequencing across a data
structure and the data structure are
separate so we've lifted that away that
is a big deal it returns something that
implements this interface on top of the
vector you could consider it a cursor if
you want as long as you don't associate
that with anything like iterators in
these broken languages it's not a
stateful thing it's it's literally this
what's the difference between rest and
move next rest gives you another thing
right move next is a cursor its
destructive on the iterator iterators
are bad right sequences and and this
abstraction is good because it means for
instance with an iterator you can't like
look at three things and then hand the
head again to somebody else that heads
that's toast you moved it you can't even
get at it anymore
that's not true with these okay so so
when you say to a vector give me a secon
you you get a new thing it's not the
vector of course some things can
implement rest first to rest themselves
what can actual lists what a beautiful
thing so when you implement this thing
on consoles they are non allocating
exactly beautiful the same exact stuff
you used to have it's just we set
this from that but that is still the
most efficient implementation of this
yes it does
so this is a famous quote and it's a
good idea we have to go further now okay
we should be implementing our functions
or algorithms on top of abstractions not
data structures pull out the abstraction
seekers implement for everything in
closure in addition I implemented seek
on almost everything in Java Java
collections any Java iterable Java
strings Java regice regice matches lines
from a file etc etc etc there's a seek
implementation for all of those which
means that all of the algorithms and
closure work on everything no no
certainly right let's just take an
example vector no it isn't no I disagree
I disagree strongly no no no no we have
to stop doing this I'll tell you I'll
have an anecdote which I'll use a couple
of times because it was extremely cool I
was just at the jvm languages summit
where there was a guy there who works
for a dual systems this is a company
that makes these mega boxes that run
java they're dedicated to running Java
they have hundreds of cores I gave him
this little closure program I wrote in
an afternoon it's less than 100 lines it
implements an ant colony optimization of
the Traveling Salesman problem I only
had a four core machine I was I did what
I had to and I used some of the stuff
I'm going to show you later the STM and
the agent system but he said I want to
try not my thing so he tried it on this
thing and boom he pulled up my program
it ran on 600 cores boom just like that
he had these cool tools for watching the
cores with vu with a you know LED graphs
popping up and down it was so exciting
and that program was churning me how
much it was churning was churning 20
gigs of garbage a second
20 gigs of garbage a second you know how
much of this overall CPU energy that
used 7% guess what a ephemeral garbage
on the JVM is cheap and that is a design
principle underlying closure okay
because you can't implement seek on a
vector without allocating memory right
what is the secret of Ector going to be
it's going to be a thing that has an
index in it right I'm going to ask for
the next you know for the rest what's it
going to do make a new one of those with
the index that's one greater than that
that's going to be an allocation so
there there are many data structures you
can't sequence over without an
allocation per step that's a beautiful
thing you have to see the way these JVM
is optimized that stuff away you cannot
be concerned about ephemeral Consing
anymore it's a real design miss feature
to prioritize that the other key thing
which we'll see in closure is first and
rest that's they are so lightweight
concepts that we don't actually have to
have a sequence do we how much of a
sequence do we need to implement first
and rest one thing and some way to get
at the other things almost nothing right
we don't have to have a realized
sequence or realize data structure at
all do we so it means that we can
implement first and rest lazily that
ends up being an extremely powerful part
of closure leveraging that so let's look
some more
so that's yes yes
when you start
now you do not you do not lose the
immutability guarantees of the sequence
right the sequence as it walks across
the collection caches everything it
finds so if you've made a pass you will
never get different answers if you've
made that sequence on top of some crappy
mutable Java thing and you've mutated
that thing for multiple threads you know
you're going to get the normal Java
error from that which is a concurrent
modification exception okay but know
these seeks when you they are immutable
right they're golden you walk through
you get the same stuff you can share it
you can share it between threads you can
point at different parts of it no really
okay so I've just shown you one right
the sequence you sort broke that down
but there is a similar abstraction but
behind all the associative things the
associative maps and the index vectors
and the sets when I showed you those
pictures of those literals well there's
there's a bunch of implementations of
these abstractions right what's one of
the neat thing about having a data
structure that returns a changed version
of itself is you can have one type for
when it's a tiny hash and as you add
stuff to it one of those adds could
return a whole different type that's
optimized for being slightly larger and
then when it gets bigger than that one
of those ads can give you yet another
different type for being huge all that
stuff happens on the live enclosure so
the key thing is that we're working with
abstractions and not any promises of the
concrete type
yes
yes numbers are always boxed all the
time but I'm going to except when
they're not which I have slides for so
I'll talk about that so all the data
structures have corresponding
abstractions call ability itself is an
abstraction okay
there's no reason to say only lambdas
you know can be called and you know the
list to thing of you know the thing in
the first position is going to be
hardwired to be this type of thing call
ability is an abstraction and it's
implemented by several of the data
structures okay
maps are functions of their keys well
guess what's because a map really is a
function of a key isn't it that's what
it means to be associative it's a
function right so maps or functions
vectors are functions and sets are
functions of their keys because that's
really what it should be
there are many implementations as I said
and the cool thing is this is an
infrastructure I've given you the
interfaces for these things you can
extend this yourself you don't have to
call me up say oh I wish you had this
kind of cool data structure you know
could you please add it you can do that
you can do that from Java or from
closure anybody can do it and all the
algorithms you have and all the
algorithms I've already written all the
algorithms all the other people have
already written are going to work with
your new data structure without change
okay that's the power of abstraction you
get a large library and by building no F
functions and I implementations you get
F times I power yes
yes has it affect in lighting right
it went if you were concerned about it
affecting in lining it would be because
you were working with the biggest ones
and so that optimization is going to
kick in after a number of times for
instance the JIT in Java will typically
watch your function and say you know
you're calling this a lot I'm going to
go set off a thread on the side and try
to figure out how to do that fast by the
time you're doing that you're already on
the big ones typically you're either
already on the big ones you're already
on the big size or you will always be on
the small size in other words you're not
doing something that's really growing it
but yes that kind of polymorphism does
impact inlining and that's an ongoing
challenge for the jig guys to do all I
can say is they're doing it better than
anyone ever has so far these compilers
are amazing okay so let's talk a little
bit about laziness because another key
component of the way closure works right
which the basis of it is very simple
first and rest are not produced until
requested right how much of a list do we
need to have it ends up none at all we
just need a recipe for finding the first
thing and a recipe for finding the rest
of the things that is really all we need
to say we have a sequence so there's an
easy way to define your own lazy
sequence producing functions it's called
lazy cons it takes two expressions it's
a macro it takes two expressions one
will work will produce the first value
when requested the other will produce
two left the rest okay everybody is not
going to reason and say oh we've had
lazy strings we know about lazy strings
we have it whatever but all the lazy
streams libraries right do they
interoperate with the consoles no
there's lazy this and lazy that lazy map
and lazy whatever
you can't comes with it that is not the
same okay if you have first and rest
your Asik you can cons concrete things
onto lazy things and then more concrete
things and you can concatenate them all
they are all the same from an
interpretive interface perspective it's
not the same thing as a lazy streams
library although you can understand lazy
cons is by understanding the way lazy
streams work in libraries like the ones
for scheme you can use them like
iterators or generators about languages
but they're much better because they're
not mutable and they interoperate I
already said so this is what tape looks
like take is kind of more haskell of
lingo take in things from some
collection right if it's still a
positive number and there's stuff in the
collection okay this is classic Lisp
blending you know there's a cons of
there ISM yes you're going to tell me
later how to write intake because here
you're writing it take returns of
sequence it returns anything that
implements seek seek is an abstraction
so it can return any arbitrary type that
implements seek not one particular type
seek is an interface so that doesn't
dictate the country type it only says
whatever the concrete type is it has to
support this interface and
implementation of I seek yes so that's
pretty cool so we just lazy cons
pulling first on that and then taking on
the rest okay
yes
well no you can take it and then you
could dump it into a vector if you
wanted to or you can use sub vector
which is a constant time way to get a
window on another vector if you really
want another vector which constant you
know constant time lookup yeah I mean
what yes the sequence library is still a
library of sequences it has the
properties of sequences which is you
know linear access but there's so many
things you do with that you know mapping
and filtering and all those great Lisp
functions now applied everything that's
the point it doesn't make sequences now
the best data structure for all usages
sometime you're going to take that thing
and because you know you need to hammer
on it from a lookup perspective dump it
into something where that lookup is
going to be fast because it's not going
to be fast on a seek but that's very
very easy to do any other questions on
this okay
so as I said before closure is primarily
a functional language
there was a great talk by Eric Meyer
where he said we were all wrong about
functional languages the only possible
functional language is Haskell because
it has to be lazy and has to have this
type system if I system has selected
Eclair effects etc etc the rest of us
were abandoned together and said well
we're still want to call our things
functional but we'll put this qualifier
on it all fine select so it is impure it
certainly is I don't believe in
disallowing dangerous things because you
know sometimes dangerous things are
useful the idea behind closure is it
gives you tools to do the right thing
and you know if you can do that most of
the time your program will be a better
program if you have to do something ugly
somewhere I'm not going to stop you that
I know you have to do that either for
performance or whatever so what does
that mean
foreclosure foreclosure means the court
data structures are immutable the core
library functions do not produce side
effects lap-band locals are immutable so
no even twiddling inside your function
because if you could you'd have all
kinds of other issues you know because
you could close over those Twitter
cetera et cetera also creates this awful
style right where you where you know in
the whole world you're doing this nice
thing with functions and applying
functions and then inside your functions
you have this mess where you have real
loops that trash local variables and you
just end up making spaghetti you know
little small bowls of spaghetti and you
know so we don't do that because of the
lack of tail calls and the desire to
have a sort of a functional way of
programming I need a special construct
for looping which I'll show you so this
is sort of what it looks like to use
like drop two things from the vector you
get us you know sequence of that
infinite sequences why not right cycle
returns an infinite sequence of looping
through whatever it was passed over and
over and over again as long as you don't
try to print it at the repple because I
haven't implemented print length you'll
be fine but that that is really a
beautiful a beautiful thing it will
change the way you write software to
have essentially all of these functions
are lazy they all return a lazy sequence
all of the algorithms in closure that
can I mean some things can't be lazy but
anything that can be lazy is so if you
pass some huge humongous thing partition
three is going to return just the window
on getting the first one it ends up that
makes it really easy in closure to maple
eight and work on datasets that don't
fit in memory also you've been paying a
huge price right by building your
algorithms in terms of functions that
return fully realized lists and then of
course in reversing them or reversing
it's awful and from a memory contention
standpoint it's a big deal remember I
talked about that ephemeral memory being
cheap you know what else is really good
ephemeral memory always lives in the
generation that could get tossed for
almost nothing right garbage acquisition
or memory acquisition and that in that
generation is pointer bumping and the
collects are extremely fast but when you
start allocating real lists or real data
structures to accommodate interim values
you know interim results pieces of your
algorithm we're creating these huge
guess what they're going to move into
the next generation the garbage
collection costs would be much higher
than if you used a lazy approach so this
is really great aunt relieve is lazy
works as these were all lazy and this is
just us you know just tons of functions
in closure now you always get a sequence
back from these these are the sequence
functions they'll take anything but
they'll return a sequence because
they're returning a lazy sequence if I
had to return a vector I'd have to
realize it and then all the problems I
just talked about would happen if you
want a vector it's really easy to say
you can just say into an empty vector
drop whatever and now you'll have it
poured in in fact into works with
anything you could say into a set empty
set or if set that has some stuff in it
already so it's really beautiful you can
say into something take blob and we'll
get dumped into the thing whatever its
concrete echoes so this is really neat
and it works on strings that works on
whatever range is also lazy you know
producer infinite lists mechanist
sequences
excuse me into - into takes a any type
of collection which is going to support
cons and then some sequence and it will
take the things from the sequence and
konjam into the collection so it will do
the right thing for the time it returns
the collection that's right so for
instance you can write a completely
generic map right because people say oh
I wish you had map vector right well if
you said into an empty vector map this
thing you would get that so then what's
different between different calls to
that just the thing right just the you
know I'm dumping into a vector and I
started with the vector I'm dumping it
through a set I had a set or dumping it
through a map and I had a map well
there's a function called empty so if
you had some collection called C you
could say into empty C map this function
on C and whatever C is you'll get a copy
of it and that code is 100% generic you
don't care it will work on anything
that's cool okay
so this is some of what maps and sets
look like this is def it just creates a
global thing called M right and we give
it a map maps or functions of their keys
so we can look something up by just
pulling calling the map itself also key
words are functions keywords of
functions of associative things they
look themselves up in the thing because
key words are so often used as names as
keys so it is important ice looking
style and you know if you ever wish you
had a better way to do attributes this
makes that nice-looking you can get the
keys you can get the values you can do
like all kinds of stuff
the basic operation for adding to a
societal thing is the social which is
short for associate again you can't own
this work so it takes it takes some
associative thing and then an arbitrary
number of keys values keys values right
and it gives you a new map with those
entries made to it if we look at them
again it'll be the same thing was there
affect this function
this quality or presumes that is the
same way as it was because it didn't
change here so I merge with is a very
cool thing it will take one or more Maps
and it will say pour all this stuff from
these successive maps into the baseline
map okay well what if there's already a
key there well use this function to
arbitrate the value combination so if it
is already a key take the value you have
at this key and add it but you can merge
with cons and build data structures this
way it's fun there's a whole library for
a set logic that works on real sets at
Union intersection and all that stuff
and go a higher level still what's a
relation a relation is a set of maps and
guess what if you build relations
there's a whole library of stuff for
that like relational algebra stuff like
joins when you get that Union there
because these are hash sets the order is
random because it's hash if they were
sorted you get a sort of thing all of
the literals are the hash versions just
so you know the literal map is a hash
map the literal set is a hash set but
you can get the sorted versions with
function calls I could have written it
with cons but then you have to treat
each guy as a pair write the values in a
collection that is a associative our
pairs logically so like if you seek on
am you're going to get a set of key
value little vectors
in the print right you get whatever you
started with while you're going to get
yet you're going to get whatever the
left one is yeah you get a guaranteed
the results going to be equal I got to
talk about equal I mean that the the
concrete thing you get is the hash map
or the hashed set I have to pick one I'm
reading and I see a literal map right I
have to prove some real data structure
yeah the real data structure I'm gonna
produce is going to be in the hatched
family not the sorted family I'm not
going to promise what they are but
usually hash things don't have any
sorting right right cuz then maybe I'll
come up with a new half sorted hybrid
promise sometimes the fastest way to
implement the hash is just to do a skip
list thing right it's proven when it
gets smaller but it's still in the hash
family in that when that thing grows
it'll become hashed you know there's I
mean oh I shouldn't I should make it
silent there's this huge beautiful
hierarchy of abstractions under closure
so we made a picture of it once scary so
for instance there's there's an
abstraction called sorted and one called
associative which aren't even you know
in the concrete tree of things and
indexed and reversible and stuff like
that the abstraction set is good so just
to get a feeling you again I can't I
don't expect you to totally be able to
grok closure code but just going to feel
for the weight of it this is some Python
peter norvig wrote this python code it's
a simple spell corrector this is a
little simple Bayesian thinking but
python is sort of the champ in terms of
syntactic doctrines
because it has no grouping right it uses
white space so and and late and so this
is the closure version on the same thing
right roughly the same and we all know
how to have a book normal as do our
editors in fact we know the superiority
about being able to move these pieces
around versus something twice face paced
so you know it's kind of neat I mean
you're deaf and blind you see you know
again we're we're using lists for what
we would have used lists for data we're
going to use vectors instead and this in
fact there's list comprehensions or
sequence comprehensions a bunch of other
cool stuff but it lets you write code
that's the same size and weight as
Python so saying that everything is
immutable usually the first thing is oh
my god that is going to be slow your
every copy in those things left and
right but the you know of course that
need not be the case we know for
instance of when you may add something
to a list we don't copy the whole list
we don't need to because it has
structural sharing and similarly there
are structurally shared versions of
these data structures so what my
persistent data structure here we're not
talking about the database persistence
right we're talking about the concept of
having something that's immutable be
subject to change having the old version
and the new version both be available
after the change with the same
performance characteristics as before so
this isn't a trick one there are some
persistent algorithms that produce
changed versions whose performance to
create degrades over time as you have
more new things older older versions
start slipping out from the performance
profile to me that's not really
persistence at all and it's not usable
in particularly the way those things are
implemented is usually with some real
sharing which means that they're a
disaster from a concurrency standpoint
really to do this correctly you have to
have immutable data structures which
closure does the new versions are not
full copies so there's structural
sharing this really is the Lisp way this
is the kind of map you know a hash table
or vector you should have in list
because you want to do the same
kinds of programming with these other
data structures that you do with lists
and so often you have this beautiful
thing you did with lists right then you
try to scale it up and you know lists
are really not the right thing that
would he do completely change your
program right because using the other
data structures is nothing like using
lists they don't have the same
properties they mutate etc etc that's
not true in closure so you can use real
Maps for things like environments which
is just so beautiful
and very very fast oh I don't have the
slides for that so I will say enclosures
data structures are the fastest
persistent data structures I know about
they're different they're based around
Bagwell's hash map trees they have a
depth of log 32 n so they're very
shallow and they're they're fast they're
fast enough to use this for commercial
development comparing to the Java data
structures they're for insertion times
you know like 4 times worse and for a
lookup times they can be as good or
better - twice as slow so that's way in
the ballpark considering the benefits
and when you start comparing them to
having to lock those other things in
concurrent scenarios there's no contest
so this is a small cost we're going to
pay and we're just going to be able to
sleep from the node all right so as I
said before we have two things impacting
closure from the looping perspective one
is I don't want any mutable locals the
other is the JVM will let me do scalpels
oK you've seen languages you know sisk
scheme does tail calls on JP I'm right
it does it with this whole other
infrastructure they're calling their
calling scheme is nothing like Javas
right you have to pass additional things
or trampoline or whatever closure does
none of that closure has
pedal-to-the-metal calling conventions
that match java so I don't have tail
recursion because you can't do that
unless the JVM does that so we have a
special construct called Riccar it does
constant space recursive looping right
it's going to rebind
and jump to the nearest loop which is
another special app or function
so you say I want to zip map to sing us
a loop it's exactly like let right loop
is like let you let all these things and
then if we need to keep going Reaver car
or car is going to go and hit this luke
rebinding bath kmv to this it's it recur
can only occur in the tail position
right and I will flag you on that right
and it's a go-to it's fast under the
hood but the semantics of the semantics
of rebinding well you know this
recurrent happens to target this but you
can never occurs the target the function
arguments themselves if there was no
enclosing loop so those workers could be
like a call to the same name function
but the biggest reason for true tale
call is that you want to build some sort
of network of recursive calls not same
you know not self calls and basically
what will happen is you can start doing
that right now if you do it you're
subject to stack overflow and when they
change this you will you won't be it
ends up that enclosure because of the
lazy sequences and this loop recur
people complain a lot before they start
using closure and they complain hardly
at all after they know how to do with
the closure way that's not to say I
don't want to tell bugs I absolutely do
and I'm looking forward to the JVM guys
doing it but that's what I happened so
that's a little zip map and you can
create you know setting keys and values
and they'll get zip together and get a
map out these two things will do the
same thing to some more you know you
have apply you can do that interleave or
you can dump into the vent into starting
out before as you wrote the code using
yourself implication
I don't I don't have purpose because I
get a lot of people coming to closure
from scheme and I really feel like I
don't want to tease you and I don't want
you to be confused about when you will
get a real tail call because I think
that's an unworkable position to be in
you know can I do this will I get a
couple so I just say no you will not
the only time you will is if you do this
when we have tail calls you can do the
other and that's really sort of clean
because otherwise people would be very
frustrated and understandably so I do
not optimize optimize self calls even
though it easily could okay
equality the equal sign is equality
enclosure there is also identical which
is reference equality we really don't do
that it's there I know sometimes you
need to do it but it's it's not
something going to do equal equal have
you read Henry Baker's great paper on
equality an object identity anybody
it's a great paper it's still great and
it's still correct
and he says the only things you can
really compare it for equality are
immutable things because if you compare
two things for quality that are mutable
and ever say true and they're ever not
the same thing you are wrong
or you will become wrong at some point
in the future so so he had this new
operator he called eagle went through
all this equal and EQ and eql and I need
a new name so he came up with a gal and
to find these semantics for you go those
are semantic or the semantics of closure
okay if you're comparing two reference
types they have to be the same thing in
order for them to be equal again here I
mean closure stuff there is still Java
stuff Java stuff has plenty of broken
equals and and I can't fix them so if
you call equal on some Java things it's
going to map to equals and it will be
broken sometimes because they they're
broken
yes object dot equals I can't fix that
use closures data structures so much
code for cyclic structures do you think
about things that are observation leads
well good luck trying to make a slick
with data structure I have all these
immutable parts well this cycle twin
which is not really a data structure
right it's a logical sequence I don't
have any special tests for equality
termination there because I don't think
we can I just keep those no I haven't
yet
don't do that lazy it's the same as
trying to print it at the repple you're
going to are going to consume it all
that's right I mean it may terminate
early no no no not not abnormally but it
may terminate early this will find that
things are not equal yeah right if one
is finite it's finite yeah the same
thing I mean cycle is infinite what do
you what do you expect take from cycle
and build what you want you know K
there's no other infinite data
structures there are infinite sequences
or aren't infinite vectors or infinite
maps okay so that's how it works
I haven't even talked about references
but closure does have some reference
types there are the things that have
those concurrency semantics all right
the big religious debate little false
end of stream or end of sequence or end
of list and the empty list okay let's
just do it closure has nil nil means
nothing you don't have anything it's not
the name of some magic thing it means
nothing
Common Lisp has nil and the empty list
and those notions are unified it might
have made sense because you really only
had one primary aggregate data structure
I don't write why should nil be the
empty list why shouldn't be the empty
vector or the empty whatever he's no
good reason scheme hunts on this they
don't like that know that
planning around upstream and you're sick
and stuff and Java has null and it means
nothing in Java could be any type
closure has real true and false Common
Lisp as nil or not right scheme has true
and false but they're broken and in Java
has Troy Glaus closures conditional
logic is like Colin West's I like that I
like nil punting I like saying nothing
is conditional false so but I have two
two cases because I have to deal with
false I have to deal with false false is
in Java lam
it's going to come I'm going to
encounter it I tried very hard there's
no way to automatically or automatically
do a nil false translation in both
directions it's not possible if you
think it is write it up and send it to
me but I think it's not so so I have
these two things or everything else
which is like Common Lisp it's also like
schemes because this seems to be
inherently wrong you know if you're
going to do this then do it right
I don't have objection default I have an
objection to if you're going to say true
or false and this is logical false but
no it's not well I mean it would be nice
if I didn't have to do both of these and
we realized if I could do mill and
everything else I would happy happy with
truly no I thought fine with Common Lisp
use of tea and milk but that would have
been done I tried to make that work okay
is there a special singleton empty
lesson singleton is the key thing there
are empty list values in closure in fact
they are not nothing because an empty
list is not nothing it's an empty list
and it needs to be distinguishable from
an empty vector because you want to say
into into it right if you only had nil
you have no power to have more than one
special empty thing so no those are no
you know common Wilson scheme have the
empty list here again yes you can have
metadata on empty thing you can tell the
difference tree and everything and
nothing because they're not the same an
empty bucket is not no bucket it doesn't
have a singleton empty list you can have
10 different ones whose identity differs
oh ok and it's not represented by nil
that's also sort of the key piece here
I'm not actually trying to establish the
rightness or wrongness of anything
except this just showing you how they're
different and the decisions I made which
generally as you'll see through this
correspond to common list choices so and
yes
yes
I do why would I do otherwise it could
it could but it I don't want to prom I
don't want you to start doing identity
stuff with that has not used equals okay
equals is good end of sequence okay what
happens when we're using a sequence of
you spell lists we're walking through
and there's no more okay well I say no
less common listed because common list
meant this when it said no more I mean
there's there's no more there's nothing
so that's the same and of course
combined with this it means you can do
nice elegant little loop things is that
that did name of common less good
I have host issues I have to deal with
the host so nil maps them L true and
false map to be TBF true and false in
the java flan and the library does not
use concrete types okay
all right season
okay so this is just some sort of a
closure II house closer to do some
things that other other languages do
other lips do closure can have already
overloaded functions okay close doesn't
have optional arguments instead it has
real already overloading that means that
a single function object can hold inside
it multiple function bodies overloaded
by arity that is really fast in Java and
it's quite nice and it means you can
report all kinds of things and you don't
really have any conditionals to do this
work inside so it's kind of anything so
this is an example drop last down is two
overloads you can pass the thing or you
can pass on anything it does support
variable arity with ampersand i think
the other guy argument after that is
bound to the rest of the things that
maps to a sequence which means you can
theoretically pass an infinite set of
things as arguments to a function as
well as you can try to print them or do
things you see
just
really you can build optional arguments
or keyword params on this so they are
not primitive in my opinion and they
don't belong in lambda or fun
make macros
I'm going to talk later about references
in the context of the concurrency
primitives and these are the mutable
things in the language but they're
actually here because of the mapping to
bars from common list so you can have
bars unlike common list where there's
you know sort of symbiosis between a
symbol and the value cell and the bar
that's separated enclosure there are
bars bars are in namespaces those are
the things that really have the data
associated with them symbols or
lighter-weight names bars are named by
symbols and they have dynamic scope they
obey a stack discipline that's exactly
like Common Lisp bars except that you
can define a bar at the root with death
it can be unbound set is restricted to
apply only to bars that have been bound
to read locally okay so you can't set
you can't whack the roots with set
otherwise they have the semantics of
special bars when you have a dynamic bar
you know def are in Common Lisp there's
a there's a root value you could set it
in common less you said f it right Canon
closure you could find it locally right
and the the special operator for that is
called binding it's not let so there's a
special thing way to bind these things
when it's bound then you could set it
and not otherwise it can be unbound is
really nothing special to them otherwise
they're extremely similar to bars you
know DEF ours except they have real
thread local semantics it's guaranteed a
binding in a thread will be distinct and
invisible to any other thread and you
can use that in concurrency programming
that's a hard promise
right
you get a whole new set no binding
propagation to threats because well
they're been long arguments about this I
decided no no you get a new binding set
if you want to propagate bindings to a
to a fun you're launching another thread
do it because I'm not going to do it for
you it's a new thread just got root
bindings set up bindings that you want
okay so it's very neat the one neat
thing though is because closure is a
list one and all closure functions are
defined with death they're all stored in
dynamic VARs which means you can
dynamically bind functions in nested
context that's really cool if you've
been into context style or apps
aspect-oriented programming you can do
that with closure out-of-the-box because
of this behavior because functions are
stored in bars you can rebind them in
context you want it in a particular
context add logging to a function you
can do that changes behavior I'll swap
out the algorithm because you know the
datasets going to be unusual in this
context totally doable it's very
powerful stuff and it was sort of fell
out of it's easy
the thing the thing about the the JIT
optimizations is if you're calling
something three times you do not care
right if you're calling it enough times
in a particular context the JIT is going
to kick in and make a custom version I
doubt it
for dynamic read bindings I I doubt it
would I put it past them no no what was
what was neat about to jvm languages
summit was these these for some of these
engineers because they had a lot of the
Sun guys and guys that work for Jay
rocket and as will you know this is the
first time they're sort of hearing the
needs of these dynamic languages kind of
stuff that would matter to languages
like this so they all were highly
informed by the presentations and so you
know we don't do that simple scape
analysis and if we did you know all
these box numbers could disappear and
various other things could happen
thread locals well it's thread locals
with the with a fast path that tests a
anatomic integer right which is spun up
when things are banned
so if nobody's rebounded there's a fast
test that doesn't you involve thread
locals to say can't be thread locally
bound fast that's more complicated than
that but that's the short answer yes
no it will shadow it's still lexical
right it's still electrical system in
other words I have to get all the way
down and say this thing really means
that bar at the root
if you've lexically shadowed it then
that shadowing happens otherwise it's a
dynamic system right it's not really
lexical then because I don't want to
muck up let let is so confusing when
left us dynamic binding and regular
binding in my opinion so I don't people
to know when they're doing this oh I
need to talk about it there's only one
let it's let's star I mean it's
sequential black yes
it's not going to interact through it's
not going to interact through dynamic
bindings it's still in a tail position
right it's just got a test to see if you
know which one to call still tail right
so that still can be optimized through
there's nothing about it that's
inherently non tail if you make a
decision then use this and that's the
tail call it's still without ball it's
not like you're taking a result back and
then doing something with them break now
it's not a tough line that's the only
place where you run into trouble so I
consider it optimizable it probably
doesn't get okay so I say closures will
list one and it has common lists I'll
def macro typically you would say and
that isn't gonna work right
that's why scheme has this si genic
system because you're going to have
these clashes but it ends up you have a
lot of these clashes because of the way
the reader and packages work in common
list not because of any inherent
inherent problem so if you separate out
bars from symbols then symbols can be
really simple in fact they can be read
by the reader and not impact the system
in other words there's no interning
happening during reading in fact there
isn't earning only of the names so that
symbols are still best comparable but
it's not interning of values or mucking
with your bar space there's no matter
you sell or anything like that they have
constant timing quality so they're fast
they're usable as fast keys and things
you typically use things for symbols for
when you're doing symbolic program but
the reader has no side effects it
returns this symbols can have a
namespace part and even a name space
qualified symbol is still just a name it
doesn't say there is storage sociated
with this it does not say or imply there
is a bar there or anything else it's
just a name with a namespace so
namespaces really are a way to just
qualify names they're not about packages
or anything else
you just say my name space slash the
thing and you get a name space qualified
someone it's just the name then the
trick enclosure is there's a process
called resolution which resolves names
so now you're in a name space you've
read in a bunch of these unadorned or
possibly named space qualified symbols
now you need to say what does for me
okay now we're going to go look up the
VAR name space and say okay in this name
space foo means this bar okay we didn't
mess with those names because it's just
by reading some source code or anything
else it also means that macros are
manipulating this name world not this
VAR world so they can easily manipulate
names and amid stuff into a space in
which they'll be qualified in where the
macros defined and the qualified version
will be what the macro emits I'll show
you that so I call that syntax quote so
we have regular quote quote foo is foo
it's just a symbol so we're imagining a
fresh repple nothing is defined right
back won't foo says well we're in the
namespace user that name resolves to
user foo there is no use of foo yet it's
just saying this is the way the name is
qualified which means when you use back
code in macros it's going to look up
those names in your namespace and emit
again names that resolve to what the
name means in your namespace but still
just plain names so we can see if these
two these two things are equal and
they're not okay if they don't look well
then we're not in that namespace world
we try to use it we can't we didn't make
any bars call foo
all right so let's define something
called so no problem it gives us this
this means something different
and now we can evaluate food we get five
we can say we can refer to foo qualified
or unqualified we're talking about the
same bar so it's the same value this is
five and five five and five are equal
even enclosure yeah
user foo is just a fully qualified name
we're in the user name space so this
will resolve to user food because we did
this and we did this when you get an
error in fact if there was already a
fool in this namespace this would have
found closer stricter about names
existing before you use them so this
works we can resolve foo right in this
namespace what is Funi this is a bar
right I said shark code is different
this is the bar the actual bar that's in
the namespace that has the value in it
so we name them like this and the bar is
called user foo a bar always has a
namespace qualified name bars are
objects everything is an object of bars
are the places where the stuck it's
stored not since the current namespace
you're in a namespace then we're
presuming we just started the rebels for
in the user name space there's all kinds
of functions for that in namespace name
space by namespace you can change
namespaces it's not like common list
that way right it's thread-local state
this is bound so spread local and it it
will unwind when you do a compilation in
it so in a compilation meeting the same
in this namespace that the current
namespace gets pushed gets bound a new
binding gets pushed and popped so it's
not global is no trashing from other
threads
No you can say anything you want these
are plain names until you get to
resolution it's fine which is really the
way it should be right who cares
it's this name in fact you may want to
use it cuz it name something in some XML
file their names symbols as names are
more useful than symbols that are bound
to be bars
the only cost really is the reader can't
read pointers right the common list
reader can effectively read pointers
that's a cool thing it has a huge cost
if I read right I read a symbol it gets
in turn it's storage that says if I read
in another file I get the point I get
the same storage ok just by reading I
have two pointers to the same storage
right which is cool and there's some
common lists code that leverages that
you can't do that enclosure it is it
cost yes
okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>