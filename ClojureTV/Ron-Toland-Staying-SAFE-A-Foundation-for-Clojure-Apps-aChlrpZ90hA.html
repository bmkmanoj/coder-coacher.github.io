<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ron Toland - Staying SAFE: A Foundation for Clojure Apps | Coder Coacher - Coaching Coders</title><meta content="Ron Toland - Staying SAFE: A Foundation for Clojure Apps - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ron Toland - Staying SAFE: A Foundation for Clojure Apps</b></h2><h5 class="post__date">2015-04-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aChlrpZ90hA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Ron Toland I worked for a
company called sony incorporated and I
put this talk together because I feel
like we have a lot of really good
closure technology we built up over the
past six years at Sony and we don't talk
about it very much and I think that's a
shame so I put this talk together try to
share with you some of the ways that
we've solved a lot of the problems that
you encounter when using closure and
production so for soso daeun who so
we're a small company we do email
archiving and search as you can imagine
a lot of companies now may have several
decades worth of email static across
different systems they may have used
lotus at one point they may have used
Microsoft Exchange another maybe they
use their own linux box for a while and
but they still want to keep all that
keep it safe and also be able to get
quick access to it at a moment's notice
so that's where our company comes in
we'll take all of that email across all
those systems ingest at all archive it
in our own proprietary system and then
we put it into elasticsearch to give
them really fast rapid searching of the
text in those emails so if you want to
pull all the email that Bob sent out
between january of 1999 and december
2001 we can give that to you all our
back-end services are built in closure
in fact we were one of the first
companies to use closure in production
the same for elasticsearch so I've got
six years of experience doing this and
put millions and millions served not to
invoke the mcdonalds metaphor to
strongly but you know a lot of people i
think the word at scale has become kind
of a buzz word so I hesitate to say it
but I do one emphasize that the system
in production handles between 10 and 20
million emails every day new emails
coming into the system so we've had to
respond to a lot of different stresses
and loads over time to come up with
these solutions that we've got so what
is safe so safe is our solution was our
main solution to this problem it's the
Sonia archive file engine what the
abbreviation stands for and it started
life as our main application for this
email ingestion and indexing into
elasticsearch but over time because of
the the stresses and strains and the
problems that we've encountered
in growing the system and and being
flexible about where we've deployed and
how we've deployed it and things like
that it's actually turned out to be a
really great foundation or platform for
building additional closure services
rapidly I'd say foundation because it's
it's a little bit more than just the
library although there's nice coat in
there that we use as a library yes but
it's a little a little less than a
framework it's not quite as opinionated
as some of the other frameworks out
there so I thought it as a foundation
some that a base that you could build
other services on top of and when I say
different services I mean very different
we've not only building a different
metadata system for handling email
metadata like the headers of the email
the to the from that kind of thing but
also a job tracking service
elasticsearch processing service we're
getting our diverse array of services
out of this foundation so every time you
design or build a new back-end service
the zetter problems you're going to have
to solve among them I've listed a eight
here yeah you know how do you deploy it
how do you get it into production how
you you know put new code out there when
it's ready how do you configure it so
we're going to want to not hard code
everything where I configure some values
how am I going to do that how do I debug
it so when I encounter problems in
production not if when how do you track
down what's going wrong and be able to
quickly fix it how do you make it
flexible so if you need a run on both
linux and windows nodes your codes going
to be different your service is going to
act differently you know how can you
handle that flexibility adoro con run
code on startup so when the service
starts I might need to connect to a
database or connector rabbitmq
connective elasticsearch you know how do
I make sure that code gets run and
consistently at our own periodic task so
we're all familiar I think with setting
up cron jobs to run so they periodic
cleanup tasks or things get run well how
am I going to do that in this case when
for this service how am I going to give
it commands so let's say I need it to
act you know differently on the fly or
have some manual way to kick off a
process am I going to use a rest api to
do that can i do is do it over command
line how am i going to do that
and how do i distribute work over
multiple instances so as the load scales
one way to respond to that is by
spinning up multiple copies of the same
service and then distributing the work
among them well how am I going to do
that how am I going to make sure that
you know this one note over here is an
overloaded with too much work and this
other node is you know underwhelmed with
too little work to do but it sort of
sitting idle how do I make sure that
it's well balanced across there so safe
answer is all of these questions for us
which is one reason it's a really good
foundation the parts that I'm going to
talk about today only cover six of the
of these questions though the
configuration debugging flexibility
running code on startup and periodic
tasks and then giving it commands when
it's running so the three specific parts
I want to talk about are the
configuration system our modules system
our way of loading different code on
start up and running it and the safe
controls which is our command line way
of giving safe commands when it's
running so config first problem solved I
know how do you configure it so for us
our system on systems on startup finds
all the config files on the classpath
name config dot something clj or JSON or
EDM finds all of them and merges them
together into a single close your map
I've got an example on the right-hand
side there of a path to a config file
etsy safe cork and fig clj and then just
beneath that I've got a sample map they
will be used for configuration and you
can see it's just a normal closure map
you've got you know keys pointing to
with strings as values you get keys at
point two you know nested maps as values
vectors in there you can even you notice
at the bottom for the max upload if it's
a dot clj file it'll evaluate the the
forms that you put in there so instead
of having to write out the long sequence
of numbers that is you know 15 gigs and
bites you can just put the calculation
in there and also a little comment so
that you know what it is that you're
that the number that you're going for
there and you can imagine that's much
easier to update you know 15 gigs the 14
gigs just by changing one little number
here in this calculation that is to
recalculate all the digits yourself and
then put it in there and then getting
values out of that config map it's
pretty simple we have a function config
that you call to get values out of it
I've got two example calls down there at
the bottom one is config core app so
that first keyword there tells it to
look inside the config file under the
core directory so that would point at
the sample 1 i've got there and then at
means you know pull the value out at
that key so in this case we'd get the
string safe sample at the second one
there is just an illustration of using a
value there that happens to be a vector
and a for loop so config core modules
means go to the config file under the
core directory pull out the value in
modules happens to be a vector let's
let's iterate over that in a for loop
and use that in some way i do want to
point out we're using we've following
the convention of using the hash
underscore for commenting in our config
files just because we found that using a
normal double semicolon we tend to if
you have any sort of typos or you hit
return in the wrong place or emacs
reformats your file for you improperly
because it's being helpful that that can
lead to you know bugs in your config
file you don't want and the hash
underscore is actually much less air
from so that's just what we've adopted
so as I mentioned it's loaded in order
so files that are found at first on the
classpath all variety files that come in
later and this is really handy we want
to have one set of configuration that's
the default checked into github for if
it were deployed to a dev or a QA
environment but locally testing on our
VM we don't want the same configuration
we're not going to connect to the same
RabbitMQ with the same DB or anything so
we want a different configuration value
there well if you put that that matching
config file inside the test directory
for like a lining and project when you
run line test it'll find that config
first and so it'll override everything
you've got it's kind of nice automatic
way to do it
also it turns out as we building
services on top of safe that a lot of
the configuration that safe has by
default we don't want to change it's
it's got you know timeouts in there that
we like it's got you know max off load
limits that we like that we don't
necessarily need to change from service
to service the service and because of
this overriding feature we don't have to
as long as we make sure that the config
file for the new service is first on the
class pass it'll override all the safes
values so we can keep the ones from safe
that we like and we can override the
ones that we need to override for the
service I like the sound effects a that
was nice so also another little
consequence of this is that safe
configure if your config file gets
too large again it's going to find all
the config files on the classpath emerge
the men so in safes case our core config
file if we were to put it all together
it would be several hundred lines long
which you know the reader hand is fine
but for humans it gets a little lengthy
you want to break that out a little bit
maybe a long lines like maybe you want
your amqp configure ravening cute config
in one file your elasticsearch config
another maybe want your DB config
another file just sort of largely break
things out so our config system can
handle that I've got three simple paths
there you know that all three with the
same room to get c safe done under
different sub directories in a core amqp
100 vs all named config dot clj but
because we've broken them out this way
we can still call into them using the
same config function we just put in a
different value for the first keyword so
config corps app will pull that app
value out of the core config file config
amqp work like you will pull the reply Q
value out of the file that's under the
amqp directory and similarly for config
yes auth username that will reach into a
nested map that's under the auth keyword
in the config file in the
directory so it's a nice way to keep
your config logically organized and this
particular piece of safe is open sourced
we pulled it out into library called
carica and that library has been pulled
back into safe and is used in production
so you can check it out alright so i
want to next piece is the module system
so this is the way that we would make
the safe server flexible so we have to
actually run across multiple clowns we
have to run ahead of ya script to run a
rackspace we have to run an IB ins
unique system and we also have to run
across multiple operating systems so
there's some email formats that we can
only process using a windows program so
we have to run on Windows nodes as you
can imagine we don't want the same
modules the same code running on the
linux boxes as the windows boxes we need
totally different sets of namespaces to
be pulled up and brought in and that's
how our our modules help us do that and
are also a modules give us a standard
way to write code that needs to run on
startup and also run time tasks so our
system hangs off of a config auto
require so I'll as we saw on the sample
config file here you see under the
modules key is the list of namespaces so
that's how that's basically how we
decide which modules get loaded up and
run for each safe service we have
several written out into a config file
and then environment variables which
will switch out which type of safe
server is starting similarly when we're
building on top of safe the new service
that we define we define sort of not
only that the code that we've written
but also in the config saying all right
load these three namespaces from safe
these modules from safe but then load
these three new modules to give you that
new behavior so it's completely
configuration control and each one of
those namespaces on startup will get
automatically require
into the running JVM and then we have
four startup and shutdown tasks we have
to macros doing it and do fini so the do
minute gives you a single place in your
module to say all right if this module
is loaded when safe server starts you're
going to run this code and on startup
what safe will do is go through all of
the doing it's that have been defined in
the modules that have been loaded
according to config and run that code so
whether that's connected to a database
or connecting to rabbitmq that sort of
startup code will happen there and
similarly for the do Feeny's that's code
that gets wrong on shutdown so if
there's any cleanup tasks that your
module will need to do or things you
need to take care of that's where you
would put those inside the module we
have tix and talks so we have def tick
and def talk to macros for defining
periodic tasks so when you should are on
each of our safe servers there's a cron
job that runs at the 41st I wish it were
42 but it's 41 it's the 41st minute of
every hour and what that does is use one
of the safe controls I'm going to go
into more detail later but it runs a
command line command to save say hey run
all the tix and talks that you have
defined and safe will then look through
all the modules that have been loaded
and then run any of their tix and talks
that are there we have functions I'll
show you in a second that help us narrow
down a bit particularly will give us a
little more control over when exactly a
thing is running well we can be assured
if we've defined an in-depth a chord F
talk I'll have a chance to run that
schedule time modules are also where we
define we use a macro called do status
so there's a standard safe can command a
safe control called safe control status
that you run just to ask know how you're
doing and it'll respond with you know
I'm with a certain basic set of
information
like you know if it's running what types
of safe note it is how long it's been up
then also any additional information you
want to make sure the user has you can
define in a module as part of new status
anything you want to put in there you
know I'm connected to postgres and this
is you know the IP address of the server
I'm connected to or I'm connected to
RabbitMQ and I'm listening to these
three exchanges you can define all of
those inside the do status gives you a
nice single place to hang that modules
are also where we use a macro called def
admin and which is where we define
additional safe controls additional
command line commands that we can give
to the to the running safe spoilers
because I'm going to talk more about
those later but the point is that you
know all of these things that were that
we're getting is a single place to hang
them all and because it's configuration
controlled through config I get really
really you know fine-grained control
over the behavior of the safe server or
the additional services that we built on
top of a couple examples here everybody
read that okay I know that for some of
the distance that may be difficult no
one's injecting so it must be fine okay
so this is a sample of a do minute I use
of the two-minute macro so in this case
is be light for an HTTP module and what
we've seen here is when you the same
server starts up always start up a Jetta
instance using the routes that are
defined under the rest apt symbol on a
port that if you'll notice is controlled
by config we're pulling out a value
config core rest services port and using
that in here so giving us again you know
config to control what this module is
doing an additional control you know
just below that we're doing checking
additional config value if we're
configured to run the external rest
services so an external API then run
second jetting instance with the second
set of defined routes on a different
port and start that up so again a single
place you know inside the module to run
okay everything we need to do and start
up this is where it is
a sample of a death talk definition so
it's it's lengthy but all the work
actually happens just at the very bottom
there that last line stage sample files
that's the function that may eventually
get called but the wind check at the top
is saying okay I know I'm going to be
called I'm going to be run on the free
first minute of every camera but I don't
want to run every hour I only want to
run you know once a day on a certain day
so we have a wind check there to check
you know is the current our one so is it
141 in the morning and is the current
day of the month equal to the configured
day of the month the config for smtp
talk day of the month for this
particular talk to run so we can control
in config not only that this talk is
running at all by loading the module but
also when this talk gets run by
controlling this config value and this
is a sample of a do status so here this
is for like a rabid mq module here we're
getting a sequence of queues and then
we're telling the user this safe note is
listening on these cues and then we're
looping over the cues and you can see
we're using the the doric library to
build out a nice nicely formatted table
of values of the the name of the cuban
its priority and then we're staying it
off to the user so one thing i want to
point out for all of these all three of
these the depth talk they're doing it
the do status is that the in no way have
any idea or need to know about other
modules that have been loaded into the
system you know this this module needs
to tell the user when it asks for status
about the rabbit and Q's its listening
on and that's it and it doesn't have to
know need to know about anything else
that's going on in the server system
it's completely independent in that way
and most of the these pieces the the
Deaf talks that do minutes of things
like that have been open sourced into a
library called carousel
the one piece that's missing is how the
pieces i'm going to talk about next to
the the safe controls which is death
admins i mentioned a so the safe
controls give us a command-line
interface to send commands to safe and
they help us solve problems like you
know debugging and give me a commands in
the fly which it's important for us as
developers to be able to do this you
know what we're testing a system you
know on dedrick you way to make sure
it's running but it for us it becomes
really important for customer service
and ops people to be able to do this as
you can imagine you know they're on the
front lines of the customers with the
client signs up for our service and they
go through the email ingestion period
then they go to do a search for email in
1999 and it's not there we're going to
be upset they're not going to call me
they're going to call customer service
and they're going to talk to the ops
people say hey what's going on and I
don't want them to call me I want them
to be able to you know solve the problem
themselves not only because I won't be
bothered but also because it'll make
them more responses to the customer
right I mean if they can answer the
question you know in five minutes
themselves and fix it if there actually
is a problem then that customers happier
you know we get more business and and I
get a better bonus just kidding about
the bonus anyway so so it's really
critical that they that we add these and
we often add these when we find that Ops
has been doing something over and over
and over again and it's starting to
frustrate them will add a safe control
and solve that problem for them and give
them a nice single way to do it so this
is just a few sample calls of how you
would invoke safe control so the first
one is the safe control status i was
talking about the second one is the safe
control util rebel this is how we get a
live rebel on a running safe node so you
know sometimes when you're debugging you
know the existing safe controls we have
aren't enough and we just need to get in
there and see what's going on so we get
onto the note issued petrolia to rebel
and we're in a live rebel system we can
require anything spaces we have we can
you know run code and really find out
what's happening and
and what's going on in the system so I'm
just going to check on time here think
doing okay alright and the last one
there is an example say control we're
going to show you the definition for a
little later on safe control import ml's
dash dash account 42 dash dash URL
walrus bucket EML because he's always
looking for it so you can see it's just
a it's really very simple
straightforward interface you know he's
easy to use if you issue safe control
help any of these commands you get a doc
string that tells you how to use the
command and what sort of arguments it
takes which is really nice yeah and so
for us the the benefit here is that it's
important that we be able to define
these commands to be able to define new
ones but also that we be able to find
new ones well and so our def admin macro
gives us that it gives us a nice single
place and a good way to define these new
commands so that we know everything will
be in place and ready for ops to use it
when they need to I'm going to want to
go through the docstrings invoke with
bellator think it's easier to see an
example so here's a the full definition
the Deaf admin import emails I'm here I
just want to know the different pieces
i'm going to we're going to zoom in on
those of a bit more but you can see the
you know the general structure to the
deaf Edmund some you know name symbol a
doc stirring a set of args and then
invoked with valid arnes function call
along with a set of flag sets and
dominators so just a zoom in a bit on
the first piece so def I've been
importing emails so that the import
slashing ml's is what allows the safe
control to be able to decide that it's
going to run this code so if you looking
back at that example state control
import FML so we have a shell script
safe control that when you invoke it it
will take all of its arguments the
import emails
Josh account etc turn it into a vector
and send it to a socket and then safe
has a socket server listening on that
socket and it is using basically a big
def multi and it uses the the first two
arguments there to switch to dispatch on
which one is getting invoked so by using
import / emails what we're saying is you
know this a.m else command is going to
live under the import the important they
all the other import commands so if you
do save control help import you'll see a
list of commands that have been defined
under including this new one this emails
and then the particular invocation here
is will be in the amount so you'll need
the right to do save control import
emails to get this one then fall in that
we have that the docs during in this doc
stirring is what defines the help text
that a user sees if you say say control
help import emails and we're this is
actually it makes same lengthy but this
is actually really short I had to cut
this into about a third just to fit it
on the slide because we're very very
picky about what goes into these
docstrings we really want them to be
detailed and up-to-date and very very
complete again not just for ourselves so
that you know we have helped text
available we go into debug the system
but for the ops people like we don't we
don't want them to have to you know
scramble for a manual or look things up
all the time when using these commands
and want them to be able to get you know
on the spot help we also want them to be
able to use the commands correctly every
time and if they forget you know what
you know how the dates should be
formatted if the rain putting you know
from and two dates to a command we want
them not to have to use trial and error
especially in a production system we
want them to be able to know exactly how
to format it so if we can have dates in
there will list out sample formats and
often at the bottom of these will list
out full in vacations to show exactly
you know how you should put in arguments
and every
to the safe control so you can use it
properly and then at the bottom there
we've got just a variable number of
hours that this function is taking so
the next piece is the invoke with valid
args and again you see that it's it's
taking in the arts that we got there's a
function there do initiate emails import
and then there's a map which has flag
sets and validators and what this
function does this handles basically all
the validation for all of our deaf
admins so it's saying take these
arguments and give it to this function
to initiate a ml's import but only do it
if the arguments that you've got match
these parameters define it of lag sets
and only do it if the arguments that
you've got also pass validation defined
the validators here so the flag sets we
have designed here defined here you can
see we're using sets to define what what
what arguments are valid so the first
listing their the set of sets accounted
URL what that's saying is that you must
pass in account and URL when a book in
this command if you just send a count or
if you just send a URL argh it's going
to kick out an error message to the user
saying you're missing arguments and it's
not going to invoke the dude initiating
ml's import that's not going to get
called at all the next piece there is a
finding some optional arguments we have
zip password and foreground and the way
that those are defined using the options
there in sets of one is saying that you
they could pass in a zip password they
could pass in a four-round argument they
don't have to but if they do they can
pass either one they can pass both it
doesn't matter we also have ways of
defining options options so that if one
optional argument is passed another one
also has we passed so you can imagine if
you were to take a username and password
at the command line one of them is not
useful you need both so but they might
both be optional arguments so we have
ways to define that as well so if the
right kind of arguments are there if the
account is there and the URL is there
maybe zip password for banner there okay
then we got to pass everything's gotta
pass validation so here we've only
defined one validator for the account so
we're taking that in and pulling the
account out and we're checking to see
first of all that they send us just one
if you send us more than one or come
account I don't know how to process that
I I'm not going to do anything so it'll
fail validation and again it'll kick out
an error message saying invalid accounts
and the do initiate handles import will
not run okay we'll say they pass one
account but they passed an account that
we don't know about that the same server
has no idea about well we check that so
we ask is it a valid account is it is it
a number for using my secret IDs for
this you know is it is it actually in
our database do we know about it again
if not throw an error message don't
invoke the function so that we know that
when do initiate emails import gets
called it's got all their arguments that
needs and they're all valid because
we've defined all that here it gives us
a nice single place to do that now an
actual safe we have a long list of
default validators because you often
need to check accounts URLs things like
that over and over and over again so we
actually have a default set that gets
invoked almost every time and you can
define additional validators as you want
open source soon so this is like I said
the piece is missing from carousel there
we do plan to have it open source soon I
personally really want to have it out
because I think it's really really
useful and I'd like to see it used more
often alright so that's all I think I
might actually a little bit early may be
right on time I do want to mention that
we're hiring like a lot of closure
companies it seems especially after the
little informal survey yesterday one of
the three of us apparently is hiring
people we're also hiring people so if
you like what you saw here if you're
like man you know it'd be really nice to
have all those problems solve for me
when I go to
the back end closure service I could
focus on other things you know other
problems to solve please come talk to me
afterwards or grab me at the puppet labs
party which i plan on going to and let's
talk thanks
so I think we have time for questions if
anybody has anything there additionally
curious about or if I've answered all
your questions that's also fine I see it
I see a hand pointing but I don't see a
hand up oh I'm sorry I'm short I can't
see your are you talking about like with
the sit with the the safe controls like
customer service messing it up so our
defense against that first of all we you
know we have good customer service
people and second of all we that invoke
with valid args lets us really be strict
about the kind of ordinance that will
accept and in certain very dangerous
things they actually will ask for this
it's funny enough the last four belts
and suspenders basically like we're it
was one case where they not had to like
not delete an account but like delete
you know some set of emails or something
and like you know if we invoke this
command you we have to give you the
account to find two different ways and
if we don't do that reject it for us
like okay yeah we built that end so well
the the lenox knows themselves you know
are protected access to their so not
everybody has access to the to the nodes
not everybody has access to the command
line not everybody has access to the
safe control invocation there yeah yep
right here on the third row
instead of Corwin courts are quartzite
so the question was why we're using
chron plus our own little you know job
scheduling instead of courts are courts
I'd the second half of that question is
hard to answer because I don't know what
courts are courts I'd is I do know that
a lot of this yeah depending on when
when courts are courts I came out this
problem was written before that a lot of
this code was actually written in
2009-2010 or these systems were built up
then so a lot of things that may take
for granted now and the closure
ecosystem simply didn't exist so it was
sort of invented here as far as why cron
particularly is use fuss around rob
scheduling I don't know the historical
background the question so I can't
answer that part I guess that's a very
unsatisfactory answer i'm sorry i've got
a question here in the shadows i can't
see your face but I see your hand you're
sort of two-thirds the way up yes you
sure so the question was can talk about
how we test in this environment unit
testing integration testing well it
turns out that unit testing is actually
pretty easy because all the all the
modules are doing in particular is
loading up namespaces so if I require
the same namespace I can test all the
functions and they are no problem when
it comes to things like configuration we
have a way to override config so say I
want to test out if I have a bad
configuration for connecting to RabbitMQ
you know one of my function is going to
do in that test itself i can i can do a
rea three deaths and invoke the override
config command and override that
particular configuration and then run
the functions that i want inside of that
sort of you know badly written now over
a config environment next question maybe
I'm looking I see a hand back on the far
left side yes you
okay so I copy most of that I think that
the question was what was the motivation
behind riding and command-line system
for sentence commands instead of site
using you know writing a web service or
resting face is that correct okay so
that's all the historical question which
unfortunately I can't answer completely
but I can say that one of the things
that the command line interface gives us
that arrest servers can't is that it's
scriptable for people in ops so I know
of instances where there's certain
series of commands that they run often
or they may be they get some log data
and then they run safe controls based on
that log data they look for more stuff
in the logs and they pull it aloud and
that's something they're able to do
because we've written this command line
interface another thing is that you know
uh safer's we're back in guys back in so
we know so it's certainly a lot easier
for us in terms of speed and ease of
testing and turn around to you know
develop a new command line interface and
it would be for us to add a new rest
endpoint with you know new set of
arguments and then talk to whoever's
right in the front end to have that also
updated we can just update safe send it
out done you know it's it's much faster
that way other
I'm saying another pointing yes thank
you you in the middle there you're very
good pointer thank you yeah okay so the
question is going to talk about the
immutability config like how fast is it
refresh your change so our config is
right in once on startup and that's it
so if you want to change your config
file you you have to shut down the
service and restart I'm saying I'm
saying more pointing oh ok sorry the
lights like right there man yeah you how
do we handle different configurations
between our builds sorry I'm trying to
parse that ok so what I think you're
getting at is we we have the config is
written out to each safe node in the
different environments by chef so chef
controls or chef knows you know this
safe is going to be starting up in QA
it's going to need this set of config
this safe is going to be starting out
but production is going to be the set up
from shake does that answer your
question there
so the question is we have different
config files for dead versus key way
they don't have a variable that we look
for unload and the answer is no because
it's both so we have some of the config
like for the different flavors of safe
safe that's going to be processing PST's
which is going to run on a Windows node
like what modules it has that's all
written in the the basic config file
that goes everywhere and that is
switched off based environment variables
and then also chef overrides some config
based on the environment when it makes
sense you know things for chef to know
about them that we as developers don't
care like you know where the elastic
search instances that we're going to be
talking to we don't care just tell us
where it is we'll go I'm trying to
screen off the light there so I can see
any final questions or I don't see any
i'm looking at pointing guy he's not
pointing so I think we're done alright
thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>