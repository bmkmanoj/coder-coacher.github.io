<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From Concurrency to Parallelism - David Liebke | Coder Coacher - Coaching Coders</title><meta content="From Concurrency to Parallelism - David Liebke - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From Concurrency to Parallelism - David Liebke</b></h2><h5 class="post__date">2012-12-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZampUP6PdQA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">handsome humor but this book we have in
common all right I'm going to talk about
parallelism enclosure some of this has
changed a bit since last week because
rich is always busy doing things all my
entire presentation just can be text
like this so you guys just read along
maybe I'll our house advance but so
obviously closure is excellent at
concurrency we have lots of concurrency
primitives and constructs and the goal
and in rich actually I seem pretty
ambitious goals in terms of parallelism
is to extend the constructs we have for
doing parallel processing and even more
viciously to see if we can do
transparent parallelism me make that
decision on the behalf of the processes
which is particularly efficient but so
right now we have team at which I think
most of you are familiar with and where
there's the power branch which many of
you may have seen I don't even remember
when you first created that and it's
based on jsr 166 full joined by EE and
right now I really consists of two
public functions PD map and pd reduce
which is for parallel vector map and
parallel vectors reduce so it really it
takes the only arguments it takes two
vectors at this point but it takes
advantage of the structure of persistent
vectors and kind of interesting ways
which I'll go over I do want to talk
about I'm going to start by talking I'll
keymap show how that works just to kind
of shed light on where it's where it has
strengths whereas weaknesses and if you
understand the kind of how it works it's
a little easier to understand the first
part of trying to do this work was
looking at that code richest code which
is a very tight and kind of unraveling
and understanding what's going on and
converting it to pictures which is what
I pretty much how i like to think i
think rich actually starts with pictures
puts it into code and then i had to
31 and get pictures of ending channel is
happening so we'll start out over P map
it's audra them some of its weaknesses
I've done some very minimal performance
characteristics just to get a sense of
how it compares to map show chunking
which is a way of improving its
performance in certain circumstances
then I'm going to talk about full join
in a very at the most general level and
show that algorithm I'll quickly do an
overview of persistent vector data
structure and you'll see there's a lot
of similarities in that structure to the
wave work joint works and then I'll just
go over FJ b tree which is the
underlying function that's private right
now but I actually think it should be
public it's stands before joining vector
tree and it's what drives PD map and
keep you reduce its private because it
relies on using some private aspects of
TD vector in height in order to make our
work we actually have to make those
public but I think that's actually a
good trade-off and I think if people see
how they can do kind of general fork
joint processing enclosure and
understand the underlying data
structures I mean there's a lot a lot of
possibility interesting things that
people can be doing and I want to
encourage that I'm just going to quickly
show an example of creating a parallel
version of filter based on TV reduce all
right so can even copy map lazy me to
parallel so just you're going to be
seeing a lot of this particular slide
this is just represents data that will
be mapping or reducing over its 4096
individual points which sort of hurts
keynote but we're going to focus on a
small software that
so T map is lazy or semi lazy and
laziness and parallelism don't really go
well together kind of naturally thing so
it's semi lazy so it's looking at the
number of processors that are available
in this case I have just two processors
and it's going to add a couple more
threads so he'll end up with four
threads and it's going to create the
futures actually creates for features in
this in this case which each run in kind
of a cached thread pool this is the
thing that rich has just changed made it
so the bindings path down to these
threads which it didn't do before making
future much more powerful and it was
before but it's lazy so it spins up
these things and then as they complete
so this first one is completed no more
threads when we spun up until it's been
consumed and once it's been consumed a
new a new future will be created you can
see now we've got two of them they're
completed but it won't spin up anymore
and then it just basically just walks
through trying to stay ahead of the
person the consumer of the data now this
can lead to a problem if the consuming
process is slower than the process is
actually doing this producing this data
then these things are going to finish up
and no threads are going to be utilized
it's lazy but you're not you're lost
power LEDs and completely so now that
they've consumed the first the first
item and I've got one future going and
on so instead of using your four
processors you're really just getting
one of the time and
weakness or what lated is in parallel or
anything so another area where P map
could possibly have issues is if the
load is a document so we've got a
process this mortgage process which is
still running everything else is being
consumed and we're just going to
consuming is the consumers now pulling
off from the front but this guy is not
done and we move for now now all the
thread all the teachers have completed
the processors the three processors or
idle and we have to wait for that guy
now is complete we can consume consume
consume and then wait wait and wait so
this is one area where keymap is
potentially problem now everything I
look that I having to stimulate a
situation would really make this
effective but for joint has a way of
dealing with this we're still works all
right so and you did point to diss me
all right so this chart represents it's
the difference in performance from map
and key map based on the function that
you're passing so I basically a test
function that I can control the duration
that are crimes so up there at the top
it's point oh five microseconds or
milliseconds so it's a very relatively
quick process and then we come down here
where we've got processes that will go
on for four milliseconds the faster you
process is the difference in performance
between P map and map is large in favor
of math which so we all know that you
don't use key map when you have for fast
process it just isn't it isn't it
worthwhile this is the on the left side
this is actually in percentage so we the
difference in map and key map again this
is just one
and on a 2-quart machine and I would do
a lot more before I make this kind of
global statement but in this particular
case is 1220 times slower but it's a
very short printing process this side is
actually just wrong milliseconds and you
can't even see the time difference in
milliseconds there so it's it's not
really in rallies on a huge hit
percentage-wise is massive but is it get
bigger is the time as the time it takes
for your function of the run gets longer
it the difference becomes kind of
indistinguishable so right here it
doesn't really matter map key map
there's no no real discernible
difference and then when your processes
get longer then keymap really starts to
take off and we've got its again this is
just this is the worst case scenario
this is two cores not even
hyper-threading and we've got twenty
percent does it almost yeah almost a two
hundred percent gain right there so
that's really effective and it really it
just depends on the characteristics of
the functions that you're mapping over
so one way to trying to make your
functions take longer effectively is to
do this chunking so if we're going to
look at a bigger section and you're
going to partition our data I mean it's
just partition 32 is going to create sub
chunks and then we'll the function of it
will pass the P map will actually be a
function that calls a map on that chunk
and it just does what we seen before but
on just bigger seconds this this
actually improves performance so this is
chun sizes we've got two at the top and
128 the total vector for this data was
512 in length and just by so chunks of
size two who were still a slower this is
prepared a standard map slower than
standard map but not not too bad and as
the chunks get bigger we you know it's
the same thing same kind of region we
got a region here where it's sort of
interesting
and as the chunks get up to 128 then
we're actually getting an advantage now
this is for a function this is this is
the function at the top of the last
chart it's the fastest the fastest
function which is the worst case
scenario for keymap and this is just a
way of trying to improve the performance
of P map in those situations chunk it up
so yeah so there we go it's this is one
way to deal with that alright so so
that's key map so fork joint is work
Dudley's done it's a divide-and-conquer
algorithm for doing processing and
parallel so the first thing it uses his
concept of a deck and I wanted to
quickly go over that so that's kind of
understood a deck is a double in DQ fork
join kind of iteratively breaks its task
into smaller pieces so here it is
breaking the first job into smaller task
putting half of it on the deck and then
continuing to do that so what happens
here is that the largest task end up in
the back where and then job is processed
things come out the front but steel are
taking occurs at the back because the
biggest chunks are at the back it makes
the work ceiling offer them more
efficient that way and so that's just an
example of kind of objects work the next
set will gets more extensive so here we
go we have our original data set this
indicates this is so this is the chunk
of work it's doing this is the current
thread we've got 4096 values in it the
first thing is going to do is split that
and to push habit on have push half of
it onto the deck and it's going to break
the current task again in half and it's
going to continue to do that go back
going to continue to do that till it
reaches below the sequential threshold
we have there was a 256 items once it
reaches 256 it'll actually do its
process but in this case it will just
keep breaking
down this is the particular example
we're going to have for workers this is
just a single worker and got a key down
here this this obviously this represents
a cue this is the queue this is the
current thread so this is now we're
stealing so this is the second worker is
going to pull off the back of the deck
take that chunk of task and go with it
and it's going to do the same thing it's
going to split in half put half of it on
its own deck and then two more workers
come in so now you've got all four
workers with equal sized chunks of data
that they're going to work on you and
they will continue to chunk split two
jobs in half and then push half of it on
to their decks and so on so now now
we're at the point where we've got 256
values this is the sequential threshold
for this particular example and they can
actually do the work that they were
intended to do this is in the vocabulary
of for joint and vector tree this is
going to be the leaf node function so
we'll be passing the function in that
will be able to operate when we reached
our threshold and
so then work completes and each worker
starts popping off things from their
deck and doing computing on it and once
they have a couple completed tasks they
can actually call second function
combine function in terms of joint
vector tree or just join in the generic
for joint sense and once that's done
they pop brake push and so on so this is
the basic scenario and go back there so
this is actually the years in that name
another example it's work stealing so
this orange worker is operating slow bit
slower than the rest everyone else is
much further along in the boot and the
blue workers completed all the task and
doesn't have anything else to do but it
sees it there's that work that's not
being on the back of the queue for
orange and it just takes it and starts
processing in itself and then orange
completes its task and takes back a
little bit some of that so this is the
work stealing this is really a way to
deal with load balancing it slows uneven
blows where P map potentially it's going
to get stalled on slow a slow process or
uneven workload distribution and there
is no way to kind of work around that
fork joint hassle have a way again I
haven't been able to effectively
simulate examples that show that the
disadvantage although the perfect well
going the statistics percent of the
variance of the performance of fork
joint is much tighter than it is 4pm so
on and on until the tasks between
adjoining everything that's been
concluded all right so how does this
actually operating in closure
now we're going to be going to be a
quick summary of a persistent vector
which is not necessarily it's something
that led to sell to a prick sorry but
persistent vector has it has a tail and
it has a root and the root is a node no
it is just an this is a java object
enclosure root is an object that can
have 32 children nodes well it's a note
they can have 32 children those in the
entail is effectively it's just an array
that can hold 32 object references as
the truth when you as you fill up your
precision vector the that object
references go to the tail until the
tails fall and then they get pushed into
the root node and then that continues
until all of the 32 children notes of
that node or filled at which point when
you create some more you actually take
that root node that node and you create
a new node and you make the brute a
child of that and so now we have two
nodes that have 32 those within them and
that just keeps filling up and you kind
of just grow your data this way so you
end up with a tree structure this is
this square representation of the tree
structures called tree view which is
what you saw in the fork joint example
and none of this really matters for this
example but just as an overview that we
got this index which is telling us the
index value of this particular point
here we've got the green box which gives
you basically what which node within
this box are we talking about the red
refers to which child here or blue
and the red and the red is which of
these children so we got 32 32 and four
so that's just a detail it really
doesn't matter but it's just to give a
little detail parallel vector which is
our persistent vector so the key to
using the key to fork join was to
actually a Richards approach to parallel
as an enclosure is to have it work on
closures existing data structures this
is kind of a different approach than
what what other languages are using
they'll create parallel versus vector or
parallel versions of math and other
things so so then so that's that's
that's a foot
alright
alright so fluke join TV map and keep
you reduced or based on for tomm
defector tree and fork joint vector tree
takes the vector and it takes a
combining function in a leaf function so
this is the current thread same exact
thing we saw before except this time
instead of splitting in two with
breaking into the four we have a
branching factor of 32 just like that
persistent vector we saw before the top
level knows there is only four which is
what's going on here so we break it
before then we break it into 32 so
instead of breaking q team to break it
with a branching factor tune our branch
effect over 32 we take the current
thread takes the 32 epic references
pushes everything else into it the deck
and then calls computer for Kiki map
this is the combined are the leaf node
function it's basically we're just
mapping we're taking it takes in the
node which is that object that's tribal
that we need to make public takes in the
node then runs a map since it's actually
it turns it into a Java object array
runs a map on it and takes the output of
that and creates a new node so in the
persistent vector this was a node it
does its processors applies the function
to it and creates a new node
then PP produce the leaf node function
for PP reduce is actually this is doing
a reduce its doing it with with a loop
and recur and that's just because we're
dealing with the Java array for
efficiency purposes but it's going to
look through is going to apply that
function to the return value from the
previous loop through and take it with a
second argument being the next value so
the output of this is going to actually
be the single value that results from
this Luke it's not going to be another
node so yours works chewing is going on
other workers are coming online they
take they push they compute and then
then to get to the very end do their
computation and then they're going to do
the joins the joint function for PP map
is going to take this collection the
result of pv map is going to be a
collection of nodes it's going to turn
it into an array and then use that in
constructor to new node and so what we
end up with is a node with 32 children
nodes and 4p be reduced it's actually
just going to do a straight reduce on
that results over the output the
collection is being passed there is just
a straight collection containing
whatever the results were of each of the
parallel reduces
until that's done so and let's just look
at the performance characteristics for a
second so just like keymap the faster
core function is the less effective TV
map is compared to map right now it's
it's actually eel it's eighty percent
slower which is actually much much
better than what we had with a team out
then we get in the middle area where it
really doesn't matter which and then we
get to the long-running functions where
TV map beats map by quite a bit which
unfortunately is only twenty percent so
this is with the example I used on two
cores PP map is not doesn't perform
quite as well as a female who talked
about the comparison here so this is
this long red line that this is the
performance difference between P map and
mac and it's terrible I mean twelve
hundred times worse that is key map so
in the worst case scenario p map pv map
is much better than key map and it goes
that way we get to this in the middle
area where it really doesn't make any
difference and then we get down here in
percentage wise it's hard to see how
much the difference because we have this
number of screwing up the scale so bad
so we look at the wrong numbers and you
can see p map is really around twice as
you know you've gained its twice as good
as map is at this point keavy map is
only around 75 75 percent better than p
map there's actually a lot of
performance improvements that can occur
in the current versions of keeping
happen which is pretty confident that we
can make that ten times as fast if
that's the case pv map would be all
around better would performing much
better in cases where p map performs
terribly and potentially perform better
even in cases where p map performs
really really well so this is same thing
for reduced
we've got it fifty percent worse than PD
reduce about fifty percent worse than
standard reduce and in the best case
scenario where you're running on long
long running functions it's about
twenty-five percent and if we can
improve that performance then we have a
parallel version of reduced available
which is provides a huge amount of
flexibility for instance we can create a
filter function just an example of one
thing you can do so this is this is TV
reduced just operating on your standard
plus so we're just going to some these
numbers up effectively what PP reduce is
doing is it's going to write a reduce
plus on a bunch of the results of
reduced plus on vectors that it's broken
into pieces the output of thatch is
going to be reduced plus on the numbers
and it works without a hitch TV filter
the function for doing a filter with a
reduced is to take the function is
basically going to have a predicate
associated with it and if the predicate
of Elway's true is going to Cange the
current value onto onto the vector
that's being constructed so which means
you need to start with a base case an
empty vector and it's just going to
continue building a bit larger vector
with value as it passed the predicate so
here's even a filter that should be
looking for even numbers so we have it's
going to be reduced calling that kind of
inner filter function on the results of
calling reduce on filter with an empty
paren or empty vector and then we get to
this point where it's gonna blow because
filter can't evaluate this is even so if
you want to use the parallel version of
reduced you actually need to modify your
function to be associative to be able to
run in cases where what you need to
community create a function that's
associated so in this example we're
going to take written evaluate and say
look at that arguments that's being
passed to this power filter function and
if it's a vector then we're going to
apply reduce cons on it and if it's if
it other otherwise if it's an even value
so it's not a vector or something that
we can evaluate is eat and then we just
do the standard thing we did before and
finally and otherwise because I vector
hurry and it's not even then just return
the the first argument so this is just
the way of doing filtering that won't
blow up with a TV producer so you get
that turns into that and then it will
actually realize that these things
aren't numbers it's not going to call
even on it's going to call vector is
going to realize as a vector and it's
just going to concatenate EV reduces to
take functions that would normally work
non-associative functions that could
work fine and reduce and make them
associative in whatever way seems best
so the performance characteristics for p
filter it's very essential e the same as
filter it does poorly when you have a
predicate evaluates very quickly and
might as well just use reduce as the
predicate gets more complicated comes a
lot it doesn't really matter and it's
pretty becomes more involved p the ppv
filter becomes a better choice so that's
the references um
so this is I mean this this is the goals
are primarily to make parallelism work
with closures existing data structures
and TV the fork join framework the
approach it takes is it it works in a
much broader range of functions so it's
so we're place where key map would fail
it's a very good choice all right
question and I think I apologize for
losing my friend</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>