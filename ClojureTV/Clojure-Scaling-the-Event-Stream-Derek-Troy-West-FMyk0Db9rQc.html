<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure Scaling the Event Stream  - Derek Troy West | Coder Coacher - Coaching Coders</title><meta content="Clojure Scaling the Event Stream  - Derek Troy West - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clojure Scaling the Event Stream  - Derek Troy West</b></h2><h5 class="post__date">2017-10-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FMyk0Db9rQc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right kill tequila everybody thanks
for coming along today to this talks
getting the event stream it's a bit of a
brand title really for what's in effect
an experience report and just an attempt
to share my enthusiasm for the language
I've been working with closure for maybe
nearly five years now and I think like a
lot of people who've adopted the
language particularly those who've come
from the Java world it's I just really
enjoy it it gives me something much more
valuable in my day today and I'm lucky
to work on some pretty interesting
projects and I kind of talked briefly
through a few of them today and then get
into some of the details of probably the
most interesting projects that we work
on and platforms that we build so start
with the big existential questions
straight away Who am I my name is Derek
Troy West I'm a director of a
consultancy in Melbourne Australia and
I'm a programmer so I spend my day is a
real mixture I guess I spend a fair
amount of time at the repple do a little
bit of java every now and then I'd like
to do a bit more Kotlin perhaps and I
spend as much time as I can coding
because I'm on the tools solving
problems I really enjoy that life I
assume we all do and the rest of my time
I spend building client relationships
worrying about delivery invoicing people
chasing late payments trying to zero in
on people that I might want to hire in
the future so it's pretty busy and I'm
absolutely knackered at the moment by
the way so if I fall asleep on stage
just leave me be entertain yourselves or
I don't know but but by no means wake me
up I flew 25 and a half hours to be here
and a mixture of just straight fear of
this talk and jet lag have kept me awake
for nearly 48 hours I'm an occasional
open source contributor to a bunch of
projects in both Java and closure
nettie the non-blocking event framework
i used to contribute to that quite a bit
more but a little bit less these days
alia makes pen it's excellent wrapper
for the Cassandra driver from dead sex
as a closure project Archie is my
company's little battery pack that we
open source for for alia Maslin is the
DSL for storm and trident that we'll be
talking about a little bit later on
storm Kafka was a little bridging
project between the storm and Kafka
protesters since disappeared and CCM clj
and if any of you are using cassandra
and closure I really recommend that you
look at using CCM clj is open source by
a client of mine and it allows you to
stop and start
Cassandra clusters from the repple which
is really really useful it's a bit it's
a bit sharp takes a bit of setup but
once you've got there it's it's really
superb my personal expertise is in high
availability streaming systems that's
what I've been spending the last five
years working on for smallish clients so
you know normal client would be under
200 staff I don't work with a an apple
or a Google or a Yahoo or I'm sure
people have got some really exciting big
big stories to tell but the sort of
systems that I build deal with in
convenient size data I guess for for
clients that aspire to be Apple's one
day and my focus at the moment is pretty
much purely on growing my business so my
concerns when I'm choosing the language
that I want to
you hang my coat on is really about
delivery and quality and not going
bankrupt that's it because as long as my
company grows and has that agency then I
get to have a firm hold of the technical
direction that me and my team follow
which is really really valuable and i'm
on twitter at detroit west
just indulge me with a little bit of
nostalgia for a minute as i explained my
background in computer science and
programming my first computer was a
spectrum 48k and my first programming
experience was in sinclair basic and it
was a wonderful little machine with
rubber keys that you you turn on and you
see nothing but a blinking cursor and
technically speaking it's not a ripple
it's it's an interpreter an interactive
prompt I think but you know let's not
get too technical as far as I'm
concerned it was my first ripple as a
six-year-old you could turn this magic
little device on and you had nothing but
your own curiosity right I don't know
how many people remember that sort of
experience it's been lost a little now I
wonder what my son is going to use when
he gets bit alright because it was
really wonderful you know I learned so
much from my big brother watching him
read things and being the the junior
member just sort of repeating what he
did and yeah you could get into all
sorts of mischief with just a little
blinking cursor and plenty of time as a
kid yeah mostly I spent my time playing
games like Chucky egg but you know skip
forward about 12 years and my next
experience programming was at university
and for some reason I have no idea why
they taught us assembly VAX instruction
set and it's basically you can see the
thing about sort of proved closet I have
no idea why in 97 we were working with
vex but I personally I loved it it's
probably still my favorite programming
language that I've used prefer it to
closure so there you go all right I
remember talking to my professor
point relating that to him and he said
well I hope you enjoy the thought of
programming air-conditioning units
because that's pretty much all you're
going to do with that and I thought I
don't know really if there's that much
of a career in that so immediately
Java I spent 15 years working with Java
it's a fine language I know a lot of
people in the sort of functional closure
community and elsewhere turn their nose
but it's a bit Java gave me real agency
to travel around the world and have a
stable career and nice colleagues earn
money so yeah there's there's a lot of
value in the JVM as rich was talking
about earlier today of course and then
as you do about five years ago I made a
wonderful girl fell in love moved to
Australia and really started enjoying
programming again found closure yeah and
about three years ago I started my
little company we're still very small we
we specialize in taking on big projects
with small teams I'd say and I'll give
you a little bit more insight into quite
how small momentarily we we build expert
systems for engineers that help them
realize when they're skyscrapers are
going to fall over and when metal beams
are bending we have built live scoring
systems for sports fans which is an easy
way of saying gambling we we've
integrated payment networks for FinTech
startups and performed architectural
reviews for telecoms and mostly in the
closure space so this is the agency that
my own company gives me in a small
environment like Melbourne to try and
drive that that's a technical agenda but
probably the most exciting projects that
we work on the most technically
interesting and actually the
burn of the company and our our
expertise is processing logs lots and
lots and lots and lots of logs so one of
my clients is a messaging security
company they work in the telecom space
and it's the old workhorse it's email
billions and billions of email they
process on a daily basis maybe not
billions a day but certainly hundreds of
millions and these email is they're
processed by my client they they
generate an enormous amount of logs
somewhere between ten to a hundred
events logged per email possibly more
than that but some they're the
pathological case they're not
statistically so important the overall
architecture of the rest of the
enterprise is made up of multiple
systems that log events in varying
formats with varying frequency of
varying importance and so what we end up
with really is multiple streams of
events that the enterprise is generating
as it goes about its day-to-day business
performing the core functions of the
suite of services that this company
provides to its clients and it's a
dynamic ecosystem where systems evolve
so there may be more or think
authentication logs logged at some point
new types of logs the details within
logs may change completely
currently the system's been in operation
for has been in production for about
three years now and it has evolved into
a couple of different platforms in that
time currently today we'll be processing
somewhere between 5,000 to 50,000 events
sustained over the period of a day and
we retained somewhere in the order of
between ten and a hundred terabytes of
data at rest
and on day zero three years ago those
logs were on disk for post-op analysis
they were just the fallback if someone
needed to go and find out from
operations what had happened at a point
in time why did things stop working a
client wants to know what happened to a
particular email account so on and so
forth so the requirements for this piece
of work that we were going to complete
was to distill meaning from these events
so to reconstitute some understanding of
how the business was operating from all
of these voluminous logs that were being
shunted out all across the place the key
things that we wanted to be able to
support and that drove the final
architecture and the tools that we chose
to use for this system was we wanted to
support ad hoc worrying over the full
event corpus which which is fine I guess
if you're going to use a database like
post press you can just fire off ad hoc
queries at your heart's content you can
join between tables you can think about
your relational data schema beforehand
that's exactly the kind of thing that a
normal database will do for you we also
want to provide reports of event
aggregation so we want to perform some
calculus over these logs as well we want
to roll them up and aggregate them sum
them perhaps we might perform some
statistical analysis build some
histograms generate some percentiles or
all of that sort of normal analytical
stuff I guess and perhaps some of the
eligible stuff could potentially be
somewhat probabilistic at times whereas
the ad hoc querying of the event corpus
has to be precise and we wanted to
provide some real-time stream analysis
and alerting over all of these event
logs and have the facility to adopt and
adapt our analysis as needed by the
business
so the key things we were trying to hit
and there you know very simple easy
things to do is to be nimble to be able
to deal with a world where the data is
going to change what the data be to be
durable so one of the products that this
platform drives is an archiving solution
that archives millions of Mail a day and
it archives that on behalf of customers
who range in in size and some of them
are all phones and you don't really want
to be explaining to lawyers where their
emails gone that you had promised was
archived correctly the system needs to
be reliable because in some
circumstances we designate that if
events can't be received we'll bounce an
email that's the SLA because we need a
positive affirmation that something may
have happened is something unusual but
in general reliability and high
availability was something that we
really wanted to achieve and the system
had to be scalable because my client
anticipates growth I guess and since
we've had this system in production
we've scaled it to an order of magnitude
of the size of log that it was initially
ingesting I wouldn't be surprised if we
scaled again a similar size over the
next twelve months or so two orders of
magnitude might need some really texture
because the hubcaps would probably come
flying off at high speed but yeah that's
always the case so some observations
that we made very early on about about
this streaming architecture is that each
stream is a sequence of immutable facts
so emails though each stream is
basically a sequence of entities facts
about entities that have occurred and in
reality what we wanted to do was we want
to index that stream we don't
necessarily want to pre-compute our
views we don't necessarily want to
mutate our data on the way in and I need
preserve a certain amount of it or
presume that we know what the read paths
and I want to support on the way out we
just want to take that stream and be
able to access it much more with a great
with a much higher fidelity so we wanted
to be able to perform ad hoc queries and
retrieve just parts of that log stream
that related to a particular entity or a
series of entities I wanted to retain
those individual events as is we
preferred item potent rights for
simplicity right from the start because
we presumed that if we're dealing with a
a sequence a stream of immutable logs
that at times things will go wrong and
when things go wrong our failure mode
often is to restart the processing from
the back of the queue and if you have
idempotent rights then that's a really
achievable thing to be able to do and
actually I have to admit when we first
started this project I think I spent
about three days building out an initial
solution to some of the computation of
the log in Java because that was my
natural language at the time and I just
sort of reflexively reached for it and
it was really hard because in a prior
life in London I was working for an
investment bank has similar sort of
problem working on a trade
reconciliation project where we had 30
different trading systems that were all
sending effectively logs that looked
very very similar through to the back
end and we were trying to build up a a
domain model that suited all of these
different systems and it was very
difficult because there was almost a
ubiquitous language all the traders
spoke in the same terms but they all
meant different things and it was a very
very frustrating experience that
eventually the project didn't go very
far but we were trying to shoehorn a
problem into a particular solution which
was the one that at the time and
particularly in London where everyone
seems to be working with Java very
natural to go through a process of
domain modeling build up an object
hierarchy well luckily we made a
decision very very early on not to
bother we'd my colleague who I was
working with on this project
was a bit ahead of the game and he'd
heard about closure and he'd been
playing with it on the side for a little
while and he just made the decision that
we were going to use closure instead and
it was a phenomenal decision it's
probably the difference between a
success and a failure on this project I
think I'd go so far as to say and really
all we're doing is we're just
transforming it and continuously
accreting data that's all we do we've
taken event stream we repartition it
effectively and write it into a database
in a specific way that's going to
support some read paths that we need to
hit to hit all of these requirements
that we have and there's an
amplification that goes on there's
probably some factor more data written
and is in the original event stream
because we're recruiting more indexes
along the way but generally immutable so
this is the architecture that we settled
on at a very very high level for our
data in flight we use Kafka which is
effectively oh it's a message broker I'm
sure many of you are familiar with
capital it's really gaining in
popularity it was not so common when we
started using it three years ago so
compute over the data that's in Kafka we
we used a distributed computation
framework called storm which was written
by Nathan Mars actually who's a bit of a
closure wrist and was open sourced by
Twitter after his company was was
acquired several years ago and then at
rest we use the superb distributed
database Cassandra and I say superb
because it's probably the thing that
drives more business to my company than
anything else which is great
it's a really really sharp tool it's a
great high availability database
but it comes with trade-offs that people
are often not quite familiar with and
it's easy to get yourself into a
situation where you need someone with
some expertise who can come and explain
to you why you shouldn't be deleting
things or so forth so at a very very
high level we have better in flight in
caf-co we compute over it in storm and
then we stick it in Cassandra and then
that's the end of the problem really
Kafka is a it's a partitioned immutable
distributed commit log it's a high
availability queue I won't probably
bother talking about it too much other
than to say that it's extremely reliable
and it's evolving rapidly at the moment
you can see actors who are in this space
actively pushing Kafka along my
presumption almost is that at a point in
time I wouldn't need storm to do the
computation anymore because you can do
computation in Kafka now and perhaps it
wouldn't surprise me down the line in
the year or to have Kafka were to start
layering in more functionality for
accessing your data other than just in a
sequential manner from the back of the
log if there was some sort of mend
tables that come into play that you
could index a little bit differently I
don't know that might be a bit too much
of a step change but it's really really
reliable and it's a very easy place to
put data in a very easy place to pull
the data out again it's also its primary
role in terms of our architecture is
it's a way of polarizing an event stream
from something that is a big problem
into lots of smaller problems that all
contain events that have been
partitioned reliably in the same way
which becomes important because when
you're working with systems distributed
systems that will work across these say
these three examples of frameworks
really the partition is the most
important thing your data your ability
to
data is completely constrained by how
you write that data it's something that
people dealing with Cassandra often get
wrong to start off with this you have to
think about your read paths when you're
writing data to Cassandra and similarly
you should be thinking when you write
there to cut through Kafka how are you
partitioning that data are you
partitioning it semantically in a way
that makes sense for however you're
going to consume it out you don't
necessarily have to with Kaffee you
could always repartition it later on but
you'd be missing a trick if you don't
take some value from it's innate ability
to polarize your data for you storms are
distributed computation framework it's
pretty powerful it's probably a little
bit more complicated than we need I
suspect we'll probably move in the
direction of onyx fairly soon I like
their data-driven approach but it's
possible once you understand it to
effectively hide its complexity by
segregating out the closure that you
have to write to define topologies and
to deploy them down to the point where
you you basically just have some code
that is going to almost behave like a
normal closure program it just happens
to get split up and distributed around a
cluster owned and executed in a highly
available manner and in a manner in
which you can you can choose to increase
the resources that are expended on
processing your stream if you want to at
a point in time if you wanted to
reprocess your event log from the back
of the queue you can spin up a whole
bunch more storm workers and you can
consume a bunch more resources and you
can fly through your queue much quicker
than your normal day-to-day if you have
to so there's one valuable thing about
having storm as opposed to maybe just
writing a bunch of micro services and
managing that yourself and then
Cassandra it's a shared nothing columnar
distributed database it's our source of
truth it's where all of our data finally
accretes it's a really valuable tool so
up until this point I haven't said
anything about closure at all really but
luckily here
I have an artist's rendition of the two
most important languages that we apply
across our platform
many of you may recognize our friend
Java and down the bottom here we have
our hero closure so Java is in my
estimation a big lumbering Colossus and
I understand that sometimes it could be
off-putting if you haven't come if you
haven't lived in the shadow of the
colossus for 15 years like I have you
might be repulsed by this thing and
think I just really don't want to have
anything to do with it
whereas our hero at the bottom here is
nimble and powerful and she's expressive
and ultimately the difference between
projects that succeed in my case and
projects that fail but I'd asked you to
look closely into Javas beady little
eyes because there's some really really
valuable things in them and it's true
the majority of the code on this project
is all Java it's nettie it's leucine
which is a phenomenal way of decomposing
text where our vacations code ahem
metrics
Cassandra drivers all of the ways that
we integrate with the JVM services that
we consume hashing encrypting there's
real leverage in closure in being able
to utilize the value of decades of
engineering that has gone into solving
problems in Java on the JVM that you
just simply don't need to anymore so
that's where I'm quite bullish about
being clear that I think I've come to
understand what the killer app for
closure is in my case but it is really
vile leverage of the JVM and those
existing services that we get the real
value so what's left for closure in that
case pretty much everything that is
contextually important for what we're
trying to achieve everything that is in
the framework that we bolted into use
for some discreet tasks all of the data
parsing
and I'll talk specifically about one
aspect of that all the data
transformation that we do across all of
our stream is all in closure the ability
to execute in parallel when we're
pulling queries back is absolutely
phenomenal we have storm topologies we
have utility web services because I
refuse to call them micro services we
have a query query optimizer and all
sorts of really interesting technical
stuff that solves problems that you
might otherwise throw elasticsearch at
or you might throw you might potentially
if you had a slightly simpler problem
domain you are trying to solve you might
buy a blank license and now you've got
two problems you've got lots of data and
you have to understand how Splunk works
so even though our hero is tiny in this
case there's 27 repositories that have
gone into two platforms that work across
making sure that that we delivered what
our client required and I see often in
Reddit little new closure ists and
questions and so forth people asking
what is closures killer app well my
belief is that closures killer app is
data intensive applications
this is a superb book by gentleman
called Martin clip 'men that I recommend
if anyone is working in this space or
considering using Kafka or cassandra or
literally any of a variety of
distributed data intensive systems you
should buy this book and read it is
extremely legible and it's been a real
eye-opener for me because I've been
building these systems for about four
years now and as I've gone I've seen
people like Jake reps write articles
about logs which I've really appreciated
Martyn kept Minh wrote a fantastic
article called turning the database
inside out which really validated some
of the things that we were trying to
achieve at the time and
a bunch of these people work on pushing
Kafka forward at the moment and by the
way and reading this book gave me a lot
of insight into terminologies that I
didn't realize I should be using he gave
me a nomenclature for describing indexes
and databases that I had described
differently myself and I just can't
recommend it enough but fundamentally if
you are dealing with data you are
pausing it you are transforming it even
at scale
I suspect that closure is the language
for you because in terms of me and on
this project
it has knocked out so much complexity
not worrying about domain modeling not
worrying about object hierarchies just
respecting that the data is the thing
that we are trying to operate over and
coming from fifteen years of working on
Java projects that's you know it's quite
a big step change so I've picked three
highlights from my experience of working
with closure over the last five years
that I'd like to talk about briefly one
is poor closure right we when I was
contributing to Metis back in the day
they used to have a pretty stringent
approach to zero dependencies which I
taught me something I really appreciated
that day it was just one Java project
was not much hang of it these days
there's a couple of optional bits and
pieces there but the real value in
closure is right in front of our faces
most of the time it's the data
structures just the ability to deal with
hundreds of terabytes of logs as closure
data structures and nothing more the
ability to adapt and evolve easily as
that data changes because I just have
closure data structures I mean maybe
this is making a very simple point but I
think it deserves to be said that that's
probably the most single valuable thing
in enclosure in my mind followed swiftly
by the core functions that some
transformers data structures we have
storm topologies running on very high
volume events that using a group by and
frequencies and things that are
phenomenal core transformation functions
over those data structures and I can't
can't state enough just how valuable
that core closure is I probably should
use transducers more I should possibly
be concerned about intermediary objects
and but I don't really play close it off
I don't I don't preemptively optimize
things if I wanted to carve that out of
this system inevitably it would be in
storm configuration or being kapha
configuration potentially Cassandra
configuration I might be able to get an
order of magnitude or more performance
gains out of that if I needed to so I
don't worry too much about just using
core closure functions on call closure
data structures even if the volumes that
we deal with core async was a phenomenal
game changer for us as we were building
a distributed query planner for
Cassandra that would support ad hoc
queries which is actually quite a fairly
compliment a complicated thing to do
and it gives the most advantage I think
is in its fine grained control of
parallelism which isn't something that I
hear people talk about too much like
configurable control of parallelism so
we use if you're using Cori sink and
you're not using the pipeline functions
you should have a look at those and how
they operate we use pipeline async to
effectively we will when we have a query
will generate a data structure which
represents all of the partitions that we
want to hit for that query and then we
execute those reads as rapidly as
possible by using pipeline async to
execute through all those partitions in
parallel and return us the output in the
same order that they went in and so
that's how I know that I can hit 5,000
requests a second against Cassandra on a
decent network from a single JVM because
I sat there with pipeline async and I
tweaked a little number to see what the
right level for parallelism is I don't
know how I would affect that in Java so
easily I don't care to think of it to be
honest because it's just a very easy
universe that I
now composing channels isn't something
that I hear about very often and maybe
I'm not quite using the correct
terminology there but again another
backbone piece of our query planner for
Cassandra
is we will execute a query against a an
index using pipeline async as I just
described maybe pull through a year's
worth of data as rapidly as possible
from a single index and we'll have
queries that operate over multiple
indexes and their returning partition by
paid by partition markers for these logs
that we're trying to identify so we have
functions that we call them and an all
confluence that take in one too many
channels and produce a conjunction of
those channels either an and or an all
logical operations on them and return
you a single channel and it's a really
powerful idea of programming model for
dealing with these this fan-out query
type of execution and then those
channels themselves are composable so
you can have nested queries that have an
in or operations and it's worked
superbly but I would say to go easy on
the hot sauce because and I have been
guilty myself of basically wedging Cori
sink into absolutely everywhere because
it's a phenomenal piece of tooling and
why wouldn't you really if you can
there's probably things I've done with
chorusing that I really shouldn't have
just leave those skeletons buried for
now then my final highlight of working
with closure over the last few years I
chose so I've checked yes I've chosen to
talk about some closure functionality a
core closure library and a community
library instapass is probably one of the
most delightful things that I've worked
with in the last few years it's
absolutely phenomenal who knows what an
email address is I can almost guarantee
that you don't know what an email
address is has anyone read RFC 532 to
that gentleman down there you know what
an email address is and perhaps the ones
at the front sorry
yeah it's look it's very messy right
it's not at all what you think is simple
and under most circumstances you're not
really concerned because you just
worried about maybe validating that
there's got an ad in it or something if
you're just building a crud website if
your client is a security company that
deals with email then you'll be very
interested in what an email addresses
because you want to decompose those
parts and you want to perform threat
detection on them and you want to
diagnose things that are related to the
makeup of those email addresses so you
have to have a very robust approach to
passing email addresses and you have to
be lenient when your client wants to be
lenient and strict when they want to be
strict and there really isn't a good
solution to doing this on in the Java
world on the right-hand side there you
can see the object hierarchy for a
commonly used library that will
decompose RFC 532 to email addresses for
you on the JVM if you want and on the
left hand side is the beginnings of an e
BN f grammar that you can that actually
comes explicitly from the RFC 532 to rc
itself finding that I could play with
grammars in the repple and parse data
was as I said it just an absolute
delight that interactive immediacy of
the repple of generating a parcel with a
grammar and running through hundreds of
variants of different email addresses
that should be parsed it shouldn't be
parsed and seeing that instant output in
data right in front of my eyes and
adjusting my grammar again and
iteratively repeating and repeating and
repeating was a just a really enjoyable
process that came with a phenomenal
output we process literally trillions of
email addresses at volume and speed it
was absolutely no difficulty with inst
of ours so much so that I sent mark
Engelberg and email and thanked him for
his superb library for open sourcing and
putting the effort into it and the
phenomenal thing about getting a grammar
working with insta pars is once you've
got it working you're only halfway there
because then you have to use the parses
function
to find out how ambiguous your grammar
is and you have to straighten up your
grammar as much as possible so that
realistically you get as few passes as
possible and it's as efficient as
possible and I honestly I just have
absolutely no idea how I would have
achieved that without the ripple without
closure data structures and the
immediacy of them and without instapass
so I'm very thankful to have been able
to use that library so because this is
my very first ripple and my very first
crash not my first ripple this is my
very first closure conch this is my
first day of my first closure conch I
thought what a better thing to do then
actually get into a ripple and show you
some of the value of how I work day to
day because those distributed systems I
was just explained to you they're big
their vast right they're complicated you
may be installed them in AWS in the
cloud or managed hardware they're on
multiple machines well they can do or if
you want to you can start all of them
from the record and play with them and
that's the most important thing to be
able to do if you're dealing with data
intensive applications because you need
to understand how your data flows
through these systems right and
fortunately ignore the warnings
fortunately when you've reduced your
cognitive load of having to constantly
think about domain models you can spend
your time playing with data in the
record instead so what I'm going to do
first is I'm going to start a three node
Cassandra cluster it has a version there
to point 0.14 it's going to have three
nodes it's going to load a key space
declaration and it's going to load some
schema and it's going to run all of
those nodes on a particular port and
this is actually starting us using CCM
the Cassandra cluster manager it's
starting these instances outside the
ripple so I can
demonstrate that this is actually a
effectively a 3-node Cassandra cluster
that has started on my local machine now
and I can come I can shell into that
using the normal Cassandra tools that
you would do if you're working with
Cassandra in production excuse me
and I can't stress enough that I don't
really learn by reading I learn by doing
right that's how I know we all have
different ways of evolving our skillset
I learned by actually getting stuff done
and by playing is how I've always
learned so now that I've got my
Cassandra cluster up and running I'm
going to create a connection to that
using a little library that we Troy West
open source it's a wrapper it's a
battery pack really around earlier and
it gives you life cycle and dependency
management and execution control over
prepared statements and in this case
I've just got a couple of prepared
statements there you can see select talk
and insert talk and I can execute a
little select talk to say has anyone is
there any information in Cassandra about
this talk keynote which was richest talk
this morning and there's nothing in
there at the moment you can see you can
see you know in CTL SH there's nothing
there at the moment at all so I can
insert a rating for that talk I'm gonna
give a richer nine out of ten for this
morning because sometimes I don't
understand the capitalization on his
slides but that's fine you know so it's
good to have something to work on rich
so I've stuck that in there and I can
select the back out of him and what
you've seen here seems really simple and
sort of there's nothing very fancy to it
but it's just demonstrated that there's
no real impedance mismatch between
Cassandra and my days
I just put a data structure into for
Sandra and I pulled it back out again
now Cassandra is deep and it's wide and
Cassandra data modeling is something
that we could talk about literally for
for a very very long time it happens
that this schema that I'm applying here
I can put multiple values into it
because that schema is effectively a
time series in a sense and now that I've
put those that data into Cassandra that
allows me to learn about how Cassandra
really works right I've got it in my
record I can smash data in and I can
pull data back out again and I could be
really really effective over these big
broad distributed systems right I've got
them in my back pocket effectively so
let's keep on going I am going to start
a zookeeper server there's some
boilerplate stuff in here there's some
configuration right you don't really
need to worry about zookeeper is a
distributed service for consensus that
Kafka and storm use to to figure out
who's the boss who's the leader of a
petition whose processing what nodes are
available in a Nimbus etc etc so I can
just start one of them up in the record
which was maybe a little bit unusual
once I've got so you keep it up and
running I can start Kafka so now I've
got a Kafka effectively a Catholic
cluster of one I think running in my
ripple so now I can really start to
reckon with these distributed systems
right here I don't have to go wandering
about what happens when I do X what
happens when I do y there is no
supposition there's only repla Taaj I
use my riffle to power my ability to
understand these distributed systems and
I've got my Kafka broker up and running
I can create a producer which is going
to allow me to put data onto that Kafka
so I can see what happens I can I can in
a outside of the record because it's
Ruby I could start a rhyme and server
and I could monitor these services and I
could see how my data is distributed
between partitions and Kafka and how its
processed in storm and where it sits in
Cassandra all from my repple which i
think is really powerful so I can send
some data to Kafka and I'm going to send
a rating for this talk scaling the event
stream and I'm going to give it an 11
because I think it's excellent and
that's just telling me that that date
has gone onto Kafka now so considering
that we've got all these things up and
running we may as well start stormed as
well why not start our distributed
computation framework now we've almost
got all of the moving parts that we need
to build a highly available distributed
system this is a system that processes
50,000 events a second so I've got to
wait a little moment for storm to get up
and running the thing you have to do
with storm once you've got it up and
running as you have to deploy a topology
into it there's your computation and I'm
just going to go ahead and deploy that
topology because I have to admit it
doesn't work my wife told me the other
day when I was seeing on the couch
watching Rick and Morty that I needed to
prepare more for my Cassandra for my
conference talk and I have to admit she
was correct for the life of me I can't
quite figure out why my
my storm topology excuse me I'm getting
a little bit distracted has just been
refusing to work I've done it's actually
fallen apart so unfortunately I can't
we'll all just have to pretend that my
storm topology deployed it pulled the
data off of Kafka index didn't put it
into Cassandra we selected it back out
again
it was the flourish for the end of my
live coding so I'm nearly nearly
finished I have some numbers for you
so we process no billions of events a
day we've been doing it for three years
zero times static typing would have
saved production problems I know this is
there being some people's bonnet there's
a Norma's mode of discussion about
online I don't really care for any of it
because it hasn't had any impact on me
whatsoever
running dynamically typed languages in
production and volume in its scale has
been absolutely fine the team size that
built this system was - and to be fair
we weren't both hundred percent engaged
hundred percent of the time so it's
probably somewhere a little bit south of
- and that I think is an enormous
argument in favor of closure when people
talk about the expressivity of closure
there's 27 repositories and I can hold
most of them in my brain because I have
the reduced cognitive load of not
worrying about object modeling and the
language is extremely expressive so that
is very much in closures favor and
finally there's one out of all of the
tools that we use to build these
distributed systems there's one that I'm
very very confident won't be changing
potentially we might not use Sadler in
the future we might use an Amazon analog
we may not use Stahl we might use onyx
we might use CAF feel we may use Kafka
for everything but I know that I'll be
using closure in the future because it's
a superb language and that's the end of
my talk so thanks for coming down I
listen to it
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>