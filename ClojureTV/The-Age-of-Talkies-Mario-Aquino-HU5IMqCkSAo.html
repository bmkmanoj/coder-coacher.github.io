<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Age of Talkies - Mario Aquino | Coder Coacher - Coaching Coders</title><meta content="The Age of Talkies - Mario Aquino - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Age of Talkies - Mario Aquino</b></h2><h5 class="post__date">2016-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HU5IMqCkSAo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">done if you got the cards in the raw
order or if card got jammed in the card
reader it was a nightmare to try to fix
a result but that's how computing was
done for a number of years until the 80s
or so beyond that what we are now much
more familiar with our the computer
interface where you have people that are
interacting with a computer by typing
out instructions on a keyboard the
instructions may be to run a program
they may be to create a program but
that's how we're most accustomed to
interacting with computers we've
actually come a long way even beyond
that what is available to consumers
today our computing products like this
one where you have computers that are
actually watching humans they're
watching the gestures that people do
what seen here is a video game where the
players are immersed in an experience
where the computer is watching them and
interpreting the gestures that they make
and responding those gestures by
changing the position and the activities
of the actors that are part of that game
for the humans that are participating in
that kind of competing interaction they
are inside of that experience because
there's not as rigid and interface
between them and the computer it's very
fluid the computer is watching them and
responding to what they do very recently
in fact earlier this month Facebook
released this technology that supports
image object recognition so from having
computers that can watch the movement of
humans to having AI systems that can
look at still images and interpret what
is in those pictures is actually a very
big deal it's a big advancement in the
human-computer interface this is
actually something that at a keynote
that was given the last keynote of
closure West last year that was given by
Melanie Mitchell I'm a giant fan boy of
dr. Mitchell what she described which
has been the sort of focus of her work
after getting her PhD has been research
into training AI systems to interpret
images and to try to figure out what is
depicted in those and it turns out it's
actually a really really hard problem
and there are all kinds of things that
the human brain does very fluidly that
we sort of take for granite we don't
even realize how many things that our
brain has to get through in order for us
to recognize what it is that we see or
that we hear and teaching computers how
to do that turns out to be actually
pretty tricky so having computers watch
us and respond to us and even interpret
what it is that they see is a big
advancement in computing so that's
that's one of the sentences another
sense is the sense of hearing so what I
think probably a lot of us in the room
myself included have in our pockets our
computer systems that you can just talk
to so there's a feature that was
released in iOS that allows you to just
express hey Siri actually I probably
shouldn't say that my phone is going to
go off or maybe all of our phones are
going to where you can just address the
computer and it just happens to be
listening to you and actually feel my
phone buzzing right now it's listening
to you and you can ask it questions you
can ask it to do things you can request
some sort of action and it'll get back
to you what we're going to find over the
next however many years is that those
kinds of interfaces to computers become
much more common because they're easier
it doesn't require us to touch something
all we do is just emote something maybe
we move around in the computer looks at
us or listens to us that's really really
handy but the technology that makes this
work turns out actually to be pretty
hard what our brains do in interpreting
language is a combination of pattern
recognition and context and the context
is really important because context
matters with what you're saying so let's
say for example I were to say to my
computer the computer is just listening
to me if I were to say
where can I get a steak so if the
problem I'm trying to solve is I want to
kill a vampire really the question I'm
asking is where's the closest hardware
store but i can ask a question that
sounds very similar where I say where
can I get a steak and in that context
really what I'm asking is where is the
closest steakhouse so I'm having a
conversation with my computer and my
context is I'm hungry and I ask it where
can I get a steak it says well the
closest steak houses whatever but what
if I follow up that question with what
about Mexican so what about Mexican is
relevant because of the question that I
already asked if I'm talking to a person
and having a conversation with a person
they're going to get that because we're
used to having those sorts of
multi-point conversations having that
work with a computer so that you can
just talk to a computer in the same way
that you talk to a human is something
that we are approaching and in fact
that's something that amazon has come
into with a product that they released
last year that's called the echo so the
echo is this 2001 ask kind of monolith
that Jeff Bezos would like to like you
to have in your living space somewhere
the echo is a device that you address by
calling it by its human named alexa so
when you interact with an echo you
preface that interaction by calling it
by a human name and then expressing some
kind of a request you might say Alexa
what time is it or alexa what's the
weather going to be like today you can
also have a conversational interaction
with alexa where you might say Alexa
what movies are playing in theaters
tonight and it may read through the
movies and then you might follow that up
with what are the showtimes for one of
those having that kind of conversational
interaction is normal for us because
like I said before that's just what we
do and systems like the Alexa platform
that's part of echo bring those things
to us in a way that people are going to
become really accustomed to so Amazon
released the ability to do too
use the same thing that the echo device
runs on top of as a service and it's a
service that's called the Alexa the
electro service Alexa is actually made
up of two different big components one
of those is called the Alexis skills kit
so the Alexis skills kit is a series of
AP is that you can use for bringing a
voice interface to your application the
Alexis skills kit includes ways to do
just single interactions where you
express some question and you get a
response to that but it also supports
having conversational interactions where
you start with one interaction and then
you ask more and more questions that
follow up contextually to wherever it is
that you started and they may lead
anywhere the interactions can be
stateful so you're going to carry along
and it's up to the developers of the
that that voice interaction it's up to
them to recognize what it is that they
want to go from one step to the next in
the interaction but it's possible and
the Alexis skills kit exposes that
through an API another part of the
election platform is Alexa voice
services and Alexa boy services is a way
for you to register a device or an
application that can run on top of Alexa
and what Amazon would like is for you to
have that be part of the echo platform
so what I want to do is demonstrate that
so coming to this conference I was
thinking about giving this talk and I
was thinking about how I could
demonstrate what it is that's possible
with this kind of voice interface so
being a good closure developer I thought
about data and what kind of questions
what I'd like to ask the data that's
relevant to a conference so what I did
was I wrote something that scrapes the
schedule for the conference and put that
into a data source and then created a
skill that reads that data source and
can respond to interactions of me asking
the schedule data for this conference a
few questions and then interacting with
that and seeing where it goes so I'm
going ahead and
go ahead and play that I would do a live
demonstration but it depends on
conference Wi-Fi which is usually pretty
dicey but this is a fair representation
of what's capable for what i wrote for
this talk wait a second actually hang on
a second we unfortunately have forgot to
do an audio thing so you probably can't
hear it very well here we go
open dragon fly ? what time is my talk
your talk is on Friday from 150 to 230
p.m.
the other talks scheduled at the same
time as yours are parallel programming
fork join and reducers by Daniel
Higginbottom and building a legal data
service with closure by Jonathan Boston
and Caleb Phillips read me the abstract
for that first talk if you don't know
your work span from your fork join this
talk is for you by attending this
introduction to thinking parallel you'll
learn why parallel programming matters
how to think about performance and how
to tackle real-world concerns you'll
learn about how the fork join framework
embodies best practices and of course
you'll learn how cloture implements
these ideas with reducers so that took
really a few thank you thank you thank
you thank you very much it's actually
really wasn't all that hard to do and
I'll show you why in a second but I want
to go into what it is that we actually
heard there so the Alexa skills kit has
these two different concepts that are
related to how you declare the
interactions that you want to be able to
support from your skill so those two
concepts are intense and slots so an
intent is basically represents a kind of
interaction that has a specific goal and
slots are ways for you to declare
parameters that a particular kind of
interaction needs so for example this is
an intent that's called get horoscope
get horoscope is an intent that has two
slots and the slots are sign and date
slots are typed so the type of the slot
type the signed slot is list of slots
and lyssa slots is probably just a list
of the 12 zodiac signs date is a type
called amazon date and i'll go into
those in a second but the idea is that
when you declare this intent you're
saying that when it's invoked what needs
to be supplied are assign that
matches list of signs and some date that
is of type amazon date so you might say
what is the horoscope for a cancer today
or what is a horoscope for a cancer on
july six which is my birthday yeah which
birthday of cancers so that's how you
would do it and what the Alexa platform
does is that it listens to what you've
said and based on the definition of your
intent it translates the components that
are part of that what's called an
utterance not going to utterances in a
second but it translates them and then
passes the data into your code so that
you can read those parameters and then
respond appropriately I'll go into that
in a second so the built-in slots that
Amazon provides are these so you might
have a date and the example of data is
today tomorrow july something like that
duration which is something that could
be series minutes like five minutes for
example and that gets translated into a
format when it's passed into your code
so that you can understand what it is
that was intended in the expression
other data types are four digit numbers
which are useful for years numbers time
for in the morning something that is
just sort of colloquial those things get
translated based on those data types US
cities common first names states all
these data types are ones that you could
actually extend they may not have all of
what you need in whatever the type of
data type is but em the Alexa platform
allows you to extend the definition of
those things so that you can make it
sort of closer to what it is that you're
going for you can also create your own
custom data types which is something
that we saw before in the list of whore
scopes so these are the intents that
were part of the demonstration that I
showed one of those intense was Mario's
schedule intent so when I said what time
is my talk that invoked code that looked
for my specific talk Mario's talk
competition is what other talks are
happening at the same time so that's
actually what you're missing in the two
rooms
next to us then read relative abstract
so I asked read me the abstract for that
first talk so that was the first talk of
those two that it mentioned and the slot
for that was order an order could be an
order phrase so order phrase maybe first
second third etc but the point of that
interaction is that you specify which of
a set is the one that you would like to
read the abstract for and that's
contextually relative to the previous
question of what what talks are also
happening at the same time as mine so
that's how you define the intense for a
skill and you can have many in tents for
a particular skill the other component
that's part of defining an intense and
an interaction our utterances so uh
Turin Cesare example phrases that are
part of their the expressions that
should trigger the intent that you've
declared with Amazon so those the way
that Alexa works with those utterances
is that it tries to find a
high-confidence match that ties what
you've said with a declared intent for
your skill so that it can be confident
in triggering that skill if it doesn't
get a high enough match based on what
you've said compared to the utterances
that have been registered for a
particular intent it what it'll do is do
what's called a reprod and a reprint is
an opportunity for you as a programmer
to provide a contextual question to help
the user of your service ask or items i
express what their need is a little bit
better and hopefully it'll be closer to
the utterances that you have declared
for your interaction so here are some
examples so what I had to come up with
when I was designing those in tents or
how I might express the things i would
say to get across that i want to know
when my talk is so when is my talk what
time is my talk so those are two ways
that i might say they might trigger the
the functionality to find out when my
talk is when Mario quino's talk is
mario's talk competition so what talks
are happening at the same time as mine
what talks are happening at the same
time what about other talks at the same
time so those are all their clothes
right there close to each other and
generally there are many ways to express
the same kind of a request same kind of
question there are lots of ways that you
can express that and one of the things i
found a little bit awkward about
defining the intense for these
interactions was thinking through the
different utterances and trying to come
up with as complete list as I could so
what I used to create this voice
interface was a library that we created
at a company where I work that we call
boom hour so boom hour is a really bare
bones closure library that exposes an
easy way to register intense enclosure
that you can deploy either to emit OBS
lambda which is what I did for this
demonstration or some other way and I
can go into that in a second but the the
library is really pretty minimalistic
but what it does is it makes it very
easy to create these kinds of voice
interfaces so I want to look at a little
bit of the code that was that shows
really how easy it could be so this is
some code right here for a very simple
interaction that's the schedule intend
interaction so the way this works is
that down at the bottom where it says
def intent so that's a macro that's part
of the boom our library that what you do
is you specify the name of the intent
and that's an intent that you've
registered with Amazon along with the
function that needs to be invoked when
that intent gets triggered so this one's
hit all intents should be should trigger
the Browse talks function and browse
talks gets two parameters and this is
what the Boomer library provides the
first of those two parameters is the
session so the session is Amazon's I'm
sorry Lex's session object and the
session object gives you the programer
the ability to set state from one
interaction to the next that you want to
have
accumulate in the interactions now if
you just have an interaction that's just
a one-off where you just express
something and you're given the answer
and that ends the interaction there's
really no need to follow up then what
you do is you pass back what's called a
tell response and in a teller response
you turn whatever text that you want to
have the Alexa voice read back to the
client you pass that in a tell response
your code goes back to Alexa which turns
the content into a voice and audio file
and that gets sent back to the client
and that's what the user here's and so
this is this is like this is it it's
really really simple and straightforward
another example is different interaction
so this is my schedule intent where I
want to get my time so and get my time
again I get session object oh I forgot
to explain the session map so the
session object is where you accumulate
state the session map is what has the
value of the parameters if there are any
that were part of that interaction so
where you can say that a particular kind
of utterance takes a series of
parameters those things are going to be
in the session map and you can pull
those out of there and process them
however is appropriate and then build
the response based on what it is that
the interaction supposed to do build
that response and then send it back to
alexa alexa i will pass it on to the
client and so all that is made pretty
easy with boom hour if you're interested
in trying this out i really recommend at
giving it a shot what i found what i was
able to do just for that demonstration
was just a couple of days worth of work
so in terms of running the stack so
running the sack is something you've got
a couple of different options for the
way that i deployed this was on top of
AWS lambda boom our is facilitates that
and that will
is just really easy and it's also
cheaper and I don't know if that really
there's much cost at all to what it was
that I was able to get working but if
you don't want to deploy to AWS let's
say that your deployment environment is
something we've got your own servers the
Alexis skills kit can support that to
the documentation for the Alexis skills
kit it defines the interactions the
requests and the responses that are part
of each intent invocation and its really
clearly defined until you know basically
what to expect when Amazon calls your
service here at the end point that you
register you know what to expect with
what's going to get passed to you and
then what will get returned boom our is
something that's written for writing a
closure app that you want to deploy to
lambda so you wouldn't use boo our for
that but the Alexis skills kit is open
to implementing whatever it is that you
want to do in many different
technologies whether it's something on
the JVM or other ones so at least for
this one I deploy data is lambda lambda
also requires specifying a couple of
different policies a rural policy and an
execution policy and basically what
those things do is they define what who
is allowed to call your function and
what your function is allowed to do when
it runs what may be other AWS services
is allowed to do to interact with and
that's just part of the I am I am
library service I'm sorry this part of a
double yes so in addition to that
deploying the code had installed a TOS
CLI and create an uber jar that has my
code and whatever dependencies my code
may have and when I deploy that I say
the uber jar and the policies and the
name of the skill that I'm sending code
up to up to Amazon that it takes a
minute or two to get up there and then
once that's done then I've got a I've
got something that can respond to a
voice interface in addition to that
amazon also provides some some things
that are convenient for doing the local
development what I showed in that
demonstration was
is java GUI that has a button that you
can press for having it listen to you
and it listens to whatever you say and
it'll turn what you say into an audio
format that it will then send up to
amazon and then amazon I'm sorry LexA
will invoke whatever wherever it is that
your service is running that responds to
a skill invocation there's also a node
server that's sort of part of it behind
the scenes but anyway there are easy
ways to do local development when you're
trying to work on your skill and these
are more or less of things that are
involved so what I found in thinking
through using the Alexa platform and
doing this development was that mobile
seems like a great place to target the
voice interface and I think that'll exit
could be a real game-changer for mobile
apps because it provides a new way to
interact with a mobile application what
we're all used to is tapping on our
devices because their touch devices and
there's a visual component to them too
but having yet another way to interact
to talk to your data if you've got a
data-rich domain or a torch application
that could be a real game-changer and I
think that that's something that we will
see happen more often in time and I
think the Alexa platform is one way to
do that but we're not quite there yet
and I'll go into that a bit more in a
minute what I also want to talk about
are the hard bits so as I was working
out the the sample application the
things that I found kind of hard and a
little bit awkward I mentioned already
the utterances thinking through all the
ways that I might try to express
something and having to basically
provide all of those to Amazon that was
it was a little bit hard to think of all
of those and i also found it clumsy
because there are lots of phrases that
are sort of similar to each other and
the similarity of that i found kind of
annoying but i would rather do is
express a grammar a grammar that matches
ways that you might express some kind of
an intent and have a tool generate the
utterances and the providing that to
alexa right now that's not part of the
election platform
I feel like it'll probably come however
there are some so other people have
thought of this too and it turns out
that there are actually some libraries
that people have written that allow you
to use grammar to use a grammar using
some technology that you can use for
generating these utterances and using
that would make it easier to satisfy all
the utterances that you want to provide
for your interaction another thing
that's hard for the human ear that's
also sometimes hard for Alexa is words
that are not part of the sort of normal
lexicon that we use in a regular
communication if you if there are words
that are part of your domain that are
either just uncommon or may even be hard
to hear or I'm sorry hard to say like I
don't know if anybody noticed but when
Alexa was reading the abstract for one
of the other talks it kept saying
clojure instead of closure and that sort
of subtlety is something that is not
uncommon if the words of your domain are
sort of settled like that if I were to
have it say assoc or a soak or sews I'm
not sure where it would land but I feel
like they'd probably be there that would
be the conclusive definition and we
could all like stop fighting about it
maybe not so accents so just like the
human ear has to sort of train itself
for recognizing accents that's something
that the Alexa voice service can have
some trouble with as well there is a
certification process so when you want
to publish your skill you have to send
it through the certification process
that some component of the Alexa team
has to go through and make sure either
that your skill is easy to use or there
are sufficient number of phrases to
trigger the interactions that you want
to be able provide through a voice
interface and you may go through a
number of different cycles of submitting
your application for certification and
having the Amazon team rejected and
having to go through it over and over
again if you've ever submitted an app to
apple's app store you may be familiar
with this kind of cycle it can be very
frustrating but that's the way it is
right now and the reason for that is
that Amazon really wants to try to
provide a platform
where people don't get frustrated by
using the Amazon echo so in terms of my
experience from using this there were a
few things that I wish were different
and I want to go over those so right now
Amazon's main interest is the echo I
actually bought one very recently just
from working with this when I was doing
the development for this I didn't have
an echo i actually just used the GUI
client that can listen to me and then
sends the the the voice interactions up
to amazon and that was just fine enough
for doing development but amazon real
main interest is the echo they want
people to have echoes and then to have
an ecosystem ecosystem there i think
that the punning asst of this whole
thing is I mean there's this is a tons
of material there anyway Amazon's real
focus is having people buy these this
device that they can have in their
living space and order amazon products
because there is this to make a lot of
money but also integrate with service
providers other service providers that
they can access and to do business with
through this product in the same way
that on mobile devices Android devices
are iOS devices that there is a great
ecosystem of ways to spend money to use
services etc amazon is trying to start
up something like this right now and i
think that's a little bit unfortunate
because the technology of the alexa
voice service is something that is
broader than just using it with an echo
even though i have one I'm not employed
by amazon also there's not really an
easy way for you to piggyback onto the
interactions between the user and your
service so let's say that I have a
mobile application that I want to expose
a voice interface to and in that voice
interaction I want to be able to pass
back to the client some extra
information that goes in addition to
what Alexa Alexa voice is going to say
to them in response to whatever request
they've sent so let's say for example
I've got a data-rich application
that I ask some question on an Amazon
I'm sorry Alexa starts reading back a
long list of things that are part of the
answer to my question it would be great
if in that response I could also tell
the client application by the way show
this graph or show this something while
the user is hearing the auditory
response to whatever their question was
that would be great there's not really a
way to do that today and I hope that
something like that comes because that
would really be a boost to the
capabilities of having a voice interface
also something that's actually not
specific to a to Alexa is the jvm
startup time if you're deploying JVM to
lambda there's a second or two they have
to wait to warm up the the lambda
instance wherever it is inside of the
Amazon Cloud and that detracts from the
user interface slightly if you're
deploying with Python or no tas
javascript you don't have to pay that
same thing but this is closure
conference and don't do that so another
thing that is actually I think really
the biggest thing that I wish was
different was this so when you provide a
voice interface well you also have to do
is provide to the user especially if
you're trying to do some sort of user
linking so let's say that I'm a bank and
in my banking application I want to
expose a voice interface so that a
customer can say a question like how
much money have I spent in last month at
bars say go out a lot socially in order
to do that what you have to do is tie
the account of the bank customer with an
account that that same human customer
has with Amazon which is a big downside
so right now if you try to add a skill
that ties your your organization's user
account to using a voice service the
user is going to be prompted with a
dialogue that has to say okay I am so
and so and i do want to provide access
to the service and
totally sucks it sucks because i use
amazon web services for my whatever it
doesn't matter what my businesses but
whoever uses my business they shouldn't
care that I'm deployed to AWS that has
nothing at all to do with the service
that I provide but in tying the account
that I have with my users amazon
requires that we go through a user
linking step that provides the user with
a dialogue that lets them know that
they're using this Amazon voice service
which is a big negative and I wish that
was really I wish that was different
because that would make adding a boy
service completely transparent which is
what i want i would make my application
so much more awesome so in working
through this having a voice interface i
started to take a step back and ask
myself where are we going so we have
computers that can watch us and responds
to our movements and that's something
that consumers have had for a few years
and it's not jarring right having
computers watches in 2001 how Hal 9000
that was called that computer turned out
to be you know psychopathic and try to
kill the people in the spaceship and so
that should give us pause right but when
the Kinect watches is sort of danced
around it's like oh yeah cool I can have
a thing that dances that's pretty cool
so having computers watch us that's
something that is that's very useful but
there's a lot more that's going on right
now and in terms of where we're going we
have cars that can drive as automatons
we can get inside them and tell them
where we want to go and read our morning
paper drink our coffee and we're
teaching computers to do things that
normally humans have done so that we can
do something else and one view of why
we're doing this and that's so much why
but where we might be going dystopian
view was in a movie
called Wally where humans were sort of
relegated to in existence we're
caretakers had to afford them all the
things that they needed so they could
just not have to do anything at all I
don't really think that that's where
we're going though what I think of is
when I go back and think about my
Richard Dawkins what I remember is that
genes according to Dawkins are these
immortal entities and the whole point of
genes is to recreate themselves what
genes do is they try to make a copy of
themselves from one generation to the
next a perfect copy ideally when I think
about where we are going humanity is
going by creating computers and do
things that humans can do that can watch
us they can listen to us that can move
around that can interact eventually it's
going to be the point where they can
think for themselves this is what life
does life recreates itself the why is I
don't know that there really needs to be
a why but that's just what life does I
think that's where we're going the more
that we have computers that are much
more like us I think the reason for that
is because that's just what life does so
that's what I have thank you for
listening I think we have just a few
minutes for questions
I have about two and a half minutes for
questions and after that we've got to
break in we can continue talking after
go ahead
so the question is can you do something
that has a side effect when you invoke a
skill yeah you can have a side effect
that launches all the missiles if you
like because really it's just another
way to trigger code in a computer that
happened to be triggered by somebody
saying launched on the missiles or order
me a sandwich there was a hand back
there first in chat BOTS I'm not an
expert in chat BOTS oh that's right the
Tay right so the the Tay was this thing
that probably a lot of computer science
has thought long and hard about and they
had really grand expectations for this
thing and in like a day it turned into
like a porn drugged sex pot racist
sexpot it's really unfortunate but the
way to interact with that AI was just
through text but it was really just a
program that is responding to some input
and you can interact with programs
either with your voice or by moving
around or by sending it whatever things
through Twitter just another variation
good question so it doesn't have to be a
precise match what Alexa tries to do is
have a confidence a high enough
confidence that the this series of word
tokens that you passed in your utterance
is close enough with a high enough
confidence that it's going to trigger
something and if it can't get that high
confidence it provides a mechanism for
the skill to reap romp the user to try
to get better information typically I
think those reap romps happen more when
the sort of parametric values that
you're going to pass into a skill when
the value of that isn't exactly clear
and you want to give the user another
chance to say it again maybe they
mumbled maybe they've got an accent
maybe they forgot to do something I'm
sorry yeah I'm not sure it's hard for me
to say exactly how flexible if anybody
the room has a better answer that I'd
like to hear it
right so boom are potentially could be
something that is sort of a broader dsl
for registering voice interfaces at the
moment I'm not familiar enough with
other excuse me other natural language
processing as a service services to say
where we could sort of change in and
adapted for other ones but that sounds
really good to me the library itself is
really in a nascent stage but it makes
what I did really easy and I'd like to I
think all this that worked on it would
like it to improve so if you've got some
natural language processing service that
you're aware of I'd like to hear more
about that and maybe we could look at it
more seriously you know the questions
all right thank you everybody for
listening I think we've got like a
half-hour break</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>