<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Matt Bossenbroek -  PigPen: Map-Reduce for Clojure | Coder Coacher - Coaching Coders</title><meta content="Matt Bossenbroek -  PigPen: Map-Reduce for Clojure - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Matt Bossenbroek -  PigPen: Map-Reduce for Clojure</b></h2><h5 class="post__date">2014-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/drzMkklC05U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everybody I'm Matt I work at Netflix
and I made pick been so what is pig pen
pig pen is MapReduce foreclosure but I
don't really like to think of pig pen in
terms of MapReduce I don't want to write
mappers and reducers I don't want to
have to think about Hadoop jobs and
string data between them really what I
like to think of pig pen is as
distributed idiomatic closure I want to
write code that looks like normal
everyday closure that you would run on
one machine and then I want to take that
never owned and run it on thousands of
machines a lot of times when people
think of MapReduce they think of queries
like I want to execute one query or I
want to join these two tables together
pediped is more about data flow I want
to take a bunch of different data
sources i want to map over them filter
them and then join them together group
line then maybe join in some more data I
want to describe a graph of operations
that I want to happen Pigpen is about
iterative development and a lot of the
MapReduce tools that I've used before
there's very long cycles you have to
write a big script you don't really know
what it's going to do if it's going to
work then you go run it a couple minutes
later you get back some results and it
most likely won't work with pig pen I
want immediate feedback like we're used
to in the closure repple pick Ben is
about unit testing there are some
techniques for unit testing MapReduce
stuff but a lot of times you end up
having to put sample data into a file
run your script and then get data back
on the other side I actually read it out
I want you to test like closure has unit
tests and with that I want generative
testing I don't want to just have a
static set of data going it I want to be
able to come up with generative tests
for my Pigpen queries
pig pen is composable I don't want to
write a large monolithic script and then
submit that and if I need to make
changes later I have to not only change
part of the script but other parts in
there I want to be able to have
independent functions of logic just like
you would in any other closure program
why would why should we expect any less
for MapReduce I want branching I want
looping so I want to be able to
conditionally run something in a script
I want to loop over things with
different parameter values and
parameterize my script these things
sound very simple but a lot of other
MapReduce languages out there you don't
really have these constructs I want it
to be parameterizing ball and i don't
mean string substitution a lot of
scripting languages call that
parameterizing and doesn't really count
I want to actually pass in a chunk of
data and work with that chunk of data
and of course I want it to be massively
parallel I want to take my code and I
want to run it on thousands of machines
tens of thousands of machines hundreds
of thousands pig pen does use a patchy
pig but it uses a patchy pig in the way
that closure uses the JVM it's it's a
host system it's a host language pig pen
is not for writing and curating Apache
pig scripts
I asked myself this question a lot when
I was making pig pen I was like there
are dozens of other alternatives do we
need another MapReduce language and what
I came to was if you know closure you
already know Pigpen so who in here knows
closure you know Pigpen I'll show you
some examples this is an example
function here and the corresponding unit
test pretty simple it's kind of a
nonsensical example we have a function
take some data assume it's just a
sequence of numbers we increment them we
filter them we group by them and then we
have a map cat operation on the other
side we have our tests we define our
data whatever we want our input data to
be we call our function and we get back
a set of values this is what the same
query looks like in Pigpen not much
different there so I'll highlight it
make it easy for you in the function the
only thing that's different is we bring
in the pig pen name space and then pig
has map it has filtered as group buy it
as map cat has dozens of other
operations that are all enclosure core
the test is only slightly different we
need to wrap our data to make it look
like a pig pen expression so that's the
pig return function and then pick dump
takes our pig pen expression and
actually evaluates and gives us back the
data wouldn't have a talk about
MapReduce without word count so yep here
it's word count the usual example for
MapReduce in this case we have our word
count function take some lines we mapped
out over the lines each line we split it
we do some normalization etc etc what's
different about Pigpen is we don't have
to define our tokenize function
somewhere else you can if you want to
but a lot of simple functions it's a lot
easier to just put them in line just
like you would any other closure code so
we define our function that takes a line
and does that we group by
identity because at this point we just
have a sequence of words so just group
by those words and here you can see this
is just closure core identity we can use
any closure function we want as their
group by function then once we have our
words grouped it returns kind of think
of it like if you took a map and called
seek on it so you get a sequence of
tuples each one with the word that we
grouped and the occurrences so if you
had a bunch of foods in your in your
data the word would be foo and
occurrences would be a sequence of foo
foo foo to count we just use closure
core count you could do whatever else
you want this function I'm just throwing
it back into a vector of work and
counting the occurrences notably absent
from this word count example is anything
to do about where the data is coming
from or where the data is going and
that's intentional I'll talk more about
that later but to actually use this what
do we do with our word count function we
take it and we make a word count query
so here we r is where we're loading data
from a TSV file we're using a threading
operator to call our word count function
and then the output of word count we're
actually going to store that into
another tsv these things are new it is
you don't have in closure core but the
closure core concepts didn't translate
as well with loading or storing data the
way that Hadoop does so I have a little
bit difference the nice thing about it
is we've abstract adiel word count
business logic from the business of
where am i loading my data where am I
store my data anything else and this
allows us to do a lot of different
things with it you don't necessarily
have to abstract those things from yours
these are just closure functions you can
put them together any way you want it's
just a good idea so what do we do with
this oops wrong way
so with this we call our word count
query we give it the actual locations
and put TSE output TSE obviously
teachers would be different then we take
that query expression and we write it to
a script and this is going to give us an
Apache pig script which you can take and
just run on any Hadoop cluster a lot of
people use pigs so it's a fairly known
system of how to use it you don't really
have to look in the script you could
it's just your closure code kind of in a
pig script but it's up to you so that's
how you take it and run it on thousands
of machines but really what I want when
I'm running MapReduce is to be able to
quickly iterate and know what my script
is going to do I don't want to have to
take it and run it on the cluster
somewhere so pig pen in the ripple so as
you saw earlier we had our unit test and
we can return data and this is as if I
had a file somewhere and on one line I
put this string and then on the next
line I put another string and would have
loaded that but in here I can mock out
my data much more simply and do it right
in the repple and define this as my data
I can then call our word count function
which didn't have anything to do with
loading or storing data conveniently and
then call this use the pig dump operator
to actually execute the expression and
return the data right in my rebel
doesn't go somewhere else it doesn't go
to a console this is actual live data
real data that you can now interact with
with that we can wrap this into unit
tests you could make a degenerative test
just as easily these are all just
functions that you're using to pass data
through your actual logic that you can
now run on thousands of machines
so how does all this work we start with
your pig pen code which is really just
closure code in a slightly different
name space except that when it's
executing when you're actually calling
Pig map in Pig filter you're not
actually doing those operations you're
building up an expression tree this
expression tree builds up as you compose
these operations as the store points
back to the map expression points back
to where it got its data from so you get
back to where the data is all coming
from then we take this abstract
expression tree we optimize it say hey
you load the same data twice combine
that together you loaded the same data
twice over here load that to you combine
that together we have this dag that
basically explains what we want to do
with our data at this point there's
really nothing to do with pig involved
here it's it's all just kind of an
abstract closure expression tree of how
do I want to process sequences of data
from there I can squint realize these
say I know I want the order to be we
need to load the data before we store it
I need to actually perform my operations
before I do anything with it from there
it's a pretty one to one correspondence
pig has map pick as filter pick s group
I so you can generate all of these
commands of scripts and then take your
script and run it on to dip pretty easy
to do we're not done there we can take
that dag and we can do other things with
it right now I'm converting it into RX
you don't need to know anything about RX
to use this RX is reactive extensions
it's a way to take a dag of operations
and execute them and kind of say i want
to subscribe to the bottom one and that
propagates its way up through and then
the operations start pushing data back
down through it's a nice way to do
asynchronous operations it's very
similar to core a sink corey sink didn't
exist when i originally wrote this so
that's why i use our x but ignoring the
details of our x
this is actually what's powering the
data into the repple and this is how we
can go from immediately getting data
back from the sample data run it through
our logic and get it back in the rebel
we're not compiling it to a script we're
not starting up another JVM we're not
doing any of the normal things that a
lot of other MapReduce frameworks
generally do to run this and get you
back your sample data joins and the
traditional MapReduce or sequel or any
sort of relational sense didn't exist in
closure so I came up with a new syntax
for it join basically takes a set of
relations soon left and right or other
Pigpen query expressions and then you
specify a function that you want as your
key selector function so assumed left is
a sequence of maps have a keyword a in
them so we're going to pull out the a
element it's assume right as a sequence
of vectors pull out the first element
and any of time those match up we're
going to join them together and call
this function that takes an L and an R
and basically you take every element
from left that joins and every element
from our the right that joins and call
this function right now I'm just dumping
it back into a map you'd probably want
to do something more intelligent with it
but it's just for demo purposes outer
joins are a common thing in relational
databases so in here instead of doing
like a left outer right outer and that
gets confusing you can just mark
relations as optional or required it
makes it a lot more sense when you get
into more than two relations as you're
joining together you can also tell it
how many machines you want to run this
code pig by default will try to figure
it out but it's not very good at that
job so a lot of times you want to
explicitly tell it I want a thousand
machines for this and that's really easy
to do just passing in options to the
join
similar pattern is co group co group is
very similar to a join except that it
doesn't flatten your data so say i have
in my left thing a bunch of values with
that same key a and a bunch of values on
the right with the same first value a
bunch of joins it would happen a co
group is going to take all of those and
give them them to you all at once s'okay
and our combiner function is the key
that it joined on LS is all of the
elements in the left hand side that
matched in ours is all of the elements
in the right hand side again you can do
whatever you want you can flatten them
aggregate them mash them together in
here I'm just throwing it into a map and
you can do whatever subsequent step
you'd want I also in Pigpen wanted to
close over data and this is how we
parameterize functions I have a function
foo here it takes a parameter P and some
data assuming data is another Pigpen
expression tree of other Pigpen
expressions then we apply map to it
what's interesting about this is our
function foo the threading data through
map that all happens locally that all
happens at script generation time that p
is present then our function of x we
multiply P and X that's actually what's
running on the cluster so what I'm doing
for you in Pig bend is trapping whatever
the current value of P is of course it
needs to be immutable data you can't
pass an atom in here or transient but if
you have just regular mutable data
structures you can then ship them off
and it will automatically ship that off
so you can use it in the cluster you can
also use stuff in let bindings you can
combine parameters and let bindings
together create new stuff you can define
functions elsewhere in your code and
then use that as your map function it's
going to be a function you define it can
be a closure core function it doesn't
matter and then you can also use that
within an anonymous function you can use
any type of closure phone
here is really the point and the way
that I'm doing this is the jar that
actually goes off to Hadoop to run this
code is an uber jar containing all of
your code so when I reevaluate this
function of X I'm reevaluating it within
the namespace you originally defined it
in so you have access to everything that
you did there including other namespaces
so if you want another library to use
with your MapReduce jobs just import
that as you're generating your script
and it will be there actually when
you're computing data on the cluster
lipstick is a library that we have at
Netflix it allows you to visualize Pig
jobs specifically each one of the blocks
on this screen here is an individual
MapReduce job you can see there's some
commonality between there the work that
I was doing here was some clustering
work and I wanted to try out three
different variations of the clustering
in Pig it would have been very difficult
i probably would have ended up just
writing the script three times and kind
of tweaking it in pig pen this is a for
loop each of those different three
variants uses the same input data so you
can see we're at the top there's just
kind of one starting point for the
script and then at that point it
branches out into our three different
variants and then comes back at the end
of the script to combine them together
the nice thing about pig pen is it
allows you to manage very large amounts
of complexity this is another example of
a script that I run this ran this one
had a few hundred Hadoop jobs in here
you could feasibly write this in pig or
in another language I don't know that
you would ever want to but in pig pen
this ends up being a couple hundred
lines of code I could take out big
chunks of logic shove and other chunks
of logic into my script and with a
simple case statement switch between
them I didn't have to worry about what
my data matchup would win anything else
line up a lot of the problems you would
normally have when you're writing
scripts it's much more functional
composition that you're thinking of when
you're writing pig pen jobs just as you
would write enclosure another tool that
we have at Netflix is Jeanie if any of
you are using it great if not you should
go check it out it makes cluster
management really easy there's a line
again plugin for pig pen that allows you
a one line you can submit your query to
the cluster this will send it off to
Hadoop package everything up for you
generate the script do all that legwork
and then Genie emails you when it's done
it is the simplest way to run
produce jobs in the future I want to get
away from using Pig exclusively it's
been a great starting off point pig has
done a lot of leg work in kind of
optimizing joins and moving data around
there's a lot of stuff there that I
didn't want to write from scratch but
we're looking into taking that abstract
expression tree running it on something
like spark or storm and for local mode
looking at Quarry sink instead of RX
just as another alternative and that's
all I've got any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>