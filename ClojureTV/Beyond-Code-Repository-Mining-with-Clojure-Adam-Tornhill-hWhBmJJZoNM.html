<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Beyond Code: Repository Mining with Clojure - Adam Tornhill | Coder Coacher - Coaching Coders</title><meta content="Beyond Code: Repository Mining with Clojure - Adam Tornhill - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Beyond Code: Repository Mining with Clojure - Adam Tornhill</b></h2><h5 class="post__date">2015-07-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hWhBmJJZoNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so good afternoon everyone and
welcome to this session about repository
mining with closure today I want to give
you a different perspective on your code
base and it's a perspective that will
help you identify code it's hard to
maintain code at risk for defects and
code that becomes a team proc tilled
ball net and to do that we need to go
beyond code so let's start by
considering what we programmers actually
do because we don't write code of course
we do that as well otherwise we wouldn't
have a conference here but that's not
where we're spending my or T of our time
if you look at the research you will see
that most of our time is spent making
modifications to existing code and the
mayor to that time in turn is spent
trying to understand what the code does
in the first place so if you want to
optimize any aspect of software
development we should optimize for ease
of change and I think that's in line
with Lehman's laws of soft revolution
and this lost they are more like a set
of observations and like all good things
they go all the way back to the 70s and
I like to present two of those laws to
you here and the first law is the law of
continuing change which says that a
system has to continuously evolve or it
will become progressively less useful
over time and this is in contrast with
the second law I like to present you the
law of increasing complexity and this
law says that as a system evolves its
complexity will increase unless we
actively try to reduce it what I like
about these two laws is that there is
some tension between them on the one
hand our code has to evolve in order to
stay useful but when it devolves it
becomes more complicated so we have to
fight that somehow this is a hard
problem
before I go into that I have a
confession to make I'm a dotnet
consultant and what that means is that I
spend most of my time working on systems
like this where multiple subsystems
distributed across multiple tiers and
that alone makes it pretty hard to get a
holistic picture of course these systems
are also developed in multiple languages
which makes it even harder to reason
about it and if that wasn't enough most
projects today are developed by multiple
programmers organized into multiple
teams and that leaves everyone with
their own view of how the system looks
now this isn't just about technical
complexity because systems that look
like that tend to come with an
organization like this so given this
mixture of technical and organizational
social complexity how can we find the
code that really matters well within the
software industry we have things like
the complexity metrics right so we have
a bunch of them we have things like
mccabe's cyclomatic complexity we have
the house that's volume measures we have
different measures for cohesion and so
on there are a lot of them but all of
them have in common is that we would
take a tool throw it at our code base
and outcomes a list of every module with
a nice complexity number so why don't we
use that more often to find a code that
also maintained well I don't know but I
know why I don't use complexity metrics
and the reason I don't use complexity
metrics is because complexity metrics
are pretty bad at predicting complexity
it's perhaps a bold statement so allow
it support it a little bit if we look at
the research and there is some research
that supports this as soon as you start
to control for the number of lines of
code more elaborate metrics
don't add any further predictive value
the number of lines of code that's a
really rough metric is that the best we
can do but there's a second reason as
well even if complexity metrics would
magically work they want to the trick
alone because complexity has this
property complexity alone is never a
problem if we have some complicated code
over here
and that code been in stable for five
years it works wonderful in production
and we never need to touch it well we
probably all have much more urgent
matters to attend to so what I'm saying
is that the code how it looks today is
important but it's even more important
to understand how the code got that way
in the first place we have to understand
the evolution of a code base because if
we understand the history of our code
were able to pre-act
a reactor into the future that helps us
identify precisely that code that really
matters and so we need somehow to get
that evolutionary data and the good news
are it's data that you all already had
we're just not used to think about it
that way I'm talking about their version
control systems our version control
systems basically the record every
change you make to the software and they
also record social information since we
know which programmer that interchange
so what I suggest is that we take an
evolutionary perspective on our code
base here's how that may look it may
look like an alien doesn't it but it's
not this is four hundred thousand lines
of code and we're going to return to
this visualization in just a minute but
please allow me to explain what you see
there in this visualization each module
each Clause is visualized as a circle
and their circles have two dimensions
first size is used to represent
complexity so the larger the circle the
more complicated the code beneath it but
I also said that complexity alone wasn't
good enough so we add a second dimension
too and this is data mined from a
version control system that's the code
change frequency in this case and that
allows us to spot an overlap between
complicated code that we also had to
work with often and that's what we call
a hotspot so let's return to this
visualization this is an interactive map
so you can zoom around to the level of
detail you are interested in but even
here on the highest level view were able
to detect a number of hotspots
now remember hotspots are complicated
code that we had to work with often what
kind of code is that what will refine if
we look into a hotspot to answer that I
decided to mine a second set of metrics
I decided to mine our bug tracking
system and the reason I did that is
because in most systems that tend to be
unevenly distributed some modules are
just more defect dense than others and
when I correlate the dead to the hot
spots I found out that the hot spots
identified seven out of the eight most
affect and sparks even more
interestingly the hot spots only made up
4% of the code yet that code was
responsible for seventy to defend
percent of all defects or if we turn it
around in this particular system if we
improve just four percent of the code we
get rid of the majority of all defects
and a hot spot analysis points you to
precisely those parts of the code base
that really matter and I'm going to show
you a bunch of other analysis that you
can do with version control data but
before we go there let's discuss the
tool I used to mine those metrics when I
started out with these techniques like
four or five years ago
I used the combination of shell scripts
and a Python code and that worked pretty
well for a long time but then two years
ago I started to write about these
techniques and that's that's what
eventually became your code as a crime
scene but I wanted to do something more
I wanted to give each my readers the
possibility to try these techniques out
on their own code base and there was
really no responsible way I could take
my unholy mess of shell scripts and
Python I put that in the hands of
someone else so I decided to revert the
whole thing from scratch and I decided
to do it in closure now why closure
well closure is an excellent choice for
data analysis but I didn't know that two
years ago I was just lucky
really the reason I choose closure was
much simpler it wasn't a technical
choice at all
I just wanted to learn the language and
I wanted to learn it because it looked
fun and fun is so much underestimate the
driver of the sign in fact I would say
fun it's a much underestimated motivator
in our industry fun is almost a
guarantee that things gets done so
that's why I choose closure and the tool
that I developed is called a code mat
and it's available on github you have
the link over there and you will get it
once more at the end it's an open source
tool licensed under GPL and code matt is
built around an architectural pattern on
pipes and filters that I think works
pretty well in our functional style so
code matt is a command line tool and it
operates on the version control logs so
we take those notes further into a
parser that converts the data to an
internal data format that gets passed on
to an aggregation step now aggregation
that's things like say you have a bunch
of commits during a single day and you
want to take all those commits and
consider them as one logical commit
that's one example of aggregation so
it's an optional step
and the result of that goes into an
analysis stage where the actual
calculations happen and code met today
supports I think 16 or 17 different
analyzes and the result from that flows
into the output module and that's a
really simple piece of code we either
just dump the thing to standard out so
we can pipe it to another program or
print it to a file as a developed code
Matt I discovered a bunch of interesting
libraries and then I think they were
applicable way beyond this current
application so I would like to share
what I found with you and I would like
to start by talking about parsing so
code Matt supports some of the popular
version control systems and those
version control systems they contain
virtually the same information but of
course in radically different formats it
would be way too simple otherwise so we
basically have to write a parser freeze
each version control system and I sit
down and looked at some different parts
of frameworks but a pretty soon settle
on insta chorus and the reason for that
is is because I find insta parse really
really simple and insta parse has a
design goal to make context-free
grammars as simple as regular
expressions now if you've been doing
regular expressions you know that's a
pretty much a relative statement but
instapass really succeeds in order to
give you a brief view on what it looks
like so where one commits entry here
from get from closure actually and if
you want to parse this with insta parse
the first thing we're to do is to define
a grammar here's what the grammar looks
like forget
so with insta parse we're pretty free
with the format cashews but what we did
was basically that we introduced one
rule for each element in the input that
we are interested in and once we have
defined that grammar that describes the
input
we just need to pass it to insta parse
parser function and that's a function
that generates another function and
that's our actual parser so let me
demonstrate that we have our get long
here and we have gotten this we have
defined a grammar and we are getting a
function back that is able to parse this
kind of input so when we invoke it we
get closure data structure aspec and in
this case the data is and Hickock format
but instars also supports and lied and I
actually think that unlight would be a
better fit for this particular
application so I'm going to look into
that
so insta parse was really easy to get
started with and it's something I
definitely recommend if you're going to
write a parser and in stuff ours also
has a second design goal it's supposed
to be fast and it looked pretty good at
first but then I try to run the tool on
projects with richer history some of
them going like 10-15 years back in time
and suddenly my program literally
ground to a halt it became slower than
our windows 95 boot and think that says
something so I was a bit surprised and
this was the first time that closure
gave me a really hard time I found it
really hard to build a mental model of
the performance due to the mix of
laziness and unless an S so I pretty
much gave up on that and through a
profiler at the code and it turned out
that the bottleneck was in the parsing
stage so again insta parse was supposed
to be fast and it turned out it is it is
really fast but it's also quite memory
hungry so classic trade-off isn't it
so in Stoppers in order to be able to
backtrack it needs to maintain a list of
intermediate results and that grows
pretty fast so once we realized that the
change was pretty simple
instead of parsing the whole log in one
sweep we changed it to read
lazily from the log file line-by-line
and as soon as we have had one complete
entry we freed that two in stars and
then we get the next entry read it out
and parse that in isolation and so on
and once we did this tokenization the
performance problem went away so that
was a really simple change and it's
something I recommend if you're using in
stars a large input please make sure to
shall get up your garbage collector is
going to thank you so that was a bit
about insta pours I'm going to talk
about some other library soon but let's
first discuss a second analysis and I
like to introduce it with one of the
best quotes I know about software
development code doesn't lie if we want
to understand how something really
really really works we have to look into
the code the truth is there and it's
described in very precise technical
details so Jo doesn't lie but it doesn't
tell the whole truth either and if you
limit ourselves to what's visible in the
code we miss a lot of valuable
information and I want to show you one
example on that this is an analysis
called temporal coupling and that's
something that you cannot measure from
code alone temporal coupling works like
this this is a simple system here we
just have three different modules and in
the first commit that we do we change
the fuel injector and the Diagnostics
module together in the second commit
we're changing something else
in the third commit we're back to
changing a fuel injector and the
agnostics module together again now if
this is a trend that continues there has
to be some kind of relationship between
the fuel injector and the agnostics
module because they are changing
together what kind of relationship could
that be what you will find most of the
time is this you have some code over
here
the code here this code uses that code
so every time you changed this API you
have to change the client as well and
this is just our plain old physical
coupling and that's indeed something you
can measure from code but remember
temporal coupling wasn't measure from
code it was measure from the evolution
of the code from version control history
so that means you will sometimes find
cases like this you have some code over
here some other code here and there's no
dependency at all between them yet they
keep changing together over time how can
that be
is that some kind of spooky quantum
physics process you shall or not what I
tend to find when I look into those
cases is so good dear old friend copy
paste so you have some code over here we
come to price the Terran here and now
each time I modify the original you have
to remember to modify the copies as well
so a temporal coupling is indeed a great
tool to detect software clones but
temporal coupling isn't always bad
sometimes you actually want temporal
coupling and I'd like to show you an
example from closure itself if we turn
back time almost 10 years to 2006
closure at that time was developed in
parallel on both the CLR and the JVM at
least that's what's evident from the
version control logs so you will see
that there was a C sharp claws called
persistent list map and there was a Java
equivalent called persistent list map
dot Java
and if we measure temporal coupling in
those early days of closure you will
find that those two classes changed
together
83% of the time and in this case that's
actually the kind of coupling you want
so if that's the coupling you expect why
not use that to our advantage
so what I'm working on right now is to
design a commit tooks forget that we
check this kind of expected tempura
coupling so say you working on this this
poly got killed basis and you make a
change here over here in dotnet land and
then you want to commit that you get the
warning that says hey normally when you
do this kind of changes you also make a
change over here do you want to continue
I think that could be a genuinely useful
but there's more we can do with tempura
coupling tempura coupling is sent
limited to interview individual modules
or classes we can analyze complete
software architectures with it and I'd
like to show you a brief example I want
to show you a micro service architecture
and the reason I choose micro services
is because right now micro services are
all the rage and that just means that
tomorrow's legacy systems are going to
be micro services so this is what micro
services looked like in PowerPoint
in reality they tend to look a little
bit more like this so we are programmers
we don't like to repeat ourselves so why
don't we extract a set of shared
utilities perhaps we even encapsulate
the communication and of course we
follow the recommendation of providing a
service template shared between all our
micro services and of course we want to
make it as easy as possible to consume
our services so let's introduce a bunch
of client libraries here now in micro
services loke coupling is kinya as soon
as we start to couple different services
to each other we lose most of the
advantages of a micro service
architecture and we were left with an
excess mass of complexity so what I
recommend and that's something that's
applicable to all kinds of architectures
is that we try to do a temporal coupling
analysis on this level and spot temporal
couples that violate our architectural
principles so we want to
investigate things like this where
services are coupled to a shared library
or even worse tempura coupling where
multiple services change together so to
do those kind of things you need an
aggregation step I talked about I won't
go into the details now but if you're
interested in that just talk to me
afterwards and I'd be happy to show you
I want to talk a little bit about
closure again Anna killed Matt builds
quite heavily on a library called
encounter so let me see how many of you
are familiar with encounter
oh cool something like 10 to 15% of you
so encounter is a set of libraries for
doing statistical computing and to do
something interesting with encounter you
have to get your data into a data set a
data set is basically just like a table
you have a bunch of columns and then you
have a rows for each entry in your data
and the cool thing with encounter is
that you get a lot of things out of the
box and I want to show you one example I
want to show you how to do a code churn
analysis it's anyone familiar with code
churn some of you great
so code churn basically refers to the
amount of change in your system so if
you're if you add five lines of code you
have a positive churn of five if you
remove ten lines of code you have a
negative here of 10 and the interesting
thing with code churn is never their
absolute numbers the interesting thing
with coach churn is the trend over time
and you see an example of that here from
one particular system and see those
large red lines they're sort of raw
shirin values over time now we have a
second dimension there as well we have a
rolling average and a rolling average
it's a great way to remove
in your data in order to be able to spot
a possible long-term trend so a rolling
average is like a sliding window imagine
you put a sliding window on your data
calculate an average move it one step
calculate a new average move with one
step and so on so I said that the
interesting thing is the trend and
that's because concern is one of the
best predictors of post release defects
the higher they earn the more post
released effect so in this case you see
that the shirin here over the past
months has been increasing and if this
is a project that is approaching a
deadline or release here that's a clear
warning sign that sign means that this
codebase isn't ready for release there's
a lot of work left before you have a
stable codebase at that point in time so
I want to show you how these kind of
shorts are generated with encounter if
you run a cogent analysis with code Matt
you get a CSV file as output where you
can see the closure on the raw shown
values organized by day
remember with encounter if we want to do
something interesting with encounter we
have to get the data into a data set and
that's pretty simple since encounter
comes with our read data set function so
we just point it to our CSV file and we
see here or pretty printed encounter
data set and you see it has the same
columns as our input data so now since
we're interested in a trend over time a
time serious
we should really look into it in
countersue library and to do something
with su we need to have a su value a su
value is basically just another
encounter data set but with one special
column a special index column containing
date/time objects that specify our time
series so we could create the sewer
object a su value by calling the su
function passing our data set and spec
defying the column that contains the
time serious and that's basically it
so we get a su value back you see the
index column here with the date/time
objects now the next thing you wanted to
do was calculate the rolling average
remember our sliding window and that's
pretty simple with encounter the only
thing we have to do here is we use some
encounter magic to extract the added
column because that's the best predictor
when it comes to kill churn and then we
passed that to Sue's roll main function
and we also specify a width or a sliding
window and my general recommendation is
to choose something that means something
in your particular context so let's say
you have working iterations of two weeks
choose a sliding window of two weeks so
the values we get back from roll mean we
feed them create a new data set and
merge that with the original data sets
we get a new data set back and this new
data set contains a rolling added column
on a large system you have a lot of data
so we need some kind of algorithms in
that data and my suggestion is that we
use the best known pattern detector in
the universe our visual brain so the
only thing we have to do is to visualize
that data and that's quite straight
forward with encounter because encounter
includes a shorts library and in
encounter shorts you will find most of
the common shorts in this case we're
interested in the time series plot so we
choose that short and we pass it our
dates but converted to millisecond since
1970 because that's what underlying
library wants and then we post a bra
shown values for that time serious and
we also specify a bunch of properties
here that will be included on our short
and once we run this function with the
data we had earlier we get something
that we can view or short like this now
this is a different data set that the
one I showed you earlier this is from
Microsoft rustling project and Ruslan is
like Microsoft's a open source flagship
product it's a compiler platform for
dotnet and this coachin was calculated
just as Roslyn hit its first release and
you see here on the rolling yet average
you see that there was a peak of
activity at the beginning of the year
and then it declined the code base can
have stabilized over the last month
there and I think in general if you work
in this traditional model I think that's
just the kind of Shuren you want to see
in your own projects a code base becomes
more and more stable as it approaches on
release
alright do you remember that I told you
that I used to work on systems that look
like this that usually come with an
organization like that that means as
sometimes have to take a step back and
remind myself why programming used to be
fun and the way I've been doing in that
is by hacking around in processing
processing is a language and environment
for creative coding and it it's pretty
much a library built on top of Java it's
Lu tends to be a bit more procedural
than Java and you see some examples on
processing sketches here and what I find
so liberating about programming
processing is that I try to break every
best practice there is I try to break
every known design principle I use
global variables i mutate state I next
conditional logic and I never write a
single test and it's so fun
so when I start to work with these kind
of techniques I used processing to do
the visualizations as well because I
already knew the language so here's an
example of that this is a tree map
showing the change frequencies mined
from Russian control data in one
particular code base but it soon turned
out that yeah it was fun and liberating
to start with but it wasn't as fun once
I had to maintain those programs so
that's why I'm happy to learn that
closure has an alternative will how many
of you have worked with quill like 20%
of you great
so quill is a wrapper around processing
and I really just want to show the
programming model here quill is even
more fun than processing a quill uses
the basic concept called a sketch so we
use a macro def sketch to define our
sketch that's just as in processing we
also have to specify a setup function
and setup is something that they cool
with call for us once at startup to
create the initial State that's just as
in processing as well and then we have
to specify our draw function and draw
will be called repeatedly to do
something interesting on screen so far
that's exactly the programming model of
processing but quill takes it takes it
one step further
quill allows us to specify a set of
middlewares
in this case I'm using something called
the fun mode and what the fun mode does
is that it removes the need for those
shared global variables because fun mode
allows me to program using your
functional style so now with fun mode
setup will return the initial State and
that will be passed into draft that
returns the next state and so on and if
you have things like Mouse event
handlers you're doing interactive
visualizations those event handlers will
be past the old state and return the new
state
so with the middleware and quill that's
a huge improvement over reprocessing of
course you still have a bunch of side
effects and I don't want to go into the
details but basically every time you
want to do something interesting on
screen it's going to be a side effect so
we want to specify our background color
or draw a text or draw a line side
effects but at least we can program in a
functional style and quit so I used trim
to solve the following problem
however they talked about predictors of
defects it turns out that one of the
very best predictors is the number of
programmers behind a piece of code the
more programmers behind a piece of code
the more quality sheis within that code
and I started to think well again that's
data that we actually can deduce from a
version control system so what if we can
find those modules in our system that
suffer from access parallel development
and again it will be a lot of data so
needed a way to visualize it and I
choose to use quill for that and I used
two visualization technique called
fractal figures and fractal figures is a
technique from this academic paper down
here and further figures works like this
you consider each module a box and each
programmer gets assigned a color and the
more that programmer has contributed the
larger the area of the box so here is an
example from closure
and if we're after access parallel
development we have two modules that
realistic out here one of them is the
compilation module and the second one is
a Java class called numbers and I think
this is the great thing with fraktur
figures because even though those two
modules have approximately the same
amount of contributors the patterns look
radically different you see the parent
to the right I would say that's quite
unlikely to be a problem
who see that dr. developer has written
virtual all the code and we just have
small contributions from other
programmers so probably we have a quite
a consistent style and programming model
there now closure and open source is a
bit special
due to review process in closure but if
this was a closed source project I would
be worried about the pattern to the left
because you have a lot of different
programmers that all wrote parts of a
larger module so it's quite likely that
we will find inconsistencies and
applications within that code so if you
find something like that in your code
base you have to look into it
investigate it and understand why and I
promise you what you will find most
real-time is that code changes for a
reason the reason code attracts many
programmers is because it has many
reasons to do so it probably has way too
many responsibilities so I think that
fractal figures are useful even without
knowing the programmer behind each color
but of course we can add that
information as well and get a second use
for it now we can use it as a
communication IDE so for example say I
want to know about the evaluation module
over here we see that it's the light
blue developer so that's Stewart alloway
he's the one who knows how that works
and if I want to know something about
closure in general it seems like the
dark blue developer has contributed a
lot that's a guy named rich Hickey so he
probably knows a thing or two about
closure so my key role in this
presentation was to show you some
examples of the wealth of information
that's stored in a version control
systems and how we can mine and analyze
it with closure and this presentation
has to really just scratch the surface
of what's possible you see a bunch of
different examples here here's an
attempt to visualize the communication
paths between programmers
we have chode h diagrams so we can
detect the relative stability of
packages and we have the knowledge
diagrams and a lot of other stuff if you
want to dive deeper into this field I
put together a set of blood seer that
introduces some of the topics and of
course there's also the book and most
importantly you have the source code for
code metier and i will be both happy and
honored to accept pull requests now
before I leave you I just want to take
the time and say thanks a lot for
listening to me and may the code be with
you thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>