<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Beyond Code: Repository Mining with Clojure - Adam Tornhill | Coder Coacher - Coaching Coders</title><meta content="Beyond Code: Repository Mining with Clojure - Adam Tornhill - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Beyond Code: Repository Mining with Clojure - Adam Tornhill</b></h2><h5 class="post__date">2015-07-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hWhBmJJZoNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright so good afternoon everyone and
welcome to this session about repository
mining with closure today I want to give
you a different perspective on your code
base and it's a perspective that will
help you identify code it's hard to
maintain code at risk for defects and
code that the concert in Prague tilled
ball neck and to do that we need to go
beyond code so let's start by
considering what we programmers actually
do because we don't write code of course
we do that as well otherwise we wouldn't
have a conference here but that's not
where we spend my ority of our time if
you look at the research you will see
that most of our time is spent making
modifications to existing code and the
mayor to if that time in turn is spent
trying to understand what the code does
in the first place so if you want to
optimize any aspect of software
development we should optimize for ease
of change and I think that's in line
with lemans loss of soft revolution and
this lost they are more like a set of
observations and like all good things
they go all the way back to the 70s and
I like to present two of those laws to
you here and the first law is the law of
continuing change which says that a
system has to continuously evolve or it
will become progressively less useful
over time and this is in contrast with
the second law I like to present you the
law of increasing complexity and this
now says that a so system evolves its
complexity will increase unless we
actively try to reduce it what I like
about this to loss is that there is some
tension between them on the one hand our
code has to evolve in order to stay
useful but when it devolves it becomes
more complicated so we have to fight
that somehow this is a hard problem
before I go into that I have a
confession to make I'm a dot net
consultant and what that means is that I
spend most of my time working on systems
like this where multiple subsystems
distributed across multiple tiers and
that alone makes it pretty hard to get a
holistic picture of course these systems
are also developed in multiple languages
which makes it even harder to reason
about it and if that wasn't enough most
projects today are developed by multiple
programmers organized into multiple
teams and that leaves everyone with
their own view of how the system looks
now this isn't just about technical
complexity because systems that look
like that tend to come with an
organization like this so given this
mixture of technical and organizational
social complexity how can we find the
code that really matters well within the
software industry we have things like
the complexity metrics right so we have
a bunch of them we have things like
McCabe cyclomatic complexity we have a
house that volume measures we have
different measures for cohesion and so
on there are a lot of them but all of
them have in common is that we would
take a tool throw it at our code base
and out comes a list of every module
with a nice complexity number so why
don't we use that more often to find a
code that also maintained well I don't
know but I know why I don't use
complexity metrics and the reason i
don't use complexity metrics is because
complexity metrics are pretty bad at
predicting complexity it's perhaps a
bold statement so allow it support it a
little bit if you look at the research
and there is some research that supports
this as soon as you start to control for
the number of lines of code more
elaborate metrics
don't add any further predictive value
the number of lines of code that's a
really rough metric is that the best we
can do but there's a second reason as
well even if complexity metrics would
magically work they want to the trick
alone because complexity has this
property complexity alone is never a
problem if we have some complicated code
over here and that code been stable for
five years it works wonderful in
production and we never need to touch it
well we probably all have much more
urgent matters to attend to so what I'm
saying is that the code how it looks
today is important but it's even more
important to understand how the code get
that way in the first place we have to
understand the evolution of a code base
because if we understand the history of
a code were able to pre act at reactor
into the future that helps us identify
precisely that code that really matters
and so we need somehow to get that
evolutionary data and the good news are
its data that you all already had we've
just let use to think about it that way
I'm talking about their version control
systems our version control systems
basically the record every change from
make to the software and they also
records social information since we know
which programmer that interchange so
what I suggest is that we take an
evolutionary perspective on our code
base here's how that may look it may
look like an alien doesn't it but it's
not this is four hundred thousand lines
of code and we're going to return to
this visualization in just a minute but
please allow me to explain what you see
there in this cell with sensation each
module each class is visualized as a
circle and their circles have two
dimensions first size is used
% complexity so the larger the circle
the more complicated code beneath it but
I also said that complexity alone wasn't
good enough so we add a second dimension
to and this is data mined for our
version control system that's the code
change frequency in this case and that
allows us to spot an overlap between
complicated code that we also have to
work with often and that's what we call
a hot spot so let's return to this
visualization this is an interactive map
so you can zoom around to the level of
detail you are interested in but even
here on the highest level view were able
to detect a number of hotspots now
remember hotspots are complicated code
that we have to work with often what
kind of code is that what will we find
if we look into a hotspot to answer that
I decided to mine a second set of
metrics I decided to mind our bug
tracking system and the reason I did
that is because in most systems bugs
tend to be unevenly distributed some
modules are just more defect dense than
others and when I correlate the debt to
the hot spots I found out that the hot
spots identified seven out of the eight
most effect and sparks even more
interestingly the hot spots only made up
four percent of the code yet that code
was responsible for 72 for center of all
defects or if we turn it around in this
particular system if we improve just
four percent of the code we get rid of
the majority of all defects and a
hotspot analysis points you to precisely
those parts of the code base that really
matter and I'm going to show you a bunch
of our analysis that you can do with
version control data but before we go
there let's discuss the tool I used to
mind those metrics when I started out
and with these techniques like four or
five years ago
I used the combination of shell scripts
and a Python code and that worked there
pretty well for a long time but then two
years ago I started to write about these
techniques and that that's what
eventually became your code is a crime
scene but I wanted to do something more
I wanted to give each my readers the
possibility to try these techniques out
on their own code base and there was
really no responsible way I could take
my unholy mess of shell scripts and
Python I put that in the hands of
someone else so I decided to write the
whole thing from scratch and I decided
to do it in closure now why closure well
closure is an excellent choice for data
analysis but I didn't know that two
years ago I was just lucky really the
reason I choose closure was much simpler
it wasn't a technical choice at all I
just wanted to learn the language and I
wanted to learn it because it looked fun
and fun is a much underestimate the
driver of design in fact that we siphon
it so much underestimated motivator in
our industry fun is almost guarantee
that things gets done so that's why i
choose closure and the tool that are
developed is called a chode mat and it's
available on github you have the link
over there and you will get it once more
at the end it's an open source tool
licensed under GPL and code matt is
built around an architectural pattern
pipes and filters that I think works
pretty well in our functional style so
code matt is a command line tool and it
operates on the version control logs so
we take those logs further into a parser
that converts the data to an internal
data format that gets passed down to an
aggregation step now aggregation that's
things like say you have a bunch of
commits during a single day and you want
to take all those commits and consider
them as one logical commit that's one
example of aggregation so it's an
optional step and the result of that
goes into an
stage where the actual calculations
happen and code met today supports a
think 16 or 17 different analyzes and
the result from that flows into the
output module and that's a really simple
piece of code we either just dump the
thing to standard out so we can pipe it
to another program or print it to a file
as a developed killed Matt I discovered
a bunch of interesting libraries and
then I think there are applicable way
beyond this current application so i
would like to share what I found with
you and I would like to start by talking
about parsing so code Matt and supports
some of the popular version control
systems and those version control
systems they contain virtually the same
information but of course in radically
different formats it would be way too
simple otherwise so we basically to
write a parser freeze each version
control system and then I sit down and
looked at the some different parts of
frameworks but a pretty soon settle on
insta powers and the reason for that is
is because I find in stock horse really
really simple and insta parse has a
design goal to make context-free
grammars as simple as regular
expressions now if you've been doing
regular expressions you know that's a
pretty much a relative statement but
instead parse really succeeds in order
to give you a brief view of what it
looks like so where one commits entry
here from get from closure actually and
if you want to parse this within stars
the first thing we're to do is to define
a grammar here's what the grammar looks
like forget so within stars we're pretty
free with the format you choose but what
we did was basically that we introduced
one rule for each element in the input
that we are interested in and once we
have defined that grammar that describes
the input
we just need to pass it to insta farce
Parsa function and that's a function
that generates another function and
that's our actual parser so let me
demonstrate that we have our get lung
here and we have gotten this we've
defined a grammar and we are getting a
function back that is able to parse this
kind of input so when we invoke it we
get closure data structures batch and in
this case the data is an hiccup format
but the insta pars also supports and
lied and I actually think that enlight
would be a better fit for this
particular application so I'm going to
look into that so insta purse was really
easy to get started with and it's
something I definitely recommend if
you're going to write a parser and in
stuff ours also has a second design goal
it's supposed to be fast and it looked
pretty good at first but then I try to
run the tool on projects with Richard
history some of them going like 10 15
years back in time and suddenly my
program literally ground to a halt it
became slower than our windows 95 reboot
and think that says something so I was a
bit surprised and this was the first
time that closure we gave me a really
hard time I found it really hard to
build a mental model of the performance
due to the mix of laziness and unless an
S so I pretty much gave up on that and
through a profiler at the code and it
turned out that the bottleneck was in
the parsing stage so again insta parts
were supposed to be fast and it turned
out it is it is really fast but it's
also quite memory hungry so classic
trade-off isn't it so insta force in
order to be able to backtrack it needs
to maintain a list of intermediate
results and that grows pretty fast so
once we realize that
the change was pretty simple instead of
parsing their whole log in one sweep we
changed it to read lazily from the log
file line by line and as soon as we have
had one complete entry we threw that too
in stars and then we get the next entry
read it out and parse that in isolation
and so on and once we did this
tokenization the performance problem
went away that was a really simple
change and it's something I recommend if
you're using in stars a large input
please make sure to shank it up your
garbage collector is going to thank you
so that was a bit about insta porous I'm
going to talk about some other library
soon but let's first discuss a second
analysis and I like to introduce it with
one of the best quotes are know about
software development code doesn't lie if
we want to understand how something
really really really works we have to
look into the code the truth is there
and it's described in very precise
technical details so cure doesn't lie
but it doesn't tell the whole truth
either and if you limit ourselves to
what's visible in the code we miss a lot
of valuable information and I want to
show you one example on that this is an
analysis called temporal coupling and
that's something that you cannot measure
from code alone tempura coupling works
like this this is a simple system here
we just have three different modules and
in the first commit that we do we change
the fuel injector and the diagnostics
module together in the second commit
we're changing something else in the
first commit we're back to changing a
fuel injector and the agnostics module
together again now if this is a trend
that continues there has to be some kind
of relationship between the fuel
injector and the agnostics module
because they are changing together what
kind of relationship could that be
what you will find most of the time is
this you have some code over here some
other code here this code uses that code
so every time you change this API you
have to change the client as well and
this is just our plain old physical
coupling and that's indeed something you
can measure from code but remember
tempura coupling wasn't measure from
code it was measured from the evolution
of the code from version control history
so that means you will sometimes find
cases like this you have some code over
here some other code here and there's no
dependency at all between them yet they
keep changing together over time how can
that be is that some kind of spooky
quantum physics process you shall a not
what I tend to find when I look into
those cases is a good dear old friend
coffee tastes so you have some code over
here real compromise the Tiran here and
now each time a modified original you
have to remember to modify the corpus as
well so a temporal coupling is indeed a
great tool to detect software clones but
temporary coupling isn't always bad
sometimes you actually want temporal
coupling and I'd like to show you an
example from closure itself if we turn
back time almost 10 years to 2006
closure at that time was developed in
parallel on both the clr and the JVM at
least that's what's evident from the
version control logs so you will see
that there were so C sharp claws called
persistent list map and there was a Java
equivalent called persistent list method
of java and if we measure temporal
coupling in those early days of closure
you will find that those two classes
change together eighty-three percent of
the time and in this case that's
actually the kind of coupling you want
so if that's the coupling you expect why
not use that to our
advantage so what I'm working on right
now is to design a committee ugh forget
that will check this kind of expected
tempura coupling so say you working on
this this poly got killed basis and you
make a change here over here internet
land and then you want to commit that
you get the warning that says hey
normally when you do these kind of
changes you also make a change over here
do you want to continue I think that
could be a genuinely useful but there's
more we can do with temporary coupling
temporal coupling is sent limited to
interview individual modules or closest
we can analyze complete software
architectures with it and I'd like to
show you a brief example I want to show
you a micro service architecture and the
reason as use micro services is because
right now micro services are all the
rage and that just means that tomorrow's
legacy systems are going to be
microservices so this is what
microservices looked like in PowerPoint
in reality they tend to look a little
bit more like this so we are programmers
we don't like to repeat ourselves so why
don't we extract a set of shared
utilities perhaps we even encapsulate
the communication and of course we
follow the recommendation of providing a
service template shared between all our
micro services and of course we want to
make it as easy as possible to consume
our services so let's introduce a bunch
of client libraries here now in
microservices loke coupling is king as
soon as we start to couple different
services to each other we lose most of
the advantages of a micro service
architecture and we are left with an
excess mess of complexity so what I
recommend and that's something that's
applicable to all kinds of architectures
is that we try to do a temporal coupling
analysis on this level and spot temporal
couples that violate our architectural
principles so we want to investigate
things like this where services are
coupled to a shared library or even
worse tempura coupling where multiple
services change together so to do those
kind of things you need an aggregation
step I talked about I won't go into the
details now but if you're interested in
that just talk to me afterwards and I'll
be happy to show you I want to talk a
little bit about closure again Anna
killed Matt builds quite heavily on a
library called encounter so let me see
how many of you are familiar with
encounter I'll kill something like ten
fifteen percent of you so encounter is a
set of libraries for doing statistical
computing and to do something
interesting with encounter you have to
get your data into a data set a data set
is basically just like a table you have
a bunch of columns and then you have a
rose for each entry in your data
and the cool thing with encounter is
that you get a lot of things out of the
box and I want to show you one example i
want to show you how to do a code churn
analysis is anyone familiar with codes
earn some of you great so coach yearn
basically refers to the amount of change
in your system so if you're if you add
five lines of code you have a positive
churn of five if you remove 10 lines of
code you have a negative share of 10 and
the interesting thing with concern is
never the absolute numbers the
interesting thing with concern is the
trend over time and you see an example
of that here from one particular system
and see those large red lines those are
the raw shirin values over time now we
have a second dimension there as well we
have a rolling average and a rolling
average it's a great way to remove
flirtations in your data in order to be
able to spot a possible long-term trend
so a rolling average is like a sliding
window imagine you put a sliding window
on your data calculate an average move
it one step calculate a new average with
one step and so on so I said that the
interesting thing is the trend and
that's because kilchurn is one of the
best predictors of post-release defects
the higher discern the more post
released effects so in this case you see
that the shirin here over the past month
has been increasing and if this is a
project that is approaching a deadline
or release here that's a clear warning
sign that sign means that this code base
isn't ready for release there's a lot of
work left before you have a stable code
base at that point of time so i want to
show you how these kind of shorts are
generated within counter if you run a
cogent analysis with code Matt you get a
CSV
file as output where you can see the
coach on the Russian values organized by
day remember with encounter if we want
to do something interesting with
encounter we had to get the data into a
data set and that's pretty simple since
encounter comes with our read data set
function so we just point it to our CSV
file and we see here or pretty printed
encounter data set and you see it has
the same columns as our input data so
now since we're interested in a trend
over time at I'm serious we should
really look into ding countersue library
and to do something with sue we need to
have a su value a su value is basically
to adjust another encounter data set but
with one special column a special index
column containing datetime objects that
specify our time serious so we could
create the su object a su value by
calling the zoo function passing our
data set and specifying the column that
contains the time serious and that's
basically it so we get a su value back
you see the index column here with the
date-time objects now the next thing you
wanted to do was calculate the rolling
average remember our sliding window and
that's pretty simple with encounter the
only thing we have to do here is we use
some encounter magic to extract the
added column because that's the best
predictor when it comes to kill churn
and then we pass that to Sue's role main
function and we also specify a wid or a
sliding window and my general
recommendation is to choose something
that means something in your particular
context so let's say you have work in
iterations of two weeks choose a sliding
window of two weeks so the values we get
back from rural mean we feed them create
a new data set and merge that with the
original data sets will get a new data
set back and this new data set contains
a rolling
column on a large system you have a lot
of data so we need some kind of Elder in
to detect patterns in that data and my
suggestion is that we use the best-known
pattern detector in the universe our
visual brain so the only thing we have
to do is to visualize that data and
that's quite straightforward with
encounter because encounter includes a
shorts library and in encounter shorts
you will find most of the common charts
in this case we're interested in the
time series plot so we use that short
and we pass it our dates but converted
to millisecond since 1970 because that's
what underlaying library wants and then
we post a Russian values for that time
serious and we also specify a bunch of
properties here that will be included on
our short and once we run this function
with the data we had earlier we get
something that we can view or short like
this now this is a different data set
that the one I showed you earlier this
is from a microsoft roslyn project and
Ruslan is like Microsoft's a open source
a flagship product it's a compiler
platform for dotnet and this coachin was
calculated just as Roslin hit his first
release and you see here on the rolling
out average you see that there was a
peak of activity at the beginning of the
year and then it declined the code base
can have stabilized over the last
month's here and I think in general if
you work in this traditional model I
think that's just the kind of shirin you
want to see in your own project a code
base becomes more and more stable as it
approaches and release all right do you
remember that I told you that I used to
work on systems that look like this
that usually come with an organization
like that that means as sometimes have
to take a step back and remind myself
why programming used to be fun and the
way I've been doing in that is by
hacking around in processing processing
is a language and environment for
creative coding and it it's pretty much
a library built on top of Java it's Lou
tends to be a bit more procedural than
Java and you see some examples on
processing sketches here and modified so
liberating about programming processing
is that I try to break every best
practice the risk I try to break every
known design principle I use global
variables I mutate state I nest
conditional logic and I'll never write a
single test and it's so fun so when I
start to work with these kind of
techniques I used processing to do the
visualizations as well because I already
knew the language so here's an example
of that this is a tree map showing their
changed frequencies mind from Russia
control data in one particular code base
but it soon turned out that yeah it was
fun and liberating to start with but it
wasn't as fun once I had to maintain
those programs so that's why I'm happy
to learn that closure has an alternative
twill how many of you have worked with
quill I would like twenty percent of you
great so twill is a wrapper around
processing and I really just want to
show the programming model here quill is
even more fun than processing a quill
uses the basic concept called a sketch
so we use a macro def sketch to define
our sketch that's just the same
processing we also have to specify a set
up
function and setup is something that
they're cool with call for us once at
startup to create the initial state
that's just as in processing as well and
then we have to specify a giraffe
function and draw will be called
repeatedly to do something interesting
on screen so far that's exactly the
programming model of processing but
quill takes it takes it one step further
quill allows us to specify a set of
Mahlon where's in this case I'm using
something called the fun mode and what
the fun mode does is that it removes the
need for those shared global variables
because fan mode allows me to program
using your functional style so now with
fun mode set up we'll return the initial
state and that will be passed in to draw
that returns the next date and so on and
if you have things like mouse event
handlers you're doing interactive
visualizations those event handlers will
be past the old state and return the new
state so with the middleware and quill
that's a huge improvement over
processing of course you still have a
bunch of side effects and I don't want
to go into details but basically every
time you want to do something
interesting on screen it's going to be a
side effect so we want to specify your
background color or draw text or draw a
line side effects but at least we can
program in a functional style and quit
so I used cream to solve the following
problem already talked about predictors
of defects it turns out that one of the
very best predictors is the number of
programmers behind a piece of code the
more programmers behind a piece of code
the more quality sheÃ­s within that code
and I started to think well again that's
data that we actually can deduce from a
version control system so what if we can
find those modules in our system that
suffer from excess parallel development
and again it will be a lot of data so
needed a way to visualize it
I choose to use quill for that and I
used to visualization technique called
fractal figures and fractal figures is a
technique from this academic paper down
here and fret the figures works like
this you consider each module a box and
each programmer gets assigned a color
and the more that programmer has
contributed the larger the area of the
box so here is an example from closure
and if we're after access parallel
development we have two modules that
realistic out here one of them is the
compilation module and the second one is
a java class called numbers and I think
this is the great thing with factor
figures because even though those two
modules have approximately the same
amount of contributors the patterns look
radically different you see the parent
to the right I would say that's quite
unlikely to be a problem because we see
that dark blue developer has written
virtual all the code and we just have
small contributions from other
programmers so probably we have a quite
a consistent style and programming model
there now closure and open source is a
bit special due to review process
enclosure but if this was a closed
source project I would be worried about
the pattern to the left because you have
a lot of different programmers that all
wrote parts of a larger module so it's
quite likely that will fill find
inconsistencies and applications within
that code so if you find something like
that in your code base you have to look
into it investigate it and understand
why and I promise you what you will find
most real time is that code changes for
a reason the reason code attracts many
programmers is because it has many
reasons to do so it probably has way too
many responsibilities so I think that
fraca figures are useful even without
knowing the programmer
behind each color but of course we can
add that information as well and get a
second use for it now we can use it as a
communication ID so for example say I
want to know about the evaluation module
over here we see that it's the light
blue developer so that's Stuart Alloway
he's the one who knows how that works
and if I want to know something about
closure in general it seems like the
dark blue developer has contributed a
lot that's a guy named Ricky so he
probably knows a thing or two about
closure so my goal in this presentation
was to show you some examples of the
wealth of information that stored in a
version control systems and how we can
mine and analyze it with closure and
this presentation has to really just
scratch the surface of what's possible
you see a bunch of different examples
here here's an attempt to visualize the
communication paths between programmers
we have code H diagrams so we can detect
the relative stability of packages and
we have the knowledge diagrams and a lot
of other stuff if you want to dive
deeper into this field I put together a
set of blogs here that introduces some
of the topics and of course there's also
the book and most importantly you have
the source code for code mÃ©tier and I
will be both happy and honored to accept
pull requests now before I leave you I
just want to take the time and say
thanks a lot for listening to me and may
the code be with you thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>