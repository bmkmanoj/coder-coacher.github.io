<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Live - Mike Anderson | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Live - Mike Anderson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Live - Mike Anderson</b></h2><h5 class="post__date">2013-01-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QJ1qgCr09j8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my topic today is machine learning
and just before I get started I just
wondering - a quick show of hands who
here is actually done some machine
learning code is the machine learning
solutions ok guys it's about about 30%
so I will take a bit of time today to
talk a bit about the machine learning of
bit about machine learning theory but
the main point of today is to actually
do some live coding and actually
demonstrate what you can do with machine
learning in closure now the demos
actually was working nicely for a week
this lunchtime I had a dreaded out of
memory exception a little debugging
success session I think it's okay now
but fingers crossed let's see how it
goes ok so it's I think it's always good
to start with the definition so what I
naturally did was went to Wikipedia and
pulled off the first thing I could find
machine learning is the field of study
that gives computers the ability to
learn without being explicitly
programmed now the quotes over 50 years
old but I think it's actually still
pretty good definition I mean it
captures the most important point what
we're interested in is getting our
computers to learn to do useful things
without you having to explicitly tell
them every single task and because in
large complex data that rapidly becomes
impossible there is actually one big
problem with its definition though can
anyone see what it is No
it it's a circular definition I mean
they use this word learn to describe
machine learning I mean what kind of
definition is that and this learning
thing is very interesting what is
learning and learning is something which
is so intuitive to us that we do it all
of the time without even thinking I mean
maybe you meet someone at a tech
conference you remember their name
if wallah you've just learned something
and it's helpful to think about what
that actually means
and I find it helpful to think about
learning in a very very specific way
learning means building functions from
experience every machine learning
problem can ultimately be conceptualized
as a function mapping some kind of input
to some kind of output it can be a
simple mathematical function you can do
spam filtering if you interpret the
output of as a probability of an email
being spam if you fancy making some
money you can do some stock market
prediction and good luck with that and
even even human learning can be thought
of this way
I mean if as long as you think of the
input and output as being a thought of
being a state of mind and so the machine
learning problem basically boils down to
taking experience in the form of
empirical data and running third an
algorithm that automatically generates a
function that embodies this knowledge so
a natural question is you know are we
any good at this well I think it's quite
useful to compare the or good analogy is
to compare machine learning to the early
years of the early years of flight we're
really only just getting started at this
and there's lots of crazy ideas lots of
things being tried out and we've had
some successes we've managed to get some
of these things to work but it's still
very early days and a lot of the
solutions are very much hand made you
see things like ibm's watson is really
really good at the specific task of
playing jeopardy it's no good at all at
driving your car or any or making you
coffee or anything like that
but it's still a very exciting field and
you know there's a huge amount of value
here if you can get machines to learn
how to do something useful in real
applications
naturally I thought this is an exciting
time and a good time to set up a set up
new startup in this space at new okay
we're building a new approach to machine
learning a new machine learning toolkit
the design is that it's going to be
general-purpose it's going to work on
any kind of data you know images sound
numbers text you name it the power is in
the the algorithms the algorithms that
you can learn to recognize deep patterns
and draw useful influences from that
data and of course we're doing all the
usual Big Data stuff making it you know
scalable real-time etc etc and as I'm
here obviously we're using closure to do
this and I thought it's worth reflecting
very quickly on why why closures turned
out to be a good choice you know first
of all it's actually a pleasure to use
and I think this is this shouldn't be
underestimated when you're doing a
start-up and you're going to be working
long nights with technology
I know people some people like to bash
Java occasionally but the JVM is a huge
advantage I mean it's got excellent
engineering the access the Java
libraries is very very important in this
space and also the ability to deploy and
integrate into real world applications
is very useful if you actually want to
get things done and build systems using
machine learning interactivity I am this
is particularly important in machine
learning cos in machine learning there's
a lot of trial and error it's really
helpful to have a weapon Wepa driven
toolkit so you can try things out see
how it's working and iteratively refine
your models so you actually get good
results functional program I think is
useful anywhere you're doing a lot of a
lot of data manipulation and closures
been great from the perspective of
building or building a DSL for machine
learning with with composable
abstractions and I think that last point
is very very important we're trying to
build a generic toolkit here and if
that's going to be useful and productive
it has to be possible to quickly plug
together machine learning components
that you need to solve a specific
problem so with that said I'd just like
to introduce a few of these abstractions
first of all is the humble vector the
vector is basically array of double
values and this is what we're going
used to represent inputs and outputs to
our algorithm now in the real world your
data is not going to come nicely pre
formatted in vectors so you need a coder
this is something which will convert
data to a vector and back again as
needed you also need to describe the
problem that you're trying to solve this
is the task and that encapsulates all of
the training data that you need to use
so what is the function that you're
trying to get this machine to learn
using some set of training data the
module is what represents the function
which is being learnt and typically that
would be a neural network although you
have the option to plug in other kinds
of different modules as well and finally
there's a that there's an algorithm the
algorithm is what actually makes the
learning work it's what actually builds
the function to solve that particular
problem so in the demonstration I'll be
I'll be showing all of these different
abstractions but before I do that I
thought it'd be worth very very quickly
covering a little bit about neural
networks and for those who haven't seen
them before
so neural network is a structure that
computes a set of outputs from a set of
inputs and it's constructed from a
number of nodes and weighted connections
between those nodes so when the
calculation happens it's performing a
weighted sum which is calculating the
values of each of these nodes and these
values are then flowing through the
network it's typically arranged in a
number of layers so you have an input
layer at the bottom an output layer at
the top and anything in between we would
call it a hidden layer and it's very
important to note that it's the weights
which are doing the learning the
structure doesn't necessarily change but
you adjust the weight so that it
performs the function that you're trying
to learn and neural networks are often a
good choice for machine learning two
reasons one we actually know some pretty
good algorithms for training them and
the algorithms are getting better all
the time and secondly there's a useful
fact that if you make a neural network
large enough it's actually capable of
approximating any kind of any kind of
function so they work as a universal
function of
tomater which is a useful property to
have so how do we train these things you
start off initializing them with some
random weights then you then choose a
random training example as input from
your training data you run that through
the network you compute the output of
the network see what it produces you
then determine the error so you compare
what the network produced with what you
would have liked it to have produced
what was the expected output you'd want
to see and then you adjust the weights
and you adjust the weights very very
slightly in whatever direction reduces
the error and then you're going to go
back and do the same again with a
different training example and if you do
this lots and lots of times each time
you're reducing the error very slightly
what you end up with is a network at the
end which has a low overall error so
it's producing the expected output of
the function as as closely as possible
over some period of over some period of
training so that's the basic algorithm
for training neural networks so let's
show this in action I'll start with a
very very simple example of how you can
do this
this is Scrabble this is if this is a
this is a great game on but I'm very
fond of playing this with my family and
one of the distinctive features of this
game is that each character it's a word
game each character is associated the
numerical score so what we're going to
do is really really simple we're going
to teach a neural network what the white
score is for each character so let me
just switch over into demo mode okay so
we're going to start off by defining the
actual the actual data that we're going
to use for training so this is the
scores for each character and this is
just a simple sorted map you can see a
is worth one z is worth ten and all the
other all the other characters have
their own scores that defined that now
we want to find a way to encode the
scores into double vectors lots of ways
of doing this but a fairly obvious way
of doing it
just have a simple binary encoding so
I'm going to find a score coder as being
an integer coder with four bits so 4-bit
binary number and if I then try that out
with the number three I get a result
here which is a a vector which is 0 0 1
1 which is the binary encoding for 3 I
can then decode using the same coder and
I get back to 3 again so that's the
function of the coder I'm going to do
the same thing for letters here we've
got 26 possible classifications so we've
got 26 values so I'm just going to use
the keys from our scores map to do that
and if i encode the letter C for example
I get a vector which is 26 elements long
and you can see that the third element
only is set to warn which represents C
is the third third letter of the
alphabet and the task that we're trying
to learn here is just a straight mapping
task it's mapping characters to scores
so I'm going to simply use the scores
map and tell it that we want to use
those two code as we've just defined
also going to define a new your network
we can have 26 inputs for the 26
characters and four outputs for the
binary score and we can have one hidden
layer with a six units in it and notice
quite extensive use of closure keyword
arguments here to be able to configure
the different the different units so
it's quite nice to be able to see what's
going on here
so I've created a small visualization
routine which will show a neural network
here it is it's got 26 inputs at the
bottom four outputs at the top and six
six units in the middle in a hidden
layer and the lines between of the
weights so the green lines represent
positive weights and the red lines
represent negative weights they're all
random at the moment so
the function we're actually trying to
produce here is going to take a letter
it's going to code it with it the letter
coder it's going to run it through the
neural network then it's going to decode
it with us our score coder to get the
answer and if we just try that running
that function with the letter A at the
moment I get I get 12 now 12 is
completely wrong it shouldn't be 12 it
should be 1 but that's because we
haven't trained the neural network yet
it's just coming out with a random
answer so let's define what success
looks like if we're going to evaluate
this network what we're going to do is
we're going to just going to count the
number of times when the output of the
network so the Scrabble score that we
get produces is equal to the actual
score from the map so that's our
evaluation function and again let's do
some visualization let's put that on a
time chart so what I have here is a
continuously updating encounter chart
and can - as a great library by the way
which is just saying how well this
network is doing and currently it seems
to be getting 2 of the scores right and
that's completely by chance you know the
network happens to be producing the
right answer for 2 for 2 letters so
let's do some training we can use a
standard back propagation algorithm and
we're just going to run that on the
network first for a short time and see
what it does so watch what happens to
the evaluation and also watch what
happens to the to the network
nice okay so what you saw there is the
the score has just gone up from two to
twenty-six so it's now getting every
single digit right it's it solved this
problem and also the colors changed on
the on the neural network because the
weights got adjusted to learn this
problem now I actually had to slow that
down if I'd run that at normal speed it
would have finished instantly I just had
to sleep in between each each situation
so you could see see the improvement
happen but that's how learning works and
if I want to just test it out let me
just run a letter Q through it you can
see here that that's letter Q as an
input and it's produced one zero one
zero as an output which is which is 10
in binary now people often criticize in
your networks you can't really see what
they're doing in this case in fact
sometimes you can yeah so you can see
that the Q here has a link to this node
with a positive link and then it has a
link or into this node of the positive
link so to some extent that node in the
center is probably acting a bit as a
feature detector is detecting detecting
that Q and saying that the high bit
should be set for Q so you can do some
interpretation of what the network's
doing that way okay so that's that's the
first simple example of neural network
learning
um so that was have a pretty easy
example so I'm going to let's try
something a little bit harder this is
handwritten digit recognition and this
is a pretty pretty badly written too and
this is a much much harder problem this
has got the type of issues you see in
real world data first of all it's larger
this is a 28 by 28 image
so there's actually 784 pixels so 784
input dimensions and it's not just
discrete values and we have some
intermediate grayscale values there as
well and we also have noise you have
some things like random pixels
distortions in the data which make it
much harder to actually learn the
patterns so how we're going to approach
this
well let's one thing we can deal with is
the number of dot input dimensions so to
deal with the fact we have 784 inputs
let's do some compression and this is a
really really nice trick with neural
networks what we're going to do is we're
going to build a network and train it
with the identity function so you're
going to train it to produce exactly the
same output that it was given as input
now this may sound a little bit stupid I
mean we can obviously void your identity
function very easily um but the
cleverness is how we've constructed this
network if we successfully train this
network with a 784 inputs a hundred
hundred and fifty inputs in the middle
and 784 outputs and it's learnt the
identity function then what must have
happened is that all the information
that was required to produce the output
must have gone through those that
central layer so what we've done is
we've encoded in 150 hidden feature
units we've actually encoded all of the
information required oh that exists
within that image so if you go and take
the bottom half of this network what
we've got is a compressor it's taking a
large dimension down to a small
dimension and then of course what we can
do is we take that compressor and we
build the rest of our network on top of
it so we're going to add some more
layers on top to do the actual
recognition
now because we've compressed down from
784 to 150 in the first layer this makes
the whole network smaller they're easier
to manage faster to train etc and of
course this is just function composition
we've just composed together two
functions functions happen to be new or
networks but this is this the function
composition we know and love so let's
give this a quick try okay so we start
off just getting some some of our data
we actually have 60,000 training
examples here which we're going to use
and again it's quite useful to be able
to visualize these things so I'm just
going to define an an image creation
function and I'm just going to map that
over the first 100 data items and again
this is this is the advantage of having
a dynamic rapid environment you can just
do these quick visualizations so that's
the that's the first hundred digits you
can see they're all handwritten quite a
bit of noise in them that's what we're
going to train this thing to recognize
we also have the labels and the labels
are the the correct digits that we are
expecting to recognize so again we've
got 60,000 of them take the first ten
that should be the same as the first the
first line of those those digits up
there so let's do a little compression
trick the compression task is just an
identity function
I'll now define the compressor which is
going to have 784 inputs and 150 outputs
let's also have a D compressor which is
going to be 150 inputs going to 784
outputs and the we constructor is just
the combination of those two so again
this is function composition the connect
function is analogous to to compose but
for neural networks and let's see how it
will see what happens with this so
define a function which is going to show
our weak instructions and try it out
and we get a lot of random noise and
again that's what we expect we haven't
actually trained this this network yet
it's just producing whatever whatever
the random weights are producing so
let's do some training let's have the
back propagation algorithm and I'm just
going to run this for a short while and
and I've set it up so that as it runs
it's going to update the weak
instructions now and again so that we
see how it's doing see how it does so
something's happening here okay that's
interesting it's starting to look a
little bit like some numbers there okay
that's getting let's get starting to get
reasonable you can now probably start to
make out that those are actually the
same input data that was at the top it's
not going to be exact an exact replica
we're actually learning lossy
compression here but that doesn't really
matter if the machine learning problem
as long as we've got enough information
to capture the features in the data that
are going to be able to us help us where
can I see the image okay that's probably
that's probably good enough I'll stop
that there so one thing we can do that's
quite nice is we can have a look at
those 150 feature detectors and middle
layer we can actually see what they've
learned to detect and this is this is
quite a quite a pretty trick so what I'm
going to do is I'm just going to show
some images that what they've become
sensitive to so this picture here shows
for each of the each of the 150 feature
layers each of 150 units in the in the
in the hidden feature layer what that
unit has become sensitive to so again
the green is a positive relationship and
the red is a negative relationship and
you can probably see that some of these
have sort of got some features so this
one here for example is looking like
it's become a one detector to some
extent and other ones you can see sort
of a detecting a combination of
different features mixed together so the
kind of little strokes and
features you'd expect to see in digits
and that's a good sign because you want
these feature detectors all to be
detecting different different
characteristics of the input input data
so that looks that looks pretty good so
now that essentially try to do some
recognition with this let's again we're
going to need a coder to say the numeric
values that we're trying to predict so
we've got ten different possible values
just using range ten and try that out if
i encode the number three i get a vector
it's ten telling the months-long it's
got a one in the in the in the index
three which represents that value that
looks good and our recognition task here
again it's a mapping task we're trying
to map you're checking something yep
again we're yeah again we're trying to
just map an image through to a single
classification what the output value is
the recognizer itself where we're going
to take 150 features we've learnt to
detect through our compression algorithm
and we're going to take that to the ten
outputs and then the overall recognition
network is going to be just connecting
our compressor to our to our recognizer
again we can just use the the back
propagation algorithm for training this
the final thing we need to do and this
is very important in machine learning
when you when you have real problems is
to also have some test data and the
reason you have this is because you want
it you want to test whether you're
you've actually learned to generalize so
that do your algorithm acts your learned
function actually works on previously
unseen data so I've got 10,000 test
cases just so we can just so we can test
how this is working and the testing task
is basically the same as the same as the
task we're using for training just with
unseen data again let's see how this is
performing here we're going to plot the
error rate so we're going to see what
percentage it's getting right and oh
sorry what percentage of errors it's
making its making it nearly nearly 100%
ours it's with not getting anything byte
which is actually even worse than you'd
expect to do by chance
but let's see how this works let's see
let's let's see if we can actually learn
to recognize these these digits though
the red line is the training data and
the blue line is the test data so I'm
talking to two charts at once okay okay
well that's looking good they're going
down and the other nice thing about this
is that they're going down mostly
together and this is a good sign this
tells us that we're we're actually
generalizing this this neural network
with the with the blue line is working
on date on digits that it's never seen
before so they're not being used as part
of the training data set and they're
getting better and better and we're now
getting about Wampus about 10% error and
if if you go and run this for for long
enough it would probably get down to
about three or four three four five
percent error I haven't got time to do
that today whether that's looking pretty
good
so I'll just stop that training there
and let's see let's see the outputs we
get so we'll just define a recognizer
function this is just going to take the
image data it's going to run it through
our recognition network and put it
through a number coder 2d to decode the
output so the first the first data item
I think that was a that was a five yes
five in the top left and see what it
gets it gets it the way that's one of
the ones it's getting wrong but that's
not a very well written five it sort of
looks a bit like a three but if I map
listen let's just map that over the
first 100 digits
there you are that's the actual outputs
from the network compared to the where
the inputs as you can probably see it's
getting it's getting most of them right
it's getting about 90 percent success
rate on image recognition which isn't
state-of-the-art but you know it's not
bad for like a quick quick five minute
exercise okay well that is that's
actually the end of my demonstration
material I thought of the I hope you
found it interesting I hope you've also
it's been quite a good demonstration of
what you can do with with closures as a
toolkit for machine learning I want to
make sure left a bit of time at the end
for any questions or any discussions or
any ideas so thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>