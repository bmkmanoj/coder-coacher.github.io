<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Moving People with Clojure - Abhinav Sarkar &amp; Kartik Gupta | Coder Coacher - Coaching Coders</title><meta content="Moving People with Clojure - Abhinav Sarkar &amp; Kartik Gupta - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Moving People with Clojure - Abhinav Sarkar &amp; Kartik Gupta</b></h2><h5 class="post__date">2017-08-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3y7xzH8jB8A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning and welcome to our talk
moving people with closures yeah hi I am
Karthik of the I work at COGIC as a
product engineer and I'm a Binoche I
work at nil and so software which is a
software consultancy and I'm currently
consulting for Gajic so this is an
experience report on how we rewrote a
very integral service at go check into
closure so this is a sort of a quick
talk because we have a lot of ground to
cover so we'll take the questions in the
end and you can of course see us later
for more details so many of you will not
be familiar what Gajic is Gajic is an
Indonesian start-up for transportation
logistics and payments it was co-founded
it was founded in 2010 and it is our
only unicorn in Indonesia it operates
over 25 cities with over 300,000 drivers
including both bike drivers and car
drivers and we have more than 25
products and we process over a thousand
dollars per sec the interesting thing
here is that we use the same fleet for
all of these 25 products you can see
some of them on the screen there is a
ride car food courier and ma you know
groceries and such yeah this is how our
go job Gajic customer looks like so how
all it got started so in 2000 till 2015
gojek used to take orders through phones
so customers used to call customer care
to book and boutique and in 2015 January
2015 our apps our Play Store app
launched so at that time we had around
800 drivers so by by August 115 our
business grew 5x and we had around
30,000 drivers 30,000 drivers and then
we started to pull allocation related
code from Java Manolos
into golang service so as you can see on
the slide at the beginning we wrote a
single service which was in Java which
did everything but as we started new
verticals like courier and food and
search we decided to move to a micro
service based architecture and we sort
of split out these things and we also
pulled out the allocation the driver
allocation related code into a go
service this was August 2015 and we did
this because well the promised
performance and concurrency support of
golang so this code now as business grew
we hired more drivers that 200,000
drivers in sep 2015 9 ice on T to
experiment with bookings with new
allocation strategies and so right
writing domain logic coding go was tough
for became difficult and so in August
2016 we decided to rewrite our
allocation service app into a closure
service and by October 2016
we finished the rewrote that as in
feature parity with our with our old
service and we spent a month on testing
and releasing it and by November 2016
it wasn't prod yeah so why so the
rewrite itself this talk is essentially
a story of the rewrite so Before we jump
into the rewrite this is what allocation
looks like driver allocations right so
many of you may not be familiar with
this domain so this is a very simplified
architecture you have customers on one
side asking for well different things
food courier whatever and these orders
go to the order management systems right
dot management systems take care of the
particulars of the booking whether its
food career bike ride or car ride but
they call the allocation service for
actually finding a driver for the
particular booking right so the
allocation service
contacts the possible drivers whoever is
near that area or depending on some
criteria and if the driver accepts then
the flow goes in the reverse order the
callbacks happened the driver app will
call back the allocation service saying
hey I have accepted this off order the
allocation service will call back the
particular OEMs and the OMS will finally
call back the consumer app saying hey
you've got a driver now right so this is
basically how the allocation looks like
in the old as well as the new service
the essential architecture has not
changed so why did we decide to do this
rewrite at all the golang service
actually ran fine for a whole year right
it did not we did not face any major
bugs or issues with it it was quite
performant but as the code base grew we
found that it became very difficult to
add new features go if you are familiar
with it is it prefers a mutation heavy
and imperative codebase essentially and
the this app was not a simple app which
basically says this data and database
ensures an API over it this was a lot of
domain logic there was lot of there were
a lot of rules about how to choose
drivers and such so the domain code was
very hard to reason about admitted I
admit that as the law issue there were a
lot of issues on our side also we
actually did not know Galang that well
at that point but soon we realized that
adding new features and being able to
tweak the strategies is more important
than high performance which go promised
so we decided to rewrite this into
closure about a year ago so we found
that the features the closure provides
immutability laziness and dynamic typing
leads to a much simpler domain-specific
code right so I will show you a sample
slide also show your sample code this is
how it looked like when you have to
choose the drive
of our particular booking you can see
this go code it's huge it has a lot of
if continue break returns nested control
flow and strange
well domain logic mixed in right but
after we did the rewrite this is
essentially how the closure code looks
much simpler right you just add this
code is way more extensible than the
before because you just add new filters
in between however you want filters are
lazy and they are easily composable
whereas the go code required you to like
it was very mutation heavy and you
changed in local variables and put
things in arrays and stuff so it was
difficult to maintain this close is in
fact so easy that now when you know the
business folks ask ask us how does
allocation work we just show them this
snippet and they understand right so
it's a well we found that when golang
was essentially hard to use for writing
complicated domain logic if you want to
focus on the specifics of this code we
get a bunch of drivers from some other
service and then basically we filter out
drivers depending on some different
criterias by balance by rating and
etcetera and then finally we get another
bunch of drivers to which we send the
the welder booking so this rewrite was
done in two months with a pair of people
working on it we achieved feature parity
and after that we decided to test the
rewrite very rigorously because this was
at the very core of the go check
business all the order management
services talked to this service so how
do we test the rewrite we spent two
weeks in testing the testing was
adjusting methodology was a bit unusual
instead of using we of course did a lot
of unit testing and search but instead
of using a like a load testing or such
we decided to test
on live traffic so this is how our
testing our infrastructure looks like so
you had the apps the driver apps and the
consumer apps calling the same or manual
systems the odd man meant system will
send the live traffic to the go
allocation system but we had a tool
called go replay it's actually the end
go so it tasks into the network and
basically sends your duplicate traffic
to another server right so we send the
duplicate traffic to the closure
allocation service and in need inner so
we needed to have the closure allocation
service no external effects it needed to
be isolated because it was in testing it
was not a production ready system yet so
we used feature toggles to toggle off
some of the downstream services to shut
down the real world effect and for other
services which we needed to have running
for the service to work
he created mock services which
implemented the same API as the actual
service and but did not have any
external effect so and some services
like cache and such we had to share with
the actual allocation the older
allocation service so that the data is
correct so we ran in this mode for about
two weeks
we found bugs we fix them the captured
all the data the dating and the
transactional data in the database and
the external world effects in elsewhere
in the mock services box office logs and
then we matched them we matched the
golang allocation data and the closure
allocation data and we matched it till
we found a perfect match so this went on
for two weeks with full traffic after
that we decided to release the rewrite
slowly incrementally first of all we'd
use we use a a ot compiled to butcher as
a single deliverables right I'm not sure
how many of you use uber jars as
deliverables in production but we
decided to go with this we compile it
because we realized that it saves us
almost ten to
15 seconds of startup time if the
comparison is a bit slower but the
startup faster we added lots of
monitoring's monitoring around the
service we are we added monitoring for
application metrics we added monitoring
for domain-specific business metrics we
added error monitoring and then we
slowly rolled it out over two weeks so
this is how the role rollout look like
so we changed a bit of code in the OMS
s-21 one particular AMS to send 1% of
live traffic to this new service and
rest the traffic to the old service and
these services shared the external
downstream dependencies completely so we
ran in this mode for few days then we
increase the traffic then we really
slowly switched over 100 ms over
completely and then over a week we
switched over all the odd management
systems and at the end of the week we
had whole of the go check running on
this closure service right so this was
actually a fantastic way to this is
actually a good way to get closure into
your company if you want so you you know
match run it in like a side-by-side
match a data and then do a gradual
rollout right so we did not face any
major bugs and the DNI's was very smooth
so we learned some lessons and I'll
share some with you ok so we use
generative testing so as this is the
core of codecs business we wanted to
test it very rigorously so as you can
see we have unit tests we have testing
at all level unit functional and
integration and we have around 100,000
assertions in just 150 tests we use test
ik library and we wrote our own
generators I will show you one booking
generator
so this is our booking generator with
some fields and native generators like
we wrote this alternator like paas
integer likely to longitude Jason URL
so so this is one sample test which
takes four great which this is a API
test which takes for the booking into
the DB like it creates the booking into
the DB and checks if it is same or not
so you can see that number over there it
says num tests 100 that basically means
I'd run this test hundred times right so
while is developing in initial failure
phase of development we caught a lot of
bugs because of like generators testing
and so we fix those work and this
release this led us to do a faster
release cycles now as generator produces
random data now there should be
something to validate that data so we
use Prisma so we used prismatic schema
to validate our data at all API levels
and config levels so this is how a
booking schema looks like pretty similar
to the general corresponding generator
in fact schema provides you a way to
generate data from schemas itself but we
ended up not using it because it was too
slow and if you are wondering why we use
schema instead of spec at that time spec
did not have generation caps
capabilities and schema provides us
coercion which spec doesn't are as far
as I know so we have swagger integrated
with our a with our app and it uses a
scheme schema as this app is the schema
so it is easy for others team to
integrate with us now if we if you are
talking about in external integrations
now what if our external dependencies
gives the errors out or give the failure
so to defend our service we use the
circuit breaker so we wrap external
external call into a circuit breaker
which makes a call and monitors for
failure when event failure teaches some
certain threshold it tricks the circuit
and after after some time its take is
the downstream services up or not
so yeah the diagram explains how the
circuit breaker works we wrap all our
external calls because this is a central
service which calls a lot of downstream
dependencies into the circuit breakers
if something goes down it does not
affect our service the next and the
circuit breaker also supports fall backs
in case fallback hooks in case things
are wrong so we so we use net flix
sistex as a circuit breaker so Netflix
is sistex has a closure library also and
we wrote our own own macro around this
library which is pretty easy to
integrate so as you can see from the
code snippet we are getting a driver
rating from an external service and
specifying or command key group key pool
key etc yeah so mystics also provides
you a dashboard to monitor your service
in a real-time so we can see how it
actually works so the service is normal
now it's editing out and history is
monitoring it right now the errors are
increasing and finally it crosses a
threshold you can see the circuit is
open now and all the calls are
short-circuited right after a while the
external service recovers and the
circuit and the circuit closes again and
sets to basic normal right you can see
it again
so it's editing out right now and then
it recovers right so hystrix as Han
gives you fallback hooks in case your
downstream services are not working
properly sometimes you can return a
default value sometimes you can retry
the call after a while it's up to you
and it also provides you the like all
these numbers are configurable how much
what is the rate at which you should
error out and what is the rate at which
what is the time you should we try a
which to check if the wrong stream has
recovered right so this hystrix was in a
very major part in our service in fact
our dashboard looks much much bigger
than this we have around 20 odd graphs
over there so it was very essential for
us to keep our service running all the
time right the service cannot go down
because if it goes down everything stops
right the another thing we needed to
have to have the service pull up like
100% of time is to be able to change all
those configuration parameters while the
service is up we can't afford redeploys
restarts nothing right so for that we
use a TC d right so etcd
is distributed key-value store and it
provides you a way to watch conflicts
right watch conflicts for changes so the
way it works is that we have this bunch
of nodes watching the HCD data the data
base all the time and then a conflict
change request comes in from the load
balancer it goes to one of the nodes the
node notifies or changes the config in
the etcd database and then the database
gives you callbacks all the nodes right
so the nodes keeper in-memory atom of
the config and when they get callbacks
they update that conflict right so with
this we can be we were able to have a no
change like hot deploy or hot reload of
conflicts without any downtime and the
swagger integration that we talked about
earlier it provides you a nice UI in
which you can go and change things so we
provided this API index weather UI and
the business people with themselves able
to change all these conflicts as they
wanted the conflicts if you are
wondering are things like you know how
much how much Park and how far should I
look for when searching for a driver how
much time should I wait etc these are
very business
and domain-specific concepts so there is
no need to have them in config file and
such we have a backer on the other
another part of config which stays in
config files which is basically thread
pool sizes and such which business
people do not care about so this enabled
us to do very quick changes it actually
takes less than a second to increase
your search radius from one kilometer to
two kilometers right and there is no
need to do any deploys or restarts so we
used a bunch of other nasty things we
found them while working on this project
I am going to share a few of them with
you I am not sure if you're familiar
with this library mount so every
long-running service it ends up having a
lot of state right you have connection
pools you have con figs you have
external service clients you have
schedulers and a lot of other things
which are stateful and must have a
manage to life cycle you need to start
and stop them and etcetera so this one
for example shows you a shed Euler right
so there is a start function this is the
stop function this is just a wrapper
over Java and the Java is built-in
scheduler so the key thing here is def
state there that's where you define a
state you say this is my state this is
how you started this is how you stop it
right and the sheduled function at the
end is nothing but just a wrapper over
the Java function right so why did we
use mount instead of say component this
is the key part in component you would
have to use explicit dependency graph
you have to configure and tell which
service depends which part of which
component depends on which component and
such in mount the dependency graph is
implicit mount scans your namespaces and
checks the imports right so for if you
can see an example here the archive and
the archives namespace is the one which
runs shedule archive and it does nothing
but just imports the previous namespace
the scheduler namespace
and the dependencies are configured
automatically right Samant will make
sure that the scheduler is started
before the archival function starts
right and you have other similar states
like the server the HTTP server itself
so this is how the final main file looks
like a course core file so at the end
you import all the dependencies all the
components that you want to run and you
just say monde slash start and montt
will start them in correct order while
taking care of you know the proper
dependencies and to stop you just add a
shutdown hook saying when the server
shuts down you same on slash stop and it
shuts down the dependencies in reverse
order also correctly right so we have
actually some time left we did not know
we'll be able to finish if we were to
cover so much I'll show you another
nifty thing we used so in when you're
writing very domain heavy code a lot of
times you need to return an error right
and either a sort of a success object or
error message so one way to do that
would be to use Java exceptions you
throw an exception in case something
like you want to signal an error other
way to do would be return like
particular specific keywords saying hey
failure this reason and such we decided
to go this other way I'm not sure if you
are familiar with this but either monad
is sort of something which comes from
Haskell it has two parts one is a
failure which is masked message failure
message and a success with a value so
the key part here is this function right
bind it takes a result which can be
either a success or a failure right and
if it's a failure it calls the failure
function if it's a success cause the
success function so you can use this
small snippet of code and you have this
nice looking very DSL like you know way
to bind errors and
successes very cleanly so process
booking takes a booking and it relies a
return your success or failure and then
bind will call the post process booking
if it's a success with the result object
otherwise we're in this case it defaults
to not using a failure function and just
chains up the failure otherwise you can
also specify a failure function right so
we use this pretty heavily in our code
base and we found this to be very
expressive you can also change multiple
binds one after another like that so
that is all we had as a major point
which we learned while doing this
rewrite if you want to get in details
you can catch up with us we'll be around
right so what happened afterwards
so this release went out in November
it's made of more than six months and it
was almost a game changer for Vortech we
were able to push features much much
faster we almost did like one new
release per week because of generative
testing and Netflix and all such things
we put her on monitoring and such we had
much better trust in our code base right
and yeah we moved much faster than after
that so we were able to complete the
experimentation features that business
wanted to be able to do a/b testing and
such inside the code base one issue we
found with this rewrite is that getting
people up to speed on closure
it was difficult in the beginning right
because most of our developers come from
Java and go and such and they find it a
little difficult to switch to the
functional mindset to write code as like
pipelines and such so we conducted a lot
of closure sessions we studied joy of
closure in those sessions and slowly
people got up to speed now we have over
fifteen or so Dells working on closed
projects we did more rewrites enclosure
we wrote more services enclosure and now
closure is one of the main languages
used in admin go check right so yeah
that's all we have
then find us these addresses if you have
any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>