<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Logs as Data - Mark McGranaghan | Coder Coacher - Coaching Coders</title><meta content="Logs as Data - Mark McGranaghan - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Logs as Data - Mark McGranaghan</b></h2><h5 class="post__date">2013-01-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rpmc-wHFUBs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our next speaker mark McGranahan is
someone I wasn't quite sure how but I
didn't know what else to say so I just
tried to say anything to the Internet
and I would read by Margaret Ranahan is
a director in the power delivery and
utilization sector research priorities
include developing these standards and
approaches for implementing the
intelligent infrastructure to support
automation higher efficiency improved
reliability and integration of
distributed resources and so I think
that's the perfect bio for Mark and he
will talk about logs as data
like us about Boggs I start by pointing
out right so everything is data it's a
pervasive concept with enclosure of
course there is code is data which is
one of the fundamental principles of the
language and one that lends the language
a lot of its power I've talked before
about HTTP requests as beta that's one
of the ideas that underlies the ring
library we have libraries that treat
HTML markup as data there is progress
within the libraries and within closure
itself to treat exceptions as data we
can manipulate SQL queries as data
within closure our CLI option parsing
tools return data decision trees compile
to and from data we even have our build
specs described as data and whole cloud
deploys represented as data so this idea
of treating everything as data is
certainly pervasive and well presented
but I'm going to look specifically at
treating logs as data and the obvious
question to ask then is why logs in
particular the reason is that we know
from experience now that the Delta
between the power we have from how we
handle logs now and how we might handle
logs is huge it's a similar type of
power gap that you see being closed by
the closure language around the core
features that that closure implements
but it's also an idea that's more
general than closure itself it's a sort
of fundamental technical aesthetic and
that fundamental idea is what we're
going to be working with here with logs
is data and trying to write down some of
the things that what we mean by treating
treating data in a fundamental way I
can't with these these few points I
don't necessarily intend to that for
them to be complete or foyer orthogonal
or anything but I think there are good
series of litmus tests for for treating
data as data
and II if we work through how these play
out within closure itself I think we'll
see why these are important the first
one is just having data as the
fundamental unit in closure we're very
rigorous about this we treat all of our
data in its most essential way it often
means things like maps and seeks and
functions and in particular we avoid
hiding data in micro local interfaces
that that kind of hiding of the type
that you get for example with Java
classes present prevents us from
manipulating that data and accessing it
through simple extractions so because
all of our data and closure goes through
those simple abstractions and is
fundamental we have just a few core
primitives that we can use to manipulate
all kinds of data there some of the most
core ones here map sequences and invoke
ability and indeed one of the important
force order effects of this data
discipline is that we can have very
general libraries just for example we
might have a program concerned with
foods that can leverage closures core
library even though closure has no
knowledge of foods of course and also
this point is a little bit more subtle
this data discipline allows third
parties to participate in that
generality in a meaningful way so
because there's the small number of
abstractions that everyone is
participating in a third party say
function can come in and be applied to
another third party that doesn't
necessarily know about beforehand all
that is predicated on these these
fundamentals of treating data as data so
what what's the deal with logs right now
what's the current kind of
state-of-the-art let's go through these
three points and see how we do for
logging well right off the bat we're in
trouble
because we're hiding our data in its own
local interface in particular we have a
here's an example of a log line that
talks about a user viewing a specific
page for an application already we've
create our own syntax for how to
represent that data and also implicitly
serialize it into a string so not doing
so well they're simple abstractions I
think a quick google of java logging
will lead you to believe this is
definitely not the case and this is
somewhat comic the xml here but it
actually makes the point of how much we
manage to conflate around logging at the
same time we have selecting which data
is important how to format it that it's
a string that it needs to come through
the log for J interfaces and then to top
it all off we we spit it out in terms of
XML this definitely doesn't have the
elegance that we like and expect of
closure and then mostly as a consequence
of those first two problems we don't
have a general library for logging it's
not even possible to construct one
because the perimeters are wrong what we
get instead is very complete libraries
that are that missed the point of
generality because they implement all
the things general ideas independently
themselves so for example log4j to pick
on it again implements its own notion of
hierarchy it even implements its own
notion of predicates and filtering these
are our notions that aren't fundamental
to logging in anyway and there's no
reason they shouldn't be pulled in from
a more fundamental tool like closure to
the language there is a bit of a bright
light within the logging world and that
it is somewhat possible to implement
programs that that participate in an
open system in a meaningful way
if you unfortunately
omit your realize your logs the text
stream you can manipulate them with the
standard UNIX tools and these UNIX tools
aren't particularly logging of course
but they are able to compute against
those text streams in some way but again
that has a problem of all of our data
now has already hit in this text and
becomes much more difficult to work with
so not doing so great necessarily on
these three points for logging
to think about how we might improve that
I think we actually need to start with
the name itself there's a lot of
problems that are now baked into the
word logging in particular logging has
become almost synonymous with strings
logs are lines of strings and also the
word log largely means now physical
files right now on disk
both of these are problematic because
they hide the essence of what logging is
which if you think about it is a record
of what's going on in your application I
think a better model for that is events
and indeed at its essence logs are a
stream of events corresponding to
everything that is happening within your
application that's data that might be
useful for a variety of reasons
especially when you consider that as you
introduce a notion of time it becomes a
record of everything that ever happened
within your application that's a lot of
potentially interesting data and indeed
that's going to be the source of our
moving up towards them the more the more
powerful model for processing logs and
events and again it's predicated on that
fundamental of data discipline around
logs so let's look at those three points
again with a different model focused on
events data generality and openness
looking at our original log line example
here there is some fundamental data here
we have a level which is info we have an
action which is that it's a viewing we
have a couple entities the user which is
has an ID 24 and a path which has home
we can just write that out as a map and
indeed this is logically what this event
corresponds to so if we strip away all
the accidental baggage that comes along
with logging traditionally we get the
essence which is an event described as a
map and this leads nicely into our
ability to manipulate logs vias
tractions we already have the math
itself which is convenient and now
omitting them is just a matter of
function invocation indeed it's more
convenient I think to describe this as a
mitt instead of log since we're moving
away from that notion of physical
strings this becomes the general logging
API and indeed the entirety of a logging
framework can be described in terms of
these existing closure interfaces and
once you have your logs in that in that
model you have this events you have the
events in a data oriented model then you
can apply all of the existing tools that
we have around map and sequence
processing and function invocation for
example this is a this is the case
that's structurally similar to one we
saw earlier except that now we've
substituted food for events and use
slightly different stream operators but
the idea is the same we're able to use
very general tools against our very
general logging data and again thinking
about the abstract interface it might be
something like this you have an event
stream and you apply various stream
operators against those events to derive
results so in that model we might do
better with what these fundamentals of
baya discipline and in particular we saw
thinking of events not as strings but as
maps and logs not as files but as
streams of events that's what's going to
give us this Delta and in particular if
you look at the very highest level I
think the key is to move away from a
model of log crunching and strings and
files and proxies and towards event
processing a more abstract model that
can be mapped down into different
physical implementations in terms of
what this looks like graphically this is
the model that we tend to see with
larger systems that use event based
logging you have various processes here
denoted by E which means
emitter emitting events corresponding to
this application or service those are
all collected together through some
single interface or physical aggregator
and then that stream is tapped by
different consumers who want to do
different things with the events to
provide different services in terms of
visibility so if you think of it in
terms of code again many things that we
want to do with our application the data
is generating can be described as a
function of some event stream and you
might have different functions here
depending on what type of visibility
you're looking to achieve into your
application there are a lot of
operations and types of visibility that
have traditionally had different paths
within applications that when you have
this very general event model can all be
folded into the same system these are
all things that we use that we
accomplish internally at Heroku where I
work via event processing and each they
all consume the same event stream but
each one does a different type of
processing over that stream to achieve a
different visibility result I think the
only really compelling way to
demonstrate that is to actually give an
example and this is indeed the example
program that kind of motivated the idea
of logs is data and event processing so
pulse is Heroku x' real-time metrics
service again it's based totally on the
event stream from our platform and it's
used to generate the metrics that we use
to operate in real-time so I'll switch
to a quick demo here
so if you look carefully you should be
able to see things moving across the
screen these values are all updated
about once a second and they correspond
to different metrics within our platform
and there are things at the top like
HTTP requests in various response types
we have distributed Process Management
sistex our packaging infrastructure and
how it's performing and various internal
messaging and queuing metrics that's
really not the necessarily the key here
the key here is that this entire display
is generated as a function of Roku's
event stream there's no special paths
into this application for emitting
metrics they're all derived from the
events that the services emit
I'd like to dive into the the
implementation here a little bit really
highlights both what it means to have
data oriented logging and also its
instincts that it is a closure
application so like we said we have all
of the events corresponding to the
system being emitted on various hosts
and are created into a single stream
that stream is then going to be tapped
by different first level receivers the
initial statistical roll-up happens
there and those are merged together in a
single merger process to produce the
final statistical results which are
emitted to the web proxies that you saw
running a second ago now will kind of
work backwards from that high level
overview drilling down into some of the
closure code that we have here so again
the idea with with the data Orient and a
data oriented logging is to be able to
manipulate those events in application
space as regular data using regular
closure and this is kind of the if you
work backwards from what you would want
to see with that it might look something
like this so here we say we have a
statistic requests per second across the
platform and that's to find what the per
second helper it just looks for events
that look like they're coming from nginx
all the events labeled with source
engine acts correspond to requests into
our system just to show a few more
examples of what this can look like and
how it's sort of a general model here we
have a statistic that corresponds to a
rolling mean over the last 60 seconds we
look for all events matching a certain
pattern in this case that process is
coming up on the platform and then we
pull out the age value of that event
which corresponds to how long the
process was alive before it clicked into
the upstate can even do things like
calculating global totals in terms of
the event stream so here we have a last
some helper which looks for all events
coming from our runtime fleet
corresponding to process counts
partitions them by the instance ID takes
the most recent one from that and then
adds up the numbers and those all
together so collectively that gives us a
view of how many processes are running
across the Heroku platform at one time
now I mention here that there's a
two-level statistical calculation that
has to happen this event stream is so
wide but it can't be handled by one
instance reliably so we have to do the
initial first level roll-up in terms of
the receiver proxies which are in blue
there and then when to do a second final
merge in the merger process which is in
red and the way we accomplish that is
again with data and functions so if we
look at our original request per second
statistic here that's being computed in
terms of the events this per second
helper is just a function that
implements a little compiler and what
that is going to emit is a very
fundamental data structure that can be
sent to all of the various processing
nodes so that purse that can help er
emits what is very loosely speaking a
streaming MapReduce recipe that's that
is able to express all of these
streaming statistical calculations that
we're doing so each one of the per
minute or mean for example helpers that
we have and has a different recipe for
building this map but this map is going
to be sent to all of our various
processing nodes and used to compute
these rolling statistics in particular
it's sent to the receivers for the
initial roll-up and then to the merger
for doing the final aggregation and then
going going down all the way to the
physical wire we have the step in the
process where pulse is collecting the
events from the system and the way this
works is there's one very thin process
which is just listening on the socket
reading off events and then putting them
on a queue and then there's a series of
workers that are taking the events off
that queue turning them into closure
data structures immediately and then
applying those events to all the various
statistics that pulse is keeping track
of
just to give you a concrete idea of what
one of those events might look like we
have a unpassed event that comes in over
the wire that looks something like that
on that density on the top there and
then that's immediately convert into a
closure map like the one you see here
and that is the map that goes all the
way through the event processing chain
and through all the various statistical
calculations that we run so it's a
high-level overview of how pulse works
we have the initial event stream it's
split out statistical calculations and
then finally a rollup into the merger
before it submitted to the clients the
point of this example is not so much the
details of pulse itself
I think it's interesting to look at how
a closure service can be implemented in
this way it's that the this closure
application knows nothing about nothing
in particular this closure application
has no coupling to the services that it
is computing statistics against indeed
you can again think of what this pulse
app is doing as being a very
sophisticated function over a pretty
complicated input it's just a global
function over this big event stream
that's coming out of the roku platform
and this raises an interesting question
for us going forward I think in this
space if you look at the the core
fundamental data structures and
abstractions that closure has things
like maps and sequences we already know
what the answer is for how we deal with
those it's closure itself events are
somewhat different because they by
definition span process boundaries it's
about collecting events from different
systems potentially operating in
different languages and different hosts
and producing some meaningful visibility
from that so a question I wonder about
is what is the kind of corresponding
killer app for events where closure is
for maps and sequences I think it's
hinted at by this fundamental
architecture diagram we were looking at
earlier we have
a bunch of processes emitting events
into a unified our Creator and then
different services potentially consuming
those streams in different ways and I
think indeed that that service
orientation is the key it's that
separation between the emitters and the
consumers so on the top there we have
one system that's responsible for
collecting all these events emitted by a
service and then on the bottom perhaps
most interestingly we have a collection
of services responsible for actually
doing useful work with them and this is
where I think closure can really shine
with this this notion of an event
processing service post is just one
example but I think the space is really
rich for closure again it's because of
that fundamental synergy around data
orientation and it's also because
frankly all of the are almost all of the
interesting libraries in the space are
already either on the JVM or enclosure
it's largely just a matter of applying
them to the right problem I don't
necessarily mean to say these are all of
them but this is certainly an
interesting subset of the type of tools
that could be applied to this problem
very fruitfully I think and finally I'll
just say that we think this space is so
promising at Heroku that were interested
in building an entire team around it so
if you're interested in closure and
event processing and visibility we
should definitely get in touch so that
was my those are my thoughts on logs as
data to back it out a little bit I just
remind us all that it's really just a
general a specific instance of the
general idea of treating everything as
data which is so pervasive in closure
and that finally it's it all comes back
to this fundamental data orientation and
discipline around good abstractions so
Thanks to everyone for listening I'd be
happy to take some questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>