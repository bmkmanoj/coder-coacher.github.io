<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Clojure+Spark to Find All the Topics on the Interwebs - Hunter Kelly | Coder Coacher - Coaching Coders</title><meta content="Using Clojure+Spark to Find All the Topics on the Interwebs - Hunter Kelly - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Clojure+Spark to Find All the Topics on the Interwebs - Hunter Kelly</b></h2><h5 class="post__date">2015-11-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ARBiyYyW4Ow" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okeydoke we're going to do a little bit
of word association to wake everybody up
after the after the break so shadows
what do you associate with this all
right anything else this perhaps well
i'm going to show you next what the
interwebs told me what it thinks Manning
so what was that results how did we get
there to explain what happens first I
need to explain what we're actually
doing so what are we actually doing
we're mining web pages which web pages
all the web pages for insights how are
we doing it we're using machine learning
to do the heavy lifting for us we're
using classifiers to filter or bucket
the data we build topic models to try
and discover concepts related to words
that's roughly what we're doing how do
we do these things well first of all we
need data so I'm going to cover briefly
going to talk about getting data
manipulating data and then using data
science to get to this fascinating
discovery of what the interwebs think of
Manning so getting under getting data we
where I'm going to touch on dmoz and the
common crawl under manipulating data I'm
going to talk about spark and sparkling
the bindings for spark and our dd's and
data frames and under data science after
a whirlwind introduction I'm going to
talk a little bit about ml lib which is
sparks machine learning library
classification random forests and lda
which is latent dear che darisha lay
allocation which is a form of topic
modeling so first of all first off
getting data how do we get data where
does it come from we use dmoz we use
dmoz and which is the Mozilla's
directory and it came out of the open
door s open directory project it claims
to be the largest human edited directory
of the web it's pretty useful when you
think about it in terms of free
crowd-sourced labeled data like all
things mozilla it's kind of a dinosaur
though and as with all crowdsource
things there's a bit of a double-edged
sword
we can't be entirely sure how accurate
the findings are but briefly ademas page
will have a topic like fashion and
fashion magazines or news or whatever
and it'll have a bunch of links things
under that topic so we use that as pre
labeled data and common crawl org is an
open directory of web Perl data that can
be accessed and analyzed by anyone for
free it's up it's hosted by Amazon the
data is freely available they do at this
point they do monthly crawls they've
been doing that for about two years now
they have readily accessible index so
you can look up sites and basically
fetch the data for those sites or pages
it's actually on a page-by-page basis
that they fetched and there's tons of
data they have the raw HTML with HTTP
headers they have a couple cooked forms
that includes links and metadata they
have just a stripped plain text quite a
great project and lots of data there if
you have ever done web crawling it's
actually really nice to have something
where you know you can reliably just get
this you're not dealing with redirects
you're not dealing with all the various
forms of HTTP goodness that's out there
in the world so how do we use these two
things together we use dmoz to sample
positive and negative seed sites so in
my case I'm looking for fashion websites
so if we end up with an example like
volcom or l-com we can look that up in
the common cross-index and we can find
all the pages that are part of voc amor
el calm and this kind of explodes our
data set and then with the information
that's in that index we can basically
just pull down from Amazon the format of
the data that we're looking for very
easily you can fetch all your data and
then very little fuss and basically you
know we started with looking for fashion
sites we scraped d bars we got like nine
hundred links or sites that we thought
okay that's maybe not very many pages
but as soon as we look them up and dee
doop them in the common crawl index we
ended up with about two and a half
million pages that's a fair few
documents and then similarly we used we
picked non-fashion examples of news and
sports and whatnot there was more than
enough again put it through the index
pumped out eight million documents no
problem so then you can balance to taste
so now that we have load of data more
than we expected how are we going to use
it we need to be able to manipulate it
in a scalable way for this we used the
apache spark project spark is arguably
the next big thing or even the current
big thing IBM are pretty much all in
with apache spark i was at a spark
conference there recently and it is the
big thing at the moment i think IBM have
3000 developers they're putting on to it
so it's a it's here to stay for a while
IBM alone have invest invested so much
in it I don't think it's going anywhere
it's quite a good project in fairness
like there's there's warts we'll go over
some of them but it's pretty good and
sparkling is the closure bindings to
spar it's one of the closure bindings to
spark there's a great presentation on
spark and sparkling and i highly
recommend you look at it he covers dr.
christian bets who wrote it covers spark
and sparkling in the introduction way
better than I can so I would definitely
give that a look at its core spark has
two main data structures or ways of
manipulating the data rd DS which stands
for resilience distributed data sets and
data frames are td's and these are very
familiar too they're actually very
familiar to closure programmers because
they they're basically look like
sequences you use map map and filter
operations map cache becomes flat map
but essentially you can think of them as
a distributed sharded sequence that you
can operate on there's you know a number
of partitions are a number of executives
that each have partitions or shards and
they operate on these sequences and you
work on them in a very familiar way
there's spark kind of differentiates
between what it calls transformations
which are things like map filter so on
and so forth and these are lazy these
are something that will be done once you
have call an operation on it which is
something like count or reduce or
whatever and then the then the kind of
chain of computations gets collected and
run again for most closure programmers
this is all pretty familiar familiar II
so to give you a brief demonstration I'm
going to give you everybody's favorite
machine learning tutorial
okay let's do a different tutorial and
this is I'm not gonna spend a lot of
time on this slide because the next
slide is very similar this is what code
looks like in closure just with the
reducer library to compute some prime
numbers or to sieve out some private
numbers to be more accurate and this is
the spark code you can see that the
shape is very similar between the two
map filter remove subtract stuff like
that so going over this one briefly and
so this up on my github there's a
project called spark Thomas prime that
has both of these examples and the full
code around it to run it but this is the
core function where you give it the
primes that you're interested in saving
out the numbers are DD is the all the
numbers that need to be Civ you know
multiples of primes sift out of it so
pretty much we want to know which is the
highest number Prime how high we need to
count up to and then we take the we
create an RDD using the parallelized
operation this basically creates a
sequence that's distributed the context
is the spark context that's passed in
and we give it the prime numbers that we
want to parallelize over then basically
flat map is the same as mad cat
basically we generate the multiples and
filter them out so our candidates then
we subtract out of the numbers we have
the prime multiples that we've seen we
cache that because because in spark when
you create one of these operations on it
if you don't cash it it'll do it again
if you reference it again so you just
need to be a little bit aware of caching
results or not so in a lot of code
you'll see cash and unpurchased kind of
littered throughout but basically if
you're using the results of an RDD more
than once you probably want to cash it
so then our new Prime's are DD is we
take the candidates we filter out all
the ones up to the one we needed and we
cache that and then the rest is pretty
much returning results we get our new
Prime's collectors give me give me the
results sword and put them into a vector
remaining our DD as we subtract out the
ones from the can attract out the new
primes from the candidates unpurchased
is the opposite of cash it says we're
done with this and then we return it so
that's our td's in a nutshell i'm going
to talk a little bit about data frames
but before I do I'm briefly going to go
on a little tangent those who cannot
remember the past are condemned to
repeat it I remember 15 years ago
everything was my sequel Oracle sequel
server blah blah blah lamp stack this
stack that's stack oh if you're not
using stored procedures and the new
fancy the great things you're doing it
wrong then seven or so years ago oh my
god you're using an RD you BMS what's
wrong with you you can't possibly scale
that'll never work you're doing it wrong
and now it's come full circle back to
spark sequel for Google's formula f one
where they said actually most
application writers aren't smart enough
to deal with eventual consistency so
we're going to write a layer that lets
us just use our DB MSS again because you
know if you're using eventual
consistency you're going to get it wrong
so I don't really know what I think
about this it's kind of I don't know
it's just the way it is but it I thought
it's worth pointing out
right data frames are the other main way
of interoperating with data and spark
you can think of data so data frames are
inspired by this in Python or actual
data frames in our its tables it's
rectangles we're back to rectangles and
data frames are the new hotness it's how
Python and our can achieve similar
speeds with some of the new benchmarks
coming out underneath the hood there's
something called the catalyst engine
which is an execution planner it's like
a it's it's like a my sequel or sequel
servers execution planner because it
knows what everything is it can say oh
there's a join there but I can do a
filter and I can optimize this and I can
do that and underneath the hood it
generates Scala source code and then it
uses heavy use of scala macros and i can
decide whether to unbox or skip boxing
and doing all these things to basically
generate optimized execution plans of
your data and oh I only need to read
these columns and these columns to do
that and so on and so forth so all the
mindshare at the moment and going
forward is in data frames and the
forthcoming data sets which is even more
strong typing for the Scala heads spark
is written in Scala by the way so that's
why i mentioned that data frames are
great in scala you know it looks very
natural they use a lot of scala magic
like implicit and whatnot so some of the
crazy things they're doing you don't
even notice you just write scala and
it's there and it works and it's fine
from an interrupt point of view it's not
always that much fun I've spent a lot of
time with java p and unpacked jarn files
to figure out what job what is actually
happening underneath the hood working
with data frames from closure not always
a pleasant and
scholar folks really like their static
declared types and it's going to get
worth with worse with the data sets
which lets you put your beans and your
classes you're very typesafe classes
into data frames and do much more magic
typesafe stuff or any type safe stuff so
here's a brief example this is basically
creating a single column data frame I've
used this as a utility method one I just
need to sub select from something for
whatever reason you'll see that I
basically need to declare the types of
the thing even though it's just a type
it's the integer type then I need to
create a schema for it and so then given
a sequel context and a table name to
create for it and features is the are
the actual numbers I want to shove into
this data frame I create an RDD where I
parallel eyes the the features I then
basically have to create a row from the
features and then from that row or from
that our DD of rows create a data frame
with the features and the schema then
registered as a tab table and then
return it so some of this verbosity can
get a little bit tedious at the moment
I'm not really aware of any libraries
that are trying to help this out
sparkling itself doesn't have a lot of
the sequel or the data frames interop
codes I know there's another project
called Flambeau which does have some of
the interop code there's definitely room
to make some of this nice I think
there's probably you could do some
interesting things with prismatic schema
or whatever to ease this burden and it's
something I'd like to look into when I
have a little bit more time I'm also
just going to show you another couple
examples of irritation and the dot
select function here in particular in
that second line here I'm just giving it
a name
but because of the varargs stuff that
most Scala people don't worry about the
compiler deals with it here you have to
give it a word and then any other sorry
you have to give it a string and then an
array of strings whereas in other cases
if you're giving it column objects you
have to give it a column or you have to
give it an array of column objects blah
blah blah blah blah similarly once you
have a row it's all the irritation of
give me the string give me the end give
me the same give me there you're kind of
reverse mapping the schema back into a
row just to try to do something with it
this is a similar example where see we
have the dot call where it's actually a
column object and you can see we're
selecting multiple column objects but
here we're creating an array first and
passing it the whole array and this is
all to deal with very attic arguments
and compilers and stuff it's not all
that much fun but having said all that
you know and this applies to everything
I say in a talk there are words but it
can do an awful lot of useful things so
we put up with them and we try to make
them better where we can so I'm going to
give you the machine learning elevator
pitch for those who don't know much
about machine learning key points on
machine learning it's basically using
statistical methods on large amounts of
data to hopefully gain some form of
insight on something and generally you
use vectors of numbers extracted by you
from your data these are usually called
feature vectors or features
classification is a very broad form of
machine learning that involves putting
things into buckets ie fashion related
web site versus non fashion related
website in my case topic modeling it's a
way of finding patterns in a bunch of
documents bunches of documents are
usually referred to as your document
corpus or even just a corpus this is the
32nd
of what you need to know so ml lib ml
lib comes with spark it sparks machine
learning library it's full of fairly
cutting-edge parallelized algorithms and
utilities for doing machine learning at
scale its goal is to make practical
machine learning scalable and easy it
achieves this to a greater or lesser
degree sometimes it's always paralyzed
it's not always scalable and it's not
always clear why and some things that
seem easy aren't and there's two main
packages there's Sparky ml loop which is
the one that's been around for ages and
that's the one that's built on top of
our dd's and there's also sparked on ml
which is built on top of data frames and
they have this idea of data pipelines
where they're kind of making things look
much more uniform in terms of you can
think of it as a protocol for there's
really only three or four different
things you do with your data so they're
trying to make it easy to do pipelines
you know with this kind of setup I'm not
going to spend any time talking about
pipelines because everything i had done
today had been in our db's but if you're
starting I would recommend starting with
spark ml if you can because that's where
all the new hotness is and that's where
all the query optimization and
everything can happen ml lid contains
all the basics vectors sparse vectors
matrices sparse matrices dense matrices
labeled points so on and so forth it has
stuff on feature reduction and it has a
ton of stuff that's that's really really
good and worth looking at it has a good
variety of algorithms they're all
designed to run in parallel the guy in
charge of the accepting patches for MLM
i think is known to you know try to make
sure that things are properly
implemented before accepting them into
the core e it's all really well
documented which is always a bonus
and there's there's a large and growing
community and when I had problems I was
able to find the answers pretty quickly
even if it was just don't do that not
always but it's it's a it's a good
community in what's there is well
documented so one of the things is that
ml lib gives us this and this is what it
aspires to it it wants to make machine
learning Lego like that you have your
blocks and you kind of push them
together and you get results
unfortunately the reality of data
science is that we often want this we we
want to build complicated things but we
often need to dig into the nuts and
bolts of things we need to understand
what's happening we need access to
what's going on under the hood one
example would be metrics so binary
classification metrics it's a class for
you know giving you metrics on your
binary classifier i'll talk about
classifiers in a little bit so don't
worry too much about this if you don't
know the terminology but basically it
has some information you want so you
give it your rdd it computes some things
fantastic unfortunately you have to give
your rdd till the multi-class metrics to
get some of the more common more useful
numbers that you're looking for and then
neither of them actually give you the
counts of the items by label one of them
usefully or otherwise will log it for
you at info level but it won't give you
access to it programmatically so if you
need those counts yourself
programmatically you have to count it
yourself again so you end up iterating
your data three or four times possibly
just to get the metrics you want here's
the metrics function I've been using the
RDD is the RDD of labeled points and the
model is is my trained classification
model so you can see basically we take
the RDD and we
map it into a tuple of the prediction
and the actual label week a because
we're going to be using a multiple times
we create the multi-class metrics we
create the binary classification metrics
and our is just the result I'm trying to
return I haven't put in all the metrics
that I wanted that I want just because
there's not room on the slide but for
example f-measure I get from the
multi-class metrics but the area under
the precision recall curve i get from
metrics and then to get the actual label
counts like i said i have to take the
RTD and Count it myself then at the very
end once you're done with it you you
know stop cashing it because it takes up
space until you tell it not to sometimes
I think they're so focused they've got
their eye on the prize of oh we've got
this algorithm we've got that algorithm
isn't this fantastic and you know we
have these building blocks and we make
it easy again they're just missing
sometimes they just miss the mark a
little bit another class another example
would be the hashing term frequency you
know they use this in their examples for
tf-idf they say use this and it makes
everything great that's so easy but when
you look at it like you can't get back
to the words you won't be able to figure
out what the numbers that came out of it
map back to you if you're actually
trying to examine your topics later and
underneath the hoods you know yes they
use hashing do they use a hash map no
they allocate like a 2 gigabyte array of
things and hash into it with a lot of
collisions and a lot of wasted space for
no real reason so some of those things
once you see them you end up rolling it
you're rolling your own for example of
I don't even know how to how to say it
it's just the functionality is all there
so there's something called a chi-square
test where you you want to choose just a
certain number of features the
chi-square selectors great you tell it
how many features you want uses chi
square algorithm to compute that compute
them for you and give them back but what
if you don't know what the number is you
obviously want to be able to look at the
results and then make a decision
unfortunately have to end up grubbing
around in the source code unless you
know the library's very well and find
out that it uses something called the
state it uses a scholar object called
statistics and it uses the chi-square
test from that so here's an example in
code of how you would actually you
compute the chi-square test to get your
feature to figure out your feature count
so we create our labels and features
data frame we look we have we basically
loads some of our features from parque /
k is just a data serialization format
but its colonial column ealer rather
than row based so it's one of the things
that makes data frames quite efficient
we get our labeled points we get our bag
of words i'm going to talk about our bag
of words and a few slides so a lot of
the first good few lines of that are
actually just loading the data up and
then finally we compute the chi-square
we get an array back from the chi-square
test from our labeled points and then in
this example I just log it to figure out
what is it what's there in case you're
curious it gives you p values and the
statistics and the degrees of freedom
and all the stuff from the actual
chi-square test in statistics and then
you can use that array to make a
decision on how many of these features
you actually want to keep so after all
those some of the warts in ml lib like I
said it is still very useful so we used
it for classification with random
forests classification briefly is using
lots of data to tell things apart we can
put stuff into two buckets or classes is
there generally referred to which is
called a binary classifier or you can
put it into many buckets in which case
it's called a multi class classifier
there's lots of different techniques
random forests logistic regression
linear regressions port vector machines
there's a wealth of stuff a lot of its
supported by ml lib it's what's called a
supervised learning technique which
means that for each data point you have
you need to know what it is so you need
a label for it so to tie this back into
dmoz the reason we start with dmoz is
because we have a label for the data we
know that this website is fashion
related or is sports related or you know
whatever your domain is you can kind of
decide and get a pre labeled data set so
yeah each thing needs a label and it has
the features basically you know it'll
depend on what your domain is in our
case the features are the bag of words
I've managed to make it this far without
having to use the words word count
myself and we started with a very basic
word cleansing which is lowercase stuff
removed punctuation basically removed
anything that wasn't a letter or digits
you know said it has to be three
characters in length just drop anything
that's purely numbers when you're doing
data science with texts word count is
it's hugely important so you can't avoid
us the bag of words and things with you
do with the bag of words it sounds
really boring at first but it's you
spent a lot of time on it um so the
first first time we were trying to
create our bag of words we ended up with
way too many words like even on a sample
we ended up with like 1.3 million
different words at this point we were
trying to just establish a baseline
which is the point at which you can
measure everything getting better so we
intentionally wanted something pretty
sucky so we didn't do any fanciness in
terms of stop word removal or stemming
or other fancy things you've probably
heard about we were trying to get you
know the worst suckiest thing we could
and go from there because we want it to
be scientific about it and be able to
measure it and quantify it because it
was so big we did say that it would be
reasonable to say look this word needs
to appear on five distinct sites not
just documents but actual top-level
sites and and it just doing that simple
filter reduce a huge amount of garbage
and gave us a bag of words it was about
460 k well four hundred sixty thousand
words so the code for us is more or less
along these lines or basically we had an
RDD where each line was kind of a Jason
map or I was a map at this point came
from Jason documents we basically mapped
pair are dd's have a thing kind of a key
value pair operation where if if you put
it in a tuple it's just it makes it
easier to group by and do certain
operations more efficiently so we
created we extracted the site from the
URL and then we extracted the clean
words from each documents bung them into
yeah map that to a pair and then we did
a reduced by key or basically just take
all these sets of words per site
just use the the closure function Union
then we mapped back to a pair again
we're basically given the site and the
words we want to split it out so words
and just a number one and then we added
those up / words and then it just had to
be five or more basically so that's a
brief example of how we created the bag
of words so random forests is type of
classifier it's an ensemble of decision
trees basically means a decision tree
briefly it looks at features and says oh
I can there's a huge number of them that
are this or that so I'll may take this
as a decision point and split it into
two buckets random forests they use a
statistical method called bootstrapping
for selection and they do this on both
the feature set they subsample from the
features that you give it and the the
training data that you give it it's not
deep learning but it's still extremely
easy to use and it's very very effective
arthur c clarke once said any
sufficiently advanced technology is
indistinguishable from magic for me
random forests are the closest thing to
Paget go on even with our crappy
unfiltered bag of words we were able to
get pretty good results with a random
force classifier an F measure of like
point 86 on that huge indiscriminate bag
of words here's a little bit of code I'm
going to have to start speeding through
things but here's a little bit of code
for just training it so as you can see
most of that's just default attributes
and creating a model so there's nothing
particularly interesting there this is
how you kind of prepare things it has a
little in the beginning we subsample
stuff then we just make sure we have a
labeled points we split our data into a
training set in a test set with known
seed we cache both of them because we'll
be using the multiple time
and then again we create the model like
from the last slide and it's pretty
straightforward to use and then topic
modeling with lda lda which is latent
Darius slay allocation a topic model
which in first topics from a text corpus
basically it does a bunch of statistics
statistical magic and graph theory and
stuff to cluster create cluster centers
around topics where the documents are
the rows that you are feed it is input
the features are vectors of workouts
like bag of words repeated if it's it's
an unsupervised learning technique but
you do need to supply the topic account
again from the demons data we had topics
for each of our points so we know
roughly how many topics we're going to
be using lda can be quite touchy to run
on large documents got a lot of out of
memory errors on executives strange
errors as you can see if you're ever
doing this and you ever see from spark
you know an error you know something
about Oh sparked off a cadet frame size
do not change this just do not change
this
um we were finally able to get a trained
model after reducing our bag of words we
had one bag of words it was 460k we then
did some stop word removal and other
things got back down to about 160 que el
dÃ­a was still blowing up did some other
things that if anyone wants to know they
can ask me later how to get it down to
11 k we trained this on about a hundred
thousand documents with a roughly even
split between our fashion non-fashion in
the corpus again we just trained this LD
a model basically for this talk there's
other things we're going to be doing
with it we have a lot more fans in his
plans but then if anybody was wondering
well why did they create an Lda model
for that the part of it was just for
that stalk but you could also use it for
things like feature extraction or
feature dimensionality reduction or
there's a number of different things you
can use I'm going to skip briefly over
this it's hair it'll be in the slides if
you want to look at it because there is
a demo if I can get one this is always
fun is worth mouse
oh there we go okay no no don't go too
far yeah sorry about that go back so if
we start with Manning so we can see so
this is a just like 15 or 16 topics from
the model we created and just a few
words from each otherwise the browser
kind of goes gaga but just wanted to
show you that even with this there's
some interesting explorations you can do
so this is our WikiLeaks topic they've
had Manning in it and we can see ok well
there's international in that as well
and that leads to a topic we've just
hand labeled as news and then from news
there's loads of other topics and and
even there you know there's lots of
interesting things you can dig into and
explorations so if we go to fashion if
we go to fashion
if we type properly without the demo
effect we can see there were two topics
here one we handle able this fashion
magazine which had a bunch of other
words magazine that was also shared our
clothing and designers topics so on and
so forth so so what's the point and what
do we do we took pre scraped pre labeled
data from dmoz and common crawl we used
closure and spark and sparkling bindings
to once the data we used the ML lib and
state-of-the-art machine learning tools
to analyze the data and explored it for
insights so what can you do
interestingly enough there was very
little fashion domain code and what I
did there there was one sequel
statements that selected for ashen
because my version of sequel light
didn't have I like and there was a
similar predicate in closure that was
the only actual domain specific code and
pretty much everything we did to get
that result there's even at this stage
there's a lot of interesting information
so the Manning was Bradley Manning for
those who didn't pick up on it one of
the guys who actually dumped a load of
the information on wikileaks there's
lots of interesting directions this can
go that you guys can take it for your
own domains some of the things we're
planning on doing is we're going to run
the classifiers over all the chrome and
crawl data so we're really interested in
finding pockets of fashion related
information that aren't necessarily
linked off the big authorities you know
maybe some small counter culture fashion
sites or you know their own independent
sub sub cultures we're also going to use
lda models to find out what are the
actual some of the you know concepts or
topics within
fashion itself again these are things
you guys could explore in your own
domains and there's a lot of cool things
you can do and have fun doing it so
thanks for your time I hope you enjoyed
it and found it useful</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>