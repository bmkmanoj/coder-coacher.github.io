<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Steven Yi - Developing Music Systems on the JVM with Pink and Score | Coder Coacher - Coaching Coders</title><meta content="Steven Yi - Developing Music Systems on the JVM with Pink and Score - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Steven Yi - Developing Music Systems on the JVM with Pink and Score</b></h2><h5 class="post__date">2014-11-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wDcN7yoZ6tQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Steven I'm gonna be talking about
developing music systems on the DVM
Pinkins core just a little bit about me
I'm a composer and programmer currently
working on a PhD in the digital arts and
humanities at North University in
Ireland
many things to new the University and
nonporous FAFSA as well as the higher
education Authority of Ireland for their
funding and support I'm on the author of
a graphical music environment called
blue I'm working on that since about
2000 it's written in Java and that uses
another program called C sound which
sounds been around since 1986 it's one
of the older computer music programs
that are still being actively used today
and it has direct routes with the
original music and series of three two
music software that was started by Max
Mathews in 1957 okay so the quick
breakdown pink and score are two
different libraries right they're
written in closure and closure for the
JVM the simple way to look at it is pink
isn't meant for generating sound it's an
audio engine and it's a library for
exploring audio engines and helping to
experiment with audio engine design
score is a library for generating notes
you would use it to create and process
lists of data that you would use to to
map as events for a music engine score
doesn't depend on pink but the two can
be used together just bellow background
I first started score as my sort of
initial project to start learning
closure a couple years ago and I use it
for composing with C sound then I
started to explore the idea of trying to
build an audio engine enclosure and
that's where pink came about I think if
you're asking maybe of course does in
the community there's a well-known
program overtone and you may wonder
what's the you know how do these differ
so I would say that overtone
is a wrapper for another other piece of
software called super collider great
software but pink would be architectural
II more similar to super collider and
then it would have meet you overtone as
a whole so the kinds of things I'll be
talking about today are architecture of
audio engines okay and also I'm gonna
spend most of my time probably talking
about pink the parts about score I'll
introduce it for a brief part but
hopefully it'll make sense as to how
this all comes together okay to start we
need to think about two things right
from our music system we want sound and
digital audio is sync with the numbers
right so we have sort of continuous
waveforms that if we're sampling at a
fixed frequency we are measuring some
some amplitude over time right and that
comes up being a sequence of numbers and
the other key thing I think we need to
think about here is space and time are
linked right if we generate 64 shorts
which would be 64 samples that 16-bit 40
44 1k CD quality audio we've advanced
time 1.45 milliseconds right so this
kind of linking with space and time
becomes very important for how we
measure time in the system okay so there
are sort of two main processing models
for computer music systems one is you
would render one sample at a time and
you would do that over and over again
where you render audio and then you
update the world or under audio update
the world and then up up into the size
of the external buffer size and at that
point you would send that external
buffer to the sound card okay now
there's also another model called the
block based processing where instead of
generating one sample at a time you
generate some X number of samples at a
time so very commonly used 64 samples
right that's sort of default for
supercollider and your data
other program use 32 different numbers
as a whole using a block based
processing gives you better performance
are you advertise the cost of a function
call right you only have to make the
function call once per 64 samples you
also save swapping from going saving
state and restoring state back and forth
on the other hand if you process one
sample at a time you have a finer
resolution right the the the base unit
of time becomes one sample rather than
one block and that means you have a
little more possibilities of ways to
calculate and schedule events things
that will be coming in okay just sort of
a little bit of a background there I'm
gonna show you now it's a quick example
okay maybe it won't you up
oh okay great so I'm using them great
okay so here I am mater up oh sorry EDM
and have my rebel here and I'm going to
just require the the namespaces from
pink now I'm gonna call this death
engine and creating a new pink audio
engine with two stair two channels sound
for stereo I'm gonna evaluate and just
start and now I'm going to create app
and sign audio function now if you see
here when I call instance it's comes
true right this is an instance of a
function so I'm gonna come back to that
for part of the design talk about the
audio functions there now I'm gonna add
this to the engine alright now I'm gonna
remove that Oh
the line of code town and I'm gonna stop
stop the engine to completely now just
before I go into that further there is a
separate namespace called pink dot
simple that has a sort of global audio
engine in general if you're just getting
started it's probably the best place to
start
it's just a little bit less typing so
start engine generate and sign then we
get the same result and we stop so on
the one hand that's that sounds sort of
boring right it's a sign tone like big
deal right well under the hand there's
actually a lot going on and it's
actually a lot going on with just a very
few very few abstractions right so pink
breakdown we have four different things
in pink to think about one is the main
processing function for the engine the
thing that takes the engine forward in
pink its general it's sorry implemented
as a thread with a function in a loop
it's it's implemented as a thread with a
loop because Java sound has a push API
meaning when we were ready we push we
rights out to Java sound Java sound then
handles going through the native audio
drivers and doing all that if we were in
sake or audio or some other audio system
like Jack we would register a callback
with the server and that would be what
would regularly call us and we would
then generate in response to the to the
ticking the other three things we have
to think about our events and events in
pink are generic they're simply delayed
function applications right apply of a
function to arguments at some given time
they're used to perform side-effects for
one time only the next are audio
functions audio functions are functions
that return either a buffer of doubles
or nil right so they're they're produced
used to produce sound and we compose
audio functions together to create
directed a cyclical graphs
so we have one giant overall graph of
audio functions that are all feeding
sound to the server to the engine and
the engine then writes that out
finally there are control functions
functions that return true or false and
these only are used to perform side
effects and they're continuous right as
opposed to events which are one-time
only so when I press start engine what
happened was on the right hand side this
is represents the the main processing
thread right so we do this we process
events we call any pre control functions
pre as in before audio the audio
functions then we render the audio graph
and if it's time to do so we'll deliver
to the sound card and then we render any
post control functions and then we do
that in a loop now a couple things here
so the the user never interacts directly
with the thread right so if you have any
new events new any new pending adds or
removes for control functions or audio
functions they're queued up right in the
purple here they're done as Adams
holding vectors right so the user may
call a function that ends up doing a
swap with cons to the vector and when
the thread is ready to pull from that
it'll use a it'll drain the atom using a
cast loop and take in any new events
merge it in taking any new adds and
removes and handle that what that means
is that the whole system is locked free
right so in general this is what the
sort of I think standard practice for
audio engines is today okay and also
another thing to know and this is sort
of a big deal right when you render
audio functions and you render the graph
it's essentially an atomic operation so
in the same way I think it's easier if
you haven't done with audio but you may
have dealt with user interface toolkits
or maybe animation right so you don't
want to have anything change in the
audio graph in any way while you're
rendering it so you
a consistent generation of sound right
after that generation of sound again
let's say I've computed one buffer that
in effect has advanced time right so
every time I go through the loop I'm at
some level some clock tick the first
time at time zero next time and I
process audio now I'm at time one and
everything that has to be here for the
events control functions and audio
functions has to respect that that kind
of idea that for time T I should be
producing X right and if something calls
me again
and it's also time T I need to produce X
again right so having a single threaded
audio engine design is makes things very
consistent and easy to deal with this is
generally the most common design I come
across I don't actually can't even think
of any other systems that aren't single
already so yeah okay so that's sort of
bit about how the processing goes now
we're going to talk about the the audio
graph right so max Mathews who was the
author of music music one two three four
or five started in 1957 he talked about
music 3 and says musics 3 was my big
breakthrough because it was what was
called a block diagram compiler so that
we could have little blocks of code that
could do various things one was a
generalized oscillator other blocks were
filters and mixers and noise generators
and for me this is amazing because this
is like 1959 1961 when this is happening
right so when he talks about block
diagrams he's talking about something
like this right at the top sort of hard
to see at the bottom here it says
exponential decay and it's a reference
to the top generators the three top
generators are all generating an
exponential decay envelope that's being
fed into the sine wave generators as
amplitude and each of the sine wave
generators has a frequency value that's
given the signals from those are all
then fed into the summing unit generator
and that was results are summed together
into a single signal and then send out
right so what are these unit generators
right so if you think about it they're
time varying right so we they they
generate signals over time and they're
composable right that it's really cool
so yeah we're gonna be able to compose
these into a directed acyclic graph we
need to support both fanning in and
fanning out meaning one unit generator
made the plan on multiple other unit
generators the results of your unit
generator may be read by multiple unit
generators right and then we require
state and we require state because
things like filters are implemented in
the digitally with you know taking into
account the current result of processing
the input sample plus the previous
result or two there's these kinds of
things these unit generators are all
stateful and one requirement for for me
is that these have to short circuit on
nil much like doing a map over multiple
sequences if one comes up nil then ends
the whole process now for pink I ended
up going with with this unit generator
in this form it's a function that
returns a function right so the top
function is a set of function and the
audio function is what act is actually
returned an implementation ends up
looking something like this right we
define some audio function that takes in
maybe two audio functions and some other
scalar values as arguments and then we
do a let we do a let create state data
something that's mutable and then we
create a let to do the pre computation
of things that are going to be constant
for the duration of the processing sound
then we return the function the function
when it goes to run is gonna do
something like a when let's if there was
a one let's it would say give me all the
buffers or call the audio functions that
I depend on and check if I got a audio
buffer or if I got a nil if I get a nil
I have to
I have to I have to short-circuit and
return nil there if those dependencies
involve and satisfy I'm gonna loop
through it
I'm gonna loop through and process every
sample up to the buffer size and when
I'm done I'm going to save my state and
return out right so it's a lot of work
and on the one hand but once you write a
few of these it starts to make sense
right ultimately what's going on is if
we think of it as an infinite sequence
we're generating an infinite loop that
we're pausing every once in a while all
right so these given generators really
are like generators or continuations
right so we're sort of yielding out a
value and then we're at the point of
yield we're saving our state so that
next time we call the function we can
resume from from where we left off so
for that I ended up writing a macro
called generator it's not a generic
generator but it does work for for this
scenario where it reads as loop var one
is set to some initial value and then
new part two is equal to some other
value for X in the results of audio
function zero for Y in the result of
audio function one calculate some values
and do a recur and one when it's time to
yield yield
that variable out that ends up being
taken care of all of the the parts about
when you expand the macro it ends up
handling all the parts about saving
state of the loop variables over between
calls and has been really useful for
that in that case okay at this point
you're gonna move mostly to code so in
the next bit of time until I'm done
we'll just be at doing some code and
showing some examples of what happens
here
now there's three things we haven't
talked about in complete detail but I'll
show you these audio functions here
demonstrate we're using my MIDI keyboard
the next thing will be events and then
finally control functions and hopefully
this will give you an idea about how
these three primitives in an audio
engine can be
for multiple purposes
okay
here we go
okay so in this example I'm gonna just
kill this rap pool and create another
one okay in this example what's gonna
happen is that we're gonna create one of
these pink MIDI managers it's a sort of
virtual midi device manager that you you
create a virtual device and then you
hook up an actual heart of it where a
device to the map to great it's gonna
require that and I'm gonna define a
keyboard which is a register virtual
device and then I'm gonna have this
instrument function right
this instrument function is going to
call blitz off and with the given
frequency and that in itself is going to
return a function all right
the function that was returned gets
threaded through to the next the next
form multiply well and so it's going to
multiply that function times the
amplitude that was given and that is
going to give another function that
function is going to get threaded
through to the panning mac pending unit
generator and the panning in the
generator is gonna take a mono signal
and put it into a stereo in this case
it's gonna be acceptor okay let me just
bind the hardware device to the virtual
device manager okay
so what I've done is I've binded a key
function so when going through Java MIDI
the MIDI values if it's sent to this
keyboard from this keyboard on this
channel zero it'll go through the
function here that I've defined all
right so let me start the engine right
so sort of silly sort of fun but what
happened here right so as I press the
key down that is going through this bit
of code so on short message note on I'm
calling that saw function that I defined
earlier with the MIDI frequency in note
number and the velocity that has come in
and I've created an audio function there
that audio function then I'm recording
it in this array here and then I'm
adding it to the audio engine directly
to the the root node of the audio engine
so
now when I do the note off its gonna
check to see oh do I have an active
audio function for that key number if so
remove it from the audio engine and wipe
it from the active array now that sounds
pretty boring right this is just a
really short harsh song
so if we do something like this
all right so now it's going through a
Butterworth low-pass filter okay so in
in this system you can sort of see how
signal is flowing even though in
concepts is that because it's it's we're
passing functions to each other
it sets up a network of functions that
will be used at runtime to generate the
sound now if you hear if you hear it
carefully I don't know if you can hear
it but at the ends when I let go it you
may hear a clicking sound
all right and that's because we're we're
we're manually ripping out that audio
function we're actually doing that we
move so it may be in the middle of
processing and then it is gone right the
equivalent of this is if you have a
thread and you do threaten interrupts
right that's like the harshest thing you
can do and the way to really deal with
threads is to signal to them hey you're
done and the thread will oh okay let me
let me handle everything I need to do an
exit out gracefully right so
this example is building up on the
previous example and it's a little bit
more advanced right what I've done is
I've mapped some of the control channels
values to different to control like
cutoff frequency and things like that
and filters I've implemented three
different patches one is a frequency
modulation
I mean FM synthesis one is additive
synthesis this year and another is
subtractive synthesis then I implemented
a delay and created some nodes and other
things and now I'm gonna first play
something first and then explain some of
what's going on
okay so you can see here already it sort
of sort of becoming already some kind of
like a real virtual instrument for for
me to keep keyboard playback now instead
of ripping out the function and just
doing a hard remove you can sort of hear
it gently slip away okay what I'm using
is dynamic variables so there is a
dynamic variable called done if certain
you know generators will respond to this
if this is found at initialization time
it'll say oh I'm processing in the
context of doneness right that means I'm
gonna wait to heat to find out it's all
that done flag has been switched and
then it will process out I used dynamic
variables in a few places as a way to
have this kind of concept of processing
within a context of XYZ so on the note
off you can see here I've recorded in
the active array I'm not recording the
function I'm actually recording the
boolean array here and then when done I
said I set the done to zero
sorry I said that done did true and then
the active no degrades and exits
gracefully
okay and maybe just to quickly look at
some of these it's for my designs right
all right so FM census this year you can
see there's something called let s right
so there was a problem I mentioned
earlier that we have to support fanning
out so if the results of one audio
function are shared by multiple places
then you have to do some handling to
support that whole concept for time T
generate X right so there's a macro
called shared actually it's an audio
function shared here and what it does is
like all the other other functions has a
sort of lambda over let over lambda here
it has a keeps a recording of what was
the last go for that I generated for the
current buffer number here that is a
dynamic variable the engine is always
setting that to the next value alright
so if the first time I get hit here the
values will not be the same I will go
take the the function I'm rapping and
call generate or just call it and store
a copy of that book of that audio buffer
and return it the next time if I'm
called again I'm gonna check and they're
gonna end up being equals so I'll just
return the value that I last generated
so this is a sort of way to handle that
whole prop problem of fanning out and
and getting into the results to a
multiple function to have it be the same
for a time T now the let s macro
essentially is it's the same as let
except that it wraps every form every
second form with that chair of macro nor
a shared function now here you can see
that you know even though the sine wave
that we used earlier had a sort of fixed
frequency here we're passing in another
sine wave generator multiplied by some
other values
modulation index and things and that
assigned to is depending on the results
of the other the inner sign - so we're
modulating this value we're passing in a
time essentially passing in a time
varying value for the frequency of sign
and that's how we get the the frequency
modulation and I said this is for
additive synthesis here you can see I'm
doing a map over a range of values and
then I'm applying the pink some operator
to all of those things and this is
actually reasonably efficient at this
point there's this whole process because
we have a function generating a function
we have a whole life cycle step of
initialization and at that point we can
do things like map over a bunch of
sequences and do anything that might be
slower because we're only assembling the
the end audio graph now here we're
applying some so that although all the
sine wave generators that get generated
are some together and then passing that
times a multiplier and panning
subtractive yeah it's also the same so
three different oscillators a sawtooth a
square another square being summed
together being passed through a mobile
adder filter passing through an envelope
multiplied by an envelope generator and
panning out now the thing here is the
the ping-pong delay so there's a special
audio function in in pink called node
right so node is two parts one of the
data structure that you can add that has
a bunch of atoms it is what has the list
of current functions that are being
processed as well as pending adds and
removes if I add a node so sorry the
audio engine has one node at its group
that means you can add multiple audio
functions to there and that audio that
node processor will check to see oh is
this audio function done not yet well
let me mix in the results and pass that
out or if it is done I'll remove you
from my my
current functions now node allows for
again dynamically adding multiple new
audio functions and in this case what
I've done here is I've created my own
node and I've taken the results the
audio results from that node and passed
it in to two different places so I
marked it as shared and I've taken that
node and I've added it directly to the
root node I've also added a ping pong
delay in between and then added the ping
pong delay to the root node so you're
going to get the results in two through
two different parts of the graph so now
if I modify my example here and rebind
that key function
yeah yeah so I mean ultimately having
these audio functions be this way allows
us to express any arbitrary graph right
that's the sort of the cool part and
allows us to dynamically modify the
graph and because we have the rules of
the engine and that everything is timed
according to the counter of the engine
everything is consistent and ordered and
it's it's really sort of nice I like I
mean really I find using fascinating so
sorry about ten minutes that's great
okay so this example is a little bit
more boring but it will be very
explanatory here I'm gonna define two
instrument functions one is for a horn
that's additive since it's well banded
wavetables into this horn and a
frequency modulation instrument and I'm
going to start the engine okay okay
so first we're talking about events now
events are delayed function application
during so you read this as this event is
I'm going to call or I'm going to apply
in store horn at time zero to these
arguments that are following so if we if
we evaluate that we get nothing right
ultimately you may think like oh but
I've just created an audio function and
I pass that to the engine right but this
initial function is only creating an
instance of of a horn but it's not doing
anything with it right because this is
completely generic it's all especially
for side effects so if we do this great
so now with events we can do ahead of
time scheduling so you can also crew if
you want to create multiple events and
then if you send it all together it will
be all played in time and because it's
being rendered in that engine in sync in
sync with the time counter everything is
synchronized correctly so and events and
having to call add a funk and but then
instantiating the instrument horn here
is sort of a there's a problem there
right so the instrument horn has been
called in my thread not in the thread of
the engine so instead if I call add
audio events
I can go back to that original event
system event instrument horn and an
audio event is sort of utility function
to wrap the this event with another
audio event and so that all of that
stuff happens and is processed in the
thread of the engine now one thing I
forgot to mention so the implementation
is this right
events are stored in a priority queue
they're sorted by their time when you
add a new event you do get a little bit
of sort cost right so if you happen to
have a lot of events in your event queue
that could be a sort of tight spot but
you'd have to have a lot of events but
if you do run into that problem you can
always create another event queue
there's no problem you can add that to
the pink engine as a control function
and you'd be fine so just a little bit
of random trivia here so you see here
now there's something a little bit
different if you hear the difference the
first function right the second the
second one had a concept of duration
right so this reads play this instrument
horn at time zero with duration 2.0 and
apply these arguments to that creation
of that function now this is actually
the style of notes that you would find
in C sound earlier music software had
something similar like this they call
them note lists it's really sort of
amazing to me because there really are
just function applications at a given
time usually with a given duration as
well so the thing about pink is that the
engine itself has no concept of duration
all right all of that was added
additionally outside via dynamically
scoped variables receive its example
here the horn function is there's this
thing called with a func so utility
again now we're wrapping over
just generic lists at this point it
should become interesting right because
as photo developers we work with lists
all the time
listen sequence is no problem right so
if we want to generate musical fragments
and ideas we can just generate lists and
then wrap it with this function to play
that list with the horn think this will
work or play it with a different
instrument right if the instruments have
the same arity of arguments and the same
types of arguments you can make them
interchangeable that's what makes MIDI
sort of successful all that all the MIDI
instruments respond to the same exact
format so in this case you can see how
you can use that kind of idea to create
musical information that maps to
different instruments oh I think I only
have a just a little bit of time left
okay well okay I'll just go here so
what's going on here either
there's generating uh specifying by hand
sorry a list of values that are being
processed and there's a part of the
score library there's a function for
called processed notes and you can give
it indexes and functions to process on
so this says o for these nodes for index
to apply DB to amp so it's gonna convert
the -6 to some amplitude value like 0.25
and then apply keyword to frequency here
so a 3 becomes a sum a 440 440 Hertz
this allows us to write in useful kind
of notation for us but still have a
generic instrument on the back back-end
so I can write with these kinds of
symbols but or maybe I can use a little
totally different kind of notation and
still reuse my instruments
okay so here's three quick sounds
okay kind of fun well what's the take
away from this right these are higher
order events events as in the sort of
historical music systems music end and
also through sound and a lot of other
systems the arguments were only data
only values scalar values right here we
can pass in functions into our events
and that allows us sort of a whole
different kind of class and notation
right so in a lot of modern music you'll
see maybe a flower is just like lick
lick lick lick lick lick lick you've
been doing types of crazy stuff and it's
notated on the on the underscore you
know like it's got a single note but
then it's got multiple graphs of what
they should be doing while that know
what's happening this allows you to have
per node instance modulations or
directions and it all makes sense right
cuz it's all just functions and yet so
let's see I think I'm running out of
time so I will just do
don't hear anything
okay well I'm just about the time so
what I would say Haley here is that
because the example here was that we had
a control function something that was
scheduled with the engine is run at once
every time the engine made its loop
through and this allows us to create
some things like clocks and this kind of
clock I can say at this tempo every once
in a while
trigger something so in this case it was
a tempo that triggered a pattern player
and and I can write a pattern of sort of
a kind of drum beat using just a values
and an atom and that it would just tick
through triggering all right and that
kind of thing was is was it was it's
sort of built upon that sort of
primitive of the control function okay
well I'm just at a time so just
conclusions those are the sort of four
main parts of the engine I think really
when you look at music software
hopefully at this point you should be
able to break down what you see into
something that resembles something like
this for quick references there's a sort
of three texts I think are really great
one is the technology cope of computer
music which is sort of the manual to
music five and then there's a real-time
scheduling a computer accompaniment as
well as toward module or modular
portable real-time software both way
Roger Dannenberg I think are excellent
papers and that's just a bunch of
systems that I think are really
important in some way or have been
important to me in some way the ones
with asterisks are in some form of Lisp
so all of those examples are online at
github you can download them all the
music examples as well and that is my
email address so and my web address so
thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>