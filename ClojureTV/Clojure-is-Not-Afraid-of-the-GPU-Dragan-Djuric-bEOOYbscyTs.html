<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure is Not Afraid of the GPU - Dragan Djuric | Coder Coacher - Coaching Coders</title><meta content="Clojure is Not Afraid of the GPU - Dragan Djuric - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clojure is Not Afraid of the GPU - Dragan Djuric</b></h2><h5 class="post__date">2016-11-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bEOOYbscyTs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so my name is dragon huge and
welcome to closure is not afraid of the
GPU and just let's start with a few
details about me I am a professor of
software engineering at University of
Belgrade and surprisingly I actually
love doing programming so I've been
doing this I've been using closure as my
primary programming language since 2009
and I also managed to put it into the
curriculum at university so I teach it
since 2010 and you can find my contacts
on github and the projects that I will
be talking about today could be the
documentation to be fine found at
uncomplicated org and a few things the
slides for this talk are at Dragon dot
rocks because dot-com was taken so this
was available so I opened the new blog
and the slides are there everything that
I will be talking about today is on the
slide so please relax and enjoy the talk
you don't have to read every word on the
slides so let's start a bit so I've seen
that the rich Hickey has a hammock and I
also have a comic this is my hammock now
I guess in Stephen King's
guide about Java these four would be
some four main Willems or so now you
remember the first time that you saw
this many years
I suppose and there were those least
guys talking about their language Lisp
and how it's awesome and when you looked
at it it was like oh it's weird but now
when you accommodated yourself to those
parentheses now is the most natural
thing in the world I guess and also I
remember the first time I saw Emacs it
was like barren know features ugly I
mean no features before you're learning
once you learn it like Emacs is the best
software tool in the world so today I
hope to convince you that native
libraries and GPUs like graphical
processing units are also comfortable
and could be used in a really nice way
and efficiently so closure is great and
I hope I don't have to convince you
about that so we'll skip this slide but
is it really great at number crunching
well I mean it's certainly not bad I
mean JVM and closure are not bad at
number crunching I don't think that they
are great either
so one of my goals with these projects
is to help make closure great for numb
number crunching and numerical heavy
algorithms now what about other managed
platforms like are more fighting well
they have many uses in machine learning
in other similar numerical demanding
applications but I don't think that they
are good either at number crunching I
think
even worse than JVM but the thing is
they they integrate lots of native
wipers and they get a speed so that's a
good thing but if you need to implement
your own algorithms in these languages
you have to go deeper in to see in the
some optimized compiler specific code
and it's not easy at all
so the CPU itself is not so great at
bamboo crunching because it's got it it
has many features but because it has
many features it's expensive and you
cannot put lots of those transistors on
the chip because lots of transistors for
lots of functionalities and it's also
runs pretty hot so we'll which brings us
to GPUs and GPUs contain lots of simple
very simple computing units and these
computing unit CL a few features not not
very much not very many but those
features that they do provide are
exactly those features that we lack in
closure and JVM like efficient
floating-point operations or hardware
support for massive parallelism and also
they become faster and cheaper each year
so that's a good thing also but the bad
thing about GPUs is that they are
notoriously hard to program like the
development environments and tools are
from the Stone Age especially if you
develop closer every day now I will
present some libraries that are part of
uncomplicated and I uncomplicated is a
family of open source libraries for
closure for scientific and
high-performance computing
my main goal when starting with this was
to create a state-of-the-art biasion
data analysis and machine learning
infrastructure and for that to work I
also had to create some lower-level
libraries because what was available at
the JVM at the time was really not
comparable to what you what is available
available elsewhere so the first of
those libraries is closure CL which is a
closure library that enables you to take
full control of open CL and GPUs and s
accelerators with minimal overhead so
it's really efficient and enjoyable to
program Neanderthal is a library for
high high performance vectorize and
matrix computations that is built on top
of battle-tested native libraries in
this area and also comes with some
latest GPU libraries so via derek goes
even beyond that and aims to be like
much better and faster than other tools
from this area so let's start with some
code dot product is one of the simplest
vectorized operations that you use in
matrix computations and to make it less
abstract let's say that X is the vector
of some quantity of some product and Y
is the vector of unit prices of those
products so if you want to know the
total amount in dollars we just do the
dot product and on the diag
you can see how its compute and it's
computed by multiplying each pair of x
and y's and then summing everything up
and this is quite simple operation it
scales linearly requires 2n
floating-point operations and 2 and
memory reads and accessing memory is
really one of the most important things
when optimizing such algorithms even
more important than floating-point
operations so it's simple and easy and
in fact it's so easy that in closure
it's a one-liner like you met with
multiplication over your X's and Y's and
then reduce with the plus operation and
it actually how fast is it well it looks
pretty fast to me for 100,000 of those
elements is 14 milliseconds which is not
bad but the point is that we never need
dot product itself we usually need it as
a part of some larger algorithm so these
14 milliseconds quickly add up naturally
we seek for a faster solution we get the
idea that in closure we can optimize it
but you by using low Precure and can it
help well yes but not that much because
not even an order of magnitude faster so
as a side note using a library like flow
kitten can give us top speed for the
data structure at hand while giving also
giving us high level of abstraction and
compact code but not much faster not
faster at all than low Precure when it
comes to numerical algorithms and we
have lots of data and we need speed it
always pays off to
turn to primitives so enclosure that
means Java rays primitive Java rays so
in this case for the same example 76
microseconds which is like 30 times
faster than loop Precure and 200 times
faster than the idiomatic closure but we
pay the price with a clear coat and
mutability and what can we expect from
some optimized native library in this
case as you can see Neanderthal can give
us simple operations simple API that we
can use for those kinds of operations
and it's even faster than primitive Java
like 32 microseconds frog for this
example but only a couple of times
faster well I mean isn't that pathetic
like who would bother with you learning
new stuff and using library for that's
why it'll be patience please
so let's try any undertone support for
GPUs I've heard that Google and Facebook
or like getting some incredible results
with GPUs and accelerators in deep
learning or so so we think ok well we
will also use GPUs and get those
incredible speeds so a good thing about
Neanderthal is that it also support its
API supports pluggable engines and it
already comes with a native engine
optimized and also GPU engine so the API
still stays the same as you can see we
just used some closure CL to choose the
GPU the device and set up the device
that we are going to use but in this
case 327 microseconds well this is
certainly not a bad result I mean it's
still much much faster than idiomatic
closure or
Precure but if four times slower than
primitive Java arrays and ten times
lower than the under-told CPU engines so
what is wrong here remember the movie
Forrest Gump I mean when you use a huge
boat to catch a few shrimps well it
certainly doesn't pay off I mean if you
want to do shrimping for a living then
you better catch millions of those
things and if you don't need millions if
you are only need a handful well then go
to the fish market don't use the boat so
when trying to speed up your algorithms
you have to take care of at least a few
things the first is that your algorithms
have to be you have to be suitable for
parallel computations and for GPUs and
luckily the way we program GPUs and the
way we program enclosure like functional
programming with MapReduce or so are
quite similar so the map operation is
embarrassing with parallelizable so if
you have lots of mapping it will be
incredibly fast reduces tricky on
parallel massive parallel devices and
tricky to program so folterseele luckily
comes with a toolbox that can help even
with that but you have to consider it
also you have to have a lots of data
behind computations because because
there are some communicating overhead
with GPU so if you don't have large
tasks well and also programming parallel
devices is not is not an totally easy
task now a typical example of a
computationally expensive algorithm
is matrix multiplication so if we have
two matrices a and B and we multiply
them we get the third matrix C and each
element of C is actually a dot product
of a row of a and column of B so the
complexity of this algorithm is cubic
and there are it requires a lot of
floating-point operation but what is
even more challenging is that it
requires a really complex reading and
writing patterns from memory because
memory is linear and matrices are not
one-dimensional so it's not it's trivial
in a naive way but it's really difficult
to optimize to optimize efficiently and
the closure implementation seems
straightforward if not overly pretty but
straightforward and for this small
matrix these small matrices like 64 rows
and 64 cons let's see the speed well 40
milliseconds for such a relatively small
task so I'll skip trying to implement
this in primitive erase or so because it
wouldn't work well especially if your
data that is if the data doesn't fit in
the cache memory so let's see
Neanderthal I mean the API is certainly
simple because these are some standard
operations and many number crunching
algorithms can be expressed through
these standard linear algebra operations
so if we can express the algorithm
through them well we should it is much
cleaner and much faster so as you can
see the same 64 x 64 matrix problem is a
thousand times faster like 40min
microseconds versus 40 milliseconds and
here this is only an example of one
operation of course matrix
multiplication but
it's an operation that is a good
representative of how some
implementation is well done
so here Neanderthal is compared compared
with a well optimized Java library not
some naive code but really good
performance Java library and as it is as
you can see for large matrices is it is
of course much much faster on the same
hardware on the same CPU one thread more
on one color but it is also faster for
small matrices so the urban legend that
- for native library to be useful you
have to have lots of data is really a
myth if you do it properly so even even
for small data structures it can be
faster and finally finally let's let's
see some GPU in action so the same
problem matrix multiplication
just with some large matrices like 8,000
times 8,000 and Neanderthals optimized
CPU engine 18 seconds GPU engine on a
quite old my desktop raid on 290x which
is three years old I think it is less
than 300 milliseconds for such a large
problem so it's 60 times faster for this
particular example the native CPU and
1200 times faster than optimized Java
library versus idiomatic loader well who
knows and of course there are lots of
other operations that are not even
delegated to the native library you need
to shuffle that data to transfer it to
GPUs or
memory so there are lots of father
yields for useful operations that
Neanderthal gives you now is that all I
mean is everything you need to just use
some linear algebra operations or a good
performant library and your code will be
like magically super fast well of course
not of course not
the point is that you get a lot of fast
operations for free and that's a good
stuff you should use it wherever you can
it can help a lot but we all have our
custom code we all need to build some
non-standard algorithms or operations
and now we come to the meat of this talk
like how can i program my own my own
custom stuff and do it at these speeds
so get some really fast performance well
second so OpenCL is the standard that
enables us a unified way to program GPUs
accelerators and also CPUs in an
optimized and efficient way it is
supported on AMD Hardware on Intel
Hardware on NVIDIA hardware so actually
you can program program lots of
different devices the problem is it is
quite complex it uses OpenGL C as a
standard so in for example this great
book that I recommend for learning this
stuff it has a hello world and this
hello world is like 270 lines of code so
it really it's really verbose and also
Nvidia's CUDA is not much better or not
better at all closer is certainly much
more pleasant to work with
I hope your dream with that so luckily
there is a library for that closures see
a closure CL is the library that enables
you to take full control of your GPU
while ameliorating those difficult
sticky points of open CL so you write
your kernels which are tiny tiny code
chunks that actually run off on those
small computing units on your GPU and
these chunks are written in a simple
subset of standard open CLC language but
everything else is done in closure stuff
like managing platforms devices hardware
contexts asynchronous common queues
loading and compiling programs managing
memory and queuing the actual
computations to those skills everything
is in closer and closer CL also comes
with a toolbox that has some additional
functionalities that help you with some
tricky stuff with reductions for example
and I know this is overwhelming at first
so that's why I make sure that for all
these libraries I always create lots of
examples I always pick up one good
textbook or developers book like this
closure CL in action and I translate
most examples to closure code so
actually if you want to learn this stuff
you take the textbook you read and you
can follow it in closer so I take care
of the learning part so you're not left
with just some code and find your way
out there are tutorials and there is
lots of documentation
on uncomplicated so let's see how how
could we implement our own dot product
on the GPU I mean finally some real GPU
code and ladies and gentlemen I present
you a kernel so see it's nothing to be
afraid of nothing too complex it's
pretty much similar to a closure
function but that will be used in a map
operation so mapping over some array or
vector so a standard and straightforward
simple see you don't need to be an
expert in C to learn this we just need
to set everything up and fire it off on
the GPU and in plain OpenCL it would be
really lots of hundreds of lines of code
in closure it's not as simple as I wish
it were I mean but it's pretty
straightforward and actually here we see
a low-level closure CL code so we are
not using any help from neanderthal
although we couldn't get this simple so
I also want to point out that this is
the whole program so this is not some
snippet this is the whole program that
was used to generate this result that
you see below so the whole presentation
is actually generated from Emacs from
the light live code so what do we do
here we get the context we choose the
device in this case the best device that
a I have on my computer not this one but
on my desktop don't count of one laptops
if you want to do this stuff
so then we create an ends a synchronous
comment queue we load the code that you
have seen on the previous slide we
compile the program we what else we do
here
we transfer the data from JVM from
closure data structures from Java race
to memory on the GPU we include the
computations and we read the result and
recall that to implement dot product we
need to map every to multiply those
pairs that we also need to sum
everything up so now we need another
kernel summing
kernel and because it's reduction it
would be a bit difficult on the to do
one CPU but luckily we can use some
helper functions from closure CL toolbox
as you can see we just added work group
reduction sum this function will reuse
local memory in cache to do this in an
efficient way without you having to
worry about that too much
and we also improved the multiply kernel
from the previous slide a bit because
the data is already loaded in memory so
we did do a small part reduction there
to make this more efficient but also
it's not in too complex site I hope so
and calling those reduction kernel
kernel circle recursively could be a
challenge but thanks to closure CLS
helper functions like encourages it's
only a few changes to the previous code
and now we can see the total result 47.5
down there the code is quite
straightforward especially if you
compare it to the real open CL C code
then you will see how this is really
simple and I achieved some great speed
ups with this approach in my work on
biood error library via there is a
highly opinionated by aciem
probabilistic closure library that is
much faster that
and the state of the arts established
environments for biasing computations
really it is not as mature as they are
but it is much much faster how I did it
and I built it primarily for doing
biasion data analysis but it I also plan
to use it for some probabilistic machine
learning and it can be it can be used
for some higher level stuff really lots
of useful reusable functions are there
and I don't have to talk about via data
in detail and I hope I will at some
future conference but I will I want to
talk about its main goals and the first
and foremost goal is it's built for
programmers so lots of programmers are
trying to get into machine learning or
statistics or data analysis but all the
tools and environments are like geared
towards statisticians and non
programming people which is of course
logical and alright but I wanted to
build something that I would enjoy using
so the first goal is that programmers
should feel at home and of course you
can do all these exploration
explorations and model building and
everything because water is pretty
interactive but the idea is that you
once you build these models you can put
them on server and make them efficient
and it's not not meant to be like a
copycat of why what are or Python do
what is actually meant to be different
and better for what I needed to do and
what actually it means to be by aciem it
means that you treat everything
probabilistically everything is a random
variable that you typically do describe
using some distributions
and then in probabilistic data analysis
and by aciem branch of this we base the
inference on bias our own bias rule so
basically we have some prior hypothesis
and then we have likelihood of data when
the hypothesis is given and we have an
evidence the probability of total data
and we want to compute what is the
probability of some hypothesis given
this data so it's like going backwards
in inference and I know it sounds and
looks like Greek to you now but it's
simple and easy once you once you get
the math but the main problem that child
by aciem methods dormant for 200 years
like until 10 or 20 years ago is that in
general case it is ridiculously hard to
compute
I mean ridiculously hard even today so
as you can see in this formula with lots
of integrals and some products or so and
vectors so usually we don't have nice
unimodal uni or one-dimensional problems
that fit well into formulas and give
some analytical example results that we
can easily compute no usually we have
some like multi-model many dimensional
problems descriptions for functions that
don't fit at all so we cannot get the
analytical solution and we have to
explore this really really complex
hyperspace with some numerical
simulation so to get the answer really
we have to compute something that is
hard to compute and something that is
not so hard to compute and then do it
many times like millions or billions and
then divided but zapped by something
that is
actually impossible to compute with
brute force so what we do one of the
techniques that can solve this is Markov
chain Monte Carlo simulation like MCMC
and it can simulate and draw samples
from an unknown distribution that's the
thing it can draw a sample sum from
something that you don't know what it is
but once you get many many samples you
can approximate the actual result and
for to do this you only need to do the
hard part and it the acceptable part in
computation do it zillions of times but
luckily you don't have to do this
impossible part so now we have something
that is not impossible to compute but
it's still hard to compute and that's
why GPUs can help now the tricky part is
that MCMC is practically a sequential
algorithm it's really difficult to
parallel wise so it's really not good a
fit for GPUs but luckily people found a
way to to paralyze even that and I
implemented that on GPUs and got really
great results
I will talk about this in a few minutes
but I do not expect you to follow this
code but I'm showing it's just an
example of what a user should write to
build a model so this is a likelihood
function that you have seen in this
formula before and it is really some
straightforward C code that uses in this
case students T distribution that by
Adara provides is in its library and
this is actually the most complex part
that you would have to write to plug it
in in by Adira and also you write the
prior function describing your knowledge
which is also pretty straightforward
with some statistical functions for from
by Adara
we set it up enclosure we load the data
and run the computation and please note
that this is not a hello world example
this is an example for from a real data
analysis from some of the later chapters
of a really good textbook doing biasion
data analysis that I recommend you to
get if you are interested in trying to
learn this stuff so this is the whole
robust linear regression example as you
can see it's um pretty straightforward
closure code not in too exotic
although it's not the simplest so
probably but what I really wanted to
show you is this like by adara runs
circles around the equivalent tools for
example in this particular analysis that
I showed to you it's by Adara took more
than 60 million complex samples in 267
milliseconds which is around 4 nano
seconds per sample and this is for
nanoseconds for some complex function
that required lots of reading from
memory wrote lots of writing to memory
lots of computing statistical functions
so not some trivial stuff really lots of
lots of operations and for example
Jackson's time which are C++ tools that
are used Pacific C++ engines that are
used from our and Python for the same
problem they took 20,000 samples in
couple in several minutes so actually if
you compare sample to sample by Adara
was like 2 million times faster taking
samples of course it is not a fair
comparison I have to admit because these
tools are using different kind of MCMC
but what is fair I think is to compare
the total time for exploring this space
and getting the result
and if we just compare the total time of
this analysis by adara was thousand
times faster and also gave much more
precise results because when you have
more samples you can produce much finer
histograms and the question that is also
interesting is this what if we
implemented this MCMC algorithm in
closure on JVM because by adele also
provides lots of closure statistical
functions it's it wouldn't be a problem
but it would be slow I think so I didn't
bother to write this because it's not
simple code but I I wrote the equivalent
of those likelihood and priors functions
in plane closure with optimized
primitive bio-data functions so let's
see some comparison that is unfair to
GPU because I created as an example
taking the probability density
evaluation of these likelihood and prior
functions so GPU is doing the actual
real work with real data lots of memory
read reading and writing JVM example is
just looping around with the memory with
the data from cache memory but still GPU
was 1650 times faster for this
particular function and what about the
encounter for example well it doesn't
support biasing stuff so not no direct
comparison possible but I took something
that encounter can do like taking 100
million samples from a normal
distribution like a simple stuff and for
this example encounter needed 27 seconds
via data for milliseconds which is 7,000
times faster so if you ask yourself okay
but do I need that many samples who
who needs 100 million samples well if
you try to do probabilistic stuff like
by Azure machine learning or data
analysis you need many of those things
you always needs many samples now you
may think but but who cares I mean one
millisecond one microsecond one
nanosecond everything is a blink of an
eye so why it's important if it's
thousand times faster well when you
consider how many seconds sees in an in
an hour or how many minutes are in a day
or such such things well you can see
that it there is a difference between
waiting 1 seconds for your computation
to complete instead of waiting 4 minutes
or an hour or couple of hours or you
wait for one minute versus several
several days if something is seven
thousand times faster or you have to
wait one hour instead of a couple of
months so if you get it right if you
really get this speed-up which is not
always possible I can tell you so don't
expect it to be easy but once you
achieve this speed-up it can really be a
great difference in these kinds of
programming so oh that's the end ok so
thank you for coming here thank you for
inviting me here and I have to remind
you that you can find the slides on
dragon dock rocks you can find the
documentation on these addresses for
each project by a delicate and
complicated art is a future one but
others are already there
and there are lots of tutorials and all
the examples from the text books that I
mentioned are in the test folder some
github in those projects so if you're
interested please don't be afraid of
this stuff you can you can conquer it
and please I don't have time to take
questions here because I ran out of time
but I'll be I'll enjoy too as
everything he'll hear during the
conference or just ask questions on
Twitter or forums or wherever I will
find this sir I will try my best to
answer or to help you get into this
stuff so thank you for coming here and
listening to me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>