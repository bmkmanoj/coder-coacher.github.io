<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning with Clojure and Apache Spark - Eric Weinstein | Coder Coacher - Coaching Coders</title><meta content="Machine Learning with Clojure and Apache Spark - Eric Weinstein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning with Clojure and Apache Spark - Eric Weinstein</b></h2><h5 class="post__date">2016-11-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2Av5n7ffe0M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thanks so much to all of you thank
you to to Europe closure to to contact
to all of our sponsors to the venue to
the city of Bratislava everyone hoped
make this possible and this talk is
dedicated to my younger brother who
passed away unexpectedly this summer
cool
so hello this is a computer talk so I
feel like you have to start with zero my
name is Eric actually like the elevators
here in Europe because they have that
zero floor which I find reassuring so I
tend to speak extremely quickly
especially what I'm excited in talking
about closure and machine learning is
very exciting so I'm gonna try to slow
down a bit especially since it now
occurs to me that many of you do not
speak English as your first language and
so I'll try to be I'll try to be good
but if you're having trouble following
along or I'm kind of going off the rails
a bit feel free to make some kind of
giant hand gesture kind of like a this
or like a I don't know something to kind
of slow me down a bit that would be
super helpful so I'm just gonna talk for
about 35 or 40 minutes somewhere in that
window so like I said my name is Eric
Eric Weinstein I work at Hulu as a
senior software engineering lead do you
can find me on github Twitter the
internet etc and this human map that I
made I write a fair amount of Ruby
JavaScript Python some go at work I
write a fair amount of closure and
elixir for side projects I'm trying to
shoehorn closure into a large Java
project at Hulu and hopefully it goes
well I'm also a newly minted hydrous
contributor so if anyone wants to talk
about any of those languages please let
me know I've been writing closure for
about three years so I'm still extremely
new this is actually my first closure
conference so this is really exciting I
also recently wrote a book called Ruby
wizardry that teaches Ruby to eight to
twelve year old kids and Ruby's kind of
like
right I feel like they're both lisps or
weekend we can fight about that at the
unsession czar at the bar later but
anyway insofar as Ruby's like closure
work you're interested in Ruby and
there's you know someone in your life
who's you know in that 8 to 12 range you
might enjoy the book please come find me
after the show and others if you're a
Ruby 30 discount so if you go to the no
starch website it'll be 30% off so
thanks also to no starch this is not a
long talk but I think we still sort of
benefit from knowing where we're going
to go so we'll talk briefly about
machine learning in general supervised
learning in particular decision trees
and neural networks in very particular I
guess we'll be using Apache spark for
the decision tree and we'll be talking
about two dsls
for spark flambo and sparkling and then
we'll transition to talk about neural
networks deep neural networks
convolutional neural networks deep
learning and we'll talk about the DL for
J library is this speed okay so far good
awesome thank you okay so part one spark
that's supposed to be a spark I couldn't
find a spark emoji so I've mixed
lightning and sparkle together which I
feel like is kind of like a spark so
like I said we'll be doing machine
learning and in particular we looking at
supervised learning and using Apache
spark to do it so what is machine
learning how many of you feel like you
have a good grasp of machine learning
it's a lot more than I expected alright
so I feel free to correct me later on
uh-huh so anyway what is machine
learning if I were to pick a word I
suppose I would pick generalization
right the idea is to help a program
assemble rules for dealing with data so
the machine can act without being
explicitly programmed and you know what
do I mean by that you know essentially
you want to do some kind of pattern
recognition if you want to ask a machine
hey is this
we're not how much should this house
cost things of that nature or if their
underlying patterns in your data that
you want to tease out kind of you know
grouping or clustering elements
accordingly machine learning is sort of
what you'll be doing and in particular
supervised learning is sort of the first
one I talked about was this idea that
you go from labeled data things that you
know what they are in the world and then
sort of making predictions about
unlabeled data based on that information
generally speaking it's either
classification or regression using the
prior example classification might be
you know if we were gonna do what people
go outside and say hey that's a car
that's a car that's not a car that's not
a car and then maybe we go to another
part of Pradas lava or Norway or the
moon and I say okay brand-new pieces of
information is this a car or not and
then in terms of so that's a
classification in terms of regression
and we might say hey given all these
data points how much you feel for the
price of a house say you know the x-axis
is square footage or some other feature
of houses and the y-axis is price how
might we predict the the price of a
house that we've not seen before that we
don't already know the president so
Apache spark is I'm gonna be terrible
and read this to you is an open source
of cluster computing framework the
parallelism that spark affords us is
sort of ideal for processing large data
sets in several talks so far we've
talked about the virtues of
parallelization and you know when things
are embarrassingly parallel I think was
the book we should you should work in
parallel and a machine learning you know
the more data the better so at a certain
point running computations in parallel
it becomes not just nice but crucial
spark is interesting for a few reasons
and it keeps a fair amount in working
memory which we'll talk about more in a
second the idea is to sort of limit IO
right if you sort of think about the
doop-doop is great for this kind of work
but as far as I know it writes to disk
after each MapReduce step and so I guess
the HDD IO sort of becomes a bottleneck
so not having that as an issue is nice
and
Parker work with a variety of data
sources sequel via JDBC for example or
you can rest and Raqqah weirdies or use
an HDFS file or plain flat text file
it's ordered it's up to you and there's
a little bit of terminology that comes
with SPARC if you've not used it the the
main sort of abstraction is the RTD the
resilient distributed data set which is
a collection of data that could be on
any node in the cluster
hence distributed and resilient because
if data are lost it's not a problem
spark can recompute the missing data and
there are two JVM objects that are
interesting to know about generally and
are valuable in working with spark
datasets which are sort of RTD and
sparks equal execution engine combined
and data frames which are sort of
organized data sets that are kind of
mapped to named columns dates SMT dreams
like I said earlier JVM objects we'll
see them a little bit in the code to
follow although they're they sort of run
under the hood enclosure because you
know we don't generally have type
information it sort of doesn't matter
what the type is I suppose until the JVM
yelled at you which happens relatively
trivial thing at least in my experience
ok so our data the data that I've been
using is police stop data for the city
of Los Angeles I'm from LA in the Year
2015 there are four features that we
currently care about there are more in
the original data set which is what that
bitly link is to so if you're curious
you can follow that link and you can
take a look at the LA open data there's
a lot of actually very interesting open
data about the City of LA and there are
roughly 600,000 instances which sounds
like a big number
until you realize we're talking about
spark and machine learning in large data
big data pipelines and 600,000 instances
is not very much you know this this can
be done with scikit-learn I have done it
with scikit-learn you don't need to do
you know sort of Big Data machine
learning for this relatively small
amount of data but I think it's valuable
to know how it works so that if you do
have millions of rows you have a nice
solution that you can leverage
so I mentioned we had a few features
that we care about so the ones that I
kind of took advantage of while I was
doing this machine learning work or the
so the police data are effectively
people who were stopped by police
officers in the city of La and the
information we have are their sex
whether they are male or female their
race which the open data keeps track of
six American Indian Asian black Hispanic
white or other and the stop tech whether
it was a pedestrian you know someone was
jaywalking something like that or if
they were pulled over in a cart and the
label the thing that we sort of care
about is this post stop activity which
is not really well defined in the
literature it's hard to pin down but it
it means arrest or search generally
speaking so given that this person was
stopped was there any kind of activity
where they searched where they arrested
brought down to the station things like
that and gender most of the data
actually here are our binary you know
it's male or female pedestrian your
vehicle yes there's post F activity no
there's not the only one that has a sort
of more values as the is the race which
as I have mentioned has six potential
values so given that we have this data
in this shape you know this many
features as many instances how are we
going to go about doing this machine
learning magic and I decided that a
decision tree would be a good place to
start
I picked decision trees because they're
fast relatively speaking as opposed to
something like a support vector machine
which can be very computationally
expensive they're robust to noise
meaning that if there's missing or
mislabeled data the tree will generally
speaking still work it will still have
predictive power although these data are
fairly clean I don't think there's any
missing data and over call finding any
mislabeled data though I did not look
all the way through the dataset and
decision trees are also good for sort of
binary and or disjunctive input you know
you know are they male or female are
they you know is it a vehicle or is it a
pedestrian there's a common saying in
machine learning that there's no free
lunch generally speaking there is no
magic method or magic instead of
parameters you can use and say this will
work well over the data or in fact it
worked better than sort of random chance
over all possible data sets and
certainly decision trees have their
downsides they are prone to overfitting
which I guess you could think of as
believing your data too much as the tree
grows and grows and grows it will start
to model and work off of every single
nuance in the data and so if there is a
fair amount of noise if there are
mislabeled attributes if there's
something you know maybe some unmodeled
feature that is sort of warping things
in a weird way that you don't know about
it will model that too
and you'll lose predictive power you'll
start to be super super good at
predicting how your particular data set
will work but when you try to generalize
to data point you haven't seen before
you'll still be modeling noise and not
necessarily modeling the true underlying
values that you care about so we can
address this in a couple ways we happen
to have low dimensionality just by my
printing out the features that I didn't
think we're valuable another form of
pruning which is literally pruning the
tree limiting the size of the trees
saying you know there is only a certain
number levels you can have in the tree
there's only a certain number of leaf
nodes that we're gonna allow and we also
have the option of certain ensemble
machine learning methods such as
boosting the way boosting works is you
might have like a bunch of little tiny
baby decision trees each of which is
maybe good at classifying some subset of
the data and by definition these little
weak learners you have to just be
slightly better than chance so if
there's two potential values as long as
they're right more than half the time
they qualify as weak learners and as
long as you have these little weak
learners the boosting algorithm and the
math is actually super super interesting
I encourage you to take a look after the
show but when you have all these little
weak learners operating together and
they begin to focus on the problems that
they haven't gotten right so far you
really can see very noticeable and very
interesting improvements in your
predictive power and we'll talk a little
bit about boosting later and as long as
these little decision trees that
themselves are not overfitting and as
long as there's not uniform or pink
noise in your data that would cause the
boosting algorithm to sort of try really
really really really hard to get those
questions that can't get right right and
turns out it's working off of noise
boosting is a valuable sort of
additional
heuristic decision trees as you might
know are you know trees they're biased
towards shorter trees so fewer levels
and towards high information game which
you can think of as kind of good splits
at the top it's hard to read but the
very top the root of the tree is
actually splitting on the person's sex
whether they're male or female this is
actually a decision tree for this
dataset though this was from an earlier
iteration from some work I was doing
with Python and in scikit-learn so this
was not generated by the spark code
we're about to see so I couldn't find a
sparkling logo but this is just sparkles
there's no lightning so I figured that
would be okay and so the 2d sl's were
going to look at our Flambeau on the
your left my right and sparkling on the
other side and this is shameless plug
time
flambo is named after a character from a
cartoon called Adventure Time and
Adventure Time is available on Hulu
though it now occurs to me that Hulu is
only available in the United States so
sorry guys anyway
so let's look at some code I've been
talking for long enough and so Flambeau
is the code you see up here on the
screen the way that this works is we
sort of beat a spark context to operate
off of so we do a little bit of
configuration and we can use that smart
context later on as we're working
through our machine learning example so
it's it's relatively straightforward
conf comes from Flambeau con F is the
flambeaux API so these are just two
namespaces available from the flambeaux
library so we have our configuration
we're setting our master to be local
we're setting our app name is zero
closure because we're here at your
closure and we pass that into FS park
context to generate our spark context as
you can see the sparkling example is
devastatingly different in fact I think
the only difference is the actual
namespace so if you wanted to import
spark as F you could have the exact same
code so you'll notice these are
extremely similar and that's because
Flambeau and sparkling are themselves
extremely similar there are a few
differences
for example there are tiny things for
example order of arguments for their map
functions they define their own mapping
function for mapping over our duties if
our call correctly Flambeau does
function and then collection and
sparkling is the other way around and
then there's some more interesting
differences the D structuring tools
available and sparkling are a little bit
different I think a little bit more
nuanced than what's available in
Flambeau and they're in a separate
namespace that if you're not careful and
you don't do t compile you'll get a lot
of weird errors so if that happens to
you that is why and so both of these are
valuable in terms of kind of setting up
our example getting configurations and
smart context things like that working
unfortunately I feel like there's a
limitation in both of these where they
don't quite make the leap to wrapping
model creation predictions things like
that so there's a little bit of Interop
that we have to do which isn't bad but
something to be aware of for both of
these libraries so we've defined a model
which uses the Apache spark a decision
tree train classifier method we have a
training set that I have kind of pre
split and set up off-screen to just is a
description I believe of the label I
guess air D if you will but it's either
yes or no in terms of post op activity
categorical features infos again sort of
off-screen but it's a basically it's a
map that explains to spark hey you're
gonna get in this column the first one
that describes sex you're gonna have one
of two possible values for race there
are six possible values for stop type
there are two possible values so just
kind of giving hits to spark genie or
Gina I've never pronounced it out loud
is just one of a couple different
methods for handling information game
and here we've said at max depth of five
on the tree and limited the leaves to 32
so limiting the size of the tree is sort
of a form of pruning which I mentioned
is necessary to avoid overfitting I
haven't really gotten a chance to tune
these much and it's very possible that
playing around with them a bit more
would yield better results so if you
haven't done a bunch of machine learning
before it's like 99% getting good data
either finding it or cleaning it
generally cleaning it and then tuning
hyper parameters disorder to figure out
what works well you sort of
intelligently tuned them you don't just
kind of spin knobs but you spend a fair
amount of time see if certain changes
that have a hunch might be correct are
in fact correct and does mention there's
a little bit of interrupts it's worth
noting that in the prediction method the
thing that's being predicted is actually
a labelled point mm not a string or some
more primitive method we technically
have kind of a little object that comes
to us from Java and says hey here's the
data and here's the label that's
associated and the prediction is
effectively coming back with the actual
label and then the predicted value so we
can later compare and see how well we
did and how's the speed is this good
excellent thank you
and so that begs the question how did we
do and I think the answer is not bad
this is I think pretty good we're about
77% accurate on the test data so given
that we've trained on a bunch of data
from the stop data set 77% of the time
were right when someone says hey here's
this type of person do you think that
they had some kind of post-op activity
this is pretty close to the best of
they've been able to do with this data
set I'm currently enrolled in the
Georgia Tech online Masters of Science
in computer science setting machine
learning and so I've worked with this
data set for a couple of months and
actually with a boost of decision tree
we can get I think 81 percent accuracy
which I think is my high-water mark so
again there's a fair amount of tuning
you can do to try to get superior
results the downside of this is this
something that I think is incredibly
important and something that doesn't get
mentioned a lot when we talk about
machine learning is that when we have
tools like this for things like Pulis
data right the immediate thing is we're
like okay great you know we're gonna get
Minority Report early we're gonna be
able to say hey given this type of
person we know if there's post-op
activity we know who we're supposed to
be searching we know who we're supposed
to be arresting and you know criminal
justice is now an assault problem and I
think the insidious problem here is that
if the data itself is biased by human
judgment
we have to be very careful that we don't
assume later down the road and we've
sort of forgotten this that the machine
is telling us what to do and the machine
can't possibly be biased and so the
Machine says to arrest this person we
should the machine says this person is
probably a criminal they probably are
which is not correct you probably does
not surprise you to know that the sex
and race of the person is an extremely
powerful predictor of post-op activity
and if it turns out that if there's some
racist dimension to the data you were
going to end up with a racist machine so
it is important for us to remember that
and not pretend that because the data
some days somewhere out of the pipe you
know up here from some machine learning
algorithm that magically that is sort of
bias free
there's always bias both literal bias in
the machine learning algorithm and bias
in the data okay so I got a bit heavy so
we're gonna pivot to decision trees away
from decision trees rather and to taking
a look at a sort of different machine
learning tool neural networks and a new
framework /library
DL for J so we'll pit away from the
binary classification portion of our
program and move on to some
convolutional neural network magic and a
little bit of image recognition and
image identification so what is deep
learning it's in the news all the time
now if you like Google is just kind of
putting out a new deep learning paper
every 10 minutes so to answer that
question is sort of valuable to think
more about neural networks and then deep
neural networks you're all networks are
computational architecture that are
modeled after the human brain the idea
is to have many neurons which are
effectively functions and with these
functions you have sort of an analogy to
dendrites in the in the brain so so
human animal neurons have these little
dendrites little branches that kind of
all go into the cell body and then a
single axon that comes out and sort of
you know synaptic information comes in
through these 10 dendrites and then sort
of fire through this axon or that's at
least through limits at my high school
biology understanding
about neurons work and so you can kind
of think of these artificial neurons you
have a function that is sort of
effectively the cell body you can think
of the dendrites being sort of like
vectors of signals and weights like
here's some information and you're so
heavily that you know this value should
be weighted and then the axon is sort of
like the output
sometimes we threshold it and say you
know it's kinda like I'm not actually
firing given these values and these
weights that come in do we fire or not
or sometimes we don't threshold we just
return a real number some value which
depends on sort of a problem that we're
solving in the domain so during training
the network will kind of tune these
weights internally and that's one of the
downsides of neural networks is they're
kind of a black box we don't really see
the the set of weights that live inside
the network and even if we did they
wouldn't make sense to us and the
network sort of Tunes these weights via
generally speaking via back propagation
if you're not familiar the idea is sort
of you know we're making these educated
guesses as a neural network when we're
told that we're wrong that error kind of
propagates backward through the various
layers of the network and we adjust our
weights accordingly I actually wrote a
blog post earlier this year kind of
translating some some Python simple
neural network stuff into closure area
name it came out okay I think so feel
free to look that up and the slides come
out which should be later today so when
a neural network has many layers which
is sort of an ambiguous definition but
generally speaking more than one hidden
layer so more than three layers total
it's a now a deep neural network
generally speaking most deep neural
networks are larger than three layers
and in practice they can have hundreds
of layers which leads to an interesting
problem and it's this idea of the
vanishing or exploding gradient and the
idea sort of works like this because
each of the neural networks weights that
come in are updated proportionally to
the gradient of the error function sort
of respect with with respect to each of
the the current weight in each iteration
these weights are very very small
generally speaking they're tiny
fractional real values between negative
1 and 1 or 2 0 1 and so what happens is
it's kind of this error signal kind of
exponentially decays as it goes back
through the layers because you're just
multiplying these super tiny numbers by
each other over coming over again and
when you have this kind of decay it's it
can be tricky you can sort of stop
learning in your network
so people who spend a lot of time
thinking about activation functions and
various methods that you can use to to
the network to make sure that you don't
have this problem there's a
corresponding problem called the
exploding gradient weights are very very
high not only does it cause the same
sort of problem you have this like
stampeding gradient to your network but
we found that generally speaking if you
have lots and lots of nodes or very very
high weights you tend to overfit and so
we have a couple tools one of them is
regularization which sort of penalizes
high weights to kind of not only combat
exploding gradients but also kind of
keep overfitting in check so that sort
of sets the stage a little bit you know
with neural networks deep neural
networks and so when it comes to deep
learning one very interesting type of
neural network is a convolutional neural
network and so it's a type of deep
neural network and its name comes from
the verb involved to sort of roll over
or roll together it is is like this
there are many filter layers each of
which sort of passes over the image and
picks out a feature that it's filtering
for so you might have a filter that's
looking for tiny horizontal lines or
tiny vertical lines or tiny diagonal
lines or little curves things like that
and if you look at the the animation
here you can sort of see you've got
these three filters in pink you can
almost imagine like this little
magnifying glass kind of scrolling over
the image and when a feature in the
image sort of lines up with the kind of
shape in the filter that that filters
looking for you can almost imagine it
kind of like I guess brightening out
right there's a very high signal it's to
use kind of a light based analogy and so
what happens then is we get a feature
map based on this we know where these
kind of like hi signal points are and we
construct a map of the entire image
saying hey here's where I found all the
tiny vertical lines and you might
imagine like an image which is all these
little tiny vertical lines then here's
where I found all the little diagonal
lines and all the little C shaped curves
or something like that and these stacks
of feature maps are sort of how machine
comes
networks see which is very interesting
because you know for something that's
modeled on human cognition this vision
scheme doesn't seem to be how we see
things we don't seem to kind of
construct the world from all these tiny
itty-bitty lines at least not
consciously and this image comes from
the deep learning the DL 4j website so I
encourage you to go check it out there I
think hundret carpathia is the creator
or source of the image so thanks also to
him I think there's a couple more he's
responsible for their super valuable
learning aids and convolutional nets are
generally well suited to image
recognition so when you see these things
come from Google it's often image
classification you know is this a car is
it's not a car image search have
clustering things that look like you
know each other it's sort of an
unsupervised learning approach or facial
recognition which is I think generally
the sort of go-to example that people
think of when they think about sort of
AI or machine learning as applied to
computer vision and neural networks so
the again sort of the the phantom
haunting us this whole time is this
notion of overfitting this idea that you
know we're going to believe the data too
much that we're not gonna able to
generalize well so highly dimensional
data will will lead to overfitting but
also degrade performance this is all
hugely computationally expensive so to
deal with this we we generally
interleave convolutional layers with
what are called max pooling or sort of
like some sampling or down sampling
layers and the idea behind max pooling
rather than say average pooling is that
whenever you have kind of a space in
your image you kind of pick out the your
feature map you pick out the highest
intensity one sort of the brightest spot
so if you look at the example there's a
little red sector with one one five six
and we kind of sample that down to six
there's two four seven eight we sampled
that down to eight and we do lose data
when we do this sort of by design and by
definition in order to make the problem
more computationally tractable but also
to sort of ensure that we find these
little pixels and they stay lit if we
were to do something like averaging you
could imagine this if we're going to
extend the light metaphor sort of
diffusing that glow and what we really
want is to know exactly where these
bright spots on the map are so we can
work with them
and so finally we have now I think a
decent mental model of how these
convolutional networks are designed you
have kind of input layer that takes in
the image you build a stack of feature
Maps and for each of these feature Maps
you'll go through and you'll sort of
down sample to discard data and keep
things sort of computationally tractable
they'll go and convolve you'll kind of
look over the image for you know maybe a
new set of features down sample again
and kind of do this ad infinitum until
you reach your fully connected in this
case the fully connected multi-layer
perceptron and you have an output layer
that might not put yes that's a car or
hey yes this this belongs with these
other red things or something like that
you know in this case we might have a
label that's super-creepy house
I guess as opposed to you know baby sea
turtle or something like that so
speaking of baby sea turtles our data
involves them this image comes from the
US Fish and Wildlife Service by way of
the deep learning 4j or yell for J
example github repo so deal for Jane as
I eluded as an open source distributed
deep learning library for the JVM there
are official bindings for Java Scala and
Python as far as I know there's not an
official closure one so there's gonna be
a fair amount of interrupting the code
that I'll show you and both deal for J
and the sort of deal for J examples are
available on github so that second link
is the link to a deal for J the third
one that's minified is a direct link to
the convolutional neural net worked
example that we're gonna follow for the
rest of this talk I don't think the
Interop is too too bad it's it's a
little ugly is my only complaint so I
would like kind of a nice deal sorry and
I said DSL or a nice kind of wrapper
library for critique learning for J so
if you know of one please let me know I
would like to use it or contribute if
there isn't one we as a community should
band together and make one
okay they came out better than I thought
i apologies apologize if it's a little
bit hard to read there's a fair amount
of Java interrupts so the idea here is
we have sort of an idea of a neural
network configuration so we're gonna
call the internet configuration builder
from deal for Jane and I've emitted some
values and some tuning for space reasons
so we have an activation function r lu
which i badging is how you pronounce the
abbreviation and again kind of parking
back to that our discussion of you know
if you you want to be very careful about
the activation function that you pick in
order to avoid sort of gradient problems
a learning rate of point zero zero zero
one which is just from some playing
around we initialize the weights to
guess the xaviar weights I'm actually
not quite sure what these are generally
speaking you initialize your weights to
very small random numbers but so
hopefully that's what those are the
optimization algorithm we picked is
stochastic gradient descent and we have
this updating method which is RMS crop
which I believe is root mean squared
error propagation or crop rather a
momentum term with 0.9 momentum is sort
of this notion of I guess you can
imagine kind of kicking you down the
gradient slope because there
occasionally if you have a very Wiggly
surface you could get stuck in local
minima if you want to get all the way
down to the minimum error the momentum
sign it kind of kicks you through these
little valleys so you can aim at the big
one and then there are six layers here
an initial layer a max cooling layer a
five by five sort of like you can
imagine like a five by five pixel
convolutional layer another max bloom
layer and then a fully connected layer
after which we output the result of our
deep learning magic and one of the
things that I I don't so much mind the
need for the threading macro and the
Builder syntax I would like even just a
nice wrapper around interets I can tell
you that in my experience
munging instant Long's and stuff like
that especially as we heard earlier you
know closure is not ideal when it comes
to error messages and sometimes if
you're going through the interrupt
message and the closure message and
you're there's no such method and you're
having a hard time you're like oh wait
this is axe
you know it's looking for a long and
this is a double so if that happens to
you that is possibly your root cause so
how did we do
so using this particular neural network
configuration we have an accuracy of 0.3
75 precision of 0.3 recall point 375 and
nf1 score of 0.35 to 9 which is actually
not terrible there are actually there
are only 4 animals that are in this data
set
bears ducks deer and turtles so if we
got 0.25 or worse than 0.25 I would be
worried because then we would be kind of
random or worse but it seems that there
is some value kind of you know there
some patterns being detected by the
network that are that are sort of
figuring out what's a what's a duck and
what's a deer and what's a bear and
what's a turtle and I think it's worth
teasing out that this notion of
precision in recall and the f1 score um
precision is the fraction of retrieved
instances that are relevant
whereas recall is sort of the flip side
of that it's the number of relevant
instances that you retrieved so given
all the possible relevant ones like how
many of them did you get as opposed to
of the ones you got how many we're
actually relevant and the f1 score is
sort of just to measure the test
accuracy if I recall correctly I think
it's it's twice the products of the
precision in the recall over the sum of
the precision in the Ricoh yeah we're
doing better than chance which is nice
unfortunately we're not doing as well as
humans humans generally have you know 90
some odd percent accuracy on tests like
this though the good news is that the
state-of-the-art neural networks kind of
the stuff you find to describe to
Google's papers or Microsoft's papers
are achieving near human accuracy and
image recognition tasks in the
neighborhood of 90 or 95 percent which
is likely the fact that they have
millions and millions of images and I
think this is dozens and dozens of
images so as we talked about earlier you
know if you're limited in the data that
you have it's very difficult to
generalize well so more data is more
better I guess so
how did we do what did we learn what did
we talk about while we talk about
machine learning and spark and how close
your head spark are awesome how flambo
and sparkling are roughly I think
equally powerful but
we've been in kind of a get you going
type way like setting up the spark
context mapping functions over our
deities manipulating and counting
instances things like that but as far as
I could tell they are not very strong
wrappers for things like decision tree
modeling for things like decision to be
predicting and would be neat I think to
have a more of that so there's still a
little bit of interrupts that we had to
handle and we saw there's lots of
interrupts with DL for J I would love to
help investigate finding building a nice
closure DSL for a deal for J so deep
learning a super doable closure but the
internet makes it not as beautiful as we
would like and I would like I think
eventually to kind of check the la open
data that we've got the the sort of
notion of you know who's getting pulled
over who's getting arrested who's
getting searched and this really for
convolutional neural networks with lots
and lots and lots of data to figure out
what's in the image classify the image
cluster images and sort of combine these
in a careful way because as I as I said
you know you can imagine having a super
powerful confident and you know a bunch
of data about who's getting arrested or
pulled over together based on the data
that we have and biases in the u.s. and
frankly elsewhere it's you know it's
very easy again to get bias data and
tell ourselves that it can't be bias
because it comes from oh machine so I'm
super interested in these fields
technically but also socially and the
takeaway is the the TL DPA which is what
I call the too long didn't pay attention
so the closure community is fantastic as
you guys know you're a part of it and
you helped make it awesome so thank you
I'd really like to work more on
libraries like Flambeau and sparkling so
contributing to those I think is huge
Flambeau I think is under active
development I think sparkling is - but
sparkling I don't think it's how to
commit since May so I would be super
interested in seeing more work on those
ditto for DL for J either building or
contributing to or forming a nice
closure first solution for deep learning
I think would be would be huge
and the complete code for this example
for the binary classification example
and for the content are on github
I feel free to pull it down run it make
a PR things like that I'm happy to have
as much help as possible although it
occurs to me I have not yet made the
repo public so don't do that right now
but when I when I share the link to the
presentation on speaker deck which
should be later today I'll make sure
it's also kind of like tweet out the
link to the the get every boat as well
but it's just my name
Araki weinstein slash zero closure and
like I said this slides will be
available a little bit later on today so
like I said it thanks again for coming
to my talk I really is my favorite thing
I like to use this one a lot thank you
so yeah so thank you so much
you guys are great thank you for
choosing to spend your time with me I'm
not gonna take questions now but please
do come find me later if you have
questions I'm happy to try to answer
them or if we don't catch each other
here in Bratislava please tweet at me or
yell at me on the internet and I'm happy
to try to help thanks so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>