<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Infinite State Machine Poetry in, with, and by Clojure - Eric Weinstein | Coder Coacher - Coaching Coders</title><meta content="Infinite State Machine Poetry in, with, and by Clojure - Eric Weinstein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Infinite State Machine Poetry in, with, and by Clojure - Eric Weinstein</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/O-8-IpMzkBo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you so much for coming by the way
I really appreciate your all choosing to
spend your time here with me it really
doesn't mean a lot to me I mean actually
one I know you don't really have a
choice because there's no other talk
right now and - I know you are all
secretly here to have a good seat for
the guy Steele talk that's coming next
but that's okay because that's that's
why I'm here - so anyway I appreciate
you just the same
thank you also to closure cons to our
sponsors to katia tech to the venue to
everyone who's helped make this possible
so thanks very much this is a computer
talk mostly so you have to start with
zero so hello I'm Eric I tend to speak
very quickly particularly when excited
or nervous and talking about machine
learning and closure and giving talks
makes me both excited and nervous so I
will try to slow down but if at any
point I'm kind of going off the rails or
speaking way too fast just kind of like
make a big some kind of signal and I'll
try to bring it back down I'm going to
talk for about 35 to 40 minutes if it's
closer to 35 we'll have time for
questions at the end if it's closer to
40 I won't take questions but feel free
to come find me later on tonight or at
the party or anything like that and I'm
happy to try to answer any questions
that you might have so like I said my
name is Eric Weinstein I am a director
of engineering at Fox networks group in
Los Angeles so the program says Hulu
that was true up until a couple months
ago I know I'm building a team in a
platform over at Fox we are doing this
for all of our television content
including The Simpsons Archer National
Geographic Fox Sports and more and yes
we are hiring all that information and
some more is in this human map that I
made for some reason oh I I read a lot
of JavaScript and go at work write a lot
of closure on the side although as rich
mentioned there are tons of people
trying to sneak closure into their work
environment and I'm definitely one of
those
people I've been writing closure for a
few years now and I recently wrote this
book over here
rubia wizardry that teaches ruby to
eight to twelve year olds Ruby's like
closure right I feel like those are kind
of the same that's a joke so if you are
interested please do come find me after
the show the good folks over at no
starch have set up a promo code the book
is 30% off from their website with the
closure conch 30 promo code so feel free
to snap a picture this slide will be up
at the end again and that's it dub dub
dub no Starcom so thanks also to no
starch some of you may have been at Euro
closure last year in Bratislava and if
so you may remember another
machine-learning talk that I gave and
one during which I had a cup of coffee
like this one that I perpetually kept
almost drinking there was in fact there
were a lot of jokes in the slack channel
and numerous bets about when I would
actually drink it and then jokes about
using machine learning to predict when I
would actually drink it this is a bit
different from that talk we're not going
to be doing classification of images and
things like that we're gonna be
generating text but the coffee bit is
probably going to be the same this is
not a long talk but I do think we
benefit from having a road map so I'm
gonna start by talking about poems about
technology and we'll talk a little bit
about TS Eliot's objective correlative
then we will selfishly look at a poem
that I wrote that is about machines then
we'll move on to a poem that I wrote
that is also a closure program and so
there's something for the human reader
and something for the machine reader and
finally we're going to look at a couple
different ways to produce poetry using
closure one of them using Markov chains
the other using a recurrent neural
network how's this speed is this speed
good next one thank you
all right so we're gonna kick it off
with a poem about the machine so a
finite-state machine going back to the
name of the talk is a model of
computation that can be in a finite
number of states and only one of them at
a particular time an infinite state
machine is like a finite state machine
but with an infinite number of states so
this is sort of like all the poems or
the universe or something
Turing equivalently romantic so that's
big are romantic cuz we're going to talk
about John Keats in a bit too some of
you were probably thinking hang on I was
pretty sure I signed up to come to a
closure conference so why is this guy
gonna read us a poem about computers
well let's take a look at what TS Eliot
called the objective correlative and I'm
gonna quote from Wikipedia it's the
artistic and literary technique of
representing or evoking
a particular emotion by means of symbols
that objectify that emotion and are
associated with it
so before I wrote poems for machines
which I think most people call software
I wrote poems for humans which I still
do sometimes I used to give a whole
bunch of readings and so I like to set
expectations so here you go I'm going to
read exactly one poem it's gonna be
called memory leak it's not really about
a memory leak per se but you know poetic
license which is also the license I'm
going to use to open-source this and I
was a dumb joke thank you for laughing
so it's gonna take about one minute to
read probably actually less than that
keep an eye on the images slash symbols
and thank you for your patience so
memory leak the machine stays up all
night writing poems smoking some it
spawns a billion threads of execution
little tales it tells itself in tales
zero it sails from island to island in
an archipelago linked by green arrows of
algae in tale one it is a blue fork of
lightning whose ends reach into the head
of every child in the neighborhood and
install imaginary friends as she a robot
made of bottle caps
the machine constructs these artificial
histories but never Dilys one one memory
runs into the next bioluminescent algae
fill the neighborhood pools a robot
sails for the new world in a stiff
breeze the humans wake to find 1 billion
machine poems overflowing the hard drive
the last one reading I would grow human
teeth of my own and dissolve them I
would send signals directly into your
heart so this poem first appeared in the
final issue of court green which is a
literary journal and that was in the
spring of 2015 so why does this matter
why did I bother reading this what is
this poem for right which is I think
something I really appreciated about
riches you know this all matters sort of
I guess as per the objective correlative
and that these images stir us to action
these objects have emotional weight and
they make us feel something similarly
poems for the machine move the machine
to action we provide instructions and
the machine complies so I think that's
going to be keep that in mind as because
we look at especially the next few
slides with a poem in a program that are
the same alright so now that that's all
over with and we don't have to listen to
poems anymore let's take a look at
something that's actually a program as I
mentioned when we talked about Elliot
there are sort of these notions of
objects these notions of symbols and
they're an opportunity for what I like
to think of as dual attention this sort
of doing two things at one time there
are dual narratives there's sort of the
tension between the line and the
sentence the relationship between a
poems title in its text and I think that
this speaks to a lot of the parallels
between poems and programs above and
beyond syntax you know poems and
programs both have style
they have idiom they have line breaks
and and beyond even that they move they
cause something to change in the world
they result in in an action or a change
in state so we're going to add a new
dimension to this discussion which is
what happens when those dual attentions
are one story for the human reader
and one story for the machine reader and
this is a poem and a program called REM
cycle and I apologize if it's a little
bit hard to read this may remind you a
bit of Shakespeare there's a
Shakespearean quote at the top we'll
come back to Shakespeare a little bit
later in the talk I sort of stole a lot
of ideas from him because when you're a
poet and a programmer you are also a
thief which is good I think it's also
tough to make a pun with this filename
it's it's REM cycle but this is the clj
extension so the JS silent so I'm just
gonna read this really quickly defog the
window to withstand the presence of
ghosts let us know that they all lived
and the window is open us in the window
define for me
let the ending be happily ever after the
ending be or me deforest now the ending
is written on the tallest tree let me be
ogre Abel and open hands ineffable Oh
strange we are haunted first me first
Abel and does anybody know what the
output of this poem would be if you can
still if you can read it the strings
that are in here are kind of and you can
shout it out if you think you know
that's all right I'm not gonna do a demo
I'm not gonna run it because those
always go horribly wrong but the he the
output of this is know that they all
lived happily ever after
which is super trite but is the most
meaningful thing I could get the machine
to produce and then you kind of note the
optional comma is the syntactic
flexibility of closure you know sort of
imagine what we could do if we were
using you know macro is an additional
you know like a full arsenal of closures
flexibility note also that I pronounced
this slightly different than it's
written but I think again you can sort
of apply a poetic license there
so we've seen a poem about machines
you've seen a poem that is a machine or
some readable type machines and now
we're gonna turn our attention to what I
think everyone is really here for which
is to see how we can get closure to
write homes for us because that would be
awesome
if I didn't have to write my own poems
all the time so we're going to use two
different techniques in two different
training corpuses corp i corpuses to
produce these results
the first one is Keats's 1817 poems the
other works of Shakespeare we're gonna
use a Markov chain for Keats and we'll
use a recurrent neural network for
Shakespeare for reasons that will become
clear in a minute why we have two
different models and two different
training purposes so the first one like
I said are the 1817 poems by John Keats
it's about 2,000 lines 13,000 660 words
and just shy of 80,000 characters I
picked Keats and not say Eliot who might
make more sense since we were talking
about the objective correlative mostly
for copyright reasons I wanted to make
sure I could give this talk and put
things on github and I did not pick
Shakespeare for this one because
narcissism which I will explain in a
minute this speed is still good cool all
right I'll stop checking and just stop
me if I start going off the rails so a
Markov chain which is the model we'll
see in a minute is a stochastic
memoryless process it's effectively a
random walk you tell the machine hey
here's a couple of words and then see
what comes next and the Markov chain is
a Markov process that obeys which the
mark what's called a Markov property and
the essential idea here is that your
next state depends only on your current
one so there's no memory there's no past
actions you can sort of fudge it a
little bit by kind of baking arbitrary
amounts of history into your current
state to produce the next one but it's
effectively a random walk and here what
we do is we pick a couple words and then
we jump to the next one and we kind of
crash around the training corpus and see
what we produce and in this case after
reading the 1817 poems by Keats we got
this prose bone by a machine I'm not
going to read this to you I might read
little pieces again I apologize if it's
hard to read I had to trim it down a bit
because the machine was proposed but I
will leave the side up for a minute so
you can kind of get the gist the
interesting thing here that I really
like is the poems from 1817 contain the
name Eric exactly one time but somehow
in my dozen or so
Markov poems the robot put my name in
there and like I said because narcissism
I decided to stick with this and not
switch over to Shakespeare so you know
get this happy Burton's Eric I echo back
each sting thrown by vigils keep amongst
boughs pavilion where she did lightning
plays etc etc you can already see like
it's kind of like English I mean it is
it sort of makes sense but not really
right but you could maybe give this to
like a middle schooler and say here's a
poem by Keats and they'd be like I don't
get it but yeah that looks like he's
right which is I think very impressive
considering the code is maybe fifteen
lines it's really just kind of tear
apart the text figure out what came
before and make it jump to what comes
next
and you can see I haven't really done a
lot here in terms of line breaks in
terms of you know you can see the
capitalisation kind of restarts when it
jumps to a new sentence I've gone back
and forth about whether I want to sort
of continue to process the text and say
well maybe the robot is smart enough to
put in line breaks maybe the robot is
smart enough to change capitalization or
start sentences or lines with capital
letters that's appropriate or maybe we
say you know it's okay for a human
editor to take the Machine poem clean it
up a little bit and say look at this
collaboration this is in fact what I did
a few years ago with a ruby program I
wrote called Bukovsky andrew cope scheme
basically would generate tons and tons
and tons of quote unquote poems i would
convert them into real poems and then i
would send them out to be summarily
rejected
so but it was a lot of fun it's kind of
like pitching conference talks anyway so
that was Keats and Markov chains the
other process at work here when I think
is somewhat more interesting is the
recurrent neural network so we've talked
a bit about neural networks or I have
rather in other talks Joyce gave a talk
on neural networks so I'm going to not
spend a lot of time talking about
machine learning or neural networks I
encourage you if you're interested to if
you didn't see her talk I guess invent a
time machine and go back and see it or
her you know you can wait for it to come
out on YouTube so I'll add some details
for color but I'm not going to spend a
lot of time on the fundamentals of
machine learning in neural networks
aside from neural networks are very
loosely based on biological brains and
I'll show a diagram in a moment but
we're current Norrell networks
unlike feed-forward networks the ones
you normally see where there's inputs
hidden layers outputs or markov
processes which do not have memory
recurrent neural networks do later
layers feed back into earlier ones and
so you get the sort of temporal
dimension to your to your models so this
is what people will use to generate
texts I gave a talk at Ruby kaya this
year about generating music with Julie
and she'll the recurrent neural network
is ideal for things like handwriting
recognition voice recognition things
like that anything were there sort of a
temporal dimension to the data that you
either want to generate or predict the
library that I used DL for J deep
learning for Java you'll see some of the
code it is very Java oriented has a lot
of examples a lot of good tools for
doing this kind of work I'll talk about
this a bit later I do want to get to use
cortex or sort of a more closure first
tool because when you do see a snippet
of the code it is not as idiomatic as I
would like it and as mentioned I
switched from Keats to Shakespeare for
this one which is much larger it's about
a hundred and twenty five thousand lines
close to a million words
and 5.6 million characters and the
reason for this was really there just
was not enough there were not enough
data in the Keats corpus to get
meaningful good results and their
current neural networks it produced some
things that were kind of Keats ish but
it wasn't great so that's one thing that
you will find is here as you're tuning
your machine learning models are working
on anything with neural networks is
struggling to get your data to be the
right shape to be it to have enough of
it cleaning it up etc etc so like I said
this is sort of the general structure of
a recurrent neural network later layers
feed back into earlier ones and the way
that training works in neural networks
is you pass a signal through the network
the neurons have their weights adjusted
at the end you say okay this is actually
what the deal was the Machine says okay
I was off by this much and then
propagates or back propagates the error
signal back through the network adjusts
all its weights and trains again so we
do do this with recurrent neural
networks and we do them in mini batches
and the output here would train on maybe
10 or 20 mini batches produce a sample
train on another 10 or 20 produce a
sample and you'll see in the next few
slides it's truly fascinating how good
how quickly and how good the results
become and how quickly they become good
having said that I am now going to spend
some time on a tangent so I'm gonna take
a quick detour we're still going to talk
about recurrent neural networks we're
gonna cover two of my favorite
relatively recent applications and
they're both by researcher named Jenelle
Shane and I hope I'm pronouncing her
name correctly this first one is recipe
generation so these are the titles of
recipes that a recurrent neural network
wrote and if you can't read them I'm
gonna read a couple of them to you cream
cheese soup cream of sour cream cheese
soup chocolate cake parentheses
chocolate cake chocolate chocolate
chocolate cake
chocolate chicken chicken cake chocolate
chocolate chocolate chocolate cake etc
etc so this was after I actually don't
recall how many training cycles but you
can sort of adjust hyper parameters and
in tune things and if you loosen things
up a bit and let the Machine not hues so
closely to chocolate I guess or English
words it hasn't seen before start to
appear and words that nobody has seen
before start to appear so this is a
little bit tough to read but these ones
are more imaginative we have beef soup
with swamp beef and cheese chocolate
chops and chocolate chips Krim Grunk
garlic Cleese beezy mist export bean
spoons in pie shell top if spoon and
whip the mustard chocolate pickle sauce
whole chicken cookies salmon beef style
chicken bottom star and then just an
asterisk cover meats out of meat
completely meat circle completely meat
chocolate pie cabbage pot cookies which
does sound like a thing artichoke
gelatin dogs
and crock-pot cold water so so this is
truly impressive and like I said the
funny thing is how good are n ends
become considering how bad they are when
they start so as you can see these are
just titles of recipes and eventually
and this is kind of like how the RNN
started to learn is it would produce
nonsense and then titles and eventually
just endless lists of ingredients but
eventually it figures out the shape of a
recipe like the actual form of one
literally on if you can read this this
is just nonsense
this was an early iteration I don't
think I see any words in there there's a
for attempting to pronounce it it's Bui
d 1e C and here's big tha yeah means etc
etc so its initial recipes are not very
good
over time however they become quite good
pears or to garnish Tamim meats 1/4
pound bones or fresh bread optional 1/2
cup flour 1 tsp vinegar 1/4 TSP lime
juice 2 eggs Brown salmon in oil add
creamed meat and another deep mixture
discard fillets discard head and turn
into a nonstick spice for 4 eggs on to
clean a thin fat to sink halves brush
each with roast and refrigerate late art
in deep baking dish in chicken stock for
me to read chip X sweet body cut with
crosswise and onions remove peas and
place in a 4 D gge serving cover lightly
with plastic wrap chill inter frigerator
until casseroles are tender and ridges
done serve immediately in sugar may be
added to handles over ginger or with
boiling water until very cracker pudding
is not
yield four servings and it's just
amazing to me the the parts where it has
seen enough of it to be like this is
what a recipe is and then other times
where it just kind of becomes inventive
but you will yield four servings is my
favorite just this whole nonsense recipe
and I'm gonna read you just one more
this one is very shortly coming back to
Shakespeare this is one I think is sort
of maybe something Shakespeare might
have had if all he had was weird gruel
1/4 cup white seeds 1 cup mixture 1
teaspoon juice 1 chunks 1/4 pound fresh
surface 1/4 teaspoon brown leaves half a
cup with no noodles and one round meat
in a bowl and like I said this is all a
Janelle Shane's work there's a link to
the bottom to her blog which i think is
uh that's the daily dot but her blog is
that on the next one it's I think Lewis
and Clark so this is my favorite recipe
we're gonna do one more before we come
back to Shakespeare so the same
researcher Janelle Shane was told that
there was a problem in in her I think
she was you know if not her City at
least somewhere nearby and the problem
was they had a guinea pig rescue and
this is sort of sad this kind of happens
when you have sort of like a hoarding
situation you know they they go and they
clean out the home and they get all the
poor pets and then they bring them to
the guinea pig rescue and then people
adopt them and some people name their
own guinea pigs but it turns out that if
you have a ton of them and you kind of
want to keep track of which one is which
you just have to as the rescue named all
of them and they had so many that naming
them individually was sort of onerous so
the rescue reached out to Janelle and
said hey can you help named these and
she said I have just the thing
so the recurrent neural network came up
with guinea pig names so this one gives
it away a little bit but that one is my
favorite so these are pop chopped and
Fuzzle
this is a buzz berry and after pie and
my personal favorites are flurry white
and star guten so I'm sure at some point
I will you know have a child and name
that child star guten and then they can
watch this talk on YouTube and they'll
know why so all this to say you know
it's it's amazing how bad the neural
network is at first and how good it
becomes and this is really kind of crazy
and both of these were done with a
project char RNN by Andrey Carpathia and
it's he mentions the he links to this
project in a blog post called the
unreasonable effectiveness of recurrent
neural networks which I encourage you to
read it's a it's a it's a really great
post I love this quote though about the
title and the quote there's something
magical about recurrent neural networks
sort of the meaning that we assigned to
what comes out and and sort of how good
the the results can be over time so I
encourage you when you do have some time
to go and check out
Andres blog to look at chart or Annan
and to also look at the deep learning
for J examples that are about to go
through anyway that's enough random
asides and tangents we're gonna come
back to Shakespeare do you guys want to
see what our our an n wrote alright it's
not it's not as good as the recipes but
it's still pretty good
cool so just to give you a sense this is
some of the code so this is the
configuration of the neural network as I
mentioned it is very very obviously Java
Interop there's a lot of Java asked
names
there's the threading macro makes it a
little bit nicer but it's still not
quite the way I like to write closure so
I'm excited like I said to eventually
move on to projects like cortex or to
have sort of a closure first solution
so that's we'll talk about that a bit
more but that's hopefully in the future
so this first round from the RN and
trained on the works of Shakespeare it
is after ten mini batches so as I
mentioned we do about your batches we
see how we went and we keep going so
it's a little tough to read but I'll try
to pronounce the first couple lines LOI
theand watch us there and
hitomi no in Cindy's belt fill vehicle
vo sir say arch in plaid my art etc etc
now this is total nonsense
this is not Shakespeare but it's
fascinating to me that already if you
look at this it starts it already looks
a little bit like part of a play and I
sort of fib to you a little bit the
Shakespeare corpus is far more plays
than anything else so this is going to
be more of a play or a prose poem than a
more traditional poem like a like a
sonnet but this is amazing to me that it
is pronounceable the network has already
figured out how to stitch letters
together in ways that sort of makes
sense you get words almost words like
Worchester you know you don't get like
nineteen X's in a row or a bunch of
numbers it knows how to break words up
into pieces you have white space you
have words that are roughly two to what
twelve or fifteen characters there this
is almost like English already at least
in a very blurry way and after those
first two words it uses a comma
correctly for some definition of
correctly like it does know where commas
go they don't appear in the middle of
words it's it's really fascinating to me
that even after this short amount of
training you get something that looks
from a distance like language now this
one is after fifty mini-batches asked
being Billy Arthur that is Fay that hath
a beg word and the face
exempt rods let linger meet la fille
yearns to with honors by the Gloucester
Hensley you can't cut these or word
plants was he like misprints dead will
stab brother he will set the x8 now this
is almost English you know we have
things that look like sentences now that
is faith at half a beg word like you
could imagine that Faye and beg where
are like weird Shakespearean slaying
that we just don't know what that means
but sure this is a line from the
Shakespeare play it's almost a sentence
it's shaped like a play you have now
things that look like stage directions
you know exempt you have austere
speaking so after only fifty
mini-batches we now have something that
resembles the structure of a play and
things that actually look like sentences
that are not just pronounceable but
there are English words in here and the
face like that is a weird exclamation
but it makes sense you could say that
and the face
I don't know when you would say that but
that is a legitimate English utterance I
like I said I'm just fascinated by how
quickly this becomes pronounceable now
this is after a hundred and sixty mini
batches and and this is what I think is
really truly amazing Silvia the sleep
villain so shouty I desire like his
Kurtis ears thrice of cursed and for
lady doth be honoring broke of this and
so who'll a subdue my comforts Queen
Elizabeth where is Booga de or Boogedy
or whoever so this like I said this is
truly truly amazing to me I feel like
you reeled this again you could put this
in front of middle schoolers and they'd
be like yup that's Shakespeare you know
I I don't know what it means but you
could I actually do want to see that
experiment I want to see somebody like
run this and put it in you know kind of
slip it into the readers in middle
schools or high schools and people just
kind of go through and say okay I wonder
with this I would love to see a paper
about what this play is about that's
what I would like to see either written
by a human or another recurrent neural
network
so honestly I you know I think well done
right like this is pretty solid and
again it's it's almost I think on
reasonable effectiveness is how it would
describe it this is unreasonably
effective this training took a couple of
hours on a 2012 MacBook Air with no GPU
just you know sitting and churning and
thinking and it barked this up this was
not run on a huge AWS cluster this
didn't take two days it took a couple of
hours with machinery that's years out of
date and I think the interesting thing
about all of this bringing it back to
the discussion of poems about machines
poems that are machines are readable by
them when we sit down to to neural
networks or build models or write poems
or write closure programs those are all
the same thing those are the same
process we are stealing we're collaging
we're making something that wasn't there
before from parts that were and I just
find this really fascinating and I would
if I had the opportunity love to stitch
together some kind of course or talk or
series of poets and programmers together
producing almost Shakespeare from
Shakespeare so we're getting toward the
end of our time probably to talk for
just about three more minutes which
means it's time for the summary the TLD
PA or too long didn't pay attention so
it's fine so if you if you were not
paying attention and you're just tuned
in now you're gonna get the whole talk
in about 60 seconds so we talked about
poems about machines we talked about
poems that are readable by machines and
then looked at some poems quote unquote
poems written by machines with a detour
through guinea pig names and recipes so
as promised I don't know if you saw the
tweet John Keats did say my name from
beyond the grave so that was cool there
was at least one guinea pig which was
great we saw that poems are emotive
programs and programs are poems for the
machine these are one in the same and
that poems candy programs and machines
can write them which is
so we saw Markov chains we saw her
current neural networks I think in the
future what I'd love to do I mentioned
this before they spent some time with
cortex translate this code into
something that's more idiomatic easier
for closures to look at and say yes I
want to do that if we want to do machine
learning if we want to write poems or
recipes or who knows
I think Gilbert and Sullivan would be
amazing and we want to do it in closure
we have to build those tools so we have
some of them now we have a lot of
leverage in the Java ecosystem on the
JVM there's a lot of great libraries a
lot of great tooling and I think the
tools that we can leverage the new ones
we can write will mean that we get to do
this kind of thing in closure all the
time
and that's really what this is all about
so like I said I think cortex is the
next app for this so thank you to Joyce
for her talk yesterday I think I had
heard of cortex at closure con and icons
rather gyro closure but amazing to kind
of see the code and understand more
about how it works and my referencing
her talk definitely does not mean that I
finished these slides 61 minutes ago
anyway we are right around 30 for 35
minutes
that's all I got so
slides will be up on speaker deck code
will be up on github either later
tonight or tomorrow thank you again for
coming to my talk I really do appreciate
it thank you for choosing to spend your
time with me and if we have time for
questions I'm happy to take a couple yes
Lovecraft in Whitman that's genius so
the question was have I ever thought of
mixing two authors yes
actually as I mentioned there's a review
program I wrote a few years ago called
Rakowski I trained it on a combination
of my poetry and Keats and it wrote some
very fascinating poems then I trained it
on my poetry Keats and Allen Ginsberg
and it wrote a poem that to this day one
of my good friends still quotes the
title Jesus the bloody toilet that poem
was not good yes hmm I would love to be
the change I want to see in the codebase
so if they don't implement rnns and I
can help with that I would be delighted
going once twice if you have your hand
up please wait oh yes
for things like yeah yes that would be
amazing although I worry that there
would be some liability in there but yes
this is the kind of thing where you have
like repetitive texture I would love to
see it right restaurant menus if you can
write recipes it can write restaurant
menus I mean you would probably get BZ
missed or whole chicken cookies yeah I
think I think there is you know or when
I gave a similar talk every beak idea
about generating music someone came up
and said hey like you know how do they
generate like hold music now or elevator
music is that something that in five or
so years machines are gonna do for us I
think the answer is yes I think I can
take one more question please wave if
you have your hand up cuz my vision is
terrible
alright thanks everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>