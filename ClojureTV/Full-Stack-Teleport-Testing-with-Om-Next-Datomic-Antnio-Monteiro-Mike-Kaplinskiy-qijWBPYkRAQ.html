<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Full Stack Teleport Testing with Om Next &amp; Datomic - António Monteiro, Mike Kaplinskiy | Coder Coacher - Coaching Coders</title><meta content="Full Stack Teleport Testing with Om Next &amp; Datomic - António Monteiro, Mike Kaplinskiy - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Full Stack Teleport Testing with Om Next &amp; Datomic - António Monteiro, Mike Kaplinskiy</b></h2><h5 class="post__date">2017-03-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qijWBPYkRAQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you thank you good morning so I'm
telling you this is Mike we're here
today to speak about full-stack testing
with foam necks and atomic and this talk
is about how we do testing at Ladder and
what processes do we use to make adding
features quickly and iterating as a
small company this is not about
releasing a new framework actually
currently the implementation that we
have is a little bit tight or internal
code base but we hope to inspire you to
at least do some of the things that we
do at ladder
so Mike will start off giving a little
bit of the motivation why we need such a
thing and I'll pick it up eventually and
speak about the process cool so as I'm
Tony I mentioned again I work at a
little company called ladder ladder is
rethinking the way that ice insurance is
sold particularly it's a completely
online experience there is no agents
which is the standard in industry the
only way to get life insurance is
usually through an agent and we offer
instant coverage which is pretty great
because normally a process that's like
eight weeks we take it down to eight
minutes
so you can get life insurance and the
time it takes you to brush your teeth
and as always we're hiring but as any
closure west snd we find through the
whole closures stack
we're on closure script on the client
with all next as our UI layer we use
transit to talk to our back-end which
runs closure and atomic so we're into
end closure this talk is actually a bit
of a riff on a talk that our CTO gave
last year closure West which was full
stack testing with them next that one
went into detail of how we do what we do
with them next and we'll give a bit of
an overview here but the main goal of
this talk is to see where we took the
framework and what cool things that we
did with it and the answer that question
actually surprised us a little bit
but before we get into the
implementation I wanted to start with a
truism
nobody likes shipping broken code right
when you should broken code it impacts
your users it affects your company's
bottom line but it mostly impacts
engineering right not only do you not
have to fix a bug that has made it out
into production you might need to fix
any of the data that's now corrupted due
to the bug you have to stay late at the
office to do this because this is time
sensitive the entire engineering team is
under a lot of stress when this happened
so
really what you want is that little red
X for those not familiar with github
that's github CI marker so continuous
integration you want some sort of
process to prevent you from pushing code
that does not work and the best way we
have of doing that is testing right but
not all tests are created equal right
just because you have unit tests doesn't
necessarily mean that your application
works the way that you expect it to when
all the pieces come together right unit
test says things in isolation but
clearly that doesn't always work so unit
tests aren't enough going back to the
problem that ladder is solving ladder is
a little bit different than a lot of the
large internet companies like Google or
Facebook or Twitter
those companies if you ask them a user
them is worth something between like
five to fifteen dollars a user right so
when you roll out a canary release and
maybe break the experience for 1% of the
user base you're not losing that much
money at the end of the day it's a loss
that you're willing to take at ladder
we're selling you life insurance you
might be buying a policy that cost
something like twenty to thirty dollars
a month and you're paying us for the
next twenty years so the the price that
we give to a user is easily in the
thousands of dollars that's not
something we want to just throw away
another corollary to this is something
like 90% of our code actually
gets run for every single user a lot of
our code deals with going through and
getting to the point where we can offer
you life insurance and whether you
accept it or not so the code that
actually deals with this giving you a
quote going through the application flow
verifying your identity generating PDF
underwriting the user all that stuff is
run for every single user so we need to
have really really high a really really
low bug rate for that because if
anything breaks in that flow the user is
stuck there's nothing else they can do
we have a very linear flow so the
problem that we want to solve is that
bugs for us actually kill the user
experience so we need tests that can
catch those bugs in life insurance we
really need to ensure safety and privacy
of our customers at ladder
we believe that correctness goes a long
way to achieving that so we need as much
coverage as we can get from tests to
give us as much confident as possible
that a users personal and health
information is not at stake and as a
company that iterates very quickly we're
always adding new features and the
specification ever-growing we near it
has to be end to end so that the entire
stack is exercised we need them to be
decoupled so that we when we add a new
feature some tests in the middle of the
stack doesn't break or we don't need to
fix it we need them to be lightweight as
well because we're running this test on
every commit or even before committing
planning them on our local machines and
we want them to be easy to write just
because nobody likes writing tests let's
take an example a very simplistic flow
through our application as an example
say user goes to water life calm clicks
the gap will bottom fills out their
personal information
and see the quote which is this one so
this is a very simplistic flow initially
we chose it to to model this it's just a
sequence of function calls returning a
resulting state that we can assert on so
right there you see we're calling
something called the starting cash
universe we call a universe for us if
something that has the entire state of
our system so it might have at any point
in time the database connection or email
senders or cash coworkers all that we're
threading that state through a bunch of
calls to opening a new browser tab
loading a route in our application
performing a mutation a state transition
that submits a quote and asserting
something on the resulting state and
this is what we started with but we
quickly found out that it's not really
helpful to reuse combinations at this
steps so say we want to take this and
get farther away in the application flow
the way that we chose to model this
initially did not allow us to do that or
not easily so this is also not easier to
write so we want something better and
better in our use case is something that
lets us match data at specific points in
the app flow with our expectations and
we want to describe the steps that a
user takes in our application the
collateral is typically as data
essentially what we want to do is model
the user so we chose to describe a user
action an action that a user takes on
our application simply as a keyword so
lower out is a keyword open a new
browser tab at the root endpoint is a
vector of keywords and argument
a sequence of steps in this model would
look like this we open a new browser tab
at the root end point we load the route
ya quote what this lets us do then is
create a new and a new sequence of steps
that reuses the first one that we we had
to get a further way in the flow to
assert new things so essentially then
the new browser call we you you see
there is exactly the same as we have in
our previous slide with the exception
that we're now we don't need to create a
new test universe the framework takes
care of creating that for us and
threading it implicitly through all the
steps that we write emulate here as a
state-transition and this refers to
flicky specifically to own next so we're
going to go to a very quick whirlwind
tour of connects to help us better
understand how this works so on next is
basically just a react wrapper written
in closure script but what it provides
additionally besides being a react refer
is something a component that we call
the parser you can just think of the
parser as a pure function that takes a
state and the query and the query is
just a way to express our UI component
requirements so you take that and the
parser takes care of hydrating that
query with the component the data that
our app state returning the UI data tree
that we pass to the react render
function one thing to note about the
parser is that it sits at the edge of
the system and so it provides a way for
all next to do a bunch of optimizations
and also as a symbolic refer yesterday
in start it abstracts a sinker
from the application developer a query
in Olmecs can be just read which means
give me a person's name or it can be a
mutation a new mutation is just
something that changes the state of the
system they're effectively refight state
transitions described as data the parser
executes them both on the front end and
the back end if they cannot be satisfied
locally and feeds the result of these
mutations back to reiax render target so
in on next the the data tree matches the
UI data tree which means that we can now
treat the Dom as a dumb render target
and abstracted away entirely in our
tasks so the separation between getting
and transforming data that is in the
parser sending it to the rendering
target is what enables us to just ignore
the render target entirely and this is
where we hook our tasks so we write our
own next parser in CL JC and we run our
tests in a single JVM process we mock
out HTTP servers and HTTP requests and
every step of the flow we execute as we
execute mutation against the client-side
logic we send the mock request to the
server if need be we store the data we
get back from the server and we can make
assertions on that data at every step in
the flow one thing to note is that this
only works if the majority of your logic
your client side logic is in the parser
which happens to be the case in all next
it doesn't it it suggests you you put
all your logic in the parser and treat
the react or whatever rendering target
you're rendering to is just a function
from UI data tree to dog
cool so now that we have this little bit
of like a step framework what do we
actually get from it and the answer
actually surprised us a little bit the
first thing that we got is actually the
name of this framework which is teleport
the reason why we call it teleports is
because originally when we wrote our
tests we had an idea of just taking our
tests and running them in order to move
our development server into a particular
state so for example if you wanted to
fill out the quote form what you could
do is instead of using a set of tests
fake universe you can use your local
development servers universe right
you run the teleport as you normally
would you take the cookies and the
client state and send it back to the
browser so for example in your local
development what you could do is open
the ladder app and click a specialty
combo that we have for our development
which gives you a list of teleports and
you can run one of them so the cool
thing here is that this teleport that we
ran here is actually going through the
entire application as well now if you
remember correctly I said it takes us
around eight minutes to get through this
flow if engineers had to wait eight
minutes to get through a flow and met
and that requires a bunch of typing - if
engineers have to do that I think they
would revolt this teleport actually
takes about 2 to 5 seconds which is
pretty cool but there's another pretty
cool thing about this so this isn't just
a snapshot of what this page would look
like your local state both the
development server and your client-side
state is completely in sync and
genuinely believe that you have a user
in this state so for example if you are
to accept this life insurance offer you
would see what would happen which would
take you to your account page where you
have a policy with us right where it
isn't just like a view of what the state
would look like your local development
server is actually in that state
and that's pretty cool we've used it to
both develop UI and test out certain
flows that are hard to model otherwise
but once we had this little system of
you know what if we run teleport on our
local development server we started
thinking that maybe why just limit
ourselves to development right one thing
we could do is instead hit our local
HTTP server directly we were already
modeling cookies and headers so why not
just hit like use CLG HTTP to hit your
local server and we did that and
surprisingly it worked it actually
surprised up a little bit but then we
said okay well if we can hit our
development server why not hit our
staging server and here we got something
pretty interesting so what we can get
from this is low testing because we
describe our teleports as of the steps
that users would take when they go
through our website all we have to do to
get load testing is pick some of them
annotate them with relative weights of
how frequently this particular flow is
taken and all we need to do after that
is run them in parallel hitting a
staging server oh and we have load
testing that actually is how we do load
testing at ladder and that's pretty cool
because if you've ever worked at a place
that did load testing it's almost a
dream to have your integration test that
run for correctness also work as load
tests that run to test your server under
load and this framework lets us do that
and that was pretty cool we call this
the remote driver it's changed the way
we do production readiness it's pretty
great so that's one thing that we get
which is pretty cool another one that's
pretty easy to see also is once we
started doing load testing which is
parallel HTTP requests parallel
teleports we were thinking maybe we
could just run our tests in parallel
pretty easy right turns out it works
we use the atomic as our database the
atomic has an in-memory implementation
and you can create as many of them as
you like so each one of our tests gets
its own database and it just works if we
didn't really have to do much but there
are some caveats of course you have to
make sure if you're running tests in
parallel that all state is local to
components we use Stewart serious
component library so things like mock
Kafka and fake s3 need to be
componentized we can't use global state
luckily the own next parser doesn't use
global State is just a function so we
don't run into any issues there but
consider something like with redux with
read F is global state it's not just
binding so you have to be really careful
to not use it because the errors that
you get when you mess with local state
and parallelism is are frightening to
say the least but assuming you do all
that which it wasn't terribly hard for
us you can just grab all of teleports P
map and you're done right you all of
your tests are running in parallel so
one of the things that we we actually
did this and one of the things that we
saw pretty early on was that we were
doing a ton of duplicate work and what I
mean by duplicate work is the ladder
flow generally all the possible ways
that you can go through an application
on ladder is pretty limited from a
client-side perspective so a lot of
flows would go through the application
and then fork depending on you know what
they do next and this actually ended up
taking a lot of time we were rerunning
going through the application fifty to a
hundred times every time we built all
the teleports and that wasn't great it
started slowing us down but I'll get
more to I'll talk more about that later
but before we get there an interesting
thing that we got from our teleport
testing is what we call snapshots
snapshots are kind of like static views
of particular points in our teleport so
what we could do is annotate a
particular step and a teleport with a
name
so like for example one of these
teleports has an annotation called
decision which was the page that you saw
previously what that is is basically a
point where we look at the globe at the
client-side state and render it to react
to a react render target both on the
client side and the server side so CR is
client rendered and SR is server
rendered
we use them next then next to the server
side rendering which is pretty cool we
just actually just say render the same
thing that we do on the client and what
we got from this is actually a little
bit surprising our designers would use
this as the source of truth for figuring
out what the different app states look
like whenever they need to find out what
they look like right instead of going
through the flow hundreds of times to
figure out what we might do you can just
look here so in a lot of ways this
actually replace a lot of usages of Def
cards for us
the reason being def cards for a little
bit hard to maintain for us
because they weren't running as test in
a lot of ways def cards would not
represent the current state of the world
you know they would be passing
parameters that would be completely out
of date for that UI component so instead
using snapshots which run as tests we're
sure that there's no way that we can get
out of sync our the UI that snapshots
render are exactly what users would see
so as I mentioned before not everything
was awesome building all this stuff
which is one of the steps in our CI
pipeline started taking a lot of time we
were generate teleports on every commit
and it was starting to take 8 to 10
minutes
when we started looking into it to
regenerate all the teleports now 8 to 10
minutes isn't like a terrible amount of
time right we're not talking days here
but consider that this happens every
time you commit we were on tests on
every commit and moreover we make sure
that when you merge into master you're
up to date
so not just conflict for us master but
up-to-date so as soon as you have
several PRS out there trying to be
merged it takes a long time before you
see or commit murder tests take more
than 10 minutes to run on each one so
this was starting to suck a little bit
for engineers so what do we do we're
like well maybe we can fix this we know
we're repeating a lot of work right
we're going through the application
5,200 times when we can go through this
like twice at most so wouldn't it be
nice if you could just kind of like
cache it right we have this like run
step function that runs a step on a
universe wouldn't it be nice if you
could just like put a memo eyes on it
and we're done that'd be cool right
turns out you can't quite do that one of
the things as antonio mentioned that we
keep in our universe is the atomic
database
it's an a' memory database but it's
called a database and turns out you
can't really hash a database especially
one that's mutable we're sort of aware
of this it makes sense you don't really
want to reuse a database between two
different teleports that is live for
both teleports right that wouldn't be
like doing completely disparate things
on the same data and it would just not
work so what could we do
these are Statesville components so we
can't really treat them as values but
there's something in the two locks to
that right what if we could clone them
what if we could copy them right and
we're using CR components so would it be
that hard to just make a little protocol
that copy stuff why not so we try we
were we wanted to try it so let's see
how far we got the first thing that we
figured out how to clone was the simple
stuff and I should note that we're only
cloning test components here we haven't
actually figured out how to clone a real
copper instance yet so the simple stuff
right we have an email client that's
used in test which is a mocking mount
client we don't actually like to send
emails from our tests it just records
all the emails that are sent so in order
to clone it just make sure to copy that
atom or it stores all that stuff
pretty easy right not much they're just
making you Adam
one thing I should point out is because
of closure structural sharing a lot of
the memory here is actually shared
between the two instances of the record
which is pretty convenient when you're
running hundreds of teleports in
parallel so we've got some very simple
records next we realize that our cotr
system map has system Maps in it so it's
a little bit message so we figure it's
probably a good idea to be able to
Chrome system Maps well it turns out
it's actually pretty easy if you look at
Stewart Diaz components there's this
wonderful function called update system
which actually runs through the system
in dependency order which is exactly
what we wanted to just clone it so this
ended up being like three lines four
lines of code it's pretty awesome what
you can do when the library does
everything for you but as I mentioned
there was one little snag that we were
hitting which was de comic what do you
do with the atomic how do you cloned a
Tomic turns out um like all good closure
things there's a library for that so
there's a library called a tome walk by
a guy with a very interesting name Val
Val Val but the the cool thing about it
is it allows you to fork the atomic
connections so what does that do if you
look at the atomic the atomic core
primitive is the databases of value so
that means that when you have a database
instance it is frozen in time you cannot
mutate it the only way to mutate a
database is via a connection so if you
have a database as a value that's pretty
nice right because you don't have to
worry about it changing while you're
passing it into different functions it's
all fixed at a point in time but the
atomic actually exports a little known
function called the atomic API with I
think it was mentioned in the talk
yesterday but one thing you can do with
with is given a database value and a
transaction that you want to run on it
it will give you a new database
you with the transaction apply but not
so the connection that the database came
from it gives you an as if view of the
world so you can ask questions like if I
were to run this transaction
what would the database look like and
that's pretty powerful right
you can do use it for a lot of stuff but
in this case what you can do is fork the
atomic connections all you have to do is
instead of keeping an in memory date on
McDade abase you keep a database value
in an atom and use with in order to
update it right that's actually what de
Tomaso's internally the whole library is
like 50 lines of code and it's pretty
awesome that you can just do that right
we didn't actually do anything here the
the atomic library did everything for us
so you know actually this pretty much is
all that we needed to clone everything
everything else falls into either being
system Maps or effectively atoms so what
can we do can we implement memorize yeah
sure so I mean this is actually very
close to the actual code we use to
implement memoization for our steps all
we need to do is create a key which is
basically where you came from and what
you're about to do to the universe put
it into the cache only once so make sure
you don't overwrite it otherwise you're
just doing the same thing multiple times
clone the system and save the key right
this is actually extremely close to what
memorize does does internal e and the
reality of it is quoted did all the work
for us so we actually did this we did
implement it and the pretty cool thing
that we saw was at the point where we
released this update to the teleport
system our teleports for taking
somewhere between 10 to 12 minutes to
run at that point because people get
batting stuff after we push this change
to just memorize the steps we weren't
even doing anything smart particularly
we just replaced the function call it
started taking about a minute to run all
the teleports so pretty good reduction
I'd say
but then again there are still a few
caveats with our approach and in the
next part I'll be going through a little
list of what those are and explain how
we try and minimize them or even get rid
of them entirely so the first thing to
mention is that our end-to-end test run
in a single JVM process this means that
we don't really get to test the code
that is going to run on a j-f engine for
example while running our full stack
tests to minimize that use case we have
specific namespaces for example
namespaces that do number calculations
which have different behaviors according
to the engine you're running in into and
so we run these these MIT namespace
tests both on the JVM and on in
JavaScript through phantom for example
full-stack tests also mean that they're
not really integration tests what this
means is that by mock way when we're
running our full stack test we're
mocking everything out such as our email
workers the things that post to our
slack channels we don't really exercise
the code that is running in production
but as Mike mentioned before we do have
the remote driver so by finding the our
full-stack test at our staging
environment we get to exercise that code
and see if everything is working
correctly might by by having our tests
run in a single JVM process it also
means that we do not get to test a bunch
of things for example we're not really
exercising the HTTP forcing server code
or the Ajax client or how clicking
buttons in our UI trigger browser events
we think this is actually not the worst
problem we have in to counter days we
actually run the remote driver and
selenium tests
on browserstack before merging into
master and we feel like it has caught
certain little bugs that wouldn't have
been caught otherwise and we're happy
with it in the future we're thinking
about doing making our framework a
little bit better through and we have
some options for that the first one is
integrating closure spec for example as
you saw in the previous slides are our
teleport steps are just data inspect
knows how to generate data so if we spec
out our Cal for steps or whoo-wee spec
out our own next mutations we have the
ability to run generative testing on
mutations that actually exercise a lot
more than just our teleports which we're
still trying to find out what the best
balance on that is because it would
possibly make running our tests slower
and that's what we've been countering ad
hoc or incremental teleporting is also
something that we're thinking about and
that means say we have a production bug
and we know what is the exact step that
took for the buck to occur we're
thinking that if we can pull out the
entire state from production run that
locally and reproduce the bug locally
that would be incredible to find bugs
early and fixing them instantly
extracting these from our internal code
base is also a goal but I want it needs
to be you need to be aware that there
are some conditions there there are the
our code base has that yours may not
have first thing is we use all next but
I suspect you click also implement state
transitions in any other framework the
other thing is we're using the atomic
and all the cloning process might be
needed to be adapted to your
a particular use case there's something
else we're considering doing to even
make our teleports a little bit faster
so as Mike mentioned we're not doing any
repetitive work anymore and so we
actually took the Loom library and we
graphed out the dependency graph of
running out our teleport steps this is
the entire review of it this is the
graph ten times bigger that this is it
100 times bigger
so bolded nodes main steps that are run
a lot more than the others so we're
thinking that if you can go through a
teleport an independancy order and run
all the steps that are common with every
other teleports first we can get those
in cash before any other step is run and
possibly take a lot of speed ups for
free just by doing that there are a few
takeaways from this talk the first thing
is if you've never watched reaches the
value of values talk I highly recommend
you do that we're as he so well putted
we're actually living in space age we
can afford to spend memory and storage
to run tests to store things we actually
store our snapshots with every get shot
so that our designer can go back and
forth and see if there are any UI
regressions and we can afford to do that
and I think you can do so if you haven't
watched the talk I then again I highly
recommend it
another thing we found is the atomic is
awesome where else have you heard anyone
speaking about forking and entire
databases
describing our steps of data included
homo iconicity has also helped us a lot
here this is kind of optional but the
way that we chose to describe our top or
sepsis data converted into code and
implicitly threat our state through them
has helped us immensely in making our
class easy to write describing all next
say transitions this data was also
something that was incredibly valuable
to us and might even be in the future
when we're thinking about generating
tests inspecting out all these things
and then again immutability and the
structural sharing that comes with its
implementation enclosure was has also
revealed extremely invaluable so that we
don't run out of memory on every test
run a a fun take away from from our
process is that we can actually now just
ditch our IDs and just commit go from
github and be certain that our our we
have coverage for that I'm half joking
but sometimes we actually do that most
importantly we feel like although our
application we're incredibly proud of it
and we think it is a very cool
engineering piece of engineering
we feel like in 2017 everyone can build
a web app and that's not we're most
proud of
we're actually proud that applying this
process having this incredible amount of
coverage is what makes what makes it
possible today that we can iterate on
our application add new features do
large refactorings and this has been so
simple and so faster and everywhere else
I've seen thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>