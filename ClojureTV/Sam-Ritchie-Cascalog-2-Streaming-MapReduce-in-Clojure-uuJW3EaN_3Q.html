<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sam Ritchie - Cascalog 2- Streaming MapReduce in Clojure | Coder Coacher - Coaching Coders</title><meta content="Sam Ritchie - Cascalog 2- Streaming MapReduce in Clojure - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sam Ritchie - Cascalog 2- Streaming MapReduce in Clojure</b></h2><h5 class="post__date">2014-01-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uuJW3EaN_3Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um yeah that's i was at Twitter until
last friday i'm now doing closure again
which i'm very very happy about so i'm
going to talk about casca log which is
this open source library I've been
working on I guess on and off for two
plus years now I also threw some buzz
words in datalog real time just to fire
you guys up I they do have to do with
what I'm talking about you know I used
to be a closure asst went to Twitter
sort of was hoping that it was fully
embraced I was pulled off into the evil
world of Scala and have been working
with types for some time now now that
I'm out I want to use the tech I used to
use but it was completely based in
Hadoop and MapReduce it was all this big
data stuff and paddle guru is like race
management for paddling sports and it's
not big data yet it's not going to be
for a long long time okay so the
solution are like preached that or make
casts clockwork for smaller data and for
other models of computation that aren't
Hadoop so that was the goal of this sort
of almost autistic rewrite of the entire
code base I did over the last couple
months and I just want to take you guys
through a few pieces of that new code so
we're going to talk about today what
Casca log is for those of you who don't
know and just kind of going through some
of the ways some of the types of
computations you can do on your data
using this logic programming model it's
quite different I'm actually stoked to
be at the closure conference where like
logic programming is not scary but
monads are but
it's hard to keep track so why logic
programming you all know already it's
amazing the analytic story I think is
really really good enclosure a lot of
the tooling is amazing I think this kind
of this level of abstraction has been
fantastic for working with data at
Twitter scale but you know within sort
of we've seen a lot of the applications
of logic programming even in the browser
with some of the work David's been doing
and yeah it's just bad ass and then you
can just you know tell people you're
coding the web apps and logic and it's
so good we're going to talk about how
the compiler works how cows clog
compiles this data log graph down into a
logical graph and then eventually down
to Hadoop and some alternative for
compilation targets that this new
rewrite will support we can do sort of
more interesting things now by being a
little lazier about the way the
compilation works and then what's next
and David I see you there the real goal
my talk is to make you smile I see ya
and we're done
okay what is Casca log what am I talking
about casca log is a daily log dsl
enclosure that helps you write until now
Hadoop jobs but primarily big like ETL
or analysis jobs on you know many many
like large logs or streams of data so
some of the questions you might be able
to answer more easily using cows clogged
would be go find me all the you know
users in the northwest who like to
paddle who are under the age of 30 who
do this you you express this in a logic
language is just facts about your data
set and let the system figure it out in
raw Hadoop you're sort of imagining key
value pairs and shuffling your data
around and you can reuse the stuff but
nobody does and it's it's really
horrible so yeah you give this is some
facts and it tries to write these
analytics pipelines for you and if there
are better ways to do things the system
can go optimized for that you don't have
to worry about it a lot of these
optimizations are really systems
problems and with a bit of knowledge
about your data the system can tune
quite a bit because of the dupe support
you can do the sorts of things will look
at at enormous scale I've never actually
worked at petabyte scale but I think
it's possible gigabyte terabyte yeah
you're effectively not constrained and
if you think this way you're setting
yourself up for growth in the future the
problem is that it's been Hadoop only
until now so we've got this great
scaling story but like you often you
want the scalability without the Hadoop
you just want to be sort of guarded
against the future so we want in-memory
modes we want to be able to do maybe
real-time streaming things like that so
let's see some code this is what cows
you wanna see it yeah all right I don't
know what this says this might be like a
Chinese tattoo that says like off
but it's real I don't know this is what
kaskell are this what Casca log looks
like we're going to go through like
exactly what this means but effectively
this is a little logic program that's
embedded in closure which tells the
system how to calculate based on some
generator of like sentences how to
calculate word counts so this thing
itself is not doing anything it's just a
declarative structure that cows clogged
can go turn into a Hadoop job down the
road there's also a serializable
function in there it's pretty cool that
that's important to generalize this
thing but yeah this thing turns into a
Hadoop job but there's nothing really
hit dubash about it so this is how you
do the same sort of thing the MapReduce
word count in straight closure I know
there's more elegant ways to write this
but this just sort of models what Hadoop
would be doing a good effectively what
this word counter does is takes in some
sequence of strings map cats across them
to split them up into words ads like a
count on to each word so you end up with
these pairs and then does a group by the
key and adds together all of the values
so it's very sort of you know you're
writing and here the steps that you need
to take you know you could pull some of
this out into helper functions and make
a nice or DSL over top of it and many
people have done that and it's great but
this thing will go count words it's not
easy to generalize this to sort of other
types of aggregations but it does the
job this is what that same thing looks
like in Casco log will go through this
more detail in a minute but effectively
what you're doing is just telling the
system three facts you're saying some
generator comes in something that can
sort of produce tuples this second line
here this like Jen says there's some
concept of a variable called text and
we're constraining it to have to come
from the generator data set so this
thing will sort of the system what cows
Kellogg does is takes these constraints
and tries to solve them given all your
inputs so when you say text come
from generator that takes like every
possible text that could happen
constrains it to come from this one data
set then you're constraining a new free
variable called word to be the result of
like splitting text into words so this
this one's kind of maps to the closure
but it's fundamentally like a different
way of thinking you'll notice the split
and the generator out of order split
showing up before there's anything to
work on but the system will figure it
out for smaller stuff it's still
compelling it's still competitive with
the closure but for more complicated
jobs like this one which is one of the
first castle inquiries I wrote actually
you know it's hard to compete what this
is doing some of the first work I did in
cask log was satellite data processing
so what this thing does is takes a
generator of like NASA modis satellite
data sets and pulls them apart like
extracts metadata puts them into a
thrift format like does all sorts of
aggregation its many MapReduce jobs
eventually kicks out sort of this
normalized chunk of data that we can
ghosts or long-term in our system and
you know if it was bigger like it'd be
beautiful would be clear to read but
this is sort of the logic and you can go
look at this thing and if you know the
Cask log idioms you can understand the
facts that are being used to generate
your data okay so quick overview what a
cask log query is they all have the
shape it's very very consistent and easy
to read once you're familiar with it
everything is Casca log first class
structure is a sub-query it's declared
this little left arrow and what those
look like is you give some sequence of
output variables that you are promising
to sort of create with your constraints
and then a list of one or more
predicates sore constraints on those
things those constraints can be
generators they can be operations they
can filter they can be aggregations at
the end of the day the system will
figure out what to do and pick the ones
that it needs to go generate your output
variables so in our example the word
count the sub-query said I promise I
will create constraints for the
variables word and count and the logic
variables are prefix of the ? anyway
here's a generator text is like coming
out of this thing or alternatively you
can say the text variables like
constrain to come out of this generator
this one's an operation it's saying
basically you can make words by finding
every possible solution to splitting the
text and then I don't really have a good
metaphor actually for like where
aggregations fit into this but
effectively what you're doing here is
saying I want to go create like a every
grouping on word go figure out like what
the count is for that particular
grouping and then this sub query itself
is actually a generator that can be used
in other sub queries so you can lock
away logic and sort of build up these
very very complex things you don't
really know where text came from you
just have a generator which could be
some Hadoop structure or some other
computation that you haven't actually
made yet so you can make these like
enormous data graphs without actually
doing anything which is something I'm
quite interested in um all these
predicates have the structure some
operation some set of inputs input
variables to the function or so and then
this little keyword and then the outputs
these are just data structures there's
nothing sort of special going on at the
end of the day you you know the
sub-query macro is just a little sugar
over turning the ? prefix variables into
strings so yeah you just give the system
a bag of these and it goes and figures
out what to do yeah so word count as we
saw before it's just a function that
returns one of these their first class
you can pass them around you can go
construct them we're taking in the
generator and then defining the sub
query which creates word and count as i
said the return value of this is like a
totally valid generator in its own right
so let me show you if i can do the emacs
switch oh I can't see it my screen but
it's going to work see there's the mouse
oh yeah
okay so here's the word count query we
just looked at and then you can go say
sorry I'm trying to type by looking at
the screen oh god here is here we go so
you can see the bottom maybe this is the
like closure version this is the cask
log version it's the same code I just
showed you and if I run this in the
repple that's a Hadoop job that just
launched in the rebel and generated well
we're counts for like two sentences but
it's still a Hadoop job in it it worked
in the rebel it's amazing I sort of a
side had when I showed up at twitter how
to wrap a liturgy aab tracker and would
play like this to show people and
up at one point and like hose the whole
cluster for the entire night like from
my emacs console that's doing this
nobody knows happening there's a victory
okay something a little bit harder
harder for me to think about anyway I
tried to write this a straight closure
and immediately gave up and worked on my
slides more find the full name of every
person following someone under 30 this
this kind of looks like sequel in fact
they're quite related given these tables
so I have an age table of person comma
age I've got a follows table of follower
in person and then I've got this like
full names mapping so if you have all
these things the question I want to ask
of Casco log this example I'll show you
is let's find the full name of every
person who's following someone who's
under 30 so you can kind of feel the
constraints like mapping you know in
here here it is we've got full name that
we're promising to create that's what we
want as we said as the problem statement
described each of our generators can
produce some set of logic variables so
follows produces follower in person
that's just a statement about what we
have full names produces follower and
full name age produces person in age and
then we have this final constraint that
age has to be less than 30 right so and
then it's like little option saying at
the end of the day we only want the dis
outputs now there's no join going on
here explicitly but implicitly because
we named different variables the same
thing the system will figure out kind of
possible solutions given that these are
the same entities so as you said before
these are all generators this is like a
little aggregation option and this is a
filter but the ordering doesn't matter
we're just telling the system a number
of things saying figure out what the
full name is and the results of that are
I'm not going to do the rebel again but
the results sure enough are just these
five names actually what the hell let's
do this okay we've got all these data
sets I actually want to show you this
because you can do some extra cool stuff
okay so that's the following query if I
run that sure enough I get those results
what's interesting is you can actually I
don't know if you can see this you can
actually just in line constraints so we
can say you know who's following someone
who's 28 that's an interesting question
and then we can run those in parallel so
this is the output of this is the output
of both of those run in parallel on
Hadoop in the rebel so we have you know
all of these and then here we've got the
first one was slightly fewer because
it's only people 28 this is people under
30 so it's really really easy to go play
with this stuff you can take existing
code and just make functions that take
these little filters and return them
it's no problem is really kind of a
beautiful way to express logic the
problem is the set of abstractions that
it sits on like so yeah yeah as you go
up the abstraction layer the emoticons
become happier
this is actually it's kind of a kinky
Hadoop elephant I don't know where I
don't know if this is official or what
but I liked it so much if you yeah
anyway so yeah Hadoop to the bottom
cascading is this Java library that sits
over top that makes a little easier
closure in Castle Lager on top of that
and they go build these MapReduce jobs
but at the end of the day like all
you're thinking about is the closure
there are hooks to get underneath
because we can't express all of the
subtle arcane configuration options that
Hadoop has to offer but for most things
it's you can stay at the top level so I
think there's a with the Hindu legend
about the world riding on an elephant's
back cascading does the same and the
goal the question I had doing this
rewrite and that's kind of what this
talk is about was is there anything
about the data log that's specific to
Hadoop or could we target other
compilation platforms spark for example
lets you do the same sorts of things we
just looked at but it holds everything
in memories you can do these really nice
iterative computations storm doesn't
have a logo but this is a storm storm is
a real-time storm is a real-time
processing system you could imagine like
using that data log graph as a
constraint solver which given some time
window is operating on your tuples all
at once so as things come through if
it's a person who's following someone
who's less than 30 kick it out into a
future queue or write it into a key
value store like these are things that
make sense but the older version of the
library everything was coded in turn it
sort of eagerly compiled into Hadoop and
it was very hard to break that boundary
it's an open source project I've
maintained it for a while and I figured
that you know as I was sort of suffering
in the Scala LAN I did like it I could
go ahead and just you know push a pull
request to the code that rewrote
everything and it turned out you know
the other maintains accepted it and so
we have this new compiler that I'll talk
about in a minute I didn't want to say
like because it's open source a lot of
people are using this now it is out
there it's useful it's on a doob
including the climate Corp which was
acquired for
billion dollars primarily because of
this technology I think yeah it's
unclear I actually did find the article
about the acquisition and control f for
Lisp and nothing and then I foreclosure
and nothing and then castle so the sad
moment last night but yeah open source
project you should all use it and quick
aside why datalog like why why should we
think this way it looks useful but is it
really that helpful when you get the
real stuff and the reasons I would say
our one your users will be happy yeah
hey I I had to get this in again I've
been reading this bike I'm sort of
appealing to authority here I've been
reading this book by Fred Brooks called
the design of design which is fantastic
he wrote the mythical man month and like
every page is just and the guys had a
long like battle-hardened life designing
things his comment that all appeal to is
when you specify something to be
designed tell what properties you need
not how they are to be achieved this was
about clashes between you know sort of
design requirements that humans have
from each other saying like I need you
to solve this problem I needed to be
using Hadoop I need this it sort of
inhibits creativity in a very painful
way I've started to come to the
believing that it's the same way with
how you know the computer works as with
these massive data pipelines like you
don't want to be micromanaging how your
system runs given that there's still
this arbitrary abstraction layer if a
dupe right like it's still key value
pairs on MapReduce so to sort of try to
pretend that you're doing sequence
operations and fit into that paradigm is
ok but like you're going to it up
and yeah I think of it as like
micromanaging your data and often what
would happen at Twitter was you know we
write in this language called scalding
which is sort of looks like the Scala
collections API but turns into Hadoop
some poor young engineer would write a
job and like realize a list in memory
instead of taking off the front in
advance and one of the Greybeards would
look at the code and be like why the
 would you do it this way like what
what were you thinking I don't know you
know it's these sort of decisions about
how the system does its business are
hard
do anticipate and especially when you
start switching compilation platforms
like for a lot of your logic you want to
be as high up as possible so that storm
can figure its thing out Hadoop
configure its thing out you know you can
jump back down if you need the specifics
but at the end of the day like stay high
level as long as you can I think that
you know we get a lot of leverage here
out about working in terms of properties
of our data I think the Scala guys are
so jacked up on types because when you
know the out they are when you know the
algebraic properties of your data
structures I would say you can do a lot
of reasoning at the system's level and
this is the same sort of thing we have
the power to do that about our data at
the application level a lot of the
schema stuff yesterday was really
exciting because I think you can kind of
get the best of both worlds you can
actually start to talk about the
algebraic properties of your data and
you know anyway that's sort of next
steps okay how does the how does this
thing work like I don't actually know
this till I rewrote it so I may have
screwed this up and I actually if anyone
who knows how to write a compiler will
like tell me I this up I'd be very
very happy but we're going to talk about
how my version of the Cask allow
compiler works I think it's pretty
reasonable so this is the kind of sub
quick what we want to do here what Casca
log does is takes a sub-query that looks
like this and turns it into a logical
graph that you know is doing effectively
can be turned into one of those closure
stream transformations or Hadoop land or
whatever so the way this works is like I
said we've got we want to satisfy this
full name we want to basically like make
a new generator which generates full
name and we've got these five predicates
to work from so what the cast Scala
compiler does is just takes all the
facts and starts to pull out facts that
it can use immediately the first things
that can use are these generators so
there's three of them in here the
follows generator produces follower
person blah blah blah we talked about
all that so those are out now and you
know so we'll cross them off from the
predicate slist they've been dealt with
again it doesn't matter what order those
are in now every one of these generators
has access to all the other predicates
and what they do is try to eagerly apply
operations
to there like any operation that acts on
variables that they contain starts to
get applied and they can you know pull
those out of this bag in this case the
only operation left is age is this
filter HSB less than 30 and the only one
of these guys that has a just that one
over there so I spent a long time in the
slide by the way this is like keynote
magic so yeah so age has to be less than
30 we can't operate directly on
constants so we insert this node in the
graph that inserts 30 into the flow and
you'll see now I've got person age and
timp-1 we've got this temporary variable
that's floating around as well and then
the filter gets applied so that stream
gets cut down now there's no other
operations to apply except this final
aggregation but like I said you got to
end up with one thing so we have three
here now so what Casca log does what the
compiler does it start searching for you
know common fields that it can join on
so in this case person is shared between
all three things so the system will in
an attempt to sort of further keep going
trigger a join at that point you now
have a generator with these five
possible fields so you know any of these
operations could be acting on
combinations of those and would at this
point it would still keep trying to
apply those operations we don't have any
left so there's only one predicate left
this distinct true by default cask log
will distinct on the output variables so
it adds this distinct in on full name
which cuts everything else out basically
what that is just grouped by the thing
and like take the first thing of each
group and you end up with you know this
nice graph this isn't doing anything
though it's just a declaration of intent
about the system where each of these
operations like that less than node in
the graph is just a record that holds
this function the inputs that it
requires which in this case would be age
and temp one and also like a list of all
the possible outputs that are available
at that time the Hadoop compiler later
can like take this data structure and go
turn it into what it needs but you could
see how to take this and turn it into an
in-memory planner or something else like
that and anything that knows how to talk
about these graphs which are much more
familiar can compile down this data log
we can just plug them in so that was the
question right it's like does any of
that look like a dupe that kind of
vaguely but it looks more like cascading
and really just looks like a bunch of
green circles of lines between them so
you know a lot of the as I said like I'm
doing this new thing it's much much
smaller I'd like to be able to work in
the browser like why not be able to code
this stuff enclosure script or in memory
or use this as a way to prepare data to
push into encounter you know there
should be no it's sort of arbitrary to
restrict this to Hadoop and the second
version sort of had to go target these
other platforms added a consortium
platforms that's a really interesting
case that I want to get into yeah so
here's this new as I said the new
picture of the world the other
interesting thing that you can do in
Casa log too is it has this pluggable
optimization layer so you can take that
graph which has you know it's just a
declarative structure it's a zipper you
can go walk it and say at the end of
your job you like have ten fields and
then you strip out eight of them you
should be able to walk the graph and cut
out every operation which generated
feels that you didn't want so you know
the ability to kind of fix point
iteration that logical plan before you
pass it into one of these guys is
another thing that's quite exciting so
how do you extend this thing like what
is it a pain in the ass to write a new
planner like what what does it actually
mean to do that I anyway these are the
three sort of two protocols in the multi
method this isn't that clean because I
was initially kind of trying to like get
performance out of this and what I came
to realize is all of this happens I've
been thinking of it as like the second
compile time like there's no need for
this to be fast you're prepping a Hadoop
job for like a Hadoop cluster and so you
know taking as much time as you want to
make that clean is great but anyway all
you have to do to implement a new like
platform to implement the in-memory or
the closure script or anything like this
is that you need to be able to tell the
planner is something a generator in the
world of this platform can it produce
data in which case it'll be
I into one of those generator nodes can
it filter which is just a way of
resolving if you only have in like one
set of fields is it input or output no
problem and then you also need to be
able to convert some operation one of
those first things within the predicates
into a node in the logical graph because
nothing happens with it it's just
bucketed there and then the final thing
that happens is this plan method once
Casca log builds the whole graph when
you call plan that's the signal to go
ahead and turn that logical graph into
like a physical plan in Hadoop or
something often the way those work is
you can then use those again as
generators so you can plan is kind of
the way to shuttle up and down between
the Cask log level of abstraction and
then back down into say raw Hadoop and
then back up into Casca logon down and
up and on and on these are the basics
you know it's we just I sort of flesh
them out for like functions and vars and
objects and multi-functions can filter
cascading jumps in and says okay well my
filter operation is also a first-class
operation you could extend this to scala
functions if you had a mind things like
this it's quite easy to build a new
platform now and will become easier I
think as people start to do it more some
platforms I've been excited about I
don't quite like I know that this is
possible the closure one no problem
closure script I think nothing in my
this compiler has anything to do really
with the JVM it's just records and
protocols that go walk these graphs the
core async one is interesting it feels
similar to a real-time streaming planner
and that your generators could be
channels and if you have some notion of
a time window through which like a join
can take place with an inner sort of
timeout looping channel the final query
could plan down to a channel as well so
you could actually build these data logs
sort of trees of channels under the hood
that make it easy to imagine these like
very very complex flows of data and then
storm is the big one that I had
initially started out trying to target
basically being able to take
same code run it on Hadoop or run it on
storm and it looks like that's going to
be just fine and so yeah takeaways like
what's what's with all this number one
is like use logic programming let the
system deal with complexity for you like
don't try you know they say there's this
quote about premature optimization I
think that it's it's fine to obsess over
these things of the if you're doing it
at a systems level where it can be as
general as possible but like ideally you
let the system do as much of that
obsessing as possible so you can share
these facts and it makes it easy to
abstract take advantage of the
properties of your data that's what
these facts are you should be able to
throw all these in at the system and
have a do work for you as long as it can
and then the final thing I found in a
lot of the work at Twitter is people
talk a lot about sharing code you know
you want to share code you want to run
in the browser you want to write it here
really I think what people are concerned
about is sharing data right like you go
do a calculation on the Twitter data you
want to produce some notion of word
count or follower pairs of things like
that sharing code means it's calculated
exactly the same way really you want
this promise that this real-time Q is
going to hold the same information as
this file system you know like if there
are shortcuts you can take on one
platform versus another you should
absolutely take them and I think it's
difficult to understand those trade-offs
on a case-by-case basis if you're coding
at the low level so trust as much as you
can the logic program and you know
success will follow that's it
questions anybody all right great oh
yeah no it doesn't support recursion but
I think that's just cuz I'm not wise
enough yet to know really what data log
should be able to support so but yeah
that's like that comes up a lot and I
think it's time I just buckle down and
read up on how data log should actually
be implemented it made it happen like I
say this is data log but it might not be
like I think it is but so maybe we'll
chat tonight and you can school me yeah
I can't see that well so if you have a
question maybe just shout out
so the way that works is you saw that
distinct predicate where that I had in
there anything that starts with a
keyword and goes to a value is treated
as like an option by Casca log and see
if that still showed up when we go ahead
and create a generator basically like
this thing just gets the grab bag of
options so those are all sort of passed
through that hit the underlying platform
at the end of the day and effectively
like there when you add these option
predicates sin they only sort of make
sense within the scope of that
particular like bucket of predicates you
gave so you could say in Hadoop it's
like you know the number of tuples you
should a great on the map side but like
you said it could be the time out or in
you know you could have some sort of
cache size that you allow and that all
this gets sent right through which is
very hard for me to imagine how to do in
like a type-safe way back in scholar
land but here it's no problem just
anything goes it's much easier to sort
of communicate through the abstraction
layer I found for me anyway
that's right
I haven't with Cass clogs so much when I
was I was doing some similar work in
Scala on this project called summing
bird and we built a little I don't know
if anyone if you're familiar with
generative testing we had sort of
generators of possible plans and some of
those could get pretty ridiculous I
found that the the complexity that flows
people actually right aren't that
enormous and haven't really been an
issue but I mean trying to make it easy
to write really complex things we're
going to hit that so that's sort of the
at the optimizer stage you should be
able to say this thing looks like it's
going to loop and you know you've done
something wrong for this particular
platform yeah the storm target is so
there's this one of the big users of
cows clogged is this company called
yield bot and Soren Macbeth this great
engineer there who's written a closure
like dsl over top of Trident which sits
on top a storm trident ads in this
notion of joins by like all the tuples
that come off the spouts are off of your
cues or assigned to these micro batches
they're called and that's sort of those
are assigned through some fixed time
window so he's got the closure layer
already he hasn't open sources stuff yet
but now that I'm like out of Twitter and
don't really and have a lot of free time
the goal is to go write a platform that
will compile down to that so that
actually should be one of the first
things that comes out just because he's
got the abstraction so nice up above and
what it would do is plan down to
effectively a trident topology and then
you can go run that using the usual ways
that you run storm that enables this
really interesting thing you can do that
Nathan Mars has spoken about a lot where
you take the same Casca log query run it
on Hadoop run it on storm as well and
then have a client that can merge the
results together and that was that was a
lot of the scholar work I was doing over
the last year so I think those ideas
will make it over before I forget them
yeah I silently released it last week I
haven't it's out now it's done it's you
know and the the final thing to do as
always is to just turn this into a blog
post and like write out how to flesh
things out Casca log tues out and the
big immediate advantages are didn't talk
about this here but it supports the
serializable anonymous functions so you
can you know go just work at the repple
and sort of jam at functions and casts
clog will figure out how to package all
those up and ship them off I think it's
a nice base from which to build these
you know a future spark planner or
things like this that really need to go
serialize logic across you know across
network boundaries so it's out but check
out the mailing lists and i'll send out
an update in the next day yeah yeah
they're they're pretty reasonable target
i think the day Tomic guys would have
something to say about how how
reasonable uh you know and yeah i was i
think i have the honor of giving the
first logic talk of the closure
conference but i think this afternoon
the day tonic guys are gonna get a
little more r and out of that but yeah I
mean that like it's looks suspiciously
like sequel and in fact they're very
very related I think it'd be yeah great
target yeah it looks kind of like pig to
like I thought I was kind of on some new
stuff but not really it's it's all you
know it's just how it should look right
it's like folks have figured this out a
long time ago for a sequel and these
like sequel optimizers are really good
now but it just hadn't made its way to
like I think the way people are building
these MapReduce dsl's like it's exciting
to be able to say like code map produce
in Mathematica or coded enclosure urns
that really like laying down the
groundwork for okay like let's get the
abstractions right let's pick a doob is
one target but really we should be
providing primitives that can hit other
areas that have really thought the stuff
through clearly like you said if you can
hit sequel you know you don't have to
write these optimizers like
that will take care of it at that point
cool I think that's it thank you guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>