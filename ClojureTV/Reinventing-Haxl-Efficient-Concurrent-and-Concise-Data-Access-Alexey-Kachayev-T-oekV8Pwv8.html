<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reinventing Haxl: Efficient, Concurrent and Concise Data Access - Alexey Kachayev | Coder Coacher - Coaching Coders</title><meta content="Reinventing Haxl: Efficient, Concurrent and Concise Data Access - Alexey Kachayev - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Reinventing Haxl: Efficient, Concurrent and Concise Data Access - Alexey Kachayev</b></h2><h5 class="post__date">2015-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/T-oekV8Pwv8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hello everybody I think it's time to
start and I'm really proud to talk today
in the audience full of exciting
engineers and exciting people and today
I want to talk to you about accessing
remote data and dealing with this data
living with it and staying sane at the
same time and as far as I've got already
a few questions
despite the title there you can see on
this slide I'm not going to talk too
much about axel library itself I will
just describe you the idea and the
motivation behind this library but 99%
of my talk will be about closure not
about Haskell so there's nothing to
worry about and by the way I shared link
to my slides on twitter so you are
welcome to follow me or from your
laptops if you can't read from this
screen find a reason few words about
myself my name is Alec alex CTO at
identified company and to be more clean
on this point and almost all the coding
kind of city or which is usual situation
for startups and we are San
francisco-based startup and the platform
that we build empower thousands of event
organizers to create own private social
networking applications that boost
engagement on AIM during the event that
catalyze learning and help sponsors to
connect to their attendees during the
event we've been using closure and
closure script for years already and
I'll be proud to make another talk
someday just to share all experience and
all knowledge that we've got so far but
today we want to draw your attention to
a single problem that we are facing all
the time in our day to day job you can
see here the simplest agenda ever
possible please don't miss Third Point
and I'm going to leave like five maybe
seven minutes at the end of my talk for
a short Q&amp;amp;A session so you'll be able to
answer all your questions this is this
is screenshot from private social
network application actually one of
thousand applications that we currently
run and you can see here what you can
see here is a timeline and most probably
you saw such timelines before for
example if you use such tools like
Facebook or Twitter and I believe you do
what you definitely can see on this
picture how complex the underlying layer
of data is and what is timeline its
number it's a sequence of post some
photos with some payload like text title
URL for the URL and with different
information attached to it for example a
profile information mentions likes
comments replies prefetched information
for URL that was shared inappropriate
claims
spam detection notes your personal notes
your friendship status and so on so it's
a huge graph of different pieces of data
and different sort of connections
between these pieces right and to
collect all of this information for
example to render this timeline on
mobile device or inside our internal
management panel we have to talk to
different data storages and to multiple
backends at the same time for example we
use react database as our main database
for social content for post likes
comments and user profiles and so on we
use Redis pretty heavily as our main
cache layer and in order to keep track
of all users activity to calculate
special activity score for each user and
we also use positivists not that much in
those cases where we need strong
consistency guarantees we store their
application settings and different kind
of meta information that helps us to
understand what your own application
should looks like and how it should
behave in different situations so let me
be true developer and stop talking on
this point and show you some code right
and I assume that we already have this
low-level primitives like fetch item and
fetch collection all already implemented
for us and I assumed that we use in
blocking calls block and HTTP requests
this is almost the old code that you
need right here you can see timeline
post main function and the end of the
slide which takes username and performs
different states steps of data fetching
it fetched timeline IDs calling to our
micro service by the way I should also
mention that we use micro service
architecture and so there are many small
servers that we should talk to then we
will take 20 IDs from this list and for
each ID in sequence we will apply this
fetch post procedure and what fetch post
will do it will fetch underlying payload
it will find in inside this payload I
then I
for those user who wrote this post and
it will nourish this information
together the same goes for fashion user
you can see there that we are fetching
payload for user profile and we fetch in
some kind of score activities corner so
you can see that this code it's really
clean it's concise you can write it's in
few minutes I believe and you can read
it you you can see here all pieces of
data that we need and all connection
between these pieces so we've done right
it's no more dragons left to slay right
and for sure no and there are few
drawbacks and there are a few problems
that we should deal with somehow the
first problem is obvious for you right
this code is clean but it will run very
slowly because of these sequential HTTP
calls and to fetch next post we should
wait between previous patches is done
which is where and very slow the next
the next problem that enumerated here is
not that obvious from the code itself
but you can imagine situation when you
have late for example some crazy friend
and most of your post in your timeline
is written but this person with this guy
and in this case what will you do you
will fetch 20 post with the same or sir
and then you will fetch the same user 20
times because this page posts and fetch
user operation doesn't have any notion
about sequential procedure at the bottom
of the slide so this work is really
redundant and we want to skip it somehow
and the next part for example if you use
various you should know that it supports
this multi get request and
any kind of Belgian you can put if you
intends to get different pieces of data
into a single request send it over
network and get all results back to you
it's really amazing amazing approach how
to reduce latency and in this code it's
hard to use this approach right not that
card let's try to do this manually and
this is reimplemented version of the
same the same piece of code that uses
chorus and package to send the request
simultaneously and some pieces of manual
deduplication logic and I'm not sure
that you can read this code and this is
perfectly fine it's nothing to worry
about because their entire idea of the
slide is just to show you how many lines
of code do you need to optimize previous
page version and this is not the root of
the problem right number of lines of
code we can deal with it with but we can
live with a huge code basically need to
the main problem here that this code is
all about efficiency it's really hard to
see what do you want to do behind how do
you do this behind all these low-level
details about the duplication logic and
a sink request and so on so this means
for us that optimization leads to
unnecessary complexity in your code and
you can see this complexity on my slide
further I will show you that this is
unnecessary complexity and we can avoid
it it's really hard to dig through the
code and to and understand their data
itself and connection between different
pieces of data and the biggest challenge
here this is not only a closure problem
right there is nothing to do with
closure in particular we can switch to
another language and problem stage and
if it's not a closure problem let's
explore space and let's ask are there
saluté other some sort of solutions
that was built in different languages or
in other ecosystems and yes there are
few there is this amazing paper there is
no fork it was written by co-author of
Haskell compiler and I'm not sure what
language is written in English or in
Haskell but you can read it and there is
a library that was described in this
paper
hugs ooh this is Facebook this is
Haskell library that was built in
Facebook and it's open source you can
you can find it on github it's real and
really popular and the key area of this
library to use applicative funders to
manage all dependencies between the data
fetches under the hood with implicit
concurrency and implicit bunching of all
requests when I sent the cable and there
is another libraries teach this is Scala
library and this is actually a part of
internal Twitter and infrastructure it's
not open sourced yet maybe I'm the key
area of this library is to build
declarative abstract syntax tree of all
data fetch operations and then take
special interpreter and that will choose
the most efficient efficient evaluation
strategy and actually this idea this
ideas should sound familiar in this
audience and not only in this audience
but for closure community in general
because goal Macra
from chorusing package uses just the
sense the same approach it takes your
code that looks like sequential code
if recompiles it to design some sort of
state machine with a bunch of cold bags
inside of it and and you don't see this
and most probably you don't even need to
nor about this fact at least until
you'll get your first stack trace
exception but this and may we just steal
this idea and to use it in chlorophyll
code and for sure yes why not quite
common approach in still in eight years
and today I'm happy to introduce you a
library that I am currently actively
working on
it's called Muse and you can find it on
github it was open sort of like a couple
of weeks ago and still in active
development it uses the same idea of
building abstract syntax tree your folio
of operation and its uses Cora async
package as an underlying abstraction to
deal with concurrency so this library
aims to solve all problems that I just
enumerated and let me show you few
examples actually I have a lot of them
but not that much time to talk about
everything so I will show you the key
idea and advantages this library so the
main abstraction that library operates
with is a data source and what is the
data source
it's an identifiable piece of remote
data and I am really sure that I don't
need to talk in this audience what
identity is and why identity is so
important I just want to draw your
attention to a single 50 it's realistic
assumption and
right if you have like for example list
of blog post you always have some sort
of unique ID or unique URL or something
that you can use to distinguish one post
from another and the second part of my
definition is not that obvious
what is remote data what does remote
even means and remote is about latency
latency is about waiting and waiting is
actually doing nothing it's pretty
simple to do and this is fundamental
abstraction in software development
world as Abba can't engineer I can for
example think about my web server
talking to my database over the network
and as a system level engineer I can for
example think about my CPU talking to
memory and we have here like different
scales but the same abstraction and here
I want to emphasize that closer future
or promise is a wrong way to express the
idea of latency because it's an
abstraction of some sort of value that I
will get in future and I don't know how
much does it cost for you to make this
value for me so there are two different
situations you can just sit there and
wait for some external system to come
back to you within a result or you can
run any kind of heavy computations to
situations and in general case I don't
have enough information to distinguish
between the two right so I will use this
helper remote request helper to show you
how-use library works it just print your
intent to send requests somewhere then
and emulate this latency is just to wait
for random number of milliseconds and
then it emulates receiving result from
an external system so remote data means
that the time that I have to spend
waiting for the one external system is
more than time that I will do some
useful computations something meaningful
for example when I am talking to my
react database I will spend much long I
will wait much longer for a response
from react then I will for example
decode by streams with Jason in it into
closure data structure and this means
for me that the data that stored in
react is remote for me so let's describe
our first data source speaker and as you
can see here speaker has an ID a name
and a topic and number of slides and his
presentation so to do this I mean to
describe data source I use this data
source protocol with a single function
fetch and as far as I use ID in my def
record definition I don't need to care
anymore about identity use will
understand what I want to do and what I
really should care about is latency and
to deal with it use library requires my
fetch function to return chorus in
channel I mean return immediately and it
will wait that someone and somehow you
will push the result of this page into
this channel into channel eventually
so what's next use library expose this
this F map function which basically
means I want this function name to be
applied to the piece of data Speaker
when it's ready and it actually does
nothing right away no computation no
data fetches it's just declaration of my
intent whatever I want to do and this is
not a data source in full meaning but
it's I'm some kind of data source
because I mean the result of F map
function because obviously if I have to
wait before I all get speaker then
obviously I have to wait before I can
calculate the name of this speaker and
which means that the result of a
malfunction still has this notion of
latency some time that I have to wait
it's an inherently composable
abstraction and you can use any number
of layers that you want to and you also
can pass a few additional arguments if
you need if you if you function with
like compare it except two arguments not
not single in many cases we have in most
cases we have more sophisticated data
and here you see the example of nice
nested data I have this speaker was ID
with a name only and I have this session
with an IDE with topic with number of
slides and with this reference to speak
a lot and so to how can I answer the
question who speaks at this session just
now we know already how to get the
speaker's name by the speaker ID
it's simple to take speaker's name by
the session ID I have this another layer
of indirection inside my data and to do
this I have to use flood map functions
you can see it and what flood map
function basically means the same as f
map but it is zooms that function that
you passed will return not a value not
something that I can use just now but it
will return another intent to get more
data to discover more more pieces of
information that I need again that all
of that sounds reasonable but how can I
get result so it's like interesting that
I can manage with this F map and flatmap
to get result I have to use this use run
function and use run will return you a
correlation channel and so you can read
a result from this channel back and it
will analyze the whole tree of all
operation that you passed and it will
choose the most efficient evaluation
strategy for you under the hood it also
had this run with two exclamation marks
variant that we all block current thread
until done and usually you don't need
this operation but for tests for demos
like this it's perfectly fine to use it
so ok what what can it do for me why
should I care why do I need another
layer of indirection the new abstraction
to learn so let's let's implement
something practical for example number
of common friends and number of common
friends is pretty common task for most
social networks
it's like essential part of most of them
so we have this new data source friends
off that will return us
for a given user ID some set of ideas of
our friends and what do I need to
calculate number of common friends first
of all to calculate number of common
friends for user a and B I have to fetch
friends for user a friends for users B
then apply set intersection function and
then apply count to the result of
intersection you can see this
description with F map and let's run
this let's see how muse will do and as
you can see on the screen both requests
for both users were sent simultaneously
and muse is smart enough to do this for
you under the hood to understand that
there is no reason for me to wait until
I will get friends of a to get the
friends of B right this is something
that you can do in your mind but it's
really hard for computers to understand
this another another thing it's hard to
imagine this situation and practice but
our implementation doesn't prevent us
from doing this and we can pass the same
ID for both a and B and in the most
naive implementation you will most
probably fetch the same list for the
same ID twice but here with muse we see
that it just skips for us redundant work
because it knows that again it's the
same piece of data I don't need to call
fetch twice there are also many other
examples and one that I want to show is
friends of friends which is also common
for social networks and what do I need
to do I have to
vej friends forgiven user ID and then
for each idea in sequence a call friends
of again and then just applies at Union
for all of them and as you can see there
are two levels of executions we call
this friends or first time then get back
a sequence and then for all operations
in this sequence we send requests
simultaneously because we know that
these data pieces are independent okay
what if my data source implementation
I mean underline implementation like my
data storage support button
or for example most of our micro
services also support its bad version
because this is good way to deal with
the latency and to tell muse that you
can find better way to execute multiple
requests at the same time and you can
use this budget source protocol with the
single fetch multi function and here's
example of what beautiful data jungle in
enclosure and what it actually does for
us it collects all requests grouped all
together that can be run simultaneously
and ask if I can do something better
than sending them all one by one so you
can see how it works and the same
example with just another declaration of
Friends of data source and you can see
that we sent only two requests instead
of four because we know how to deal with
this version and this is really good way
how you can reduce latency when your own
worker is remote sources
so let's recap our example with time
line fashion and this is just the same
code that I showed you like few slides
ago and it used this use library
facilities to declare data sources but
it's still clean and concise and it
looks pretty the same as me if
implemented that we saw and still you
can just describe all you can see
description of all data pieces and all
connections right and the most
incredible fact about this code that
it's actually free from all problems
that we talked about and if you will run
this example you will see that there are
only three three different stages levels
of executions use will fetch for us this
main time line the sequence of IDs then
it will fetch all posts simultaneously
at the same at the same time and then it
will fetch users and users scores also
at the same time and it will duplicate
and just keep a redundant work if some
user was mentioned in the list of posts
posts twice so how did we achieve this
and it's really not that easy answered
not that easy to answer this question
and I think that the key success factor
is this nice separation of concerns and
the key idea that I use Muse to tell
what they want to do what data I need
and what they want to do with this data
and I use chorusing to tell how I need
this data to be fetched
so Muse is a top layer abstraction and
chorus Inc is low-level operations
description for example how do I want to
request to be sent and what do I want to
do with timeouts and what do I want to
do with retries reconnections and so on
and in most cases I don't want to see
these details when I am reading code
I need this only when I have to solve
some problem but I just can can avoid
this and another point here is that it's
really general generalized abstraction
so Muse doesn't have any notion any
ideas about concrete data sources that
you use you can talk to me yet you can
talk to Reyes you can talk to other
micro services or elastics or virtual
day Tomiko what what do you want to do
this is like nice layer of
generalization and of course there are
some known restrictions that you need to
know about in advance if you want to
choose this library to move on and the
first and and the most the most like
restricted one if that it assumes that
your data fetchers are side-effect free
which basically means that you should
not rely on the order of operations and
this is okay for most cases when you
read data if you will try to write data
and then read it back and use will run
really have heavy and aggressive
optimizations for you and most probably
you will fail because this means that
you you are trying to modify some not
not a global state but some kind of
super global state the place on top of
your application and use library just
have any ability to look into this state
and to help you another thing you need
enough memory to store your fetches and
it's it also is really a realistic
assumption but in in in some cases if
you don't have enough memory to store
all information that you have to collect
you can use some tricks for example you
can turn off caption for previous layers
of execution if you know that you don't
need this information anymore but this
is a trade-off between memory
consumption and low 80s or all of the
operations and if you're sure that you
will not have enough memory most
probably you should look into other
approaches to solve this problem for
example into streaming libraries
something like this for example if you
need to fold huge sequence you don't
need to store all all items from this
sequence at the same time and the next
use library uses chorusing package as
you will soar to run fetches
simultaneously and to deal with
concurrency aspects and this is only in
implementation detail and as far as use
was born from real application and from
real use case it's perfectly fine to use
correlation in our application with all
the information about execution context
but as a library developer I really want
to be less attenuated on this in future
so we'll probably I will do some I will
add some flexibility on this so you'll
be able to use any kind of concurrency
primitives that you want or that you
need to
there are also some ideas about future
development cycle this is not any kind
of official roadmap it just just few few
few idea that I am currently working on
and the
first on my list is better error
handling and there are two two kinds of
error that I want to deal with and the
first one is some situations when you're
when three of your operation doesn't
make any sense and for example in
Haskell or in Scala we can use type
system to deal with some with these
errors but in closure it requires some
sort of manual work many work from me as
library developer not for you for sure
and the second kind of error that I want
to deal with is exceptions that will
that can occur from fetches from your
fetches and I can save you from yourself
but I can just try to help you
understand what's going on and why the
next thing is the bug mode I want to add
this functionality to pass special debug
mode on flag into run
interpreter that will trace all fetches
and trace all latency just to give your
ability to look inside and to check
what's what's going on and this is
really helpful when you see some
problems but don't understand why the
next the next part is applicated
functors interface it's a bit tricky but
just now a library built on top of
monads
the idea of mana and the problem with no
nuts that they are inherently sequential
right if you have left and right side of
computation wrapped in more not you
have to evaluate left side and then pass
the result of evaluation into the right
side this is an idea of Monat and I
don't really need this sequential
execution here I want to be more
flexible and to give this flexibility I
can use applicative founders because you
see if you have function and arguments
that I've wrapped in the same containers
independent containers I am free to
evaluate both containers at the same
time I don't need to wait until function
is evaluated before I all start
evaluating arguments and this is really
easy to do but they're hard part here is
that I want this code to to keep code in
some shape of idiomatic closure
I don't want to turn your code into
Cusco obviously the next item this might
look really strange for you I want to
get rid of F map and flatmap functions
and the idea here that the tree that of
all of your operation is just closure
data structure and you have free to
choose what way do you want this data
structure to be built you can do this
with F map and flatmap functions you can
do this manually you can send it over
the wire or you can read it from your
disk right from the file so I I want to
find by the way how to express the idea
of all of these data operations and
maybe I will end up with something that
looks like go macro maybe I will end up
with something that looks like data log
query
I'm not sure on this but if here is like
a huge room for many experiments and I
am exploring different even crazy
opportunities and the last but I am sure
at least closure script support I'm
really sure that there are too many
cases when
need dysfunctionality on your
client-side application but maybe I've
already got few questions about it so I
will do there's nothing here
the problem that I personally believe
that if you need to talk from your
client side to many many multiple pecans
or different data storages that are
always better way for making some proxy
on your server side but it depends on
your station and your application so I
am looking for feedback or any kind of
feedback from adopters and feel free to
hit me up to talk about your systems and
about your date and maybe about your
ideas how to solve the same problem and
do not hesitate to share what do you
think on this and stay tuned for more
looks white
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>