<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cascalog: Making Data Processing Fun Again - Nathan Marz | Coder Coacher - Coaching Coders</title><meta content="Cascalog: Making Data Processing Fun Again - Nathan Marz - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cascalog: Making Data Processing Fun Again - Nathan Marz</b></h2><h5 class="post__date">2013-01-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7qq_PmwplEc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our next speaker is Nathan Mars talking
about Pascal log and ways of doing
closure on Duke I was doing closure on
Hadoop long before it was cool but
fortunately people like Nathan Mars came
along and made it cool and made it much
more fun because trust me you do not
want to do it the way I was doing it so
that is it Nathan please take it away
so I'm specific times so it's a very
early morning may late evening but i
gotta say i love closure in the morning
and I find there's no better way to wake
up and I do a little cast one so let's
get started so so back in i think around
january so a lot of activity on twitter
when the tunisian revolution started
happening so we can do this talk is
analyzed the tweets about Tunisia from
that time your kids I use a cast blog
and through the analysis I'm going to
demonstrate our catalog and how much fun
it is to use but before we get to that
let me just cover the basics of cash
blog so that you guys can follow along
as I do the analysis so cast log is a
very high level abstraction on top of
hadoop mapreduce lets you write
MapReduce jobs and just pure color code
and it doesn't really look like
mapreduce at all but underneath it
executes MapReduce so does everyone here
does imager not know what a new map it
is this one person to thank you people
so if you is a system for doing very
large scale data processing when I say
large scale I mean like really massive
scale you can process petabytes of data
due to do so it works is that you write
your jobs in terms of the
MapReduce paradigm and paduk an
automatic automatically skeleton
complication across however many nodes
and happy imposter so the same program
and run on three knows that can run on
100 minutes or thousand tons the cool
thing about the new biz that is all
tolerant so that if something goes wrong
during computation like a machine goes
down the application will sell feel and
to keep on going now the problem with
Hadoop or just the MapReduce paradigm is
that once you start trying to do complex
computations and becomes really hard to
just express the logic in terms of
MapReduce becomes very tedious and very
verbose so there's a lot of tools out
there that provide higher level
interfaces to MapReduce catalog is of
course one of them there's also tools
like big high at skating and many others
but what cast blog distinguishes itself
is that it gives you also ability to
apply abstraction and composition
techniques to your data processing as it
turns out data processing and data
querying is programming and so you need
these techniques just like you do any
other right cuz you need these
techniques to manage complexity I find
this is a point that's often lost in the
data base and data processing community
and this is going to be the major theme
of this talk all right let's start
covering cast blog and how it works so
the upcoming slides I'm going to refer
to this age data set and age dataset is
just a set of two tuples with
persons name and the second field was a
person's page so here is an example past
la grange this authority gets all people
who are less than 30 years old so this
is just closure code so like all
clothing code the first element of the
list is the operation execute so this ?
arrow operator is part of cast laws API
and it both defines and execute a query
the first argument to that operator is
where to admit the results to query so
in this case is says put the results in
standard output next you'll find a list
of output barrels and the rest of the
query is going to define these output
variables and these alpha variables are
what will be admitted into the alberta
which is in this case standard output
and the rest of the query is a listen to
the cortical credits and we're cutting
it serve to constraint and define the
other properties so here it says the
person very will come from the gauge
data set obvious it's the first field of
the gauge data set and it's paired with
the variable age and then that age must
be less than 30 so predicates are in the
heart of cast log and they all have the
same structure so the first element of
the list is the predicate operation so
here the predicate operation is the
multiplication operation then you have a
list of in the fields followed by the
list of applications and you can put
Bill's alpha wheels are separated by
that colon angle bracket key word the
fields can be constants for barrels and
variables are any symbol refix with
I ! so it's promising of predicates as
functions that take inputs and produce
outputs it's better to think of them as
to funding a constraint between a set of
inputs and a set of outputs so let's
look at a few examples so the first
thing that we're going to left says that
when you add 2 to X you should get
values 6 so even though x is an
infographic on this predicate this
product is actually defining a
constraint on s so there's will only be
true when x is equal four so the next
example says and you will play a lie to
you should get some value C and when you
multiply B by three you should get the
same value C so this is the funny and
constraint between a and B 2 times a
mystical pretense big the last example
says when you multiply X by itself you
should get the same volume extract and
this of course will only be true for 0
and 1 there's four times in predicate
sin cascara those functions which define
a constraint between a set of inputs
instead of outputs so example the
functions or addition or occasion
there's filters filters defining
constraint on just a set of inputs so
that'd be something like created that
unless then there's I readers which
define a computation on a group of
tuples so much with filter is operating
on 1 top of the time aggregator operates
a movable couples I read it will be
something like
so many the violators generators which
are a finite source of couples so
examples of generators could be absolute
freak me out examples of generators
could be like a closure vector with any
doubles or it could be you know like
three terabytes of data and files in
HDFS it could be a table from my sigma
table it doesn't really matter it's just
some source of Tulsa's ok let's look at
a few example worries just to get a
syntax down so this is the core yet sure
before all this gets all people that's
the 30 years old and this quarry
contains a generator predicate and a
filter render k so this is 40 here
imagine you had some full name data set
where the first field is person ID in a
second field it's beautiful name for the
person this curry gets all the
personalities for which the first name
is Leon so this contains a generator
predicate and pub predicate this
aquarium right here gets all the age
values from the gauge data set that had
more than five people associated with
them so we're going to cover my readers
in a couple minutes I just want to point
out that this Court has a generator
it I read a predicate a philtrum
productivity and when I want to point
out is that but although product is look
the same there's no difference between
the different types of etiquettes and
this consistency in syntax let us do
some really cool things they prolly are
actually here's an example in drawing so
let's say you had these two data sets
the caller's data set and the gender
data set so follows dataset says that
the first field is a person who follows
the person the second field al-assal
David Holub baba loves David and then
the gender data set is just first meal
to the person's name the second field is
the person's gender to according below
that says the grid below that gets all
the male people the emily follows the
way once that says I'm looking for all
barrels person where follows Emily
person and gender person male so the
Catholic does it look at this query and
it says you use the same grounded person
across two different sets of data the
right way for me to resolve this free is
to do an inner join between the two data
sets so joins our implicit and cast a
log I find that this is a much better
way of thinking about joins in the
explicit way that you would use in
something like c4 okay so let's let's
get into some running symmetric of them
so I actually put all the code I'm going
to run on github so you guys want to
call a long overlooked I it's just not
like it up
you guys see that I make a bigger bigger
good
okay so person was going to go through
more worries more basic grades that
we'll get to that Tunisia data set so
the point is I'm going to do they use
this we're going to be making use of
cell playground data sets these come
with a school project so I guess I this
bigger
so here's that age dataset i was showing
before here's uh because that trend of
data said there's the follows data set
year but a different data sets alright
so here's like for a show before getting
all people less than 30 years old let me
run that okay you can see down below the
rebel it ran a MapReduce job and the
results are at set of mine so let's say
want to print out not just the person's
names but also the actual age of the
person and all you have to do is add age
the applet wearables and otherwise it's
the same query run that and you get
people with their agents okay this next
query gets all the integers from the
imagery data set that equal themselves
when square so the connector dataset
just containers you know just the
numbers negative one to nine and on top
of it says it then where many come from
integer and look like at myself yet run
that you get zero and one so just like
go to code the operation to a periodic
so here's the same query except get all
integers that equal themselves cubed so
on that and you get negative 10 at one
so this next example this is an example
to join so this takes the age and gender
data sets and joins them on person so
just cuts out the age of gender role
people that are in both datasets on that
right you see it works by them this quiz
involving a little bit simplistic so
here's a bit of a more interesting
cooperate so this query here it's all
all those relationships where someone
problems someone younger than that so
the way this works that says what we're
looking for person one in person to
where the age of person one is eighty
one person one follows person to the age
of person to is age 2 and h 2 is less
run that those are the results so this
we look at this query it's doing two
drawings of light filter which actually
ends up being two MapReduce jobs
underneath maybe see the actual
execution is hidden from you in a
scratcher all right this next worry gets
the number of people less than 30 years
old so in person says that we get all
the engine valves filter out the ones
that aren't less than 32 only keep the
ones less than 30 and then do it count
let me run first and then explain it as
we have five so it works is that each
cast log sub query and do one round of
aggregation and the way sub-query works
is that it does it applies as many
predators as a cab so here it's able to
you know execute the age how to get so
reading the age values and then do the
filter and wasn't can apply own kind of
gets it does the aggregation and what it
does is it looks at your aqua travels
and it sees what output rivals has it
been able
divided so far so in this case it's not
able to define any other perils so it it
doesn't global grouping so let's
contrast this with this query down here
so this gets the number of people every
person follows so it says let's find in
person in calendar where a person
follows someone we put an underscore
when we don't actually need the value of
that era wall and we again due to count
higher theater so this case Cass clog is
able to do the follows predicate and
it's not able to apply anything else
before aggregation so Catholic Allah
says okay I've already defined the
person variable I mean a partition to
date set by person and apply the
aggregator within each partition as you
run that and then you get the Fallen
scout for every person may have not made
complete sense quite as much juice into
a long as today and I completely lost
for basically an idea is like this
partition of data set by an ascent
environment all right so here's a more
more interesting worry so this is the
query that that requires a sub-query to
run so this pouring gets all follows
relationships where each person in a
follows relationship follows more than
two people so the way to find this is we
define a sub grade so we called many
follows and a sub queries to find just
with this arrow operator right notice
doesn't have that ? so that means that
we're going to find a sub break but not
execute it so let me actually
so if I were to do something like this I
could define a variable for many
followers and Chris intern and helping
athletes right so this as many follows
tug breeze doesn't do anything until I
executed and do a guy it's just some
data structure with a bunch so anyway so
in this quarry we first we define many
fellows which defines the set of people
who follow more than two equal so we
just say let's get all the person for
which person I'll someone get the Cal
and then filter on the count being
greater than at two and then before the
next few we say let me get all person
one in person 24 which person one is a
member of many follows person to is a
member of many follows and person
waterfalls persons and we ask you this
right now those are the results okay one
more example before I get to the
Tunisian stuff so this is how you do
work out a squad so let me show you the
data Zeppelin's reading so here i have
the Gettysburg Address just split into a
double for every line of your dress and
db9 workout is to admit the number of
times every word appears in the address
or it appears the data set so here just
to find a custom operation that can
split a sense couple into a set of word
tuples my time again to the house
actually gross which is a little bit
beyond the scope of this talk but then
he is the absolute word count query do
we say we're looking forward Cal where
sentence comes from the sentence dataset
which we just looked at we split
sentence couples into many word couples
and then we do it count alright so this
count sees that word has already been
fine partition with my word another
count for every word I run this right
you get all the workouts that's that's
pretty cool workout is like a canonical
Baptist example and this great little
visit to it sweet
which is a requirement and total by the
way audacity why they hired us to solve
oh there's guys in code inside sweetie
all right let's get to the Tunisia data
set all right so let me just show you
what the data looks like real quick
and so there's have the data here is on
my local file system and there's a bunch
of different types of data sitting in
this particular okay so I basically have
a self folder for every type of data so
they look at like content right this
just contains a bunch of JSON where the
ID field indicates a tweet entity and
then the value is the actual content of
that week so you can see a bunch of
tweets all about then I have Leslie
before we get to this let me just show
this closely explained like the data
model we use score data this is a data
model at this data is sorted so the way
we store all of our data is as a graph
so the idea is that have different kinds
of nodes in a graph so a node would be
like a person or a tweet or a URL things
like that and then the data we store is
neither properties on node or edge
between notes so exactly properties
would be like alice is female or Alice's
locations all stand an example enters
would be things like from react for edge
which in the case a person who is
responsible for some reaction to be
between hoss fight tweets as the options
so this says Allison responsible for
three one two three then you have
reaction edges which are edges between
tweets so that indicates a reply or
retweet so here you know between one to
three is a reaction to tweet four five
six and then tweeds can have their own
set of properties so if we content and
various other properties
okay so we look for this data for sure
content short description together since
Jason if the ID in this case will be a
person's Twitter user ID and the value
is the description and their profile
location in is going to be same
structure IV is a person superuser need
a value is the location a list of
profiling and then if we look at like
reaction which is edge this is just a
tab-delimited file which is containing
two tuples of Queen IDs so the first two
attendees are reaction to the second
Queen 8d and that just goes off for all
the properties okay so let's analyze
this data okay the first thing I would
do is just to find some sub queries so
that I don't have to deal with how the
data is encoded all the time so I just
want to get that data parsed out so I
can manipulate the data as data and not
as however it's stored so first thing
I'll do is write a punch or Right a sub
parade which all all reaction data I
will parse the reaction data into two
tuples containing the to tween edges so
here is the first bite into it so first
I'm going to find a an input tag and
then call source here says use the
function of HFS text line so this just
means three lines of data from the new
file system which in this case will
default on my local file system from
this path so this path gears and end up
being flushed m / 30 / the action and
then we're going to do is we're going to
define a sub-query that returns two
tuples containing one gram of reaction
and another parallel to and we're going
to read all the lines from that source
tab so each these will contain 11
catalytic string now we're going to
parse that tab delimited string into two
couples so this will give us a reaction
string at two string and then we're
going to convert the reaction string to
a log quick convert the tooth drink
finally I just say sting false by
default pass blog will do a distinct
amount of tuples in this case we don't
need that so I turned off so this
destroyer just works
what data's I'm going to pan out other
couples comment
okay so you can see here something's up
there's a under samples and ideas that
were corrupted parts the way I executed
this was instead of using the flexion
one arrow operator am I usually ?
operator so I said the ? arrow operator
both defined and execute a query if you
just have a self-aware they won't
execute you just use this execution
operator Russian Romero is just a thin
wrapper around so great operator and
execution operator and this first end
function will don't look at a couple
minutes ok so this reality the
definition is cool for someone to show
you how we can just get rid of a little
redundancy in in the definition so all I
did here is instead of doing saying
multiply the reaction strange reaction
and multiplying two strings it to
actually wrap long applying in this each
which will look at what this does a
little later on but basically what this
does is this actually expands into those
too long I called it before it's a
little more minutes I screw this way so
if you look at what this sub query is
doing like is doing something kind of
generic it's just parsing a
tab-delimited set of Long's into a
supper break so I said before that cast
long gives you the power to do not track
composition so let's abstract the idea
of parsing a tab-delimited set of logs
into its on top change that we can reuse
it for different data sets right so we
have like reaction edges but we also
have you have torn edges that we'd like
to apply the same the same function to
I'm instead of just be doing the code so
here's a function called tad parsed
longs that will be done for you so this
takes in the name of the subfolder cars
and then the number of fields that it
expects apart from those lines so if I
just jump ahead a little bit we're going
to define the actual data as tap marsh
blogs reaction to expect two fields and
react or data of these finals similarly
so we attempt our song works kind of
creates like a 10-minute for what the
parade should look like so the actual
number of output parables the query it
depends on this mountain peels parameter
so it does is it generates some set of
bars for however many fields a device
report so here says I'm out putting
those bars it reads the line from the
lion attachment source file and then
parses that line and then here it has a
set of intermediate bars that it binds
the output of that line and those were
also generated and then we call on the
fly on each of those intermediate
barrels to producer out the barrels and
we have the distinct balls right ok
that's cool and then we can define
reaction either like this and react or
data like this and now we've done the
finding our sub queries for our edges
the next thing to do is to define the
sub queries that we can meet about
properties as precise values so we're
buying a function that converts the
strain of JSON into a set to tuples with
a property so it just reads the base on
string and then produces the law edifier
and then
right and I'm going to have a similarly
a query builder for each of my different
property types right now I'll be able to
find my description data subgrade like
this probably need a description and all
my different types of data in very
similar ways so property data it also
reads text lines of a file system and it
says I'm in ID and bow where's mine we
read all the lines data source then we
decode line and to the ID and Val an
essay alright so now we've abstracted
away holiday distorts now we can
actually manipulate our couples of data
so first career than just show has a
really simple great so i actually just
run it so so worrying levels amid all
the Twitter user IDs for any given
location and ASM so here is a classic to
this location matches great for Tunisia
all right about that and location
matches is a function and will return a
sub grade that I can execute this is
returned via scepter user IDs I run it
for a different location like let's say
once the atomic avoid Valley reading
about oh I we have different set of
people and the way this location matches
function works is it just takes in the
location we looking for and creates a
self break that returns person IDs and
enemies from the location data sub query
that we defined up here which just a set
of properties we say the first field is
a person were looking for and then we
just passed that closure bearable
location into the second field
constant right and by putting a constant
in there that acts as a filter it will
only return us the person i beez
imagines that location so this is what
it means when your alignment is first
class there's no nonsense like you have
a crime try single just use closure and
cast all together because you know
they're all part of the same people all
right all right let's start getting some
interesting news old times data set so
this first degree i had here he's going
to get all the names all the names of
people who have more than 10,000
followers that tweeted about to be a way
it works is we leave from our followers
to health data so this just contains two
tuples of a person 19 our twitter user
ID and the followers count and then we
hadn't joint that to get the person's
name because the name date is set and
then we only keep tuples that are which
the followers count is greater than
10,000 so if i run this
I get these results all right so let's
do let's figure out what was the average
number of followers for people who
tweeted about Tunisia so what I do here
is I leave the followers account data is
held followers count I'm going to use
the count on Gator that's defined an ass
broad to get account the number of
however they have and then I'm going to
sum up all the following accounts
together some some and then i'm going to
divide this on wed account to get the
average number of followers right so
that's how you as a big average and if i
run that i get this was all 1281 now
obviously like average regional
operation we don't want to have to
always type in cal some individual time
so let's let's abstract the way that
concept with average so that we can
likely use that kind of operation so the
first attempt which obviously is not
going to be a good attempts I've died
average is we actually the fight of you
and I hear an average I'm using
something called a talker but basically
this definition is going to receive all
the tuples with a group and then it's
going to some the is going to some the
values and tuples and then also
calculate values and divide them to get
you the average right so obviously this
is a workable definition but this
definition of average has health
problems first of all the way this is
defined I'm using only executing the
reduced step the average was a type of
hydration and you can you don't have to
wait until the reducer to do the United
Nation you can actually do partial
aggregations in a map phase so that you
don't have to send so much data
the producers and if you can do this
part of my engagement you're much faster
but this this definition does not take
advantage of I'll but I think this is
the second and more important problem
with this is that this competition is
redefining the some a Grenadier and the
top aggregators within like it seems
like should be able to take advantage
take advantage of like our existing
Council magnetars and somehow maybe
compose them together to get average as
it turns out we can do exactly that so
here good average is this good
definition here to find average so here
I'm using the sub courting operator it's
a little bit different because instead
of defining some alpha parables I have
like an input variable and an alpha
travel his definition so instead of
defining himself worry just to find
something called the way this appliance
average is as a composition of the
kegerator some aggregator and a division
function so I have a little slide that
shows out worse so here's a definition
of average now the way how it works is
that when you use average and a query
but has one will do is first expand your
predicate macros into the predicate that
compose that quarry and in the process
any intermediate barrels in predicate
metro are given a unique names that they
don't conflict with other ground rules
in a parade right so it's called a
predicate macro as you find a predicate
to expand into a set of other etiquettes
it's a great general facilities unless
you do a barber tree composition and
it's super super powerful and the cool
thing about this definition of average
is that because it's is just to be using
the countless some aggregators what also
reuses all the optimizations and
evaluate errs so Calvin some are defined
to do those maps item partial
aggregations so address benefits from
all those optimizations because it's
defined in terms of that I think
okay all right I slimming show this free
right here so he's holy hydras number of
followers so we might be interested in
what's like the distribution of
followers so here I have a query I will
admit that were basically bucket eyes
every piece of followers count data and
then helped a number of tuples in each
bucket so the buckets are going to
define our like 0 to 10 10 to 100
100,000 and so on and this curry here
takes all the followers have couples
uses that appetizing option to not
follow Stalin improve on those buckets
and by the time is just to find that
here which is a close your culture is
going to produce a bucket and it's going
to do the count so just gets it out for
every bucket and then what I'm going to
do here is I'm using this this little
function from Casablanca trip which will
actually take the results that sub query
and scream it into catheters that we get
nice crap so let me run this runs and
then over here we get the public at
distribution
okay so let's do some 1520 degrees this
Creek here alright so here I have a sub
break on location tweaks which counts
for every location the number of the
number of people have that location so
this is really simple yeah so this just
gets ola patient data and joins against
tiant or data so that we only get the
locations for people who actually
tweeted about Tunisia and then we could
do the cow so instead of running that
I'm going to do is run publications
Briggs rather than talent controlling
locations this was pronounced up on
other locations in terms of count so
here the way this works is it says let
me get the location and the calculor
location and personally on a source of
data from location tweets which contains
the full set of data I have to do the
top end calculation we sort the tuples
by amount we do a reverse or actually
and we apply this limit aggregator which
takes in reductionist so this limit I
Gator will omit the first 100 up
overseas but it's sorted so it'll be
like the top one out of tuples and in a
mist location amount to the same as
location and amount i run this
as I did we get all the televisions so
London in egypt cairo canada cars
tunisia so it's good so I was known
about abstraction so this seems like a
lot of work just to do a top an attic
for it so the turns out you can just
extract that idea of top M&amp;amp;S into its
own function so that you don't have to
do like always sorting and stuff
yourself all the time and so I showed
before first end let me show how it
works again so here's the same
complications to find in terms of this
purse n function so it just takes in the
set of data that we want to do first and
on and then the amount of trouble with
one and then some optional sort of
matters I never run this to get the
exact same results so first n is defined
in casket log and basically looks like
this is a function that takes in the
subgrade as input and returns you the
new sub query based on that sub grade
that adds top end to that separate which
it has a living smaller space though so
the way it works that takes in the soap
for it isn't but right all the
parameters I showed and then we created
for a like using you know that primal
generation stuff that i showed before oh
and you can basically just see that
template etro before up you know taking
the source data doing the sort by the
founders gave it and then doing that
another operator just get a much much
magic api for doing this the schools are
like you're passing in a set of
MapReduce jobs as is dropping adapters
other MapReduce jobs which you get that
and pass them out more or executed by
more minutes so let me show you the cool
stuff all right isn't it weird to run it
so this pretty here just work out on
beautiful Bob descriptions of people and
it returns that this is where you'll
return the top of other words so this
grace has someone who I already ran it
was right
so we see is Willa Tom works so student
like love although i think if you
actually were to do that across well
this isn't that haven't done this but im
richer if you would actually do that
against people's profile twitter you get
like justin bieber let's go one more
example i had way to make grace berg
logical one more example is it
believable is this example we really
showing you kind of power of having your
life within your this pre here is called
chain very simple I'm social a simple
version you don't have time to the other
one it is that we have those reaction
edges right let's say we want to look
for chains of reactions so reaction in
that reacts to another tweeted after
knowledge week last week throughout the
function called chain harrisonville at
acen just pairs of tuples or two tuples
and your desire to chain length and then
we'll return you mull chains of that
like returns your cell for you that
gives you change left leg so I would run
this
so change it so reaction data let's say
change of length 4 I run that and then
right here to change the next four I
could put them change of length 7 I
don't take a little while longer the run
I entertain the left side so this is the
simple little function here wait works
is that we are going to construct a new
software based on those two couples that
would give it so we do is we we to hear
word instead of using that sub great
operator your replica to define
predicates dynamically so use this
construct function which is like the
functional version of the subway
operator so here says was it
constructive 40 with these alpha travels
so we generate chain length album
rattles here for album travels and I'm
going to do is we're going to go to use
partition to generate like pairs of
output variables and what we do is that
for every pair of apples from the output
variables let's create a predicate which
reads from pears pears generator and
then admits that Harry variables so it
is that you get like routine and likely
you'll get like something like this airs
right in pairs bc and ever like a chain
of length or you get this so this
function just tries that Cory I'm ugly
cool turns out that this this this
definition of it is really slow internet
you can do this great like Wade bastard
and then over here there's an optimized
version of that we all the time and I am
just at a time but I think that gives
you guys a good overview of past login
how you comply abstraction</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>