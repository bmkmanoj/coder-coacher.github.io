<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayesian Data Analysis of Nonparametric Models in Clojure - Michael Lindon | Coder Coacher - Coaching Coders</title><meta content="Bayesian Data Analysis of Nonparametric Models in Clojure - Michael Lindon - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayesian Data Analysis of Nonparametric Models in Clojure - Michael Lindon</b></h2><h5 class="post__date">2017-10-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/76dhEBGuyLY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hi everyone I guess it's time
to make a start thank you all for coming
it's nice to see that there's so much
interest in Bayesian statistics cuz I'm
Michael Lynton I'm a PhD student in
statistics at Duke University and this
is my first Fletcher College so I'm
really excited to be here and I'm really
happy to meet y'all I had been feeling
quite isolated in the in my basement
office of the old statistics building at
Duke I'd been using closure for about a
year and a half before I actually
realized there was a huge meet-up in my
area in Durham and it's also the place
where the clogging detect offices are
located so I was very happy to see that
I've been using closure to do for doing
statistics for about two years now and
it's been a really pleasant experience
so far I wanted to come and share my
experiences with you this is going to be
a brief outline of my talk the first
part is going to be a bit of an
introduction to Bayesian statistics for
those of us who have maybe heard the
term but are not quite familiar with
what it means secondly how we how we
going to do that in closure what
libraries are available that'll lead
into a library that I've been working on
which started out as a library for
random number generation in closure but
has since then developed into more of a
toolkit for doing Bayesian statistics
and then I'm going to show you some
examples about how that's actually being
used in research we are working with the
neuroscience department at Duke who are
trying to understand behavior in signals
in the brain and they're actually using
closure code to do their analysis which
i think is pretty cool so the first
question I get asked in my statistics of
community is when I say that I'm using
closure the first question is what is
closure and the second question is you
know why are you doing that and it's a
question that I've really asked myself
sometimes because there's all of these
wonderful libraries in all these other
languages and when I thought about it it
really boiled down to one thing and this
is my perspective on the problem it
seems to me there's a trade-off between
the number of libraries that already
exist in a language and how easy it is
to develop new libraries
that language so for example just
considering MATLAB and are to begin with
these languages they're usually for some
students their first exposure to
programming and they might do that in
the best Nats 101 class or whether
they're doing engineering and as I say
this is their first exposure to
programming and really what they want to
do is they want to do the statistics and
they want to do the engineering they
don't want the programming really to get
in the way because this is only a
vehicle to get to their statistical
analysis and you can pretty much get up
to speed and get started with these
languages without knowing much
programming so you can really get
started with our if you just know about
for loops and arrays and if statements
they tried to keep the programming to a
minimum so that statistician can then
concentrate on doing the statistics or
doing the engineering in MATLAB Python
and Julia are obviously a lot better
than this but they're still very
opinionated about the style in which
your mentor program these languages
encourage a certain style of programming
now it's often been said that if you're
doing mathematical work functional
programming is really the style that is
optimal for you because if we have a
mathematical description of the problem
this requires some translation to
actually implement this on a computer
and if you're coding in a functional
style this translation is minimal in a
sense so I would argue that if you're
doing sort of mathematical or
statistical work functional a functional
style of programming is great and in
particular if you're developing new
tools you want a language to be
expressive and allow you to implement
those tools very quickly and I find with
closures it's not cliche to say that
it's expressive in the functional style
really is suited for this over and above
some of these other languages it's worth
noting that before R became the lingua
franca of statistical programming there
was actually another competitor and it
was called Lisp stat and it was written
in Common Lisp and
a lot of professionals actually
preferred this I spoke with professors
in our department and one in particular
she actually had a book on her shelf
which was lists tab and I asked her what
went wrong and she said well for some
reason the community just liked our
better and say eventually statistics
departments had to switch from using
Glossip start to our because the
community in our was growing so much
faster and I think the reason with that
is you can do your statistical analysis
in are very quickly without knowing much
programming
if you're to use Lisp start well you
have to understand lift syntax and you
also have to understand functional
programming right and so a lot of
students who are their first exposure is
are they then stick with that and you
don't really learn some of the more core
more sophisticated concepts of
programming using R and MATLAB and a lot
of students then find it very hard to
switch from that to another language
because it's easy to use these languages
it doesn't force you to learn some of
these more sophisticated concepts but
when you do learn these sophisticated
concepts when you're coding your next
algorithm or or building something new
you see all the time opportunities to
use these and you want to use these so
when you go back to using R for example
you end up fighting the language because
you can't code in the style in which you
want to code the design choices that
have gone into developing this language
are not in harmony with the way that you
want to code so that is the reason why I
use closure I've also I've got written
down on my cue cards here that are is a
language that can't do anything but for
what it can't do there's a package and
and I believe that's true because all
packages for all these days are actually
written in C++ and then they're just
called from R for reasons of speed and
also just the flexibility of how you
want to code so what is Bayesian
inference
well informally when we're expressing
belief about something something
uncertain we can usually map that to a
probability and you can argue that
probability is related to information if
I gave you some extra information your
belief would change
and Bayes rule this can be formalized
very precisely in a mathematical sense
this is a rule for updating your belief
given information so in a statistical
context we want to assume distribution
for our data and if we restrict our
attention to a parametric family we want
to find the distribution within that
family that best represents our data so
we use these are parameterize parameter
theta and we express our prior belief
about what that parameter would be by
specifying a prior on theta once we
observe the data which is denoted Y we
can then update our prior to a posterior
distribution for that parameter and this
is the Bayesian way of doing inference
and statistical models so let me give a
concrete example of this actually like
to bake so here's an example suppose I
run a bakery and the last 20 days I've
been recording how many people come to
my bakery so and that list I've got
about 20 observations these are the
people the number of people that came to
my bakery and I assumed that the number
of people that arrived is a Poisson
distribution with some rate the rate is
denoted as theta and I want to do
inference on that parameter theta so I
start with a prior distribution over T
so I know it's non-negative because I
can't have negative people arriving to
my bakery I also have a pretty good idea
roughly of how many people Agana
comments it's not hugely popular so
something like a hundred or something
and by a Bayes rule I can update my
prior distribution to get to a posterior
distribution which in fact is another
gamma distribution where the scale and
the rate parameter have been updated
using the data so here's an example my
prior distribution is plotted in blue
whether density rather and using that
data that I've generated there I update
to get my posterior and the posterior is
shown in green now if I actually
generated that this dataset from a
Poisson distribution with a rate
parameter five and you can see the
Bayesian learning that's going on here
the prior didn't really
a favor anything in particular but given
my data now the posterior distribution
is concentrating around five and this
expresses my uncertainty about that
parameter so here's an example every
different statistical model using sparse
regression so it's a linear model my
output Y depends on these support inputs
linearly and I also have some
observation noise Sigma square I'm gonna
put a prior distribution on these
regression coefficients that's normal
and I'm also going to put a prior on the
parameter tau this prior distribution
corresponds to a belief that I believe
some of these regression coefficients
are 0 this is a prior distribution that
encourages sparsity and I have what's
called a Jeffrey's prior on the variance
parameter there so now I'd like to get
the posterior distribution for the
parameters of my statistical model
namely the regression coefficients the
noise and also this parameter tau but
unlike in the previous example this is
the model where this isn't available in
closed form there's no mathematical
expression you can't write down on paper
what this is so we need to be able to
extract information from this posterior
distribution and that is when we return
to Markov chain Monte Carlo algorithms
these are a way of generating random
variance from a posterior distribution
now this wasn't available in closed form
but actually conditionally they are
available in closed form so if I take my
parameter for Sigma squared and
conditioned on data and all the other
parameters I get this inverse gamma
distribution and similarly for these
other guys the distributions are
available in closed form so let me
switch over to some live coding now
so I've generated some data I've got 300
observations that's the length of my
data set and I've only got was that a
question yeah
thank you
so JD generated some data and the true
regression coefficients of my model are
1 0 0 and 2 and the true value of the
Sigma squared parameter that is equal to
1 so this is specifying my distribution
this is specifying I want an many random
variates from it and this is the linear
component of the linear regression so
what I'm gonna do is I'm going to store
my parameters in a map that's defined as
initial state and I have these three
update functions what is that update
functions are going to do is they're
going to sample these parameters from
their corresponding conditional
distributions the inverse gamma the
normal and the inverse Gaussian and
they're just going to update the value
of that parameter and the map in the map
so that updates beta that updates town
and that updates as to I now compose all
of those updates into a function called
transition so transition as a map that
map's maps to Maps and what I do here is
I define MCMC I take this function this
transition function and I iterate it
from the initial state I drop 100 and
then I take 2,000 so let me show you
what that looks like if I just plot for
you the drawers for beta0 and beta1 they
look like this now this is an example
where it's an iterative procedure but
the point estimates aren't converging to
anything these are generating random
draws from my posterior distribution and
that doesn't look like much but if I
show you this we can use the draws from
the Markov chain Monte Carlo to extract
information about our posterior
distribution this is the posterior for
Sigma square now in truth that was 1 and
we can see the Bayesian learning that's
going on the posterior distribution is
concentrating around 1 similarly the
true value of the first regression
coefficient was 1 and the posterior
distribution is concentrating mass
around 1 similarly for the other
regression coefficients you see these
were
one zero zero two the final one the
posterior distribution is concentrating
mass round two so typically that's how
we do Bayesian data analysis we use
Markov chain Monte Carlo to extract
information from our posterior and we do
this by generating draws in an auto
correlated fashion from the posterior
distribution but in order to be able to
do this for a wide variety of models we
need a lot of distributions that are
disposal within closure so I've been
working on this this is a library which
again started out as basically a library
for interacting with probability
distributions within closure it's on my
github page just Michael Linden slash
distributions and the documentation is
found here so this started out actually
just providing idiomatic closure
wrappers for the random number
generation taken from the Apache Commons
math library and then there are some
distributions which they don't provide
so I've added those and I've also
included different ways of interacting
with them as and when I've needed them
when I've been doing analysis so these
are the univariate distributions that
I've implemented so far some of these
rely on the Apache Commons math library
some don't okay continue
so here's some basic examples we can
create a distribution say we want to
Gaussian or normal distribution with
mean zero and variance one very simply
like this distributions are implemented
as records and then we have a number of
protocols which dispatch on them if I
want to generate random various from any
particular distribution I just use the
sample protocol dispatch that on my
distribution and I found it generate
three draws from that distribution just
like this there are other ways I want to
interact with these distributions to
create these algorithms I sometimes want
to evaluate the probability density
function here or on the log scale which
is good for avoiding overflow and
underflow numerically the cumulative
distribution function and in the inverse
of that function there's also other
properties which we want from these
distributions and we can extract them
with some other functions that I
provided you can create truncated
distributions so if we believe that
something is normally distributed but
it's restricted to the interval between
1 and 2 we can very easily create
truncated distributions and then all of
the previous functions will work on
these we can create mixture
distributions so if I believe that a
random variable is gamma distributed
with probability 0.4 otherwise normally
distributed we can create mixture
distributions like this also for
performing marginalization if I specify
a conditional distribution for my data Y
given theta and then a prior on theta if
I want the prior predictive that that
creates usually that requires performing
this integration we can do that using
this marginal function
if you want to acquire the posterior
distribution given some data so here
this goes back to our previous example
this is a Poisson distribution I've
supplied my data and the parameter of
interest is the rate and this is the
prior I specified I specify on the race
and I can easily get the posterior
distribution using this so this is I
think this is another example I think
it's the same one actually this is the
histogram of my data that I got the
number of arrivals to the bakery for
example the pink was the prior and I've
updated it to get the posterior
distribution which is concentrating on
five now for the posterior predictive
once you've learned about the parameter
of your statistical model you want to
use that to drive business decisions in
the future so now I've learned about the
rate I want to ask the question what is
the probability that five people arrive
at the bakery tomorrow and to do that
you use the posterior predictive
distribution so all of these
distributions you can acquire the
posterior predictive using this
posterior predictive function so the
probability of getting X where X is
number of people that arrived at my
bakery tomorrow that follows a negative
binomial distribution there are a number
of different sampling algorithms used
for sampling non-standard densities so
for example this is what I call a
non-standard density
there's accept/reject you simply supply
the target density that you would like
to sample from a proposal distribution
had dominating constant and this returns
a lazy list of random various that are
distributed according to a target
density
similarly with metropolis Hastings and I
guess we guess didn't get interesting
stuff nonparametric regression so here
for example instead of assuming a linear
model where my output depends on my
inputs linearly now I'm going to assume
that there's some arbitrary function so
my output is now some arbitrary function
of my inputs and I'd like to learn that
function so in this case this
be say predicting wind speed or this
could be a stock that is a function of
time shown in here in pink and then I
have some noisy observations of that
function at some irregularly spaced time
intervals and I want to try and learn
that function so you may be familiar
with generating single random variate
s-- or you might be familiar with
generating random vectors but you might
not be familiar with it generating
random functions so I have a function
here random function and I can generate
a random function and I can evaluate it
at different inputs and I can plot it
let me show you what it's like when it's
plotted
so here I can generate random functions
and this is actually remarkably easy
enclosure because the complication in
other languages is that functions are a
sense infinite dimensional objects the
length of the vector has gone to
infinity
so for every input here on the real line
the real line I need to associate the
value of the function and in other
languages is very difficult to view
because it's an infinite dimensional
object but we deal with infinite
dimensional objects all the time in
functional languages and the way to do
that is through lazy evaluation so these
functions are lazily evaluated the way
to do that is through these are modeled
as Gaussian processes so this is a
stochastic process and the property of
this is that for any finite number of
time points my function f follows a
finite dimensional multivariate normal
distribution if we make those
assumptions on the function and then we
have noisy observations of that function
what we're interested is is the
posterior distribution of that function
which we can use for inference into
prediction with so the way to construct
these is that they can be constructed
sequentially so if I want to know the
value of the function of new time points
the distribution is characterized only
by the value of the function of previous
time points that I've observed so if I
know the value of the function at inputs
1 to N and I want to know it at a new
time point that is simply a normal
distribution where the mean and the
variance is some function of these guys
so the true function here is shown in
pink and I've observed it at these times
in red I want to generate draws from a
posterior distribution and I've
generated three draws here these are
three functions that are drawn from our
posterior distribution and if I generate
many more I can use that information to
summarize the properties of this
function so this represents our 95%
posterior credible intervals for the
value of the function this is typically
very hard to do in other languages
because you can't really represent these
infinite dimensional objects let me give
an example of how this is implemented
so what I do is I first create an atomic
reference to a sorted map essentially
what we're trying to store is inputs and
outputs and inputs are naturally sorted
because they're on the real line and one
of the nice things about closures once I
decided that was the data structure that
I wanted to use it was already there I
didn't have to work too hard to find it
and I'm using a special representation
where the value of the function only
depends on its nearest neighbors so if I
know the fact if I know the value of the
function at 10 and I know the value of
the function at 12 if I want to learn
the value of the function at 11 it only
depends on its nearest neighbors so what
that means is I need to very quickly
retrieve the value of the function to
the left and the value of the function
to the right because the inputs are
sorted I can retrieve that in
logarithmic complexity what that does is
it first tries to find the largest time
before the current input and the
smallest time after the current input
and then if it's sandwich like that it
generates that from the appropriate
conditional distribution which sure is a
multivariate normal otherwise I've sent
it to the left it generates it from when
it's only conditional left and by
supposed to here otherwise it generates
it from the stationary distribution
so this is very useful when modeling
arrival times say for example this might
be download requests or you might have
clients wanting to use some service of
yours at some point modeling arrival
times is important because it helps you
to scale your resources to be able to
cope with the demand we're going to
model this as an inhomogeneous Poisson
process and the way to generate a
realization from that is you first
generate a realization from a
homogeneous Poisson point process so
here the rate is a hundred and to do
that you first generate a Poisson random
variable with a rate 100 and you specify
you generate a hundred uniform variables
on this interval and then you thinned
them with some probability and you
thinned them the acceptance probability
is simply the ratio of these two these
two guys this is the intensity function
that I'm trying to sample from and the
intensity function characterizes my
Poisson distribution
so given this data I now want to go back
and make inference about the intensity
function of the Poisson point process
that generated it so we can do a Gibbs
sampler again this is for the hyper
parameters of the function and I ran
this through a thousand iterations
I've only plotted a few here because I
can't plot all a thousand I think I've
plotted about ten functions from the
posterior distribution here these are
shown in light blue and the true the
true function is shown in pink and you
can see that the posterior is
concentrating around the true function
this represents 95% posterior credible
intervals - what the function was and
these functions are defined not just on
the unit interval they're defined on all
the real line so you can then use this
to extrapolate this function and learn
what that function is going to do in the
future you can use this to do prediction
you can use this to compute estimates of
how many people are going to request
that download in the future and then
scale your resources appropriately we're
using this in neuroscience so this is
the members of the prolab we have
Jennifer grow and have postdocs and PhD
students and this is my thesis advisor
Sujit doctor
what we do here is they have this
experiment where they play a sound to
the subject and they record when a
neuron fires in the subjects brain and
well they simply get a set of arrival
times and they call this the spike train
and we want to model this as an
inhomogeneous Poisson point process and
we make inference about what that
intensity function is is shown on the
left in their play to sound be a
different sound we record the spike
train the arrival times when the neuron
fires and we use that to make inference
about the intensity shown here and then
they played two sounds together and it's
poorly understood how the brain responds
when it's receiving two simultaneous
inputs and we record the arrival times
and that function is shown here the
third one and they have a number of
experimental hypotheses about this how
the brain encodes information from two
simultaneous sources as parameterize as
follows if this is the intensity
function of the dual sound trials and
this is the intensity function of the
single sound trials we believe that it
mixes between them using a function
alpha and the target is to learn the
function alpha and once we learn the
function alpha that tells us something
about how the brain is encoding this
different information so we want to know
if it's time varying or if it's static
if it's firing like a or if it's firing
like B or firing somewhere between a and
B so for example when we do this we find
that it's firing like a for a little bit
and then it fires like B for a little
bit and then if I is like a for a little
bit so the cell is switching its
response between firing like they're two
different inputs at different times and
this is called multiplexing and this way
of encoding information is also used in
telecommunications when you want to send
multiple signals down the wire we're
also able to quantify the posterior
probability that a child is switching on
non switching so this expresses our
posterior belief that it was a switching
agressors and non switching trial here's
another example
when it's listening to a the intensity
function is like this when it's
listening to be the intensity function
is like this and when sharing both
sounds is like the following on the
third yeah
and then the question is what does the
Alpha look like how is it how is it
multiplexing this signal and for some of
them we find that it's static so it's
not time varying at all it's simply
firing somewhere between the two cells
so whilst making this library for
distributions Messe this is is numerical
there's not a lot of symbolic
computation going on here and in order
to reach a higher level of abstraction
because what people find themselves
doing mr. time is deriving these
conditional distributions and these are
done by hand why is it these guys for
example you look at the density of the
complete model and then you condition on
one of these parameters say beta and you
try and work out what the conditional
distribution is using closure something
that I would like to do is to take more
advantage of the symbolic nature there's
it's not a complete can like a computer
algebra system yet however we have core
logic and there's another interesting
work going on in this space
there's expresso which is an engine for
rewriting expressions and the reason why
I wouldn't do this on a general-purpose
computer algebra system like Mathematica
or senpai is that typically you have a
number of different rules to rewrite
expressions and the order in which you
apply them will affect your outcome and
also with the order in which you rewrite
the expressions will also affect the
outcome it requires some user input and
how to derive these expressions and
typically there are some tricks of
deriving these which are known to
statisticians but it's not automated yet
it's very hard to create an algorithm
which is general-purpose enough to do
inference in any Bayesian model there
are some people doing that and that is
the field of probabilistic programming
there is some work in cloture to
implement ballistic programming there's
Anglican if if you're interested in that
but enable to derive these Gibbs
sampling steps when you really need
symbolic computation and that is
something that I've been working on
another thing is to be able to implement
automatic differentiation there are
Markov chain Monte Carlo algorithms
which exploits derivative information if
your objective yeah and that would
provide a level of abstraction were to
create a very general purpose Markov
chain Monte Carlo algorithm for doing an
inference so that was my talk thank you
for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>