<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance and Lies - Tom Crayford | Coder Coacher - Coaching Coders</title><meta content="Performance and Lies - Tom Crayford - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Performance and Lies - Tom Crayford</b></h2><h5 class="post__date">2015-07-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0tUrbf6Uzu8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hi working on performance is
the thing I do is fun it's rewarding all
of the time because it will teach you
things about how software actually works
know how you think it works but how it
actually works this is performance and
lies my name is Tom Crawford I run a
product called yella which tracks
perceptions your application hits in
production and Java has pretty gnarly
performance constraints most of the time
this is because your your app most of
the time sorry you're out most of the
time sounds very few exceptions right
it's not breaking very much maybe some
weird code passed somewhere is broken
but most of the time your app is quiet
not sending much to me the thing the
thing is though is occasionally people
ship bugs to production to break
everything right they or they take down
the production database in every single
page on their site five hundreds and so
I have to deal with all that input and
sort of squash it down in a way that it
all fit into a database and I can tell
people hey you know here's how to
actually fix that bug yellow sort of the
rough the rough benchmark dealer is on
at the moment is about a million events
per second per node which is not really
very difficult to do on modern computers
but it's where it is at the moment so
the agenda for this talk is roughly this
repeatedly I will talk about wins and
lies a win is a obvious performance win
in which I did something that is fairly
intuitive it made sense and I got a
performance where a lie is a case where
something weird happens something I
didn't expect
happened and it led to some weird
performance impact the first half of the
talk is on the JVM and the second part
is on our beloved hand tools of closure
but before all that I have a disclaimer
to make which I think most performance
talks have to have in it which is that
all of this is current it is how how the
system works at the moment right how the
current versions of Java work how the
current versions
to work it will be allies in the future
because things will change so the first
one has nothing to do with the JVM or
closure but it is the biggest
performance thing I have ever done the
biggest win for me and I think it
impacts pretty much any system that
Havard ever has performance constraints
which pretty much everything does in my
book and that is measuring right you
need to measure your performance if it
matters to your application you need to
be measuring it all the time in in
yellow space this is done through a
suite of benchmarks which post at a very
tiny web app that draws graphs over
commits and this is this is really
useful it's a super valuable tool and I
would highly recommend doing something
like this on your application this isn't
a novel technique I first learned about
it from Carrie burn how who gave gives
this excellent blog post called
continuous automated performance testing
but it's been around a long time
pie pie is another great example it's a
alternative runtime for Python and pipe
I have this dashboard in which they sort
of compare the performance of their
implementation versus the cpython
implementation and they do that for
every single compare and it's it's a
really valuable tool one of the nice
things about this approach is it means
you can experiment right you can try out
hey what happens if we upgrade the JVM
or hey what happens if we upgrade closer
versions or hey what happens if we
change from Jason serialization to
transit right you can actually have
numbers for your application not just
you know coordinate excess transit is
fast you get you get to try out what
these numbers actually are for you
but the cool thing is is that doing this
involves that nasty word it involves
benchmarking and benchmarking is
difficult so when closure 1/7 first came
out I thought awesome
I can use my huge regression suite of
benchmarks and find out hey is closure
1/7 actually faster is it slower like
did they break some
thing weird that they should know about
and around the benchmarks and about 20
seconds after the benchmarks finished I
treated arks neva with hey I'm seeing
ginormous performance regressions like
way more than should be normal for a
small language bump I'm actually in the
worst case the worst benchmark went from
3 milliseconds to 5 seconds which is
three orders of magnitude it's not
something I could ship and I was like
worried about this right I don't want a
ship I don't wanna be stuck on close to
one six forever because close one seven
doesn't works my app so next step you
dig in right you try and reproduce her
and I couldn't reproduce her and this
was pretty surprising and I dug a bit
more and here's what actually it turned
out to be so this is how most most
benchmarks run most benchmark Suites run
like this they boot the JVM they run a
benchmark then they run another
benchmark and they run a bunch more
benchmarks and then at the end they shut
down the JVM right this is pretty normal
it's pretty standard workflow right you
don't want to boot the JVM all the time
because that is a slow but there's a
problem with this and the problem is the
JIT rate the JVM has this just in kind
of time compiler which is really an
amazing piece of code but the JIT
doesn't like seeing a benchmark run like
this
so here's roughly what the gypsy's when
you do that so you run some benchmark
and it calls some piece of your app's
code right of course he has to do that
and then that code calls into closure
core or Java Java util string or some
other Java thing or some other core
thing some shared piece of code and the
gypsy's that as like a unit and it's
like awesome I can make that fast I know
how to make this fast I've got like all
this code running through it I've got
all these performance calendars I have
all these optimizations to play with and
it makes it fast
great your first benchmark was fast but
then you run your second benchmark and
your second benchmark again has a has a
path that calls through into some
closure core code but the closure core
code was probably already optimized by
the jet
right and so it's weirdly slower because
the JIT thinks hey benchmark one might
still happen right and so if it still
happens I still have to be optimized for
that case and so it doesn't properly
optimize for the second case this is
really gnarly and the the term for this
is path dependency right the the
optimization is the jet does depends on
the order in which it sees code and
therefore if you run your benchmarks
like this all in one JVM you can
actually change your benchmark results
pretty dramatically just by changing the
ordering of your benchmarks right you
just run benchmark two before benchmark
one and suddenly benchmark two will be
fast this is this was pretty frustrating
fortunately there's an obvious fix right
you boot the JVM on every single
benchmark run and things are just fine
cool benchmark is really hard this is
like a very minor minor case of
benchmarking being hard so when people
think about the JVM at performance one
of the first things you think about is
flags
right Java has all these come out mind
flags you can pass to it and they do
things to performance the the sort of
the the best documentation for this is
for some reason hosted on blog spot.com
it was published in 2011 there's not on
the oracle website and this blog various
documents over 800 options that the Java
command line takes which is a lot of
options like UK you will not tune 800
command line options at least I worry I
read the whole thing but I'm not gonna
try benchmarks on 800 different options
that is just not a thing I can do as
another thing I have time for and either
probably do you so instead I have a list
of sort of the the biggest impact ones
or the ones that teach you the most
interesting things by knowing about them
it's just three so the first one and the
one everybody will tell you about when
you're doing java fonts of any kind
is that you need to change your heap
size right the default JVM heap size
like 500 Meg's you're probably running
on a server with more than 500 Meg's of
memory so change your heap size you get
a big win and four yellows benchmarks
this gets about a 15% performance
improvement which is pretty pretty big
gets like to command line flags to
change this stuff the next one is
aggressive ops so aggressive ops is a
flag that I'm pretty sure our son
invented when they got tired of telling
people here are all the vector all the
options to make things faster and this
sort of just turns on a bunch of other
flags and it's about a 6% win which is
you know pretty reasonable is one flag
cool the last one is actually on by
default but it's it's such a cool name
and it's such a weird idea that I have
to tell people about that and that is
use compressed oops and a is a is a name
name of name of the thing that is
inherited from small talk and OOP is
just a pointer to an object on the heap
that is that is what Anna troop is as
far as Java is concerned so what use
compressed herbs does is if your heap is
under about 32 gigabytes the JVM can
address the whole thing with only 32-bit
pointers which is weird normal normally
32-bit pointers would only get you four
gigabytes of heat but this flag sets
that and allows the JVM to address it
because the JVM knows how your objects
were laid out right Java objects have
headers which were certain amount of
memory and they have some other stuff
which is always a certain amount of
memory so you can erase this pretty
large about a memory with 32-bit
pointers and this has a pretty dramatic
impact because it means the garbage
collector has to do less work it means
the caches are hotter your CPU caches
are hotter and that's a huge way it's
about an 8% when on yellow sponge marks
if you turn it off because it's on by
default but it's it's a cool thing so I
have to talk about it so overall said
it's 32 percent win by changing some
flags right awesome easy takes you like
three minutes or something you
but but flags so this is a flag that you
can pass to the jvm this flag is set by
default by line again which lots of
people use for closure work and what
this flag does is it tells the jvm hey
optimize entirely for code loading don't
worry about the JIT don't worry about
all that stuff all I want to do is load
code as quickly as possible and this was
around originally I think because people
like we're trying to write command line
apps and Java and they were like well we
want to boot a JVM have it load the code
and then come down as quickly as
possible and we want this by default in
line again right we want to boot our
apps faster we want to rebel to reboot
faster on the rare occasions where we
have to reboot it and we want our code
to load faster in development the thing
is is if you run benchmarks with line
again this flag is on unless you turn it
off explicitly which means you're
running into problems right you're
telling the JIT hey don't optimize
anything don't worry about it and that
causes a huge problem so fearless
benchmarks it takes me down from about 1
million events a second down to about
300 ATK events a second which is a huge
impact and Lanigan just sets this by
default for everybody so how'd it turn
this off it's very easy you just throw
this in your Lanigan project on clj it's
like one line this just the important
thing about here is this is the replace
that when you say replace the lining and
it says ignore the default options just
use these options I'd give you go give
you otherwise Lanigan will just say I'm
going to put the default option is in as
well so it's pretty big deal
sorry the jvm we have to talk about
inlining next because inlining is so
important so in line is a JVM
optimization so say you have some these
are all very trivial examples but say
you have some function and it increments
a number very basic right they have some
other function and it calls the first
function right so what inlining does at
JIT compiler time it will say hey we
know that I always cause B so what's the
point in like going through you know
function call miss Direction all that
stuff we can just take the body of B and
stuff it directly inside of a write and
this is this seems like an only
relatively minor win it's like you're
removing the function call overhead
great good but it doesn't seem like that
big a win but it's probably the most
important optimization the JVM does and
that's because it makes all the other
optimizations happen it means that the
JIT can see through lots of nested code
paths and just decipher them all and
then optimize them so as an example of
that I got some function it takes a
number and a foo I don't know what if
there is and it does a thing that number
of times something destructive I presume
sadly but whatever and it was a thing
that some number of times yep so they
may have some other function and it
calls this thing with zero now you
wouldn't write code like this by default
but you want to write code like this by
default right but in in complicated apps
you run into your run into cases like
this all the JIT runners in conveyed
cases like this a lot of the time so
what it does here is the jitters like
okay cool we've got this this body of
code that's being called from this other
function and we're in lighting so it in
lines it and then it this is this is
nothing write this we don't need to do
this because it's doing a thing zero
times so we can just return nil which
means this benchmark runs at like 20
gigahertz a second which is faster than
your CPU runs and it'll run in like your
1 nanosecond or something like that
which if you benchmark this code that is
so inlining is great in lining is a
really really powerful enabling feature
of the JVM it may
a lot of other code optimization is
actually possible especially in closure
where we have all these little functions
that call each other
it's a big deal but sorry back in August
I was working on a new feature for yoga
it was something like this it has it
says this error always happens on this
browser right or this error always
happens for this user or this error only
happens for this particular server whose
firewall you messed up so on and so
forth
pretty small feature actually it turns
out it was about 30 lines of code and I
didn't expect it to impact performance
at all really I would thought you know
this is gonna be fine
so I ran my benchmarks of course cuz you
have to do that before working on before
finishing new features and yellow was
mentioning at about 250 K events a
second at the time and this small
feature which literally adds like 30
lines of code and calls it in one place
took the benchmark down from 250 K
events a second down to 25 K which is
not a thing I can share at all
so as seems to be my habit these days I
tweeted that Alex Miller who is one of
closest maintainers and works pretty
heavily on performance and he said hmm I
don't really know what's happening there
maybe you should dig into a read longer
so to the hammock so I just I just said
I'm not gonna ship this feature right a
10x performance degradation is not a
thing I'm allowed to ship but I can
think about it and maybe I can come up
with some other way the the fixes are so
about three weeks after that I was
watching this talk by Martin Thompson
and he just said this line that sort of
fixed this actually he said this line he
said smaller methods are often faster on
the JVM because they help the end liner
I thought hmm okay I knew what inlining
was at that point not to the level I've
gone into here but I knew roughly what
inlining was I thought well cool I can
just extract some functions right maybe
extracting functions will fix that so I
spent 20 minutes extracting functions
very manually right like you take the
body of this thing and you call it at
some new function
and that took yellow from 25k vince's I
can back up to 248 K just from
extracting methods not a thing you'd
expect right you wouldn't expect
extracting methods to even touch
performance but it touches the in liner
and so it touches performance so the
actual corroding question here which is
actually a pretty common pattern is you
have some map and you're just putting a
bunch of things on that pretty pretty
simple code you'd think the thing is the
in lining in lining budget is dictated
by byte code which is what the JVM knows
about and this code expands as a lot of
closure code does to quite a lot of like
code it turns out enough to blow the
budget so the budget in question is at
325 byte codes this is just a hard
number you could it's a flag so you can
you can mess with it if you want to but
the JVM grows pretty weird when you mess
with inlining flags so I wouldn't
recommend that I would just recommend
keeping your code nice and small the
other in line really related flag I want
to talk about is this one which is a
very weird flag so what this flag does
is it says if you have nested call trees
of closure functions you know a function
a calls function B cause function of C
and someone was so forth and that thing
goes over nine levels the in line is
just like I give up this is too big for
me which means if you have like not more
than nine levels of ring middleware like
that won't be inlined
for example but you can you can tweak
through this one as well but it's it's
hairy as well the end liner is pretty
pretty weird and pretty pretty essential
to the JVM so I would also recommend
just not having lots of nested function
calls in hot code paths obviously
everywhere else it doesn't matter so the
cool thing about these though is that
they're static right you can find out
from your code how many by crows it has
there's a closure library for that no
disassemble you can find how nice did
the function calls are there's a there's
another library for that whose name I
forget right now and you can just walk
it and look at your code paths and say
hey do my hot code paths actually blow
the inlining budget or are they are they
fine
and that's a big win you can have your
CI fail when that happens so that was
enlightening it's it's a big deal but
it's tricky to get right sometimes and
closure definitely macros especially can
cause a lot of problems for the aligner
so now we have to talk about the biggest
lie of all profilers people think
profilers are a good tool they are a
good tool but you have to be aware of
what they actually do but before talking
about profilers allied we actually have
to talk about safe points so a safe
point is I thing in the runtime of the
JVM a point during the execution at
which the JVM says memory state is sort
of known here we know what the JVM is
doing we know where all the memory is
and we can decide for that this is a
useful useful tool and it's very
important for the JVM to know when
memory is because it allows it do
garbage collection and other such useful
things
so here's roughly what an ideal picture
of your app running would look like
right there's some light the line
represents your code it codes execution
write your code is just executing all
the time right it's not doing anything
else it's just your code your code all
your code your courage is working as
fast as possible but that's not what
actually happens in the JVM because of
safe points and safe points so the thing
about safe points what they used for is
they trigger garbage collection and
various debugging things currently
happen at safe points so your code is
running along and here's the safe point
and it doesn't do anything whilst you're
garbage collecting and then it runs some
more and then there's another safe point
and sort of stuff so forth this is what
run running code on the JVM actually
looks like it's not it's not synchronous
all the time it's little pauses here and
there and safe points can happen at two
places they can happen at the back edge
of a loop right like when you're in a
Java for loop and you get to the bottom
and you're about to go back to the top
there can be safe points there and it
can happen in method return which
happens to be everywhere in closure code
but you want safe eyes they're good
things they let the JVM do garbage
collection properly which is super
important to closure programmers because
we allocate a lot of memory
the thing with safe points though is
they interact with sampling profilers in
a very weird way so a lot of the older
bro fathers for for the JVM are sampling
profilers I'm not going to name names
here specifically but if you would like
to ask me afterwards I will divulge
names on those things so here is what
your code running looks like again as a
reminder right there are Steve pauses
and your car is executing but the thing
is is that the older profilers are
something for followers they can only
see the thread the thread stacks at safe
points they cannot see it the rest of
the time and they need that to do that
job that's where they profile which
means that all sampling profilers can
see if your code is this thing they
don't see it most of the time they just
see these like little blips of like here
here here and they don't don't see the
rest of the time which means your
profile is probably lying to you maybe
fortunately there is a there is an
excellent solution as this which Oracle
shipped with Java 7 which is flight
recorder flight recorder is a new
profiler it came out of jail rocker but
it's it's a great tool it solves all
this problem because it is built into
the JVM and therefore it can do its job
properly as opposed to sort of quasi
properly like the other profilers the
thing is though is that this is an
Oracle tool so of course there are flags
to tell it to do its job properly and so
you have to turn on these flags if you
want white recorder to actually do its
job as opposed to sort of profiling just
a safe points even though it says it's a
tool design just not to profile a safe
prints so that was some winds and some
ways that was the JVM let's talk about
hand tools let's talk about closure we
all wanted closure it's a good thing so
a thing that's commonly bandied about in
the closure community is that map's
achieved right like you should use maps
through the core of your domain model
you shouldn't be using classes and stuff
because maps just do the job just fine
and they're very simple and they're just
data they're easy to manipulate
they're easy to reason about and closest
maps are actually very well optimized
and designed
as talked about yesterday the thing is
though is that if you're using maps
everywhere and they're sort of a core
bit of your domain model they're looking
at looking things off in them isn't as
fast as it could be right like if you're
looking something up in a map you have
to walk through an array and maybe walk
through a tree to get something out
fortunately there is a there was a good
and well-known fix for this that I'm
sure most people in this room know about
which is def record so def record
creates a java class with known fields
it looks like a map it behaves like a
map but it is much faster for some
fields the rest of the fields just go in
some entire map another thing of closure
people do quite a lot of in my
experience is they allocate very small
vectors you know this vector might
represent a point I would prefer to use
a map but people people do do this and
sometimes for valid reasons well maybe
they're using it as a key in a hash map
yeah whatever
and this seems very very trivial right
it's just a vector it's like four
character or characters five characters
a few clues spaces do you think this is
looks pretty fast but it is not that
fast because you're actually allocating
this pretty large array comparative to
what you think you're doing so Zac tumin
has this library that fixes this it
defines types Java classes for specific
lengths of very small vectors so there
is a type for vectors of length one it
has one field for the element that is in
that factor and then there is another
type couple too for vectors of element
of length two and that has two fields
it's surprising how often this kind of
thing happens so a while back I profiled
yellows test I'll check sweet and it
takes about 23% of its time allocating
two element vectors because test dog
check actually uses two element actors
to represent trees the shrink trees that
are so important to it but it spends 20
percent of his time allocating these
very small vectors so we ripped we ended
up ripping it out and replacing it with
a very small def type and got a 23
percent performance improvement awesome
going back to the records thing for a
second onyx is a upper coming
distributed computation framework for
closure and I was recently talking to
those guys about sort of profiling and
benchmarking and all these all these
things and they said we'll call will
sent your profile maybe you can maybe
you can spot something in it you know
you've been doing closer performance
work longer than we have and the very
first thing was looking up knowing key
in a map right at the top of the profile
and it takes like six point four six
point four percent of their benchmarks
it's just been looking up some known key
in a map so the other thing about small
collections though is there is a fix
there is a patch Zac Tom and submitted
to closure which is not been accepted
yet but I'm very hopeful about this
patch is actually kind of crazy it
generates I think over six thousand
lines of Java code from closure with
like string interpolation as I
understand it but it defines very small
vectors and very small Maps as Java
classes just like sail jato photos and
this would be a huge performance
improvement to most closure programs
because most closure programs deal with
off some small Maps and lots of small
vectors so the thing about records
though is they behave like Maps right
you couldn't associate anything on to
them and it has to behave like a map
which means there has a map inside it
right if you have social some unknown
key onto a record that's going into some
internal map and this is fine this is
kind of what you want most of the time
but when you're doing performance stuff
and you actually care about not going
into the map it's kind of annoying
because it's silent right it just goes
into the map and everything all your
code works it's just a lot slower than
it should be
fortunately it's in a field you can just
look at it you can run your run your
performance suite or whatever and then
just look at this in your records at the
end and see is it empty oh hey there's
this missing key that I didn't put in
the def record perfect
so I mentioned onyx earlier I talk to
them a lot about benchmarking and
performance stuff and they in their
first
week of doing performance for I've got
about a six times improvement with just
a few tricks records and something I
have not talked about which is unrolling
a sociai mauling get in but both of
those are relatively trivial things to
do and they got a six times performance
improvement which is awesome in a week
of work so computers are fun and
software is fun and working on
performance is fun it is you see things
about how your software actually works
backed last year I caught this bug in my
code this is meant to be caching a thing
on a thing does somebody want to point
out the bug all right or white white
what why why is or a key word what are
you what are you doing why are you
looking up the keyword in a string
you'll see so this is fine right
everything seems to work all just says
oh you're looking at me up in a string
well I know that's not a map so I'll
just compute the full back all the time
this is great
so this actually took yellows API
handler down from 1.4 milliseconds to
300 microseconds of the 99th percentile
which was you know five times or
something when and it was one character
just just one character change and this
was in production for about a year and a
half and I only found it because one
idle Sunday I thought hey I've never
profiled yellows API handler before you
know it runs in 1.5 milliseconds that's
was really good for a web api handler
like i don't care but i thought well i
just profile it anyway and see if
anything comes up and oh this comes up
so we're about wrapping up but i don't
really believe in believing talks
without paper references so i am going
to give paper references these two
papers surprisingly to me are actually
by the same authors I sort of have read
them over the past couple years
independently discovered them and then
when I was putting this together I
discovered they were by the same people
so the first is called evaluating the
accuracy of Java profilers it's about
the safe point issue we talked about
earlier and it's it's that they sort of
were the were the people to first like
shout about this in academia and you
know it was a while bar as a well
received paper and it's a very good
paper the second paper has my favorite
paper title ever and I think I will
probably include any performance talk I
ever give just because of the title this
paper is called producing wrong data
without doing anything obviously wrong
and it's about how tricky benchmarking
is not even on the JVM during that paper
they sort of talk about how if you
change the length of your UNIX
environment variable strings that can
actually affect performance many orders
of magnitude just because of memory
layout they also talk about how the link
order or C programs which is
non-deterministic and pretty random most
of the time can affect performance to
similar levels of magnitude it's a great
paper so that was a I talked about some
wins I talked about some wires I talked
about the JVM I talked about handles
working on performance is pretty
rewarding because it teaches you things
about how your software actually works
my name is Tom Crawford and that was
performance and lies thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>