<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Datomic Auto Schemas  -  Paula Gearon | Coder Coacher - Coaching Coders</title><meta content="Datomic Auto Schemas  -  Paula Gearon - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Datomic Auto Schemas  -  Paula Gearon</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8jXEqvTnOTg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well good afternoon everyone I'm Paula
Guerra I work at Cisco and this
presentation is about automatically
generating daytoy schemas from
third-party data
in particular this is a case study from
the system that I presented at the last
con which was Naga
it's a rule engine for graphs or for
graph databases sorry and it's written
entirely in closure
I presented pretty much the the
operation and semantics of the system
last year so if you want to have a look
at it you'll find that it on the YouTube
playlist for the cons last year now I'm
not expecting that I'm going to be
presenting anything novel for anybody
who's in who's familiar with diatomic
but if you so if you have any any sort
of experience then all you're going to
find is that I'm just gluing things
together in slightly different ways so
when I presented my talk last year Nagas
had preliminary support for diatomic but
since then daytime's become our graph
storage of choice so we've had a lot of
extra focus now on the features and
requirements of storing things in de
topic one place where they Tomic differs
significantly from other graph databases
like RDF stores is in the need for a
schema so a lot of the de Tomic specific
work was focused on that the work was
informed by other projects that I've
worked on working with de Tomic where
we've done similar sorts of things
initially the Loom project which was
originally developed at revel it expand
then acquired by Tara data and an open
source project called caste which would
pass closure source code and store it in
de Tomic
so this store is specifically about
loading JSON in today Tommy that's
because the data that we receive in our
systems is always given to us in those
Jason Naga uses the semantics of
description objects which get encoded
into graph form well that's not
essential for the data therefore the the
data that we're processing to have graph
semantics it does require that the data
get converted into a graph to work on it
fortunately the the data that my group
cisco has been working on is already in
a graph shape so Naga was a natural fit
for it however we do receive all of our
data in JSON form that's because Jason
is simple it's portable and it's a
common standard for network
communication especially over HTTP as
rich pointed out yesterday it's the
default format for transport so now you
have to be able to work with that as
well to start with we knew exactly the
shape of the data we wanted to execute
rules over meaning that we knew all of
the attributes we'd be getting and the
Associated data types
I wrote that schema by hand and loaded
the schema in the in today Tomic before
trying to import the jason for it then I
needed to load more data types that were
also represented in Jason so I extended
the schema to handle those new
attributes then I needed to load more
types of data and I extended the schema
again all of this by hand but I was
inferring most of this from example JSON
objects and it seemed to be a very
straightforward conversion so after that
first expansion it started becoming
clear that I really wanted to have a
tool to to automate this
every time I did I did it by hand to
need for that tool increased so
eventually we I bit the bullet and and
wrote the code that was going to do all
of this automatically which is what I'm
going to be showing here now despite the
talk of this the title of this talk
being about the atomic scheme is just as
much of the work is in the structural
representation of the data so if we
consider what Jason looks like we can
consider how we want to represent it
this is indicative of the kind of
structure that we're going to get in our
JSON documents at the top level we've
got an array of objects where the
objects in this example represent people
it's possible for Jason to have an
object at the root rather than an array
like I have here but all of our systems
don't do that
the one occasion I needed it I just
wrapped something in in a single element
array so you'll see here that the
objects have attributes of type
first-name the inner objects have a name
instead age children type first name and
name are all strings ages an integer and
children is an array of objects and you
can see that those those objects are
people as well so if we avoid
considering there's a raised structure
just for a moment we can see that Jason
is in a tree and that's possibly evident
from the syntax but I pulled it out here
with the roots of starting at the top
level objects you know each individual
object in the Jason is represented as a
node in the tree which in this case of
given an arbitrary identifiers this lets
me refer to those objects by ID which
would be important in the next slide
because tree shape in Jason is easily
represented as a directed acyclic graph
for a DHA each attribute relationship in
adjacent tree then becomes an edge in
the graph the attributes become labels
on each edge though I'm not showing them
here again the objects in the original
Jason require nodes in the graph which
requires unique labels so we can get to
them and we can arbitrarily assign those
as well you can see how each of these
edges can be represented by a three
tuple of entity attribute value so the
atomic is a graph database which is
exactly what Naga needs but the atomic
also has an end of the API this
represents nodes in the graph as
entities with edges of the graph being
attribute values for those entities
these entities look very similar to JSON
objects so naturally we should try to
store these JSON objects as entities
where possible the idea behind this is
to allow the data to fit with less
manipulation it also means that anyone
who wants to access the de Tomic store
directly without using the Naga API or
to see the data that continues to make
sense and they'll be able to interact
with the data using standard entity API
s that wasn't originally the plan
because we had hoped to swap different
databases but as we've come to rely on
they Tomic more and more people going to
look at that data tend to use the de
tama kpi's so the first step is
considering how we want to encode
considering how we want to encode json
data is to look at the types of data
that json supports you'll see here but
the set of types isn't very large its
strings numbers objects arrays boolean's
and the special value nil now many of
these aren't supported in dates or null
sorry
many of these unsupported in day Tomic
sorry many of these are supported in day
tonic with a couple of tweaks
one point worth noting is a Java Script
uses 64-bit floating-point values for
all numbers but because our systems are
all using cheshire integers or Long's
enclosure are encoded as integers in the
JSON stream and decoded that same way so
we don't normally need to deal with
ambiguity there either because Cheshire
will generate floating-point notation
for attributes that use floating-point
numbers and and it'll use the point 0
notation when it's corresponding when
that happens to correspond to an integer
but when the original data starts as an
integer
it'll be encoded as 1 and decoded back
was back into 1 but as you can see they
Tomic has a much larger set of data
types available to it so if we consider
the types of jason requires we only need
to use a few of them you'll note that
I've highlighted both long and double
strictly speaking javascript would only
need the double but because we want we
want to be able to maintain integers
we're using both and we can we can infer
that based on the the data stream we
receive also we're using the reference
type for objects since that's the type
that is used to refer to other entities
to things that are missing from de Tomic
our arrays and null times we need to
deal with those using structures in the
graph instead of something simple in the
schema and I'll discuss that in a moment
so for a simple conversion if we just
look at the embedded people children in
in the first example that I showed
we can see that we have it's a very
simple data types which are
straightforward to work with in this
case we're just looking at type name and
age
now those attributes refer to strings
for the type of name and integers for
the age and those same attributes get
used for both objects now each object
property gets converted into an
attribute whose ident is the attribute
name and whose value type is the data
type for the value of that attribute and
you'll notice that we're using the
cardinality of one for everything and
that's consistent with objects in
JavaScript or Jason since a key can only
refer to a single value we used to have
an attribute definition which referred
to the DB petition and this would be
shown as a reverse attribute on each of
the declarations but the need for that
went away recently but if anyone would
have a look at the nagas code you'll
find that most of our attribute
definitions still refer includes those
declarations now the previous slide just
jumped straight in and used Jason keys
is the basis for attribute keywords that
worked in the example but how reasonable
is this most of us probably use
namespace attributes in DES Tomic or we
should but they told me does get along
just fine without namespaces on keywords
since the rest of your database should
be using namespaces on attributes then
there ought not be any conflicts however
if you're concerned about using multiple
JSON documents with conflicting
properties everywhere then you may want
to consider an approach that map's your
document type into a namespace and
incorporate that into the attributes
being scanned appropriately
a more important issue is that Jason
strings don't follow the same rules as
keywords for instance an attribute on
adjacent object can include a space now
that's not leaking closure syntax so
what are we going to do in that case
one approach might be to just convert
those labels into camelcase or replace
spaces with dashes but that runs a minor
is give colliding with a different key
however since we're generating our
schemas with code and we're not trying
to construct those keys using closure
syntax instead we can build them using
the keyword function and in that case
spaces actually work just fine and they
doesn't really care
it's a naive approach but it lets us
bring anything in and we can export it
without any problems one potential
problem is that it's harder to query for
statements that use these keys but
that's an unlikely case for us so it
hasn't been a concern the only time that
actually come up is in testing because
our standards don't allow for spaces in
any of our keys
now as I mentioned earlier we need
somewhere to deal with the null value in
Jason but they Tomac does not include
nulls or Nils because not having the
value has the same semantics as a value
that's not however it is feasible for a
JSON document to care about the presence
of a key even when the value isn't there
so we want to ensure that we can
round-trip that information when that
occurs we use a ref type for an
attribute and refer to a singleton
entity with an ident of null now that
might conflict with the datatype of the
attribute in another context but I'll
show you how we deal with that surely
now one aspect of attributes that's
often overlooked is that they're defined
using standard entities and those
entities are open which means that we
can annotate them with anything we want
in the case of arrays we're going to
need to use a structure of connected
entities which means using a ref
attribute but then we want some way of
knowing if the attribute is referring to
a child object or to an array so we use
an internal attribute called JSON type
on array attributes to indicate this now
the reason we need to use the structure
to represent arrays is because multi
cardinality attributes have set
semantics and we lose the ordering the
way we deal with that is to build a
linked list using refs the cons cells
for these lists just needs an attribute
to refer to the value and a ref
attribute to refer to the rest at one
point I use the previously mentioned
null entity to indicate the end of the
list since I have a strong background in
RDF and RDF it includes an explicit null
terminator for these lists but that
ended up being redundant and it's just
annoying to deal with so we just leave
that off because attributes must have a
datatype each console needs to have a
different attribute to use for each type
of data that could be associated with it
it's a little annoying when we want to
read from these cells but it just
requires a variable in the attribute
position which we can then bind to a set
of attributes instead of referring to
one specific attribute so it's not
really a big deal one issue with linked
lists when reading with the graph API is
that it requires recursion to get down
the list which can be done with a rule
but if we're using the entity API it
provides lazy access to the entire list
in one step which can be quite useful
finally despite all of this effort to
maintain ordering we often use arrays
because we did actually want sent
semantics and the only data type that
Jason provided was an array because of
that we've added in the contains
property which gives us a single step to
every comms entry in the list
it's just a convenience as it's just a
little easier to query for that instead
of using a recursive rule so this is an
example of how an array looks we're
looking at an entity with a single
attribute called data that refers to an
array the entity here is the red node
the array contains three values and we
can see the console for each of these
entities entries as the three nodes
across the page and which are then
connected with the rest attribute you
can also see our contains attribute and
the and the how that connects across to
the other consoles and that includes the
reflexive one at the beginning note how
each node is using a different attribute
for the different data types which is
how this array is able to store a string
along an tubulin all at once
a big pattern
maintaining the order of the array well
it's a linked list so data refers to the
first node in the list and then this
that's the first element of the array
the second node of the list is the
second element of the array I beg your
pardon I'm sorry I can't yeah
the linked list contains the ordering
okay I'm gonna move on him
so nested objects are the other major
part of structural representation we
just do that with a ref type and that
was naturally designed for nesting
attributes as well not nesting
attributes for connecting attributes in
de tonic and just like other values
these refs can appear in arrays in fact
for our particular use case the
predominant data type that we store in
arrays are refs again we distinguish
these ref types using the JSON type
attribute on the ref definition now
going back to the example on our first
slide this shows the Willmar entity at
the top with its name type and age and
the the children attribute 'href it
which refers to its single entity list
single entry list and that's the console
in the middle that cell then refers to
its value which is the pebbles entity in
the same way that a console usually does
being a single element list it just yeah
refers that way the way we managed empty
lists is when we have a young a
reference to a console that has no four
that has no first value
so the schemer for that the scheme is
specific to that data about Wilma with
pebbles is this here which is a little
too much detail for a single slide but I
wanted to demonstrate what it looked
like of the 5 MLM --nt seer they're
labeled type first name name age and
children the first three are strings and
ages along children is just a type ref
and all of them have cardinality one and
then children has this extra attribute
of Jason type array so by iterating over
our data we can we can accumulate this
and build that as the schema that we're
going to assert before our data goes in
so this is an example of some of the
Jason I worked on lifted directly out of
our systems all of the Jason I've been
working on up to this point had been
fine until I got to this specific
document I saved it for posterity which
is handy because it meant I had it to
show for this talk the issue here is
that the source property it's a bit
small here so let's look at it more
carefully as you can see the first use
is for an embedded entity which required
an refere attribute and the second is
for a string that broke everything it
required a schema that can handle more
than one data type for an attribute
which isn't how they told me usually
works so the way we handled that was to
rename attributes that we identify as
having more than one type you can read
this I hope then it shows a bit of
pseudocode that demonstrates what we're
doing first of all I created an ID and
the in the let block that and use that
for an entity with the label source then
I created two separate attributes for
source string and source object which
referred to their appropriate data types
you can see that I've said object and
not ref and that's because it allows for
distinguishing between objects and
arrays now finally I linked those
attributes back to the stub entity with
the original name and that allows me to
find attributes using that name I can
from that name I say what are the
attributes for this and it'll also let
me get from those attributes back to the
original name when I'm exporting from
day Tomic now we use use a stub entities
for the original name and that's
explained on the next slide because not
all of our data comes in at the same
time
and our schemas may need to be built up
over time
now attributes that didn't conflict
earlier might get used for new data
types in a subsequent document so that
stub entity on the previous page could
actually represent an existing attribute
from an earlier import now that allows
the attribute to continue along
undisturbed in the the earlier the data
that was important earlier and new ones
can take up this this new pattern of how
we handled duplicates so the general
procedure we have here is that we
iterate over all the entities we receive
and accumulate all of the attribute
names and data types are referred to
well so it cuts down into arrays and
objects once we have all of the pairs we
can then build a schema then we need to
build the data that nagas we need to
build the data now nagas relatively
agnostic about the graph store that it
uses so rather than storing entities
directly we we build the edges and
assert them all in a transaction in a
transact operation or transact sync now
building those edges uses a similar
process to iterate over and into each
object and so that we can find the
entities at the top of our document
those top-level entities are each
annotated with a boolean flag indicating
that they're on neither / entity there
are a couple of ways to accumulate data
as we recurse in the past I used a
monadic approach but that's frankly
really messy for Naga I used a
transducer and I'm sold we also use the
protocol for the recursion which made
the dispatch on various types easy to
follow and with very little code
now I mentioned briefly that Naga uses
Ovid databases and we do that by
wrapping access to the databases using a
protocol this is initially so that we
could switch between database
implementations transparently but it has
also provided some extra benefits in
particular because queries have to go
through the protocol we can intercept
them to deal with some of our schema
peculiarities it also means that we have
a central place to store some of the
metadata we put on attributes and we
don't need to constantly keep querying
for that information
it also allows us to introduce new
features such as long-running
transactions which interleave reads and
writes now the implementation of a
protocol we do through record and of
course that's immutable the records are
immutable now the record contains the
connection the attribute metadata so
that we don't need to query for it again
the latest DB and possibly the most
recent transaction ID if we perform it
than anything that results in a new
database means a new record avoiding the
reread of the schema metadata does need
some extra bookkeeping and that's
because transaction transact operations
are usually compatible with each other
regardless of state so we can ignore the
state of the connection but updates to
the schema change that so we consider an
update to the schema as being a state
change for the connection and that it's
pervasive across the system
so if query rewriting because we're
renaming attributes any queries that
refer to the original names aren't going
to work anymore and we handle that by
rewriting any queries we get now that's
not difficult because the atomic queries
are structured data which means that we
can just iterate over each Clause
descending into conjunction disjunction
and the negation causes as necessary now
we're not currently updating predicate
clauses as we don't have a use case for
that but conceptually it's the same so
long as the predicate isn't trying to do
anything too fancy and external closure
code that we don't have any way to see
once the queries been executed and we've
got our data back we need to scan those
results to see if any predicate values
have come back in the results and if
they have we need to rename them on the
way out and that's particularly
important when we do queries to reiax
port our data back to Jason so this is
an example of how we might rewrite a
simple query when we're just looking for
two elements that refer to the same
source it doesn't remove the case where
a and B are the same but it's just for
illustrative purposes you can see how we
convert each pattern into an or
constraint with the various options the
source is converted into both source ref
or source string for those cases where
the original sources also in use we can
bring that in as well now a different
approach is to introduce a new variable
and replace all uses of the original
source attribute and bind that variable
to an array containing each of the
attributes that the sources renamed to
and we have done that in some places
typically more in ad hoc code
particularly when it's getting when we
have the list of renamed attributes and
the the queries are more complex and
harder to rewrite
now transactions have nothing to do with
Jason but the infrastructure that
enables everything we've done so far
also provides other features so I think
it's worth mentioning that there are
benefits that go beyond the one music
ace of handling a particular data type
anyone who saw my talk on Naga at the
last cons will know that executing rules
requires multiple write operations
interspersed with reads under normal
operation that would leave the data
structures in an incomplete the not
inconsistent state during a run which
another user might see we want to avoid
that and we do that with long-running
transactions now this is performed by
using a speculative database through the
use of width and then replaying the data
using transact or transact sync when the
commit is performed ideally we would use
the transaction log for this replay but
speculative databases don't maintain a
log so we can't call TX range on it
now the way naga manages these
transactions is through the use of start
TX and commit TX functions these each
return a record with a transaction
started all committed respectively
start TX just checks to see if a
transaction is already underway and if
not it records the current transaction
ID and returns that new record with it
with that ID recorded if the transaction
was already happening then why are you
calling start TX you might want to check
your code now commit TX is where the
replay happens now in the general case
we would need to keep our own log to
replace the one that the speculative
database isn't keeping but Naga has a
useful shortcut Naga semantics are
currently for monotonic rules which
means that we only need to be concerned
about assertions we don't have to worry
about retracting any data this means
that you can just query for all entity
attribute values which were asserted in
a transaction more recent than the last
recorded transaction if you can read it
you'll see that we also filter out
datums which describe the time of a
transaction since those datums are
internal for diatomic now because
because we can't do unbounded queries in
the atomic we also attach the attribute
ax in this case to its ident which is
information that we required anyway and
once we have that data we find all new
entities that were asserted and convert
them into temp IDs for reassertion then
we reshape the datums into an add
operation and transact them all in
that's the commit operation now in
between the start and commit for a
transaction reads are performed as usual
but rights we'll use the with operation
instead of transact or transact a
transact sync the whole thing is
accomplished in a tiny amount of code
like less than 20 lines this is one of
those cases where closure shows me just
how much utility can be created with
such a little effort and reminds me why
I enjoy this language so much
ever be wrapping up quickly but finally
once data has been imported into Naga
and processed we need to get it back out
again to send it to the systems that are
interested in that data or in any
changes they or if there are any changes
that those systems were interested in
when they had previous versions of the
data the first operation is a query to
get all of those top-level entities by
searching for that margot into the
attribute that we that we attach to each
of them and then we map the json reader
over each of these IDs i mean that the
jason builder over each of those ids the
that rebuilds jason objects from raw
edges and the code could and arguably
should use the atomic center the api for
reading that data and creating the JSON
but this is an example of the graph
agnostic nature of naga and it because
it gets to use the attribute rewriting
code that's built into the wrapper api
that i mentioned earlier so if you're
doing this for yourself with the entity
API then you'd want to recurse into each
entity rebuilding the arrays out of
lists and updating the attributes to
their original names both of which are
relatively easy tasks everything else
will already be in the correct format
now this was all built for Naga as and
as I described I made a couple of
decisions which made sense for that
system but may not be the best choice in
other circumstances so it's not a
standalone library however however most
of the code is isolated into its own
namespaces so you can inspect it if you
like and a lot of it could just be also
lifted out of Naga
conversion of Jason into and back from
edges or data items for diatomic it's
all performed in the nagas data
namespace in the nagas project the
schema generator and the API wrapper are
both found in the namespaces found in
the Naga storage they atomic directory
and if anyone wants to look into those
and you have questions then please feel
free to get in touch and ask me I work
remotely from my home and it's nice to
have someone to talk to for a change
that's my contact details and the path
for the for the github project for Naga
are there any questions yes
yes well I probably didn't prototype all
of the alternatives I mentioned that
I've been building up structures and
arrays in de Tomic in other projects in
the past one of them was a where I
stored a set of elements which were a
bit like the comms elements which
instead of being connected with a next
arrest attribute they instead stored
their index and I rebuild it that way
that that will dwell it it also meant
that I didn't require the contains
reference to quickly get to every
element in one step
and I did like that but the code for
rebuilding it was more complex and I
didn't get the natural recursive but the
code for rebuilding it was much more
complex and I didn't get that natural
recursion that that the coms list
provided for me I think I tried another
approach at one point but they were the
two which were the most convenient and
really using a linked list has proven to
be the most effective for me
well if that's everyone then I'm yes
well sorry what's rare they are rare for
the code that we've been working with
they have been rare I initially wondered
if perhaps I should presume that
everything could have a conflict but
when I considered that I would end up
with with new with renamed properties
for everything and while I can hide that
behind the API whenever somebody wants
to go in and look at the look at the
source sorry look at the data structure
using de tama kpi's from a rebel then
they'd be encountering this all the time
and it makes for a less intuitive data
analysis if you if you're looking at it
manually I did go back and forth on that
for a little while but one benefit to
this is that because conflicts are rare
it's nice to be able to have the
original name it also means that when
conflicts do come in we often have the
original name sitting there and then we
can see alternative names show up as
well it means that we've got the ability
to deal with conflicts and there doesn't
don't appear to be any significant
performance impacts to that but in
general our data still looks clean and
very much like the original data that
was imported yeah
yes
oh okay so when I create attributes that
must be a different transact - so before
the rule so when rules occur that's the
only place we use these run long-running
transactions the data must already be in
the database so when we do a JSON import
it requires the scan building up the the
attribute definitions and transacting
that then once once the attribute
definitions are in then we run over the
data once more and build up the
structure which goes in in a second
transact we're building up as much data
as we can inside the graph database and
then at various times maybe straight
after a load maybe at particular
intervals maybe on request we're then
running the rules at that point it's
only during running the rules that we
need these single transact operations it
wouldn't be possible to try to squish
all of the attribute definitions and the
data and then the rule execution into a
single transact
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>