<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Parsing Text with a Virtual Machine - Ghadi Shayban | Coder Coacher - Coaching Coders</title><meta content="Parsing Text with a Virtual Machine - Ghadi Shayban - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Parsing Text with a Virtual Machine - Ghadi Shayban</b></h2><h5 class="post__date">2016-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9Q--oX5muxw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Gotti shaven it's a privilege
to be here with familiar faces
unfamiliar faces and it's amazing to see
a room full of so many people here to
see parsing so parse things not yay it's
not typically a super exciting subject
but I hope we're gonna we're gonna spice
it up today we're gonna make parsing
great again and we're gonna do we're
going to have we're gonna take an idea
from the history books and we're going
to bring it into closure and we're gonna
see a library that I've written that
employs this technique which is kind of
a unique technique for parsing so I'm
before II start just a little bit about
Who I am I've been part of the closure
community for I think seven years now I
was at the first closure conch it's been
a wild ride I love this community so
much so many good ideas talking to
everybody in the hall has been amazing
um so you know here's to here's to seven
more 20 more will see where we are I'm
also a pianist by by training I went to
school for piano and conducting and that
that's really been my background and I
switched to development pretty late in
my in my life I've worked on closure
core the some of the internals gave a
talk on the compiler a couple cons ago
worked on this on the initial team four
core async so it's it's been a lot of
fun um I work for a company now that's
not doing closure
it's called pocket doc we're an API for
health transactions we provide
connectivity to a lot of different
insurance backends and we sort of
provide an adapter to you know four
hundred different insurance companies
who speak different languages need to be
all parsed differently and we provide a
lot of data api's and curated data sets
and my colleague this morning gave me an
encouraging note
thank you Thank You Colin the the
Internet is forever just remember that
he's a brilliant data scientist that
works with us
so today the the subject at hand is
parsing and we're gonna take a really
informal definition of parsing and that
is it's taking a flat stream of
information for example in a file and
we're going to turn it into usually a
structured set of information in memory
that's not your typical definition of
parsing but that's what I'm going to go
go with today because I designed a whole
library around that very premise um the
implementation that I take is a very old
technique it's comes from 1971 it's a
parsing machine there's a lot of
different ways of parsing you can do
regular expressions you can do
functional Combinator's but a parsing
machine is is kind of one of the more
esoteric and fun fun ways to do it
there's a few libraries that do it this
way Lua has a great one called L peg
Google has one called re two JRuby has
one called joanie and I think that's
based on a ruby library that has the
same technique so that's the that's the
approach that I took and I'm going to
explain how it works from the high level
to the very nitty-gritty we're going to
be looking at byte code so yay
so who considers themselves a regex
expert yeah so what what is this a
terrible regex for email yes that's
exactly right so um so this is kind of
terrible because it accepts dots
everywhere and I missed the repetition
operator on the right hand side so
that's something that there's yeah it's
very approximate there's no underscores
this this one is not battle-tested um
but the nice thing about regex is if
they're generally pretty fat
and but they can't express non regular
languages anything that needs nesting
they can't express the compliment very
easily like the string that doesn't
contain a word like if you had reserved
words in a programming language you need
these ad-hoc rules like the longest
matching rule because they're inherently
non-deterministic sometimes
concatenating reg X's isn't associative
the whole the whole operator system in
reg X's is pretty ad-hoc there's
laziness there's repetition there's
possessive operators you can be greedy
and there's not really a lot of formal
semantics around it there's not a formal
performance model there's a lot of
pathological cases there's a famous reg
X that you can look at that I think
generally look it looks okay but it can
take 27 seconds to parse out on a bad
string so reg X's are pretty limited
they're so limited I did some googling I
love this a read an email address
regular expression that 99% works it's
like and I found that there's there was
a website that was like the Maven of reg
X's it was a whole library where you can
you can just look at different reg X's
for different purposes so if that
doesn't you know impede your sleep at
night I don't know what will this is a
production quality email reg X so so
there's no abstraction capabilities in
here
it's everything you like about string
string interfaces all in one string so
just to put it out there the you know
the username is the top thing then the
ampersand is at the end and the bottom
is the domain right you can if you if
you squint you can kind of see that and
then there's a lot of really subtle
things like the domain has these two
oops on the bottom line that kind of
looks similar but one of them doesn't
have a dot at the end so like imagine
the mistakes debugging this thing and
the dot is there to handle not having
two consecutive dots in a row it's just
it's just it's just wild um so there's a
lot of subtleties going on here it might
as well be this certain language that my
mom told me not to say on camera so so
regex is let's just we're gonna put
those away for the remainder of the talk
another approach is Combinator's
Combinator's are really nice it's the
idea that you take a function and you
build up more functions out of these
little you compose functions together to
make more complicated functions and but
each one of them has the signature if
they take input and then they either
produce a value and a remainder of input
or they fail so these are pretty nice
they're kind of they're functional but
they're they're are opaque black boxes
you can't look inside them to see what
they're doing you can't perform
optimizations against a whole bunch of
them you can't do like a whole grammar
analysis of of what you're trying to
parse and make more intelligent
decisions so I'm we're also going to put
those aside for the for the remainder of
the talk and just before we get like
really really started with this I want
to define a few words and informally
we're going to say a grammar uses rules
to describe a language so formal
terminology is usually oriented towards
the generation of languages not the
recognition of languages so that that's
generally where all the the hierarchy
the the language hierarchy comes around
there's the the Chomsky hierarchy and
that involves a bunch of further
restrictions you have context-free of
regular languages and each one can have
has stricter rules and they generate
fewer types of of languages so today the
topic at hand is going to be parsing
expression grammars this is a
type of grammar that sits in between
regular grammars and context-free
grammars and I think it Accu PI's kind
of a sweet spot in terms of x4 civet e
so context-free grammars generate
ambiguity you can depending on the rules
that how you traverse the rules you can
end up with different parse trees you
can end up with a series of answers
rather than exactly one answer and
parsing expression grammars have a an
additional constraint where you can't
you can't express both a and B at the
same time you do never generate
ambiguous parses you always generate a
deterministic parse so on the way they
do this is they they have an operator
called ordered choice order choice says
so if you took the the Yogi Berra quote
about you know if you if you encounter a
fork in the road take it that's like a
CFG pegs
if you encounter a fork in the road
always take the left one that's what
that's what peg means so it in this rule
we're gonna start with a little bit of
notation it means try to parse a if a
succeeds the whole rule succeeds never
parse be if a succeeds if a fails then
attempt to parse B so it's always it's
always biased toward the left side this
this whole peg theory came around in
2002 with Brian Ford's thesis so this is
a relatively new new idea and even more
than just this order choice operator the
emphasis of pegs is about recognition
it's about developing a parser an
algorithm for forming up what you're
trying to parse rather than trying to
describe all the possible languages that
your grammar can entail so it's really a
different mindset pegs are that you can
almost call them practical expression
grammars you know they're they're really
true
they're biased towards recognizing the
strings on another other features is
that you don't need a separate scanner
or lexer and the parser you can do it
all in one so um this is in line with
the closure philosophy you've
constraints being generally good things
they lead to interesting designs so
we're gonna focus on those pegs with
ordered choice okay so let's go through
we're gonna go through some semantics of
pegs we're gonna go through the
implementation of a library that uses
pegs and sort of the choices that I made
along the way so just to start off
there's really not a lot to this you can
match literals so it could be a string
that you're trying to match or the
character X you can try to match classes
of characters like all the hex digits or
all the alpha numerics you can apply
other rules and this is sort of like
calling another function to parse to
parse something else so there's a you
know it's it's it's sort of like a
function call and like I said there's
that order choice operator okay
order choice is about resolving the
ambiguity before it even happens
so this is really intentional when
you're writing the grammar you specify
what you're trying to parse first that
that's the the onus is on the grammar
writer to do that and CFG s can
backtrack a lot so they can backtrack if
they fail at some leaf rule they can
backtrack all the way up and try to do
something else but in pegs the
backtracking is strictly local so when
you're within a rule you're never gonna
backtrack like outside of the scope of
that rule you're only gonna backtrack
locally so that's that's kind of nice
but it has implications you cannot do
something like this foo or foo bar you
can't try to parse a prefix first
because it'll always succeed on the
prefix it'll never match foo bar so this
is an invalid rule now some of the peg
libraries check for this mine doesn't
you know that that's sort of something
you have to you have to know when you're
designing the grammars you never try to
parse the prefix first so here's an
example of doing repetition this is if
you want to match a c style comment so
you start with the notation I'm gonna
use here is the the rule is gonna be on
the left hand side then there's going to
be a left arrow and the the definition
of the rule is on the right hand side I
kind of like this this notations from
the pegged paper in that the left arrow
means you're recognizing a rule rather
than usually the arrows go the other way
which means in CFG S which means you're
describing a language so this is more
oriented towards parsing so this is how
you loop you try to match a starting a
starting comment followed by this closed
rule and the closed rule is going to be
either so you see the the ordered choice
operator it's going to be either the end
comment or dot which is anything
followed by trying to reapply this
closed rule so it sort of recursively
chews through anything that isn't the
end comment so that's kind of like how
you do a loop in in a peg everybody with
me so far
okay similarly this is how you search
for something on pegs always work in
what's called anchored mode so they
you're at a point in the input stream
and you try to parse forward it never it
never sort of shifts by itself if you
wanted to search for foo within a long
string you do this you make an order
choice so either foo or chew a character
and then recursively try to apply the
rule again so sort of the variations on
the previous slide
there's a couple other operators in peg
there's these predicates so these are
for doing look ahead and is one is when
you want to match a rule but not consume
any input so it kind of tries to apply
something and then if it succeeds it
will backtrack to the previous position
and then try to do something else so if
we wanted to express the comments you
can use these predicates
similarly the not
the knock predicate does does- look
ahead I got those backwards um so yeah
so if you wanted to match a comment you
say start the comment and then anything
that isn't the end comment repeated
followed by the end comment same same
thing different way to express it so
those are extensions to the original peg
definition but they're essential if
you're trying to match anything with a
peg they're essential to you to use
those ok we're gonna move on to the
implementation of an engine so we said
we're not gonna use regular expressions
we're not gonna use functional
Combinator's we're gonna use something
completely different and this was really
novel to me I had been following
Christophe Grande and his endeavors with
parsing he has a bunch of amazing
closure libraries there's one called
seek X which is regular expressions over
sequences it's like it's like 250 lines
it's the most beautiful code you've ever
seen it has a it has a virtual machine
in it it has this macro based assembly
language if there were an Oscar for like
best closure library I would is this
one's brilliant but on his endeavors
sort of inspired me to look into this
and as I said there's a few other
libraries that do virtual machine
approach and the basic idea is that
you're going to compile your rules down
into a set of instructions for a virtual
machine to execute and these are not
going to be Java bytecode these are
going to be your own custom domain
specific bytecode that are exclusively
oriented around parsing okay they're
they have no they have nothing to do
with Java bytecode but um but you know
how do we do this we have to develop all
the by cos we have to define them and so
before you define the the bytecode you
have to define what the machine is going
to track and that consists of three
general areas you need to keep track of
where you are in the input stream the
position in the thing you're trying to
chew you need to keep track of the
instruction that you're executing
and then you have this virtual stack of
entries for calls and returns and
there's two types of stack entries which
we'll get into later but on just a just
a foreshadow a little bit one of them is
going to be for general calls that's the
return address stack entry and then the
other one is when you want to backtrack
you're gonna stick a little marker on
the stack and you're gonna pop it off
later when you need to backtrack so
that's the Machine State then we have to
design instructions the instructions
fall into three general categories
there's control flow you can call things
you can return from rules you can jump
instructions like just on what you call
them unconditional branches write a jump
and then there are these two pair rule
pair instructions called choice and
commit and those are the ones that are
gonna the there they always come in
pairs and those are going to implement
the order choice operator there's a few
more instructions around matching your
input you can match it character you can
you can match a a set of characters you
can chew any character and finally
parsing is just about recognizing but
you never really just want to recognize
what you're parsing you want to turn
those into data structure so you have to
capture certain things you have to like
you have to take a fork and you know
pick out the inputs and so we have two
special instructions that will put a
lasso around what you want to want to
capture so that's the general that's the
general design of the instruction types
and the the there's a wonderful paper by
the author of the L peg library Wood
who's the creator of Lua and he goes
through this he goes through the entire
design of the Lua L peg library it's
just a great paper it's really really
accessible I totally recommend it and I
read it several times while implementing
this so but we have to we have to first
figure out how do we compile down our
rules into into the bytecode so what
we're gonna do is we're gonna take a
bunch of rules and we're gonna just look
at snippets look at excerpts of byte
codes
dreams to mash certain things so we're
gonna play like a little human compiler
okay
so I'm gonna write the rule on the left
hand side and then the instructions that
describe it are gonna be on the right so
this is the most simple thing if you're
just trying to match a string
you're gonna admit three instructions
one's gonna match F one's gonna match o
and one's gonna match o straightforward
right okay a little bit more
sophisticated if we wanted to match an
open parenthesis and then calling some
rule a and then a closed parenthesis
it's gonna be also three instructions
you're gonna match the parenthesis
you're gonna execute the call operation
then you're gonna match the the closed
parenthesis but what does the call
operation do it will save your address
of the it'll save the address of where
you are which is the call operation
it'll jump to this code for a there's
going to be some amount of bytecode
instructions for a and then it's gonna
end with a return which will pop off
that saved address and go back to the
next instruction make sense yeah okay
all right and then if you want to parse
a rule followed by another rule it's
really just gonna be concatenated the
byte codes together you're gonna
concatenate all the byte code for a your
splice it out and then you're gonna
concatenate all the byte code for B so
the concatenation is exactly
concatenation of rules is a
concatenation of byte code all right so
we're gonna ramp up the difficulty a
little bit and to soften the blow
I'm going to give you a picture of my
dog who's barely visible and then we're
gonna linger a little bit
okay so here's order choice this I'm
gonna introduce a new notation we're
gonna have two labels what the labels
are going to do are we gonna refer to a
position in the input stream
sorry no a position in the instruction
stream and some of the instructions are
gonna refer to these positions the
positions are symbolic right now like lb
l 1 lb L 2 but later when we when we
finish omitting all the byte code out we
have to link those they have to become
not symbolic they have to become
absolute positions so but that's done at
the very very last step you turn the
labels into actual integers okay so this
is where it gets fun order choice you
have to use these two instructions
choice and commit what choice does is it
says remember this position so that you
can come back to it later in backtrack
and what commit does is say the opposite
says forget that position you don't need
to you don't need to remember that
position to backtrack because you
succeeded we're just going to throw
throw away that position so for example
if you wanted to match foof or a bar but
prefer foo what you're gonna do is
you're going to say choice label 1 which
means remember the position label 1 as
the BACtrack entry in case stuff goes
sideways jump to label 1 and then it's
gonna try to match F then o than o if
that succeeds it's gonna say you know
what forget that backtrack entry commit
it and jump to label 2 which is gonna be
the next instruction after all this
stuff in case you tried to match fo o
and it failed we're going to pop off
that choice that we saved earlier the
label 1 we're gonna jump directly to it
and then we're gonna try to match VAR ok
make sense so I'm so choice it's about
REM remembering commit is about
forgetting ok so we can use choice to
also represent optionality you can try
to match foo and that's that's pretty
simple um you remember the entry just
after
phoo and if in case foo-foo fails just
jump to it directly make sense okay now
we're gonna go to repetition this is
again a variation on the theme there's
another snippet for repetition try to
match foo many times we're you're gonna
remember the position right after what
you're looping over you're gonna put
that on the stack - maybe backtrack -
you're gonna try to match foo if that
works you're gonna jump back up to the
top and start again so that commit
really punts you back to label one and
then you're gonna try to do it again and
if the last time it's going to fail and
it's gonna jump to the instruction
immediately after this loop okay so ah
some some more complicated examples are
gonna come next so you get a
progressively cuter picture this is my
dog mo he's a mess so here's how we do
the not predicate this is we have to
introduce a new instruction called fail
and that's just gonna unconditionally
fail so what this does this snippet
starts by saving the current state using
the label - then it's gonna try to match
a if a succeeds we're going to commit
we're going to throw away that thing
that we remember earlier and we're gonna
jump to label 1 and label 1 as a
landmine its fail so what this means is
basically if a matches fail the whole
thing because we're trying to match not
a if it does not match we're going to
pop off that initial choice the label -
and jump directly to the - the code for
B so it makes sense I've seen a lot of
nods that's good all right
so that was probably the most
complicated example everything else is
derived from all the things that we've
seen seen before except capturing so if
I wanted to match a and also capture
what I tried to parse what we do is we
put some we put a begin and end around a
so the this just uses the the capture
instructions and there's a separate
stack entirely in memory and that just
saves all the captures that you're
accumulating and so that's basically all
there is to it
the original Lua L Peck paper has a lot
of optimizations that you can do on the
bytecode so you can if you try to match
a or B or C with parenthesis sort of
group to the left that's not going to be
very efficient because it's gonna keep
backtracking out and trying the same
thing over again so what they do is they
punt the associativity they move it to
the right they say A or B or C they just
sort of shift at the library that I
wrote does implement the associativity
optimization but it doesn't implement
the others yet tail calls are useful
when you're trying to loop if you try to
match you remember that searching
example where you said a or eat a
character and then recurse when you
recurse you're gonna want to do a tail
call so you don't like continually on
accumulate stack entries another
optimization they do is what's called a
head fail this one's a little bit more
complicated but what it means is that if
you try to match a rule that starts with
a character you have to like do a call
instruction and then try to match the
character and then that fails and then
you have to backtrack out so what they
do is they sort of move the matching of
the character outside of the call and
they sort of pre match and that improves
performance dramatically I have not
implemented this but it's actually
pretty straightforward
and finally there's one one last one
where they optimized capturing
just like in a head feel if you consume
a lot of stack entries captures also
consume stack entries and if you try to
capture something you you know you
remember where you started capturing and
then you fail the match you have to
forget that and so that that's causes
churn on the stack so what they do is
they also sort of move the capturing
they delay it until later until the last
possible second and all of these require
just a tiny bit of analysis on the on
the code okay so the library I wrote was
inspired directly by L peg L peg is a
really really nice library I'm gonna
show you a snippet of it that that's
pretty cool but um but I didn't want the
imperative approach that L peg took I
wanted a more like data structure
approach I wanted it I wanted grammars
to look exactly like a de Tomic query so
just a quoted map that was what I wanted
to how I wanted to express the grammars
I also wanted the nice capture system
that L peg had where you can not only
capture regions of text but you can also
apply functions to the captures which
really simplifies parsing and then I
wanted to use the power of closure to do
a macro system in the grammar and I'll
show you I'll show you that later but um
but first let's look at L peg I don't
expect you to read all of this but this
is really cool this is L pegs using L
peg to match or to parse unicode utf-8
and the entire logic for it is it's just
in like five lines at the bottom and so
um so they're notations a little
different they use a crap-ton of
operator overloading so they're plus
operator is actually the ordered choice
slash operator in the peg that we saw
earlier and their slash operator is
applying a function to a capture so um
so what they're saying here is you know
if you want to parts utf-8 try to match
a byte call bite on it or that fails try
to match the two by format this the the
dot R is there like range of characters
so try to match that followed by
continuation buy it and then apply f2 if
you succeed an f2 is like just doing the
decode logic but as a really really
elegant it's like a couple dozen lines
and this is just utf-8 it's obviously
right you know it just got all the nice
properties so I wanted something similar
to this L peg is based on they try to
parse abstract bytes I wanted to start
at the character layer so I already do
the the utf-8 parsing in two characters
so this is a example of the library that
I wrote this is parsing JSON this is the
entire JSON description in like 40 lines
this is the whole grammar this not sure
if zooming in well something's doing
it'll work maybe but but basically the
it's a big quoted map it has the name of
the rule on the left-hand side and then
the description of the rule on the
right-hand side how do you express the
rules
well vectors mean concatenation so at
the top you say JSON is either white
it's white space followed by a value or
at the bottom you could say let's say
the digits rule that's one digit
followed by another set of repeating
digits so that's how you parse digits in
JSON
so that vectors are our sequencing or
concatenation then anything inside of
parenthesis is going to be like a
special form just like in just like in
Lisp so the slash operator is order
choice that follows the the peg notation
directly so a JSON value is an ordered
choice of a string a number an object an
array JSON true JSON false or JSON null
and you know and then you just jump from
each symbol to the to the rule that
defines it in this grammar you don't
have to like apply a function you don't
have to hug it with parentheses to apply
it you just you just refer to the symbol
directly
and that jumps to the rule I think
that's about it there's a couple other
things I'm gonna highlight a little
later the architecture of this library
is exactly the architecture of a
compiler it starts with the grammar you
analyze the grammar you produce an ast
which is just a bunch of nested maps
they kind of look like tools analyzer
ast s then a compiler takes that the ast
and emits a bunch of parsing bytecode
then you take your input the parsing
bytecode and then you need some
implementations of these actions so you
kind of the the combination of the 3
gets becomes the input to the virtual
machine and then that does the parse so
this is like exactly a compiler
architecture same exact same form I
wanted to have the ast in the middle
because I wanted if I went directly from
the grammar to the bytecode I couldn't
do any of the code analysis for the
optimizations so that that's why that
that part is critical this is just a
snippet from the virtual machine I
implemented the virtual machine in Java
why because it just seemed appropriate
that you know you're gonna do a bunch of
mutable things it's just gonna be a big
switch statement
you're gonna loop around pop off an
instruction and try to execute it and
the switch statement just calls into the
little helper methods here's an example
helper method if you wanted to match
character get the instruction find the
character that you're referring to you
know if it matches then you know bump up
the PC the program counter move forward
in the instructions stream move forward
in the subject stream the input stream
otherwise fail here's a couple other
examples if you wanted to do the choice
and commit those pair instructions
basically make a new stack entry
remembering all the stuff that you need
to remember bump the stack up bump the
the instruction to the next instruction
the PC the the program counter similarly
if you want it to commit where
just discard the stack just move move
back one in the stack and then jumped to
back to the instruction that you're on
so that that PC equals instruction PC
that means basically all of the
instructions are just an integer array
once they're all flattened out and
linked and you you access the next
element and that becomes your return
address you just jump directly to it and
then loop loop back up into the switch
statement um so this was the first
virtual machine that I wrote and it was
it was a lot of fun but it took it took
a long time to debug and I realized why
we value immutability and this community
so much but basically the only thing
worse than immutability sorry the only
thing worse than mutability is
implementing mutability it was a
disaster and just implementing mutable
state was just i opening um so back to
the json grammar when you match a rule
you want to take the things that you've
captured and sometimes you want to turn
them into data structures so I made
these things called actions and at the
end of at the end of a rule typically
you put an action and it transforms what
you've captured into something in your
domain for example if you matched
true/false or null at the end the action
that you want to do is to push a boolean
true on to the stack not the string
representation of true which is what you
captured similarly if you're capturing
like a JSON object you want to you want
to take all the key values that you
encountered along the way and reduce
them into a closure map on the flip side
of it if you want to describe the class
of characters that you're matching you
want to express them symbolically you
don't want to say like that crazy
regular expression string that you
nobody could parse you want to just
refer to
digit you want to refer to an escaped
character you want to refer to
whitespace symbolically and delay
providing the implementation for that
match until later so that's what that is
about the the class business the
matchers and the actions they're all
described using an interface so you can
provide your own implementations so for
characters it's just you know simple
match this Unicode code point and I
envision people implementing really
efficient mattress for example if you're
trying to match a ski you can match it
using a bit mask really quickly and you
can in some cases match without
branching and so you know lots of
libraries don't do that right now
so that is derived from Scala has a
library called parboiled and they have
these really nice character matches in
there and so just a note on things that
you do with the if you're doing actions
when you're trying to like manipulate
the captures that you've accumulated you
want to make sure that you do legit
things you can't you can't pop too many
elements like beyond the stack frame it
is I'm exposing like a mutable capture
stack so you have to be very careful
that you don't like you don't you know
buffer overflow your your capture stack
so you know there's a few different
types of actions you know pushing things
reducing your captures if you wanted to
capture an array or applying the
function to the top of the capture stack
and so those are pretty uh pretty clear
and like I said the matching is a
different interface that's pretty simple
you just you know there's various little
helpers to to do matching
so just as a as a final example I wanted
to show how to match CSV so this is like
an incomplete version of CSV so a CSV a
line of CSV is a record and that
consists of a field followed by a number
of separators and then another field and
a field can be quoted or unquoted I'm
alighting all those definitions and then
the separator is just the comma okay but
you could see if you had this grammar
description you can turn a CSV matcher
into a TSV matcher using like a sock you
can just change that directly that's not
the goal of the library but that that's
something that kind of falls out pretty
nicely where you can just just you can
programmatically manipulate grammars um
but this this that first line where it
says field and then a separator followed
by a field that's a lot of boilerplate
that comes up well comes up a lot so you
want a way to abstract it you want a way
to describe that idea of just repeating
something with an interspersed separator
so there's a macro system in the grammar
where you can take where you can take
some boilerplate looking thing and you
can abstract it away using a macro and
so a macro is just a rule it's like a
higher-order rule it's a rule that takes
in a rule and then it emits another rule
so in this case um I'm saying define me
a macro called join and it takes a field
in the separator and then here's the
implementation for the macro it takes a
pattern and a separator and it returns
you a vector of pattern followed by a
print the parentheses star than the
separator in the pattern so this is this
is a really nice way if like building up
building up or abstracting away a lot of
boilerplate where this comes up a lot is
in Peg's you really want to consume
white space eagerly whenever you match a
rule try to eat as much white space
behind it as possible so you can write
like a little white space macro that
wraps
another rule and it just emits that rule
followed by you know whatever choose
whitespace in your in your grammar so
what I want to build for the so what I
want to do to sort of finish out this
library is I want to implement all this
all the other analysis and optimizations
in the original el Pegg paper I want to
have them all there and those really
improve performance a lot performance is
what kind of drove me to this approach
in the first place virtual machines are
super fast for doing this thing the JSON
grammar that I tried out it's 99 lines
of closure and it performs within a
factor of five of the jackson json
parser which is like a hand-rolled ten
thousand line custom java recursive
descent parser it's nuts but i'm you
know with a factor of five without much
tuning i like there's something to be
said about like finding the right the
right abstraction i I also want to build
more character matters like maybe maybe
vectorized character map matchers I want
to generalize the input so that you
don't just match characters you can
match like you can match sequences any
any abstract tokens so that would be
super useful in a lot of domains and
that's kind of what Christoph grande ckx
library does it just parses sequences I
would also want to maybe use that to
have a better interface for the whole
capture system I think exposing a
mutable stack is maybe not necessarily
the best way to do things but I have to
think a lot about that more so anyway
just to recap closure is stupidly
powerful for doing this stuff it's like
a couple thousand lines of code and you
know you get in a library that's within
five times the speed
much larger library I think it's really
nice to use peg systems because they're
they're just a sweet spot in between
regular expressions which are just
terrible to write and much more
complicated systems like full CFG s or
you have to know about ambiguity you
have to know how to resolve that stuff
and finally it's been really rewarding
to work on this library but you know
it's it's about the journey not the
destination so if you have any questions
please please ask me after the UH in the
hallway thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>