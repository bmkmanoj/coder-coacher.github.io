<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Concurrent Stream Processing - David McNeil | Coder Coacher - Coaching Coders</title><meta content="Concurrent Stream Processing - David McNeil - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Concurrent Stream Processing - David McNeil</b></h2><h5 class="post__date">2013-01-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FSkKTxc2RHQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everyone who's speaking at the cons
you're about to find out what happens if
you don't submit any biographical
information to me before I introduce you
this is David McNeil he's going to be
talking about a couple of areas of
closures that are particularly near and
dear to my heart the first one is I love
in his abstract where he talks about
large-scale parallel in process I think
one of the things that happens around
the industry is that we do good things
but we punt on what's possible in the
local process so i really think that the
vision of what you can do in a single
process is particularly important and so
i'm looking forward to this story and
yet I cannot help noticing that the
words fork join also appear in the
abstract and it seems to me that a year
ago we were watching a David talked to
us about fork join and here we are a
year later watching a different David
talked to us about fork join and yet the
fairly straightforward a couple of
hundred lines of code that it would take
to actually have a cool fork join story
integrated into closure for everybody's
benefit has not been stepped up and
delivered by anyone so I'm going to say
right now that rebel it excessively
agreed following the conch that they are
going to step up and do that work to
push the fork joint support that he's
going to talk about here today into the
end zone to make it something that's
consumable by everyone from closure so
thank you very much David
well thank you Stu yeah I think you'll
see this is actually a bit different
than what David talked about last year
but we can still talk about it yes so
I'm David McNeil I've had the privilege
of working at revel it except ool time
closure developer during that time we
have one set during that time we built a
database Federation product and we
continue to build a database Federation
product enclosure really it does two
main functions the first is query
translation and the second is database
Federation so on the query translation
side will take in a semantic query so a
sparkle query and translate that into a
set of equivalent sequel queries on a
data source on the database Federation
side again we'll take in a single
logical query and we'll break that up
into a set of physical database queries
that need to be run on many database
sources so in both of these problems the
pattern is a single query comes in at
the top and then many queries need to be
run against underlying data bases all
those queries produce result sets which
we process as parallel data streams and
then we have to do additional query
processing on top of those to kind of
fill in the gaps of what a database
would be doing to present that as a
single logical result set to the end
user so this is the core problem i'm
here to talk about today is how we have
tried to cast that problem and our
solution to it in a way that takes that
really leverages closure well and fits
in well with closure so this is this is
all running production code that we've
shipped so what you'll see is the
trade-offs that we've made between the
amount of time we can spend on design
and of course the pragmatic pressures of
shipping so by no means do we present it
as it's the Paragon and we're done but
rather we we think we've done some
things well we think there's room to
push further and apply more design to
this so the kind of operations that we
need to do on these parallel data
streams they should look very familiar
to you if you're familiar with closure
sequence operations or even if you're
familiar with what database
queries do it's kind of the what are to
me somewhat universal data stream
processing operations we have other
requirements besides the basic
processing there's kind of these non
functional requirements we want to do
things like have good exception handling
we want to make sure that as we're
processing very large streams that are
too large to fit into memory that we
don't blow the heap we also want this
kind of thing you get from databases
sometimes which is a visible workings
aspect so you can go in and you can see
what processing is running and you can
see how much data is flowing through
different nodes in the processing you
can have user initiated or system
initiated cancellations of those things
so it's really kind of like building a
database engine enclosure except where
the sources of the data rather than
being in like index files controlled by
the database the sources of the data are
themselves data streams coming in from
underlying databases so of course data
processing it's nothing new and we do
stand on the shoulders of giants so we
have these massive pieces of great
technology out there that are available
at our fingertips to be used and the
question is how do we effectively
leverage those not only do how do we
effectively leverage them to do good
work but how do we end up with a design
such that we don't like anger the Giants
too much right there not too
dissatisfied with what we have done so
we kind of feel that pressure and that's
part of what I'll talk through today and
really this the story starts here i'll
start with the sea unix world which is
obviously so much a part of our daily
lives in a particular i like this memo
which is out online it's from one of
Dennis Ritchie's colleagues dated in
1964 that looks typewritten I guess it
must be typewritten and he talks about
having a way of coupling programs like
garden hoses and so it's great to me to
think that that long ago we have someone
expressing the core idea of what we're
actually trying to build today so here's
an example of a UNIX pipe invocation
that any of us might write and use on a
regular basis and I'm going to walk
through this piece by piece because as
it turns out if you understand well
how UNIX pipes work then you can
transfer Oh much of that understanding
to understanding library that we've
built for data processing so I'll take a
tour of the parts here obviously you
have processes so when you invoke a
command like this you're allowing like
this curl and said and all these
commands to be run in parallel each of
them have their own address space they
can be threaded and each of them has a
very narrow well-defined way of
communicating with its upstream and
downstream partners of course they
communicate via pipes which are these
asynchronous communication channels
between the processes and these pipes
can carry this end of file indicator and
this is kind of a pointer to one of the
three main things that a process can do
in terms of its downstream partner it
can produce data it can produce an error
or it can indicate that there will be no
more data forthcoming I'm done clothed
this stream is closed so at the bottom
you have these processes and pipes and
on top of it you have this nice compact
syntax to invoke it into that syntax of
course you can pour the various
operators there's a wide library of
existing predefined operators and you
can write your own all this is run
finally in this implicit execution
environment that knows how to run
processes and tie them together with
pipes so I'm going to like hopefully
take what you know about this and now
when I start talking about our library
you can kind of bring some of that
understanding with you so what we have
built is a library that allows us to
define data structures which represents
which represent pipes for communicating
between processes and nodes for
representing processing of the data that
flows through those pipes this for us
just like with the UNIX model this is
kind of a low-level mechanism and the
question is what kind of syntax do we
pour on top of that so think of a syntax
like the command line the UNIX pipe
command line that you'd want to invoke
and of course as you can see from the
picture here and as I've described it
what we're really building our trees not
simple straight pipes so we the input on
the left here would correspond to two
data streams coming in we do some kind
of processing and produce a final single
stream out so we need to represent a
tree we're in a closure we can obviously
look to another
pioneer of our industry and we can
choose s expressions as our means as our
syntax for representing these
computations that we want to do so this
is a peek at what I'm going to talk
about this is a stream processing
expression using our our operator so
it's kind of like our dsl for stream
processing all of our stream operators
they end with this plus so you can see
through there how the stream flows
through this expression so what we've
built is a compiler that will take an
expression a stream processing
expression of that form and compile it
down and produce this structure of nodes
and pipes which is the thing that's
actually run and that's run on a fork
joint pool that's where the fork join
comes in so this now shows you all those
same elements that I showed in the UNIX
pipe invocation I have a very brief demo
to give you a feel for what this might
look like in practice for the demo I'm
going to use the example of word count
so this right now is just a plain
closure sequence expression our library
is not involved with this at all at the
bottom we have strings coming in and
we're going to do a map and reduce on
top of that so we're going to do a map
which will split each string into its
constituent words we're going to count
those up and then the reduced will sum
all those up so this will give us a
total count of how many words appear in
the input set so this is for of course
slightly more impressive so here's the
text of war and peace I can do that and
there's 580,000 so this now is the
equivalent in our stream processing
language so I've built this expression
on the right it's very similar to what
we were just looking at I have the data
stream coming in the strings coming in
from that file I'm going to turn that
into a stream using one of our operators
and this thousand that will indicate how
to chunk that so it'll group it into
chunks of a thousand for processing and
that chunking is really mostly invisible
to the parts above it but it's kind of
important to get the performance
characteristics that you want so this
map invocation is
we will this is very similar to the
plane closure version using the same
split the same count it's just the
operator that we use is our P map +
operator which implies the P implies
that will be implemented in a parallel
fashion similarly the reduce is very
similar to the closure version except if
you're if you notice there's an extra
operator out here which I can talk more
about that later but basically it's very
similar to the closure expression so I
build this up as just a data structure
so it's just a list by putting the tick
in front of it and then I can pass it to
this exact stream function what that
will do is that will go out and create a
fork Joint Task pool register this
process the stream expression with it to
be processed and return the result so
you see it returns the same value that
the regular clothes your expression
returned you'll notice it comes out as a
sequence so the way our stream
expressions work is even if you do this
reduction everything that's flowing
through the expression is treated as a
string so it looks very similar to the
closure expression but what's happening
under the covers is actually much
different and to kind of give you a
glimpse into that I have an example here
where I made a few changes to the
expression I'm just going to take the
first 10 lines and I replaced these
mathematical operators with just some
demo functions what they do is they keep
track of the full history of the number
so every time you do an addition it
keeps track of what numbers were added
and only that but what thread was doing
the work so here you can kind of see the
history of in this case the number 64
and you can see all the threads that
were involved with producing that number
so this is just kind of my token little
indicator to you that it actually is
doing this whole parallel processing
thing under the covers
so that's the high level view of what
we're doing i'm going to start from the
bottom up and try to describe to you how
we've actually built this so we have
pipes at the bottom the idea is you have
a producer thread and a consumer thread
that you're a synchronously passing data
between these are the operations we've
defined on our pipes so on the producer
side you can in queue data you can close
the pipe and you can signal an error
that should sound very familiar to what
I described on the UNIX side and of
course you can get those out on the on
the consumer side pipes are made to be
multi-threaded to get many producers and
many consumers and the pipe will
guarantee that each item gets delivered
once we also have this ability to
register a call back on a pipe and the
way that works is anytime an item is in
queued on a pipe the calling thread will
run the callback function before it
returns and i'll show how that's used
later on in the talk so we capture all
this as a protocol so that gives us our
point of abstraction this is one area
that was very clear preparing for the
presentation that we need more design
work applied it's obviously enclosure
style it's kind of offensive to have
like all these different concerns
grouped into one interface if you will
so you can easily see breaking out the
in queuing and the dq'ing and looking at
Stewart Sierra's work I think the
analogy would be to break it down even
further to have the idea of you know can
this be closed or can this be an error
maybe it's very fine-grained interfaces
but behind that abstraction we can build
different implementations underneath so
we can make a multiplexer that looks
like two separate input pipes from the
consumer or producer perspective but
it's actually going to a single output
type conversely we can make a tea where
a producer writes one item and it
actually goes to two separate
destination threads so that was pipes
the other key piece is the nodes so a
node is analogous to a UNIX process it
has an input pipe and an output pipe its
job is to read from the input do some
processing right to the output if you
look at the fields inside of a pipe they
are inside of a node they have their
pipes they also have their task function
which is what they're supposed to run
that's the operation they have to do on
the the input data we have a place to
keep state so if you think about like a
reduced node
you can think about the reduction being
stored in this state and then we have a
concurrency indicator so if you've
written a note to be single threaded you
can set a concurrency of one otherwise
you have a concurrency of n and multiple
threads could be running on a node at a
given time so to think about the threads
then we have the original producer and
the final consumer but then in the
middle we can have multiple threads
running on each node just like closure
sequences data is passed not signal
values but nodes past chunks of data on
the pipes so if our data items are maps
we're actually passing sequences of maps
as the data items through the pipes so
we take all these primitive parts the
pipes and the nodes and various pipe
fittings I describe those muxes and
tease and we can wire them up into this
overall processing tree to describe the
processing we need to do you just think
about all the threading action there's
primitive parts that you put together
that you can build very complex highly
concurrent parallel computations you can
describe them with these primitives from
an external perspective kind of bringing
it back to the application I talked
about if you take one of these
processing trees the way to think of it
is each pipe would be connected to say a
database query and its result set so
there'd be four database queries here
each one wired to an input pipe and the
person who submitted the query they'd be
waiting for that final result on the
output pipe
so a question that comes up with
libraries like this is trying to get to
the bottom of is the data being pushed
or is it being pulled the way I'm
drawing these pictures it's clear the
overall flow of the data is from left to
right but then within each piece there's
kind of pushing and pulling happening so
pipes they expect the producer to push
the data into them and then similarly
they expect an active consumer who's
pulling the data out nodes are the
opposite so nodes expect to be active
kind of on both sides they expect to
pull their data from input and push it
to the output so kind of at a micro
level if you think of the overall tree
there's pushing and pulling being happen
happening throughout that processing
tree so I talked about the threads
running on the nodes but I haven't
described how we do that so here's my
shot at that I'm only concerned with the
threads that need to run the processing
node so I'm not concerned with the
producer threads or the final consumer
thread I'm only concerned with how do i
schedule threads to run the task node
functions and here one of our main
design goals was to treat these worker
threads as a scarce resource so we don't
want to waste them we don't want them to
be wasted waiting for input from a
producer we don't waste them polling
various inputs so we want it to be
asynchronous as data arrives we want
tasks to be triggered to run on a thread
pool
so this is where the pipe call back
comes in when pipes are wired up to
nodes the node will register a call back
on the pipe so when I produce your
thread and q's into the pipe it'll
automatically run the call back and what
the callback does is create a task which
will be asynchronously scheduled in the
thread pool so it doesn't do the work of
the node it just creates this little
task which tells the thread pool hey you
need to do something on this pipe the
other way the task is generated is the
task function that we really put inside
of that task not only has the task
function of the node but has extra
wrappings around it so you can see here
what it does is it will run the task
function on an item and cue the results
of that into the output pipe but then
it'll go on and try to create another
task so tasks can be created not only
when data arrives on a pipe but also as
tasks complete and it's not obvious but
that's actually sufficient to keep the
whole machine running so when data comes
in you kick off work and as you finish
work you kick off new work and it just
keeps everything running of course we
don't just create those tasks
willy-nilly we create them under certain
constraints so if there is no data on
the input pipe we don't create a task
and we don't create more than the
concurrency node or the concurrency of
the node indicates is allowed so then
the net effect of this is as data is
flowing through the input pipe the
original input pipes and then all these
intermediate pipes you're kind of
kicking off all these tasks every time
either data item arrives or our task
completes and those all get scheduled
down onto the thread pool the thread
pool we're using is Java fork join this
is you know I view it as another big
piece of very well-crafted carefully
designed code that we can just leverage
by dropping in kind of the 10 second
version is it's a thread pool that has
been carefully designed to avoid context
switching we're not necessary and to
avoid contention on the work fuse
so now moving up a level I think of this
black box here as a processor so
processor has a worker thread pool and
it has a bunch of process trees that
it's supposed to be running so
externally if you want to make one of
these trees you could make a s
expression a list that head that
describes what you needed go to a
processor and say here I register this
please run it one of the things I said
we wanted to do is be able to handle
very large streams that don't fit into
memory so we have all these process
trees running we have all these pipes
all those pipes are accumulating on the
heat we're using up heap space for all
those and the obvious question is what
happens when you run out of heat so I
have a whole nother sub presentation for
this but the one slide version is we've
created buffered pipes so Java makes it
really easy to do memory mapped memory
map files and so we've created a pipe
implementation that leverages those so
we detect when we're low on memory and
at that point we swap in buffered pipes
and those buffered pipes will they're
able to write the data out to disk so
you can really just dramatically expand
your storage and then once we're back to
a out of the low memory condition we can
swap back to using regular pipes and
this actually doesn't happen on the pipe
level we actually have the idea of a
composite pipe so you can have a pipe
made up of subtypes so parts of them can
be buffered and parts not depending on
how your memory conditions are I wanted
good exception handling so when a thread
anywhere down in this process tree
throws an exception we have the code in
place to percolate that all the way up
and out as the final result similarly we
have our hooks and all these pieces of
the processing tree so either on a user
initiated basis or assistant initiated
basis we can track down all the parts
and shut them all down when necessary so
that's the pipe and node like the bottom
most layer now I'm moving up to talk
more about this the way we express these
stream expressions way I think of what
we've built is that a stream expression
is this core tree where the skeleton is
our stream opera
Peters and then you hang off of that in
specific holes close your expressions so
showing the same thing a little bit more
graphically I think of this tree of our
stream operators with just leaves in
there of closure expressions and the
reason this is important is because that
plays into how we end up compiling that
so we don't do general code walking of
the expression we expect the core
expression to be in our operators so the
way the compiler works is it starts from
the bottom up and it transforms this
expression into equivalent pipe an
equivalent pipe node structure it's not
just one to one operator but like each
operator doesn't become a node but
rather the compiler looks for sequences
of operators that are compatible that
can be run in a node together so they
have compatible concurrency limitations
they don't use the state in different
ways and it takes an account how the
streams are being joined together so
going through the operators we provide
they mirror the closure sequence
operators so these do what you would
expect and we have parallel versions of
these again we have parallels to closure
operators to take parts of a stream we
have an additional one which I'm not
aware of enclosure it's a way to just
combine two streams together that's the
MUX and then the let operators those
really come into play with the need to
operate on multiple streams of data in
an expression so here's an example in
this example you can see if you look
closely there's actually two data
streams flowing through here in the top
in the binding for the let we have one
source data which has this hello and a
simple test so what this will do is it
runs that stream expression to
completion to get a final reduction
value and it binds that to the symbol
word count and now there's a second
stream being processed in the body of
the left so we're able to use the result
of processing the first tree in the
context of processing the second tree so
you can think of this is a node in RS
expression where two streams came
together
we have another operator which is
similar but different it's this let's
stream and what this does is if you look
at the main body down there we reference
this the symbol tuples in two places so
let's stream allows you to assign a name
to a sub expression and use that in many
places in your main body so this again
is a way of having multiple input
streams which here are just copies of
the same stream but you're processing
them as two separate inputs to the body
of the let so our chunkiness of our
streams we allow that to leak through in
a few places so that you can actually do
operations at the chunk level if that's
appropriate for what you're doing so you
can map over chunks at a time reduce
over chunks a lot of times the order of
data in these is important so if you
have data coming in on a stream and you
want to preserve that order you can call
a number operator on it and it will
number all those chunks you can then
send it through some parallel processing
that scrambles the order and then you
can apply apply the reorder operator
after that and it'll put it back into
the original order that it was in before
the parallel operation the reach hunk
operator you might have operators
operations that make chunks really small
which hurts your efficiency so if you
can put our compiler can look at that
and put in a reach on cooperator where
needed to get back up to full size jump
and finally we have a couple of
operators that really just used by our
compiler and the node operator when the
compiler runs it'll make decisions about
which parts of the expressions should be
grouped together into a node and we
actually annotate the stream expression
with those so here we decided to put one
above the P reduce and then another one
above the map above that so we capture
that in the expression expression so
it's available for downstream use you
don't to recalculate it and it's helpful
for debugging another node that our
compiler adds it's really just for
debugging is this number operator
and this is really just like line
numbers we number the expressions in the
tree so that as we're looking at debug
output and as we're looking at kind of
the names of various pipes that make up
these trees we can have a road map that
correlates those back to places in the
original stream expression so just like
with UNIX pipes you have built-in
operators and you can add your own you
can write macros so here's say we're
going to be doing a lot of this word
counting and we want to parameterize it
by the regular expression that you want
to provide this is just a plain closure
macro are our stream expressions are
just list so you can use whatever
closure mechanisms exist to produce
those whether it's functions or macros
this does the obvious thing so here I
change the regular expression to be an
underscore and it counted up for words
based on underscores the split between
words you call macro expand on that it
just does what you'd expect it expands
the macro out to the core operators that
we've defined so yeah for me this just
brings a smile to my face it's a very
satisfying way to cast the problem and
to build the solution to it coming from
a team where we've solved this problem
in the straight java space you know we
can attest to this is a much more
pleasant way to go about solving this
problem and yeah it's good so digging in
in a more detail into how the compiling
works so I've shown this compiler which
is the path on the Left I'm showing here
you take a stream expression you convert
that into this node pipe structure so
that implies a certain interpretation of
what those operators mean so we have a
compiler that knows what those operators
need we have another implementation that
understand or that gives a different
meaning to those operators and we'll
convert a stream expression into a
closure sequence expression this is
useful for debugging because it takes
all of our executions and stuff out of
the pic
sure but it also turns out to be very
useful for the compiler itself so the
compiler its producing a node pipe tree
that corresponds to the stream
expression but if you think about what
the task node needs to do inside of each
node what the task function needs to do
at the time the task function runs it's
running over a chunk from the stream so
at that point the input data to the task
function is just a sequence so what we
do is we convert the stream operators to
their closure equivalence and that's
what goes inside of the node so at the
bottom when it actually runs it's really
just running closure sequence operations
so that wasn't it all obvious to us when
we set out on this but I thought it was
a pretty neat result as far as how it
ended up being realized so we've gone
back and forth with different ways to
implement our compiler this is the way
it is right now so I'll talk about it
first we have a zipper which knows how
to walk through the S expression and
then we have a multi-method that knows
how to compile each node if you haven't
seen zippers it's I think of it as just
kind of a functional trick to deal with
an immutable tree when you want to
mutate it so our compile multi-method it
expects to be given a list so it
dispatches on the first value in that
list so we have a P map and a P reduce
you know each of these symbols we have
implementations for ad structure their
arguments that they expect they produce
whatever they're supposed to do and I
kind of give you a hint at the bottom
here of what the code looks like we make
this expression zipper that'll zip
through the expression and call compile
on each node so that's a zipper
multi-method implementation we've also
gone back and forth with a an
implementation based on closure eval so
we have a zipper that knows how to walk
the tree but of course closure knows how
to walk s expressions so we have
alternate limitations of these where
we've implemented all of our operators
as either functions or macros
so they're rather than us zipping
through the tree you just call eval on
it be about the closure evaluator walks
through the tree and invokes our
functions at the right point just
speaking for myself personally not
necessarily the team we've had the
zipper version is easier to deal with it
could be a failing on my part because I
mean bottom line macros are hard so the
compiling we're trying to do is fairly
challenging work and to add into that
the mix of having to deal with
understanding you know the the read time
and how things are being evaluated and
you know getting all your squiggles and
ticks just right it's just it adds up so
we go back and forth between this I
personally go back and forth between
this I can go back and look at the the
eval based versions and I say it looks
pretty easy but I forget how hard I
fought for each one of those mysterious
symbols so I don't know for what it's
worth that's what we've done kind of a
pragmatic approach we've wanted to
follow the list philosophy and the
closure philosophy at the same time you
know we have to write code that we're
comfortable with and that we feel like
we can maintain and push forward so
that's what we've done so kind of in
summary this is how we've approached our
dsl we express it as s expressions and
for the most part we pass it around as
data so will we can pass those around we
can do optimizations on those to try to
make more efficient or just simpler
queries as you may have noticed we use
unqualified symbols for our stream
operators and these expressions that
again as a result of the fact that we
look at these a lot and we want to see a
nice trim version like someone would
write not like the expanded version like
you would get if you if you called if
you let the macro if all the symbols
were expanded to be namespace qualified
so because we have this very well
defined path of the skeleton of this
tree is our operators we avoid general
code walking and we allow users to write
macros to to add their own operations
into that so the thing that they can't
do is like where this team @ + is you
can't just put a function in there
can't say you know if it's evening go to
this database and if it's morning go to
this database we haven't had any drivers
for that but if we did have that sort of
need then that seems to me like an
argument for the eval based version of
the compiler because now obviously eval
would know how to run that so something
that we've done that I haven't
necessarily seen other folks doing is we
have multiple kind of compilers if you
will that give different meanings to
these operators so I'm really actually
curious to hear feedback from folks on
that so like like I showed we have a
version that creates pipe node
structures we have another version that
converts them to closure just playing
closure sequence equivalents and with
some of our other libraries we have
things that converts them to like maybe
a record tree structure so we ended up
having these operators but then we pour
in different meanings for those so that
when you kind of evaluate it you get
different outputs depending on your
context so then to bring it back
together to how do we actually use this
in our applications we're basically a
database so we bring in we have queries
coming at the top we parse those we
create a representation of the plans
actually using closure records we
perform you think of kind of doing
refactorings on that or optimizations to
change the shape of that query without
changing the meaning we look for like
dead parts of it we look for duplicated
parts all that sort of thing so we can
do all that up at the query plan level
and then we convert that query plan into
a stream expression representation which
of course gets turned into this pipe
node tree and gets run and the way I
like to think of what we do is we take a
query in at the top and then our code
will write a program which will satisfy
that query and then we run that program
on the fly so again for me it's a very
satisfying way to think about the
problem and it turns out to be really
powerful so what else do we have to look
at we're in the process right now of
adding several operators that are
specific to tuple operations so
everything that I showed so far is the
deals I mean it doesn't care what the
data type is
could these strings it could be maps
whatever that obviously we deal a lot
with tuples so we're adding in these
extra operators that do the things that
you do in a query like projecting and
joining and computing aggregates so
we're following the same pattern that we
followed for the stream expression dsl
for this we kind of we see it as
layering another dsl on top of that
that's all about the tuple operations
another item that we see down the road
is to run these processing trees in a
distributed fashion so the way I think
of this right now is we would break the
processing tree up into parts we would
farm out those parts two different
processes and then provide a pipe
implementation to tie the processes
together so this is actually a pretty
active area there's a lot of it real
interesting links like roots here is
click nathan martin storm it's kind of a
totally different take on this problem
Zach Tommen's lamina we actually started
with that and ended up kind of our
pressures were different than his I
guess so we ended up splitting off and
doing our own thing but there's lots of
very interesting libraries out there to
take a look at and I could say we're
shipping products so if you want to
check out our products here they are
along that lines rebel it occurs we've
been hiring and I'd say we've got a good
team we're solving hard problems I think
we're doing good work and reason closure
so you can't beat that so thank you very
much for your attention thank you
closure cons folks for having me and
thank you rich foreclosure it's good
stuff that's it so questions
a couple minutes for questions Chris or
okay anything you comment on yeah I'd
love to know that's something we need to
do as far as I'm concerned we have the
skeleton of a system up and now there's
many man months of tuning of optimizing
just gathering heuristics on how to
effectively use this piece I don't have
it yes so reporter implementation
around occupational
it's
i oh
okay that's interesting so the question
is for joint is mainly about paralyzing
criminalizing computations not I oh it
actually comes back to one of our key
design constraints finest life and that
is when we use that worker thread pool
is we really isolated from the i/o so we
don't do any of the i/o and in the
Portland rule itself
so here we only use the port removal for
the processing of the node functions and
it's asynchronously triggered after the
data has arrived so we specifically
didn't want to tie up any of those pork
loin thread waiting clocking source your
question
are they closed that the question okay
so I'll try to answer that so the
question is we have multiple threads
running in a note are they just clones
of each other how we were made them so
kind of come back to this picture we
don't believe allocating threads to a
note but rather in each node we kick off
these tasks which get scheduled in the
Fort Drum cool so you can think of those
tasks as really lightweight threads I
think is useful way to think of it so
then you can think of our processing
knows as there's many tasks running in
them which make them really close to
unity processes now you have state which
is like their memory after space and you
have many little threads right now but
we don't assign a thread to a node and
in census or sweating a task so thread
the worker threads will periodically go
through different nodes and you have the
the underlying nodes that we write they
have to be carefully constructed so that
those the state is accessed in a safe
manner so we do that work once kind of
in those base operators the mapping and
the reducing they all ones that you
state you state a safe way and we get
that implemented once an immune is build
on top of that we can write a stream
expression that just leverages all that
without having to think about the detail
concurrency requirements
yes we mentioned that you tracked
exceptions within the entire tree and
that different note goes and exceptions
it papi ended up and how do you mean by
that that it stops the competition at
that point yeah because yeah is there a
way that are there options to construct
a processing a graph that resilient i
mean i do believe costume that when it
goes you just lose a certain proportion
of you all right that's not part of what
we do but there's no reason you could
write a task own function yourself that
caught exceptions and handle them in
some way right we don't have a nickel
typically how many do you know you don't
expect the problem i said no we don't
have a model that allows that
yeah thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>