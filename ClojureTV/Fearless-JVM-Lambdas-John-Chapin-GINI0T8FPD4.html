<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fearless JVM Lambdas - John Chapin | Coder Coacher - Coaching Coders</title><meta content="Fearless JVM Lambdas - John Chapin - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fearless JVM Lambdas - John Chapin</b></h2><h5 class="post__date">2017-03-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GINI0T8FPD4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to fearless jvm lambdas just to
clarify I'm talking about
AWS lambdas in this case my name is John
Chapin I just started a consultancy
called symphonia with with a colleague
and good friend Mike Roberts and we're
focused on the server Louis and cloud
tech so that's that's one reason we're
talking about about lambdas today we're
also we've been closure ists for five or
six years now so that's that's sort of
where these these two topics intersect
just a quick little bit about us so like
I said we're service and cloud tech
consultancy based in New York doing
everything from strategy all the way
down to sitting and pairing and
bootstrapping teams we're giving a
couple of workshops that are coming up
one at OSCON that's a half day and one
at Q Khan in New York there's gonna be a
full day and those are just gonna be
around building scalable service
serverless applications we're gonna be
using the JVM I think Java for both but
both of those and then we have a report
coming out with a Reilly soon titled
what is serverless so feel free to look
for us there and we've got a pretty
regularly updated blog and you can grab
us on Twitter as well the other thing I
wanted to plug real quick since we're
here in Portland is that we're actually
really happy to be sponsoring the write
speak code conference for 2017 so that's
a four-day conference for women and
non-binary software developers it's
August 23rd through 26 right here in
Portland the CFP is open through April
23rd and you can find them on the web
there so I encourage anyone who's
interested
spread the word or put in a proposal so
cool so getting back to JVM lambdas so
here's the pair of the burning questions
we're gonna try to answer in in today's
talk we're gonna start really high-level
so what is serverless and then we're
gonna discuss some use cases good you
know both good use cases and bad then
we're going to go deeper what is a dub
you slam de how does that work how does
that work with the JVM and then one more
step how do we write good rather great
AWS lambdas using closure on the JVM and
then we're going to answer this last
question here you know so we have this
kind of
this nice choice that not a lot of other
you know programming communities have
when working with a platform like AWS
lambda we actually get to choose between
the JVM or the node.js runtime I'll go
into a lot more detail on that so those
are the questions we're going to try to
answer this is the agenda that's going
to kind of cover that if we have some
time at the end I'll talk a little bit
about logging in metrics but those uh
we'll see if we get there or not so cool
and uh please hold questions till the
end and I'll stick around here and I'll
be at the unsession z-- tonight and feel
free to grab me wherever you see me in
the conference so cool so let's get
started so what is serverless yes there
are servers the point here is that we as
users of these platforms and users of
these services aren't messing around
with with individual servers so can I
get a show of hands real quick who is on
the public cloud right now who's using
the public cloud AWS or so okay so
almost everyone there are some reasons
not to but most of us are most of us are
on public cloud so these these are sort
of benefits of serverless and these are
gonna echo a lot of those benefits why
we move to the public cloud in the first
place which was to reduce labor cost
reduce risk
resource costs drastically increase our
flexibility of scaling but the thing
that we that we're excited about it's as
symphonia and it's just that I'm excited
about whenever I talk to anybody about
this is shortening lead time so that's
the time between conceiving of an idea
and being able to push it to production
and try it out and get feedback you know
from your customers or whoever and and
and keep cycling on that so shorter lead
time is really the thing that that
excites us the tech is interesting the
cost savings are interesting but it's
helping teams move really fast that's
really the cool part really where we've
seen a lot of a lot of benefit and my
partner Mike Mike Robert so he wrote an
article on service architectures I'll
have the URL at the end goes into a ton
of detail on these benefits we're gonna
cover a lot of those things in that
O'Reilly report as well so let's move on
so what are traits of server lists
either systems or components so these
are some of the traits the costs are
based on precise usage so if you're
not using one of these things like if
you're not using lambda you're not
paying anything for it so compare that
with ec2 where if you want to have an
ec2 instance idle you're still paying
for it right the provisioning is based
on usage not instances so what that
means is that instead of like you know
taking dynamodb for example instead of
saying I want ten machines worth of
DynamoDB to handle whatever my load is I
say well I want to make this mini
request I want to read and write this
much data to DynamoDB right so there are
no long lives you know trait I'm sorry
no longer post or application instances
so we're never dealing with something
that's got to be kept alive or you know
that looks like a a machine these
services and systems are self auto
scaling and auto provisioning in in most
cases so what that means is that as the
load increases they auto scale to meet
that load and they all have high
implicit high availability and what I
mean by that is that we're not concerned
if an individual host or instance or
process that's behind that service or
that platform goes down somebody else is
managing that floor or somebody else is
on the pager for that in our case the
service we still use it as normal and
maybe we get you know transient error or
something like that but we do have to
deal with but so anyway so these are
some of the these are the traits and
some benefits of server lists I'm going
to talk about how we divide up the sort
of server list technology world into two
areas and so the first of those and
again Mike goes into a ton of detail on
this and his in his article but the
first of those areas is called back-end
as a service so these are generic hosted
application components that we bundle
into our own apps and so examples of
that are things like firebase if anybody
use parse that was a popular one a
couple of years ago people doing like
backends for mobile apps would use would
use parse importunately it's gone now
things like Cognito
in auth cro so another question we like
to ask is is how many people have
written their own logic to deal with
users to deal with you know signing up
or authenticating users right all of us
have done that
and was it different the second time for
the third time you did it probably not
so that's sort of a commodity thing that
these these services cover so these all
have those traits that we mentioned in
the previous slide and so that makes up
sort of one area of what we like to call
service now the other area and the one
that's gonna seem more familiar and this
sort of the subject of today's talk is
functions as a service and so these are
these are platforms where you just take
a little bit of code you ship it to that
platform and it gets run in response to
events or in response to API calls the
platform and the runtime are managed and
some examples of this we're gonna go
into a debate lambda in detail
other examples Microsoft Azure functions
Google Cloud functions which is now out
of alpha into beta as of a couple weeks
ago open wisk auth0 has one called web
tasks the some of the folks it's not
Trello but the other company associated
Trello just put one out as well anyway
so this is the other sort of large
technical area so there's backends of
the service and functions as a service
so let's talk about we talked about some
of the benefits let's just touch on the
challenges and there's plenty of
challenges to this stuff as well it's
not a panacea it's not a magic bullet
for anything and I don't want to try to
give you that impression so when you
have these these little ephemeral
components or these these ephemeral you
know things sort of that you're dealing
with state is a is a big challenge so
fortunately as closure programmers we're
used to this sort of trade offs you make
when you when you're dealing with state
and so an example we like to bring up is
like that
the Heroku 12 factor model where you
offload state from your processes right
so if you have state in a process you
push it out to a database or something
like that and you sort of count on this
ability to be able to kill a process or
bring it back up you know sort of
whenever you like
latency is another huge challenge of
this so when you have all these little
components and this is similar if you're
running a micro services architecture or
something like that you have the same
problem right lots of these little
components talking to each other over
network server message buses and all of
those little communication delays add up
right so very very low latency
applications may not be a
good choice so the other one is the
other sort of inherent challenge is this
loss of control and so we had this when
we moved from running our own you know
having our own bare metal machines and
our own data centers we gave up a little
control move to the public cloud right
and then we gave up a little bit more
control moving to a service architecture
in many cases the benefits that came
along with with giving up that control
we're worth it the trade-off was worth
it
in some cases maybe not maybe you do
want to have control of that full stack
so and then the last these last two that
I've got listed here we're not sure that
those are inherent flaws there
definitely there definitely challenges
right now but we hope they'll improve in
the future so testing testing server
lists applications is certainly
difficult right but there's a couple of
ways to look at it
on one hand these are very small
components and so unit testing them is
usually quite straightforward
and so you know you have a lot of
success exhaustively unit testing you
know little lambda functions on the
other hand into end integration testing
is extraordinarily difficult it's almost
impossible to do locally it becomes a
situation where we advocate things like
you know if you want to test this
serverless application into end spin up
an entire the entire application and all
of its associated infrastructure in a
different AWS account for example
because there's also you can also sort
of stomp on on existing resources if
your test in the same account that
you're using for production we like I
said we expect these things to improve
this stuff I'm gonna go over the history
just just briefly and we'll see how new
it is so we really as a community and as
an industry haven't really arrived in
best practices for a lot of this stuff
yet although a lot of what we've learned
doing Microsoft micro service
architectures will help tooling is the
other sort of challenge right now so I
think the only platform that has a good
sort of debugging story for example
where you can actually attach a debugger
to a running process and look at the you
know inspect the the state of the the
state of the process
I think Azure functions
Visual Studio integration is the best
story for that now but that doesn't
exist for most of the other platforms
out there and the tooling around
deployment and around managing sort of
big you know systems of these things is
still very raw we haven't figured out
quite the right level of abstraction but
we're getting there so so fundamentally
serverless
is this area of backends of the service
and functions as a service so that's
sort of that that's like the twenty
thousand foot view so let's just briefly
touch on why why would we choose service
what makes for some good service use
cases what are some bad ones what are
some good ones so some really terrible
ones and I've hit a few of these these
things already if you go back to those
sort of challenges so very low latency
applications not a good choice for
service right so you know something like
high-frequency trading we may never see
that in this type of architecture
large-scale in-memory stateful things
those problems that you're trying to
solve that just require tons of memory
and are super stateful so also not good
choices long-running stateful processes
are also not good choices these just
sort of run up against some of these
inherent limitations in server's
components especially functions as a
service components so on the flip side
of that what are some marvelous service
use cases so this first one here is
actually this is an area where that it
had quite a successful experience doing
building a system that was using ABS
Kinesis and lambda to do billions of
events a day terabytes of data but the
the attributes of that system that made
this successful with a service
architecture whether it was tolerant to
some latency and it was asynchronous
this is data coming in being processed
and going through a pipeline and then
eventually ending up in a storage system
so that was a great use case latency
tolerance synchronous applications so if
you're backing web you know like a web
api or a mobile app or something like
that
when I say latency taller I don't mean
latency of minutes I mean latency of you
know tens or hundreds of milliseconds
being acceptable another great use case
and this is actually where
this is sort of the gateway drug for
service for a lot of people is gluing
little pieces of infrastructure together
or little little you know operations
processes or you know orchestration so
you know examples of this might be in
cloud formation you can you can call out
to a lambda function to do a little bit
of logic and and and you know put that
into your your your infrastructure
automation so the other thing I want to
mention is so we talked about so the bad
you survival's use cases and the good
ones don't be afraid of hybrid
architectures either these are okay
there's not there's not a
one-size-fits-all answer and so for an
example this you know this is a this is
a very abstract example but if you had
API gateway and lambda so those are both
what we would consider service
components and then RDS which which is
not people do this all the time the
thing you have to be careful of just
like you have to be careful of this in
any you know like a micro-services
architecture is that different
components have different properties
especially properties of scaling so it's
very easy for components that scale very
quickly and very wide to overwhelm
components that don't and so there are
there are strategies for dealing with
this but the point I just want to make
here is that it doesn't have to be all
one or the other if some parts of your
problem are best solved by a non
cyrillus component or piece then
investigate ways you can use that
because it's it's it's probably doable
so cool all right so that's like I said
that's the sort of 20,000 foot view in
some of the use cases so let's dive into
AWS lambda cool so at a glance so we've
talked about functions as a service you
know sort of one of these one of the one
of the two technical you know sides of
service in general so this is a
functions as a service platform its
build in 100 millisecond increments so
one of those those traits of service
things is that is that we scale and we
build in very granular very elastic in a
very granular an elastic way so it has
these runtimes available for we're going
to talk mostly about the Java runtime
which we can use closure with and then
also node Python and interestingly
c-sharp so you can
sort of tell where Amazon thinks the
competition is it's event-driven and so
all of these integrations that lambda
has with other AWS services most of
those are happening the form of events
coming in to the lambda platform or you
can call an API you can just say hey hey
invoke you know this function with this
data and it's got these synchronous and
asynchronous invocation patterns so you
can invoke a function and wait for the
response to come back or you can invoke
it and then just it just goes on its way
so cool briefly about the history so I
was surprised putting these slides
together
I thought lambda was like a few years
old at this point it's like two and a
half I've been excited about it for what
feels like forever but it's really not
that old so we got we got no js' a gig
of memory in 60 seconds worth of runtime
back in November 2014 a year later we
got a Java Runtime and perhaps not
coincidentally more memory and then
we've gotten steady improvements in that
since then so API gateway is a huge
enabler of this stuff to back you know
mobile apps in api's and things like
that
we got Python more runtime in October
2015
B PC support versioning etc etc etc I'm
not gonna have time to go into all of
these things and there's even more now
that I haven't put on the slide because
it's just you know every couple of weeks
we get a new feature with either the
land of platforms specifically or some
of the things around it so speaking of
the things around it I wanted to just
touch on the rest of this is not the
rest is a sampling of the AWS server
this ecosystem so these are pieces that
play well with lambda well so obviously
lambda plays well with itself API
gateway so an interesting thing is
things like dynamodb and s3
SNS sqs these are these are services
that have been around for longer than
we've been using the word server lists
right so these have those properties of
you know really elastic scaling we're
not talking about host instances we're
not provisioning things in terms of
machines right so those all play really
nicely and are a great part of mini
serverless architectures we're not
getting unfortunately we're not going to
dive deep on anything other than lambda
today but these are out there and
there's plenty more so
cool alright so the lambda runtime
environment
if anybody's used it before you're
familiar with this this is irrespective
of the language runtime you choose so
this is the the environment that's
available for any lambda function so
right now you get between 128 Meg's and
1.5 gigs of memory the minimum CPU speed
and i/o scale accordingly to that and
I'm gonna show you a really weird graph
later that's gonna explain why I chose
the word minimum there you get two
virtual CPUs 500 Meg's attempt space
there's some limitations on the size of
the the code artifact that you can
upload to the platform and then standard
out in standard air go to cloud watch
logs and like I said if we have some
time at the end I'll talk a little bit
more about cloud watch logs so cool so
this is all publicly available this is
what's documented this is what's out
there you know for you to just sort of
you know for you to see but if we peel
back the curtains a little bit so
there's been some work done in this area
by by people that are curious so there's
a company called IO pipe that have done
a lot of introspecting of the containers
that lambda functions run in they have a
monitoring solution for Python and
nodejs
but so so behind the curtains lambda is
is container based so it's LXE
containers those containers are created
on demand so when you you know when you
create a lambda function and you invoke
it for the first time or it is invoked
for the first time that container is
created with your code in it the I'm
gonna go into more detail on this when
we talk more about the the closure
lambda specifically but basically the
minimum lifetime of that container is 5
minutes so if you hit it once and then
don't do anything else with that lambda
function that container will stay alive
for about 5 minutes the maximum
container lifetime interestingly is 4
plus hours for for a long time the the
community thought it was just four hours
until very recently we've seen some much
longer live containers so what that
means is that if you invoke a lambda
function it brings up the container if
you're regularly invoking that function
that same container will stay warm for
several hours another interesting
property is that containers can be
snapshot at this is snapshot at after
they've been initialized
container starts up it loads your code
your code goes through whatever
initialization that it needed to do
we'll talk more about those steps that
can actually that can be snapshot it and
set aside and then if another request
comes in later and it the the platform
can take that snapshot and sort of
rehydrate it and use it that means that
you're not necessarily going through the
same reinitialize ation process but if
you did things like open network
connections to databases you can have
issues there so something to be aware of
so we talked about we're talking about
sort of containers being created and so
that's it's got kind of a word that has
a lot of negative connotations with it
that's cold starts so container cold
starts as sort of like the big gremlin
of the of the server lists and functions
as a service and specifically lambda
world so I want to explain what goes on
in a container with the cold start and
then I'll talk about what goes on in the
JVM with a cold start so like I said the
lambda platform receives an event that
containers instantiated with whatever
code you've configured to you know to be
attached to that lambda function the
language runtime is initialized and then
whatever process it has to go through to
load your code in initialize your code
that takes place and then finally the
last step is your handler function that
you've configured in the AWS console is
invoked with whatever your event data is
so those are the steps that a container
goes through when it cold starts so what
what causes these cold starts what
causes these containers to be created so
obviously the first invocation so if you
created a new lambda function you invoke
it for the first time you go through all
those steps if you change the code or
change the configuration the platform is
going to say what would you change
something so let me let me get rid of
all those old containers and not you
know invoke them anymore and create new
containers representing this new code or
configuration if the concurrency that
your your lambda function needs to
handle whatever you know whatever is
going on in the rest of the platform if
that increases so it's creating more
containers to handle more concurrent
operations so each of those containers
that's created undergoes a cold start as
well and then there's this idea of
container reaping so we talked about a
maximum
lifetime for these LXE containers and so
at some point the platform will just
decide you know hey this container has
been around for four years or four years
four hours I'm gonna kill it and start a
new one for the next request cool so I
promised a really crazy graph and and
what this is gonna show I I feel like I
have to explain it before I show it
otherwise people get kind of sucked in
this is going to show that the what's
documented as far as relative
performance of lambdas based on the
memory configuration is the worst case
so here's the crazy graph so what this
is showing this is this is the same
simple benchmark it's just a simple
Fibonacci bitch mark running over about
48 hours and it's the same code running
just with different memory settings so
128 Meg's which is the sort of least you
know powerful lambda up to 1.5 gigabytes
which is the most powerful one and so so
this is this is weird-looking right the
performance is all over the place so
this is the the duration in milliseconds
that that benchmark takes to run so
lower is fast higher is slow so in this
highlighted section right here hopefully
everybody can see that this has things
spaced out the way that the way that
it's documented right so that 128 mega
is the slowest one and then at just the
right steps proportional to the memory
setting we improve the performance right
and so that 1.5 gigabyte lambda is the
fastest so cool so that makes sense
that's what's documented this is this is
the worst case what's interesting is
that sometimes the performance can be
much much better and so in this case the
you know the 128 mega lambda and the 256
Meg lambda are running at about the same
speed which is actually also you know
very close to the one gigabyte and the 5
the 512 is in there and you know none of
those are much you know are very far off
of the 1.5 gig so so what's going on
here sorry let me just scroll my notes
look cool
so what's happening here is that these
containers went in this worst-case
situation those are containers that have
been scheduled on a host instance on the
platform that's already loaded right so
it's just adhering to whatever the
minimum guarantees it needs to to make
sure everybody gets the you know the
amount of CPU that they paid for but if
a container is scheduled on a very
lightly loaded host instance it can use
more resources than its necessarily you
know had allocated and then as things
you know as things move around you know
the performance will change so what's
happened over here on the right side is
that and and some of this is speculation
on my part but is that each of those
separate lambda functions is in a
different container probably on a
different host instance that probably
doesn't have much else running on it so
they're able to take advantage of that
performance now memory like raw memory
usage does not work like this if you
exceed the amount of memory that you've
configured for the lambda you get killed
but CPU performance does have this this
interesting variation so two takeaways
from this graph takeaway number one
benchmarking lambdas is is hard if
you're benchmarking a lambda just by
running it a few times in succession and
it's not the 1.5 gigabyte memory setting
you might get badly wrong information
right so you could benchmark you you
could you could run 128 mega the lowest
powered one you could run that 10 or 30
or 50 times in a row
and think wow this thing's this thing is
performing great and then when you run
it in production a day later it's ten
times slower so the other point is that
in addition to the performance that
you're paying for with the the 1.5
gigabyte lambda you're also paying for
consistency right so that's just that's
going to be the most consistent
performance that you're gonna see and I
think I missed the x-blade
I missed explaining this earlier lambdas
built in those 100 millisecond
increments and then memory is a
multiplier of that right so you're
paying by the gigabyte second
essentially cool
so we've were 25 minutes in I'm gonna
try to speed up a little bit and let's
talk about closure on AWS lambda so is
anybody in the room running closure on
the JVM on lambda right now in
production fantastic
cool I would love to chat with you guys
afterwards and just find out more great
whoops that's not closure so I wanted to
do so so all the documentation is is
based on Java so I wanted to start there
and then transform this into closure so
we can sort of see how it goes right
this is one of the simplest lambdas you
can write it's valid it's a valid lambda
it's got a class and a handler there are
no special AWS libraries required for
this it's it doesn't have a special AWS
interface it doesn't have a special aw a
superclass here it's just the signature
of that Handler and it's using it takes
that handle or configuration and just
use this reflection to find this and
invoke it and this is how you would
specify that particular handler in the
in the lambda console when you configure
your lambda or through the API you can
also do it there the package name the
class name then the handler so let's
turn this into closure so a little
different but but not too bad it's just
a standard gen class you'll have to a uh
compile this and when you when you use
gym class and specify your method like
that you have to handle two arguments to
the to the function one is is the sort
of this that represents the the class
the instance and then whatever the input
is so yeah so this is basically the same
thing I think I lowercased
the namespace name to be idiomatic
enclosure so you end up with a lower
case class name so cool
so that that is as simple as it can be
you don't actually need anything up
anything other than this to run your
closure code on the JVM in lambda so
that's that's cool I was I was you know
I I think that's great
so cool and to deploy that it's just a
simple uber jar for us for us you know
JVM programmers and for everybody else
is the zip file and of course as if is a
jar file so that that all works fine
great cool so I'm gonna back up back out
to the runtime just for a minute here
and talk about lamda in the JVM I
promised I would tell you sort of how
the JVM you know handles these cold
starts and how to deal with that and so
the first time you think about it you
think oh lambda functions you know
they're sort of ephemeral they can come
and go the platform's just gonna do
whatever it wants and I have to handle
these cold starts and so an ephemeral
JVM that sounds like a really great idea
right said no one ever
cool so the so the JVM runtime just to
run down the staff so it's open JDK 1/8
so server VM the map's heap size is
always going to be at least right now
it's it's always set to 85% of whatever
you've configured the lambda memory as
it's overriding the default using the
serial garbage collector I don't know if
that's gonna change for for Java 9 or
not it's using tiered compilation and
it's using class data sharing
so all those all of those options are
well
sorry those last those last two ish
options are geared towards making cold
starts a little less painful we don't
have control over those startup flags we
can we can inspect them and see what
their values are and so we'll be able to
see when they change but we can't change
them unfortunately
cool so JVM cold starts so this is you
know we've all we've all seen that the
JVM can take a while to startup this is
what's going on so it's it's doing class
loading it's doing any kind of
initializations calling constructors and
you know static initialization blocks
unfortunately in the case of you know
for us closure programmers it's also
loading the closure runtime there's a
great section on the closure wiki that's
sort of like looking towards the future
and talking about ways to make closure
startup much less painful and much
faster lots of group there's lots of
great ideas in there and hopefully we'll
see some of those come out soon and then
as you run the JVM it's doing this JIT
compilation so if you're you know if
you're if you're
if you leave a JVM up and running it's
going to improve in performance you know
as it as it runs especially as you do
the same things over and over again so
so knowing what we know about JVM cold
starts and how to build these deployment
packages you know to load our closure
code into lambda how do we how do we
make that as fast as it can be because
keep in mind we're paying by the hundred
milliseconds right so any you know every
time we shave 100 milliseconds off of
this we're actually paying less which is
a property you don't get in a lot of
systems so so this is what I like to
call the lambda diet I'm gonna sell an
exercise video as well so I mean
fundamentally the the key here is that
fewer classes is a faster startup right
and again that entry on the closure wiki
I mean they talked about this at length
where they're saying okay if you do a
simple class in Java you you have to you
have to load literally one user class if
you do the same thing in closure you
have to load you know a thousand classes
that are you know all the closure core
stuff and so on and so forth so if we're
choosing to use closure to some extent
we we can't affect that but we can't
affect and we do have control over the
dependencies that we bring into our
projects and as closure programmers as
Java programmers I know I'm guilty of oh
hey I need that one little thing from
this library let me just dump that in my
line engine file or my maven pom and
then you know then I look after six
months of doing that and my dot m2
directory is like 60 gigs or something
so we're used to just pulling in
dependencies when we need them and not
really thinking about maybe how big they
are or what
what kind of transitive dependencies
they bring in so we need to ruthlessly
call our dependencies it's not to say
you can't have dependencies it's just
that you should be very conscious of
what you pull in and why you're pulling
it in and I'll talk a little bit about
how to make some of those choices
especially with regard to the AWS
libraries because those are those are
some of the I used to say some of the
worst offenders it's more like you're
you're often using AWS libraries when
you're writing lambdas and some of them
are better than others and if you if you
for example if you pull in the entire
AWS Java SDK
or if you use a library like amazonica
you can easily end up in a situation in
which you've got 40 megabytes of
dependencies for just a couple of
functions right so AWS actually ships a
bomb now this is a little bit more
relevant for the for the maven users but
that's a way to sort of ensure that
you're using the same dependency version
across many different AWS SDK libraries
so get you know get familiar if you're
not already with things like line depth
tree maybe a dependency tree those let
you see dependencies and transitive
dependencies so you can be very sort of
conscious of what you're pulling in
maybe it's a little bit heretical to
mention SBT in this context but I'm
gonna go for it anyway
it's got a cool feature called
dependency stats which in addition to
listing out dependencies and transitive
dependencies it tells you how big they
are and so that's really useful sort of
for saying wait a second I'm just using
this one little thing but it's it's you
know got a dependency on this XML
library which has you know dependency on
something else which is bringing in you
know 40 or 50 Meg's worth of stuff so so
be conscious of your dependencies
unfortunately as we've probably all
discovered this so doing tree shaking in
closure is is pretty hard it's actually
pretty hard on the Java side too there's
a lot of dynamic class living stuff
going on on the lambda platform and so
every time I've tried to optimize a jar
and get rid of classes that weren't
being used it inevitably comes around to
bite me in the end so cool
I talked about a talk about handling
input so one reason that we bring in
dependencies is to get so on the Java
side of things we like to use you know
it's it's it's a statically typed
language so we like to have these typed
events coming into our function and the
AWS runtime will actually deserialize
JSON into pojos for us so that's super
cool if you have a you know it'll see
that handler function signature if
you've got a POJO there it will it will
attempt to take incoming JSON and turn
it into that poggio for you so that's
that's cool and that's super useful on
the Java side but what we often do is we
say okay I want to handle an event from
you know Kinesis for example Kinesis
maybe they might have fixed this by now
but didn't
you know we would say okay I actually
need to go get the Kinesis SDK library
just to get that event POJO and then
maybe I don't do any other Kinesis
operations but I've just pulled in that
entire library to get that one little PO
Joe
so what we've what we've been advocating
what we've been doing is actually
especially for the AWS you know events
sources go out and find the that that
library on github grab the one or two
classes that you need and just put them
in your project a source we're also
working on a more comprehensive library
that's they'll will auto-generate some
of those things you'll be able to find
out on symphonious github sometime in
the near future and that'll let us avoid
this sort of problem of bringing in
these huge libraries just for one or two
little events and the AWS event types
are kind of scattered across libraries
there's there's some that they provide
in a kind of condensed form but
oftentimes they're just there in
whatever sdk library that they're coming
from or the other option and something
that makes a little bit more sense
especially in a dynamic language like
closure is parse the incoming JSON
yourself you can actually just you can
specify that handler function as an
input stream and do whatever you want
with it and so for us it's it's a little
bit easier to deal with JSON in that raw
form than it is in a language like Java
so cool cool so let's get into got about
five minutes left let's talk a little
bit more about closure on lambda so yeah
a ot compile all the things that's we
all could have guessed that so you have
to obviously have to a ot compile that
namespace that's contains your handler
function so that gen class works and
it's gonna transitively a ot you compile
all you know all the namespaces that are
included there without going overboard
optimized for some performance you know
we should all be using one our
reflection to true and then fix those
reflection warnings there's a couple of
compiler options that can help us a
little bit I think these make more of a
difference in closure core where they're
already used unless you're writing like
several thousand line doc strings in
your application code these might not
make it looks the first one might not
make a huge difference but it's worth
doing as is direct linking so
so both of those should result in
smaller class files fewer class files
and faster loading cool so for closure
on the JVM you you basically have to use
the 256 Meg lambdas or larger what
happens is you get these sort of
inconsistent and sometimes transient
errors about JSON decoding even just
with simple strings so I just recommend
starting it start at 256 that's you know
part of the price we we pay for using
closure with all of the search features
it needs a little bit more memory be
very opportunistic about initialization
so when when these lambdas cold-start
like I said that constructors called in
closure you know if you have a death
something that's going to be part of the
static initialization the opportunistic
and do some work there to set you set
yourself up to run very quickly later
but on the other hand handle failure
really gracefully so if you're doing
some static initialization you set up
like a client to talk to a database
don't ignore errors when you then go try
to use that client you know catch your
errors may be reinitialized the client
if you have to during the course of the
lambdas execution so just handle those
things gracefully you can have multiple
lambda functions per closure namespace
and that's a great approach if they
share dependencies and they share that
sort of initialization and set up the
the thing you have to do then though is
you have to redeploy that same jar for
each of those lambda functions so if one
function uses some dependencies and one
uses another rather than deploying the
sort of combination of that to you know
several times to the lambda platform
break them up and deploy each one
specifically with what it needs cool so
this is a little bit more of the oh man
I'm sorry about that blue there it's
hard to read so this is a different take
on the on the handler functions so if we
use static there we don't have to handle
the sort of instance variable coming
into our handler functions so we just
get to deal with the input this is just
showing that you know that death is
going to happen when when the lambda
cold starts and initializes your
code so we're doing our expensive
initialization there and we're doing
some operation with that client that we
initialize but we're dealing with
whatever errors came out of it so this
is and this is all pseudocode but you
get the idea
cool so I mentioned enclosure to start
uptime improvements Java 9 is gonna have
a little some some small performance
enhancements nothing at least in my sort
of glance over what's coming in Java 9
nothing stood out to me obviously
project jigsaw is a whole other area and
the initial you know if that when that
comes initially everything's going to
still sort of continue to work as it
does now but if for example if the
lambda runtime changes to prefer that
over over jar so we're gonna have to
rethink how we build these deployment
packages but I'm not going to go into
too much more detail there I do want to
talk about this though this is closure
or closure script and so like I said
we're sort of lucky we get to choose do
we want to run closure on the JVM on
lambda or do we want to run closure
sorry
closure on the JVM on lambda or closure
script on node on lambda so I did a
little benchmarking these are naive
benchmarks but this is this is sort of
the set up and so this is a short
benchmark this is this is that sort of a
similar little Fibonacci benchmark
closure versus closure script so what's
interesting here is that we see marked
here the cold starts and this is I think
this is about 18 or 20 hours worth of
data so we got three starts for the
three cold starts for the closure lambda
and those are those top ones so in this
in this short benchmark that doesn't
take very long to run closure on the JVM
as a sort of as we expect takes longer
to go through that cold start process
then in closure script on node so if
this is our application profile if we're
sensitive to the occasional cold start
and and you know slower performance in
that case then maybe we would choose
node here here's the sort of the stats
on that so the the average duration of
that function execution is hard to see
in the graph but was was about twice as
fast for closure on the JVM but again
it's that max duration that cold
start where closure script certainly has
the advantage in this benchmark
and it's a little bit more consistent as
well again but overall closures closure
is faster here but when we when we
increase the number of iterations of
that benchmark and we bump it up and so
we get a longer runtime let's see what
that looks like and so in this case it's
really just no contest this is that same
Fibonacci benchmark but with an outer
loop of I don't know 10 or 50 thousand
you know iterations through and we see
that even when closure is cold starting
closure on the JVM is cold starting it's
still it's still you know far better
than the closure script you know similar
versions so in this case if this is if
this is sort of what your application
ends up looking like and this is over
like 40 something hours and I'm not
gonna try to explain all the little the
sort of sawtooth pattern there a lot of
that can be ascribed to the platform
some of can be described to the runtime
but I think we all go crazy if we tried
to dig into every little every little
flip of the graph fundamentally though
closure was well ahead in that benchmark
yeah says like yeah like four times
faster and in no case was was a closure
cold start slower than the fastest
closure script operation so cool and so
this is just this is sort of like my
quick take on how to choose between the
two so if you if you have plenty of
volume so you're keeping a lambda
function warm you should almost always
you know choose choose closure on the
JVM all other things being equal if you
don't have a lot of volume so you're
gonna so you're seeing more cold starts
you know or cold starts are a greater
percentage of the invitations of your
lambda function and you're sensitive to
latency so you really you know you would
actually prefer less latency on average
and less standard deviation from that
latency the enclosure scripts actually
probably the better choice if you have
low volume but aren't that sensitive to
latency then it's kind of a toss-up and
you know use what you feel comfortable
with funding
this is this is all things being equal
if you have a preference foreclosure
script or closure and you have other
reasons then you know evaluate those in
the context of what you're doing so cool
so I'm already four minutes over so I'm
not gonna I'm not going to dive into
logging in Metrix there is a version of
this talk online that's on the Sinfonia
web page that has a video link to a Java
sig presentation that goes into a little
bit more detail in the logging metrics I
did want to flash oops yeah this was
gonna be good too
sorry trying to get to the cool so
that's this that's the TLDR
server lists it's evolutionary it's faz
and baz it's great for web apps data
pipelines keep your lambdas skinny and
warm mostly use closure on the JVM and
and it's going to continue to get better
as we get new versions of closure and
new versions of Java and then the that's
the that's the one sentence of what we
skipped over is use real logging and
scalable metrics and then here's some
resources a lot of this was pulled from
stuff that we've already got out there
and we have a very sort of very mellow
and informative newsletter if you're if
you're interested in server lists in
general and 80 but about AWS and and
Java service specifically so cool thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>