<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Zach Tellman - Predictably Fast Clojure | Coder Coacher - Coaching Coders</title><meta content="Zach Tellman - Predictably Fast Clojure - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Zach Tellman - Predictably Fast Clojure</b></h2><h5 class="post__date">2014-03-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iQwQXVM6oiY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Zack I'm here to talk about
predictably fast closure and hopefully
you learn a thing or two so what happens
when we call and s2 we know to returns
returns the third element of the
sequence but how do we actually get
there if it's a vector then we call
closure laying indexed which has an end
method if it's an array we simply look
up the third element if it is marked
with the java.util random access
interface then that means that we can
efficiently call get on java.util list
and so we do if it's a string or some
other sort of character sequence we call
char at and then box at in a capital C
character because n3 turns an object not
a primitive but if it's none of these
things if it's just a seek then we call
next and then next and then first and
this can basically mean anything because
if it's a lazy sequence we might be
doing arbitrary computation this might
be as simple as walking a linked list or
it might mean that we're traveling to
the moon and back again and so we don't
know what happens when we call n we only
know what it returns and this isn't
necessarily a bad thing referential
transparency is a property that any
expression can be replaced with the
value it returns without changing the
semantics of the program and this is
something that we rely on quite heavily
it means that we can replace a function
with a memorized version of itself it
means that we can treat lazy and eager
evaluation as interchangeable
it means that we can map over a sequence
sequentially or in parallel but what
about these here we map a function over
sequence and then again map that same
function of the sequence but first we
sleep for a second these return the same
thing they're equivalent but are they're
the same and if so what if we sleep for
an hour or a day or a year at some point
we have to admit that the only way that
these two things are interchangeable the
only way that they're equivalent is if
we have all the time in the world and
this is true of most applications of
referential transparency memoization is
only sound if we have enough memory to
hold all the possible
and all the values associated with that
parallelization is only sound if we
don't exhaust one of the many shared
fixed resources specifically in the case
of a ring HTTP server we can only treat
all the requests as isolated from each
other we don't exhaust bandwidth or file
throughput or file descriptors or main
memory or computational capacity if we
hit any of these boundaries it starts to
break down it's no longer sound way of
reasoning about our systems and so when
we talk about referential transparency
it can't be as some sort of pure
mathematical truth it's a convenience
it's a lie it's something which we use
the gain leverage to reason about the
complex systems that we build and it has
utility but it also has boundaries to
that utility and this is true of pretty
much any abstraction that we use these
are tools that we can use these are
things that allow us to build systems
that are as complex as the ones we do
build and they have capabilities which
we need to use but they also have n
capabilities which we need to be aware
of and so when we talk about the
correctness of the things that we build
it can't be in some sort of abstract
sense it needs to be with respect to the
problem that we're trying to solve to
the inputs that we're dealing with the
volume every other sort of dimension of
these systems and if we don't understand
the abstractions that we build atop we
can only say that right now things seem
to be working we don't know what
tomorrow holds because we don't know
what the breaking points are of any of
these abstractions and so I've called
this talk predictably fast closure
because closure is a language which
values generality it gives us these
unifying abstractions like the seek
abstraction which allow us to forget
that for whatever reason in Java the way
to get a particular element in a string
or an array or a Java util collection is
different it allows us to reason at sort
of a higher level within our programs
with this generality is that tension
with predictability for us to understand
what's actually going to happen when we
are concerned about these sorts of
shared fixed resources requires us
digging a little bit deeper
understanding what actually happens
underneath the covers and so I could
have also called this talk looking
into the abyss because we stand atop a
tower of abstraction enclosures just the
topmost layer underneath that is Java
underneath that is a JVM underneath that
is 60 years of accreted abstractions
we're not going to get to the bottom
we cannot reason from first principles
about these things and yet closure
despite being atop this tower is
actually a really good way to sort of
peer down into it I've been using
closure for about five years and I've
become an immensely better engineer
through the use of it because it gives
me the tools it gives me the repple
gives me access to all these Java
libraries it gives me access to
wonderfully reasoned out abstractions
and it gives me a way to start to kind
of dig down to broaden my understanding
to make my understanding more nuanced
and yet um I'm never going to get it
right we're never going to get it right
we cannot again reason from first
principles about this first principles
are out of reach we need to think like a
scientist we need to have a hypothesis
of how our system works we need to build
the top the hypothesis and you need to
be prepared for when that hypothesis is
disproven we need to monitor the systems
that we build we need to understand that
this is an inevitability and that that
is a thing that prompts us to again
deepen our understanding as a system
make it more nuanced and so to be
concrete about this I work for factual
we provide an index of things in the
world and this means that we can be
asked questions about what's actually in
the world I'm here where the nearest
three coffee shops but we can also take
other people's data that relates to the
real world and use our index to clarify
it to give it context and so it turns
out that my job involves a lot of taking
other people's data putting it somewhere
to be process then taking our sort of
clarifying improved version of that data
and delivering it back and despite all
this despite the fact that you know
we're operating at a volume which by no
means is large volume of anybody even in
this room but is larger than anything
I've dealt with prior to now and the
fact that this can with one or two new
contracts double almost overnight I
don't want to live in fear of this
falling over I don't want to be woken up
at 4 a.m. and half
multiplex ssh into a dozen different
amazon boxes that is not what I signed
up for and so my real responsibilities
are in the course of building the
systems that I build I need to make sure
that my understanding of the systems is
in advance of where it needs to be at
the bare minimum I need to feel secure
that when things change out from
underneath me that I am NOT on the cusp
of things falling over and so this talk
is sort of taking the things that I've
learned and trying to articulate them in
the sort of time allotted but also to
kind of give you an idea of the larger
sort of mentality here though the way of
thinking about these sorts of problems
and sort of give you a push in the right
direction if you want to walk down the
same path and so let's dig of it what
happens when we call count well close
your core count has an in-line form but
really just proxies through to closure
laying RT count which is a Java function
closure laying our key count is pretty
straightforward it checks whether or not
it's an instance of closure laying
counted which is true of a vector or a
map or a set if it is it calls dot count
otherwise it calls count from which is a
slightly more complex function which has
handlers for a string or an array or a
Java util collection or if it's a seek
it walks it from beginning to end
counting one each time and the o equals
null idiom here is make sure that we're
not holding on to the head of the
sequence if that's in fact we have to do
and so looking at this if we may sort of
reason that the cost of this indirection
the cost of this unifying abstraction is
not that high if you're dealing
something which is a vector or a set or
a map because we have a function call in
closure core count which proxies through
to a very simple function which has a
single if check and then calls the
function directly and if we do a micro
benchmark using criterium this seems to
be supported calling closure core count
is approximately ten nanoseconds calling
dot count directly is about five and you
may notice that this is in fact twice as
much time but let us never forget that a
nanosec
is not a very long amount of time there
are a million of them every millisecond
so any sort of non-trivial computation
kind of makes this background noise
especially going to desk or going to the
network and so we can take this as a win
we have this unifying abstraction but
we're not paying a particularly high
cost for it
and yet here is a more complex function
which I wrote recently which takes in a
rain takes a vector and tries to find a
matching subsequence within them and the
details are not important but I've
highlighted the fact that I'm calling
count on the vector and then again in
loop calling enth repeatedly on the
vector and this is taking about 200
nanoseconds for a ten element sequence
or a ten element vector rather and I
knew that there's an overhead here due
to this indirection
and in fact this sort of process of
discovery of what it is we're actually
calling count on is being repeated we
first discover that it is a vector when
you call count and rediscover again that
yes it is in fact still a vector each
time that we call enth and I just wanted
to get sort of a sense of the overhead
and so I replaced just the count and all
of a sudden the it halved the amount of
run time and so this call this little
bit of indirection that cost 5
nanoseconds just a moment before now
costs a hundred it's increased by twenty
fold and so you may well ask what's
going on here and the answer is I have
no idea um but I do know something a
little bit more general which is that we
have all of these things that are
between us and what's actually going on
in the machine and they try their best
to flatten in direction the JVM in lines
function calls it has all sorts of JIT
optimizations that try to do something
similar on the processor we have branch
prediction which tries to make is if an
if statement doesn't really exist as
long as we have sort of a predictably
majority sort of outcome there likewise
we have the cache on the CPU which tries
to make it so that we don't have to pay
the cost of going out to main memory so
much and there are countless other
examples of this and what this means is
that all these things are great they
make us able to use these layers of
indirection these abstractions that
again give us the leverage to reason
about our systems and they work
wonderfully
up until the point where they don't and
so the point here is not to sort of harp
on count or any of the other
abstractions that are enclosure I'm not
questioning the utility of them the
necessity of them but when we talk about
performance it cannot be in isolation it
can't be about an abstraction a micro
benchmark is not predictive of a more
complex real-world use case and so we
cannot simply measure different pieces
of our system in isolation and assume
that they will come together in some
sort of clean way it might be slow or it
might be faster but it certainly won't
be the sum and so I'm going to be
talking about certain things I'm going
to be giving measurements in nanoseconds
and take these for what they're worth
which is not very much but with that
said there is a common pattern when
someone first uses closure and they are
somewhat perf minded and that is they
take something that they've written in
Java and they've been told that closure
is based on the JVM and it uses Java
bytecode or JVM bytecode and so it
should be as fast as Java and so they
write something in sort of idiomatic
closure and they run it they say why
isn't as fast and then people sort of
leap to reply and they answer and you
know golf away at the code but usually
one big part of this is that closures
numeric stack is a lot more forgiving
than Java is numeric stack and so we can
add together two primitive Long's but we
can also add together a box long and a
primitive long or things that don't even
exist in Java land like closure Lang
ratio we can add together big integers
and non big integers and it's a very
sort of welcoming environment for math
and as a result the arithmetic operators
in closure are much more powerful than
those in Java they're much more general
a problem here though is that if we want
to make sure that we are doing the fast
sort of arithmetic we have no way to
know and in fact I gave a talk about a
year and a half ago about writing a go
AI enclosure and this was a problem
which was almost sort of artificially
concerned with math performance and I
complained that it was very hard for me
to know what sort of math I was in fact
doing the only way for me to understand
this was to attach a profiler and see
what functions were being
or take the code compile it and
disassemble it and I was putting these
forward is almost sort of punitive ly
difficult ways to understand what's
going on and that was because I did not
know about a very nice library called no
disassemble
which was created by Gary Trackman and
contrary to its name it allows us to
disassemble classes and using it is
fairly straightforward we use notice
assemble we create a function which does
something and then we disassemble that
function and we print out what's
returned because what we're getting is a
pre-printed representation of Java
bytecode and looks a little bit like
this which looks a lot like a java class
because of course a function is just
that and so here we see the bytecode for
the static initializer and for the
constructor which we don't particularly
care about we also have bytecode for the
invoke method which is sort of where all
the magic happens and so that looks like
this and if you're like me you don't
know what JVM bytecode really does but
it turns out this is actually fairly
easy to sort of reason our way through
we have the first bit here which is sort
of a prelude it takes the arguments and
sort of puts them onto the stack in
whatever order we need them and we can
kind of ignore this this makes sure that
the object is what it needs to be for us
to be able to invoke the function and
closures will omit this before any such
invocation and this is again fairly
ignore below JVM is very good at sort of
taking this out when it doesn't need to
double check here we have a bunch of
static function invocations which are
sort of the meat of what's actually
going on here here we take the object
and we turn it into a primitive double
then we turn a call the actual math dot
log function and then take the return
value which is primitive and box it
again and then finally we return that
value and so if we add some type ins
here and we type in both the input type
and the return type as primitive doubles
to match what's actually being returned
and taken by math dot log the invoke
function changes a little bit here it
casts the object to a double and then
calls invoke prim which didn't exist
before this only is created on a
function when we have primitive types in
the input or output values and invoke
prim
is pretty simple it calls math dot lock
and so this is sort of an eminently
inlinable form of a function we can be
fairly sure that the JVM is going to
inline this for us that it will remove
this layer of indirection and so this is
pretty good unfortunately where we were
able to coerce unknown types into
doubles because math dot log only takes
a single thing when we have a function
which is slightly more welcoming like
math.min which will happily take a short
or an int or a long or a double we have
to determine what to do at runtime and
so this at the very bottom here calls
closure laying reflector invoke static
method and this is what happens on the
compiler side when you see those
reflection warnings and the cost here is
small but real that's on the order of
about one microsecond or a thousand
nanoseconds but if we call this with the
plus operator we do not get a reflection
morning because closures numeric stack
will happily accept all comers and so
here we call in the closure laying
numbers add and at the top here add will
check the type of the first argument and
sort of route into the appropriate
operation here we assume it's a long and
so that calls into long ops ad which
will get the primitive long value of
each and then call into ad taking two
primitive Long's
which will finally add them together and
then check for overflow and if there is
no overflow return the value and so this
is what happens every time that we add
two numbers together if we have type
hinted the input types then that will
happily call straight into the last
function the function which adds them
together and check for overflow and then
because we have failed a type in three
turned valuable then box that value if
we type in the return value again we get
this sort of very inlinable form and
this is again you know a very nice sort
of direct route into the math when we
have all the type ins situated but this
is still doing work that Java doesn't do
we're checking for overflow and so if
this bothers us unnecessarily then we
can use uncheck that and we here we see
that we're not even calling in to
closures underlying Java implementation
we simply get an L add or
long add instruction and so this at long
last is as fast as Java and you know
lest you think that unchecked add is
some sort of secret path into performant
math we can just as easily call
unchecked add with untyped arguments and
it will happily go through that same
sort of power of calls at the end though
just won't check for overflow and so
this is kind of a problem again this
generality is extremely powerful but in
the case where we don't want that
generality where we want to narrow the
scope of the problem that we're trying
to solve it doesn't get us very far
we're not getting feedback the
reflection warning is a useful form of
feedback but we don't have anything
which is sort of more nuanced than that
and so in pursuit of this I wrote a
thing called primitive math improve math
is a fairly straightforward library it
has a big Java file which looks like
this which resembles in some way closure
Lang numbers except that it only deals
with primitive values it has one add
that takes too long so an add that takes
two doubles and nothing else and it has
some macros on top of that so primitive
math slash plus XY will expand out in
place to a call to that Java function
and the reason for this is because we
have two types of functions or the same
arity and closure doesn't allow for
collisions in that form and so when we
again have the same type into thing
where we type in all the different
pieces we again get this inlinable form
sadly it does not BL add instruction
because that is the purview of closure
and closure alone but again this is sort
of something that JVM is very good at it
will kind of bring this up and surface
the el-ad from the underlying Java and
then maybe service that again up into
whatever sort of scope that is being
called and so this is of comparable
performance to what we saw before but it
has a very nice property which is that
when we call it without all the
requisite type hints it will yell at us
and so primitive math is not an attempt
to improve on what closure does
it is an attempt to solve a much simpler
problem and give us feedback when we
escape the scope of that much simpler
problem and you know some of you may see
that I'm using job and you think that
you know hey no fair you told us this
can be particularly fast closure and
here we're seeing Java all over the
place but you know this is what closure
is closure is a Java library atop the
JVM and if we want to create again a
narrower solution to a narrower problem
then using Java is often a good solution
at least in part another problem that I
deal with fairly regularly as a result
of all of the data that I have sort of
shuttling back and forth is dealing with
Java IO and this has become somewhat
complicated over time when Java first
came out we had byte arrays and input
streams but in around 1.5 they decided
this was on enough so we got Java nao
which gives us byte channels and byte
buffers and then of course we have
strings which are not exactly bytes but
are sort of a representation and so we
have to deal with readers and all the
other sorts of transformations and the
problem here is that going from any one
of these to any of the other does not
always have a direct solution often we
have to sort of chain these different
things together and it's hard to do this
well it's hard to do this in sort of a
general way and after having solved this
problem poorly about three different
times in the form of making a util
namespace where I added in the
transformations I needed for that
particular problem I decided I was sort
of sick of doing this over and over
again and so I wanted to do something a
little bit more general and so I wrote a
library called byte streams which
traverses this graph and figures out
what transformations need to be composed
to go from any one point of the graph to
any other and this works pretty well and
we don't even have to first the graph
each time because we can memorize that
we can turn it into a simply a composed
set of functions that we go through each
time and so we don't even have to pay
the cost of sort of minimally traversing
this graph each time a problem I ran
into into though is that some of these
transformations like going from a string
to a byte array are quite cheap and
memoization
is relative to this quite cheap
transformation a little bit more
expensive and I said before that
nanoseconds are not a very big amount of
but I want this to be a general-purpose
library which means I don't get to
dictate what sort of context what sort
of performance sensitivity is you know
necessary in this context and so it's
sort of as a library author it's my role
to either articulate where this library
should not be used or make it so that it
can be used in all different situations
and so I sort of wanted to understand
why is memoization relatively this
expensive and so I delved into that and
memorize works more or less how you'd
expect it has an atom that contains a
map and then we get the arguments as a
sequence and we look up that sequence of
arguments in the map and if it's there
we return the value if it's not then we
calculate it and put it in the map and
so understanding the performance
characteristics of this requires us to
understand what a map is and then this
is actually a contextual answer when we
create an empty map this gives us a
persistent array map which is pretty
much what it sounds like it is a flat
array of alternating keys and values and
when we look something up in a
persistent array map we walk over the
keys and one by one compare them with
the key that we want to look up if it
ever goes over eight entries then it
will transform itself magically into a
hash map which will find the hash of the
key and then walk down and do a certain
number of hash comparisons and then do
roughly one equivalence check and so
looking at this it seems like sort of
the key thing that's going on here is in
fact the comparison of the sequence of
keys and we can look at how two vectors
are compared and here we see that we
first compare the sizes and if they're
not equal we short-circuit but if they
are the same size and we create two
iterators and we walk through them one
at a time comparing each and turn and
this is the only way to really do it on
a vector of arbitrary size but we don't
really care about vectors of arbitrary
sizes for this particular problem we're
trying to take argument sequences which
by and large are not that big and so I
wrote a thing which was focused on this
very particular problem which instead of
trying to create something which is
attempting to do better than vector
because that is not really a winning
game I try to just solve this one
problem of
let's not make a general-purpose
collection let's make one that is very
specific let's make a particular
collection which exists to hold nothing
let's make one that exists to hold just
one element and to and up to six at
which point much like the persistent
array map sort of spills over into a
vector because at that size it can do a
better job and so the comparison for two
tuples looks like this instead of
comparing each field by iterating over
it it simply expands it out in place it
says I'm going to compare my a to your a
your B to my B there is no iteration
here the iteration is at compile time
because of course this is just a vector
this is not really closure that we're
writing anymore this is meta closure and
writing meta closure kind of sucks
there's not very good tooling here but
this is inarguably sort of a useful
thing to be able to do this is something
that closure gives us that Java would
never give us and so I think that this
is not a problem with the approach so
much as a sort of tooling surrounding it
might have some notions in this space I
haven't really had a chance to explore
them out fully if people have ideas or
even an interest I'd be happy to talk
about it more a one little oddity that I
found when I was writing this is that
since vectors are not a siik when we
call first on them we first have to wrap
them in something that makes it look
like a C and so calling nv0 on a vector
is as we'd expect quite fast calling
first on a vector is about ten times
slower and so one little change I made
is that the tuples are both implement a
persistent vector and I seek and so
enter star sort of compare ibly fast
so looking back at byte streams the sort
of lesson we can take away from here is
that lets front load the work let's
memorize the work let's do the hard part
once and this has sort of two benefits
one is that it's less expensive across
all the other runs but also that we
don't have to be particularly
performance minded when we're writing
this run once code we can write it
expressively we can write it without
concern for sort of the overhead of all
these things because that cost is
amortized across every subsequent run
and so this means that we can sort of
split our code into the bits that are
hard to write and hard to reason about
and the bits that are sort of less hard
to reason about and generally more
complicated and this is good because
writing things that are performance sort
of oriented everywhere is a pain in the
ass the things we can learn from clj
tuples that we don't even necessarily
have to do it at runtime we can in some
cases do it at compile time and writing
code this way is again a pain but you
know I think that this can become better
so I work with Kyle Kingsbury who some
of you may know as aether and you know
he's written a couple things he
occasionally opens on distributed
databases and how they don't quite match
up to the marketing material he has
recently been writing an introduction to
closure called closure from the ground
up and about eight months ago we were
getting lunch and we were talking about
local mutable variables and I was saying
that you could definitely make a let
mutable macro that would be perfectly
safe as long as you just walked the
entire body and made sure that certain
invariants held true and Kyle said yeah
sure I guess you could but why on earth
would you and so that afternoon I wrote
a library that did just that and you may
be wondering my god why because you know
we have internalized why immutability is
valuable it gives us the ability to
reason about our programs in a much
simpler way because we don't have to be
concerned about who has the permission
to do what
but of course this is sort of the most
pessimistic interpretation of what
immutability is supposed to do
immutability assumes nothing about this
because anybody could be holding on to
the same piece of data at the same time
if we narrow that a little bit if we say
the only one person is allowed to look
at something mutation is fine this is
what transients are transients are only
allowed to be looked at to be operated
on on a single thread and so we don't
have these same sorts of race conditions
and so when you look at what let mutable
does this is sort of an extension of
that idea rather than enforcing this in
a thread context it is being enforced in
a lexical context this X that we've
defined which is mutable cannot escape
this scope cannot be seen by anybody
else and so you know it's questionable
why we'd want to iterate a hundred times
and add one each time but you know this
is safe there is no way that we can have
any of the sort of reality bending
effects of uncontrolled mutability and
so what this looks like is you know
fairly straightforward we create a
container for this which is a long
container because this is a unlike a not
going to box the number we want to have
this be as efficient as possible because
we can and wherever we see X we expand
that out in to get X except where you
have the set bang which expands out into
set X and the only sort of concern here
is that if we closed over this container
that it might escape it might go off
into some other threat never to be seen
again
and so we enforce that whenever we
create a closure that we turn the
container into just the value and so
again this is safe this is bounded this
does not change anything invariance that
closure gives us and so the main thing
here that I want to sort of you know
communicate about this library this
library by the way is nothing it's a
hundred lines of code it was half a joke
half a proof-of-concept people still use
it occasionally it's a little bit
terrifying but the point here is that
you know we have these sorts of very
carefully engineered projects like core
async or core type that do these very
careful analysis of the code you saw
Timothy Baldrige is talk and it was very
interesting and they're really
interesting things that can come out of
the
but as long as you're willing to narrow
the scope of the problem you're trying
to solve you don't need to over engineer
anything these sorts of experiments are
relatively simple to do they don't have
to necessarily be provably good ideas
from the outset there's a huge number of
things that we can do in the space which
are again just narrow solutions of
narrow problems that give us acess
something that the closure core does not
and so the real message here is that you
don't have to have more insight into
software language design than rich
Hickey to improve upon closure you do
not want I think to try to supplant the
general solutions that he created
because they're well-made but it's not
hard to find a narrower scope where you
can improve upon the capabilities that
they give you
you just have to very carefully
articulate what the scope of the problem
is and so there are a lot of other
things I would talk about here if I had
the time um
vertigo is a library that takes a byte
buffer and as long as it has sort of
fixed layout Allah
I see struct allows you to treat that in
oh one time as a closure sequence allows
you to access it with the same sort of
overhead and memory semantics as AC
structure immutable bit set combines
Java bit sets and closures immutable
Maps and creates something that for
integer sets can be up to three orders
of magnitude more memory efficient than
a standard closure set automatic is
something that uses runtime eval to
create a an extremely efficient version
of a finite state machine again this is
something that Java just simply cannot
do and narrator takes fairly complex
mutable statistical data structures
which cannot easily made immutable and
allows them to be parallelized by
putting them in a thread local context
and then constraining the numbers of
threads that can be used putting on a
particular thread pool and allows them
to be combined and so there's sort of
side steps the question of mutability
and ownership and so you know try to
understand what's going on don't
understand for its own sake understand
in the direction that
you need to to get out of head of the
problems that you're trying to solve but
you know dig down a little bit
be curious and if you want to do
something in the spirit of what I've
talked about here you want to make a
bounded solution to a bounded problem be
a very articulate about what that is
first to yourself and then to others and
if you're going for a general solution
then you know have respect for what a
big sort of problem that is how
difficult that is and make sure that
again you think very carefully about
what that entails and how to accomplish
that hopefully by having solved the
problem or a subset of the problem a
couple of times beforehand and so I work
for factual and at least in my own
personal experience um the work that
we've done has been a really great way
for me to gain a deeper understanding of
the systems has in fact forced me to
gain a deeper understanding of these
systems and so if you have an interest
in that learning more about sort of both
systems engineering and analysis and
being sort of forced by nature of the
work that you're doing to gain a deeper
understanding of engineering I encourage
you to come up and talk to me or anyone
of the I think twelve people from
factual that are here at the conference
and will happily talk to you more about
that and with that I'll leave the floor
open to questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>