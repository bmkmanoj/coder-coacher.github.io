<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>WormBase database migration to Datomic on AWS: A case Study - Adam Wright | Coder Coacher - Coaching Coders</title><meta content="WormBase database migration to Datomic on AWS: A case Study - Adam Wright - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>WormBase database migration to Datomic on AWS: A case Study - Adam Wright</b></h2><h5 class="post__date">2016-12-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RzE9iencxT0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah thank you everybody for coming I
been having a great time I think that
there's been a lot of great
presentations I so I hope that everyone
enjoys this one I think it's a good
topic just a general knowledge of things
with what's going on in research and
also how we're applying it with the
tools that we talked about at this
conference so so there's three main
areas that we'll go over
worm bass itself which not too many
people will know too much about but I'll
go over some of the research with that
as some interesting things we're doing
why it makes sense that we're going with
day Tomic and we've been doing a project
for a couple years now to work on things
in the way of migrating the database
over and using it in production and then
we've been using it with AWS to get
things going so I hope you guys enjoy
the presentation so so yeah I'm Adam ray
I I had gotten a degree in biology
originally I didn't really know what I
was doing I knew in high school I did a
lot of programming I really liked it I
did a lot of math and - and I ended up
and doing another degree in engineering
I try it so I threw out my career I
tried to use programming to further both
of those and warm bass is one of the
major projects that I've been working on
over the last several years it's a
long-standing project and yeah it's it's
one that I is has a legacy database that
we has been around for a long time
there's a large code base behind it and
yeah I really enjoy working on it so
warm base has three different sites so
we have very few people at each site and
they're very specific to what they're
doing so I work at the Ontario Institute
for cancer research in Toronto at the
Institute in general what we work on is
web development type stuff I
Hingston site at EBI a they work mainly
on the genomics and migrating the
merging the day-to-day together and
getting it ready for displaying on the
website and at caltech they have a bunch
of curators that go through the data and
we go through a release cycle and so
once the data is done then a ycr will go
and add the features on based on the new
data every time we have a release which
is a two month cycle we have to make
slight changes to the website and move
forward so basically there's the three
Institute's I'm from Toronto in the
middle I so I thought I'd start with a
quote I don't have quotes all through my
presentation like this one I really like
Alan Turing everyone knows him so
basically I am really happy that we've
come a long way we keep on coming up
with new interesting things I love great
ideas and all the things that people are
doing what they do a lot of stuff I
might not use but I'm glad people are
doing and just moving the field forward
so with biology though this I this would
be one of the main characters that I
would say started at all and I'm glad we
didn't go the same direction as biology
so back when he discovered he basically
discovered genes he was the original one
he was studying Peas and he came up with
all sorts of great ideas once he was
done with that though it took a couple
hundred years for people to get focused
tackle on that again and I'm glad with
alan turing we continued on if we didn't
continue it way to be still making very
minor discoveries today with and we
wouldn't be where we are with closure or
anything so that's one of the major
things that we can learn from that is
whenever we have great ideas we need to
recognize that go back to the drawing
board and try to figure out what what it
is that we saw like people just didn't
think his ideas were that great or the
biggest deal to know to move forward
with it but it was a big deal in the
obviously so here in biology this is
like biology 101 bite so this is known
as the central dogma here you start with
your genes you with your genes you which
don't change at all from cell to cell
it's I fundamentally identical
throughout your body and then they there
are certain changes with them to on what
RNA gets expressed that goes out into
the rest of the cell and then you have
proteins and so the proteins in general
are the things that do stuff initially
when they were going through this though
there's a lot of RNA in the nucleus they
didn't know that DNA was the biggest
thing they thought that it was probably
the RNA they had this concept of gene
from Mendel and research since and they
realize once they figured out the double
helix structure and how it worked on how
we passed genes from one cell to other
and duplicated them that it was
definitely the genetic code and then
these days were going back to the RNA
were figuring out that it isn't all
about the DNA that the RNA is going back
and influencing the DNA strongly so
there's a lot of research going on with
that so especially cancer research and
stuff where we have a lot mapped and in
the way of the genomics but once we got
the RNA and there that will learn a lot
so there's still a lot to learn though
is kind of the major point there and so
this brings us to model organisms these
are just a bunch of the main ones a rat
Mouse worm yeast fly and that would be
zebrafish
so there's model organism databases and
these are major things in research that
there's huge databases of information on
these obviously we where I work we focus
on the worm by major discoveries have
been brought out of all of those and
hopefully be brought out to human
research or just have really interesting
discoveries for the purpose of knowledge
and maybe for agriculture for whatever
purpose but we learn a lot through the
different ones why why they're so major
is because there
they're very simple and one of the
things relative to the human cell they
are extremely simple and they have they
have very short lifespan so you can they
for pretty much all of them live for
sure with the worm they have every cell
mapped out through their development and
they so from just being born to a
full-grown worm they can know what
interacts with what to become that worm
and they can know what proteins act with
what interact with web proteins so if in
the human research we have a certain
sequence ideally we'd be able to go to
the worm and see if they've discovered
something with that would be an example
of something so yeah so that brings us
to worm base why is it around it's been
around for quite a while there's a fair
bit of funding for it like historically
there's a lot of researchers researching
it so this is the full-grown worm I'm
not expecting people to memorize it but
yeah just knowing that they can map
things very well with the worm and
there's a whole community of people kind
of like we're a small fraction of the
programming community very passionate
people there's a passionate people about
worms and they love them and or I've
been working on this for a couple years
I'm starting to love them do so this is
in general how it works though we have
the researchers they go out into the lab
and they it could be anything from
adding stimulus external stimulus of co2
or whatever and they see how the worm
reacts they try to figure out why the
worm has that trait and what's causing
it so they could change the genes they
could do all sorts of things and then
there's a paper written ideally that
we'd discover something and then there's
we have a bunch of curators that go out
and they read all these journal articles
they figure out what they truly believe
what they don't and how it would fit
into our into the data side and then it
gets put into the database and then
ideally the researcher we get to figure
out that instead of reading a very large
number of papers they can go through and
narrow it down through the years instead
of reading less say 30 years of papers
on worms
you might want to have it summarized
with some nice diagrams and stuff so in
the end their initial II was just a
database and then with my boss it
started with making a website this is
the new design it was made about five
years ago they used a popular perl MVC
to make it and it's heavily used by any
of the researchers there in the world
I've talked to them and they'll use it
on a daily basis they'll go through you
can look up jeans you can all sorts of
relationships so the database itself has
a very complicated schema and yeah so
they need a database that can handle
this so initially they created this
database called ace TV and ace DB is
particularly made for the worm so there
was a at the time they didn't have a
database that could do all this so it's
a graph database it keeps track of
timestamps for when people make changes
the it's fairly simple in the way of the
user interface being so that some
biologist that has some perl or some
sort of experience in the way of
programming can easily go in and go and
learn from what's in the database to add
fields or see the relationships and make
queries so I had a powerful curry
language with it which similar to des
Tomic has a very powerful common
programming experience with it and so
these are the types of things that they
were needing to be able to make this
very large database and keep track of
all of the different types and have it
change as research changes as things are
added and into the field over several
years so on the downside though we ended
up having so it was initially made I've
seen documentation of being fairly large
in the early 90s so to actually use it
there aren't so many there was many
groups using it back then the over time
it just we're having problems with scale
and falling over with so we have a huge
caching layer with the website and we we
just are not finding that it's working
in the way of being able to have many
people working on it because you can't
have too many people writing so we'll
have a merging system we'll have people
working on different ones we'll merge
them together for the release cycle so
there's an ingrained way in which we do
it within our Institute and we have this
challenge as of a few years ago we took
this on to figure out how will we solve
it what are we going to do it's the
central part of everything we do it's a
major thing to be able to not go a few
more years and try to figure it out so
we have this we started with a database
selection there is someone else before
me that I was slightly a part of it but
there is someone hired they did this
totally going and researching the
benefits and the drawbacks of it all the
different databases so here are several
of them there's also so the orient DB
you might not have heard of but it's
like neo4j it's a little more open I was
a fan of that one at the time and then
there's relational databases there's
MongoDB which most of you have heard of
a lot of these didn't meet the needs
that we wanted one of the biggest things
that we wanted was the time stamps so
that we could know when someone made a
change and who made the change and go
back into history and figure out what it
was previously this these are some
features that we thought would be highly
beneficial couchdb we actually used for
the cache we had some experience with it
but it wasn't going to scale for us it
didn't have the tooling that we wanted
and the person that was looking into it
also had a Java background so they were
we already were leaning in certain
directions with it but and then so
surprise surprise a closure conference
we chose de Tomic and we we really liked
it we we started with it we didn't know
at the beginning whether it was the
right thing
Thomastown the previous developer he
went and he made a lot of great code he
had a lot of domain knowledge so he was
able to create a lot of code fairly
quickly and he chose it so he really
enjoyed a closure he really took
advantage of the time stamps which I'll
go over a bit later and the fact that
you can we could in this case take our
graph database and poured it into de
Tomic as a graph database we have so
like we have entities and edges and the
edges we actually have some information
on we were able to get that into there
there so it was very impressive so so
far we've implemented most of these
things on this diagram that most of you
have seen it's often made website and we
just haven't implemented we haven't gone
into the caching layer that comes with
the atomic but we're highly looking into
it we're at the stage right now as we
use it more and more heavily that we
would want to be using that so this is
like the take-home message this slide I
feel like everything that we're trying
to do here the we have found with the
closure to DES Tomic to everything that
the maintainability is the biggest
selling feature of all of this so you
can go in and you can have confidence
that things aren't changing the like
this is a mantra that gets told
repeatedly by we have found a lot of
success with this we found that we have
a very small team of developers and we
have been able to write fairly
complicated queries and manipulate the
data acquire that and go back into our
own code and figure out what's going on
and it's not I just a matter of the
language forcing it on you but it is it
seems very common throughout the whole
closure community I find and through all
of the documentation presentations those
were all very helpful for during the
South so yeah we had to go from this
prototype to production while
what was involved with it so like we had
to go from we had some code that would
migrate the database from ace DB today
Tomic we had we have a whole pipeline
for this and which I'll go over a bit
later and it's a major stuff we had to
make sure that this was dead-on that all
of the data was being converted over
correctly they it was able to be done
within a reasonable amount of time and
then we would be able to repeatedly do
this because we can't just convert over
one of the things we've found is that we
can't convert over to the new database
all at once this is going to take us
we're part of the way there it's going
to take us maybe another six months to a
year
to get the full website running off of
it and so we do one URL on a rest
interface at a time and yeah so with the
REST API we have - it was nice that we
have things isolated so that we can do
one at a time it's not all at once we
can break it up and we so we are very
cautious with each one that it is
producing the same answers as the
previous one and there's a certain
amount of feedback that we get as we do
this so it's a very iterative process
though we might find there's an error in
how we are migrating the data at this
point it's very stable but there are
things like that and then the other
thing that we need to do is we need to
make these curation web interfaces and
there was a prototype made of that but
this is also in the next stage of what
we're working on is to try to make it so
that the the overall goal is that we can
have this website that curators can go
in right away to make a change because
someone notices that throughout the
process that something wasn't cured it
correctly whether it's a spelling
mistake or something more major and that
it would be operated a right now we have
the issue of that it could be several
months later that by the time that fixes
made maybe the person that made the
request I it doesn't affect them anymore
they're aware of it and they've moved on
maybe so if they see it happening right
away we might get a lot more feedback
with this
the other side of it is that we want to
display on the website all these changes
so like in the way of what were the
updates over the last month and if
you're looking at jeans or whatever
entity you're looking at list the top
ten or that sort of thing and if they're
wondering because they saw something was
a certain value they could look back and
see what it was several months ago and
hopefully we'd be able to get rid of the
really really cycle that we currently
have and just have this continuous
integration with the database so with
this diagram well I'm showing and it
looked very similar to the talk
yesterday is first we make we output the
data from a ste B as XML files we
convert that over to e DM files and then
what we're doing that's quite special
that we really like that we could do and
it has had a lot of benefit already is
that we sort those logs in edn and as we
import them then we convince the trans
actor at the time of each one of the
items in the EDM file they that's the
time at which it was in so then when we
can search the database now we have a
trace back over the last 30 years of
what God put in when we don't have like
things that were deleted which as we go
forward then we'll have the immutability
that day Tomac has but we do have
timestamps if you put the the stuff into
the database in the right order so and
then we have some testing which of
course is very important and so we have
so that on the bottom right is the
atomic database itself and then we have
a version on it so every two months
we're still creating this and this
pipeline we yeah are still modifying but
it's fairly complete and it works quite
well to go and query through the time
and so dye is a very positive in the way
of now we have a lot of confidence going
forward that on the website we'll be
able to show people what the data was at
different points in time and do some
analysis with that
so okay so yeah here's the architecture
that we started with and this up until
six months or a year ago was our full
architecture so we have a Postgres
database with inter mine which is a
query tool they we put data in it's very
standardized several of these mods model
organism databases have this and it has
a common search interface that the user
on the website can use so all of the
data goes in there in the catalyst which
is the perl MVC you have several
different data stores so Sapien is for
the search index and then we have a
caching layer with CouchDB we have ace
DB and so this is what we started with
what we're going to with the
intermediate step and what we've added
in is this de Tomic layer where catalyst
itself will pull information through a
rest interface from the atomic and we
keep on adding more and more of this as
time goes on so we actually are reducing
currently reducing the amount of caching
that we're needing to do which is
beneficial because during the caching
step we are having problems with pulling
the data to the databases it will fail
at times it will just over time as the
data gets larger it takes more time to
do and that's one of the major things
that we want to take out and as we've
been releasing stuff with atomic then
we've been we have noticed errors so
like we go through the process where the
curators look at it with an internal
website and then once it gets on the
internet there's still times in which
people find errors so we go and push
those changes right away so we don't
cache those pages that were those the
information that's coming from the
atomic and we are able to have it
updated right away which is nice so this
is what we're going towards in the
future though if there was still will be
there's some complexity left out of this
in the way of we're going to have like
an elastic search for
that will be indexed off of diatomic is
what our plan is
and by overall the amount of work being
done by a catalyst won't be as much so
and yeah in the MySQL database there's
sequence information we're not really
sure there's some complexity like that
of should it be in the atomic or not at
this point we're not too sure it's not
changing over time so like sometimes
there would be a new release but it's
very rare
so we're chances are we're either gonna
map it with the rest interface have
closure map it to what's the inside of
des Tomic or have just catalyst be the
solution to that and which that codes
already written but ideally we would
take out as much of the work from
catalyst as possible and use it mainly
as a templating engine so the next step
of what we were doing the once we got
things running we were fairly confident
with it we had to start doing things we
have a very small team so what worked
well for us was even though you already
have a jar it's nice to dock her eyes it
and know so there's certain environment
variables that we need set and
information like that so once we have it
in docker we for the REST API we put
that inside a docker I should say and we
use elastic Beanstalk so that's a AWS
service that I didn't quite understand
by reading it before using it but it
does all sorts of things for you it's a
very impressive in the way of we can
just we create this docker file we
deploy it and it will go through and it
will keep track of logs it will keep
track of the health of the machine and
you have a very simple way to do
continuous integration where based on
your latest commits whether you've done
your tests and everything you can say EB
deploy and it will just deploy it and
then we have more stuff running on like
another URL running on our rest
interface and
we don't we have a lot of confidence
with it it's just running a fairly
simple docker container that's running
like a Java server with our jar running
on it and that worked quite well and
then with cloud formation it's very
similar to elastic Beanstalk
it's another AWS service you so there's
tools that de Tomic provides for you
they allow you to create the JSON
configuration file for spinning up a
trans actor what we did with that was we
started with what that creates and then
we added more and so one of the things
that we wanted with ours was we wanted
to add tags into onto the machine
because we have automatic processes
based on what the tags are on what we do
with that machine and how we calculate
how are you - lies ation of the
resources and stuff so we wanted it
ticked and then the other thing is we
have it within a virtual private cloud
on AWS to reduce cost and to just manage
security in the way of permissions and
all sorts of stuff so we added a layer
into that this is all available so if
you want to get hub and looked up warm
base there's you can see our JSON file
there we've also pulled out the de Tomic
pro key a license key from that so we
provide that as an argument and we have
certain tools so we have that in a file
and we just pass that in so and then
ansible so we don't use ansible so much
anymore for like we were using it at the
beginning it's moved over to talker but
that sort of thing in the way of
provisioning stuff we use a mix of stuff
we still use some ansible we use a lot
of make files and some some Python
scripts for spinning up these machines
and that's worked quite well mainly
because there isn't too much
configuration to do I I'm quite a fan of
ansible in the idempotent way that
people normally
can use it in the way of keeping track
of state of machines and stuff but yeah
elastic beanstalk and particular has
taken over a lot of that so it just uses
configuration files within your repo so
and so this is how a little bit more
depth of how to use elastic Beanstalk so
when we make our container we end up
having to put the AWS credentials within
it and this makes it so that it needs to
proprietary and not just on docker hub
or something like that and when you have
your AWS account there's a certain
amount of free space that you can have
so we use the container registry which
is the one at the top there that people
can pull stuff or they could put their
docker containers in and so and they
provide a nice web interface and
integration with AWS tools and then we
go and we use the elastics Beanstalk to
go and spin update so on the right that
would the REST API there is the ec2
container that it spins up and we every
time you do an EB deploy it to the user
it looks like the exact same machine and
within the web interface also it as if
you're using the exact same hardware but
it spins up it truly in the backend
spins up a new one and it works quite
well for just being aware of the status
of the health status of your machine and
everything that everything is up and
running correctly so this is the cloud
formation how it looks we have so we
spin up a neutrons actor for every day
Tomic release that we have so we the
database itself is dynamodb which is a
AWS database that's basically how they
have it is just a single table which
works really well with how des Tomic
works and so you have a table within
DynamoDB and then
you go and you point your trans actor
towards that it so one trans actor
points out a single table so we you can
have many databases within one I know
Moe DB but we split it up for just being
able to roll over one database to the
next without worrying about deleting it
completely stuff like that so we have
separate machines for each trans actor
and this is done with a single command
on our development server to just spin
up these machines and they're there
we've been doing this for about six
months maybe eight months now and it's a
very clean process for just managing
your resources and we have adjusted the
size of machines and stuff like that we
found it very useful and so the people
that I mainly work with uh so my boss
dr. Lincoln Stein Todd Harris and I
manager and then the main team that I've
been working with a Matt Russell he's
here with me please
stop us if you have any questions about
diatomic we'd love to talk about it
we've had a lot of success with it and
we're really excited about what we're
gonna have in the future in the way of
functionality on the website Thomas down
he's the guy that started everything he
wrote a lot of code he was we wouldn't
be where we are today without him he's
he had a lot of domain knowledge and
wrote it down for us so there's and then
Sybil and Juan Carlos they're starting
and with how we've set up everything
using Lyon and everything they Sybil
just started in within the last month
and she's been very productive already
she's and she loves helping out she she
already was quite proficient with she's
mainly a JavaScript developer and some
parole but she and I guess Python and
she really enjoys it she I saw on any
project out there I would highly
recommend closure for the purpose of the
developers are enjoying it and then
we're just starting in with one car
and it's helping us with how I started
with how we have a very distributed team
across the world very different at time
zones trying to communicate between
California and England isn't very easy
but having very organized code and very
organized projects very organized tool
based from the closure community has
been very useful so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>