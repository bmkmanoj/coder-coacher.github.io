<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>One Million Clicks per Minute with Kafka and Clojure - Devon Peticolas | Coder Coacher - Coaching Coders</title><meta content="One Million Clicks per Minute with Kafka and Clojure - Devon Peticolas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>One Million Clicks per Minute with Kafka and Clojure - Devon Peticolas</b></h2><h5 class="post__date">2016-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VC_MTD68erY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey guys my name is Devin pedicle I am
an engineer at an awesome company called
chart beat and the first thing I just
want to say real quick is this is my
first closure conch it's also my first
time giving a talk at a conference and
you guys have been awesome I know no one
here and I've made feel like I met a
million people in the past today so you
guys are pretty great
but yeah so this is my talk 1 million
clicks per minute with closure and Kafka
now I know you guys came to this talk
based on the title and you assume it's
about how we're going to process a
million clicks per minute using closure
in Kafka and I'm really sorry to do this
to you but the title is actually wrong
the correct title is how not to process
1 million clicks per minute with closure
in Kafka and then how to I'm gonna tell
you guys a story of how we built this
application because this was the first
real product that we built using closure
as the consuming as the language to
write our consumers off of Kafka so I'm
gonna move pretty quickly because I have
a lot of slides you might say that like
an imperative language I have a problem
with scope but you can go right here and
it's github calm slash X slash slides
and that's where all the code is and
it's all numbered
so I work for this company called chart
beat what is chart beat we're a company
that does real-time and historical
analytics for publishers on the web so
we tell them things about their visitors
what are they doing what are they
reading with how many of them are there
are they new are they old stuff like
that
how do we do that so we have client-side
code that runs on their site so this is
an example gawker.com someone I can
admit that as a customer of ours all the
others you could probably tell from
chrome inspector so when a user goes to
Gawker comm what happens is our codes
running in it's a small bit of
JavaScript that's running and tracking
their state and sending what we call
pings to our servers now a ping is the
smallest HTTP request we get away with
it's a get request look going for a 1x1
pixel image and actually inside of the
query parameters of that get request is
all the data that
we want about that user at that time and
so we send lots of these throughout the
time of their on page about one every 15
seconds and together they're what we
call a session and this describes a user
being on a page and when they stop
pinging they're no longer other page so
we can tell whether or not someone's
there or not you might say that sounds
like a lot of requests it is a lot of
requests so our patterns because we
mostly serve European and US clients
right now our traffic patterns have this
nice little curve to them and this is a
typical Tuesday we get up to 250,000
requests per second this was March 22nd
during the Brussels attacks so we went
up to 300,000 requests per second so I
think chart beat has this really unique
quality where if you are say a person
who's on call to normal company you get
paged when something goes wrong with
your servers if you're a person on call
at chart beat you get paged when
something goes wrong anywhere so what is
this data end up looking like we end up
turning into dashboards that look like
this primarily this is gawker.com
dashboard or a screenshot of it and it
shows you how many people are on the
site and what pages are on stuff like
that where they're coming from the
dashboard that we are going to be
building in this talk is actually
slightly trickier it runs on top of
their site it's that little blue thing
at the top right hand corner and when we
open it you get an overlay on top of the
site that shows a little tiny pin on
every single link on the page
and says and ranks them based on how
many clicks each one of those articles
is getting if you were to click on one
of those pins it opens up and it could
say okay this position is getting 3.3
clicks per minute and you get a nice
little historical graph of what that is
over time so if I were to summarize this
in a user story because I have drank
kool-aid I'm a big believer in scrum or
as my product owner would write it Adam
in the bottom right hand corner looking
dashing as a homepage editor I want
real-time clicks per minute on each of
my articles on my home page and I want a
historical graph of each of the clicks
on each article on my home page
so I can make informed decisions about
the content on my home page now the
important word there is real-time that
means we can only have a few seconds of
delay we need to be doing this pretty
quickly so how do we get how do we track
a click that's probably the first
question so as I was saying all these
pages are sending pings now you might
think the best way to track a click is
to actually track when a person clicks
on a link but what we found is it's
actually much more consistent to track
after they've already arrived on the
next page since we only really care
about internal traffic anyway we know
there's going to be pain or code on the
other side so what we do is when that
happens when a person goes from say
Gawker comms homepage to an article page
there's what we call an HTTP refer it is
spelled incorrectly in the spec that is
part of the HTTP spec with only one R so
if you look at this it you can use the
HP refer to say okay we came from Gawker
coms home page and the current page is
Gawker TOCOM slash article one we can
say there was some page movement from
between those two pages and we can ping
those both up and the other thing we're
gonna ping up is the time that we've
been on the page so this is important
because as I was saying we're gonna send
lots of pings during a session so we
really only care about is the first ping
the one where the time on the page is
zero because that way we're not counting
a click more than once so how are you
gonna deal with this when we have so
much traffic to begin with so in comes
Apache Kafka so what is Apache Kafka
Kafka is a hut a fast high-throughput
distributed message bus basically you
can kind of think of it as you know or a
commit log is actually what people often
call it you have producers that write to
it and you have consumers that read from
it you might think of something like
rabbitmq when you see something like
this but it's actually slightly
different in a really interesting way it
would like RabbitMQ what happens is 2q
udq off the queue and it goes to some
consider it's pushed to some random
consumer and that consumer processes
that message with Kafka a consumer
subscribes to some partition of the data
that you want what we call topic and
that consumer gets to track where it is
in actually a log so this is why people
call commit
they're the producers writing this log
on one end and the consumer is reading
up this log on the other end and that
gives us a couple really cool properties
one of which is we can kind of have
order to our data which you wouldn't
really have that kind of guarantee if
you were d queuing and it might have to
come back another thing that you have
out of this is that the data can be
replayed depending on how much of that
log you want to retain there are people
who will retain entire days worth of
blogs that's good if you have an outage
or you need to correct some data or if
you just want to do have something else
that reads in a different way so what's
our new pipeline look like it looks
something like this so we have a
customer who's interacting with
gawker.com home page and on that page we
have our JavaScript running that's
sending pings those pings are going to
go to an AWS elastic load balancer
that's gonna do what it sounds like it's
going to do it's going to load balance
it's going to send to a bunch of what we
call front servers each one of these
front servers is running nginx and a
small server written in C++ that all it
does is it consumes messages from over
HP and then produces them well pack
message packs these pings first message
pack is the type of serialization of
JSON gets the size down a little bit and
then it writes it to Kafka to one of the
brokers and then on the other end you
have consumers that are reading from
Kafka now there's an interesting thing
that's happening here the producers can
write what's inside this broker you see
these partitions
it's the each one of these is like a log
and it's appending on one end and the
consumers are reading from the other now
a producer can write to any partition
that it wants and the way it decides
which partition it writes to is based on
what's called a partition key you choose
some way you want to partition your data
in advance
so the way that we're going to be
covered partitioning our data on in this
talk is based on the customer so that
means is all of Gawker comms data is
gonna be going to the same partition on
the consumer side a consumer subscribes
to a partition so every partition only
has one consumer that's subscribed to it
and what that means is that at any given
time you it gives us nice little quality
where you can now maintain some sort of
state in that consumer about that
partition it's pretty cool you could
imagine this is like super
useful for caching because all the
sudden you have like a much smarter way
of caching or you could do some really
interesting things where you kind of
join a stream onto itself which we're
getting to a little bit later so what's
that code look like so this is our first
iteration of our consumer we're gonna
use something called CLG Kafka it's a
very common library for dealing with
Kafka and this is kind of how we're
going to this is where code is going to
look like keeping out the details of how
we're writing to a database so the first
thing we're going to do is we're going
to create a consumer object using this
consumer function it's going to take a
bunch of options it's also going to take
which topic it's consuming from then
we're gonna use this messages function
to get a lazy sequence of messages from
the consumer from this topic then we're
going to iterate over each one of those
messages as I was saying those messages
are message packed so we're going to
unpack those messages now this is an
interesting thing to note real quick
this is the highest CPU utilizing part
of our program right now this is what
takes in those crunch and then we're
going to step through each one and say
is it the first ping because that's the
only one we care about we only want to
count one click per session and then
we're going to identify the source which
was the HP or fir as we were saying the
target which is the current page as
we're saying and the minute that
happened and just increment once in the
database sounds pretty simple right
oh we all went this poor Courtney
I love this guy I don't know what's
going through his mind so what happened
so the issues actually and I did this
the issue is actually that we are
holding on to the head of a lazy
sequence I don't know if you guys are
familiar with this but when you have a
lazy sequence so this is a example of
lazy sequence we have this function
called lazy nums it returns us a
sequence which is every item is an
increment on the atom up there called
counter if we were to iterate over lazy
nums declared in line inside the do seek
what you see is 1 2 3 4 and then if you
did it again you would see 5 6 7 8
that's because you're mutating the atom
however if you were to assign lazy nums
to some sort of symbol and then iterate
over that it would hold on to the
previously iterated over things it would
cash and this is just a strange property
of lazy sequences maybe it's not strange
to everyone also strange to me coming
from Python and being used to generators
so what's happening here is we're
holding on to an infinite lazy sequence
this thing's just going on forever which
is why we're oh I mean so the workaround
for this is not beautiful but it's
doable we just iterate directly over the
lazy sequence so we're good now we're
iterating over the lazy sequence we're
not gonna but we're running really
really slowly I love this guy
he's so good so why are we running so
slowly it's because well we're
incrementing the DB every single time if
you think about it say Gawker coms top
article is getting a hundred clicks per
minute that means we're probably
incrementing a hundred times more in the
database than we should be
that's way overkill there's got to be
some way to reduce this right and this
is when you get into a very common
pattern that you see in cafe consumers
which is that you're going to batch
those messages so this is an example of
our batching code so we have our
messages which is a lazy sequence we
recall partition on it and partition out
5,000 messages at a time which is for us
fine considering how many requests we're
getting and then we're going to instead
of iterating over individual see
messages we're going to iterate over the
batch
each one of those batches we're going to
unpack using map we're going to filter
out to just the first pings using filter
and then we're going to actually I can
step through these map filter then we're
going to group this data by the HTTP or
fer the path and the minute and again
this is the source the target and when
it happened and then we're going to
iterate over the grouping that data the
source the target a minute and the
messages and then increment that count
in the database so now we should be good
right we no longer have this issue with
our writes the database we produced them
massively by doing this batching oh but
we're really inconsistent in the way
that we're using our CPU now what does
that mean just like this poor Corgi I
don't know if you guys saw there's a
theme to my gifts by the way I live in
New York and I can't have a cord in my
apartment but so just like this poor
Corgi who's going up and down our CPU is
going up and down and that's usually a
bad sign
really what we want is something that's
much more level because that means that
we're properly there's nothing blocking
or lock stepping there so for both our
CPU and our Corby we want to keep them
on level ground so let's talk about how
Kafka works and I'm gonna say right now
as I show you this slide there's a note
in here that says to do replace the
slide sorry it's a little bit mess so
this is roughly how it's working so we
have our producers and they're writing
to our brokers and it can write - it's
actually a push when it's writing to the
broker because the broker can just it
has this moving window it doesn't really
care but on the consumer side it's
actually a pull it's not being pushed by
the brokers and the way that's actually
happening is inside that messages
function that we were using there's
something called a Kafka stream object
and inside that Kafka stream object
there's essentially a queue and that
queue has some number of chunks and each
one of those chunks is some number of
bytes of data and whenever we get look
whenever we're not at full on that queue
we're gonna go out and do a pull or do a
fetch and then come back with data from
the brokers so you can imagine there's
now this situation that we can get into
where with our partition function here
what's HAP our tician doesn't really
return a lazy sequence of
lazy sequences it returns a lazy
sequence of fully realized sequences so
what's happening here is to get those
five thousand messages we're actually
going to network to make sure that we
have them there and so until we have
those five thousand messages we can't
start doing the most CPU intensive
operation which is unpacking each one of
those messages and what we're doing is
we're just locked stepping network into
CPU and the network into CP do and this
is why we're getting this weird pattern
and the way we can fix this doesn't look
that crazy instead of using the messages
function we're use this create message
stream function which is a very very
very light wrapper on top of the Java
function that does the same thing and it
returns what the Java API usually works
with which is a Kafka stream object by
itself and that is exposed as a Java
iterable so because it's Java iterable
we can just use take on it and take will
return a lazy sequence and then we can
start unpacking our pings now that
group-by down there is going to force
out the laziness but what this allows us
to do is pull from network as we're
unpacking at the same time and now we
have this nice balance and you might
think this isn't happening concurrently
because of the single thread but it's
actually not because the message stream
object has its own threads that are
going out to network to fetch data so
this really is a concurrent operation
that's happening just using a lazy
sequence which is pretty cool you might
be saying Devon like I know about
concurrency I write closure man come on
closure has these awesome things called
channels and you should be using a
channel here and I would tell you you
know you're kind of right you could use
a channel and if you did use a channel
it's really easy because there's already
an exposed function and core async
called on to Jan that takes a Java
iterable and turns it into a channel and
all of a sudden all of your messages are
in channel and you can put a transducer
on them and then you can get out batches
from your transducer so this is
essentially the same thing except
there's this weird underlying thing
that's happening which is that on to
Chan is functioning as a go loop which
is going to wait on the Java iterable
and then write into the channel it's
going to park when it writes into the
channel but it can't park on the job
iterable it
sitting there so what you're ultimately
doing is you're just locking a thread
against the consumption side of things
so this is actually not I mean it's nice
and there's places where this is really
useful there's times when you want
things as channels especially in
multi-threaded situations but it's not
that helpful for us right now if you're
just going for raw performance so you
might be saying what do you mean
multi-threading so multi-threading is
pretty important with writing closure
consumers and it's pretty easy to so
there's a stripe slight tweak on a
function that we're using to get our
stream object it's now create message
streams this creates for Kafka stream
objects and we can just iterate over
each one of those spin above thread and
do the exact same thing we were doing
before to take messages from that so you
should be doing this when you're writing
a consumer and this is actually how you
get the most out of your consumer now
there's when you have multiple stream
objects we need to keep in mind is that
each one of those streams itself acts
basically as a consumer and has its own
to internal cue and matches up against
one of the partitions so if you are
gonna be doing this there's a magic
function that I don't know if this is
right but it works really well for us
the number of partitions you have should
be equal to the number of processes
times the number of Kafka stream objects
times the number of consumer fetchers
threads which you can configure inside
the consumer
I'm sure someone who's optimizing things
later will be happy with that so and
this is something I should just mention
real quick there is a new consumer
that's a considered beta quality in
Kafka onine one thing it does offer is
much better ways of dealing with
committing to the brokers I'm not
focusing too much I'm committing on this
talk because it's an get really
complicated real fast but I would say if
there's some dicey situations with the
high level consumer right now which is
what we've been talking about and this
kind of takes care of a lot of those
situations but it does lock step you
against network but I would recommend
using this for you know when you really
don't want any data loss so how are we
doing we're doing really good we have a
successful application everyone's happy
we're having some
here's a chart beat everything's great
we're all launched we're good but then
comes product Adam comes and he says ah
Devin look at this thing this is really
cool but you know when I look at this I
think I I see there's getting three
clicks per minute on this position here
or in this article right here but what
does that mean sure that's like doing
pretty good on the page and the thing to
the left is doing a little bit better
but how do I know the thing to the left
on it not doing better just because it's
in a bigger position and the thing over
here is not doing as well cuz it's in a
smaller position how is this doing
compared to what I would expect it to do
in this position can you give me
position metrics as well I go sure Adam
sounds easy so what's our new user story
our new user story is now as a homepage
editor I want real-time clicks per
minute on each article and its position
on my home page I want a historical fix
for each article in its position on my
own page and that's how I can make
informed decisions not just about the
content on my home page but the
arrangement of the content of my own
page and this is actually like a really
big deal for home page editors and
publishers they it's exactly like there
was an editor who used to sit there and
edit you know the New York Times paper
like the front page of the paper to make
sure that people would you know pick one
up and read it this is a very big job
where there's someone sitting there all
day who's using this tool who's
basically just making decisions about
where the content lays out constantly
all day as new stories appear so how are
you gonna get position data so this is
where things get kind of crazy before we
were just using the HP refer to say we
went from page a to page B from Gawker
coms homepage to Gawker accom slash
article 1 now we need to get position
data and we can't consistently use
cookies the reason we can't is because
under EU laws for example you have to
say that it's okay that you're using
cookies there's plenty people that don't
have cookies on the browsers I know lots
of you out there probably are those
people and so what we do instead is that
we send what we call a Hail Mary ping on
the click event
so a hail-mary ping is exactly what it
sounds like it is just like a Hail Mary
pass we don't know if it's going to make
it but it generally fails with a pretty
evenly random distribution and what that
lets us do is it lets us say okay if I
saw like you know two clicks from page a
to page B in position one and three
clicks from page a to position two then
the ten clicks I actually saw a go from
page a to page B I can say four of them
go to position one and six of them go to
position two that's what my data
scientist calls imputing I don't know if
that's the right term but but what that
means that requests what that requires
of us that means that we have to retain
that data somehow we need to join the
stream onto itself to look to join these
Hail Mary pings with the position data
against the click pings so I'm not gonna
get too much into the slide because it's
kind of messy but this is our new
consumer the biggest difference here is
that instead of writing to the database
we are now we're taking those groups
clay at that group click data and just
storing it as a map and they were taking
this position data and we're creating a
bunch of these position maps and then
we're storing those position maps inside
of some sort of data structure to hold
on to it and right now that data
structure is a vector and so there's
three important things that this vector
is gonna have to be able to do one we're
gonna have to ill add new positions to
the day to the vector two will have to
be able to decay out old positions
because we want it to be a moving window
say the past five minutes of data and
three we're gonna want to be able to
look up what the matching data is when
we say there is a click from page a to
page B we're gonna want to say what were
all the positions that we saw four
clicks that went from page a to page B
that work that work there we go so this
is what those functions are gonna look
like with the vector adding new
positions is just going to be you know
at conjuring those positions on decaying
the old positions is just gonna be a
drop while and pulling out the positions
that match is just going to be a filter
pretty straightforward right oh but
we're going really slowly again why
oh my god my favorite one about this one
by the way is that guy in the back right
corner that doesn't realize what's going
on he's dumbstruck
he never knew that door opened so why is
this going so slow well because so
appending on to the end of the list
that's an O of K operation for K items
that you're adding on to the end pretty
simple
decaying old positions using a drop
while well that's oh of n because it's a
vector you can't pull from the front
very well it's a pen to the right pull
from the front and getting the matching
positions that's actually the worst
that's Oh N but the thing is this is the
thing that's being called the most this
is the thing where every time a writing
database we need to get the matching
position data so this is the thing we
really need to optimize for so how are
we going to do this in comes closure
data priority map you might be saying
what is a priority map so a map usually
refers to a hash table and priority
usually refers to a priority queue which
is generally implemented as a heap now
with a hash table you have oh of one ish
look up and over one ish insertion and
with a priority queue you have oh of one
ish peeking at the top of the queue
whatever the highest priority thing is
you have ov log n insertion and ov log n
removal or popping off the queue so a
priority map is when these point to the
same data and it's really cool what this
lets us do is it lets us create a map
where we assign some sort of priority to
each one of the items and now insertions
into our map is going to be oval again
removal from our map is Bo of log N and
look up on our map for the key is still
o of 1 which is really really really
awesome
and the one slightly more complicated
thing about how we're going to this data
structure is we're gonna have each one
of those nodes instead of being a
position itself it's going to point to a
queue so the way this is going to work
is the hash table is going to be the
page a to page B the priority so the
queue that we're pointing to is going to
be all the position data that within our
moving window that we want for from page
a to page B all those positions and then
on the priority queue side it's going to
you be organizing itself based on the
oldest item in each one of those cues so
that sounds really crazy and you must
think like how on earth does this get
implemented it's actually really really
really simple of this library all we say
is priority map key FN buy and we look
at peeking on the queue so that's each
one of those cues we're gonna pipe peek
the first thing that's of1 because it's
a cue we're going to look at the time
stamp and then we're gonna sort them all
by less than so whatever is the toppest
priority in our priority map is always
going to be the cue with the oldest item
in it which is pretty nice so what do
our new functions look like adding a new
position or adding new positions it's
just going to be a pretty simple reduce
with an update on to the positions and
the only weird thing is that whenever
there's nothing there is we're gonna
have to create a new empty queue
decaying old positions this is a pretty
fun one what we do is we create a loop
and we keep looping peaking at the top
of the priority map and then peaking at
that queue and looking what that
timestamp is and that's always going to
be the oldest thing so we just keep
doing this until we've decayed out our
window and when we need to get rid of an
item then we pull out that queue pop off
the end and reinsert it and then getting
the matching position data is going to
be really simple it's a map just to get
so the run time for these is gonna be
something like oh of K times log n for
the adding new position data which is
slower but not that big of a deal
decaying old position data is going to
be the number of things to takeda times
log n which is probably faster and then
looking up the position which is the
most called function of all is going to
be o of one now so run time is way
better we're super successful look at
this guy so this is what it was before
position we had position data and this
is what it is after we have position
data now we can see other articles that
were there and we can see what the
typical would be and we can say that
this is doing fine but that story that
was there earlier was actually much
better so what have we learned so the
first thing is be
the head of an infinite lazy sequence I
think this is actually something you
could run into when doing any type of
closure stream processing batching
whenever possible is a great way of
handling data it lets you reduce Network
calls don't lockstep your network and
your CPU embrace the laziness of closure
priority maps are the BOM
multi-threading one possible and writing
a consumer and closure is actually
pretty easy if you think about it we
started with something very different
and we went through lots of reef actors
and we ended up with something that
didn't look that insane it's pretty nice
and what do you think of what it would
look like in Java it's not that fun like
I've written Java consumers to and
they're awful like really just I mean
maybe I'm biased but they're the worst
and it's funny because when you look at
every one of these you know stream
processing frameworks and you look at
what they're written in they're written
in scala enclosure and when you look at
their recommended api's they're all java
and this is because they expect everyone
to be using Java it's an easy language
but what's nice about that is it means
that if you want to be writing with
closure you're always going to be a
first class citizen when you want to be
doing something because you're just
interfacing with Java Java Interop is
fine it's so easy so this is great so I
think what I really want to get to here
is that closure is not that it's not
that closure is good for Kafka closure
is really awesome for general stream
processing I actually think this is kind
of the future of this language I know a
lot of people who come to this
conference are people who don't get to
use closure in their day to day job and
they're kind of looking for reasons to
use closure right and there's all these
reasons to use closure you can use it to
write web servers you can use it in
closure script you can make apps with it
but all of those spaces have like fairly
kind of strong competition stream
processing really really really sucks
like writing and Java is not fun for
this kind of stuff and closure is just a
Java interface and then you're
processing lazy sequences it's a lisp
it's perfect for this so I'm really
hoping that you know ten years from now
when people think of closure it'll be
the same thing as like when people think
of Ruby they think on Rails they think
of C++ they think of gay
when they think of scheme they think
about writing scheme interpreters like I
really think that I really think that
closures future is going to be stream
processing thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>