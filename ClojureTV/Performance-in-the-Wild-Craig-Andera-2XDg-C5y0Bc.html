<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance in the Wild - Craig Andera | Coder Coacher - Coaching Coders</title><meta content="Performance in the Wild - Craig Andera - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Performance in the Wild - Craig Andera</b></h2><h5 class="post__date">2013-01-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2XDg-C5y0Bc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I am privileged to introduce our next
speaker as man taught a label ssin
everything he knows about C sharp I
actually and the reason Craig is here
today I made him do this talk we were
both working on this project under said
you got to present this and he was very
humble he wasn't going to do it and then
I revealed that I had compromising
photos of him with Stu Holloway
i granted that if he didn't didn't do
this talk enclosure cons that i would
send them to kneel as we're in this
performance related it's not really
about closure performance it's about how
do you approach performance engineering
and how that approach served us when we
were running a recent closure project so
there is some closure echidna start by
talking though about model based
optimization
a way to look at your systems to ensure
that when you when you do try to make it
as fast as they need to be and you do
that the right way okay and specifically
about one family of models that I like
to use what I'm working on the types of
system that i tend to work on from there
we're going to how you apply that into a
technique I like to use called the
optimization loop that and then finally
if the enemy of the payoff is where i
talk to you about how the closures just
we work on benefited from this approach
all right this stuff like i said i don't
practitioner this stuff all comes out of
my practice this is not you know some
theories his stuff that has worked for
me I hope it'll work for you but I
didn't have to work on transactional web
based systems you work under the types
of systems i think there's stuff in here
that you can use but you have to play a
little bit too good work for you okay so
your mileage may vary all right so i
want to talk about model based
optimization and almost a little story I
tell me if this sounds familiar you are
working on a project you get to the very
end complete slavery or whatever and
someone says hey it's time to make it
faster right now is the time to optimize
I make sure you're eating our targets so
you go to the USA so how fast does it
need to be and they say three like
really that's all you got there like yep
that's what I got make it three so you
go to the homepage you measure the
homepage and briefs through the
homepages to turning in three all right
light years or whatever and then you're
done right and you should okay yes this
is an exaggeration but you understand
what I'm saying here right like well
what something what's wrong with this
picture here's what right it does late
the very end of the project is not the
best time to do with this stuff we'll
talk about why that is it's done
haphazardly does the homepage
performance assuming that you know how
long it takes the homepage to return
when you walk up with for the first time
is even a good measurement of the
performance that
does that number even mean anything in
terms of what you're trying to do
fundamentally the problem here is that
this stuff is being done in the absence
of an adequate model my opinion right
you have a lot of your model design hi
to walk up to the system it measure how
long it takes the whole patient but
that's not an adequate month and the
reason for that is that performance is
not hardly ever visit a scalar quantity
right it's not three okay if I ask you
how long does it take you to go to the
grocery store and buy some stuff and
come home right as this transaction
you're going on conclusive stuff you're
coming back you're not going to tell me
17 minutes and 32 seconds you're not
going to characterize that operation as
a single number you're going to say well
it depends whether I've got to take the
kids and what I'm buying and how traffic
is right so these things apply your
application as well and it's works right
because your application probably
doesn't do one thing right you're not
building a home page presumably that
does nothing but just you know display
the home page it has a bunch of stuff in
different combinations right the world
was not deterministic here we enrich
likes to talk about you know
observations of the world was the world
like the world not it stochastic
stochastic things involving randomness
there's all sorts of things that can
happen and we need to characterize in my
opinion we need to characterize a system
in a stochastic mindset right when you
have a stochastic model of the system's
performance so models right like I said
you have a model for your system even if
you do it haphazardly your model is
right that it's always this way and I
don't think that's right but we're
really looking for is a more useful
model so we can answer questions useful
to me means can I answer questions about
the system if I have a model that tells
you what the system is going to do
ninety-nine percent of time
tastic 5th ring ninety-nine percent of
the time when I got 10 users through
business that whatever right have a
model describes that that's useful I can
answer questions with that I can design
make decisions about a global system
with that what will it do with my
dimension on hacker news and some others
10,000 users that may be something
that's really useful to you to know okay
if a model that can explain that you can
make you sit capacity planner there's
all sorts of things you can do if you
have a model hey if your model is just
the system always returns the homepage
in three seconds for whatever you know
three years three months it's not very
useful so we want to build these models
want to have a model for our system or
family of models that allows us to
answer questions the thing is this model
is assuredly wrong okay why because it's
not the real world right what's your
system going to do in the real world
what you're going to test it right
you're going to build a model you're
going to do that by going testing
because if you build a surface systems
that I do and it's going to tell you how
the system behaves are various flows
right stochastically will get a function
that will explain to us what the
distribution of response times will look
like but that's not really the same
thing as saying if the system will do in
the real world there's all sorts of
reasons that could be right but they
come out of your assumptions right
things like you know you're testing you
test environment is productions off
limits or you can can't we can't have
any production
that's common small versus large data
right maybe your system you tested it's
got 10,000 rows the database but after a
year and you should get 10 million rows
and data and once the system is limited
by database performance which is
dominated by you know how you index
whatever so you know these experiments
are you looking at I was working at the
system that i worked on was Stewart
Sierra we were unable to test the system
in a load balanced environment or a
number of reasons that most more
technical so we had to say well we
assume that the load balanced
performance the system will be different
that was an assumption right sort model
was wrong it was not going to tell us
the fun of the performance of the real
world system the thing is is that's not
an excuse for not doing this right I
mean like the quote that I love is it's
not about him whether the model is right
or not right all models are rolling the
question is how long have to be before
is still useful and I really like this
because I it comes back again into can I
answer questions is this instant use and
I think it's really very possible to
build a model of your system that has
all the problems and conquer but it is
still useful for answering questions are
still useful or decided should i think
is that the next optimization or not how
many servers do I need there's always
much baggage right but I think it's
still an important to remember that
prominent role models can be useful um
the other thing that you need to do here
is to make sure that when you're going
to miss processing built your mom that
you are explicit and deliberate and and
careful about identifying their
assumptions hey customer we think the
system will perform like this because we
mentioned it in this environment that
differs from the introduction and primer
in the following ways okay can make
incisions with that so like said there's
a model that I used it's really a family
of house it's a model generation
technique that i like to use when i'm
building the transaction web systems I
like to make sure
I think about the system as when I'm
characterizing the system I want to
characterize at characterize it as what
is the distribution of ladies response
times however you want to say that what
is the distribution of legacies add a
given load right when the system is
pressing this many requests per second
what is the distribution late 69 motion
of them come back in this long time 95
years of them to vacuum this amount of
time etc etc what's the distribution of
the response times additive throughput
for a given transaction mix that's
super-important right it's really easy
to walk up to a system and measured it
doing one thing maybe reads and never
throw rights in there but there's you
know or or whatever right your system
has 17 pages and you just hit one of
them right you want to be thinking about
all these things are you want to be
thinking what is it the distribution our
response times added you know across who
puts four and given transaction mix or
mixes whatever is important to you plus
whatever the constraints you have this
could be anything right we had a system
where and I'll talk about later the the
limits were that in the 99 percent of
the responses had to come back in 10
milliseconds or less at such and such a
load and the responses which we're
coming out of the back end system had to
be no older than five minutes right why
is that important because it controls
how we can do cash you couldn't cash
birds you no longer than five minutes
right so so these these are the things
that you need to identify right but i
think that the fundamental features here
are what is the distribution of latency
at four about of a set of turn puts is a
really really useful default case to
make right and the way you get this is
um by load test so we've all probably
done load testing and you probably in
generating a graph it looks like this
all right so i drive a system with you
know one singularly
so then I Drive away two years then I'd
write about three users and I see what
the throughput looks like how many you
know how many requests an advantage in
the system for some transaction mix
right ninety percent reads ten percent
rights whatever that means to you and
you get a graphic looks like this okay a
couple things to know here first of all
that the bumping this is real more data
that I pulled from a test that i did way
back much years ago and HUD one of the
things that i preserved is you have to
get used to this right like we were
never able to explain why through what
went down and back up and get me load
this is no more feminine right this is
the way it was but the other thing isn't
this is not a stochastic model what are
we missing distribution of latencies
right like I don't know you know what
the response time is yeah I'm servicing
180 requests per second how long do they
take right in transactional system that
matters I don't think any of your
customers care that you're serving a
million users if they have to wait 30
seconds right these things are typically
important in transactional systems so
you really want is this right this is
what you're after and again there's this
some weird mr. on the right on the right
there that's the district relates these
notice that the system is actually much
slower low load never figured that one
either sorry right we didn't care about
that regime so it didn't matter that
much but the important thing here is i
can look at the system and now if i have
this this is my model right for some
transaction mix this is my model i can
answer all sorts of interesting
questions right can we meet customer
expectations well I've got 10 users each
one is hitting the system whatever once
a second make an appt you see okay bye
throughput is 10 requests per second I
need them to be served in less than 2
2nd 99
the time I can answer that question with
this model it's not the real world right
we have some assumptions but it's still
useful right so this is again I mean I
keep hitting on this but it's I think
it's super important it's something I've
seen even senior engineers get wrong the
distribution plates is for editing that
characterize I globally for a different
transaction it's really super useful to
look at the system that way like Santa's
other questions you can answer as well
so so you developed your model right and
it turns out it doesn't meet your
requirements you know it's obvious oh
geez well even in the best case you know
our requests are coming back way too
slow what should we do about it I think
that there's a pretty easy set of steps
involved with a small there's a set of
steps they're not always easy to do an
easy simple whatever I was speaking in
the graduate so there's a set of steps
you can follow and i hope the
optimization you benchmark the system
okay to give you an idea of what your
what your model currently is right this
is us measuring what's the distribution
of response times add given throughput
or you know cross limits for a given
transaction X great where are we it and
maybe answer is you don't great but
probably not so analyze this figure out
what you need to figure out what the
thing what's preventing me from being
your goals make a recommendation what to
do a reclamation like you do nothing and
then make an optimization repeat dilute
you know rinse and repeat okay so let's
take a look at each of these
individually so benchmarking like I said
this is where you're going to do your
load test right you're going to drive
the system at low low and medium low
high low get your curves that your
distributions okay there's a bunch of
things you can do what you should do
you shouldn't do you definitely want to
measure the framers remodel right and
what I mean is don't forget your
training to the distribution you need if
you need to make sure that you drive
your load test in such a way that you
can actually get that Davis you can
build your model um remember transaction
X we can film that enough understand
what you're measuring okay this comes
back to that what are the assumptions in
my model right your model is wrong why
is it wrong what are your assumptions um
you may say I am I am measuring user
experience response the moment from
where the user hits no clicks a button
until the moment where the homepage
finishes rendering but if your loan test
is being instrumented by doing timing
where the request hits the web server
you're not really measuring that right
you're measuring response time inside
the server and that's fun right again
you have to make assumptions of this
model but you need to remember that you
need to you need to understand what are
you measuring what is it then your
action to mention anything change your
test or update your assumptions or
modify your mom do whatever it is you
need to do everything advancers you need
you need to sorry I think you be more
specific advice there but these you know
it just depends on the project I've seen
this change bradamanti political results
of the project
technical constraints on what are some
things you can forget it don't mistake
number of threads per load I've seen
this happen to it's very easy you know
if you use it to like jmeter which is
one that I've used to great effect to
generate a load of that offensive you
know the parameter the knob you can turn
to jmeter the EC's now the turn is how
many threads you're in one thread two
threads three threads through turning
away as fast as I can it's not really
the same it's certainly not the same
thing as the real thing you're trying to
characterize which is you know horribly
low of what's response time look like
it's definitely not number of users and
it's certainly not any kind of metric
other than here's what I put into Jane
here i just mentioned that because i
have seen people do this over and over
again they draw the graph and say here's
what we got I mean the graphic and the
throughput graph at the bottom it says
one two three four five threats it's
completely useless erase that axis from
here you're not helping anybody you will
confuse your customers they will think
that's number of users right your
developers with like that's number of
users or something else completely don't
get those things confused it seems like
an odd thing to mention that i've seen
them enough that i left him um don't
forget to look for errors just happened
us we were measuring the system the
system the numbers that came back with a
little strange we said that's a little
bit weird let's look into it it turned
out that all the requests that came back
for errors there was some miss
configuration and we were measuring or
we measured we're measuring how long it
took to render narrow paycheck and i am
not particularly useful but but it's
important right when you when you
produce this report you're looking
you're whether you drive out of their
load test your you want to say okay oh
my got this through wood this is the
distributions of Layton sees and hears
how many errors their work okay and if
that's not part of your transaction
weeks if you
defining that some of those requests
should result in errors that never
better be 0 right so update your board
to retina and giant red letters or
whatever is my mic cutting up I can't
really hear of it I'm gonna I'll try
that for a second if that doesn't work
I'll go to the old Craig boosted audio
is that better okay you hear me better I
great don't keep going if you're done
it's a waste of time that's one of the
things the loop helps you with right
remember the the UH there's a big red
arrow right there you're done stop stop
optimizing you can continue to measure
your performance if I talked like this
anybody in the back that can't hear me
great alright so yeah don't keep going
if you're done and don't wait to start
right you can do this early in your
project I'll talk a little bit more
about that when we talk about
optimization I mentioned tools briefly
we'd use jmeter on other projects we've
used a be HTTP / if there's tons of
stuff out there again what you're after
is be able to drive the system at
different loads and be able to capture
the distribution of latency said you can
do those two things it doesn't really
matter what you use right um as far as
analysis that's in the wrong spot we'll
talk about by the video oh I'm sorry
that is actually the right spot the
analysis of the load results um you can
use whatever you want to I some people
use excel but we've got this great tool
for analyzing reams of data is called
closure you might have heard of it I
hear they have a conference um yeah we
use this on our project in fact luc van
der hart um stewart Sierra's co-author
another colleague of ours and relevance
um wrote a plug into jmeter on that lets
you actually visualize the results right
in jmeter pretty cool j meters written
in java
integrates well with that so i'm not
sure where that stuff is you know if he
has that a line or not Stuart I don't
okay so will bug him if you'll get that
out there but I'm really if you can
produce a chart that's all you need
right okay so you've done your
benchmarking you're not done okay now
you do some analysis there's something
that's preventing my system from being
fast what is it well let's analyze it to
find out do we want to find the single
biggest factor in the performance of
your system ideally okay we want to do
it empirically don't guess I've seen
senior engineers do this too oh it's
gotta be because we're using an
insertion sort right that's got to be it
no measure how do you do that profile I
put on the slide but we've used York it
works great with closure really nice too
old highly recommended now notice that
you don't actually have to when you're
doing the analysis you don't necessarily
have to drive your system the same way
that you do during your load test is not
always important that your system be
processing its peak capacity because
oftentimes what you care about and in
transactional assistance it is common
especially in early rounds that it's
it's simply easier to analyze a single
request for the system or you know a
small number requests through the system
and one thing is dominating your
performance so much then it doesn't
matter but as you go on and you make
more and more sophisticated
optimizations you may discover that it
actually is important to run your system
at load but generally in early rounds I
haven't bothered to do that the other
important part of analysis is lots of
hard thinking especially in the later
rounds it can be tricky right hip
typical cases one thing dominates eighty
percent you take that out oh now there's
one thing that's sixty percent you take
that one out well eventually you're at
the point where there's a 40 or 50
things that all take two percent of your
time that's hard you're gonna have to
think hard about what your system is
doing in order to figure out what the
one thing is
limiting you again be aware of
transaction mix right I said that you
can simplify you don't necessarily have
to run your system at peak load in order
to do this sort of analysis but you do
very much want to make sure that you are
representing all the different
operations that your system does right
in a balanced way or maybe you have to
say okay we're going to analyze the read
theft or analyze the right path look at
those things figure out where you need
to spend your time okay again no generic
advice here this is the hard work all
right so you vendetta you identified the
thing is dominating your performance the
next step is make a recommendation again
this is usually pretty easy at first
I'll give you a hint it's the database
now I'm kidding actually that's that's
not right write me want to be empirical
that's another mistake people make oh
it's got to be the database and it often
is but you have to measure okay use the
data you've just done this profile
you've got load tests you've got day to
make use of it look at it see what it
tells you the best recommendation you
can make is we're done now that could be
for a couple reasons one is you've hit
your target right in that case you
probably stopped at the benchmarking
phase the other one is it's too
expensive to continue right the customer
has changed their mind we told them it
would take another five weeks to do this
to get whatever we think is going to
happen you know whatever we think is
going to make the system better five
weeks for the five percent no thanks
we'll read our SLA service level
agreement will just rewrite it okay
whatever your recommendation is it's
assuming it's not let's stop the next
the next thing you want to do is
optimize right fix the one slowest thing
scientific method right don't change 95
variables at the same time change one
thing this loop is intended to be pretty
tight right you can get through it
pretty quick here's the kicker
you might find out that redesign is
necessary there is no guarantee that you
will find something where it's like if I
just replace this algorithm if I just
cash this the system will suddenly get
ten times faster sorry you might miss
with databases you might have here
something truly horrible like write
rewrite the system in C right this is
why you want to start early there's a
great quote that I love that's such a
pair of course right you've probably
seen some of this or some variation I'll
paraphrase um you know any problem
computer science can be solved by adding
layer of indirection any problem
performance engineering can be solved by
removing a layer of indirection it's
funny but it's true um you know and what
does that mean it means that layer of
indirection is something like your
virtual machine you might have to
redesign you want to start early now you
don't need to starting early you could
start at the beginning of the project
when you've got a completely you know
you just you've generated your framework
right it doesn't do anything yet you
don't necessarily have to start on day
one um but I think there's value in
starting well ahead of actually being
done with the project and it's this your
your customer your stakeholder may not
yet have identified where they need to
be but there is value in knowing whether
the change that you may just now
affected the system in a relative way
right my performance is that 99% of
requests will come back in less than 10
milliseconds when the system is loaded
up to 500 quest per second and my done
or not no idea customer won't tell me
but I know that's twice as slow as
yesterday I did something right that's
important information so I think there's
real value starting early even when you
don't have requirements
so I want to talk to you now about how
we were able to apply this the wii
reticence a relevance we're working with
a customer on a web service written in
almost pure closure and i'll get into
the almost in a second it was a
intrusion of texas detection system like
i said web service so you know Jim
someone make a request we make a JSON
response it was actually almost an ideal
case because they actually could only do
one thing so for us transaction mix was
simple tons and tons of this one type of
request right um in terms of where we
started now I realize that these are not
stated to casta CLE right we have
started a good place part of the reason
I'm here talking to you is because the
things we discovered were that this was
a terrible metric for where we were we
started out about 75 requests per second
peak the most you can jam through the
system before performer started to the
grave was 75 requests per second before
throughput started degrade with 75
requests per second at about 25
milliseconds average latency that's
average what an awful measure right an
average so half your users are doing
worse than that who cares right so
already we already we know we could do
better than this in just in terms of
characterizing the system so what did we
do well the biggest thing we did I think
was to prepare the customer we actually
sat down with them and said okay here is
how you need to think about the system
you need to be thinking about guess what
right what is the distribution I don't
know if I didn't finish that right
what's the distribution of Layton sees
you know at some throughput this
percentage of the time and we got that
right which was great because that's
what I would actually work to in terms
of applying the optimization loop once
we got buy-in to do the work and
customer identify where they wanted to
be in those stochastic terms we got them
into that stochastic mindset we took a
two card approach right we used story
cards we would generate one card which
was a combination of benchmarking and
analysis and recommendation so we would
run our load test and also do profiling
and then out of that we would generate a
recommendation the recommendation was
actually another story card so we had
card one card the output of that card
was another card which was on the
optimization we wanted to do right and
then the output of that card was the
work and also another benchmarking card
right and this worked really well for us
because it kept a customer involved we
were able to do these I forget exactly
but we're able to do a bunch you know a
few rounds one or two a week and at
every point we generate this card
customers say oh I see what I see where
the system is that now oh I understand
your recommendation yes let's do that
optimization we would do the
optimization they say yeah optimization
work is done I think it makes sense to
benchmark again worked really well for
us to kept that involved and it made a
record in the project history of what we
had done right that was useful too we
spent a month maybe a little bit more on
this yet so we start here's where do we
start remember 75 requests per second
peak 25 milliseconds average latency
right so who knows what the distribution
was but we know that it was certainly
worse in the ninety-five percent case
from 25 milliseconds so where do we get
to well we did pretty good you made the
system 20 times faster in terms of
throughput we got our latency all the
way down to less than 10 milliseconds
when the system was running at 1500
requests per second now that's
interesting isn't it right because it'd
be very easy look at the system and go
yeah the pink throughput a second
request per second that's the regime we
live it but it's not we can't let the
system get driven that hard because if
we do our latency requirements would
have slipped right they needed us to be
under 10 milliseconds you can't drive
this list of all the way to peak
capacity and still meet 10
seconds ninety-nine percent of the time
they were thrilled we got we got to I
mean this is we got to where they wanted
to be in two years over the course this
month that part of that is an artifact
of the system I mean there's there's
nothing that says that just because you
follow this process you will see the
same result but I still think its
history share with this crowd because if
nothing else it's a great existence
proof for four people you know can you
build fast web based systems with
closure hell yeah these numbers are
awesome this is one machine remember I
said that we couldn't test on our
production load balanced environment
weird story I won't even go into it
right we do we did one of our one of our
explicit assumptions was that our load
balanced um performance would be with
every would scale close to linearly we
have good reason to believe that's true
right again it was an assumption we were
explicit about that it's important to do
that but we think that with two systems
right sort of the minimum you want just
for availability that we can get to 3000
request a second and still maintain 10
millisecond response time at fifty
hundred crust per second I'm sorry at
ninety nine percent confidence this is
great um of course a funny thing
happened on the way to the you know
endline the goal i'm actually lots of
funny things happen in stuart now I got
to work on this a lot together um one of
the fun things that happened was that as
we did a couple rounds and took out the
easy stuff which was the database right
once we were cashing the database uh
this is a case where you know you got a
mesh we got to use the data the thing we
found that was that was dominating the
the single request response time was
logging right we were using syslog
through syslog for j and that was
something like eighty percent of our
time in any different request was spent
in logging okay totally unexpected right
we had one guy on the team who was like
not it's got to be this close ruling you
know I don't fertilize there was
something
totally was not logging so did our
analysis and wow that's it and over the
course of the weekend Stewart Sierra and
Erinn bedre is also here roll a very
thin see wrapper around the syslog API
call wrote a job a layer on top of that
and we were able to invoke it from
closure and I want to say that something
like quadrupled the performance of the
system is amazing right and I think that
was that was a clear demonstration of
the benefit of this loop right up being
very delivered at each step saying okay
it's we're going to be scientific right
let's do our analysis let's look at data
oh yeah that's it and just applying just
a little bit of that platform low level
platform code it's literally 10 lines of
C boom suddenly we were way back up
there now one of the interesting
consequences of that was once the once
we unblocked ourselves and were able to
get to those higher levels of throughput
we were generating an enormous amount of
log data huge 2 gigabytes an hour is
that right something like that the
client initially wanted us to keep all
of that we were like men wear
yeah we have just like allocate a new
free account right mr. Moore yeah and so
you know we played around a bunch of
stuff one of the things that we tried
was using the syslog forwarding
mechanism you know she can send syslog
messages across the network uh at which
point we discovered that for what the
version of syslog that we were using
which was dictated to us by the clients
infrastructure um it was dropping ninety
percent of the messages which I suppose
is one way to store that much data yeah
the other interesting thing that we
thought we resolved that providentially
that's another story you can catch me
afterwards if you wanna hear a little
bit about that but one of the other
things we found was that we had set up
automated load testing so every night
you know in the middle of the night on a
system would run the load test which we
inscripted through jmeter and it would
email us every morning results of the
test one day we saw that our peak
throughput was down like a half
something ridiculous um like whoa what
happened we eventually tracked it down
to a one character change in a config
file if you put a minus at the beginning
of a lot of syslog config file for the
version of syslog that we were using it
means log asynchronously which was fine
for us someone had taken that out boo
right so that was a huge value to us in
that's the start early thing right you
wanna you want to get this stuff
automated because it tells you things
that have nothing to do with absolute
performance meeting your goals it tells
you there's been a change right and we
could easily have gotten to production
with that and not reap the benefits of
the this isn't really could benefit from
every drop of performance we could
squeeze out of it I mentioned the fact
that we never tested load balance and
one of the other cool side effects of
this was by automating our load tests we
got a stress test for free right where
we're drawing that throughput curve oh
look here's the peak capacity of our
system we're doing this we're driving it
is hard
it can keep up with well we've got a
test already that tries it that hard
let's just run for longer so every
weekend we would run it over the weekend
and we spent weeks and weeks doing that
because we found out that the system had
problems over the long run nothing to do
with closure the rather thick factors
there but that was great right that was
a nice side effect of all this so this
was a really good experience for us to
be none worked out great for the
customer and we got lots of nice
benefits out of it too so what do I want
you to take away from this is could
really be summed up as when you're doing
performance engineering be deliberate
right think about your mom say what it
is make sure it's stochastic because the
world is to Casting but at the same time
recognize that your model is wrong right
that it deviates from the real world if
you do need to make changes to address a
performance review system I would
suggest you you use this loop right
measure adjust measure again right
that's essential we're talking about and
we had we had very good luck with that
so be deliberate really that's the
message right well everyone's still
awake I think so I appreciate that Dom
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>