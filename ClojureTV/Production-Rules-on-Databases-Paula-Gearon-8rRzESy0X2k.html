<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Production Rules on Databases - Paula Gearon | Coder Coacher - Coaching Coders</title><meta content="Production Rules on Databases - Paula Gearon - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Production Rules on Databases - Paula Gearon</b></h2><h5 class="post__date">2016-12-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8rRzESy0X2k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the conch my talk today is on
production rules and it's an abbreviated
title it's actually on grafted databases
examples of that might be the atomic or
RDF RDF databases orient DB systems like
this I work for Cisco Systems
specifically the threat grid group
within there and we're doing everything
these days inclusion so what is logic
it's a formal mechanism for describing
the world it also provides a mechanism
for reasoning about what we know in the
world
and incidentally Wikipedia is really
great at providing definitions for
things you just take for granted one
type of logic is predicate logic as
programmers were most commonly familiar
with boolean logic but there are lots of
other types in predicate logic we define
a set of entities that make up a
universe of known things or a universe
of discourse a selection of those things
which can be used in particular roles
correspond to what we call an
interpretation of a defined system and
because of that we use the letter AI for
this universe of entities we also
describe a set of connections between
entities and this is a subset of all
possible connections between each of
them these connections form
relationships and the formula shown here
just shows binary relationships although
higher ERA T's are possible now this
example shows a universe of four enemies
in entities named Fred Mary Barney and
George a parent relationship is also
showing and that's formed from Fred to
Mary and another of Barney to Mary
however that's not the standard way of
representing relationships
instead they're usually shown in this
prefix form with the type of the
relationship first and the end of these
in the relationship provided as
parameters now this sort of frameworks
enough to describe almost anything you
might want it's even possible to stick
to exactly this with just binary
predicate since higher arity predicates
can be represented using binary
predicate only now once data has been
represented rules can be used to provide
a mechanism for reasoning over the data
we can express this using propositional
logic with the rule of inference this is
called modus ponens opponents or more
commonly just modus ponens
it's simply Latin for the way that
affirms by affirming the basic structure
is that if a given proposition P is true
then an associated consequence Q must
also be true for instance if I am in
water then I am wet or if I am on the
stage then I am nervous rules are often
represented in math and logic
programming with formula syntax called a
horn clause now given a statement of the
form if antecedent then consequent then
horn Clause is represented as the
consequent a colon - then the antecedent
a decedents may be complex expressions
this is most commonly a comma separated
list of every predicate that must be
true for the consequent to hold for the
purposes of logic programming symbols
that start with a capital letter
indicate an unbound value or more
commonly called a variable every
possible combination of values is
considered and for the full set of those
that work this is known as a valid
interpretation those values are then
used in the consequent expression and
are asserted back into the system in the
example here we might informally say the
if a has a parent be and that person has
a brother C then C is a zunka more
formally the antecedent refers to values
of a B and C that can be validly
substituted into the relationships of
parent and brother note that B appears
in both of those relationships
indicating that the same value must
appear in both places
if tuples of a B and C are found then a
new assertion of uncle will be made on
the pair a and C some of the systems
using horn clauses are prologue and
classic data log prologue is capable of
much more than assertions and simple
modus ponens operations it's capable of
performing searches on the defined range
and identifying all the interpretations
that meet a program description and this
can be much further than the simple
forward reasoning that I've described so
far now well that's much more powerful
those searches can also be completely
unbounded and they can't scale in the
general case classic data log looks and
acts a lot like simple Prolog though it
has various restrictions on it now this
evolved over time so that there are a
lot of modern variations that may share
data log semantics but no longer use
that same syntax now data log was
specifically designed to consider a
universe of discourse that is defined in
a database these necessarily impose
restrictions on the operations allowed
in data log the result is a system
that's less powerful but considerably
more scalable data log more probably
describes a set of semantics arising
from the restrictions and allowed
operations rather than a specific syntax
these days
that's why systems like SQL and atomic
refer to these semantics but early early
data log systems always used the syntax
that looked almost identical to Prolog
this started to evolve there's more
database features such as aggregates are
incorporated into data log variations
eventually some syntax some systems did
away with the original Prolog horn
clause syntax entirely now the original
data log restrictions were all
introduced for specific purposes there
were no nested complex expressions
that's to ensure tractability in the
original query database queries the
negation was limited to allow for semi
naive reasoning recursion had to be
stratified and existential declarations
were prohibited to prevent infinite
loops now the result was a system that
was guaranteed to terminate and was
sound and complete combined with the
scalability provided by databases this
made it ideal for certain kinds of
applications
now I mentioned semi naive reasoning a
moment ago this is a technique to
identify changes in large amounts of
data without even compare all the data
from one iteration to the next if data
is only being added as it is in
monotonic systems then just counting it
will identify if the change has occurred
however this only works on stored data
or on algebraic expressions on the data
that don't include any kind of negation
otherwise if you if you're including an
operation with a subtraction in it for
instance if one side were to go up and
the other side also went up then the
final count could stay the same even
though you've got a change in the data
hence we've got these constraints on
negation
now as databases took on the semantics
of sorry I've data log the evolution of
databases has led to lots of variations
in data log SQL took on an entirely new
syntax and rapidly gained numerous other
features such as aggregations Sparkle
borrowed both from data log semantics
directly and also explicitly from SQL
similarly for de tonic in each case
standard joins are allowed and without
operations like stored procedures
recursion isn't possible each system
also has its own variations on what can
be done inside embedded expressions and
while insert select statements do allow
for the creation of new data without
recursion these do not allow for
unlimited existential expressions
thereby staying
sound and complete so here I have an
example that runs on either a prologue
or classic data log system the first
part of the program is a list of for
assertion or statements called axioms
these declare data that is known to
exist this program describes a small set
of familiar relations
namely that fred has Barney as a sibling
and Mary as a parent mary has George for
a sibling and George's male the second
part is a set of rules for determining
other relationships based on the ones
that are known the first says that a
male sibling is a brother
the second describes an uncle as a
brother of a parent and the third says
that siblings share the same parents
for clarity we can show these
relationships as a graph this graph
shows all of the axioms described in the
previous slide each arrow represents a
predicate assertion so the execution of
rules that result in new statements will
introduce new arrows to the graph since
priority is not assigned to the rules in
the first instance that will be
processed in the order that they appear
in the program the first is the rule
that generates for other statements
since George is both Mary's sibling and
male then this introduces a new arc the
second rule is the uncle rule we can see
that Fred has a parent who has a brother
so we can generate a new uncle assertion
the last rule is the rule that says that
siblings have the same parents Fred is
Barney's sibling and Mary is his parent
so Barney gets Mary as a parent too now
that we have that information we can see
that the uncle rule is relevant again so
this rule needs to be run one more time
running uncle again creates a final
relationship between Barney and George
now that all possible arcs have been
generated the program can terminate but
how do we run it a naive implementation
might just run the rules over and over
until the program hits a fixed point in
the data but that approach is very
inefficient and can be quite costly the
canonical algorithm for running rules
like this is called wrecking which is a
Latin word for net the basics of the
algorithm were first published by
Charles 4G in 1974 but he didn't put out
a full formal description until 1982 in
that paper foggy included a proof that
the algorithm is the most efficient
approach possible for processing a
stream of data and that said not all
data is presented as a stream data may
already be stored and indexed
particularly in databases and this has
led to a number of refinements and
variations to ready over the years the
basic structure of ready is to
scribe rules as a network of nodes which
can match on the data that's stream
through once data matches it can be
joined to other match data which results
in these joint operations stored in
another layer of nodes by breaking the
matching and joining operations up into
simpler nodes redundancies can be
eliminated minimizing the effort of
processing a diagram for representing
this is here this isn't for the the
program I just showed but it it shows
that layered approach of of nodes you
know select nodes at the beginning our
identifying statements of the form that
they're interested in though those red
nodes provide a level of pre-testing
looking for the types of data going
through and depending on the system
typing may not always be applicable
identify data is stored in alpha memory
nodes which is represented by the yellow
ellipsis there so the data that's
presented earlier on can be matched
against data that arrives later for
instance in the program I showed earlier
the sibling statement for Mary and
George arrived before the statement that
George was male and both of those
statements were needed for the brother
rule updates the alpha memory nodes
leads to beta memory nodes being updated
where these are a mechanism for
optimizing the steps necessary for joint
operations once once they join results
in new assertions then the rule provides
its output which is often new data to be
asserted into or retracted from a
database now I'm the author of the nagas
rule system which is a very rare variant
without ready variant designed for graph
databases nagas the third engine that
I've built based on these principles the
first was in Java which I built for the
maldera RDF database in as an open
source project
the second was a commercial
them developed enclosure for revel it
Excel accepted two RDF databases through
Sparkle but in recent years I've been
spending time working with other graph
databases like our NDB and a Tomic and I
decided I wanted something more general
in RDF so I started this new rules
project initially as a hobby and I
called it CONUS which is just great for
rules but when I showed it at work my
manager said yes this is what we want
you can work on this now so but it had
to be copyright Cisco however he said I
could keep an open source so that was
fine and I had to change the name all of
our projects at work are named after
characters in the TV series Avatar The
Legend of Korra
my daughter adores dogs and Korra has a
big polar bear dog named Naga so Naga is
entirely enclosure it's licensed under
the GPL it's on github one
characteristic of the system at the
moment is that it is purely monotonic
meaning that it only adds new data to
the system it doesn't have any rules
which allow you to subtract data now
there are a number of reasons for this
which I'll get into some of them this is
both for the semantics of the system and
it's also based on the processes that we
have which require a history so I don't
expect to change it in the near future
but it is a possibility that it can be
introduced and I'll explain that as well
so we saw saw earlier on how binary
predicate sin a simple familial data
model can be represented as a graph this
is true for all binary predicate z--
during the 1970s it was shown in
description logic research that all data
can be represented using binary
predicate that means that we can
represent all our data in graph
databases now graph databases represent
entities
and scalar values as nodes and
assertions are arcs that connect these
nodes now when being stored the
statements are often not always
represented well they can be viewed as
tuples of three elements containing the
first node the arc and the second node
now the nomenclature varies across
different databases but common ones are
subject predicate object which is what
RDF uses or entity property value which
is the terminology in diatomic most
databases actually store tuples that are
wider than 3 3 values and that may
include metadata time stamps transaction
information this sort of thing
despite that tuples are often called
triples referring to the principal graph
data and that's a term I'm going to
continue using here non binary predicate
can be handled with a few simple tricks
using binary predicate unary predicate
to manage by defining a specific
property that describes those predicates
in the RDF data model that predicate is
called RDF type since a unary predicate
describes the type of the entity that
the predicate is applied to larger era
T's occurs surprisingly infrequently but
they can be modeled by creating a
reifying entity where each position in
the original predicate is represented
with its own predicate connecting the
reifying entity to the appropriate
parameter an example here is using
descriptive labels but that's not
typically available to an automated
process so Nagas using numeric
extensions on the original predicate
name the actual predicate names that are
stored are hidden from the user so the
chosen scheme isn't important it just
needs to be consistent one of the main
tasks that a graph database undertakes
is to index triples in various ways the
three principal indexes for triples are
ordered based on each of the elements of
the triple in turn with the rotation
between three different orderings now
not all databases follow each
of those possibilities typically for the
sake of space and/or speed but the
indexes are still able to be emulated
when particular lookup patterns are
required the reason for this specific
index structure is that any pattern of
bound unbounded values in a triple can
be found immediately typically with
order of log n performance in the worst
case pattern matching and storage exists
is exactly what the Alpha nodes were
doing in a ratty Network essentially the
ingest of data into the database with
The Associated indexing has
pre-processed the effort of the Alpha
nodes in the ready network consequently
we can use rapid simple simple index
lookups to implement the first part of
the network now be the Nerds perform
joint operations which are also managed
by the database from a client
perspective it isn't possible to pass
the results of your initial simple index
lookup to a subsequent joint operation
so that lookup may be needed again when
you do a join however we can usually
rely on internal caching in many graph
databases to handle that even when it's
not available often dis caches and
things like that will save some of that
work internally and even when it's not
supported the lookup cost is still
relatively minimal that means that graph
database operations can efficiently
provide all of the elements we need for
a good recce style implementation
the basic representation of a rule
against the database looks like this
where I'm using de tonics pseudocode
here the uncle rule at the top is built
from a query that takes the rule
antecedents and converts them into
patterns in a where clause the joined
result gives a sequence of values for
the ANC variables the head or consequent
of the clause is created by mapping a
reshaping function over the results and
that gives it a new set of triples
suitable for assertion back into the
database when we look at our rules in
the system we can see that both the
uncle and parent rules have a pattern
that looks for parent statements with
the first and last entity's unbound
shown as a and B in the uncle rule and
ANC in the parent rule those patterns
can be shared between the rules not all
graph databases allow constants to be
selected into results which is why re
shaping function may be necessary for a
database adapter but in showing it here
I can demonstrate that the output of the
second rule may affect the data being
searched for by the first rule that
means that the first rule should be
tested again after the second one has
been wrong also
the output of the second rule may also
affect its own input which means that
each time it runs it needs to be tested
again to see if it needs to run again
identifying these rule dependencies is
how we can iterate efficiently and
identify the termination condition so
this image represents the rule program
that I showed earlier with rules for
determining brother parent and uncle
relationships the patterns represent
each rules alpha nodes which are shown
in the middle and you can see that
there's some sharing happening the joint
operations represent the beter nodes in
the top right you can see the rule Q
waiting to be executed the brother rule
comes up first
that's dependent on the gender and
sibling patterns these patterns haven't
been looked at before so they don't have
a current count which we use for the
semi naive reasoning that means that the
rule definitely needs to be triggered
and the counts have to be updated to the
current values numbers popped up there
once those counts have been retrieved
they're stalled on our rule basis for
each pattern any rules affected by the
output of the brother rule should also
be added to the queue the only affected
rule here is the uncle uncle rule but
that's already scheduled so nothing
needs to be added now the brother rule
has been processed it can be removed
from the queue the next rule to come up
is the uncle rule its patterns have not
been resolved either so these need to be
updated for the current count once those
cancer in the rules can be annotated and
the rule can be triggered no other rules
are dependent on the uncle rule so we
move on the next rule is the parent rule
now the parent rule is dependent on the
sibling and parent patterns now these
haven't been resolved yet in the context
of this rule despite already being
resolved in the context of other rules
meaning that the rule needs to be run
and we need to get these counts
fortunately other rules already have the
current counts for these patterns for us
and they're clean so we don't need to
ask the database to look up those
numbers we can just copy them across now
we need to consider the dependencies on
that parent rule that rule affects the
counts returned by the parent pattern so
those are the numbers need to be marked
as no longer current also the rules
depending on counts that are no longer
current need to be scheduled on the
queue
the next rule to be run is the uncle
rule again now that it's been
rescheduled the only pattern with a non
current count is the parent pattern so
that count is checked and that's updated
now to two so the count for the parent
pattern changed from one to two that
shows that the rule doesn't need to be
executed again nothing depends on Uncle
so he can move to the next rule and the
parent rule also has an old count for
the parent pattern we don't need to ask
the database for that since the uncle
rule just got it for us so we can use
that again we saw a change on the parent
pattern count because that number
updated so the rule needs to be run
however this time the result of the
joint doesn't change so so there's no
need to reschedule anything further in a
normal ready Network that would be
handled directly but it requires storing
the full output of the rule or at least
something like a hash code on it that
means executing the complete rule query
second time which databases
typically don't or cache and possibly
processing that result in the rule
engine which can have significant
performance issues instead the more
efficient approach in the general case
is to follow those dependencies back to
the Alpha nodes again rescheduling the
uncle and parent rules once more so now
without stepping through when the uncle
rule runs it will refresh the parent
root pattern and see that the number
didn't didn't change so nothing happens
that the parent rule will also do a
refresh on the parent pattern reusing
that number that the uncle rule just
retrieved and also won't run at that
point this the queue will be empty and
the program stone
negations involve both retractions of
data and also the removal of data in
subtraction operations in a query for
instance a query may look for parents
except those with a spouse that would
require identifying entities that match
the second argument in a parent
statement and then removing entities
that appear in a spousal statement a
semi native evaluation on the result of
the entire rule would mean that extra
parent statements could be offset by
extra spouse statements and that would
result in a rule result size that didn't
change even though it contained
different data but we can avoid that
problem because if we determine if rules
should be run by looking at the size of
the simple pattern resolutions and not
the results of the complete subtraction
then we can see those those changes as
they come through now seminary reasoning
is also stymied by retractions since
removing and adding different statements
can result in counts appearing to be
unchanged however retractions can be
modeled as assertions of retracted data
now the query layer needs to be made
aware of the need to do these
retractions dynamically and that'll
create more complex join patterns but
simple indexes will still be monotonic
meaning that the pattern matches will
continue to work and then after a rules
run is terminated those retractions can
be merged into the main graph retracting
those things for real so I've
implemented that that kind of approach
in the past but Naga doesn't currently
support it now many things can influence
the complexity of a rule program but the
primary factor is usually an exponential
factor in the number of rules so
reducing the number of rules can have a
significant benefit we can often
accomplish this by the use of ontology
vocabularies which describe properties
on predicates that requires second
operations which is often not possible
in logic programming but the symmetric
nature of the graph representation gives
us a virtual second order system it's
not formally second order but it's
virtually there now a lot of research
into ontological descriptions has been
codified in the web ontology language or
owl and for anyone wondering why that
acronym is out of order it's a reference
to the character of owl in a a Milnes
books Winnie the Pooh because he could
read and write and would write his name
WOL an example of excessive rules can be
seen when declaring rules for
transitivity I've worked with logic
systems where every transitive property
had to be declared as another rule like
this here we've got four different rules
each for each of the properties sibling
ancestor prerequisite and contains
they're all transitive and these rules
describe that for us it works fine but
that same pattern is being repeated over
and over and ideally we want to find a
way to extract that commonality now one
approach is to declare each of those
predicates to have the type our
transitive property and write a rule
that generalizes for that property since
systems like RDF and diatomic allow for
namespaces on properties like this or on
entities then nagas supports it even
though you wouldn't normally see that in
traditional data log or Prolog now note
that those type assertions on each
property are axioms and not rules you
might also notice that the property P in
that rule at the bottom is a variable
which is the second-order nature of this
now the important thing to note is that
now we just have one rule rather than
the four rules we had earlier now that
has a nice effect of being more
declarative about the predicate
properties
rather than describing the process of
transitivity each time and also it's
more efficient
au is a very large and complex system
but you know there's some low-hanging
fruit that it that can be provided in
rural systems some of the examples of
this arch transitivity symmetric
properties reflexive properties
functional and inverse properties
property chains at the bottom there
require a little more effort to encode
since they require lists of properties
but they're extremely useful as they
allow more general descriptions quite
often the more common case of what we'll
see in in rules like the uncle rule that
I showed earlier
now Naga has a few other features that
go beyond typical data log it allows for
scalar value comparisons such as
comparing the ages of individuals the
built-in database that Naga has also
allows calls to external functions which
can be useful for processing scalar
values des Tomic also does that sort of
thing for instance no can also import
data into the graph from jason for
processing and export the final graph
back into jason that's an important
function for thread great at Cisco as
we're using the means to identify
additional information relevant to
security incidents and that's all
expressed as JSON information we assert
those extra annotations on the incidence
and in the end the whole Lots re-emitted
it as Jason finally Naga was actually
built for embedding in an application so
it was all API based rules can be built
directly and there are several macros
simplify the code around that sort of
thing
but that was really hard to present to
my colleagues and manage
and so I created a command-line tool to
show how a program can connect those
parts along with a parser for for the
data log style syntax I showed earlier
and that ended up being the principal
user tool it was just an example for how
the API might be used and to show simply
his rules coming in that we can modify
it easily by just updating the Brule
program and we can see it popping out
and despite the fact that the the
command line program was simply designed
for these demonstration purposes it's
ended up being the the principle way
that people are interacting with it at
the moment but the API is still the
focus of development so at this point I
wanted to provide a demonstration of how
this might look so the simple program
that I was showing early earlier is this
here audio it's not showing up on the
screen because I need to mirror my
display so I apologize and that would be
like this
sorry so this is a simple program that I
showed earlier and if we want to run
this in Prolog we can load it up easily
and composable and then we can ask for
instance for an uncle
everything that satisfies the uncle
query and we see Fred - George and
Barney - George just as I was explaining
earlier nagas is in the current
directory so I'll just be using line
again unfortunately there's some
overhead of startup but we can run Naga
over the same program and because this
was built for demonstration purposes it
prints out the input data and it prints
out everything that was really was
asserted into the database that didn't
exist in there previously that includes
both of those uncle statements that we
saw coming out of Prolog plus the
brother and parent which Prolog can also
find but we would have to explicitly ask
for those statements so that shows that
Margot is able to run exactly the same
kind of code but we've also got this
example where we work with Jason so if I
have a look at some equivalent data to
the assertions that were provided
earlier sorry that should be we can see
those same entities I've used an ID
attribute on each of them to identify
them I unfortunately because of the ID
now we need to consider how that would
look in the rules and we need extra
statements now to connect the ID to the
entities in
in Jason because unless sir Jason is
embedded
there's no way to refer across to other
Jason entities now if we have a look in
the data directory you'll see I have to
peel files which are programs and that
input that we have and I just wanted to
do that so that people could see that I
wasn't cheating and the output will go
into a file that I'll call out Jason and
the program to run his family again
we've got the lining in startup time and
now we're going to cat it but if I Jason
if we have a look at this we'll see the
very similar data to what we had earlier
only now the Fred entity has been
annotated with an uncle and the let's
see the George entity has been annotated
with apparent the Mary entity has been
annotated with a brother and if we
consider a different program now where I
look at the fact that symmetric so the
sibling is a symmetric property you can
see that I have a rule now at the bottom
for handling symmetric properties with a
second order expression and we can do
exactly the same kind of thing
and if I look at at that output will see
that you'll see Fred still has a sibling
Barney but now Barney here also has a
sibling Fred so just a slight
modification to the rules and we can we
can get these this new data coming
through but it's sucking in Jace and it
all goes into the graph database all
comes back out
so now that has a few more plan features
the first one that I need to get in is
aggregates this connecting to graph
databases is done through a protocol I
did specifically aim at datalink because
this is the graph database I've been
using most for the last few years but I
have specialized a lot in RDF so I keep
these compatibility issues in mind so in
a lot of cases aggregation is just going
to be an extra function which I have to
figure out you know the best way to
connect that through to the appropriate
database we need an integration to a
message queue because not the output of
every rule need not necessarily be
assertions into the database again we
want to provide for node instantiation
and to so when a rule is met we might
want to create a new node and have
multiple properties on it that will be
very useful but we need to be careful
because that can lead to infinite loops
now the example I showed earlier showed
that JSON can be difficult to reference
in rules because of the need to refer to
which property another JSON entity has
so we're also developing a syntax to
make that more intuitive so we can
declare ID for instance to be identifier
on on jason edit ease and we can just
refer to them directly and not need
extra extra statements in our rules and
right now nagas principally uses an
in-memory database that i've implemented
here I also have a working prototype
that talks a de Tomic and I hope to make
that public soon on github I also want
to build adapters for sparkle from the
foj and orient DB most importantly the
program needs documentation especially
around the api's
so while we're using it at threat greed
and Cisco Nagas intentionally
general-purpose it's not tied to our
systems at all and I hope that it'll be
useful for others this is the project
URL and my contact details so thank you
very much everyone
are there any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>