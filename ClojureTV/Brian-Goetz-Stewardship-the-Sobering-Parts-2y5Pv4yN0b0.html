<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Brian Goetz - Stewardship: the Sobering Parts | Coder Coacher - Coaching Coders</title><meta content="Brian Goetz - Stewardship: the Sobering Parts - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Brian Goetz - Stewardship: the Sobering Parts</b></h2><h5 class="post__date">2014-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2y5Pv4yN0b0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I've been watching the evolution of
cloture for quite a while and I've been
thoroughly impressed with the
consistency and clarity of thought that
rich has brought to the goal of making
software easier to build maintain and
understand and as Stuart said this is a
goal that I've also dedicated quite a
bit of time and effort to and who knows
if either of us will be successful but
certainly I hope one of us is because
the state of software development today
well leave some room for improvement
now rich has taken an approach that is
advocated by the modern Greek writer
nikos kazantzakis who once said you have
your brushes and your colors so go paint
paradise and then in you go and this is
definitely a fun route because you build
exactly what you want and if you're
lucky you gather a like you know
community of like-minded people around
you as you go like this community here
and I've chosen a slightly different
approach a more incremental approach
trying to pivot a larger existing
community towards a better way to
program which is definitely slower going
but it also has a lot of leverage it's
the way I look at it is if I can help
nine million developers become even
slightly bigger better programmers this
really adds up to quite a big difference
so for purposes of today's talk maybe
you should pretend or maybe you already
do that my title isn't Java language
architect but instead this and today I'm
going to talk a little bit about my
experience piloting Java over the last
few years and some ideas about where it
might go next and some lessons that I've
learned for the journey so far as Stuart
mentioned a few years ago I wrote a book
on writing correct concurrent programs
in Java which is something that most
people at the time thought was either
trivial or impossible and the key
message of the book was that both of
these views are woefully wrong I
remember at one point rich made an
offhand comment that he created closure
because he was tired of telling people
to go read my book so in the spirit of
Al Gore having invented the internet
I'll just take a tiny little bit of
credit for the inspiration behind
closure
so I want to talk about pragmatism for a
minute pragmatism is generally
considered a good trait in engineers so
how many people here in this room
consider themselves pragmatists warning
it's a trick question okay so some
people feel off the trick question all
right so let's dig into what that means
so I understand the local custom is to
ramble on about Dichter dictionary
definitions for a while so what Webster
says that pragmatism means a reasonable
and logical way of doing things our
thinking about problems that is based on
dealing with specific situations instead
of ideas and theories or more succinctly
a practical approach to problems and
affairs so well that sounds pretty good
who could be against having a practical
approach to problems and affairs right
so okay let's dig a little bit more the
American philosopher Charles Saunders
purse sometimes called the father of
pragmatism stated what is now called the
pragmatic Maxim in 1878 which is
consider what effects that might
conceivably have practical bearings we
conceive the object of our conception to
have then our conception of these
effects is the whole of our conception
of the object okay that's a little wordy
and I know not all of you have
philosophy backgrounds so here comes the
TLDR version in the voice of well if
maybe not the words of a better known
philosopher there's are no good there is
only good for so what Master Yoda means
here is that we need to look past our
own aesthetic and principal beliefs and
judge things on the basis of whether
they work in the real world so we should
judge the good of something based not on
its intrinsic properties but on what it
is good for so some of you at this point
are probably feeling a little bit
tricked by my trick question of who here
considers themselves a pragmatist but
too late to change your mind
we're already we're already moving let's
see where it goes so one of the things
that you can learn from the history of
programming languages is that there's no
there's no perfect programming language
now this should be kind of obvious if
you think about it because even if there
was the sorts of problems we saw
Computers change over time and if the
problem is changing then the tool that
might have been perfect for one set of
problems is unlikely to be perfect for
tomorrow's problems so I prefer to take
a more evolutionary view of programming
languages and tools and and so the
analogy goes like this imagine an
ecosystem where the programming problems
are the prey the programming languages
and the tool or the Predators and the
state of computing hardware and
infrastructure is the environment so oK
we've got an ecosystem now we let
evolution to its thing and we watch how
you know the denizens of this
environment is going to change over time
so if we view this through the pragmatic
perspective we should judge the
programming languages aren't good and
bad in and of themselves but instead
that language acts may be good for
certain kinds of problems and may not be
good for other sorts of problems and if
that's a useful way to talk about things
then maybe we're better off rather than
talking about good or bad talking about
languages as being evolutionarily fit or
not for the environment that they happen
to find themselves in and you know
continuing the analogy some creatures
that are evolutionarily fit for one
environment managed to adapt and change
as the environment changes and others
don't and they die out so I think this
is a pretty useful way to look at
programming languages now evolutionarily
fit doesn't mean pretty it doesn't
necessarily go the other way either but
there there are certainly different
kinds of ugly there are some things that
are ugly because their ugliness actually
serves their evolutionary mission I mean
think about the anteater or the naked
mole-rat right those things are hideous
but they're very evolutionarily fit for
their niche and then there are things
that are not ugly but they're more like
warts
they don't serve the evolutionary
mission but they don't interfere with it
either and you know in this ecosystem
mutations aren't really random they're
deliberate decisions made by designers
and so with benefit of hindsight we
might be able to judge certain decisions
or features as being errors within their
evolutionary context so I went looking
for a really good example of this
programming language feature that ran
completely counter to the languages true
Design Center and so I went looking for
example I I
I first tried the wisdom of crowds
approach so I asked my Twitter followers
to name the most bizarre language
feature that they could come up with and
I tried to be explicit about the
criteria I wanted bizarreness relative
to the languages principles rather than
just the personal taste of the the
reader you can imagine about how well
this worked out and so I was inundated
with participation age blather like oh I
couldn't stood that so many cool ones
are required here in this language you
know and stuff like that but what I was
really looking for was an example of
where a language designer had lost their
way where they had deliberately gone
back on their principles to illustrate
just how easy it is for short-term
expediency to lead you into making a bad
decision
so after Twitter proved useless I you
know and answering the question that I
actually wanted the answer to I asked
some system smarter people and of course
I got exactly the answer the example
that I wanted from guy Steele and there
there's there's a lot of lessons in this
but I'm going to move on so at the risk
of sounding like a 70 year old Gary
Bernhard I don't know if Gary's here see
here okay let's talk about COBOL okay
quiz time how many people here think
that COBOL sucks is a programming
language he always fall for they raise
your hand track how many of you have
written a program in COBOL Wow I'm
impressed with that okay so now remember
we're being pragmatist right so we
shouldn't say COBOL sucks as a language
we should say whether it is good for or
sucks for certain kinds of problems that
might be called upon to solve and in
this evolutionary metaphor COBOL was
actually quite a fit predator indeed for
the programming problems of the 1960s
and this makes sense if you think about
it you know the database technology the
time involved storing data on punch
cards or magnetic tape that emulated
punch cards and you know the most common
programming problems of the day we're
producing accounting statements and
query reports like you know sales by
salesman and region and things like that
and so a language built on imposing weak
types on 80-column strings and doing
that to a bunch of records that have
been read in off some of yo device and
doing so
simple aggregation and spinning out some
suitably formatted summary records was
kind of just what the doctor ordered in
1960 while we're at it I could note the
Fortran was similarly evolutionarily fit
for the programming problems in the
1950s which were largely dominated by
things by the production of artillery
firing tables and things like that so
these languages that we like to look
back on and maybe poke some fun on were
actually you know quite evolutionarily
fit predators for the ecosystem of their
day so a little bit of a history lesson
here so the imperative part of a COBOL
program was divided into paragraphs so
here we see a paragraph labeled foo para
which does his body does some simple
computation and control flow was
accomplished by the usual suspects you
know you had vaguely stack based
subroutine calls and unconditional jumps
to name targets and this isn't beautiful
but it was what we had and you could
write reasonable enough programs with it
so ok here comes the bizarre part as if
talking about COBOL and 2014 wasn't
bizarre enough the alter statement does
anybody know what the alter statement in
COBOL does few language historians here
yes so the alter statement is cobol's
self-modifying code feature so this
statement means change the jump target
of the paragraph called foo para to
branch to baz para instead and of course
to make that make sense it had to be the
case that foo para referred to a
paragraph that had only one statement
and that was a go-to so my claim with no
evidence whatsoever was that this was
COBOL at moment okay so we don't really
care exactly
when COBOL jump jumped the shark but
stuff like this actually keeps me up at
night because Cobo really a COBOL was a
serious evolutionarily fit predator you
know for solving serious business
problems in 1960 it had a reasonable set
of organizing principles which might
look primitive by today's standards but
they were the basis for an entire
industry that really did accomplish
amazing things and then we discover
stuff like this and what's scary about
this is some serious people with serious
responsibilities thought this was a good
idea and the lesson is smart people can
do some really dumb things when they're
under pressure and some
times it's very hard to tell the
Brillion compromises from the complete
sellout of your principles until way too
late and I'd rather not be judged by
history as the doofus who did this to
Java so stuff like this keeps me up at
night so okay let's talk about Java now
Java may not be the prettiest thing but
it still is the dominant predator in
today's programming ecosystem and I know
you guys have a bit of a love-hate
relationship with Java on the one hand
you like and need the JVM and all the
great services it provides and on the
other hand the JVM wouldn't exist
without the popularity of the Java
language and more precariously the JVM s
ongoing development is coupled to the
ongoing development of the Java language
which might be a little bit scary for
some of you so my little story about
COBOL jumping the shark was really a
long winded way of getting into the main
theme of my talk today which is
stewardship so what does this mean
a steward is somebody who has
responsibility for maintaining property
entrusted to them by its owner and this
is the word that I use when I describe
how we approach evolving the Java
platform so coming back to our
evolutionary view stewardship means
ensuring the Vltava language and the JVM
remain suited to the combination of
programming problems and programming
environments that we're going to find
today and tomorrow which may be
different from how what they were in
1995 some people ask me why do I even
bother
with such an old technology I mean I
could be working on something much newer
and cooler right and it does take a
tremendous amount of effort to move
something as big as Java so why bother
and as I said earlier there's a simple
answer to that that's leverage having
your code running on a billion machines
is leverage teaching 9 million
developers to program even a little bit
better has a huge aggregate effect on
the world of programming more than
anything else I can imagine wouldn't it
be great to teach 9 million developers
that say side effects are overrated
where also you get that chance right so
that's why I do what I do one of the
biggest questions that any community is
going to grapple with is change how much
how fast the world changes
we have to change with it otherwise we
risk becoming irrelevant but we also
need to evolve carefully because you
know in you know the context of our
community even the smallest change you
know could break someone's
mission-critical code so there are
forces that push us in both directions
as forces that push us towards more
change keeping up with the changing
Hardware reality business problems
developer demographics and even
developer fashion and also of course
trying to fix some of the inevitable you
know mistakes that we've made in the
past on the other hand there are some
forces that resist change we have a very
low tolerance for making incompatible
changes and we don't want to buy into
fashion of the week because that risks
alienating the loyal users who kind of
got us where we are right and you know
they may like things reasonably well as
things are and and there's always the
possibility that we could end up making
things worse rather than making them
better so these are forces that resist
change and just like in politics you
know the folks at the wheel are probably
going to be hated from both sides right
because one you know what one group says
you're not going far enough and the
other says you're going too far so
ignoring the fact that someone's going
to hate you no matter what you do any
change you make is going to receive
extensive scrutiny and second-guessing
and this sucks but it's better than the
alternative right I mean there's an old
saying there's two kinds of programming
languages the ones people complain about
and the ones no one uses so you know one
option if you're you know allergic to
being complained that is just do nothing
throw up your hand say oh it's too hard
to do anything and you can get away with
that for a long time but this is kind of
the cowardly route right so instead what
I think you know you should do is adapt
to the forces in play so I posit that
the rational counter strategy in such a
situation is to put more eggs in fewer
baskets the consequences of even a small
change are surprisingly expensive to
estimate and certainly easy to
underestimate you're probably better off
investing in bigger changes that have
more of a chance of a positive
cost-benefit ratio so this is actually
something people in a Java community
find incredibly frustrating because we
seem to be unwilling to make seemingly
minor changes or fix obvious mistakes
the reality is you know
with millions of users and billions of
lines an outstanding code out there
there's no such thing as a minor change
and nothing is obvious at some point I
realize that there's kind of a
disconnect between what people think I
do and what I actually do I was chatting
with one of the researchers at Oracle
Labs and he made this comment that he's
always surprised to hear me describe my
work so I made some charts to illustrate
the disconnect so in his view my job
would mostly be about technical aspects
of language design it's a nice view
don't take this away from him you know
you've got type theory and soundness
proofs and adding features so we can
express more kinds of programs safely
and sensibly and make the intent of the
program more clear and totally
reasonable academic view and probably
does accurately describe the ongoing
development of Haskell or ml and I mean
don't don't get me wrong these things
aren't part of my job - but what threw
him is that what I actually talked about
are the software concepts that have more
to do with user psychology than about
type theory so you know I'm constantly
saying things like that's not where I
want to spend my complexity budget right
I have a limited budget for users
adapting to new ideas and I want to
spend it where they're going to get the
most payback or we might discard an idea
on the basis purely of cultural
constraints as well that's just not how
Java developers think about it now and
you know these are concepts that come
you know if you're coming from an
academic programming language background
you might think about so you know one
day he said to me it's like all right
finally I get it you're not Enzo Ferrari
you're Henry Ford Ferrari is all about
building the best fanciest coolest car
you could imagine right and if adding a
new type of turbocharger or massage
chair makes the car better then that's
what you do but Henry Ford set out to
build practical cars for ordinary people
so forget the turbocharger in the
massage chair but if you can improve the
gas mileage or make the dashboard
provide more useful information that's a
big win you know the Honda Civic is a
pretty fit predator in its ecosystem -
even if it's not as pretty as a Ferrari
so this charts a little bit less
charitable and therefore a lot more fun
and this is what I sometimes think
people in my community imagine that I do
based on what they say I mean if we go
clockwise from from 12 o'clock you know
you got so you got your secret backroom
meetings every once in a while you poke
your head out of the secret back
backroom and make some self-important
comments on a mailing list right and
then maybe you come to a conference and
make some more self-important comments
and from a technical perspective it's
mostly about stealing features from
other languages and of course lots and
lots of syntax bike sheds and you know
it's fun to make fun of your users but
that it's certainly therapeutic but it
it's also important to remember that
these are the people I'm doing this work
for right
so in some sense success looks like not
bursting their bubble about how it works
and you know just making it work better
anyway and you know james gosling
described java as a blue-collar language
it's a tool for real programmers to get
their work done and there may be plenty
of rocket science that goes into
designing it or implementing it and
certainly specifying it but you
shouldn't have to be a rocket scientist
to use it now this turns out to be kind
of a difficult constraint and to make it
even harder a wide-spectrum language
really needs to have wide spectrum users
otherwise you're going to lose sight of
where the mainstream is going next year
so while java may primarily have its
home in server-side business logic it
really can't afford to alienate the more
cutting-edge users like the high
frequency trading guys all right so this
one looks a little bit closer to what it
feels like I actually do so you know you
definitely do have you know the meetings
and interactions on mailing lists and
conferences but the technical focus is
really different it's not as much about
adding features as is about keeping
features out you know if you look at
some academic languages or some non
academic languages that I won't name but
whose name might be an abbreviation for
scalable language it seems like the goal
is at every feature you can write but
that's generally the last thing you want
to do every feature add some VAT weight
every feature might interact not only
with every other feature you have
but every other feature you might want
to add in the next 20 years so every
feature is one step closer to collapsing
under your own the weight of your own
complexity like being purl so we have to
choose very very carefully and when you
do add features there's a tremendous
amount of work to fit them in with the
existing features and to analyze and
test for interactions with existing
features and not just existing ones the
future ones as well and analyzing the
compatibility consequence of everything
and of course no feature analysis is
complete without discovering just how
badly it interacts with serialization
which is my favorite feature so the
really fun part is this one down here at
the bottom right that's what would James
do I get to do that every once in a
while so as you can see this is a
somewhat more conservative view of
language evolution that's also the price
to success so I'd spend an awful lot of
time thinking about compatibility I mean
so much so that I spend time thinking
about how to think about compatibility
because it comes in many forms you know
you've got your binary compatibility
which is about whether call sites
embedded in existing class files will
continue to link you've got source
compatibility which is about whether
clients or subclasses will continue to
compile you've got behavioral
compatibility which means things
continue to work in the same way you've
got civilization compatibility which is
about whether instances serialized on an
older version or even a newer version
will continue to deserialize properly
and then there's a whole bunch of more
subtle compatibility issues such as
changes in performance characteristics
to make matters worse there's actually a
cross-cutting concern that's fundamental
to all this analysis which is separate
compilation because of dynamic linkage
every call site is linked and runtime
nominally and this is why you're able to
just drop a new jar on the class path
and keep going you always have to deal
with the possibility the only part of
the system's been recompiled and
normally this isn't a problem but this
can show up when for example you change
your code generation strategy or when
the generated code for a given class
might depend on information from other
classes this shows up with in Java with
bridge methods all the time so you know
separate compilation can really you know
actually torpedo and otherwise
worthwhile
language feature now this isn't to say
that we never violate any of these kinds
of compatibility because after all you
can't fix a bug without breaking
behavioral compatibility and sometimes
there are bugs we don't fix for this
reason but you know sometimes after
sufficiently quantifying the amount of
you know code we might break we might
consider whether it's the right thing to
do but not lightly so a lot of people
think that we're kind of crazy to be so
rabid about compatibility but really
there isn't any other responsible choice
because if you have billions of lines of
code out there pretty much any
incompatible code change is going to
break someone's mission-critical code
and it takes a particular brand of
arrogance to say yeah you should break
his code it's for his own good
we do get these suggestions a lot and
I'm sure these people think they're
being helpful rather than arrogant but
the mistake is they're only focusing on
part of the picture so what I see is
that developers tend to overestimate the
importance of code and underestimate the
importance of stable programs and it's
not too surprising right to developers
look at code all day so of course that's
where their aesthetic Center is but you
have to keep in mind that's only part of
the picture and there's a little there's
a lot more to it than that so just
because you're constrained by
compatibility doesn't mean it's
impossible to evolve it just means that
you you know the set of sensible
evolutionary directions you know is a
little bit constrained so I'll take a
simple example from the the java five-oh
days when we add an auto boxing to the
language so auto boxing is an implicit
conversion between a primitive and its
box reference form nice little feature
let the compiler figure out the
conversion mechanics for you great ok
but now we've got a problem with
overload selection because prior to Java
5 if you had two methods and you had
this method call only one of them was
going to be applicable and the overload
was really resolved so that's great but
once you add an implicit conversion from
into integer now this call sight is
ambiguous because both methods are
applicable so if we didn't do anything
else I'm adding this you know autoboxing
would have introduced a source and
compatibility because call sites that
previously used two are uniquely
resolved become ambiguous and won't
compile
and this is the sort of thing that's
really easy to miss when you're focusing
on a localized view of the code rather
than a global view of the codebase so
seemingly harmless things like a little
implicit type conversion always is going
to ripple into method overload selection
and type inference and all that and so
overload selection has a lot of detail
but but the concept is straightforward
enough you know you take the the type of
the arguments you figure out what those
are you search the class for applicable
overloads and of those you pick the most
specific and hope there's just one
so what auto boxing did was increase the
set of applicable methods so to resolve
the conflict we sort of turn to an odd
trade-off which is we trade
compatibility for more complexity so to
preserve compatibility we say let's
adjust the overload resolution by
proceeding in steps where the first step
is do the old thing and if the old thing
fails then you proceed you know to the
later steps and so we invented this
concept of strictly applicable versus
loosely applicable we're strictly
applicable is the old rule and you do
that first and you know if that wins
you're done and otherwise you then move
on to the second step and we did this
trick again in Java eight with the
inheritance semantics when we added the
ability to inherit behavior from
interfaces whenever there's a conflict
between inheriting from a superclass and
a super interface we say the superclass
is going to win and we do this because
this guarantee is that any code that
works prior to Java eight works exactly
the same way after Java eight might not
be what you would pick if you had a
blank sheet of paper to work with but
it's a reasonable choice that balances
the various constraints so the trick
here basically is you focus your
evolutionary effort on imparting
semantics to programs that previously
didn't have any semantics and then you
define all the features this interacts
with to give priority to the old
interpretation there's a pretty good
trick as long as you use it sparingly I
mean if we went out of control with this
we could have a 93 step overload
resolution algorithm that wouldn't be so
good but if you evolved conservatively
this trick generally works well enough
so there's a subtle irony to this
accusation that we're obsessed with
compatibility and that we should just
you know
the guts to make an incompatible change
and I've been looking for a way for
quite a while to illustrate this irony
and the best I've come up with so far so
this picture is Central Park now by some
estimates the real-estate value of
Central Park is about half a trillion
dollars this represents a dramatically
unstable equilibrium in real estate
development right leaving half a
trillion dollars of real estate
deliberately undeveloped doesn't happen
by accident and it doesn't stay that way
by accident so the rightmost border of
the park is 59th Street and as you can
probably imagine property on 59th Street
is pretty darn valuable I'm sure it's
occurred to every single real estate
developer ever that wouldn't it be
really cool if I could build a building
on 60th Street and of course you know
the coolest of this to being able to
break the sanctity of this we don't
build here rule relies on being the only
one to be allowed to break that sanctity
otherwise you know our would be clever
developer would be very surprised and
dismayed when you know his
brother-in-law built a building on 61st
Street so the value of Central Park and
the dependent value of park front
property exists solely because there's a
collective discipline of we don't build
here and a confidence that this
discipline will be continued in the
future once that confidence is is in
doubt the whole system comes crashing
down and the only reason it even occurs
to someone to build there is that it's
there no one's been allowed to do it
before okay so here's the strained
analogy Java remains the world's most
popular programming platform precisely
because we never have thrown our
customers under the bus like this by
breaking their code class files you
compiled in 1995 still work today and
they'll still work in 2035 breaking
changes are like that building on 60th
Street
it may seem expedient in the short run
but there's no such thing as just one
exception and the only reason people
even think it's a good idea is we've
never done it so they never seen the
consequences of what happens so people
continue to trust Java but because they
trust that we're not going to devalue
their investment and you only get one
chance to break that trust so the
bizarre consequence is the more
discipline you have the more you're
going to get ridiculed for that
discipline I guess discipline has to be
its own reward like my dad always said
so here's a story from the evolution of
Java that illustrates how the
cost-benefit equations shift when you
take into account you know existing
developers in code in the most recent
version of Java we have we finally added
lambda expressions now some of you might
find that laughable should should we
wait a minute we need to break okay but
this actually illustrates a lot of the
challenges of stewarding a mature
platform so the concept of lambda
expression op is you know obviously
straightforward enough the hard part is
always how do you interpret the notion
of lambdas in such a way that fits in to
what's been done before and so it
doesn't look nailed on the side doesn't
blow your complexity budget doesn't
break existing code doesn't require that
9 million users can't do their job until
they go back for more training and in a
way that allows users to gradually
migrate their code from the old way to
the new way without having to do it all
at once so what starts out as a pretty
simple seeming problem turns out to be
riddled with a lot of soft constraints
that have to be carefully balanced and
these are all various forms of
compatibility not just compatibility
with code but compatibility with
existing mental models or attitudes or
skill levels now in a statically typed
language when you add a new expression
the obvious question is what's the type
of that expression so we have to decide
what the static type of a lambda is and
there's an obvious answer to this
question a function type after all
nearly every language that has lambdas
and has types has a notion of a function
type starting with the lambda calculus
so it turns out that this answer is
obvious and wrong when you start pulling
on the function type string you discover
it raises a lot more questions than it
solves in the context of Java and I have
a whole talk on this issue so I'll just
give you the highlights when you design
a language feature you don't only get to
design the part of the part where it
meets the programmer you also have to
design where it meets the compilation
target and users might claim oh I only
care about the source code I just want
to be able to write the code I want to
write and they say that you know until
it doesn't work the way they want it to
right so if you don't have a clean way
to express the code in terms of the
underlying computing substrate you're
going to have performance potholes and
complexities in the corner cases and
then the users will complain
so if you say the type of a lambda is a
function type you need a way to express
that in the JVM and that raises many
questions like what what a type
signature look like in a class file
what would the bytecode be for
invocation do we need a new bytecode for
that how do we create instances of
functions do we need a new bike code for
that how do we map variants at the java
type system to the VM now the JVM
doesn't have any native way to represent
function types so we'd have to invent
one which we could we can add new type
signatures and new byte codes and new
verification rules which of course is
expensive or we could simulate them with
generics which are erased which would
suck so we could have these boxed erased
function types which is not what anybody
wants so and the bottom line here is if
you have a gap between the way the
language represents a concept and the
way the underlying representation in
byte code or or or whatever you compile
to this always causes problems so
instead you know we backed off from me
what would be the perfect way to do this
perspective and double down on
well how does Java do this already which
turns out there was a good answer
lurking there we looked at how we
abstract over behavior in Java without
function types and the way you do that
is you write an interface with one
method like runnable or comparator or
action listener and you use that as the
argument type of a sum method right so
you've all seen code like this right if
you ignore the syntactic pain of inter
classes which obviously you can't ignore
this works reasonably well the parameter
has an obvious type it can represent
primitives without boxing and references
without a ratio so that's good so you
could call these things nominal function
types so in the end we decided let's
just formalize the the idiom by giving
it a name so one method interfaces are
now called
functional interfaces and we define a
conversion from a lambda expression to a
compatible functional interface type and
then we teach type inference to
propagate you know information from the
signature of the single method into the
lambda expression you know which is a
Toa
not totally obvious from a type theory
perspective but in hindsight should have
been the obvious thing to do just by
looking at the code that Java developers
actually write so the bottom code is a
lot better than the top
we're pretty happy with the results
there was a lot of work that went into
making type inference work this way a
lot of work under the under the hood so
doing this you know saved a huge chunk
of our complexity budget so instead of
introducing function types it's the type
system which users would have to learn
about and we have to specify and
implement them and the compiler in the
JVM instead we just gave a name to an
idiom that everyone already knew and
this enabled us to spend the complexity
budget on better type inference and you
know which was where like I said I'm
almost all the work went into
overhauling the interaction between type
inference and overload selection which
are features that are under the water
line right most Java developers don't
even know that they exist until they get
get a compiler so there were a lot of
obvious but wrong choices that presented
themselves during this evolutionary
exercise and I don't really have a lot
of concrete advice for how to identify
obvious from wrong from you know uh
obvious and maybe right except to be
aware that they come along pretty
frequently and you have to be open to
these subtle warning signs that suggest
that the obvious choice might not be the
one you want and of course no matter how
much thought you put into it you could
still be wrong right I mean the best
best that language and library designers
can do is to move slowly and
deliberately enough to minimize the
impact decisions that might be seen as
mistakes in the future but we can still
be wrong you know Java is alter
statement there's only one bad decision
away so it turned out that this decision
to embrace nominal function types rather
than structural ones paid this huge
dividend that we didn't see from the
outset so pretend for a second that
adding function types to Java turned out
to be really easy so we just rushed down
that path without thinking about it then
we pat ourselves on the back for having
made Java so much better right now see
what the ecosystem does what happens
when we release this new and improved
Java well remember one of the assets of
the ecosystem is the huge variety of
libraries out there if we introduced a
better way to represent behavior in the
type system we immediately relegate all
the existing libraries to doing it the
old way and now begins the 5 to 10 year
churning process where library
maintainer x' have to spend their effort
maintaining both old way and new way
libraries at the same time and migrate
from old way to new way and such and
suffer dependencies where you can't
migrate because your dependencies
and migrated what a waste of time right
these libraries are fine and by thanking
an idiom that existing libraries already
used to model behavior we achieve this
very rare component kind of
compatibility which is forward
compatibility with language features
that didn't exist at the time the code
was written so all those old libraries
that used this common idiom were already
lambda ready from the outset so because
doing real function types turned out to
be hard we kind of dodged a bullet in
terms of the adoption and migration
curve which would have imposed you know
billions of dollars of unnecessary cost
onto the ecosystem the choice not to do
structural function types illustrates
another lesson worth pointing out if you
don't know the right answer at least
don't do the wrong thing and that sounds
really easy to say but it's actually
really hard to do we didn't know how to
do structural function types in a way
that met all our goals expressiveness
and performance and complexity and
migration compatibility so rather than
press forward anyway which was tempting
we had the discipline to leave it alone
and we did make a number of subtle
decisions that left the door open to
doing it in the future such as
explicitly disclaiming any meaning to
the identity of the result of lambda
conversion as it turns out we think we
figured out how to get there in the
future and we're pretty glad that we
didn't do it wrong when we were under so
much pressure at the recent java one in
san francisco james gosling told a
similar story about why generics weren't
included in java from the beginning it
wasn't because parametric polymorphism
wasn't understood in 1995 it wasn't
because we thought it was a good idea to
have a static type system that was so
weak that you could put typed elements
into a container and not retain their
types when they came out it wasn't you
know it was because we didn't know how
to do it right some people might argue
we still don't but I think we're pretty
glad that we didn't get C++ templates in
1995 or or patterns from data so it took
longer than we'd hoped but the generics
we got were a lot closer to the spirit
of the language and the generics we
would have gotten in 95 weren't so if
you don't know the right thing to do at
the very least don't do the wrong thing
and sometimes that means doing nothing
and that's okay it has to be okay
the lesson here is reality hardens like
concrete before you do anything there's
many possible potential paths ahead once
you do something
there are many fewer maybe there's one
so it's true for little things as true
as well as for big things always be
aware of what doors you're closing
implicitly with every decision you make
so okay just in case you've not seen
what Java eight code looks like let me
just give you a quick peek here's a
quick example you know it's a typical
filter Map Reduce operation on a
collection okay big deal Java finally
caught up with idioms that Python and
Ruby and JavaScript have had for many
many years well there is a big deal here
and the big deal is how we've integrated
into the existing libraries it's not
just bolted on it's not just skin deep
we've integrated parallelism into the
model Java always had good libraries for
parallelism but the sequential and
parallel expression of a computation
always look very different and that
meant to go parallel you had to rewrite
your code and retest your code and not
anymore
now you can just say take the stream
make it a parallel stream and the
sequential and parallel code look almost
identical so there's a lot of power
beneath the surface here and that's
where the power belongs here in this
example people is an ordinary collection
so all the collection implementations
were retrofitted to support
decomposability so that all collections
can be sources for parallel streams
similarly all the stream operations like
filter and map work off a general
abstraction for element access and
decomposition so anybody's collections
or other data sources like generator
functions or queries can act as sources
for stream pipelines and it might look
like if you're used to reading java code
this looks like three distinct
operations filtering mapping and
reduction but the library is lazy so the
operations get fused into a single pass
on the data so and then as a sort of
last little bonus the implementation
tracks where the data came from and what
his characteristics are so if your
source would say at reset and you
filtered it and asked to sort the
elements it would say oh I know the
elements already sorted so I'll just do
nothing there so not only we caught up
with some other languages you know other
curly-brace languages in expressiveness
we really blow them away in performance
now the imperative version of filter
MapReduce isn't
terribly ugly either right but more
complicated Java code tends to get ugly
fast because there's so much accidental
detail that leaks out right so you know
here's an example of your typical
idiomatic Java code that does a query on
a collection doesn't really matter what
it does the important part is you have
to wade through too much detail to
figure out what it does so if you you
know rewrite this with the stream
library in Java 8 it actually looks like
the problem statement and as you know
code that looks like the problem
statement is more likely to be right now
I know I don't have to sell this
audience on any of the benefits of this
approach my actual point here is it's
possible to teach a 20 year old language
some surprising new tricks you just have
to be willing to work within the
constraints so Java 8 was a big upgrade
probably the biggest ever to the java
programming model we had coordinated
improvements to the language vm and
libraries subjectively four years ago I
think the dominant attitude and the
community seemed to be you know Java is
done for it's the new COBOL it's had its
day today I'm seeing a very different
attitude there's a lot of excitement in
the Java community about this new and
improved Java so coming off a success
like this it's about time for the second
system effect to kick in right so what
are we doing next right we're just going
to sit on our laurels no of course not
we're going to do lots of cool things so
to start with I think we've come to a
much more sophisticated view of
coevolution between the language and the
JVM now don't worry
Jade that the JVM is going to become
increasingly beholden to the Java
language in fact the opposite is true
but in the job of five days when we
added generics it wasn't an option to do
it in the VM so everything had to be
done with front-end compiler
transformations and this was doable but
you know it had some of those impedance
mismatches that I talked about earlier
that result in problems like bridge
methods that poked their heads up every
once in a while so you know since then
there's been a lot of consolidation in
the JVM market it's basically just
Oracle in IBM in the server JVM market
which makes it a lot more practical to
evolve the JVM if there's reason to do
so it's a lot easier to get two big
companies to agree on something than 12
so our approach
for evolving the platform is a practical
one we try to find the biggest pain
points and hammer on those and for Java
eight the pain points are mostly
language driven improving the ability to
abstract over behavior but pain points
are sometimes Hardware driven too and
you know we want Java de and the JVM to
perform well on basically the moving
target of modern hardware for whatever
that means at a given day so for any
given feature we often have a choice of
where do we implement it do we do in the
library in the front-end compiler or the
VM now language development is slower
and more expensive than library
development VM development even more so
but there's often a right place to put
put a feature and when it comes to
hardware driven features very often the
only choice is in the VM so language VM
coevolution means finding the balance
between these two opposing forces on the
one hand we want to keep Java language
complexity out of the VM on the other
hand we want the mapping between source
code and bytecode to be straightforward
and to be free of impedance mismatches
so the Java 100 design leaned really
hard towards the ladder Java 100 is
basically an assembler for the JVM
instruction set but the cost of this was
we burned way too much about the java
language and the Java object model into
the into byte codes like invoke virtual
that understood the Java semantics of
method inheritance and linkage so over
the years we've been hammering at places
where the VM instruction set is to Java
specific like the invokedynamic
bytecode we added in seven and this
allows non Java compilers to efficiently
encode language specific semantics in
the VM in a way that the VM can see
through and therefore optimize now Java
5 leaned a little too hard in the other
direction by doing everything in the
front-end compiler which increased the
semantic gap between the language and
the bytecode and that created complexity
and corner cases so that might be too
little VM involvement but we also could
have made the mistake of going in the
other direction
in fact one of the persistent calls for
fixing something that's broken about
Java is that we should reify generics
now when pressed most developers can't
tell us exactly why they hate erasure so
much if you ask them the usual answer is
some permutation of kizuite sucks but
a naive interpretation of refined
generics would mean pushing the generic
type system including wildcards into the
JVM so how many of you here think it's a
great would be a great idea if the
bytecode generated by the closure
compiler not only had to satisfy the
java generic type system but you also
had to pay for the potentially
undecidable generic subtyping checks had
run time on every array store good idea
right yeah thought so
so the winning move here of course is to
pull the rabbit out of your hat right
you have to factor a feature into a set
of sensible language agnostic VM
primitives that can be used by a
front-end compiler so in Java eight we
had a nice illustration of this remember
the bit about converting lambda
expressions to one method interfaces so
you need a way to take a lambda body and
it's carried arguments and convert it to
an instance of a compatible interface so
we could of course just had the compiler
spend classes for this and that's what
the prototype actually did but instead
what we did was build on invoke dynamic
to expose a VM optimizable
implementation of lambda conversion and
we stuck this in the platform runtime
with a good specification so any
compiler could use it and in fact Scala
will be using it in 212 for their
lambdas which means any optimization
work we do in the core platform accrues
to their code as well without any
changes or well theoretically without
having to recompile your code but that's
not really an option for Scala
developers most of the time but we like
it okay so I talked about tracking
changes in the hardware hardware
certainly has evolved a lot over 25
years you know in the in the ancient
days he had these single core single
socket systems with no cash no
instruction level parallelism no
pipelining and in these days an
arithmetic operation and a memory fetch
cost about the same thing so the
performance model was really simple
basically you counted instructions and
over time systems got more complicated
we added cash we added multiple levels
of cash we added multiple cores on a
chip we added lots of cores on a chip we
added multiple sockets and you know
these systems got more sophisticated and
as a result memory access became more
expensive and less predictable so memory
access had cost you as little as a few
cycles or as much as a few hundred
cycles depending on where the data is
served out of and at the same time cores
got fancier and more complicated with
deep pipelines and superscalar execution
Oshin so if a cache miss cost you 300
clock cycles and you can issue four
instructions per cycle a cache miss may
cost you a thousand arithmetic
operations that's obviously a worst case
but that's a big deal
so it stands to reason that whatever
trade-offs we made in 1995 of that
laying out data in memory can't possibly
be optimal for today's systems so what
does the JVM type system give you you've
got 8 primitive types
you've got heterogeneous aggregates with
identity classes and homogeneous
aggregates with identity arrays now this
is super flexible right you can model
anything you want with it and we do but
the big problem is it's hard to get away
from object identity unless you can fit
your data in perfectly in one of those
eight primitive data types now what is
identity for it exists to serve
mutability and polymorphism not all
objects need this but all objects pay
for it and you might think that the EM
might just figure it out I mean JVMs
have gotten really good at slicing
through the cost of code abstraction but
they never really got very good at
slicing through data abstraction because
the VM can't really figure out when an
object is going to need its identity
unless it can see all the way from the
allocation to where it goes out of scope
and so the VM has to be pessimistic and
how it lays out data so imagine you've
got you know a simple tuple class like
point got an X and a wifey oldham let's
say it's immutable and final so it can't
be polymorphic and it doesn't really
need object identity but you get fries
with your order whether you want to them
or not so you got an array of these
things how does it get laid out well
array has an object header and each
element to the array is a reference to a
point object which has its own block of
memory and its own object header and
from a memory efficiency perspective
we're not doing so well right we have
two words of header plus a pointer for
two words of data so that's a big
overhead and because these objects are
heap allocated that's work for the
allocator and the garbage collector but
the real cost it comes from the in
directions of all those pointers and if
you've got an array of plain-old in s--
you get great locality when you traverse
the array because you pull in one end
and it pulls like the next 7 or 15 into
l1 with it and then like when you go
read the second one it's already pulling
in the next block of them because it's
predicting what you're going to do and
that works really well on the other
and if you've got an array of objects
like in this picture you get pretty
decent locality fetching the pointers
but then you're probably going to cache
miss on every one of those key
references so remember that thousand X
penalty for cash missing oops and it's
not even a deterministic penalty right
instead so you pay in performance and
you pay and predictability now sometimes
this doesn't matter you might have it
small data set you may you know meet
your performance requirements with
plenty of despair but you know if you're
doing high frequency trading or big data
this cost counts a lot and you didn't
ask for this layout it's just you know
what you really rather have is this
layout right so you know when confronted
with a situation like this developers
often take matters into their own hands
and sometimes they have to and sometimes
they just can't help themselves so
developers will do silly things like you
know shredding a an array of points into
two arrays of primitives hoping to
regain the locality that they lost an
object layout and of course this codes
harder to read harder to write harder to
maintain more error-prone and the sad
part is developers often do stupid
things like this for performance reasons
with no actual evidence that their code
doesn't meet its performance
requirements or in fact without even
having performance requirements or
metrics in hand but you know regardless
of whether developers do this out of
genuine need or some destructive
obsessive-compulsive disorder wait it
doesn't really matter right we have to
stop putting developers in a position
where they have to choose between
getting good abstraction and getting
good performance so this brings us to a
vm feature I think you're going to like
value types now I know I don't have to
sell this crowd on the value of values
but having the EM support for values
would be really nice and if we recast
our point class as a value what we're
saying is we don't care about identity
and all the stuff that that brings out
with it and by giving up on that bit of
flexibility we gain the ability for the
VM to treat an aggregate like an XY
point as a value so values are
implicitly immutable and final and they
derive the quality from their state
rather than our identity and they can be
passed well by value in registers or on
the stack instead of in heap nodes and
you know they can also be flattened into
arrays and
and inlined into into other objects so
values could actually have most of the
things that classes can have they can
have methods and constructors and fields
and their methods can be public or
private and they can have you use
generics and because values or
monomorphic there's no dispatch cost for
invoking the method of the value type
they act just like static methods so
basically our mantra here for value
types is codes like a class behaves like
an int so the applications for this are
obvious right numerix tuples algebraic
data types like optional STL style
cursors instead of mutable iterators
like Java has and because value types
can encapsulate data just like reference
classes values can act as secure but
cost-free wrappers around sensitive data
such as internal references or native
pointers and they mediate access to the
data and provide type safety without
adding an extra level of cost so you get
safety encapsulation and performance
free lunch whew
so since this affects data layout it
means it has to be a VM feature first
and a language feature second but the
language support for it will be a pretty
straightforward mapping to the M feature
the way we like it so I bet that non
Java languages will actually benefit
from this way more than we will because
non Java languages have to express their
internal data structures using Java and
so they're paying all the penalties of
this kind of layout so even if none of
you ever directly program in Java with
this you're all going to benefit from it
tremendously so when you look at
generics with box primitives you see all
the same layout problems that you see
with arrays in efficient data layout too
much indirection loss of locality too
much allocations and so if we're adding
value types we have to look at how does
this interact with you know with
existing features like generics so if
you have an ArrayList you know of
integers it's going to be represented by
an object that's going to point to an
array each array is going to point to a
boxed integer yuck so from a Java
language perspective once you add values
you immediately see that the uneasy
truce we made in 2004 where you can
genera fly over references but not
primitive starts to fall apart so when
there when there are only eight
primitive types we were kind of willing
to look
the other way but once you add user
definable primitives it really has to
work with generics so we're going to
have to bite the bullet and support
generics over primitives and values so
you can say list of int instead of list
of integer and have under the covers
have it actually point to an array
events this will be accomplished by
on-the-fly bytecode transformation so
you get deep specialization like this
and again if you're not Java programmers
you may not be as interested in the
details but you're going to be very
interested in the mechanism I think
Gaudi talked about this a little bit on
Thursday so recall earlier when I was
talking about looking for language
agnostic building blocks that the VM can
provide the compilers and one of the
interesting building blocks is the
ability to describe a class not by name
but by recipe so at the bytecode level
this ArrayList of in class will actually
be written in some encoding of the class
resulting from applying the
specialization transform to an ArrayList
with argument T equal int and once you
have that mechanism for describing a
class by pattern instantiation instead
of by name this trick can immediately be
used for lots of other things
proxy classes tuple classes adapter
classes etc and again you know this is
something that's a compiler writers
dream write things that you would have
to generate statically or play games
with class loaders you can generate
actually within the VM class definition
model so in case anybody's interested in
getting involved these features are all
being developed in open JDK under two
projects valhalla and Panama so the
first is about value types and
specialized generics like I've talked
about the latter is about better Interop
with native code and access to native
data on the native heap and together
these are sort of a comprehensive story
for more flexible and efficient access
to data in memory better data layout
direct access to data in native memory
more flexible arrays and unified safe
and fast api for exotic access to
variables like fenced and atomic access
whether they're stored in fields or
arrays or even off heap so these are all
what JVM features first a language
feature second and most of these will
probably find some use in enclosure to
improve footprint and performance so
it's a really cool time to be involved
in JVM evolution
so okay it's getting to be time to wrap
up so I'll just close a little bit of a
warning be careful what you asked for we
all want the software we right to be
successful but success brings with it
some cost and constraints that can be
really frustrating at times on the other
hand these constraints most notably
keeping the promises that you made to
your users whether you made them
implicitly or explicitly forces you to
spend time upfront thinking about
whether something is the right thing to
do the best use of your resources what
the possible consequences and the
possible interactions might be and most
of the time thinking deeply about what
you're going to do ahead of time really
pays off even if it means you can't rush
forward as quickly as you might
otherwise want to and I think these are
issues that the cloture community may
soon be facing if you're lucky so suit
up and Oh almost forgot the pony
thank you everybody for listening
so do we have to have questions I'm
happy to do questions yeah take we'll
take five minutes for questions right
there so the question is what's my idea
about Scala versus Java 8 so I think
Scala is great Scala is able to do
experiments that we don't have the
luxury of doing and they're going to do
some things that are successful and some
things that are unsuccessful and we can
learn from both of those things and
there's a lot that we can learn from the
experiment Scala have done and then we
can steal the good bits so I think it's
great to have that you know have them
niihka system I do think a lot of Java
developers who left Java because it was
too painful are now taking a backwards
look and saying hey maybe it's getting
better over there and we are seeing a
lot of a lot of big sites move from Java
to Scala and are moving back but ya know
I think it's great and I think it's it's
very interesting to see some of the
consequences of the decision they're
making and that may have looked like
obvious choices at the time and then in
hindsight may look like they were
mistakes and it's good for someone else
to be making those mistakes for us I
won't predict the future the question is
will Scala survive I don't know I hope
it does I don't know other questions
over here yeah so the question is okay
he laid the decision not to push
generics into the JVM in 19 in 2004
which was really the the time we had a
chance to do that and having sort of
missed that boat can that ever be
revisited or there are other things that
we can do is that basically your
question so you know we spent a lot of
time thinking about is it practical to
do reified generics and the conclusion
that we mostly come to is well I think
people think they want it for the wrong
reasons the compatibility costs are
devastating I mean if you look at what
c-sharp went through they only were able
to get away with it because they did it
at a time before they had any userÃ­s
which is great if you have that chance
but you know if you miss that boat
you're you've missed it the pain points
of erasure I think can be solved without
reification so one of the biggest pain
points for example is it's really hard
to write serialization and
deserialization libraries that you know
convert objects to xml and back but a
lot of that pain can be addressed by
better type
girls without having to go all the way
down to reify in generics in the VM so I
think there are easier paths to getting
there that don't involve upsetting the
whole world alright a couple more
questions
this guy had his hand up before go ahead
yeah you the question is can you think
of a decision that you wish you could
have made the other way and I'll give
you a good example this wasn't a
decision that I made personally but I
know the people who made this decision
to feel this way about it so when we add
an auto boxing to Java and Java five we
defined equality by identity rather than
by value that seemed like the compatible
thing to do at the time it turned out to
be a dumb thing to do we would have all
been way better off if we defined
equality unboxed boxed integers by value
instead of buy it by reference but it
like I think it was one of those things
where it just seemed like an obvious
choice and it wasn't hard so we went
with it and then after the fact realized
you know that we had kind of painted
ourselves into a corner it's not a
dramatic example but it's it's I think
the people involved all agree that
should have gone the other way on that
one over there so the question is you
were dumping on civilization before so
obviously there were some big mistakes
made with civilization what do we learn
from that it's not your question yeah so
civilizations are really tough one
because on the one hand as a language
feature it made every possible mistake
in the book right
it wasn't integrated into the object
model it was this weird thing nailed on
to the side it is horribly insecure it's
horribly non transparent it uses magic
methods it violates basically every
principle of object-oriented design and
yet I think if it hadn't been for
civilization which is the basis for easy
remoting Java might not have been where
it is today because of the success of
Java I think had a lot to do in you know
in the late 90s with how easy it was to
build distributed object frameworks and
so on the one hand I really regret
everything about serialization on the
other hand I think the Java would be in
a
dramatically different and worse
position now had we not done it that way
so I think the idea is okay but the
magic Nezu
magic nosov it isn't and I sort of
understand the rationale for a lot of
the decisions some of the decisions were
just boneheaded and were rammed through
under time pressure but it's um I think
the attempt to be magic was the big
mistake all right I think I have time
for one more question so the question is
people are much more aware of functional
programming as a paradigm than they were
in the past and is the JVM evolving
towards that so I think the answer is
yes that but the way I look at it is the
choice between object-oriented
programming and functional programming
is not an either/or right
object-oriented programming is one way
of modeling the world it's fairly good
at modeling abstraction over data
functional programming is another way of
modeling the world and is very good at
abstracting over behavior and most
programs have both so the way I like to
look at it is you should learn classical
object-oriented programming and you
should learn classical functional
programming and then you should think
about how am I going to use these
concepts to help me write the program in
front of me better drawing from both of
those and I think to answer your
question I think there are a lot of a
lot of pieces that the VM could pick up
fairly easily you know one of the
constant calls for is tail recursion it
turns out that the reason the JVM
doesn't do tail recursion is for very
stupid reason and it's only one that
we've recently been able to you know to
move a step forward in so it turned out
in the implementation of the JDK classes
not for any good reason but because this
is how the world is there are a number
of security sensitive methods that rely
on counting stack frames between library
to JDK library code and the calling code
to figure out who's calling them which
is a horrible brittle you know way to do
it but you know it was an expedient
thing to do in 1995 and we recently
replaced that with a mechanism to be a
lot more a lot more robust and to get
rid of the nasty frame counting stuff
but the
was that made any kind of tail recursion
no go because it meant anything that
mucked with the number of frames between
the nominal caller and the nominal
destination was a security you know
error so we actually in the last like
two or three years have removed what is
actually the biggest impediment to doing
tail recursion to the M it's on the
radar it's not the highest priority but
it will eventually get done all right
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>