<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Feynman Machine: AI architecture in Clojure/Neanderthal - Fergal Byrne | Coder Coacher - Coaching Coders</title><meta content="The Feynman Machine: AI architecture in Clojure/Neanderthal - Fergal Byrne - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Feynman Machine: AI architecture in Clojure/Neanderthal - Fergal Byrne</b></h2><h5 class="post__date">2017-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/um2uq5oURT8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and so this project that we're working
on is joint work between myself and
Louise and then we just decided this and
just for this kind of first introduction
that we'd just I do basic talking for
this one so and so they had a fine line
machine what is this most people not too
sure so just pop quiz and how many
people have heard of a fist is called
Fineman okay how'd you keep your hands
up keep your hands up now keep your
hands up if you can answer this question
one person no no okay the answer is joan
violent okay and actually in her own
race a very eminent physicist and she
was richard little brother did a sister
and then she had decided to become a
scientist like our bigger brother when
she was woken up at five o'clock that's
at the age of five and and they snuck
out and to a golf course i think near
their home and richard showed her
something like this obviously not
exactly the same and she was fascinated
immediately and she decided become an
astrophysicist and find out exactly how
these things worked and she's still
alive she's 90 years old this year
and she just retired from the Jet
Propulsion Laboratory in 2002 I think it
is so in between them in 1963 and
Richard Feynman gave one of the first
lectures and as part of his famous
series about chaos and the emergence of
chaos in real systems he said this so
just the red words are probably most
important things so this is obviously
really love with several of these words
and so the complexities of things can
escape it's and latias equations
and perhaps nothing short of God is
required to explain the complexities of
the world and the example he gave
possibly inspired by and his sister he
was studying this one he described a
simplified son which was the only thing
that worked at the time the real son
along with all its structure and
sunspots and everything and are really
in the equations we just haven't found
out the way to get the mesh so fast
forward with 15 or 20 years the late 70s
and a discovery was made about chaotic
systems and this was that there is a way
to get things out of thee not out of the
equations but out of the system itself
and what you see on the left is a
dynamical system which is the kind of
thing that he was talking about a
chaotic system and on the right is
something that's mechanically
reconstructed from a signal coming out
of the system on the left and this was
turned into a theorem which tells you
exactly how to do it and what mechanism
to use and exactly when it works and it
turns out it works almost everywhere you
look this is the same system that we'll
see now and both John's final unused and
also the final machine uses and
basically you can ignore the details of
mathematics this is what it means it
means that unlike what Richard Feynman
was talking about you don't actually
need to understand the laws of motion
the equations the differential equations
in order to build a model of a system
you can just get it from the data and
that's what that mathematics basically
says so about ten years after that was
discovered John one of John Simon's most
major papers and with about the
long-term dynamics of sunspots on the
Sun and she was analyzing data like this
and she sends some breakthrough
information based on analyzing this
using that type of mathematics
and down here at the bottom is this
Tarkin theorem which is at the end of
the paper and that's also the basis at
the Fineman machine okay so I'm just
going to give you a very quick overview
of the idea of the final machine the
principle of us and we're going to talk
just very briefly about our current
software on which the rest this is based
and we're going to talk about some
choices for bringing closure into the
equation and just a very quick overview
of Neandertal which is part of the
project and then they should hopefully
be some time to see some code and
possibly some downs
okay so dictionary of just made up
afternoon and a final machine is a
network of intercommunicating nonlinear
dynamical systems which basically means
is in clockwork machines which can adapt
to predict their future inputs it's also
a theory describing how the brain
operates at the level of brain areas and
it's a brain inspired architecture for
machine learning so now just a word of
warning there is some serious graphics
going to appear next rich Hickey would
slip if you saw this and this is it just
look at the title that's the translation
of the Japanese on the last right okay
so I thought this is the best version of
the of the picture and this is a partial
map it's a landmark paper from 1991 of
the visual cortex of the macaque monkey
ours is actually about twice as complex
as this as far as we know em and this
might even be worse this is just the
connection map and as now in 2008 is
probably twice as much as this and I've
just the frontal part of your brain okay
so that's just a roof a kind of
underground map of that so obviously if
you're going to try and get inspiration
from how the brain actually works you're
not going to start with something like
this
so in traditional Clojure fashion and of
man has essentially reduced it down to
this so a and this is actually single
brightest it's really good I'll show you
exactly hello
how that happens in a second but and at
any one time the information is just
travelling along one path through this
hierarchy so this is a multi-level and
it's a spatio-temporal predictive auto
encoder and the technical terms and so
essentially there's a number of stages
so down at the bottom is where you
connected the world and what it does is
it takes in a stream of inputs and it
encodes them on the way up and then it
passes them across at the right hand
side and it decodes them into
predictions so at each stage predictions
are being made at by quat be going on
the left is going to see next and down
at the bottom every time step you get a
prediction of what you're going to see
on the on the input side so now this is
the scary part for me so this is and so
this is live Figg wheel powered quill
that is not supposed to happen here with
me a second here was not close before
sorry
okay and okay sorry
live dammit that was actually working
perfectly a second ago
alright so I'm gonna have to skip that
to the end am I trying get it in if we
get time and what I say is the that's
what you should tell us what you do when
you put that at the beginning anyway
right okay
and so if we go back here and what I was
going to show you is just a quick
demonstration of heavy and how each
encoder works for anybody who's familiar
with have feed-forward neural networks
work it's broadly similar but the big
there are some big differences so for
example we there is no back propagation
in this it's all run off local
prediction errors and which makes it
dramatically more efficient and as well
as that we use a whole lot of different
optimizations and so unfortunately I
can't give you a live demo test which is
just mk2 for the moments I think we have
time at the end okay so what was I just
move on to the next part basically as I
was saying the inputs come in which are
either just pure sensory inputs or they
can also be a copy of the currents and
actions that the that the system has
been taking and in order to generate
those inputs and at each stage there is
a an encoding of that and that's passed
up on the left-hand side to the top and
then it's passed across to the decoder
at the bottom at the at the top and it
generates a prediction of what's going
to happen at encoder n at the next step
M and then that's passed down and
combined in the lower levels with the
encoding coming over from the left so
the decoder gets two pieces of
information something that about what's
just happened and something that's
usually longer range longer term and
that allows it to make it more it's
basically combining information from
there's information basically combined
at every point from that originating
from all over the network and and
and then at the end it produces a
prediction of what's going to happen
next and it uses the principles of
Tarkan's theorem to effectively each of
these things and the whole thing
together at each of the levels and each
of the encoder and decoder are all
nonlinear dynamical systems that are
really abstractions of brain areas and
the communicate with each other using
sparse binary vectors which you would
have seen ten minutes ago and which are
very efficient ways of encoding
information and and they allow you to do
things like for example and distribute
these things across multiple computers
and stuff like that and it's very
efficient in it we suffer no degradation
in performance and because all you're
doing is essentially and you have a
network of large mutable objects that
are sending each other and immutable as
sparse binary vectors which can be
compressed very easily okay so that's
the kind of architecture of the final
machine and I'm just very briefly going
to talk they do two technologies that
we've built and one of them augment EO
is from and at this time last year we
released a Annie augment EO is our new
version and which I'll show you a
picture of a demo for kids I don't trust
any of this
I don't trust the internet anymore and
Annie Eggman era was actually only
publicly released 13 days ago so and and
this is really it so both of them are
essentially the same architectures I
showed you in the in the diagram earlier
and augment EO runs essentially large
networks and massively parallel
architectures like GPUs even OpenCL
Annie odd Mineo is a pure C++ library
which just runs on your CPU but it's
optimized for a low powered hardware so
for example about three weeks ago you
can get this on YouTube therefore just
search for raspberry pi or just all my
raspberry PI's at my desk and you'll see
a video which I'm not going to
trying even Jonah of my colleague Eric
and that's a self-driving car and the
self-driving car learns how to drive on
a raspberry pie on the car and nobody's
ever even tried that to our knowledge
before nevermind succeeded so he's now
working on one that works on a Raspberry
Pi zero which is the thing you get free
on the on the front of a magazine for I
think it's like three four dollars on my
best and I think we get that working
sometime the next week or two okay so
the idea is is that we're looking for
something that's as powerful as the way
that the brain learns and something
that's as efficient as the way that the
brain learns doesn't require millions of
examples you can train this thing to
solve the driving task by just and
giving an example driving in about two
or three minutes okay okay so we have a
number of choices to build a closure
version of this technology and
essentially where this project is is
trying all of them so the first version
of us is to build a wrapper for the
existing libraries so we already noticed
some serious engineering going into both
of those and and essentially it's just a
matter of engineering and then putting
an appropriate closure interface on the
front of it so this is really how that
works so we have of Mineo and el demonio
as they wear and they come already with
swig bindings that generate a rudder for
both java j'ni interface and and the
work that we've done essentially is just
to create those wrappers so and pardon
me now hopefully this will work
okay so so this is the wrapper for a
Armenia and I don't know if you can read
up to some light on the screen and but
basically this is just a set of pure
functions that operators are a set of
functions that basically translate
maps and vectors and traditional closure
data structures into the appropriate
setters and getters for for these
mutable Java objects so and this one
including a demo program so that's an
entire demo program there about 30 lines
but the wrapper for eaat Menil is 93
lines
sorry no yep 1985 lines I think and I
think step is the last one there okay
and then this is the wrapper for sorry
this is the wrapper for and odd Minea
and it's also Abed 85 95 lines so you
can see it's doing something similar and
and all its really doing is just
translating from the closure data
iterating over it and then using these
setters and getters and now I'm just
making sure that the closure that you
write is a lot more idiomatic so I'm not
expecting you to read all of this most
of that is actually just setting up data
and I'm then calling these various
helper functions and then running the
hierarchy inside the loop I think it's
actually one two three nine one two
three is the air this one here it's the
one that actually runs the runs the
network and I just basically pulls out
and just measures errors and stuff i
delegate the prediction so the idea is
basically that you can treat the thing
as if it was as if it was some hidden
and data structure and you just pass in
closure closure stuff and get closure
stuff back out of it okay
and the other strategy then is to
reimplementation e of Minea using the
undertow so obviously this is a bit more
ambitious and we're in the early stages
of this at the moment and but
essentially what we want to do is we
want to exploit the power of Neandertal
which is about as fast as you can get
any closure code to ruin when you're
talking about large-scale linear algebra
and and keep the kind of idiomatic
closure interface but have the closure
going as far down as possible with it
before it starts sacrificing performance
okay so I'm just going to give you a
preacher I don't know how many people
are familiar with in the undertow there
was a talk last year but it's from
dragon he's a super genius but I highly
recommend if you have any kind of heavy
numerical work to do and you want to use
closure this is really really I mean
it's beautiful website loads of
documentation for all the libraries
everything just works out of the box as
long as you get it set up correctly
there's a few things which he warns a
bit about the Intel and libraries but
once you get over that and it's happy
days it really is fantastic and so we're
concentrating really on kind of
reimplemented the algorithms in such a
way that we can reason about them and
change them on the fly and and we're
going to be doing that in the undertow
so the demo that you would have seen how
that's been working actually is running
live algorithms it's not just a cartoon
animation that's actually just possibly
whether it crashed the browser and what
is actually running live algorithms and
from within closure script from within
quill and so we're looking for obviously
some more robust with Neandertal so this
is really a quick sketch so this might
seem familiar as kind of an architecture
on the left that talks to OpenCL and the
architecture on the right that talks to
CPU library and and but he's obviously
done an awful lot of engineering like I
really highly recommend take
taking a look at this and okay so
progress so far we have working wrappers
for both a odd Mineo which let me have
two weeks and four of Minea and and
we're currently building some demos
using those so and we're going to be
connecting up disk we'll stuff to us and
we're in the which we were 100% sure
that Neandertal will work and what we're
really trying to do is trying to
identify how to get the algorithms into
such a state that you can and you can
talk to them through pure data and get
the underlying system to change how the
algorithms work because we want to do
and we want to use this as a test bench
for developing new versions of the
algorithms okay and trying to see where
we are in time okay we're good okay some
takeaways and first takeaway obviously
and is that if you're interested in
machine learning and for some serious
purpose not just find out about it
because this stuff is actually and has
quite an entry level because it's it's
essentially it's a combination of hard
neuroscience hard applied Maps and
machine learning and they're three skill
sets that don't don't often intersect
but we've tried to package things up in
such a way that's traditional and
machine learning people can get access
to it either through C++ the poison
bindings are actually really good and
and hopefully in the next month or two
we'll have the the whole cloud your
system working and and in particular if
you want to visualize and experiment
with variations on the algorithms and
then contribute back to those and we'd
be delighted to do that because
everybody's problem may have slight
tweaks to the way that the thing works
and but it's it should be a very low
cost experience in the last time I
downloaded these things it was literally
just 20 minutes and you'd be up and
running you'd be able to do demo
programs and start writing your own ones
it's actually quite a good experience
and so if you have a machine learning
problem and you can't get any
traditional approach to work and your
stuff has to do with things changing
over time your data has something to do
with change things changing over time
this is certainly something worth taking
a look at and if you do have such a
problem and our technology solves it
then and we really really like to talk
to you about how that would work
okay so and and then particularly for
heavy-duty numerical works and I really
recommend taking a look at Neandertal
even it hasn't to do with machine
learning it's a really good idea okay so
hey let me just see if I can get this
thing up and running one second
bear with me a moment I'll see if this
sorry I'm no figlio expert it's not the
right thing to connect into or pardon
and I'll try not know this be a Sigma 0
one second
okay very good
okay so and this is actually based on
that thing we've added a kind of state
machine onto the top of quill so each of
these phases that you see in the top
left corner is just going to show a
different state so it's it's like quill
in that you generate a state I mean you
get one pure functions to up data and
then you draw it but actually each phase
if this passes on the state It's Made on
to the next phase and it's actually
quite interesting so this is a neural
network so that on the left is an input
layer that's just getting some data and
on the right is what we call the hidden
layer which is and which is going to be
connected to it so we do that by
connecting up a large set of weights
that connect each of the elements on the
left to each of the elements on the
right and you can see I think the D and
the hidden layer now is updating in
response to the inputs okay so that's
done what a big this is where you need
linear algebra software you have a big
white would like these are tiny vectors
so you have a big white matrix on the
left multiplied by the
the input vector in the middle and you
get the output vector on the right so
yeah one of the things that everybody
does in neural networks is they impose a
non-linearity on the output so they
don't just take that because that's just
a matrix transformation and you can just
and multiply all those together so you
have to put in a non-linearity and one
of the problems with that is that the
traditional deep learning people and
they need everything to be
differentiable because they have to do
back propagation but if you don't have
to do back propagation and you just use
some local prediction error then you can
do things like this which is just a
winner-takes-all so I think if you see
the histogram just beside the hidden
vector that shows you the values and the
blue dots are basically the top five of
the hidden layers values okay so you get
it the output vector is a big
and a big factor that's nearly all zeros
and only a tiny number of ones and the
tiny number of ones is called a
winner-take-all algorithm or a case for
us or the algorithm and this has huge
advantages if you don't have the
backpropagate if you do have to back
propagate you can't do this and but this
is how the brain works because uses
spikes and and so there's a whole load
of information theoretic properties of
spiking neural networks that you can't
do and you can't do a continuous real
valued variables okay so that's how a
very very schematic view about how an
encoder works and the decoder works in a
very similar fashion except you just
turn it around the opposite direction
and it generates a prediction given and
given and an encoding so it just goes it
just operates in the opposite direction
and sometimes we actually just use the
same weight matrix and just turn it
around okay right so and that's the end
of this just very quick link sorry the
didn't happen in the middle but okay so
the apologies for the roundabout version
of that and
okay so thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>