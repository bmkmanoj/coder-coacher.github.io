<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure Dispatches From a National Research Lab | Coder Coacher - Coaching Coders</title><meta content="Clojure Dispatches From a National Research Lab - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clojure Dispatches From a National Research Lab</b></h2><h5 class="post__date">2017-03-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RB65-zYLNSY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello my name is Jennifer and today I'm
bringing you some closure dispatches
from a national research lab the goal of
this talk is that by the end of this you
will understand how mountains and a pile
of dice are all related to closure there
might be a couple other takeaways as
well so before we get started I wanted
to give an overview of what kind of talk
this is this is an experience report I'm
gonna kind of outline a problem that we
had just to solve and how we use closure
and a variety of different closure
libraries to solve that problem and the
reason that I have dispatches in the
title is because I liked this definition
of dispatch as an official communication
sent by a special messenger that lets me
call myself a special messenger or
perhaps even a dispatcher which then I
get to use this awesome card from the
pandemic legacy board game which I'm a
huge fan so it was a win all around so
this is an overview of how this talk is
broken down I'm gonna give some
background about where I work what what
my background is and the problem that we
were trying to solve and then I'll go
through the approach that we took to
solve that problem and then I'll
summarize with some opinions about how
well that approach did or didn't work
there will be a liberal sprinkling of
opinions throughout the approach section
as well all right let's start with those
mountains so I work out at Sandia
National Laboratories we're one of many
national labs around the United States
and one of our goals is to work with
government agencies to solve problems
that are facing our nation as a whole
so Sandia itself has been around since
the late 1940s so it's been around a
pretty long time we have over 10,000
employees last time I checked I do not
work with all 10,000 of these people but
it's a big place we're doing a lot of
different kinds of research and
different kinds of projects so the name
Sandia is from the mountains that are in
Albuquerque so India is an Albuquerque
the main campus there's some other
campuses around the United States as
well but the reason that it's named
Sandia it's for these mountains and if
there are Spanish speakers in the
audience you'll know that Sandia is the
Spanish word for fruit and so not for
fruit for watermelon so we are not named
after
froot were named after these watermelon
mountains because that's the color they
get when the Sun sets on them okay so a
little bit about my background I started
working at Sandia in 1999 and if you do
the math that equals a very very long
time
but in that time I was starting to think
about the different technologies that
I've used in the languages I've gone
through but when it when I stepped back
and thought about it I've really spent
all that time just working with data and
the systems there that have to deal with
using data storing it getting it back
out and that's not all that interesting
of a problem I think we all do that and
some capacity but what I've really
enjoyed is getting to talk to users
about that data one of the cool things
about working at Sandia is that we have
a lot of subject matter experts that are
kind of at the top of their field and
they're doing really interesting work
and being able to talk to them about the
data and how they want to use it has
been a really cool experience so as I
mentioned we've been around for a really
long time we don't adopt new
technologies super fast sometimes so how
do you get closure into a National Lab
you bring in a Marc Bastian he's the one
he's responsible for kind of getting
closure going in our in our area and
this is a picture of him giving a talk
at the closure khanjan 2015 and one of
the really cool things about working at
Sandia is that I can work on a whole
bunch of different projects in my time
there I've worked with different groups
of people different kinds of data
different kinds of applications and so a
couple of years ago I started working
with Marc on a project that was written
in both closure and Scala and I had the
great fortune of getting to work on the
closure side of things so I did what you
do with any new project I started
looking through the code trying to
understand it fixing some bugs adding
some new features and I have to say that
the first time that I looked through
something and some code and realized
that that data was in a map and that
that's how that data was represented I
didn't have a class hierarchy I didn't
have this really strong typing I was
immediately hooked I was like this is
what I want to do and since then I've
never gone back I have the great fortune
of getting to pick some of the languages
that we use on our projects and closure
is my first choice for most of what
we're trying to do
this is my about me slide in case you
can't tell I like a lot of different
things I don't have a lot of focus it's
a little bit of a self-serving slide to
say hey I like all these things but if
you look down the right-hand side you'll
see an awful lot of tabletop
role-playing and board gaming references
and this is kind of fair warning that
those are going to keep showing up that
kind of fantasy thing will show up to
the rest of this talk so be warned and
prepared okay so that's the background
about me and about where I work let's
start talking about the problem that
we're trying to solve this is a snapshot
from the sandia gov website I don't know
if this person on this site realizes
that he's on a big screen right now but
I wanted to kind of hone in on the
research section of this website so
those are down the left but they're kind
of hard to see so I've pulled him out
here and if you look at the different
research areas and the different kinds
of problems that we're trying to solve
out at Sandia there's a pretty broad
range of data represented there
so traditionally in the past what we've
kind of seen happen is that we had
researchers working in a given field and
they needed data and they couldn't
really get it so a lot of our jobs are
software folks were to try to help them
get data and they could work on solving
problems with that well now we kind of
have data coming out of our ears so our
problem that started as shifted we have
to be able to figure out how to help
them deal with all of that data and the
other thing that we've seen recently is
that while traditionally you would have
a subject matter expert there kind of
working on their silo of data they're
like I have all my data in here and I'm
gonna work on this problem using this
kind of data and everything is good well
recently what we've started to see more
and more is that these researchers are
looking up and saying hey I've got my
data you've got your data your data
looks really cool well what can I do if
I take my data and your data and I start
bringing them together where it's
finding that there's this whole new
class of problems that we can solve when
we start enabling them to do that so
this sounds like a really good idea
right but what happens when this starts
to happen in practice so we have
the case that I've seen is somebody will
say oh I'd really like some of your data
and they're like right here you go and
they're like I don't know what this is
oh no how do you use this like oh it's
okay here let me find this parser that I
wrote and I'll give it to you and you're
like well how do I run it I don't know
so it kind of there's kind of sort of
issues around that and so then
inevitably we have someone step up and
say you know what we need we need a
universal email that will define all
data for all time and we'll never have
this problem again yeah that's not how
that goes we end up with another
standard that we have to support on top
of all these other standards that have
been around for a long time I've worked
with data was like that had formats
defined in like the late 60s things that
have been defined since the 90s and so
we have to support both those and
everything moving forward so this is
kind of a problematic approach here are
some of the other challenges that we see
a lot of times for the same kind of data
it's coming in from different places so
we don't have a lot of control about
where that data is generated and how
it's coming in to cut into Sandia
and so even data that theoretically
represents the same thing it looks
different so we end up building like all
these streams coming in and all these
little custom parsers on the front of
them and that doesn't scale up very well
at all a lot of times we're messing with
kind of emerging datasets and they have
these proprietary formats so again we it
doesn't scale as we keep trying to deal
with all of these things coming in we
get lots of versioning issues we get a
lot of duplication because we have
individual researchers pulling on that
data on their own kind of putting it in
their home directory and someone else is
doing the same thing so if they try to
share or collaborate it gets tricky my
absolute favorite is when I'm talking to
a scientist about this really awesome
data set they have like great we would
like to bring that into our system how
do we get at that and they say oh let me
go turn on the power on that server
under my desk okay can I get to that I
don't know we'll try okay I can see it
but I can't log in and then I get there
and I don't understand it and so this is
this has been a problem repeatedly the
thing that I have across the bottom
represent some of the other issues that
we see everybody has their own tools a
lot of times in their home directories a
lot of times written
and languages that aren't supported that
we can't get to run on something else so
we have a lot of issues around just
getting the data and understanding it
and being able to bring it into our
system now that I've kind of outlined
the problem I'm gonna go into our
approach to solving this problem we have
kind of a unique system in that we
aren't building a system that we need to
support 24/7 necessarily we're not
trying to turn a profit on this system
and we also need to be able to let the
researchers and the people who are close
to the data and know to know that data
hook into this system as well so we have
kind of this weird like we can't wall
off what we're working on from the
people that are using it they can
actually get in there a lot so this
presents some challenges as you might
imagine so I'm going to go through a
couple of aspects of this system that I
want to talk about I've highlighted here
this is just kind of a data flow diagram
I'm real big into pictures and here were
the dice start to show up we we chose to
use docker containers for reasons that
are kind of outside of the scope of this
talk that has to do with kind of the
systems that we're running on what I
wanted to talk about a little since
we're at a closure of conference is how
we're treating those containers so we
have containers and they have services
and this is not a new idea but what
we're trying to do is treat them as
functions the same way that you would
treat a closure function and this has
actually helped us to reason not only
about the code that we're writing and
closure but also about the services that
we're using to build up our system as a
whole okay so let's start walking
through this so if you can see that the
front door of this system is kind of the
next step down what we're going to talk
about right now is the data before it
gets into the system we have coming in
from different sources and we've given
users clients where they can put that
data into a map because I love putting
data into Maps so how are we going to
talk about that data confession super
duper love dice if I can use dice in the
slide I'm very very happy so I started
putting together slides that were
talking about what my data looks like I
had some Eden
and then it started to get kind of big
so what I wanted to do instead was take
a step back and think about data and
different types of data represented as
dice so if you'd like a little bit more
of a concrete example one of the
datasets that we deal with a lot is
seismic data so you could think about
one shape of dice as the streaming size
waveform data coming in off of a
seismometer that could be one type
another kind of data might be the
seismometer data itself like where is it
located on the earth what's the state of
health of that of that instrument so
those are some external types of data
you can think of another type of data as
things that we've developed in-house
like 3d earth models and the like okay
so what I'm trying to convey is that we
have a lot of different kinds of data
that we don't know all of the kinds
right off the bat and then we have to be
able to support all of those so in case
you're wondering how we do that we've
decided to go with closure spec to start
to talk about our data we've been
thinking about this system we work on a
lot of different projects so we think
about systems for a little while before
we start building them and so we were
talking about this you know for B last
year and they listen to the interview
with rich Hickey on the cognitive cast
last June I was like okay that is
totally what we're gonna do SPECT is
solving a lot of problems for us so the
examples that I've highlighted here are
a couple of ways that we're using spec
so we have we need to be able to have
common attributes that a lot of data has
in common and this is an example of a
latitude attribute that anything that
has to know that's position on the earth
can use and below that I have a station
latitude so for like a seismic station
and it can just use that same spec so
we've got some reuse already built up
but we have some specificity happening
as well which is really nice I just went
to the spec training on Wednesday I
think there's a better way to do this
I'm not looking at Alex but I kept this
in just because I wanted to keep the
example the other thing that we're
leveraging let me hold on before I go to
the next slide the other thing that I
wanted to say was that we have a lot of
information about what our data should
look like so we have a data shape and we
know that it's
have some of these things that we spec
out but the other thing that we have
that's kind of odd is that we have some
data that's kind of hitched along for
the ride as well so if you have like say
seismic data and it's coming from
different places there gonna be some
things in common it'll have some traits
that we expect out but there's some
other data that our users really don't
want to lose lose that we don't need to
care about but that we can't drop so
spec lets us talk about data that we do
care about in a very specific way but we
don't have to check the data that we're
not concerned with and we don't have to
lose it so this has been a really big
win for us so how do we support adding
new types of data to the system for that
we're using the multi spectrum spec
which is hard to say so multi specs are
kind of like multi methods and that you
can dispatch on a function and what
happens is as data comes into the system
it has to know what shape it is and then
based on that shape the multi method can
say okay you need to be validated
validated against this particular second
it can go find it that's nice what it
lets us do though is that we can start
adding new shapes to the system without
having to change anything the point of
entry to this is this multi spec and so
if we want to add more spec to the
system we don't have to change anything
and this is this is pretty cool because
we are like requirements kind of just
keep flowing in at soo can you bring in
this data can you bring in this data so
we need that flexibility had to be built
in from the beginning okay
so we have these clients where people
are putting where users are putting
their data in maps and we know that it
has to conform to a spec but we need to
actually this is my motion for like
wrapping up a present full of data we're
gonna have to send it into the system
some way and encode it so for that we're
using transit and I have a little
snippet of of code here these are all
just like teaser code they're not fully
working code but they're just
highlighting the points that I want to
make and so here's just an example of
creating a writer and then putting shape
data in that writer and then that will
package our data up but then we have to
actually send it into the system and for
that we're using another
library called HTTP GET this supports
transit and message pack it was also
fairly straightforward to use I'm gonna
pause here because this is kind of how
this talk is gonna go we I have a whole
bunch of different needs that that we
had to meet and we really wanted to to
meet those needs using existing
libraries and using closure so that we
didn't have to handwrite all of this
stuff ourselves because we needed to get
up and running as quickly as possible so
we can get researchers using that data
so we can find out how they were going
to use our system so we're doing kind of
a flyover of a whole bunch of different
libraries I'm just gonna talk about how
they're supporting what we're trying to
do okay so we have our data packaged up
it's time to send it in through the
front door this door does not lead to
the mines of moria that's what it looks
like it leads into the system and right
now we have a pretty simple front door
it's just um its composure API this is
something that we've used on a lot of
different projects so it's very natural
for us to put it in here I've heard
talks about other things at this
conference that I'm gonna investigate to
see if they might be better fits but
what I wanted to highlight here is that
it we pretty much just have something
that's receiving data checking it and if
that data is valid sending it to
wherever it needs to go in the system so
let's talk about the data validation
part so if users are sending you some
map data and you know that that data has
to be validated against a spec you'd
think that you might have to write a
bunch of code to have that happen in
reality we just call specs valid method
and it does all that checks all those
checks for us which is pretty awesome
like we didn't have to write any of that
validation code the other cool thing is
that if there is a problem if the data
isn't valid we can call explain data and
it will return a map of the errors that
were found in that and we can go through
that map it's just data and build up an
error string that's useful for the user
we didn't have to write any of that it
was just a huge one right away to be
able to just build on that so if the
data is valid we need to get it into the
system
and for that we're gonna use Kafka so
I'm just throwing a little teaser in
here about the Kafka components because
they integrate with composure API I'm
gonna continue on okay so next in our
system is Kafka the idea of using Kafka
is not like a revolutionary one I'm sure
a lot of people here are using it the
particular reason that we chose it is
that as I mentioned before we kind of
have to give users a hook into our
system so we needed a way that we could
have lots of different processes
operating on the same data so picture
data coming into the system and you have
a process that wants to grab it and just
write it don't do anything to it just
keep it you have another process that's
like well I want to do some like quality
checking on that data and I want to save
that you have another process that wants
to take that data funnel it somewhere
into some sort of processing pipeline so
by using Kafka we can have lots of
things pulling that data off and doing
different things on the on operating on
it in different ways it's not like the
first process that gets there gets the
data and nobody else gets it so we made
the decision to use Kafka and we were
able to just there are quite a few Kafka
libraries so we picked one and we were
able to just kind of get up and going
with Kafka relatively easily there's a
couple of Kafka isms at the beginning of
this end method of sending end we just
handed data as a map and that's how we
get our data routed into the system yay
I get to write some closure code so
right now all of the processes that are
pulling things off of our Kafka are very
creatively named writers because they're
going to write data so that we don't use
it in the future there could be any
number of things that are operating on
that we already have a researcher asking
if they can give us an algorithm that
they're developing that can like get at
that data as it's coming in and do some
processing on it so these are the
writers that we've written and we are
using component to deal with the
statefulness there it was kind of like
one of these moments and I gotta have to
keep track of stage and I don't feel
like that's right and then I watch
Stewart Sierra's talk and I thought okay
he's already thought about this he's
already kind of proposed a solution and
a design
pattern in his component library so we
adopted that and I just have an example
here of like a snippet of part of
creating a de Tomic component so this
component implements the lifecycle
protocol and you have to have a couple
of different functions I'm just showing
the start function here but the cool
thing here is that you create your
connection and you throw it in a map and
then component knows how to deal with
those kinds of things and so once you
have your component what do you do with
it in your system
we've throw it into a component map or a
system map of components this is kind of
a boring map it has one component but in
practice we have lots of different
components and the cool thing is that
your system map can kind of have
dependencies set up so you could stay
set up or you could specify how you want
what order you want those components to
be stood up in and so if you have one
thing that depends on another you can
specify that as well
and the ones those are in the system map
you call start and component we'll walk
through all of those components and
start them up so it's nice to have this
already thought about and already
developed that we could just leverage
and move forward so where are these
writers writing this data right now we
have a couple of different data bases we
have some stuff that needs to be written
to a file system we also have
requirements coming in for bringing in
more databases so right now we have one
writer per database that handles getting
things written to persistent storage and
we're currently using Mongo in DES Tomic
those are the ones I want to talk about
here so we had a customer tell us that
we had to use Mongo for her the
geospatial support that it offers so
that's why we're using it here and to
interact with Mongo we're using a
library named Munger it makes it very
easy to be closure with Mongo I did a
lot at the kind of the command line that
comes with Mongo and it was nice to be
able to think about my data it just says
Maps and I could put it in the database
that way so what it took to get us
started with that is just creating Mongo
collections that look like our shapes
that we know about and then we can just
move ahead with calling a pretty
straightforward insert function and and
it goes there's always logistics
databases but we're just gonna say
everything worked great day Tomic is all
the rage at this conference so what I'm
glad that we are using it as well
we are not leveraging the full
capabilities of diatomic at the moment
but we wanted to pull it into our system
right away to start getting familiar
with it because there's a case coming
down the line that we're gonna need to
support pretty soon and that is a case
where as data comes in it gets
manipulated and transformed by different
processes along the way so if you could
picture a date a piece of data that's
moving through a system an algorithm is
making a calculation and changing it and
it's putting it with some other data and
changing that and then you get kind of a
result at the end an analyst that's
looking at that result wants to know why
that result is what it is they need to
be able to say what was my data at this
point in time what changed it why did it
change it how did it change it this is
pretty important if you're kind of gonna
make decisions based on that so it turns
out date helmet kind of like already
does this for us so it was a cool thing
to be able to get this in right away
using date omics pretty easy once you
have your schema set up so that it looks
like you're you're shapes you you can go
ahead and just call transact with your
connection that you pulled out of a
component and it will you can write your
shape data that way something that's
that I mentioned about spec with the
multi specs is that I can add more shape
types to the system as needed but
something Elizabeth mentioned yesterday
is the fact that you can actually change
your schemas as needed so the cool thing
about des Tomic is that as we see our
data needs change and our data
definitions change we can grow with
those changes with DES Tomic and we can
still support everything historically as
well that was already in the database so
this is a pretty powerful concept to be
able to leverage so I don't know if
you've all noticed but that once your
data is in a database users usually want
it back they don't want to just know
that it's there and safe so we
have a graph QL layer that sits on top
of all these different databases we've
had a lot of systems in the past where
researchers would actually just hit the
database directly so if they wanted some
data they'd write a sequel query and
they get the data out and they'd write
scripts around this and then if you ever
needed to change the database they get
really upset because it would break all
over there because they were hitting it
directly so we wanted a layer of
abstraction so that we could make
choices related to our databases and
what we chose and how we implemented our
schemas without affecting how they
without them having to know how to
access that database so graph QL
provides a really nice layer for this it
provides a way for users to specify what
they want and then the implementation
behind-the-scenes knows where to go can
take that request and figure out how to
fulfill it based on where that data is
in our system so currently we're using
graph QL clj pretty excited about the
licinia talk I went to earlier if we
we've kind of set ourselves that so that
if we find that we need to change
technologies everything's pretty
isolated so it should be okay to do but
here's an example of using graph QL clj
to parse a query which is a user how
user would ask for data and to validate
a schema that we've defined the other
thing that's really cool about graph QL
is that you can get you can send up a
graph QL endpoint so we have a lot of
cases where users are in different
languages and are hitting needing to get
data in different ways so by having this
endpoint they can hit they can get data
if they're using MATLAB they're using
Python if they're using closure if
they're using Java and I'm sure there's
other things that I'm leaving out but
this is a nice unified way for users to
be able to get at data and whatever tool
that they want to use all right so
here's that whole diagram again just as
kind of a synopsis you have data shapes
these are our dice that you've defined a
new view spec to do that oh I have all
of this labeled old
so you package that data up with transit
you send it over using HTTP kit
composure API is used with combined
respect to validate that data and get it
routed into the system we have all that
data is going into Kafka we have riders
that are pulling that data out of Kafka
and writing it to different databases
and then graph QL is used as an
implementation of the graph QL spec is
used to get that data back out okay so
what are our opinions on this approach I
would say it's been a critical success
so for those of you who don't play a
tabletop role-playing games a lot of
times with a 20-sided dice it means that
you've had a resounding success
so we've been pretty happy with these
choices the choices to use a language
like closure the choices to use existing
libraries instead of handling our own
have enabled us to get up and running
and get this into the hands of the
researchers that need to get at the data
very quickly I to be inclusive
so here's success and a couple of other
role-playing games as well all right so
what are some of the things that we love
beyond just having all these great
libraries that we can use one of the
things that's nice with using closure
and using libraries that other people
have developed is that you have a
generally smaller code base and the
project that I'm describing here is not
the only thing that our team works on we
actually work on a lot of different
things so we are constantly putting
things down and picking them back up and
forgetting about them and then coming
back
so anything that decreases the amount of
cognitive load it takes to kind of get
back into a project is a huge win for us
because we're doing it all the time
not only does closure have composable
parts but by separating all these demand
all these different containers we also
have some composable parts just in
general and this has also been something
that's been highly beneficial so I
mentioned we have to be able to let our
users kind of put things into our system
so by giving them those boundaries of
how the data is coming in and the
is going up we can actually support that
for them I didn't really talk about this
previously but being able to have some
interrupts with the JVM is great we have
a lot of Java code and we're gonna have
to start making that Java code available
in the system as well so being able to
use something like closure that has a
pretty nice Interop system is a huge win
as well not everything is always super
rosy so I have a daughter who's in track
and she's a thrower she doesn't have to
do the whole hurdle thing but when I was
watching her the other day I was waiting
for her turn to come up I was watching
girls run hurdles and it struck me that
if I had to do that there is no way I
would get over those hurdles it would
just be like I wouldn't even try because
it's just immediate falling but as I
watched the girls that were doing it
they were able to clear them because
they had training and they had
thoughtfulness and they had skill and
how they were going to approach this
obstacle that was in front of them so
the hurdles that I'm gonna present are
not in movable obstacles so much as
things that we need to figure out how
we're going to navigate with care'
hurdle number one is most definitely
that there is not a lot of closure at
sandia we are one of the few teams that
I know they're using it and the reason
that this is tricky is that it's hard to
bring new people onto our team it's hard
to transition things that we've worked
on to other teams so the part the way
that we're kind of addressing this is
that we're starting to kind of share the
love we're starting to have sessions
where we talk about closure and what we
love about closure and what it spotted
us we have one coming up where we're
gonna do the same thing for day Tomic
and we're hoping that by opening these
up to this larger audience we can start
to get some more excitement about how
cool closure is especially for all these
problems that are completely focused
around data the fact that speck is still
in an alpha version has definitely
caused us of some areas where we've
stumbled
mostly because there are definitely
folks where I work that really want to
be using things that are released in
solid I don't see us moving away from
speck so we've got some plans in place
for how we're gonna mitigate
but that's something that we've had to
think about mm-hmm
this feels like an admission of failure
to me I had a lot of trouble getting
come getting component figured out not
so much than following the examples but
figuring out how I'm gonna use it
throughout the whole system and getting
everything to integrate well yeah I'm
just saying that it was hard so if you
had a hard time with it too then you can
feel validated because somebody else did
too
oops so moving forward this is a really
new project so these are kind of the
different are first take at how we
wanted to do this it'll evolved for sure
one of the things that I've alluded to
is that we want to start bringing in a
lot more processing so right now we're
bringing in data and we're storing it
and that's great and users can get it
out and they can do their own processing
with it we want to bring things in that
are gonna be hammering on the data as it
comes through so that's on the horizon
and the other thing we'd like to do is
incorporate some closure script I've
heard a lot of really great talks about
closure script here and I think it would
integrate pretty seamlessly with the way
that our system is currently constructed
so I think that would be a great next
step so did we solve the problem that we
set out to solve I would contend that
you don't actually completely solve wash
your hands walk away from a problem like
this but I do think that we've set
ourselves up to grow we've said we've
made some decisions that have given us
some flexibility to evolve and I think
that we're gonna be able to handle the
requirements that are coming down the
line and the reason I say this is I
don't have those moments where you wake
up in the middle of the night and
realize okay I've made a decision
it's an irreversible decision and now
I'm stuck and I'm not gonna be able to
to change at the system in the future so
I think that it was a good I think all
these choices were good and that's it
and of course went through it much
faster than I did in practice so I'm
hoping that by the end of this you do
understand where dice and mountains are
related to closure if you have any
questions I'll be around and please come
find me thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>