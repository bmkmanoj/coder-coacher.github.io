<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stu Halloway - Data.Fressian | Coder Coacher - Coaching Coders</title><meta content="Stu Halloway - Data.Fressian - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stu Halloway - Data.Fressian</b></h2><h5 class="post__date">2013-12-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JArZqMqsaB0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll say a little bit about
what I was doing when I met Alex since
he brought it up
my stock-in-trade at that time as a
developer was IDE expertise and so I
would go to conferences and demonstrate
that I knew more key combinations in
IntelliJ IDEA than anybody else in the
world
and I was the fastest most devilish most
nefarious user of IDE technology and
those who now work with me a cognitive
must really find that amusing because
they know that I can't sort of tighten
my way out of a paper bag with the
current tool set I've thought about how
to how to actually do the closure
version of that right so you would you
would come to a conference and you would
set up a hammock and you would you close
your eyes and people would watch you for
half an hour and then you'd say aha and
and and stand up and leave so I do think
if anybody wants to do that I think it
would be a real blast if somebody would
would film one of the coding Coty's
where you just play classical music for
twenty minutes and there's a blank
screen and then you say AHA and type in
the whole thing and and it's all correct
so but to turn to more serious matters I
want to talk to you today about data
depression and and before I do that just
to talk about freshen itself and so I'm
going to rewind the clock several years
and the clicker works too when we
started to build atomic if you think
about day Tomic in particular or any
database in general there are lots of
different places that data needs to move
around you have transactions obviously
and that's you know whatever data the
user puts into the system and also
whatever you report back to the user as
a result of a transaction you have data
being exchanged for a coordination so
keeping track of the different processes
of the system so that you can maintain
availability or maintain acid guarantees
or whatever databases have logs so you
want to be able to write the most recent
change in an efficient way
databases have catalogs so you can walk
up to them and ask you know what's in
you in the long term a database is
really only storage if it's just the log
right if you just threw stuff down you
know under keys or in a tree and could
them up later I wouldn't really call
that a database but if you have so you
want to have indexes which is yet
another different kind of data and
because the atomic is built on closure
principles there is a distinction
between values and refs that sometimes
even makes its way to how you think
about the binary encoding of data so
when we started working on des Tomic I
remember my first year of des Tomic very
vividly picture scene in a movie where a
harsh taskmaster is making you know wax
on wax off except that the harsh
taskmaster was making me read like 150
academic papers on data log in the
mornings and in the afternoons assessing
different binary data encoding schemes
that existed out in the world we
certainly wanted to have approaches that
ideally met all the characteristics that
we needed we're open source we're well
known and well established after
assessing the things that we had out
there in the open source world that were
well known and well established and
doing some performance benchmarks and
some of them we realized that that was
not going to work and set out to create
freshen one of the things we did not
know at that time was were we in a game
of having one scheme for binary encoding
or in schemes in other words those
different requirements that day Tomic
had was there gonna have to be something
that was you know specifically optimized
for log and another thing that was
specifically optimized for transactions
and remember that we're talking here
about data in motion and data at rest
right so we're talking about putting
data on the wire but we're also talking
about where data is gonna live long term
and so happily after a longer time than
I would have thought initially we came
out of that with a binary encoding
format called freshen I want to start by
saying about freshen that I will slip up
and say the word serialization but I
don't like the word serialization to
talk about what we're doing here and the
reason that I don't and if you go and
read the Wikipedia article on
serialization or or maybe if you're
really rigorous even go further than
that
read something else about cereal
everything I know about programming I
learned on Wikipedia right there I feel
like that's probably enough but if you
go and read about what people talk about
serialization it's obvious when people
think about serialization that objects
are driving right people are thinking oh
man I've got these great objects and I
got to go stuff them somewhere and so
really where they go is kind of second
class in your thinking when you start by
saying serialization and furthermore
obviously just given the heritage that
we have as an industry serialization is
gets into the business of object graphs
and references and that kind of stuff
which we didn't need any of to build the
atomic and if you wanted to have that
kind of thing you'd want to have it as a
separate layer you know above whatever
you did to deal with data so instead of
using the word serialization we're going
to use the word data encoding and the
difference at the highest level is just
a philosophical one right the data is
driving so you're thinking in terms of
what is this binary data going to be
because when somebody comes to your
system six years from now what are they
gonna have right they're gonna have the
binary data that's at rest somewhere
that you can give them or they're gonna
have binary data that you're gonna send
them on the wire and so when you come at
it with these presumptions you end up in
a lot of different places than you would
if you thought just in terms of
serialization in particular you don't
have a consumers must understand
everything attitude a lot of times with
serialization the big goal is fidelity
right so I can take X over here and get
back exactly X over there and obviously
it's nice to do that when you want to do
that but it's more important to say that
over there cares about your data and
they don't care about the way you
thought about it and so you'd like to
look at it in a different way also given
the well-known biases in this community
about values versus objects we didn't
need objects and we didn't need identity
we almost got there with a an existing
protocol called Hessian and these are
not my slide bullets I just copied these
from the Hessian web page this is the
design requirements of Hessian
there's a lot of really good stuff in
here it must self-described the
seriouser of tear lies types so in other
words there's not going to be any
external schema or context it has to be
language independent it has to be
readable and writeable in a single pass
it has to be compact it has to be simple
and I like the benefit of simple here so
they can be effectively tested and
implemented right there their
presumption is if you don't make things
simple it'll be impossible to actually
make them at all which is not a bad
starting place and then finally fast and
then there are some details about you
know supporting various other things so
Hessian is pretty cool
that being said if you read further than
these requirements they also came
strongly from the Java space and so you
can find other places in the documents
where you know supporting EJB in a
first-class way is a requirement and so
forth so so there's objects of all we
shouldn't laugh we should feel sad right
but so objects semantics you know really
became important in Hessian and so
Hessian is does a lot of things that you
would want and it does several things
then that we didn't need so freshen is a
refresh of the ideas and Hessian hence
the name another thing that you'll see a
lot of similarity to when we talk about
freshen is Eden and this original aside
originally said as of yesterday freshen
is like Eden and and Rich's really
helpful review of my slide deck said
Eden is like freshen that was the one
piece of feedback I got thanks rich so
but the idea here is that the design
proceeded in this order so a lot of you
are probably familiar with Eden because
because Eden was actually launched right
and there were there was a web page and
you know a conversation about it and a
blog post and and what have you and also
Eden was was addressing a more
immediately visible need for closure
users who were already committed to the
path of using closure data for
serialization but if you understand the
ideas behind Eden then freshen is gonna
line up with that pretty precisely and
the only reason that I invert the order
here when I'm talking about them is that
the design work that led to Eden was
actually the design
refreshing so freshman was designed and
implemented and then out of that there
was some head scratching and oh we could
take these ideas back and retroactively
lis apply them to closure data
structures and it's already lined up you
know not surprisingly because the design
work is done you know over time by the
same team
so both Eden and freshen our language
neutral so obviously we're here because
we're passionate about closure but it's
super important when you think about
data to try to be as broad-minded as
possible you know whatever language
we're using ten years from now I hope it
doesn't bear any resemblance to the ones
we're using now but it would be nice if
we could still read the data that we
saved so so being language neutral is
super important I'm going to come back
to that we want things to be self
describing and this is really consistent
with being in a dynamic world right
we're not going to have a lot of pre
negotiation in fact we're gonna have no
pre-negotiation about what data can look
like which means anything can show up a
specific example of where this is an
important requirement in a programming
language like closure is that you can
say anything writes a programming
language and so trying to pre describe
what you can say could really become a
pain in the ass on the day tommix I'd an
example would be transaction functions
right if we had to have a type system
for transaction functions we could
descend into endless Wang curry right we
could you know two years from now we
still wouldn't have transaction
functions in day Tomic but we would have
a great discussion and maybe a couple of
novel innovations in type theory which
would be cool but we wouldn't be doing
any transaction functions during that
time so obviously we're talking in terms
of immutable values namespaces are gonna
be a first-class idea you know floating
along with the ideas of symbols and
keywords freshen is extensible in
exactly the same kind of way that Eden
is or vice versa which is that you can
make new named things that are
recursively defined in terms of the old
things which means that you can compose
things and finally these formats are
batteries included the idea being that
they have enough stuff that at least on
day one you're not oh yeah you forgot to
put numbers or something like that in
the format that being said given that
they have all these things in common
it's of course reason
to ask why are there two of them and the
answer is really performance so eaton is
designed for readability by the second
best computing substrate we have out
there which is people and so the
internet being the first one and so so
it's got a lot of of bloat in it but
freshen is a binary protocol along with
Hessian it's driven by a bytecode spec
so the way that freshmen actually works
is that if you are of type X then
there's gonna be a bytecode that says
you know type X follows and we'll come
back to well how's that extensible in
just a second
freshman is also aware of primitives so
a big idea in the closure world is that
we are aware of the platforms that we're
gonna run on we do not want closure to
be a private island and it's a fact that
the platforms that we run on bless
groups of numbers that run in the range
from long men value to long max value
those numbers are privileged above other
numbers which is really unfair
considering that that actually is zero
percent of the total set of integers out
there but nonetheless they're they are
treated importantly and specially and if
you're doing high-performance stuff you
may actually want to be able to go to
and from platform primitives finally and
this was certainly the part that was
most interesting to me when we did the
design implementation work and this part
will go into in the most detail later
the talk freshmen allows you to do
domain-specific compression so if you
know something about your domain you can
actually win your writing write a very
small piece of code that compresses
amazingly your data and readers don't
have to know that you did it so this is
a really cool property you can think
about how you would go about doing that
before I show you the design you want to
make a system where writers can say I
know how to compress the data and I'm
gonna tell you how to do that and
readers don't know I'm not gonna write a
custom reader at all and that's just
gonna work now I'm not going to talk
specifically about other binary encoding
frameworks or serialization of
frameworks too much because it just
invites unnecessary
flame wars and the important thing here
is to be specific about what we're
trying to do what our goals and
objectives are in freshen an and Eden
and then sort of compare that to what
the available choices are so there are a
lot of temptations out there when you
have to if you are working on and as I
have a lot working on a consulting
project and your job right now is to
take some data that you have in your
favorite programming language and put it
somewhere right there are a lot of
temptations to avoid if you have the
goals that I stated before now it's
always important to keep the goals in
mind so I want to avoid the flame wars
later where people are saying well I
have different goals well those are
that's totally reasonable given the
goals that we have though given the goal
of language neutrality a whole swath of
things that are out there are right out
right there are a lot of tools out there
that can say I'm going to encode in such
a way that everybody forever will know
that this was made by Java and they will
have to be aware of Java or everybody
forever will know that this was made by
JavaScript and I'll have to be aware of
JavaScript that is right out because we
want to be language neutral and we don't
want to end and you might say well maybe
I can very carefully do something that's
driven by Java but if I'm really keep my
wits about me I won't screw up the other
guys I don't believe it right if you
don't start with a presumption of I want
to be able to each different languages
you're not going to get there
by accident or by a random walk another
thing a sufficiency requirement is to
have sufficiently rich types and here I
will use a concrete example if your
binary encoding or text encoding or
serialization whatever you call it if
your mechanism for dealing with data
does not have a system for expressing
types that is as rich as where you're
going as your language and doesn't have
a way to extend it to get there then the
job is not actually done right which
means that every other statement you
make about such a thing is entirely
suspect so let's imagine for example
that JSON was the fastest and in every
way the best language for communicating
data in every other way JSON does not
have a rich set of types and its types
are not extensible therefore you
actually haven't finished turning JSON
back into data until you do extra work
to actually get your types back
right if you have a date or a richly
described object or a UUID or anything
there are no there's no first-class
support for those things in JSON which
means that when you say I have this
benchmark showing that this thing is
incredible
you haven't even actually done the job
yet right because in order to do the job
you have to actually get back to your
where you want to be in your program and
of course if you haven't done the job
yet that job ends up being done at the
edges of thousands of different programs
literally a thousandfold reduction in
programmer productivity and I'm not
joking right this is a big deal
right if you get if you leave out
something if you're not sufficient and
everybody has to make things sufficient
in an ad hoc way right by adding these
little bits at the edges then you have
really real problems another place where
you can have problems is having said we
want to be rich we're never gonna be
well that kind of came up fun we're
never going to be rich enough in our
type system to express everything
somebody else is going to want to
express so you want to be extensible but
the extension point can't be entirely
arbitrary right it can't just be okay
now here you can do anything because at
that point you can write something that
everybody else can't read and a great
example of this failing would be Java
serialization right in Java
serialization there's a you know there's
a trapdoor that you can go through that
says I'm gonna do whatever I want here
and the minute you do that then you and
all future readers are in lockstep right
you're now coupled together around those
choices that you made so extensions
points have to be defined in terms of
the core semantics in such a way that
somebody who doesn't want to play is not
screwed up by trying to read the data
that you've made finally and perhaps
most importantly being self describing
what we want to avoid here is contextual
description or schemas so in other words
I send you a piece of data and you know
how to read it because we had an
out-of-band understanding that was not
communicated in any you know electronic
way or we had an out-of-band
understanding that was explicitly
communicated so and before I communicate
with you I must send you a schema that
tells you how I'm going to communicate
with you and it's perfectly fine to
build systems this way
they are going to have the ability to
get to some performance characteristics
you couldn't otherwise right if we have
the ability to pre negotiate in detail
how we're going to talk about things
right then I can tell you for example
the number 42 encodes Wikipedia all of
it right so whenever I want to
communicate to you Wikipedia I can just
send you the number 42 which is really
efficient compression right but if you
have to have that kind of out-of-band
communication you know you haven't
really communicated at some point right
because it's really where is where is
the actual communication it's there and
it's in the outer bound communication
from a functional programming
perspective it also makes everything
kind of stateful right in order to
understand your message X I have to
understand some pre message Y and a lot
of times if this context is not done
with an explicit schema it's just done
by agreement then there's a schema that
never got written down anywhere and so
the actual secret of understanding your
data is not even carefully specified or
encoded so there are a lot of different
choices out there where you could put
data that fail these requirements that
does not mean that those choices are
terrible it does mean specifically that
they fail these requirements and so if
these requirements are important to you
as they were to us you might end up
where we ended up so that being said we
still care about efficiency so given
that we don't have the tricks of saying
I'm going to communicate to you in
advance that the number 42 means the
entire content of the Wikipedia what
efficiency tricks do we have left well
we can obviously by doing just extra
work we can make things that work with
the primitives and the arrays that are
available on existing platforms bytecode
languages themselves are efficient I
mean we came to Hessian to begin with
because it was efficient and then there
are encoding strategies for dealing with
performance so you can do pact encodings
where you say I'm going to encode a
small value and I'm actually going to
encode the type and the value in the
byte code so for example the number 5
with the type information that says it's
the number 5 in freshen is encoded as
wait for it the number 5 and in fact the
first 64 integers are encoded that way 0
through 63 I guess are all encode to the
int which is me
and so that's a packed encoding because
your encoding to fax in the bytecode
write your encoding here comes an int
and here's the infant's coming so that's
packed encoding on the other hand side
another performance concern that I
haven't looted to yet is that it's
important to be able to run in streaming
contexts where you might not have enough
memory to process everything all at once
and so chunked encoding says that we're
never gonna make anything very big right
you can send a terabyte of data through
a freshman stream but we're never gonna
make any individual piece of that very
large and so the kinds of things that
are variable length in freshen get
chunked up such that you never get
bigger than say a 64 k byte of something
and so you could process arbitrarily
huge things without having to do
anything special at the binary encoding
level to worry about it and then finally
and the biggest one right the superpower
is the domain where caching which we'll
talk about in a minute so I'm going to
show you the Java API first and then
we'll look at the closure API so it's
really simple if you want to write
something you have an instance of a
fresh in writer which is on top of
Java's writers and you write an object
you also have the ability to read an
object which again is just read object
write as allows you if you've looked at
Eden before and looked at tagged things
in Eden it's the same idea here you can
say I'm gonna write this with a name so
when somebody reads it back later they
can say oh this isn't the map a maps to
one this is the map a maps to one named
encoding or grade or whatever it's going
to be named in addition to these there
are also a pair of streaming writers so
if you have a collection of things of
arbitrary size not known advanced in
advanced you can say begin closed list
this writes a byte code that says read
everything else and just accumulate them
up into a list until you get to another
code which says I'm done with my list
now which is the in list code which is
the other one
and the other thing that comes into play
when you start doing streaming and we'll
talk more about this later is that the
domain aware caching can be reset so if
you have a long stream of data one of
the great things about caching is once
you sort of say I'm gonna say 42 points
to the
pedia then later you can just say 42
except that if the stream is really long
you don't have to read all the way back
to the beginning of the stream in order
to understand the caching so you can
have a long stream of things but you can
have sort of caching subdomains within
that stream where the caching is defined
to mean different things then you have
the mechanism for making new types in
terms of old and so this is actually the
handler that's built in to freshen for
writing big decimals and so what you do
is in order to write a big decimal you
register a callback function which
because we're in Java we don't have
functions so it's goofy interface stuff
but you register a callback function and
then you write a tag so big decimal and
then you say the big decimals to come in
two pieces and then you write whatever
those pieces are and it's the job of a
reader then to say oh that's a name
thing I need to know how to recognize
that and do something with it in support
of this building new types in terms of
old is the entire stack of making
primitive things so what this allows you
to do is to say I'm gonna have one
carefully structured type which is the
very bottom of my system from a
performance perspective and that type
actually is you know maybe enclosure
terms it might be a def type right so it
has a bunch of primitive fields in it or
it might be a def type of arrays or
something like that and I'm gonna write
a write handler for that that works
directly at the primitive layer so I
don't have to do any boxing while I'm
doing serialization now all this sounds
great except for the minute I add big
decimal to my system what happens to you
if I didn't give you a reader for it or
if you're working on a different
implementation of freshman that doesn't
have that reader and what you get back
is an interface called tagged so anytime
you see a tag like big deck and you say
oh I don't know what a big deck is no
one gave me a handler for that then the
underlying implementation of freshman is
responsible for handing back your
language equivalent of this interface
here shown in Java an interface that
says I am a tagged thing with this name
with this mini with this value with
these pieces inside of it and so what
this does is it gets intermediaries out
of the business of having to understand
things right this is the opposite of
what I said in the narcissistic design
talk where I talked about
you ought to use static typing so that
intermediaries have to understand
everything alright this is I'm gonna
pass something through 17 intermediaries
all of them are gonna read and write
this thing and when they read and write
it they're just gonna get a level of
indirection that says I have a tagged
thing I don't understand this which is
fine I can do something with it
and pass it on to the next guy so domain
caching the idea here is that in the
fresh and bytecode spec there are a
number of byte codes that are reserved
for cache references and so when you're
writing things into a stream you can say
right this cached it will write at once
it will then assign it a code just for
simplicity I'll say 42 although that
actually is not a cache code but let's
say that we assign the cache code 42
then when we see that data structure
again rather than writing it we can
write 42 and the thing about this is
that the caching mechanism is fully
dynamic it's the writer who chooses what
to cache right when I'm writing an
object I could say right this object and
then there's a boolean flag and make a
cache code for this and that caching
mechanism is entirely generic from the
point of view of readers right so the
caching mechanism has already baked in
to both reading and writing and all
writers have to do is select what kinds
of things are going to be cached so what
kind of compression can you get from
this it depends right if you had time to
write the entire Wikipedia and then you
wanted to write it again you could claim
an extraordinarily big compression ratio
but it's going to allow you to get
compression proportional to your own
knowledge of your own data in my
experience this always beats generic
compression if you're not shipping
random data right if your data has any
structure to it that you can talk about
then you can talk about that structure
in terms of caching and beat generic
compression to give a specific example I
would find doing some testing over the
last couple of weeks that you could beat
a generic series ation scheme by at
least double right so if a generic
serialization scheme sterilizes to 500k
you would get to 250 K and that is
even if you then turn around and
compress both so even if you then turned
around and use gzip or snappies or
something like that right because those
things are gonna do a decent job of
sucking out similarity in a generic way
now if you don't compress if you just
let freshens domain aware in compression
complete with non compression then I can
claim arbitrary victories right over
over other forms right I could I could
show you examples where you got 10x or
100x or a thousand X compression ratio
but a more realistic number is probably
2x that's pretty big though right if you
have a whole system that you can make
half as big as it was that's not nothing
so what this looks like from the
perspective of someone who wants to
write data is there is a write object
method that just takes the cash flag
that says do I want to cash this object
there's also this is kind of advanced an
interface that you can implement on an
object that says I want to nominate
someone in my stead for cashing so get
objects to cash and the idea here is if
you have a very deeply nested thing you
might not be able to specify at the top
what kind of cashing you want to do
you're not going to write all the code
to do that you're gonna have that deeply
nested thing be smart enough to say I
nominate my cashing guy to give one
particular example of how this works
day Tomic stores datums right in any
attribute value transaction assert or
retract and one of the obvious questions
that people would ask is how can you
efficiently store the time that every
fact in your system happened right
that's given given the granularity of
storage you're given that it's stored
with every single really tiny fact write
these datums are smaller than rows in a
database or documents or objects in a
graph store so how are you gonna
efficiently do that and and this is
literally what happens right when you
write except it's not written in Java
but when you write add a Tomic datum
there are various places in the system
where the time is cached and of course
in a transaction the property of a
transaction is every datum and that
transactions going to have the same time
all right so writing that T that thing's
going to be cached once and then it's
going to be the same byte code after
that so first off you get this
compression ratio and secondly
repeating of the cache code itself
probably lends itself to more
compression in any sort of follow-on
compression algorithm so that's the
world as of three weeks ago which is to
say that freshman is out there it's an
open source library there's a reference
implementation in Java there's an
implementation in c-sharp now as of
later today there is a idiomatic closure
library for doing this stuff called data
aggression so this was something that
that we did not release because it
didn't exist but now it does and so the
idea behind data depression is that it's
an idiomatic closure wrapper for
everything I just said in particular it
has built-in handlers for all the stuff
you're likely to run into in closure
with some painful exceptions which I'll
come back to and it also has domain
compression for def records both as an
efficiency for def records and also as a
template that you could use to look at
how to do domain compression one thing I
want to say about this I do a lot of
telling people that simple is important
and that easy is nice too but really
simple is important and every once in a
while I probably should stand up in
front of people and say look we made
something that was both simple and easy
instead of simple and screw you which is
you know sometimes which sometimes
happens so this is actually simple and
easy right so the easy API just says I
want to read stuff and I want to write
stuff well those api's are literally
write and read then those are actually
bad those are wrappers those are the
ease rosante parts so create reader and
create writer is the underlying API and
then there's read and write and so forth
and all of those things can be composed
in particular any of those things in the
freshman namespace can be composed with
adding your own handlers on either the
right side of the read side so that's
the mechanism to extensibility there are
a number of types that you need to worry
about if you want to have fresh and work
in the closure ecosystem you probably do
not want to get back well we should talk
about this right imagine what the Java
implementation
to do the job implementation of freshen
promises that's going to deliver you
values which means that the things that
it gives you back have to be immutable
but it's written in Java and it's not
going to drag in all of closures
persistent data structures so if you go
and look at what this fresh in Java
library does is when it's reading back
complex data structures it makes a
mutable collection and then it calls dot
what is it to unmodifiable collection or
whatever on the end as a closure
programmer you're going to be really
ticked off by that in particular you're
going to be ticked off at in the case of
Maps because you then cannot associate
of em or do any of the other stuff you
want to do with persistent data
structures so one of the things that
data depression does is read that reads
back collections into the persistent
data structures that you would expect
then there are a number of types that
are particular to closure that have no
standard representation in Java so even
though they can be supported and freshen
you can't really see them in a nice way
in Java so keyword symbol and ratio the
only thing interesting to say about
those is they just work so if you pass
in those things you can do so then big
int is special for reasons that as a
user you really don't care about but
just to mention it for completeness big
int is special because closure has its
own representation for big int separate
from big integer to deal with so to all
the nefarious issues around hashing
inequality in Java versus in closure so
the big int is actually extended on the
read side in dated up fresh and so you
actually get back closure begins so you
see things round-tripping up to what you
would expect
and then finally def records cache their
keys and also there's an extension point
you can do to selectively cache values
so the idea here is if you had for
example an address book with a fatass
out a thousand entries in it and the
address book you know each entry had
eight keys and you encoded that as json
you would repeat those eight keys on the
first address entry you'd repeat those
eight keys on the second one and so on
and so forth odd infinitum in the def
record caching all those names are
cached and so the representation of them
in subsequent records is one byte each
and so again right it's totally bogus
right I can generate example data that
shows you a thousandfold compression
right just by making
enough key names and making them big
enough but you get the idea and here's
what that looks like in closure code so
this is the actual right handler for Def
Records in data depression and so the
two important things here are that when
you write the actual symbol for the
class that's marked as cash true and
then when you mark the actual keyword
name of the keys in the map that's also
marked as cash true and then on the read
side what happens is you read back the
record name and the map that you're
gonna use to construct the record and
then you look and see if you actually
know that constructor so one possibility
in a closure program of course is that I
had a def record that you don't have
right so when I when I you know when I
pass the desk Bracker din you go back to
read it later it doesn't exist what
should we do well what the default
implementation currently does and this
is subject to discussion among this
group but as it says look if I can
construct the record I will if not I'm
taking my hands off of it and I'm gonna
give you back a tagged object that says
look here's a record for which you don't
have the implementation you got to go
figure out what you want to do with it
in some cases you don't care right
you're an intermediary so you're saying
fine that's okay with me I'll pass that
on now the great thing about this
extension mechanism is you might
disagree with me completely about what
to do here
one possible choice is you know what I
don't even care about records on any
consumer side so I want them just to
look like Maps well fine right it's you
know replace this handler which you can
do on all these functions replace this
handler with one that instead of making
tagged objects just says you know
whenever I see a record I'm just gonna
dumb it down to a map on this side and
that the important thing here is that
the decision-making here is completely
decoupled between the reader and the
writer right there is nothing that I can
say as a reader that's going to put you
in a position as a writer or as a writer
that's gonna put you in a position as a
reader to say well I'm stuck right the
most stuck you can get is to say I don't
really understand this so I'm going to
sort of put a little box around it and
pass it down the line the extension
mechanism in data dot freshen is
data-driven so if you've ever looked at
protocol extension there are these
higher-level helpers but then at the
very bottom there's the function that
works with maps
it's similar to that idea so the actual
registration of new handlers is done via
maps of type names to handlers for those
names and I'll show you one example so
this is defining my own extended handler
so I have a deaf record called splotch
it's from a children's book and splotch
has it in a color it turns out that
color itself in splotches is always a
keyword so the value of color is a good
candidate for caching all right that's
not always true something's in deaf
record might be a good candidate for
caching something's might not but the
value is a good candidate for caching so
there's this helper function here field
caching writer that says give me back a
new writer for records that will
selectively cache the values of certain
fields and it caches them based on a
predicate so the predicate here says if
the field name is color I want to cache
that as well there's more details here
that that get into things that are
perhaps less completely considered and
again sort of open subject to change
over the next couple of weeks you take
that map you pass it to associative
lookup which says given a closure data
structure make the thing that freshen
wants to see right it doesn't think in
terms of the map interface and then
inheritance lookup does magic on top of
that to say when I specify something I'm
actually talking about this and all
derived classes so it actually creates a
self extending map that says oh if I
ever see a derived class I'm gonna find
a base class handler for that and then
not have to walk the inheritance
hierarchy again that gets quite advanced
but the plumbing is exposed so if I did
something you didn't like you could do
something else or even you know leave
that piece out the bike codes are
documented on the freshen wiki so you
can go and look and see how the bike
code spec works and the wiki is actually
I wouldn't say comprehensive but the
documentation that's there is good and
correct and there's you know five or six
sections this is at the day Tomic
freshen wiki and so you can look at the
bike code representations you can look
at how the packed representations of
numbers work so 0 0 means the num
numeric value and value 0
there are also packed representations
for some collection and aggregate types
so for example the bike code d1 means
this is a byte array of length 1 and
then the following byte is you know
whatever the contents of the array are
the chunked representations go in the
other direction
and guarantee that things will not be
bigger than 64 K and so there's
non-terminal in terminal chunks that let
you sort of break that stuff apart the
byte codes are all documented in there
in a table in this spec in the wiki so
if you're trying to write a conformant
implementation you can look at the byte
codes in addition to looking at Java so
help want it it'd be great to get a
review of the initial implementation
you're dependent on me getting internet
access and actually putting the repo up
so I will do that later today there are
a couple of things that I specifically
don't like about the way it works right
now one of them is that we have not done
anything about metadata this is also
true in Eden right I think Eden doesn't
say anything about metadata yet that's
considered a closure specific thing that
goes beyond you you know eventually we'd
like to do metadata the challenge of
course is that the representation of
this in every other language requires
careful thought even sort of genero
sizing that notion so the reason it's
not done is a design you know hammock
time needed the second thing for which
hammock time is needed is that the
reference implementation for freshen is
in Java not because we couldn't have
written a fast performant closure
implementation but because we couldn't
tolerate in some settings the overhead
of closure itself and so it would be
really nice to be looking at you know
future versions of closure runtimes that
are lighter weight one of the things
that's great about closure script is
that you have that division right
closure script doesn't say it does
everything it has specific things that
closure can do that it leaves out I can
imagine a future version of closure that
leaves out some of the developer time
capabilities some of the same things
that closure script leaves out reified
namespaces eval and whatever and targets
a leaner deployment thing that would
have led us right fresh an enclosure to
begin with
to sum up freshen is binary data done in
the spirit of
and if you like the spirit of closure
which you can summarize via the sort of
requirements I laid out earlier this may
be a good fit for you for data in your
own applications and then I would end
with a plea and that is if you look back
at all the different requirements that I
talked about by far I think the most
important one is the requirement that
the data format be language neutral and
when I talked in narcissistic
narcissistic design I talked about
people who are making language driven
serialization decisions right I have
some data right now I have the hottest
new library on the interwebs that says
it's the fastest or whatever it has the
coolest characteristics and one of its
characteristics is it makes the
presumption that my data is closure and
that everybody on the other side is
always going to be happy with that that
is there are times when it's appropriate
to do that but think carefully before
you do closure was designed not to be a
private island and it's a real shame if
we sort of unwittingly take it back in
that direction by building tools and
approaches that presume closure from end
to end on the stack it's definitely not
where we'd like to end up thank you
do we do questions how does this work
you're in charge you have time for two
questions are there any questions yes
that's a good question let's go find out
so I you know if we can do something
like that without changing the I mean
there's nothing about the the bike code
spec of freshmen that says you couldn't
do things like that I would think right
you could look you could look at
directly at things so there could be
alternative I think there should be able
to be alternate implementations that
work that way I have done some of that
work but I think that that the work of
benchmarking is so it's so full of
specific assumptions that are baked in
that like to say something casually
would take me a day and and to explain
it carefully would take six months of my
time and I can't do it i if people want
I think it's much more important for
people to test for the thing that
they're trying to do and so I would say
if you're evaluating this if you find
someplace where there's something else
that meets these requirements that
performs better we'd love to know about
it and so we can address those issues
but I do not want to get into the sort
of language war stuff that that falls
out of that and so measurin see that
counts two questions one more yes
so in fact the namespaces for the
closure tags is one of the open
questions that we're going to answer in
the next week whether or not there will
be a clj slash in front of all the
closure specific ones or a cord closure
/ but definitely the tags support
namespaces I just chose not to use them
on the slides for brevity and I think
probably we should be using them
everywhere that we're actually making
stuff for real
so yes namespaces good thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>