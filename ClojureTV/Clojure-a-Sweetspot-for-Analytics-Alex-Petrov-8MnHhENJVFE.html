<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure, a Sweetspot for Analytics - Alex Petrov | Coder Coacher - Coaching Coders</title><meta content="Clojure, a Sweetspot for Analytics - Alex Petrov - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clojure, a Sweetspot for Analytics - Alex Petrov</b></h2><h5 class="post__date">2015-07-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8MnHhENJVFE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">who of you is using okay who of you have
heard of closure VX anyone so okay and
who is using closure of erik's library
okay it looks like more people use
culture of X libraries that know about
closure X that's it's very interesting
which actually can be the case very well
okay so yeah my name is Alex I'm from
Munich I work for the company called in
stana and we're doing the revolutionary
changes in the monitoring of application
monitoring performance monitoring and
data analytics in the as I said that's
like the third time I've been trying to
push my talk to you were a closure but
that's the first time it got accepted
but every time I was coming to the
conferences there was something saying
yeah you should watch that talk he's
talking about the libraries you guys
wrote and this is very exciting for me
to be here and having an honor to
present something with been working on
lately myself and I wanted to talk how
awesome the closure is because first I I
didn't know that I'm gonna be the first
speaker on the conference and I was
aiming for very scientifical talk
because it was going to be about closure
as a sweet spot a sweet spot for
analytics and I thought that yeah I
should just jump them with the formulas
and tell like show all the super macros
and tell about the Bayes theorem and
everything that possible to stick into
40 minutes but after I heard that it's
going to be the first talk i was like
stop I can do that because the first
talk should be about how awesome the
closure is and like why we are all here
kind of in a way but as a side dish I
will serve analytics so as I said most
of them of my free time I'm spending
working on the library's under
of X and lately we've been working more
and more to help backend engineers to
work with the back ends without setting
their hair on fire and we have around 35
+ closure libraries and we have used
reports from all over the world frankly
speaking like most of the people i know
enclosure community have been at some
point their career a closure career
using a closure of x libraries and we
have around 20 plus active contributors
and we value documentation more than we
value anything else because
documentation is well is the way we're
we can communicate how our libraries can
be used and help people to get started
with them and switch into the different
language such as closure is already
complex enough so we wanted to make this
transition as smooth as possible and
make our libraries as accessible as we
could thank you
and if you think about it why is closure
sir awesome first of all I wanted to
give a slice self selfish intro to
something is Sam Aaron here okay he's
not here but he should be around on the
conference you should know what overtone
is if you don't yet I'm very thankful to
Sam what for what he's done on overtone
but I just wanted to say that overtone
isn't just for music like you can write
awesome music with overtone but you can
also write control software for things
like this one this is the just a core
controller I have a couple of different
ones at home and we've been using this
hardware in order to control our
disaster scenarios since we're
monitoring company we need to come up
with a disaster scenarios for instance
our Amazon instances are getting slow
the latencies are going up the CPU on
certain machines is blowing up or the
memory is going to be over filled in a
few minutes and using this software or
the using overtone and this kind of
hardware it was possible for us to come
up with the disaster scenarios which
were so smoothly reproductive that it
wouldn't be possible otherwise so it was
like the best interactivity that was
possible to get the second thing which
was thrilling us about closure is
something we now call in our company as
a monetary to minority report I'm sorry
about that and have anybody watch this
movie Minority Report yeah this movies
like in this movie there are several
scenes where the guy is flushing like
data on this his screen and zooming into
it in trying to find the suspects for
the crimes and what we've been doing
data analysis for one of our customers
that's exactly what we did we set all
together on the big table took the rapel
and started slicing and dicing the data
everybody had his own hypothesis
everybody wanted to prove whether the
data look this or that way and we've
been just clearing the data on the rebel
and visualizing the results right on and
we've been compiling the code going
iterating through it and it wouldn't be
possible with pretty much any language
other than closure I can guarantee you
and the last thing the closure is really
good for the math there are arguably
several languages which could be
slightly better for the math but the
simplicity of closure and the way
closure allows us to express things is
unimaginable in all the other languages
I just wanted to say a couple of things
about what to expect from this talk I am
going to talk about the libraries which
I found really awesome and just one of
them hasn't been written by ourselves
and it's not going to be anything that
you couldn't read in our documentation
but I mean what you're gonna do right I
mean if you ride the documentation you
have to repeat yourself from time to
time and who is writing back end code
with closure here okay so because it
feels to me that majority in majority of
cases the closure is used to write
actually backhand code and actually
that's why we've been we've chosen it to
write our back-end codes too but the
life of the backend engineer is
extremely difficult because you're
locked between several worlds you look
between the world of mathematicians were
coming up with new algorithms who are
coming up with the new things that have
to be tried and things are getting more
and more complicated the science is
moving forward and on the other hand you
have the business requirements like
certain latency requirements things have
to be fast and things have to be
predictable and you have to be able to
deliver software in a certain time
period and
the pendulum of fate is cycling
somewhere between the dynamic city and
statistical the type system for instance
several years ago everybody's been
hyping and saying that yeah we have to
use some very dynamic language such as
Ruby or Python these days the pendulum
is going backwards and everybody sent
yeah we should use something with
dependent types and like Idris or
Haskell and I think that closure is just
in the right place and just in the right
position and it's the language where you
can utilize using the macros and verify
most of you have software during the
compile time at the same time you get
all the flexibility and the Nemesis it e
during the execution time in the the the
knowledge that the backend engineer has
to have these days is also very
complicated and complicated to get also
for instance everybody who is doing
something with the complicated backends
and data processing pipelines has to
know a lot about calculus and linear
algebra a probability theory like
information theory and different things
and he has to have distributed
programming skills and understand the
concurrent programming very well so it's
not very easy thing to do and from the
technical perspective in order to be a
successful backend engineer or in order
to write the successful and ticking
back-end modern one you have to have
several things in my opinion first of
all the era of batching is already far
away from us it's like we are living in
the days when everybody is talking about
real time of course on JVM like hard
real-time is impossible to get although
we should be able to get the information
from out of our back ends in the
shortest amount of time possible and in
order for that we should have some
solution for crunching data on streams
and this is one of the things I'm going
to mention in my talk the second thing
is in order to have a back end which is
performant more often than not you
should use or utilize the binary
protocols rather than text based ones
because if you were just pushing the
Jason over HTTP sooner or later you will
hit the barrier on an enormous scale
that your messages are just like too
large to be transmitted over the network
and in order to optimize that you will
have to sooner or later switch to the
binary protocols and we have a solution
for that too and you have to be able to
model the situations like what would be
if I would have more like Cassandra
servers if I had like five hundred
thousand requests and I had a certain
amount of Cassandra servers and the
latency is of a certain type like what
would be if certain things have changed
also you have to be able to get the
access to the data as fast as possible
because whenever things go wrong and
production everybody is coming to you
and they're making some claims and in
order to verify these claims you have to
have very fast access to the data if you
cannot do that you will be most likely
fixing the bugs which either don't exist
or actually are misleading because well
the effects of the bug are often
misunderstood and lead to the different
fixes and of course analytics is in a
way getting something out of nothing
because well we are trying to infer the
information which has been observed like
in Kwan's right so we are observing
something which is which we are trying
to interpret in the way that well is
useful for us and our businesses so the
problems that we are trying to solve
with analytics are of course
understanding the shape of data and
trying to do under two to get the
information out of the data in the pace
that is suitable for your business needs
we can take for instance several classes
of users which are more likely to buy
our products less likely to buy our
products and optimized for certain
things or do a B testing or predict how
likely it is for the user to perform a
certain action and we can also model the
real world and try to understand how the
world will what the world will look like
if we made a certain changes to it and
while working on the data processing
back end I've I've met several really
nice libraries or I've seen several very
nice libraries and one of them was
Anglican and if if you're aware there
was a recent development it was called
first called Church and not Catholic
Church and different one and it's just a
framework for for the probabilistic
programming and Anglican is something
that you open when people are saying
okay what are the odds right and
whenever people say okay what else you
say okay just a second I have to check
with my rebel and tell you exactly what
the odds are so the Anglican it's a
programming language which is described
designed to describe the probabilistic
models and then perform inference on
this model you can use it in order to
make some predictions or recommendations
or for Diagnostics for instance and if
you have a certain claim you can verify
it against the data and get to know
whether the claim is rather true or
rather false of course anglican as such
as most of the modern probability theory
evolvement SAR based on the Bayes
theorem in the Bayes theorem is
basically allowing you to say okay if
there are certain things that were
during certain period of time and there
are certain hypotheses you can say how
likely it is for the hypothesis to be
true giving certain well priors and yeah
given certain prior probabilities and
anglican is if you if you take a look at
the coin flips problem which is very
traditional way to explain the
probabilistic programming for instance
you have a coin and if you flip it
several times you get a certain amount
of heads and tails and in anglican you
can express such things very well and
for instance we try to model the coin
which is not rich so it's the fair coin
which is going to if you throw it enough
times give equal amount of heads and
tails and so you say that okay I'm
flipping the coin it's a Bernoulli
distribution it's going to return 0 0 1
with a 50 0.5 percent chance and so we
do okay we sample the Bernoulli
distribution flipping we take just 5 50
samples and we either say it's going to
be heads or tails and we try to
visualize it and you see that even
though the coin was fair we have
observed that the coin has showed heads
more times than it has shown tails and
the problem with that is because the
sample space of yours is very small or
incredibly small I'd say for do in order
to identify whether the coin is fair or
not you have to do many more trials and
the same or similar shape of results you
could get if if your coin wasn't fair
for instance your coin was returning
heads in your modal only forty-five
percent of time and you would get pretty
much similar picture or result so we
have been throwing the coin around
50,000 times and on one side we get like
around 28,000 tails and around 22,000
heads so it's roughly forty-five percent
but despite the fact that like the Y
scale is entirely different the picture
itself looks just the same and this of
course may lead just purely
theoretically to very confusing results
and while languages like Anglican have
been helpful for us in order to avoid
such mistakes and in order to make a
right conclusion for coin flips you have
to have a fair coin and flip it enough
times in order to see that actually if
you throw the coin 50,000 times it's
going to give exactly the same amount or
roughly same amount of heads and tails
and I've been thinking okay how could
one use angry can to do something more
fun for instance yesterday I've been
running through Barcelona and ran into
zach telman and I've been thinking okay
what are the odds of meeting Zach telman
in in Barcelona but well the thing is
that here the priors are a little bit
skewed towards your closure so it would
be very difficult to actually calculate
the exact probability because well
technically if there was no your closure
here the probability of Zack being here
would be much slower so I thought that
ok I'll try to simulate another scenario
i'll try to make a bill to Cassandra
cluster one node can safely say and
serve let's say 10,000 requests and
latency is going to be normally
distributed there's just my assumption
you can don't judge me for that and the
mean of it is going to be 20
milliseconds and standard deviation is
going to be 5 milliseconds it's probably
like normally working Cassandra cluster
and every extra request which is added
on top of this
thousand request is going to make our
cluster exponentially slower so we're
going to model that so we say okay we
have a base amount of requests and we
defined a query in this Anglican
language we say okay we have an input
data of the amount of nodes which are
going to be serving the the request and
we have the right rate right rate is
exactly like how often things are being
written and we calculate okay the pair
note how many rights do we have per node
and we then calculate overhead and after
that we just sample the exponential and
normal distributions and summarize the
probabilities or they actually we
summarize the outcomes and of course the
the numbers here are completely made up
and the real scenario would be much more
complicated to describe but you can get
a rough idea of what's going on so if we
have five nodes which are serving
roughly fifty thousand requests the
distribution or histogram would be
looking pretty much like that so there
will be still things which are running
like much slower will be things which
are running very fast like much faster
than 20 milliseconds and well this is
going to be the histogram if we have
five nodes which are serving five five
hundred thousand requests per second you
can see that the latencies are
dramatically increasing so we have a
thousand milliseconds so it's like
response which is taking eight seconds
and then you can start questioning
yourself okay what can I do how many
nodes should i bring into my cluster in
order for them to actually be able to
handle this load and having this query
ability you can just start well pretty
much benchmark marking your non existing
cassandra cluster against the non
existing data
for instance you are you have tried like
six note 7 notes 10 notes and then you
finally figured out that okay in order
to handle the 500,000 requests for me
the optimal way to do that would be to
handle it with a 30 notes and why
Anglican is so cool first of all because
its enclosure of course and like
whenever you can write the probabilistic
programming program in the language
which is so interactive well it's
bringing you to a whole different level
of interactivity because you can really
interact with things as if you were in
this movie Minority Report and the way
Anglican is implemented they write a
macro and walk through the code they
take all the expressions like map filter
reduce all the conditionals and replace
them with their stack or continuation
passing style counterparts and whenever
you're you're accessing the state of the
internal program so for instance you say
had an observation there was a certain
things i think that i am sure what's
happening during that time the internal
state of the program is being changed
and it's been reflected in all the
further procedures and i'd say that it
would be either very complicated or
almost impossible to integrate or to
write a language which is so closely
integrated with the target platform as
with example with anglican i highly
advise you to just take a short look
into the code because the way they do
things is very interesting and
unimaginably cool the second library i
wanted to mention is a statistic ER and
this was our attempt to write a machine
learning starter kit for the
foreclosure and it's been very easy for
us or relatively easy for us to do so
because enclosure there are well first
of all the expressiveness of language
allows you to do like a lot of things in
a very easy way for instance what we
needed to implement the gaussian knave
based algorithm and who's aware what
knave base is okay couple of people are
that's cool okay Nate vase is just a
classification algorithm you could take
a certain amount of blue dots and red
dots and if you put the dot in between
those you could say what is the
probability of the dot belonging either
to one or two the second cluster and
cluster is being just a color in that
particular case and the way Nate base is
working is first of all we are
calculating the prior probabilities of
things so the prior probability for the
blue is going to be the number of blue
divided by the total amounts of all like
a total amount of object so blue / total
and for red is going to be the same
thing number of red divided by the total
amount of objects and in order to
calculate so-called posterior
probability for the four star belonging
to bloop cluster we would take the
number of blue which are close to the
star and compare it to the number of red
which are closer to the star divided by
the total number of red and the way it's
implemented enclosure looks pretty much
like that first of all we are
calculating the priors so we're taking
the amount of blue so we take a train
data and okay I think I'm a little bit
confused let's really do it
okay in order to calculate if we have a
vector of data in order to calculate how
many things are belonging to well to the
to the data we just took the count of
the each vector and reduced it so we we
just summed the count and this way we
could tell how many points there are in
every cluster and after that we
calculate the P so the probability so
the one on the top so we just say okay
the amount of data points in the current
cluster is going to be divided by the
total so the amount of points which we
had in general which we just calculated
the right over here and in order to
calculate how close these certain points
are to the one or the other cluster once
again assuming that both clusters are
normally distributed we calculate the
mean and variance of the each cluster
and after that we will we will compare
how close the certain point data point
is to that place and the second step is
actually the classification step so we
take the amount of the number of items
which are closer to the blue mean or
Center and we take the number of items
which are nearer to red one and then we
calculate the posterior probability so
we basically say there is a well
gaussian formula that you can look up on
the internet which is helping you to
just understand how close things are to
the normally distributed cluster with
the mean of a and variance of our
standard deviation of b and then you
pretty much just filter out or you
compare
compare all the or compare how close so
you pretty much say okay there's this is
my proximity or how close things can or
should actually get to the blue and then
you say okay there's one two three four
five six seven dots of the blue color
and only 12 of the red colors so the
probability of the star being blue is
much higher than the probability of the
star being read but the point being is
that the first of all writing
mathematical code enclosure is very
simple because first of all you have a
very rich standard to library and second
of all you have power of java or JVM
underneath and every time there is
something that you're heavily missing
there will be someone who would have
already implemented it for you and the
best example for me was implementing the
linear regression with gradient descent
because this is a slightly more
sophisticated algorithm or at least the
one which students are advised to first
not write themselves but rather take the
implementation which is already existing
and linear regression is just once again
a method for either classification or
prediction and the gradient descent is
the method for optimization for instance
if you have a curve in order to find a
local minima for the curve you have to
calculate the gradient which is the
first derivative or the vector of the
partial derivatives on the each
direction of the curve and then you have
to take the descent so well derivative
is usually showing us where the line is
ascending and we have to take the
negative gradient so we have to go kind
of backwards in order to find the local
minima and once again
in the understanding of algorithm isn't
maybe not that difficult although
implementing it in a very correct way is
much more complicated and that's why I
decided that okay I'm not going to
implement the gradient descent all by
myself rather I will take the mass three
library which is Apache Commons
mathematical library and just take the
implementation from there and the linear
regression is consisting of the two
things first thing is just an objective
function objective function is whenever
you have a I will show it on the slide
whenever you have many points the
objective function would be comparing
how far your current line why is to the
point so it will basically draw the
lines from each point down to the line
itself and then it will try to well
calculate the least squares or the
square distances to the line and this is
exactly our whatever we're trying to
optimize and once again it's pretty
simple to express and the formula is
like we're just trying to we we
calculate the actual y which we've got
using our well slope and intercept and
then we compare it to whatever we have
already seen in the data or so to our
observation and this is pretty much like
whatever the way you could implement it
so yeah you pretty much take the so you
reduce on on the squares squared sum and
in order to calculate the objective
function gradient you should already be
using a little bit more sophisticated
techniques such as well matrix
multiplication and once again this is
formula I mean you don't have to
entirely understand it but once again if
you just took a formula from Wikipedia
for instance and to closure matrix
library in under 10 minutes you can
already get up and running with an
algorithm and the optimization itself is
already available from the math three so
it would take you maybe a little bit
mathematical background and around 20
minutes of time in order to do really
awesome things with closure and I can
say that whenever you if you want to do
some data analysis with the a closure
first of all there are many many
libraries available already available of
JVM so for instance support vector
machines are very hard to get right and
there are many researchers researchers
who are working on this problem for many
years and writing your own algorithm
would be well close to insanity and if
you could just take the implementation
of the other person and implement it
yourself well it would be much wiser and
most likely it will be also more correct
the second of all the closure matrix is
an amazing library it's also first of
all its feature complete it can do
whatever you can imagine it can
calculate our egg in vectors it can
transpose the matrix for you calculate
or calculate the dot product of matrix
and scholars and vectors and whatever
you can think about so basically
whenever you're doing anything which
related to algebra you could use the
potion matrix and whenever you're
working with or implementing a
sophisticated algorithm the pattern
matching for instance is a very
important thing and at the reason
closure match which is just once again a
set of macros which I'll help in you to
understand or to match on the data which
is going through your function and the
last but not least is the closure
functions are extremely easy to test
because the language which disallows you
mutability by default
is I think the same is choice for today
and with the immutable data structures
almost all the time nothing goes wrong I
mean I included these slides on purpose
because I wanted to be flexible enough
but I'm not going to go through through
them I mean I knew that I will either
skip through one part of the talk or the
other but I didn't know which one
depending on the audience responses it
would be so the conclusions the first of
all the closure is is going to be the
language which I'm going to be choosing
for any back-end that I'm working on for
next I don't know a couple of years at
least if everything goes right it going
to be the amount of time is going to be
much longer of course but the reasons
for that is of course the choices that
have been made by the designers of the
language and by the community who's
being working on it first of all the
wisest choice in my opinion was to go
with the immutable data structures by
default and making them persistent is of
course a whole different level of
coolness although I mean you most likely
will notice it whenever you are starting
having well the higher load possibly and
another thing which we've been thrilling
me enclosure all the time is the
threading model or the way it allows you
to work with the concurrent objects
namely SDM or items it's well very good
to have it with you all the time and I
can I just wanted to tell you that the
language toys will most likely influence
the way you're writing software for
quite a long time so having a started
programming closure I think the your
choice is pretty sane and good because
first of all if pragmatic enough because
you're not going to re-implement half of
the world in the new language once again
because there are plenty of things
available on JVM
integration with JVM is very high
quality and works in majority amount of
times and yeah so thank you very much
I've been Alex from closure of X and we
still have like three minutes for
questions if you have any otherwise I
I'm gonna be around for the next two
days and hit me up if you'd like to
discuss some subject or anything else
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>