<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Zach Tellman - Always Be Composing | Coder Coacher - Coaching Coders</title><meta content="Zach Tellman - Always Be Composing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Zach Tellman - Always Be Composing</b></h2><h5 class="post__date">2014-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3oQTSP4FngY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so I'm here to talk about how
we can always be composing when I first
started working with closure the first
thing I played around with was OpenGL
and so I found a Java library that gave
bindings I started playing around with
it and after quite a bit of struggle I
was able to get a red triangle to show
up on the screen which was genuinely
exciting I these are a lot of things
that you have to not screw up to get a
red triangle on the screen you have to
get the right geometry you have to
actually have it be lit you have to have
make sure the camera's not pointing the
wrong way and so building on that
success I created more triangles notably
actually this is called the sierpinski
pyramid which is a sort of a fractal
shape and it's actually fairly
straightforward and I'll switch to two
dimensions for your convenience you
start with a triangle and you shrink it
down to have its size and then you copy
it twice then you shrink it down to have
its size and then you copy that twice
and the reason that I'm talking about
this is not because it's a particularly
complex thing to do in fact it's kind of
the geometric equivalent of the
Fibonacci sequence but it is a sort of
different than the Fibonacci sequence in
that it actually does something right it
has a side affected it draws to the
screen and everything that we do has
these sorts of effects right we don't
just write software to you know raise
the ambient temperature of the room we
want to do a thing and so I'm gonna talk
about some different ways that we can do
the sierpinski triangle not because it's
complex but it's sort of a full-stack
trivial example right encompasses all
the different things that we have to do
and so maybe we can learn a few things
from it and the first way that we can
sort of address this is macros and so we
can write ourselves a seer Pinsky macro
which takes a body which is really just
anything and first we wrap it in a
function and then we create a scoped
transform which shrinks everything that
we draw by half and then we just invoke
the function
three times first in the lower left and
the lower right and then above and if we
want to use this to draw a sort of a
sierpinski triangle we can wrap it in as
many sierpinski macros as we want to
right if we want to get two levels you
wrap it twice we want to get ten levels
you're up at ten times and obviously
this gets very tedious and so we can
sort of raise ourselves away from that
sort of tedium by creating another macro
which allows us to specify exactly how
many sierpinski's we want and so now we
can kind of you know go to our hearts
content or at least until the computer
gets you know crabby and breaks so this
works but I mean there's some fairly
obvious problems we as a community have
realized that macros are not necessarily
are our most composable sort of
abstraction and this has sort of two
issues one of which is that we do
something immediately once it's done we
can't sort of unring that balance so
there's no potential for downstream
composition from this and because we've
defined our abstraction in terms of the
structure of the code the only way that
we can interact with it is by altering
the structure that code with macros and
so this leaves something to be desired
and so maybe we can kind of raise
ourselves up a little bit and use
functions and this is not too difficult
we take our draw triangle function and
we call it now it's a renderer and now
if a render function which simply
invokes it and this is a very small
piece of indirection that we're sort of
injecting into what we're doing but it
gives us some pretty significant gains
because now we can take any function and
sort of wrap something else around it we
can close over it here we create
something which will take a function and
draw it elsewhere and so this is
downstream composition this is something
where we are taking something and
because we've deferred doing anything
with it we've deferred invoking it we
are able to do things to it we were able
to sort of turn it into a piece of data
and like functions are not a very rich
form of data they're sort of
impoverished but closure is a data
oriented language and we have a rich set
of tools for working with these sorts of
things and we want to sort of bring this
back into our strengths right back into
sort of somewhere where we can leverage
all these functions and all these
libraries so to use these sorts of
higher-order functions we can define
first a set of offsets and then on each
of those give ourselves a transform
which will for that offset scale it and
offset it by that amount
we then map that over the input function
giving ourselves three derived functions
and then return a function which invokes
each of them in turn and if we want to
actually kind of reach in to this
hierarchy into the different sort of
sierpinski triangles we can just
generate an infinite sequence and
starting from the trial renderer as the
base case and then iteratively applying
the sierpinski function and so now to
get sort of deep into that we just call
n and so this again works is actually
giving us an entirely equivalent result
to what we had before but as I said
functions are sort of a not entirely
useful piece of data right we can invoke
them we can close over them but we can't
really look inside them they are opaque
and in this case because we are having a
side-effect it's very difficult to test
this right we can kind of you know
examine whether or not our image is sort
of pixel perfect but that's about it
and so we can kind of try to raise our
level abstraction even more do this
entirely with data and so in this case
we would have a list of offsets and from
these you derive transforms that are
themselves pieces of data because of
course they're just sort of little 2d
matrices and so we defined three of
these that will offset and scale
appropriately and now we define a
sierpinski method which takes a list of
shapes and returns a list of shapes
which is three times larger with all
them sort of offset and scaled
appropriately and again to do this we
can just start from the base case of a
list of a single triangle and
iteratively applies to your Pinsky and
have a function that does render all and
this works exactly as you would expect
it would and I am using this as an
example not because I think that any of
you are particularly concerned about how
to draw like two dimensional fractal
shapes but because I think it's sort of
a nice example of this spectrum of
options that we have
and there's a little bit of a truism
that is used which is sort of data is
greater than functions or greater than
macros that's because they're more
general because they give us sort of
greater freedom right and as I said they
give us greater leverage with respect to
the tools the closure gives us but
there's also a counterpoint here that I
want to raise which is important which
is data doesn't do anything by itself we
have to sort of eventually ground out in
code that does some things we have to
wrap over our function our data in
functions and execute our functions
somewhere within our code and so this is
unavoidable and there is a balance here
between sort of the affordances that the
code gives us to be able to actually do
something with it and the flexibility
that it gives us in terms of what we can
do and I want to be clear before I go
any further that this is not a perfect
sort of model right taxonomy czar not a
way to actually perfectly model the
world they sort of grouped together
things that are unrelated they separate
things which are related and most
dangerously they sort of convince us
that we know something that we don't but
at the same time taxonomy is are often
necessary and where they're sort of well
made they give us both the vocabulary to
talk about it and they give us sort of a
predictiveness when the periodic table
was created it had some gaps in it where
it suggested that there would be
elements and sure enough there were and
so the the value of this sort of
taxonomy of this spectrum of data
functions and macros is not that like
it's somehow perfectly models all of our
options in fact there are a lot of gray
areas in between but rather gives us
some way to talk about these problems
and potentially suggest approaches that
we may not have thought of otherwise so
let's talk about something which is a
little bit more maybe applicable for you
guys a little bit more your day to day
sort of operations and this is just
doing transformations on a sequence here
we have a sequence s which we filter
over with the predicate G and map over
with a function f and the relationships
between this the sort of flow of data is
very much encoded
the structure of the data of the code
rather the filter wraps the sequence and
that wraps the filter and that is sort
of the relationship between them that is
a sort of first-class semantic quality
of closure but of course we have macro
so we can kind of play around with this
a little bit right we can use the double
threaded arrow to structure it so that
the flow of data is left to right rather
than inside out and this is very useful
but still is sort of immediate right it
executes exactly as we sort of the find
out when we define it if we want to
defer this we want to maybe execute
within a different context on another
thread we have to wrap it in a function
and this sort of treats these functions
as indivisible right once we've closed
over them there's no way that we can
sort of pull them apart and so a more
traditional sort of functional approach
is to use sort of partial in composition
right this allows us to take sort of
arbitrary sequences of functions and
compose them together and of course we
want to actually use this we need to
then wrap this in another set of braces
and invoke it with the sequence and it's
worth considering how far we've come and
in fact how far away we've gotten from
sort of the essence of what we wanted to
do right what we had before is a very
straightforward description of what we
were doing and what we have and the
bottom here is less straightforward and
this is not to say that one is
necessarily better than the other but
you know we should always remember that
the value of abstraction is not that it
helps us solve any particular problem
more effectively it's that it helps us
solve a wider range of problems but
that's always going to add a level of
indirection that's always going to take
us further away from solving this
particular problem that's in front of us
and that's a difficult balance to sort
of strike it's a it's a very sort of
fine equilibrium and I think that it's
interesting to consider transducers in
this context and I think that you guys
have all seen the talk sort of sort of
learned what these are and it's notable
that transducers are not they are higher
order of abstraction than these sort of
direct invocation of sequence
transformations but they are a very
specific sort of composition they're not
a higher order of abstraction are
actually very narrowly targeted at a
certain kind
of operation that we're trying to do you
can't compose the transducer with any
function and you can't even transduce
with everything kind of sequence
operation certain operations like sort
which require us to realize the entire
sequence are not represented as
transducers so if we're looking at our
code and we have some sort of great big
chain of operations one of which is one
of these non strand useable things we
now need to separate that out have
certain things defined as a transducer
then apply them because of course we
sort of pulled ourself away from the
immediacy we need to now apply our
transducer and then do these other sorts
of operations separately and this is not
to say the transducers are bad I think
that there are really interesting and
very very useful tool but I do think
it's interesting to kind of look at how
these are going to shape the tutorials
for closure because there's a nice sort
of simplicity and immediacy to being
able to say you know map a function over
my sequence I think that it would
probably be a little bit difficult for
someone who is entirely new to closure
to have their first operation over
sequence to be defined as a transducer
that would take a lot of sort of
foundational explanation before they get
going and so there are going to be
inevitably a lot of mailing list
questions and Stack Overflow questions
the effect of should I be using a
transducer here yes or no and there's
going to be a lot of disagreement over
that because again there is a balance to
be struck here which is not at all
obvious and again as I said you know
these transducers they they sort of sit
on this spectrum here and you know they
they sit above the code level things but
they are more precise and that is that
where they derive the utility from but
if we look up at sort of the data side
of things right the same sort of
spectrum exists in that within that sort
of microcosm of just the data and let's
consider a case where we have a sequence
of numbers and we'll assume that is much
larger than this but this fits on the
slide and let's say that we want the
mean for whatever reason and that's
three in this case and we can go and we
can very easily compute this but once we
have that three we don't really have
anything to do with it right we can't
take that mean and another mean and
combine them in any way really this
exists only to display to somebody right
we have a
totally grounded out this is sort of the
executable form of data the presentable
form of data if we do care about
composition we need sort of an
intermediate representation which is a
little bit richer this is a 2-tuple of
the sum and the count and any two of
these things can be added together so
now we can kind of have this ability to
have this downstream composition right
we haven't completely lost that freedom
but of course the mean is not a very
useful thing right and you know even the
mean plus the variance doesn't really
give us anything unless we're really
really sure that we have a uniform
distribution and you know let's be
honest we don't here this is an scones
quartet which is a favorite famous set
of distributions which all have the same
mean and variance in the x and y axes
and the same correlation across both
axes so I mean we need to have a richer
sort of view into our data we can't
reduce it down to a single thing and
assume that that's representative
because it pretty much never is so maybe
a better idea would be to use something
like a reservoir sampler which gives us
a fixed number of representative values
no matter how big the sort of sequence
is and this will give us sort of a
richer view into this data pretty much
for any real measure of that data right
if it gives it sort of a sufficiently
large number of samples and this is true
in every case except for the case of the
mean right we are losing values and
therefore our mean that we calculate
from these values would be lossy and so
this reservoir sampler on the one hand
and this sort of to tupple representing
the mean on the other hand are not sort
of neither as a superset of the other
they are both sort of more useful in
their own particular applications both
are more useful than the single scalar
representing the mean and both are less
useful than the raw data itself and of
course if we can we should always hold
on to all the data but at you know my
employer if actual we have far too much
do this so we we agonize over these
intermediate representations right
because we need to not only encompass
what we need to be able to know what we
might one day need to know and we often
find ourselves going back and realizing
that we've completely prevented
ourselves we've blocked off some Avenue
that would
been really interesting and this is a
difficult problem it's very hard to get
right and this is because you know our
goals are constantly shifting and our
understanding of our future goals is
constantly shifting and so I'll sort of
make a paraphrased appeal to authority
here that you know specializing here
where it's not necessary is not a very
good thing but I'm not sure how many of
you actually seen the original quote in
context and it reads we should should
forget about the small efficiencies say
about 97% of the time premature
optimization is the root of all evil yet
we should not pass up our opportunities
in that critical 3% so what he's saying
and what I'm sort of trying to say by
proxy is a little bit more nuanced then
the cherry-picked quote would suggest
the idea here is not that specialization
is good or bad it's that it is something
that has to be targeted it's something
that you have to be gaining something
from at the same time that the
abstraction that you sort of a louder
main should be giving you freedom that
you care about so let's talk about
something that we're familiar with
regular expressions they're pretty much
the most popular DSL in all of computer
science and they're extremely useful
there are a really concise powerful way
of describing something which is fairly
complicated but they have some downsides
they're only composable through string
concatenation right we can take any two
strings representing a regex and or them
together or something like this but this
is only really composition in the same
sense that C macros are just as powerful
as lists macros which is to say you know
not and you know they're also like these
sorts of functions that we've closed
over their opaque once we've compiled
them we don't get to kind of look
underneath the covers and see what
they're sort of representation of their
operations are and this is sort of a
shame because they're actually built
upon a data model which is very well
understood which are deterministic
finite automata and I started kind of
digging into this when I wanted to write
effectively reg X's for byte streams and
in my sort of naivete I thought oh this
will be fine I'll just go and kind of
adapt you know some reg X thing and kind
of you know hack it together and it'll
you know be a day
and once I started to kind of dig into a
TomTom theory though I realized that
there was a much richer set of tools
there than I'd originally realized and I
tried to encompass this in a library
called automat which is sort of a
full-stack implementation of automata
theory exposed as a set of Combinator's
but then you know needs actually exposed
a sort of execution model which is
reasonably fast and reasonably
expressive and the way that it specifies
things is effectively through the inputs
that it will accept and so this vector
here represents concatenation so it will
accept the sequence one two and three
and we see that at the end there these
sort of double ring represents an accept
stage we have matched this pattern and
we can combine these through the sort of
typical things we can order to gather
this and so we know now will accept
either 1 2 3 or just 1 and 3 and we can
even do more complicated things using
you know sort of familiar reg XE
operators like the question mark clean
star and plus and so this will accept 0
or 1 1 0 or more 2's and one or more
threes and they can see that this fairly
complex operation is reduced down to
only three states and a fairly small
number of transitions because within the
Tomica theory there are all these sorts
of tools for minimization right we're
able to take fairly complex things and
sort of reduce them down to their
essence and of course this can be
combined arbitrarily with any other
thing that we come up with but having
accomplished that you know again within
this sort of spectrum that I have you
know described I'm quite a bit away from
you know actually accomplishing anything
right I've implemented automata theory
and now I have these series of data to
data transforms but I need to get down
here and in fact that's very hard to get
down there because regex is sort of
occupying a privileged position within
the closure ecosystem right they are
automatically compiled for us which is
not something that they have in Java and
so I would not want for anyone to go and
run away and start replacing all the Reg
X's feverishly with this sort of you
know new abstraction that doesn't really
provide any value reg X's are a very
good solution for their problem and they
have affordances as a result of this
sort of
reader literals that library can't
really sort of match but there are
places where the sort of freedom
afforded by this is quite useful one
that's been sort of discussed recently
is sort of HTTP routing libraries
notably in nodejs at Netflix and they
were just kind of going through
iterating through a series of reg X's
and this is a place where composition is
actually very powerful right and these
sort of additional complexity given by
the indirection the fact that we first
have to specify then compile then
execute these things is okay because
they sort of gain us this amount of
expressiveness that we want and we're
willing to sort of bear the additional
complexity another sort of a very common
example of side effects is the
conveyance of data right this is
something that we do and often it's
hidden away from us so a ring handler
will allow us to treat a web request as
a pure function of course under the
covers
it is both receiving and sending data
for us and we may in our ring response
give an input stream which means that
there's an implicit advance there that
we're sort of kind of waving our hands
up like squinting our you know eyes and
trying to like ignore but it's there
right we are side affecting the world
and because you know most of what we do
you know is sort of Network aware
there's tends to be this big fluffy
thing between us but it's it's pretty
much the same thing right we have these
processes which are feeding into queues
and things which are reading off of
queues and sort of feeding it forward
and you know accuse or make what makes
the world go round but what happens is
that when one of these processes stops
maybe because it's in the middle of a
sort of complex computation maybe it's
doing garbage collection
maybe something downstream of it is no
longer accepting data it stops reading
which causes a queue upstream of it to
fill up which causes the process that's
feeding to that queue to stop which
causes the cloud to get dark and stormy
which cut which you know causes the
thing that's trying to feed into this
because you know TCP is among other
things in a queuing mechanism right
there is a first-class back pressure
mechanism and we can affect other
computers by sending too much data or by
sort of refusing to accept it right that
politically causes back pressure there
and so then this sort of continues on
and on and on and I think it's notable
to sort of look at how that works how
the sort of code that you know causes
this sort of behavior it looks like and
here I have sort of hypothetical
functions take bang and put bang which
are both blocking take will block until
it has a message put will block until
them its message is accepted
I notice that nowhere here do we have an
enable back pressure call right nowhere
have we said you know okay well we're
blocking so now we really need to stop
accepting rights rather this is sort of
in emergent sort of implicit property
here and the reason that it happens is
because there's an order of execution
right the take precedes the put precedes
the next take and because there is an
order of operations one of these
operations taking a long time implicitly
affects the other one right it causes it
not to occur and so back pressure here
is this emergent property because we
have this structural relationship
between this it's just that one happens
before the other and that one can cause
the other not to happen and if you look
at you know the quarry of sink
equivalent it's exactly the same right
we've changed a few things the put and
take look a little bit different but
otherwise this is an entirely identical
thing and we don't use threads because
very cleverly we have sort of ripped the
code apart at the seams
right wherever there might be something
that causes our code to block we stop
and then wait for it to you know happen
and then sort of carry over into the
next chunk of our code and yeah you know
this is actually something which is
entirely tentacle to the first one it
says that we're not using up a thread
and the the backpressure is again
entirely implicit there's no first class
representation of a thing which hasn't
happened yet and that's because it's
something a little bit more fundamental
right it's not just that the
backpressure is structural it's that we
have these cascade of side effects these
cascade of events and they are causal
right one allows the next thing to
happen and that causality is encoded
within the structure of what we've
written and in this case is perfectly
fine but it's worth considering that
complex causality arbitrary complex
causality calls arbitrarily complex code
structure right and the only sort of
tool that we have to work with that is a
macro because we've sort of this is the
the medium with which we are trying to
express what we're trying to do and this
is not mind you sort of an attempt to
say that quarry sink is bad for any
reason as I said you know having this
sort of immediacy is very valuable this
is a very direct sort of representation
a very easy to understand representation
what's going on but I do want to point
out that this is not an inherent
property causality doesn't have to be
structural and so I'm in the habit of
writing libraries that deal with in
network and I was writing them before
quarry Singh came along and then quarry
Singh came along and you know took the
world by storm and I was kind of
spending a lot of time trying to wrap my
head around and understand exactly how
this should work and the conclusion I
came to which is highly debatable but I
believe that quarry sink is an
application-level
abstraction because it brings along with
it not just these sorts of stream
exchange abstractions but also an entire
execution model and this is something
which is perfectly fine there's nothing
wrong with the execution model but while
as the application developer you can
choose to use that I think as a library
developer it's not fair of me to choose
that for you because it's very hard to
opt out and that's something which
necessarily affects you know what
libraries you can use and whether or not
they play nicely with this particular
execution model and so I wanted to make
something which was a little bit less
opinionated and I think as a result
maybe less directly useful but it allows
it's sort of representations to be
coerced into quarries think of that as
in fact your wish and allows it not to
be if you'd like to avoid it and so
there are two basic concepts here which
are called streams and deferreds and if
you're a familiar quarry of saying this
would be channels and the newly-minted
promise channels a stream is lossy right
you take a message it's gone deferred
represents a single unrealized value and
you can sort of consume it as many times
as you like and it's worth noting that
in this sort of case right the order in
which I take messages from a stream
matters a great deal
the the causality of these things is
sort of first class with a deferred it
doesn't matter right I can sort of hand
this tomb as many people they can
consume it in whatever order they like
and I'll sort of all come out in the
wash and so this is an important
distinction I think these are not sort
of interchangeable they have very very
different properties and notably when I
take something from a stream when I ask
for a message it will return it as a
deferred it will say here is the
eventualities of a message it might be
immediately realized but now I have
something which represents a thing that
I might not yet have this is a
first-class representation of something
I will at one point get and the
something that I can pass into other
functions this is not something that
just exists between the seams of my code
and with this I can kind of do the
normal sort of composition I can chain
on this which allows us to execute
functions once a value is realized and
here I'm giving a future because
manifold tries to sort of accept
anything which looks like an eventual
value so we can give it a closure future
or closure promise or a host of other
sorts of things and here we chain on the
eventualities of a1 and then increment
it twice and this will give us a thing
which will eventually yield 3 and in
this chaining we can sort of introduce
additional sort of eventualities we can
say here's a one that will eventually be
available and then later on it will
eventually increment it and then it will
immediately increment it once that's
available and this will give us the same
result we can also dip different things
together and we can give it one and then
two and then three all in sort of
different forms which are either
realized or unrealized and it'll give us
some a eventualities of the list of one
two and three and this is again not
necessarily more useful than the sort of
more direct code representation that
Cori sync has is different it's a higher
level abstraction you can do certain
things with it but it's a less direct
sort of specification one thing that
this is useful for though is a data flow
so often we will have a service which
gets requests and is requires to go to
multiple backends to do this and we
would like to do this simultaneously but
as I said because core async has this
sort of linear flow it stops whenever it
doesn't have a particular value it's
very hard for us to get this exactly
right because there's an order in which
this all needs to occur in order for us
to get sort of the optimal concurrency
and the promise channels that are
inquiry Inc which I haven't looked at I
am only sort of found out about these
few days ago but because they're being
treated as channels as things which are
sort of have causal sort of
relationships with each other we can't
just kind of go and wait for that to be
done and sort of continue on on another
line of computation right we have to
kind of do each thing in turn and this
means that we have to make sure the
order in which we do it is just so or we
will get sort of a sequential execution
of something that could be parallel and
so one thing that we can do in manifold
because we have this sort of stronger
distinction between things which are
streams and things which are eventual
values is a macro which I've called let
flow which allows which will basically
walk the entire let binding figure out
what the data dependencies are and
execute them in the correct order right
and some of these values will be used by
multiple things it doesn't matter we can
kind of freely rearrange this we will
get the the optimal concurrency and many
of you may be looking at this and
realizing that it looks a great deal at
least in intent to prismatic scraf which
is a very similar sort of thing is
trying to define a series of data
dependencies will execute these things
in parallel that's a very nice library
and it also you know it allows us to
kind of represent this as data rather
than as this sort of code and I want to
not necessarily say that one of these is
better than the other but I will point
out something that we say that code is
data but I think that this is not a
particularly useful observation right
yes it is data but code is a very
particular sort of data right it's a
very narrow subset of the possible data
that we could have
and code is data which has semantics
that we all agree upon right we have
learned how to write closure we look at
Alette binding we know what it means
when we look at data we don't know what
it means because it doesn't mean
anything right that that is defined
further down the spectrum and so as you
know we're talking about sort of core
async and the fact that it represents
these things as a code structure rather
than as sort of values and when I talk
about prismatic which represents it as
data rather than as Lillet binding these
are all trade-offs which are
fundamentally trying to figure out how
many affordances how much you know sort
of direct applicability do we need to
give for this to be something that
people are going to use because that's
what it is to try to you know make this
composability composability is not the
question of are people able to use two
things together it's are they willing
are they going to actually put in the
sort of effort and that is sort of
necessarily a factor of how easy you
make it how good your documentation is
how clearly you explain what it is that
this is meant to accomplish and so I
yeah you missed this on the title slide
uh so I I'm here to talk to you about
how to be composing and I've talked
about a wide variety of things and know
we're in this talk did I tell you how to
be better
composers right how to enable better
composition because unfortunately the
answer is that this is inextricably tied
up in what you're trying to accomplish
what I want to kind of do here what I
hope to have accomplished here in this
talk is to explain sort of the the
different forces at play right give sort
of vocabulary for talking about this and
I hope that this will just kind of begin
a conversation right and and I hope that
people who have sort of opinions about
this will come up to me and tell me you
know about their particular problem or
telling me that I'm completely wrong
about this or that you know my taxonomy
sucks like whatever you know is a result
of this I hope that we talk about it
more because this is what causes our
ecosystem to flourish or dwindle right
the degree to which all the disparate
pieces can work together
and you know as I said a lot of this is
actually not the code itself but our
description of the code the clarity of
our intent both as communicated to
others and to ourselves and so I think
that you know this is something that we
should all think carefully about and
discuss and debate and you know I look
forward to seeing what comes of it thank
you
yes the question was where do I put
monads in my spectrum um I mean like
somewhere between functions and data
right I mean I I it's it's it's
something which is more meant to be
utilitarian than all accompanying so I
don't know I someone who is more you
know has used functional programming in
that sense in anger more than I have
should probably you know say yes
the question was how does automatic
compare to regal regal is a finite
machine compiler which allows you to
specify finding machines and compiled
them down to C or Java or other sorts of
things I was highly or heavily
influenced by regel Ragle though is
something which is again sort of it's a
it's a DSL right it's something which
you can you know write down in
cross-compile I was hoping to have
something which is a little bit more
freeform allowing you at the repple to
kind of you know bring things together
and sort of freely compose different
pieces and so it's meant to be sort of a
common Eider version of wriggle
effectively okay I think that's it
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>