<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Alice Liang -  How to grow your own distributed array database | Coder Coacher - Coaching Coders</title><meta content="Alice Liang -  How to grow your own distributed array database - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Alice Liang -  How to grow your own distributed array database</b></h2><h5 class="post__date">2014-03-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ps_HwCvAcfk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello hi my name is Alex Lee I work for
this company called the climate
corporation I'm here to talk about how
to grow your own editor a database okay
so at the climate corporation we have a
big problem it's a big data problem we
have tens of terabytes of data and we
need to be able to store this in a fast
way or you need to be able to store this
and write this fast we also need to be
able to query for this fast so this data
you need to know some things about it
before we actually really go into the
database part so here's just some
satellite tiles and one of the key
concepts is a what is the data set so a
data set is a collection of data so for
this is we could call this satellite
data and it will have some attributes so
attributes are just additional
information so that you can have some
context about what to do with the data
like where did it come from like so the
location is probably the United States
it came from satellite company unnamed
and we got this data on May fifteenth
2014 and for this data we have some
variables so these might be describing
different things in that satellite data
set so this is a name to reference data
so this could be temperature it could be
something else like the amount of red an
image or something like that and to
reference this data we need some minimum
number of coordinates to specify a point
within that so here you can see is two
dimensional since this is geographic we
might call those two dimension
latitude longitude so this describes not
only the number of dimensions in the
data but also what a dimension like it's
an axis for which you can define things
on and so what does this do to look like
so a one dimensional data set could be
like the price of a stock over the
course of time or we could call this
temperature for San Francisco and what
this looks like enclosure is probably
just a flattering or a vector great two
dimensional this could be temperature
over all of San Francisco which is like
a seven by seven mile grid or this could
be your raster data like an image we
could represent this enclosures maybe a
doubly nested vector and this would be
dis and what if we have what would a
three-dimensional data look like so this
is like temperature over time maybe or
raster data or images over time which we
know as videos and we just introduced
more more layers of nested pneus but
what about four dimensional data well
this could be like temperature over time
at different elevations so this is
changing of a 3d model and this is just
the beginning of the dimensionality of
our data we have five dimensional data
so it's like forecasts of temperature
over time at different elevations so you
need to have to represent the future in
multiple ways and maybe we have more
dimensions that we care to represent but
we don't know about it yet who knows so
this is part of the problem another
thing about our data is that a lot of it
comes in this format called netcdf so
this is a self-describing machine a
dependent data format and the reason we
care about this is because it comes with
many third-party tools that other data
scientists and weather scientists are
ready written for us and these all
written in other
languages like C C++ Fortran etc and it
would be really really nice if we could
use these tools and when when you store
the data so what applications just this
have well we can store weather forecasts
oceanographic modeling geophysical
modeling the list is endless as they say
so what does this data look like well
this is another satellite data image and
this is the original image that I saw
and at the heart of it we're all these
datasets look like are these just
endless big matrices so this mage a
matrix is two dimensional so three
dimensional data sets or me like like
tensors and then we don't really have
anything to describe anything that's
more than three dimensions but our
database will oh so we need to have a
database second take all these into
account so what do we do is generally
with large arrays well you don't
generally store it I mean for you guys
if you don't work in for a very large
arrays arrays are just things that you
have in your code like great it's like a
hundred elements a thousand maybe a
hundred thousand if you're being brave
but you're really not going to be
working with very very large arrays but
we do so how do we start well if you're
like Google Maps you don't want to store
you don't want to store everything in
one big file that's really bad you don't
want too low the entire Earth every time
you go to Google Maps that's just you're
going to have a hard time rendering it
and so what we do is we're going to a
different blocks and we can maybe store
them as separate files or separate rows
okay so what happens when we try to read
it in this way so say we store some data
in s3 it's these are just it was just a
big thing and we sorted we've split it
up to four tiles and just for reference
these the original tiles maybe like five
thousand by five
axles so you're looking at 25 million
points here so and if you want to add
some dates to this then you probably
want to store it like in different
buckets right so if april 2013 may 2013
june of last year great so say we want
to get April data from last year for
tile one so I'll going to do is we read
so we read one file and it's great we
don't throw anything away everything is
useful to us okay say we wanted to get
all of the damage that's 13 for that
tile that's still great like this works
perfectly for our needs but say we want
to get that little bit because your
google maps and you don't want to render
at the entire thing your client or user
is only looking for that data at that
part now you have to get three files you
sure most of those three files away and
so that's just a big waste right you are
sending a lot of weight over the wire
you're so I moved away and not only that
you have a lot of extra space that now
you've wasted like reading that data in
and that's just little slow you down so
that's not good what if you wanted
before and we also have this problem of
what if what did this data look like
last year like what have we've changed
it we can't get this if we just sortin
us three we just over eight files and
what if we wanted not just three points
what if what if this data set has data
for old last 30 years that means we have
to throw away a lot more data just just
because we want to render some like time
series of how things changed overall
with all these tiles and the worst is if
you have a query that spans
multiple titles and this is pretty bad
when you're on 2d space but when you
have many more dimensions like four
tiles turns into eight and then 16 and
then pretty much you're going to be
throwing almost everything away just
because you want some little piece so
what are the solutions right way we
could break this up by pixel so we can
have if we start it is in a file system
that will have like 25 million time
series for just one tile that we've
broken up and imagine we have maybe 20
thousand images like this like how is
this going to scale not very well you're
gonna have a hard time also when you're
when you ingest more images then where
you have to do it you're gonna have to
break them up you're gonna have to pull
all every single one of your time series
update the very last one and then write
them all again so that's also really bad
so another thing you want to do is
probably want to write this if you store
it in that pixel data structure way you
are not going to preserve the original
data set structure obviously because
you've already broken it up and so this
makes querying for it in a like in a
very non-intuitive way you're going to
be querying many files or rows throwing
most of the way just because you want to
represent it as maybe like a image or so
the solution maybe is to assort both
ways but that's very very hard to scale
so this brings us to look at the summary
of what I've what our needs are so we
have a lot of data we have terabytes per
data set I think one of ours is maybe
five or six terabytes for just one
arrays so still give you a sense of size
and we want to create we want to read
fast we want to write fast we want this
to scale the face of increasing data
volumes because we're cost
dating this and we want to have many
query we have many great patterns not
only are we going to courage
geographically we want to query in the
time series what if we want save and
we're like the forecasts what if we
wanted to create create all of the
different versions for this point like
that just is not sustainable in a
traditional tiled format what we also
wanted to know what things look like
last year like some fast food of the
data we also wanted to use those
third-party tools they mentioned that
work with that cdf so when we looked
around there weren't any great existing
limitations so obviously the solution is
to grow your own so today I'm
introducing Doc Brown at the climate
corporation we like to name a lot of our
services after characters from back to
the future so here he is and these are
the explicitly the features that we
wanted from Doc Brown so fast reads
wanted distributed rights because we
have very very large volumes of data who
wanted this to the immutable version and
scalable so that we have cheap updates
and we wanted this to be easy to extend
for other backends and I'll get to that
so before we get dive deep into how doc
round works I wanted to do a demo so you
can see what it looks like to store and
query from a neuro database
no he saw the ending okay so we have a
service that we have run to serve up the
data to do so we're going to four so
this one takes a while to start up maybe
it's because my computer's slow this is
just a video so I'm not doing any of
this life but basically what happens is
that we're going to ingest turn 250
megabytes into in memory store and then
we're going to query it through Python
actually so fine that has a netcdf for
library and good thing you saw the image
at the end because that's what you get
out great so done with the demo so some
key ideas that I wanted to get across so
doc brown is a dub data store so its
doors an array as an array it doesn't do
anything fancy with the data it just
stores it as it comes in the data is
stored in chunks which I'm going to get
to the data is versioned Doc Brown is it
library not a service and we have this
concept of slaps and slices so what a
query looks like is very much similar to
like a Python slice so enclosure you
would have a reader and then you get
from it a start and stop so this is long
each dimension and what you get back is
something that has data in it and has a
label of slice to tell you where the
date is located so we got back some
factors or a vector or to dementia
lector and it's starts at zero zero and
stops at 35 so one of the first design
decisions was the doc brown is a dumb
datastore and so metadata is essential
what does this mean what is metadata so
metadata provides basic information
about the data it provides context like
what I do with this array like what is
it
like okay so so so we have some
attributes which provides you relevant
information and you can say that oh this
is the dimensions that my dataset is to
find upon and this is how long it is and
we have some variables so these are
these are the raise themselves and you
have multiple arrays per data set and
there are three attributes that each
you're a must-have so that's a type
field value and shape so shape is just
saying I'm to find on these two
dimensions or fill value which is for
how we take care of missing values when
the data comes in so what metadata looks
like it's just a closure map so we have
some attributes so I own this data we
acquired it Jenny first as a 14 and
maybe a source of public and dimensions
are XY and time and it's 100 500 x 365
and maybe we have two variables like
precipitation and temperature in this
data set and so we can see that if types
there Josh Droppa types and their
different fill values because of how the
drama types are and the defined on XY
and 10 books great so another design
decision was that we trunk the data so
what does the chunk house is different
from a tile like well so I have some
hand-drawn diagrams here because I got
really frustrated with my editor so we
be nice hopefully it's legible so so
chucking a dataset so this will be very
similar to chucking a data set when it's
or when you're just are storing a notes
like different files right so each
element is probably those one of those
boxes and maybe we want to query for
like the boxes that are colored red so
you don't really want to read the entire
thing so we can do is we we chalk it up
so
great 0 1 2 3 right yeah so this is very
similar for two dimensional arrays and
we break it up so that each child has a
strong coordinate and Kurt you know
which one to pray great so when checking
is different it's when it's
three-dimensional like what if you want
to crave for that redbox you have twice
the amount of data because you've
introduced another dimension to it or
however you have more than twice amount
of data so we do is we also chalk it per
dimension so you can kind of see that
each chunk is to find on three
dimensions so if you want to just like
the red one you only get the red box you
want the red blue the blue data you get
the blue box so you only query 1000 or
111 and that's one-eighth the data that
you would have to have query if you had
the entire look entire data right so
what this means is that when we query
over any of these dimensions they all
have the same performance because you're
going through the same number of chunks
you're not and we can make we can
optimize these chunks sizes such that
your according the least amount of
chunks possible so this make can make
your reads very very fast especially if
your data set is a terabyte so another
design decision was that we think of
data slabs and their indices of slices
so so pretend we have this two
dimensional data set and imagine that
lower left corner is zero sir and so we
actually that flat I had earlier was a
lie so we actually it's not just a
closure map it's a record that's called
a slice and so has a start and stop
and we have this concept of slabs which
is the data paired with a slice which
tells you here's my data and this is
where I'm located this is very useful
internally for passing around the data
to reconstruct what data you want so for
an actual read you would get do a get on
this we have to slice and again got you
back a slab and this is the basic
contract of the library API so another
design big decision was that we version
the data so what does this mean this
means that we can see what they do look
like at any point in time so versioning
persistence mutability what does that
sound like closure this also sounds a
lot like to date comic as well but so we
have ways to do this maybe it'll be
similar to the way to tell my does this
because it's very similar to wake
closure does this in the way we do this
is by having an inverted index so
remember chunking so say we have a
two-dimensional Chuck so we have some
chunk coordinate that tells you where
that chuck was that points to some like
binary data and so maybe we won't have
an index to keep track of that data but
what if what if the data is the same for
each like binary we don't want if we
wanted to do that we wanted to duplicate
the data so we can solve this by hashing
the data so this makes a comment
addressable so if say you have a very
very sparse data set one of you by
sparse is that most of it is probably
going to be the same so an example of
this we like precipitation especially
precipitation over a desert like it's
not going to rain most of the time so
you probably don't want to store all the
zeros
that's just a waste of space so we can
do as you hash it and so they're all the
data that has the same data has do the
same hash so we can do is you can point
the toe coordinates at the same hash and
then you solve that problem of
duplicating a lot of your data and this
is another way we can represent it so
instead of having those truck or dance
with their own little thing there just
pass on the tree on the tree so we also
want to version this data so we add
another version to it so we have a
separate index / variable because each
array is different alkyl Odyssey and so
the scheme gentle general schema is we
have like several one-to-one mappings so
we have trunks which map hashes to the
binary data we have an index that will
match an indexed and a coordinate to
some hash and we have a version that get
from one version ID and a variable son
index because variables different with
Indian is so we borrowed this concept
from closure so closure has this thing
about persistent factors and it's very
very memory or space efficient because
of how the shirt structure is so when
you insert or when you append then you
just write know that so obviously this
is a binary try and closure has 32
instead of two but so remember so this
is what the doc from general schema is
represented as a tree so when we add
another version we just point not we
don't just point to the old chunk
coordinates or the chomp hashes so their
old version had 0 0 1 0 0
11 and we're adding 0 to 2 and 12 so
there's actually no need to write all of
those extra ones we took it'll
okay great so what this actually looks
like we have this logic such that when
we update the second version well then
there's no need to look at things I've
never written so we have a logic to look
at a pre the previous index and then by
then we can get wood chunks live where
hope that made sense shared structure
it's great so what this means is that
when we update just like that one chunk
or if we just insert something at the
end for us we have to weather data
coming in every day this means that
small updates are cheap in the sense of
the grand scheme of things when you
already have like three terabytes of
data there's really no need to rewrite
any of it great so another design
decision was that dark brown is a
library and not a service so what this
means is that Doc Brown is just a client
library where you can read and write
things and there's no surface that's
hosted that will take your like a web
queries and like serve it back to you so
what does what does lie we do so this
manages state which means that give them
some data set you worry like where does
this all live how do I get it since
manages your data access to some caching
so you can write data through this this
points at some pluggable back end which
I will get into and because it does that
you can decouple the logic and this is
actually very similar didja today domick
how many guys know the atomic or great
this is good audience so instead of
having the like client library and the
treads actor
back end we just have the clan of the
trans actor for us is the same and we
have some third-party beckons so the
library just sits on top of the backend
API so we have for these are all
pluggable in switchable so we have form
of notations of these back ends in
memory file system and sqlite and
Amazon's dynamodb so dino DB is what we
reuse in production we use memory or
file system for like for testing
purposes like because we have to pay for
identity and these third-party beckons
must have some atomicity and consistency
guarantees such that we can achieve some
of the features that we want why do we
not want to service 24 hour reads and
writes well what this not having it as
it library gives us is that there's no
like grouping machines that are
bottlenecking s like what if we get a
lot of data flight requests for data at
the same time if you have a bottleneck
that means oh no I can't serve you right
now because there's other two men one
surveying so a wait so what happens this
leads to really really want Layton sees
so if this is just a library then all
you're defacing with is the database and
so the only thing that you are would be
bottlenecked by would be your back end
and that's how much is your problem to
solve then having some third more like
service in the middle that links a
middleman saying no you can't do this
right now so this gives you a role
elasticity for demand like you don't
have to there's a little one less audisi
only crew pft manage so this is one
other week this is the reason why and
also because we've two coupled it back
end this means that we have already done
the optimization for you in the sense
that so the backend API that the backend
API plugs into our library so
yes because the back in library plugs in
the back in API plays into our library
so that we've already solved all the
problems so what like what Doc Brown
does very well is it manages the
chunking very well and there's a lot of
race conditions that we have to be
careful of so if you write this basic
thing then you get all of the other
things for free and this also gives us
lower operational costs because it's
easy to test on their back end so I
think it took someone one day or two
days to write the note it took someone
on my team like maybe one day to write
be a memory store it was very easy and
we could test in memory so our academia
is in with closure protocols and so if
you want to write a new back end all you
have to do is implement all those
protocols and you have yeah you have a
new band that's easy right so this is
the general how this is how general
things are a lot this sorry word vomit
this is how if you had a memory store
and so what you get is this schema the
schema is a protocol that manages
creation deletion of like tables for
example how's your dataset and so from a
schema you can list your data sets and
you can get maybe you care about
precipitation so this gives us
connection to some data set and then
given up some version of that data set
you can you get to see the chunks in the
index which is how you will get the data
so the high level library is what uses
all of this functionality to give you
back the data so if you want to extend
all you have to do is implement these
four basic particles
so another design decision was that we
read and write it in a session so what
this means is that reads are separated
from rights so reads are just pegged to
some version so that's the state so
there's no court there's no need for
coordinating because you're just reading
what you see and there's no chance
that'll be updated so this gives you
really nice reads and you're not really
bottlenecked the more important thing or
the more interesting thing is
distributed rights so what this means is
that rights are not really held up by
queries and because these are also tied
to a version but you can ride to from
hundreds of machines there's there needs
to be no coordination among the machines
how do we choose this we do something
called a little optimistic concurrency
control so optimistic concurrency
control assumes that everything is
golden you can up there's no there's no
lock you can update the state of the
world if and only if when you update it
your view of the world is the same as
when you started when you first read it
so there's some limitations this
approach because we only have linear
history there's no branch a but for us
that's not really an issue because all
we do is try to maintain one state of
the world we just have to have versions
because we need to be able to reproduce
what the world look like yesterday but
the world the future only has one we
only have one future so I'm saying the
other thing is that because of the
optimistic concurrency control so say
you have a really really long running
right and you haven't like committed
your version yet if another version
slides in and finishes before you do and
you're trying to commit
version then you can't because the world
isn't pulled under you what if you saw
what you thought was true is no longer
true so you do what happens is that you
have to start over again and so there's
a lot of garbage collection that we have
not implemented yet so there are some
other bells and whistles we've wrapped
so the index has been cashing in the
trunk store has some caching as well as
compression this is just helps us for
compressing link image or raster data or
other data and so these are all people
that worked on this project with me I'm
not the only one I sent the privilege of
talking here today and so I think I have
six minutes for questions and I think
I'm going to if my computer works we can
show you the demo otherwise it'll just
be questions thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>