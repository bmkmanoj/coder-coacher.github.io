<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Timothy Baldridge - Data All The ASTs | Coder Coacher - Coaching Coders</title><meta content="Timothy Baldridge - Data All The ASTs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Timothy Baldridge - Data All The ASTs</b></h2><h5 class="post__date">2014-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KhRQmT22SSg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Tim Baldrige I work for a
cognate echt I'm actually based out of
Denver Colorado
so a little farther the west coast than
most of cognitive tax people but I want
to talk a little bit today about a
contributor and specifically this is
gonna be a talk about some recent
developments in the compiler area of
closure but I need to start this with a
little bit of a disclaimer and that is I
have written basically well not
basically I've written none of the tools
analyzer code so what we're gonna see
today is actually code that I've written
just kind of showing off the library and
how it works
I contacted one of the current
developers Nikola momenta and said hey I
would you like to come give a talk to
closure West about this tools analyzer
stuff I think it's pretty cool and he
said yeah I'd love to but I'm in classes
on I live in Italy
so you're stuck with me today but
primarily what we're going to talk about
is one of three phases of a compiler in
general compilers can be broken up into
three pieces
they are the lexer and parser the
analyzer and the emitter closure kind of
follows this pattern to some extent and
we can we can see the first step of this
compilation process the lexer and the
parser if we just call read string on a
string that contains something that
looks like closure code and what we get
out of this is a list with a simple plus
and the numbers one and two some
compilers for various reasons if their
syntax is a little more complicated than
a list will actually break up the lexer
and parser into two distinct pieces but
in closure like most lists these are
combined into just the reader the
closure reader so the output of the
reader is a list with some symbols and
some numbers in it in this case that
doesn't really tell us a whole lot about
what this is it doesn't tell us if the
plus is perhaps a literal symbol or a
reference to some function or something
of that nature and so this has to be
analyzed and we need to have enriched
information about this list for the
compiler to use know
what we're familiar with when we work at
the repple is the read eval print loop
right so the analyzer in the emitter
normally when we work with them are
combined into a single function call
called eval so if we call eval with that
list that we got out of the reader of
one until we get back three but
internally inside eval there is actually
a two step process
the first process looks at that list
that we handed in that form does some
analysis of it creates some enriched
information and then hands that off to
the emitter which emits the actual Java
bytecode we can actually dig into the
compiler and find that code using Java
interrupts so here we're gonna call
closure Lang compiler analyze hand at
this constant that we won't worry what
it is right now and a form plus one and
two and when we run that we get this
which is an internal class to the
closure compiler now if you were to go
and look at the closure compiler code
it's only about 10,000 lines of code a
lot of clothes and private members on
that it works really well for what it
was designed to do and that is be a
really good closure compiler but it's
not really open to extension there's
really no way for you and I to go and
write a analysis pass or an optimizer
pass without actually recompiling
closure from scratch and somehow getting
our changes committed to the code base
and go through that whole process now
it's interesting too about these classes
are very closed they're private they're
inside the compiler we can't really mess
with them at all and it turns out that
this is actually the way that most
compilers are built in the real world at
least most of the ones I've experienced
so as an example I went on to the
internet and well first of all here's
some other examples of AST s or these
internal classes we have a number
expression we have a static method extra
expression like we saw before and a body
expression which is what a du ends up
being after we analyze it so anyway I
went onto the internet and I found this
example here of writing a toy language
called kaleidoscope which is used by the
LLVM team to kind of show minimal
language it's really easy to implement
and somebody ported those tests this
tutorials to Haskell and this is their
ast now this is pretty clean but once
again we have this statically defined
set of expressions and their types
here's an example from dotnet before I
became a closure developer I did a lot
of this sort of stuff for fun and for
profit and which is basically here we're
going to construct a function called
it's a predicate that says if the
argument is less than 5 so we can call
into the link expression library that's
part of the standard net now we can
create this ast and at the end we say
compile the ast and we get an actual
function so at runtime we can
programmatically generate code in
c-sharp and it's a really nice feature
once again
sealed classes defined and blessed by
Microsoft we aren't really allowed to
touch them and here's the example same
sort of thing different language you
know camel now what's interesting with
all these examples they have something
in common they all use types not data
and as for a couple years now I think it
was maybe three years ago at a closure
con someone started this whole thing
about data everything data all the
things and it's kind of been the rage
ever since and it's really one of the
the core values of closure is that we
should represent everything as data so
that we can manipulate it with a common
set of functions we should be able to
associate to our ast s this soch
all these things that we're used to
doing so what would it look like if we
started representing code as data on an
AST level to represent this intermediate
ladder layer after it comes out of the
the analyzer as actual data well an easy
way to do a constant would be something
like this we have a hash map we have an
OP that says this is a constant we have
a value which is in this case 42 and we
say this is a type of integer so that
whoever looks at this in the future
doesn't actually have to say well what
is the value is it a float a string or
whatever we're going to just kind of
analyze that once and attach this
information
well that works pretty well so how about
an if here's an if op if we have some
metadata here of the lining column that
this form was found at and then we have
test then an else which represent the
three branches of an if we have the test
that we're gonna do our conditional
check on and then we're gonna go to
either the then or the else branch now
this is the way that closure script was
originally written and I'm not actually
sure I did a little research to try to
find out who it was that originally
tried to represent close your ast zazz
data and I really didn't come up with
any good answers because it's it kind of
came out of the hole closure script
development but Richards involved so I'm
going to blame him because we can blame
him for anything in closure but anyway
this this method of looking at data
looking at a STS works fairly well but
there's something missing and I think
this is a key point when we start to
look at representing data to our to our
our programs and that is there's almost
not enough information here there's not
enough information here for future
passes to traverse the tree of data we
don't know which of these members are
child nodes in this case meta is not a
child node it doesn't actually get ever
get executed it's just meta information
test then and Alice however our sub
knows and so it'd be nice to be able to
express that to a program so that we
don't have to actually hard code that
information in the code itself and say
if you're on an if node you need to
recurse into the test that an else so
what we do is we attach another entry to
this hash map called children and the
important thing to notice here is that
children is actually a vector when I
first looked at this I think why don't
we use a hash set it's actually a
collection of child knows that makes
sense but a vector is actually permanent
parts even more information to this ast
through represent the children in
execution order with the exception of
then and else will only ever be executed
either one or the other but we'll ignore
that part of it for now
but test will always get executed before
the venn brain
or the else branch and I think this is a
good example of using a data type which
vector for instance is an in order of
data type using that data type to impart
even more information that our code can
use so if we wanted to say for instance
execute a function on every single node
in our tree in the order that it would
be executed by an evaluator or an
interpreter or actually when the code is
compiled it's very easy to do that
programmatically so when the tools
analyze our library there are actually
four required semi required it's not
enforced but four recommended entries
for every single note in our ast the
first one is the OP which provides
polymorphic dispatch on the node the
second is the form which is the
unanalyzed form as it existed before we
ran the analyzer on there and it's there
for reference than if we need to use it
for something children is what we talked
about child nodes in execution order and
then environment is the full state of
the analyzer when that node was created
and that's a pretty big thing to think
about that we have the full state of the
analyzer when this node was actually
generated now it seems like that would
create a ton of memory usage but this is
where the power of immutable data and
structural sharing steps in because op
is just a keyword so we're not too
worried about that form is a pointer to
the original form it's not actually a
copy of that form it's just a pointer
into that s expression children is most
often a constant every single if node is
going to have test then an else no more
no less that's a constant all that if
nodes are going to point to the same
vector and environment is going to
change very rarely just certain parts of
it will so the parts that don't change
due to the way persistent hashmaps work
those parts would just be kind of copied
around so if we define an AST like this
we can define a function like I have
here called post walk that starts at the
children and executes a function on
every single node walking up the tree
we can then create something call we can
then just print the create a function
that prints each known or that the type
of each node and we should see if we go
back up to our example here we have to
Kant's inside a do so we should see
Const Const and then the du which is
exactly what we see alright so for some
examples today and we'll actually load
up the closure tools analyzer JVM
library so that the what originally
started as made backup right a little
bit history here so a lot of these ideas
of how to write compilers started in the
closure script project about a year ago
there's a Google Summer of Code project
and some students took that analyzer
code and pulled it out and started
working on what they called closure in
closure which was a reimplementation of
the closure compiler and analyzer
enclosure itself around six months ago
or so that was adopted as an official
contributing to three pieces the first
piece is a platform agnostic library
called tools analyzer and that contains
all of the code that's the same on every
closure compiler for instance do is
implement is analyzed the same way on no
matter what platform you're on
and then there's closure tools analyzer
JVM which has the JVM specific analyzer
code in this case it would be something
like def type def protocol or
implemented very differently on the JVM
than they are in closure script and so
the JVM specific code is in this library
the third library is the emitter which I
believe is called tools emitter JVM and
that contains the actual code that takes
the ast and emits Java bytecode I'm not
gonna talk about that library today
we're not actually gonna use it if
you're interested I take I suggest to
take a look at it before they go another
step farther and represent Java bytecode
as data and so instead of using mutable
class builders as the current compiler
does it actually just represents code as
a lazy list of byte codes which is
pretty sweet
so we're going to load in our the jvm
analyzer and if we analyze this if if
true 42 otherwise zero pretty useless
expression but if we analyze that and
print it we get this large amount of
data but it's very rich data there's a
lot of information here that came out of
just this single if block and let's dig
in really quick to look at this
environment so I said this on every node
is attached the state of the analyzer
when that node was created we have to
create an initial environment when we
call the analyzer we do that here empty
environment so the empty environment on
the JVM version of this analyzer
actually includes every namespace that
we know about at that time the current
namespace were in and a whole bunch of
other information so if we take that
environment pull out the namespaces it's
an atom so we do rough it and then call
keys Sider does something I didn't
expect there we go I switch the cider
recently from n Ruppel and things aren't
all mobile bugs aren't worked out yet
but anyway here we have a copy of all of
the namespaces that our current runtime
knows about and this allows the analyzer
to start linking up bars perform a
little bit of analysis and know if
you're calling a bar or calling a static
method or calling a class or something
along that lines and if we dig in so
here we have a function that just
returns X and X as an argument and we're
gonna analyze that and we're going to go
into the methods of the ast so these are
multi arity functions that's what
closure functions have multiple era T's
we're going to get all of those era T's
find the first one we're going to go to
the body of that and we're going to look
at the environment and the first thing
to notice here is that all of this is
just closure keywords methods we're use
the same functions we use every day in
our applications and yet we're dealing
with compiler technology and concepts
here so here's the environment of this X
node here we have the file that it came
from which is some temporary file that I
don't know
who decides what that is we have the
internal name this function didn't have
a name so the analyzer automatically
generated a name for us we have some
other random information we have the
namespace that this came from and this
loop ID information because it is
possible to recur to the top of a
function so there's an implicit loop
that you could take advantage of in
every closure function and that's been a
labeled for us already and so if you're
writing any Mitter a lot of this will
save you some time okay so let's do
something fun let's write a closure
emitter that will take the ast and spit
closure out again somewhat useless but
it is kind of helpful for debugging to
not have to look at all these deeply
nested hashmap site and in fact I
remember on the mailing list someone
complained about trying to print out the
ast from closure script and realizing
that their simple hello world ended up
like a hundred megabytes of data because
of all the structural sharing there's
good stuff duplicated all over the place
so it's a lot easier if you're playing
around with this to build something like
what we're about to build so every node
has an op field on it so we can just
simply call deaf multi to CL j op so
we're gonna dispatch on the type of the
node and now if we run this we get an
error that says no multi method to CL J
for dispatch value invoke this is
actually the way I like to develop these
analyzer passes is I start with a multi
method run it it tells me what method I
need to implement I implement that
method maybe do a little bit of print
line on the the keys of the of the node
that it hands me implement that run it
again something else breaks kind of back
and forth until it finally works okay so
what we're gonna do is this is going to
be a function we're gonna we're going to
turn this ast back and s expressions so
who here is familiar with syntax quoting
okay if you haven't worked with syntax
coding just view this like as a template
so we're creating an if s expression and
where this little tilde is we're
splicing in the value of to sale J on
test in this case so all we do is we in
our destructuring of our of our multi
method we pull out test then and else we
call two
J on every one of the nodes spit spit
that out and if is implemented now Const
we have the unanalyzed form if we have a
constant was closure and it's still
closure and we're going to closure so we
can just omit the form as it is do the
body of a do is broken up into two parts
the statements the site affecting parts
whose values are just thrown away and
then the last form in the do is the
return value we could include this just
as a single vector but a lot of times it
makes a lot of sense when you're writing
an analyzer passes that you want to
distinct you deal with them separately
in two different ways and we'll see that
in a little bit so to do is just a map
on the statements and we call to see LJ
on the return as well function has to go
through all the method names f'n method
isn't an individual arity call of the
method and we have some magic in here
for very attic don't worry about that
and then a local just emits the name of
the local a lattice to see LJ on every
one of the bindings and on the body and
the binding here emits pairs of names
and the value that the binding is
initialized to and then if we look up
and let we call mat cat to actually turn
those all into a single vector and then
finally our invoke that we saw above
which just calls to see LJ on the
function and all the arguments okay so
that's a lot of work to go through but
if it all works correctly this output
form should look just like the input
form and it absolutely does not and this
is kind of the the cool thing about the
analyzer is it does do a lot of work for
us a couple things to point out here one
it named the function for us like before
which is kind of cool it renamed all of
our locals and the important reason for
this is because of local shadowing you
know basically if you can imagine a
function with an argument called X and
we later say in this let X is now 42
outside of that let the value of x has
not changed and so on a most plat
for most VMs will not support this
feature out of the box and it's just
easier to rename all your locals to
unique names and go from there
so that's what the analyzer does by
default and we can actually see this
here if we say X equals 42 and then X
equals x and return X and we analyze
that we see it's renamed all the
variables to either x0 or x1 to remove
any overwriting of existing variables
and there was something oh the other
thing we can point out too is that a lot
of closure forms have implicit dues
inside of them loops lets functions all
have an implicit do and the analyzer
actually turns that into a do node so we
don't have to worry about that when
we're writing our passes but one thing
I'd like to point out about this too is
how simple this code is it's very easy
to look at this and say okay I have an
error in two clj that it doesn't handle
a certain node type and we can just look
at the node type that it's not handling
and we see it right here to see LJ do or
FN or so on and so forth the other thing
that's interesting about this is that
it's open dispatching somebody else
could take this code that I have in a
library and they download it as a
dependency and say I want to add a new
node type I'm gonna create some node
type that no one even knows about and
I'm gonna go implement all the multi
methods needed for the compiler to work
with it and no one has to be the wiser
you can actually extend this code with
your own code and not have to go modify
the original source okay so let's do
something a little more fun here and
we're gonna create a little quickly a
constant folder but once again this is
just working with these ast s in a way
that treating them is this their data so
let's start off with what we're gonna do
here is we're going to make a analyzer
pass that goes through all of our code
and tries to find constants and tries to
find certain calculations that can be
done at compile time on this ast perhaps
to improve performance I really doubt
this would actually improve anyone's
performance unless they were a really
bad code but it's
as an example so in this case our multi
method is has two arguments so we have
to pull the OP out of the first argument
and Const in this case is going to be a
atom of names two known values names two
constants if we know a variable it has a
constant value we're gonna throw it into
this out so if we have a constant and we
want a constant folded constant it's a
constant and there's nothing to do so
just return it if we have a binding we
have a name and the initialized value so
we simply say if this is a constant then
add it to our map of known constant
locals and return nil because we've now
removed this this binding from the let
or returned the ast unmodified we did
not evaluate Const up here so we got an
error all right there we go if it's a do
we're going to filter out all of the
statements this is what I was talking
about before we have statements and the
return value we're gonna go through all
of our statements and say are any of
these statements constant if so there's
no reason to even have them in the code
because the return value is thrown away
completely
so we filter the body update the body
and if the do is now completely empty
except for the return value we remove
the do completely out of the equation it
has returned the return ast node and
here's here's where we start to get the
the real meat of what we're trying to do
here if we have a local we have the name
of the local here and in it is actually
never used and nil so don't worry about
that part so we have the name of the
local and we look into our constants
atom and say do we have unknown value
for this local if so replace the local
node with that constant otherwise just
leave it as it is and then a what is a
lot like a do we filter out any of the
bindings that are now nil just kind of
clean it up a little bit
VARs we don't do anything we remove them
and if we're creating a literal hash map
on the fly and all the keys are constant
and all the values are constant then we
just turn it into a constant itself
and these don't do anything at all but
they're just needed so we don't get
runtime errors so here's the real fun we
are going to whitelist
three bars that we know are pure any
input to an associative social into will
always return the same output so we're
going to whitelist those and have a
helper function here and here is where
we actually start doing some evaluation
if we have an invoke and the function
we're calling is a var and the var has
been whitelisted and every argument to
that var is itself a constant then we're
going to call the var with all the
arguments and create a constant note so
we're running this function at compile
time instead of runtime and we do the
same thing for static calls because some
things like integer addition are
actually calls to closure line numbers
and not to VARs and this is much the
same code okay so then we run our
analyzer we call post walk which will
start at the children nodes and work
their way up
much the way actual Lisp evaluation
works and here's where we have the reo
of fun so here's a kind of ridiculous
example where almost everything can be
done at compile time but we define X to
be 42 a in this map is X plus 1 into is
a concatenation of two vectors and we do
some work on an empty map and then we
dissociate the empty map and when we run
this we get a constant now what I think
is kind of cool about this is I mean in
reality we're not going to run into a
whole lot of situations in code that are
this drastic of an example but I think
the real cool part of this is in this
function right here this method this
invoke it's very clear what's going on
there's nothing no specific dispatching
on on what the types of the nodes are or
or you know finding calling methods on
these nodes or anything
like that it's just simply data
manipulation like we use every single
day there's another aspect of this
that's pretty cool and to do this I'm
just gonna quickly evaluate this pass
what this map optimizer does is a kind
of crazy idea came up with that if parts
of the literal hash map have to be are
non constant and parts are constant most
closure compilers will currently emit a
the entire hash map is created at
runtime so if I have 20 items in a hash
map and only one of them has to be
figured out at runtime it will make all
of them be recreated every time at
runtime so this kind of splits the hash
map up and we can see that when we run
this here so if I have this and I've
need a few more methods here we go so in
this example here a and 1 are both
constant the value of B is not well it
is kind of constable we need the the
evaluator for this so what this does
it's passed does this breaks it up into
two parts we're gonna create a 1 as a
constant and then we're going to create
a social call that Associa is on to be
the value of adding 4 and 3
now the COBE just wrote was a constant
folder so be cool if we could run that
code and then run this code because we
get even better optimization perhaps and
we perhaps even have another pass like
this one here that's a verify that says
if you try to call an integer as a
function you're just gonna call the
integer in the function position we
throw an error at compile time and say
now you can never do that that'll never
work
so all of these passes we've looked at
are just functions and we could run post
walk or pre walk over and over on our
tree but because it's a mutable data
changing a node way down in the tree is
going to require the entire tree to be
modified in a bunch of allocations to be
done but since these are all functions
we can actually group them into groups
of either
pre-walk or post walk passes and use
comp to combine them into a single pass
so if you were to look into the tools
analyzer JVM code you'll actually see a
part where they have about ten passes
currently that are all just copped
together in a single block and run one
time or they're just like two or three
actual passes so that's what we do here
we have our verify invoke we have our
concept fold node and our map optimizer
and this all runs as one large pass and
we can run this and we get the error
that we expect from our invoke and I
think the comments change but because
what's happened here is that we've added
one and two together that ends up being
three and then we're calling 3 and the
compliation fails so we just we've
improved performance a little bit in our
analyzer past by combining the passes
into a single function so real life
example ie wrote originally the the go
macro and core async and when I did that
was kind of before a lot of this tools
analyzer stuff really was accepted as a
contributing to 700 lines of code and
the go macro were just around analyzing
s expressions and pulling out this rich
data so when I start looking at this
code then tools analyzer I thought I
should be able to use this and I
recently ported that into core async and
it works quite well and here's two
functions from that code what we have to
do in two separate phases is go through
our ast and find all of the async
operations and mark them so I go through
the ast and I just associate a keyword
actually I changed the node completely I
create a new node that says the code
inside this node is asynchronous but
then we need to propagate that
information all the way up through all
the parent nodes because it's kind of
infectious if a child node is async all
the parent knows need to know that it's
async as well and so that's what this
pass does is this says if any of the
children
there you go if any of the children are
asynchronous then I am also in a
synchronous order and marked as a
transition node these are two passes
that once again could be done separately
two distinct passes two distinct pieces
of logic and two functions that we need
to use comp to combine them so I
recommend that if you have any interest
in this checking out the tools analyzer
project and just playing around with it
there are a bunch of different projects
that can take advantage of this Koree
sync is one court typed I think has its
own version of main analyzer as well and
it would be nice to eventually replace
both of those with with this code
because one of the nice things about it
is that if we have a canonical analyzer
that's easy to use and extend for these
multiple different implementations we
get around the problem that I came into
with Corre sync and that is I've wrote
my own analyzers so now my the the bug
I've introduced a whole set of bugs into
this analyzer and I there's little
things I didn't know about closure and
and people have to submit bug reports to
me to fix and all this other information
where if we just had one canonical
representation of the analyzer everybody
could benefit from that there is a
Google Summer of Code project to replace
the closure script analyzer with the
tools analyzer code so this would be
like tools analyzer closure script as I
mentioned a lot of this code was pulled
out of closure script originally and at
that time was when all the child member
the the values and that have the
children the children no thing was
implemented and we need to actually
integrate those changes into closure
script that's a fair amount of work but
if we ever got to that point once again
and we'd have one more extra analyzer
removed from the equation there's
another Google Code project Google
Summer of Code project around the lean
JVM and I'd kind of like to see that
move forward and their approach is to
actually take your entire program as
much as they can and load it up as one
giant AST
start doing tree shaking and tons of
transformations and just make everything
static and as fast as possible for
production code
it's another ambitious project that it'd
be cool to see move forward um another
area is so right now the analyzer is
pretty much self-hosting the analyzer
can analyze itself which is cool the
emitter is very close to being able to
emit itself when that happens we'll have
a pretty good closure enclosure compiler
and then the question is how do we use
it the JVM emit code right now just
gives you eval so you can go to the
repple and eval something and run it
with this whole new compiler tool chain
but there's no way to really bootstrap
your entire program using the gnu
compiler and so there should be some
discussions at some point about if that
ends up being a switch in mine or some
other method and then there's a whole
other set of code in the go macro that
uses a representation of code not as an
AST what was called static single
assignment which is a kind of a level
lower than an AST and i would like to
pull that out sometime and make that
generic so that even more people could
take advantage of that and write their
own optimization passes so that's my
talk today I hope I encouraged some
people to take a look at this it's not
as scary as it sounds it's just data
manipulation
and we have about five minutes for
questions
yes no there's not
I've done some work with so the question
was for the cameras was there any use of
closures zip in fact no there's not and
even the pre walk and post walk we see
are not the standard closure ones
because they don't go through every sub
no they only go through the nodes marked
as children and so they kind of have
their own implementation there yes yes
okay yeah the first question was how
does the tools analyzer work with macros
there is so the tools analyzer JVM
project hooks into the standard macro
expand or has its own implementation of
it I haven't actually looked at that um
if you download tools analyzer it's a
very open-ended extensible system and so
if you run that it'll actually throw an
error and say I don't know how to expand
macros and you have to bind a dynamic
bar to a function that you create that
does that know that does the macro
expansion but know is macro expansion is
part of the analysis process and so it
has facilities for that and depending
which while of the libraries you use it
does something different but yeah that
question
right so the question was is there
something that runs the passes over and
over until they finally converge into a
fixed point no there's not anchor and
currently there's nothing even close to
that complex you just run post walk or
pre walk however you feel like I mean I
could have run them over and over here
until I get the same thing out that I
put in but I don't know that anyone has
tried that I would love to see something
like that yes right right there may be
something in that so the comment was
that there might be something in in the
analyzer that does that that's like one
pass that maybe does that it's not like
the the framework itself a one your
passes over and over yes what's the
advantage of using a vector instead of
an ordered hash map I don't think there
would be over an ordered hash map the
key is that that it is ordered and the
order does mean something yes
how the future version of the go macro
use tools analyzers so there's a branch
in the core async code that actually
does this right now
but what we have to do is there's a lot
of analysis so just the actual concept
of taking the code within a site ago
analyzing and finding asynchronous
operations we've involves a whole lot of
things and involves macro expansion and
involves finding out what VARs point to
asynchronous like putting gets and that
sort of thing so it's really easy with
the analyzer you just analyze it
traverse the tree until you find a call
to put or get in core async and you
found the async notes where as the old
version had to go and look into
namespaces and revolve a resolve aliases
and it was basically a real
implementation of the closure analyzer
right exactly I saw the old version of
the go macro would take the s expression
and just look at it and find out where
all the VARs go the new one hands it off
to the tools analyzer gets the ast back
analyzes that and use that to drive the
the output of the macro yeah all right
yes
the question was reporting closer to a
new runtime how far does this take it
takes you really far what you still have
to do is you will have to define how
like def type def protocol the
polymorphism is handled on your runtime
and then you'll have to work on the
standard library so none of this touches
like how persistent collections are
created or handled now the nice thing is
you can go to closure script where all
the persistent collections are written
in a common like subset of closure and
you could copy a lot of that code and
modify it and sell some parts of it are
JavaScript specific but that's kind of
the way it has to be yes
is there the question is is there a
standard list of opps not no there's not
and part of the I would say one of the
things I appreciate about it is that
it's not formally defined you could run
into some problems where someone creates
a node and hands it to your analyzer
passed and you don't know what it is but
if you use multi methods and you'll use
a lot of this open-ended stuff if
someone defines a new node they could go
and define how every pass should handle
that node and it kind of allows
everybody to extend from whatever way
they want yes
you're right right so there's a um the
question was whether features that have
besides representing a STS as data a lot
of it is just that everything you need
to write a good analyzer has really
already been done when I wrote the
original go macro I had to go back and
rethink oh great I didn't handle
metadata properly so now line numbers
are all in the wrong places which if you
use to go macro you're probably aware of
and a lot of that is fixed a lot of that
is it just works out of the box a lot of
another example is the the dot form
enclosure there's about six or seven
variants of it and I never quite got it
right in the glow macro the analyzer now
just emits it all is the same no it ends
up being the same ast node and so I can
handle that one node instead of handling
all the different the different edge
cases and that's part of that whole idea
of writing one really good analyzer
instead of all bunch of half good ones
yes last question
right so the question was if you add a
new type of a node which we actually do
in this in core a sync we just find like
an asynchronous node if you then have to
tell the other analyzer passes about
that it depends where you are on the the
tool chain and core async we're at the
very end of the I don't call any other
analyzer passes after I've run the
analyzer it's just my passes so I don't
really there's no problem with that but
in general yes you'd have to go in and
extend their multi methods or maybe like
an example of our constant folder here
we have a default that just does nothing
on the node in which case every other no
that that we haven't handled specially
it is unmodified all right thank you all
for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>