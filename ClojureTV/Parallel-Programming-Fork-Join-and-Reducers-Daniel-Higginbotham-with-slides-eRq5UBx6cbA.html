<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Parallel Programming, Fork Join, and Reducers - Daniel Higginbotham (with slides) | Coder Coacher - Coaching Coders</title><meta content="Parallel Programming, Fork Join, and Reducers - Daniel Higginbotham (with slides) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Parallel Programming, Fork Join, and Reducers - Daniel Higginbotham (with slides)</b></h2><h5 class="post__date">2016-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eRq5UBx6cbA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right we're gonna talk about oh you
can put your hands down and actually
probably keep your hands down because
I'm gonna go kind of fast this is a lot
of stuff to cover and I probably won't
have time for questions I'm gonna be
talking about parallel programming the
sacred art of doing two things at the
same time my name is Dan Hagen balsam I
hail from ads irk North Carolina I'm on
Twitter I have a website brave closure
calm which is home to a book closure for
the brave and true also has now that's
it thank you very much
now there's also now a directory of open
source projects so if you're looking to
contribute you can go there and there's
also a closure job board I think they'll
first and only close your only job board
but to get really get started here this
is some code which shows the kind of
performance gains that you might be able
to get from the reducers library so if
you're not familiar with the reducers
library it really meant a lot of closure
core functions for operating on
collections like map and filter and so
forth and does it in such a way that you
can get dramatically sometimes better
performance with them through
parallelization and elimination of
intermediate data structures or
collections so this code is just showing
a very basic operation of taking a
vector of 10 million numbers
incrementing them filtering them and
then summing them and the reducers
version is seven times faster and this
was run on a quad-core machine you might
think Oh quad-core machine parallelized
went that before times faster
oh no it's seven times faster and so
we'll see hopefully by the end this talk
have a better idea of where that
performance comes from this talk is
really not about reducers per se it's
more about parallel programming and the
concepts behind it and I think that that
will help you understand actually what
the reducers libraries doing so to get
there we're going to end up at the
fourth joint framework the reducers
library relies on the fork/join
framework in order to do its
parallelization it's a really cool thing
to understand the fork/join framework
that we're going to start with some
basics of what is parallel with them
what it's parallel programming why does
it matter why should you care then we're
going to talk
performance because this is really all
about performance this is why we care
about the reduces library because it
gets better performance we're going to
talk about some general performance
strategies and then focus specifically
on data parallelism and once we cover
all of that we'll be ready to then look
at the fortuyn framework which embodies
a lot of best practices for parallel
programming cool so let's start with
some basics why should you care why do
you care about parallel programming the
main reason is performance you may have
noticed in the last 10 years that CPUs
on your computers they're clock speeds
have barely increased not at all like
the previous decades where clock speeds
were increasing about doubling every 18
months or so right what we have now
instead is an increasing number of cores
on our machines and the reason for this
well there are three main reasons for
this these three limitations the first
one is called the power wall if we were
to dramatically increase the clock speed
of a processor then it would actually
exponentially increase the amount of
time I'm sorry the amount of power that
the processor would have to consume it
would also dramatically increase the
amount of heat that it would output so
really not feasible to dramatically
increase clock speeds the next
limitation here the memory wall well the
deal with that is that even if you did
increase clock speeds the memory bus
isn't fast enough for the CPU to
actually you know access your data and
make use of this increased clock speed
so that is another limitation that I
hope you all sounds fancy and so that's
why I put it up there but I'm not really
going to talk about it you can google it
this really
through enough time the other reason why
is robustus and this actually is kinda
has more to do with distributed
programming but I hopefully the idea is
that you'll learn here I think a lot of
them are kind of applied to distributed
programming as well like in this system
we have here we have the Zulu system and
he has a yes many tentacles he has a
good feel over there will where if one
tentacle is separate or something like
that he can still capture you and devour
your soul and send it to the bith
whatever scale so there's a so those are
so those those are the main reasons why
you should care about parallel
programming unless you can go back to
the previous slide I forgot one thing I
forgot to mention there it's also this
this trend is just gonna continue right
with multi-core machines right so in
order to make best use of modern
hardware and to write programs that are
able to run faster without having to
change anything on new hardware that as
its released you have to explicitly code
for parallelism so learning these skills
I think it's like it's valuable to learn
them and they're only going to become
more valuable in the future all right so
that's why you should care now what
actually is parallelism what's parallel
programming to explain this I'm gonna
start with concurrency because I think
it's just useful it's a useful way to to
understand parallel programming and so
I'm all you guys are probably familiar
with this example it's a it's a classic
computer science example like the gaga
model of concurrency and parallelism
it's based on Lady Gaga's song telephone
and in this phone I'm sorry in this song
Lady Gaga is at a club and one of her
suitors is blowing up her phone with
texts and calls and whatnot but she says
hey I just want to dance okay and so in
the lyrics she says I cannot text you
with this drink in my hand at and so
the this Gaga motto is we would say that
this is a non-concurrent system she was
refusing to even contemplate managing to
test at the same time all she wants to
do is drink and well I can understand
that I feel like this is a very not a
great programming system so our
computational system so if we were to
upgrade the Gaga model if the lyrics
would be I will put down this drink to
text you and then put my phone away and
continue drinking at and so in this case
we would say it's a concurrent system
because she's managing two tests at the
same time she's not executing them both
at the same time but at least she's now
handling both drinking and texting one
another wrinkle of this would be to say
if he were to you know send a text
message and just stare at her phone and
wait for the first suitor to respond he
would say that that would be a blocking
operation she's waiting for some kind of
response right and that's preventing her
from drinking which is terrible and so
if she were to put her phone in her
pocket right and wait for it to buzzed
and she would say we would say that
she's handling that task asynchronously
all right so concurrent but not yet
parallel so who were to up create the
Gaga model or the Gaga system even
further the lyrics would be I can text
you with one hand while use the other to
drink yeah and so this is now a parallel
system she's actually executing multiple
tasks at the same time so this is the
theoretical model we're all familiar
with it from our you know cs101 and so
that's just like the conceptually what
concurrency and parallelism heart so the
way that this is implemented on the JVM
is with threads and so I've Tom I've
talked about managing tasks and I think
of tasks is just just some work you do I
think that actually is a function call
but from the execution standpoint rather
than the semantic standpoint you don't
really care about what's happening in
the meaning of it you just care about
well there's some work that has to get
done and so these tasks get assigned to
threads and then threads are scheduled
on processors and then that's how the
threads are the tasks executed so in
this diagram the squared D Leo's are
tasks the solid line is a thread and the
dashed line is a kind of execution
timeline and then at the bottom there's
an alligator and it's rissalah
marshmallow
so on so this next example shows how you
how you could actually have a concurrent
system you'd have multiple threads and
your processors which is between the
threads that's called interleaving the
tests are interleaved one important
thing about this is that you don't know
the actual execution order of these
tests so that can lead to well that is
it means that the execution is gonna be
non-deterministic and that causes all
kinds of problems oh my god no
with race conditions and deadlock and so
forth
closure makes it easy I'm not gonna
spend time on that but it's cool we have
great things to deal with that um so
finally here we're just looking at
parallelism how how this would work with
multiple cores you have multiple threads
and then the the system your computer
will try to map each thread to a core so
that those tests can be executed
simultaneously you still don't know the
actual absolutes
actually execution order of these tests
but yeah so that's parallelism roughly
how it's implemented for reals
so that is your crash course on
concurrency and parallelism now let's
talk about performance because that's
what we really care about here usually
when people talk about performance to
use it in a general way to refer to that
thing to improve so that your program
sucks less so there are actually a few
different yeah a few different aspects
of performance the first which we
probably care about the most is latency
and I define it as just the amount of
time it takes some tasks to complete at
any level of granularity so like how
long it takes for some page to load or
just your database query to to come back
anyway latency we care about that a lot
the reason why we do parallel
programming is so that these
computations take less time throughput
is the number of operations per second
that your system can handle we don't
really care about that so much in this
talk utilization that is the degree to
which some resource is active during
some time period we care about that a
lot right so we have these multi-core
systems now we want to make the best use
of them we want to make sure that all of
our
are being used right that's why we're
doing parallel programming speed up is a
term that is specific to parallel
programming and that is the degree to
which your algorithm will run faster on
parallel hardware versus just being run
serially so want to get decent speed up
like at the beginning we saw like a
seven seven fold increase in the amount
of time it took to do some computation
oh yeah and finally there's this thing
no one cares about so skip it power
consumption all right so that's that's
kind of a brief overview of performance
and now we're just gonna look at some
general strategies for dealing with
performance and so the last one you can
see is data parallelism but I want to
talk about a couple other ones too just
so that you can see you can see we're
parallel programming fits into the
broader scheme of things the kind of
programming universe and you can have a
better understanding of where reducers
fit in just in the you know performance
universe right so we're gonna start with
latency hiding and functional
decomposition you guys can read it so
like that's the worst thing just read
slides out loud anyway but I'm gonna do
it on this one so so late you see hiding
is just like a fancy word for something
that you do all the time right with
programming it's just handling stuff
asynchronously right you wanna you
you're some operation is blocking you're
waiting for it you want to put it in the
background and hide it right so that
your main thread can continue so we do
this all of course all the time in web
programming like Ajax is a thing with on
our servers right when you're you're
jiving a file often will use a future or
something like that so you retrieve a
large file so it's not blocking the main
thread so fancy word for something to do
all the time
there are many real-life examples of
this it's pretty important the comic
shows a tragic example of not making use
of latency hiding and so the narrator
here is saying alone so very alone
nothing but my thoughts an occasional
noise or worse no books no phones
not a single printed word it was the
worst four minutes of my life
so if he had been making use of latency
hiding there he would have had a better
time all right so a functional
decomposition is when a group of
environmentally conscious teenagers
summons an avatar of the earth to clean
up pollution it's also just another term
for something we do a lot it's when you
split your program into logical modules
and have each module run on a separate
thread right and so on the JVM our Java
programs do this actually all the time
always because when you start up a Java
program the garbage collector runs on a
separate thread right so you're already
doing this but one thing about
functional decomposition is that you
won't get a kind of constant factor
speed increase right so if you split
your your system into two modules and
each module is running on a separate
thread and well okay so you have two
threads well let's say you have a
quad-core system then you have two cores
that are their idle unless you do other
things to make use of those cores right
so you get a kind of constant factor
increase I think that core async is cool
for this kind of thing you have almost
basically separate programs running but
are able to communicate within the same
program but yeah so functional
decomposition fancy word for something
we do all the time data parallelism so
this is the real meat of the talk this
is what we care about you really care
about so data parallelism is when you
take some computational tasks and break
it down into subtasks relatively usually
a homogeneous subtasks and then execute
or try to execute those sub tasks in
parallel right so often like the kind of
basic example if this is with the map
right you have you want a map over like
say you know ten million items right you
could break it down into smaller
collections of say 512 items I'm just
picking those numbers randomly and then
and then operate on those sub
collections in parallel and then we
combine the results I feel like this was
dramatically demonstrated and the movie
Terminator starring the Governor of
California and so in this case the the
task was eliminate humanity right and so
this was a broken down and
and to subtest of just you know kill
whoever's in front of you and those
tests were distributed among terminators
you could say it was a parallel reduce
operation alright so okay okay fun times
over now alright no more left so so this
is this is where shit gets real
alright data parallelism all right so
now we're going to talk about some
theory the work span model this will
help you kind of reason about parallel
performance in the abstract and then
we're going to spend a lot of time on
implementation right so the
implementation concerns of data
parallelism are you know huge if you if
you structure your sub tests incorrectly
if you do this incorrectly the parallel
version of your code could actually run
slower than the serial version so no one
wants that so we're gonna talk about how
to avoid that right so let's start with
the work span model who's sort of the
work span model it's raising hands okay
me and like two other people
cool great this is perfect all right so
the work span model helps you reason
about parallel performance by describing
performance at two extremes right so at
one extreme we have the amount of time
it takes your algorithm to run on a
single core machine and that is the work
of the system right so with data
parallelism we have some tasks we break
it into subtasks right and here this is
just some made-up example we've split
this task into 15 different subtasks
right and we'll say that each of these
subtasks takes unit time so the work
here would be 15 units of time right on
a single machine it just has to you know
do each of these individually 15 units
of time so the other extreme is how much
time it would take to execute this
algorithm on a theoretical or ideal
machine with infinitely many processors
right and so that is determined by
something called the critical path and
so the critical path is the longest
chain
of serial dependencies within your
algorithm right so the deal with serial
dependencies you can see here the some
of these tasks are highlighted with gray
right so the very bottom task depends on
the task before to be completed
therefore you know you can't paralyze
that right so even you know no matter
how many more cores or processors you
throw at this right that you can't make
that goes faster right so the longest
chain of these tasks is called the
critical path and it determines it's a
more like that it's like the upper limit
to the amount of speed up that you can
get by parallelizing your algorithm so
the work span model defines these limits
and help you kind of reason about what's
the theoretical limit to the speed-up I
can get right so we could say the work
here is 15 the span is 5 and so at most
I can get maybe a three times increase
in performance by parallelizing this no
matter how many cores I put it on right
so we can look at a few now we can look
at a few examples of the work span model
so with map the span is one because each
application of the mapping function is
independent of all other mapping
applications right so on this you know
ideal Hardware we could run each mapping
application on a separate core and do it
all in parallel in the literature there
is an adorable term for this it's it's
called embarrassingly parallel which I
just love it's like excuse me I'm was I
was I just parallel part of pardon me
so that's embarrassing parallelism
reduce is this is I think we all know
what reduces but anyway reduce can be
parallelized on this is important we
there's a whole library called the
reducers library which relies on this
fact I'm actually I'm not gonna spend
too much time on this this diagram just
shows like hey yeah you you run it in
parallel and it runs faster cool
next we have a scan operation so scan is
also known as partial sums it's kind of
like reduce but you keep all the
intermediate results as well and put
that in a new collection so scans
interesting because it has has more
complicated data dependencies right and
so this next slide kind of shows that
now we're gonna get you the details of
how scan works the important thing that
I want you guys to take away from this
is that when you parallel eyes scan you
actually end up doing more work than in
the serial version so I like on the
left-hand side we have these seven tasks
and on the right hand the left hand
serial side seven tasks on the right
hand side there are 11 tasks yeah it
runs faster so this is kind of
counterintuitive and I think it's
counter to how we think you know
normally when doing serial programming
you try to eliminate work but with
parallel programming sometimes you
actually do more work but the overall
task is faster so that's kind of cool I
think that's part of what makes parallel
programming fascinating for me Oh
also I forgot to mention there is a cool
paper out there just like I think if you
google like parallel scan PDF or
something like that you'll probably get
something good
all right so those are kind of the
theoretical concerns with parallel
programming now we're going to talk
about implementation all right so the
main deal with implementation is that
you you want to balance amortized over
head right so reducing the amount of
overhead relative to the real work that
you're trying to do versus keeping your
course saturated or load balancing right
so you might initially think oh I want
to parallelize this algorithm I'll take
I want to parallelize map for example
right and so I'll assign each mapping
function application to its own thread
and I'll create like a billion threads
and they'll all run and this will happen
instantaneously if you thought that
you'd be very wrong because there's a
lot of overhead associated with thread
creation also if you create too many
threads then your system will just grind
to a halt right so you
need to manage your threats right on the
other hand so there are techniques for
doing that right but if you go too far
or go too extreme with those techniques
then what happens I mean you would say
if you take it to its further extreme
you'll say like well just put it all on
one thread and then it'll be serial and
I won't have parallel overhead right so
but that would that obviously defeat the
purpose right so you want to keep your
course saturated as well so we're going
to talk about some approaches to doing
this thread management and then also a
few different approaches to decomposing
your main tasks into subtasks we'll
cover those all right so let's talk
about thread management the main way
that we manage threads on the JVM is
with thread pools that's a picture of a
thread pool from from the Lord of the
Rings movie it's like these these three
ring knife sorry nine ring wraiths you
anyway whatever I'm gonna I'm gonna
explain I'm gonna explain thread pools
now all right so the deal with thread
pools is that they're a layer of
indirection between tasks and threads so
instead of putting a task directly on a
thread you hand your task to a thread
pool and in the thread pool manages
things like creating the thread and
ensuring that not too many threads are
created and so forth so thread pools
they mostly live in the java.util
concurrent executor package that has
really great documentation and I think
it's just fun to read so there are a few
different kinds no I'm serious
but so there are few different kinds of
thread pools right there there are
caching thread pools and the deal with
those is that if you hand it a new task
and there is no there is no idle thread
it will create a new thread for you but
then the thread will stay away I stay
around for some amount of time like a
minute or something like that so that
even when it's done with its work if you
hand this Deadpool a new task it can
just go on that idle thread it does have
to create a new task closures future
futurists use that which that code
roofs know there are also fixed size
thread pools right we use these often I
think with databases a fixed size
Deadpool often just create
a certain number of threads when you
create a thread pool that way you reduce
the amount of time it starts to at the
warmup time to connect to a database but
fixed size thread pools have their uses
as well I think core async uses a fixed
size thread pool of the number of cores
on your machine plus 42 thread pool is
also managed queuing your tasks so if
none of the threats are idle then it
will just keep track of all the tests
that you handle it and then hand it off
to the threads when there when they are
idle so cool
all right so thread management that's I
think that's basically it so again
here's this code it's just I'm not gonna
spend time on it but just I think if you
just go through this closure source code
like closure core namespace and look up
the futures and then just keep digging
then you'll end up and you'll end up
with what you have here and you can see
like oh cool okay
closure actually starts up this cache
that pull at the beginning and then uses
it for futures pretty groovy all right
so that's thread management the next
approach that we have to dealing with
parallel / fellow programming
implementation is test decomposition so
the first thing we're going to talk
about here is granularity so granularity
refers to just the size of your sub task
right and so you may not it might not
seem like it at first but this graph
shows a pretty sad state of affairs and
then the happier state of affairs the
the white squares are just the subtests
though the work that you want to do
right and greenish or I can't tell if it
actually looks green but the colored
rectangles represent the parallel
overhead that you have and so in the
very first bar here we're showing like
this is some task that you want to do it
takes 16 units of time it's at 16 tasks
subtasks that each take one unit of time
right so 16 units of time total alright
so if you were to decompose this poorly
like for example by using P map with ink
right you each each subtest would then
be parallelized and all that overhead
associated with parallelization like you
know scheduling this task
you know wrapping up this task for
parallelization
would be incurred and you would actually
end up spending more time on the
parallel version which would suck and so
in this case this hypothetical case you
end up taking 24 units of time versus 16
units of time with the serial case so
the the bottom example shows like okay
let's package together these subtasks
and then run those packages in parallel
and that way um er ties this overhead so
the work done at the actual work that
you want to do Dwarfs the work taken for
parallelization and you get real
performance benefits from
parallelization so okay I mentioned P
map this is often this often bites
people if you do P map with some very
computationally intensive tasks write
some very simple tasks then you end up
taking more time with P map than you
would see really all right so
granularity is something to keep in mind
the next idea here we have is tiling and
so the idea what's hauling we kind of
showed it in this slide but the idea of
tiling is you want to split your to your
some collection of data into these sub
collections and you want to run your
parallel operation serially on these sub
collections and you want to take that
and then paralyze that so you can see
here we have this we have these eight
items we've split it into two groups and
then we operate on those two groups
serially but then that operation is
performed in parallel right so that's
the idea what's huiling and it's
probably comes actually from image
processing where they work with literal
you know tiles of the images you would
split it split these images images up
into groups of pixels and then operate
on those pixels serially and then
recombine the results later on
so that's tiling and we're gonna see
later on the fork/join framework does
these things like manages granularity
and tiling whatnot for you in really
cool way the next idea here that we care
about is parallel slack and so you might
you might you know think from what I
just told you about about advertising
the overhead here with granularity you
might say like okay I'll just split my
collection into
number of sub collections equal to the
number of cores that I have right so
about four cores
I'll just have four sub collections
right and then there'll be like very
little overhead there right but then you
can run into a problem where let's say
one of these sub collections the the
processor gets stalled on it for
whatever reason right then you have all
these elements in there that are bound
up in the sub collection that can't be
processed when you have other processors
that are free right so so what you want
to try to do here is introduce what's
called parallel slack and you do this by
over decomposing your system you you
create an amount a degree of potential
parallelism that is much greater than
the actual parallelism available on your
machine all right and another benefit of
introducing parallel slack like this is
that if you do run your parallel
algorithm on newer hardware with more
cores and it's automatically able to
take advantage of the newer hardware so
this is just kind of something that's
that's really common to any kind of
queueing system and it shows up even in
such you know diverse areas as lean
manufacturing and was it lean
development whatever but anyway yeah
parallel slack the idea here is you want
to over decompose your system to create
a lot of potential parallelism but not
go so far as to have your
parallelization overhead dwarf the
actual work that's being done cool all
right the next idea here we have is
fusion and the deal with fusion if you
want to eliminate the creation of
intermediate collections right so if
you're doing a few different operations
on some collection like mapping and
filtering and so forth you don't want to
create intermediate collections because
that's really incidental to the actual
work that you're trying to do you want
to try and just do one pass over your
collection and the reducers library
actually does this it's part of why at
the beginning we saw a sevenfold
increase in speed well part of that was
not just the parallelization but it was
because the reducers library eliminates
these intermediate collections all right
and so one way that you can think about
this is you know
with this code that no one would
actually write you could actually you
know do three different map operations
over some collection well that would be
silly why would you do that instead you
would just have one mapping function
right that process is the collection in
one pass and so if the reducers
libraries design designed in such a way
that when you compose things like map
and filter and whatnot it composes those
operations of map and filter into one
one function that operates across the
collection in a single pass alright so
that is data parallelism I guess I'm
talking kind of fast because I'm farther
than I expected to be but now we're
ready to to actually look at the fork
join framework all right this is what
we've all been waiting for the fork join
framework is an executor right so it's
in the same family as Java thread pools
but it offers additional functionality
and so with the fork join framework you
handed some kind of computationally
computational task and you give it rules
for how to decompose that task right and
so that's where the forking comes from
the the four-twenty framework will just
decompose some what with reducers it
decomposes some data structure into sub
smaller data structures and says is this
smaller data structure small enough that
it meets the base case conditions if
it's not then just Forks again it keeps
on doing this recursively does this
recursive domp decomposition until I
reaches some kind of base case and then
runs whatever processing you want to do
on the base case and then later when
it's done with that kind of recurrent
whoa it combines the results you get
your final reserved result so that is
the basic idea with the fork/join
framework this recursive decomposition
of a task right into some base case and
that's where the actual work gets done
that's where you get the most benefit
from parallelization and then the
results of these computations are then
later combined until you get some kind
of final result so the fork/join
framework handles thread management for
you so with fork/join I think
usually creates a number of threads
equal to the number of cores on your
system
it handles tiling for you so for example
well it can anyway the reduce it with
the reducers library you're doing some
kind of collection operation and this
collect these collection operations are
are fused together automatically also
with the reducers library handles this
tiling for you the number 512 like it's
I mentioned earlier but the reducers
library by default will split your your
data structure like a vector or map into
sub collections of 512 elements each and
then operate on those in parallel so
what does tiling it also handles
parallel slack for you so this the
fork/join framework will over decompose
your problem right creating a great
degree of potential parallelism if you
are working on a smaller on smaller data
structures like say they have you know
200 elements or something like that
then you can tell the reducers library
to to have a different base case for the
number of elements that I should operate
on right so you can continue to have
this parallel slack and the fork/join
framework also introduces a cool
technique called work-stealing and the
idea what works dealing is that if one
of the threads is idle is a fork showing
framework will actually still work from
it and then put it on some other non
idle thread using this double ended
queue it's smart and it's able to find
like the most well poor work from the
kind of most complex end of this queue
of tasks that you need to do that's
pretty cool actually don't really know
all the details of how that works but
it's really neat cool so that's the fork
drawing framework and this diagram just
kind of shows what what I just talked
about but so like it's just meant to
describe like okay when you paralyze
your task with the for join framework
that actual pair alleged work this is
the
the area with the kind of purplish
background you can have multiple
collection operations that are fused
together you see at the top then it
would do some kind of reduction and then
get some kind of results and then those
results are combined cool and that's the
for chewing framework that's really it
all right well let's recap there's
hardware I actually hate recaps because
I feel like this I mean the video is
going up in like an hour or something so
just if you just watch the video thank
you
all right you have time for questions
are there any questions
no no question oh yeah go ahead GPU
compute so the question was have I done
any work with GPU compute enclosure and
the answer is no sound sounds cool
though yeah yeah that's a great that's a
great question and I'm pretty sure that
the answer is no so when you when you
use reducers the functions in the
reducers library what they return back
is actually an anonymous object this
reified object which has which
implements this reducers protocol for
for the reduce operation and the return
value that you get from reducer
functions also kind of keep track of of
the initial collection that you might
hand it all right so that behavior I
believe is is somewhat different from
transducers because ultimately reducer
functions they have to work with reduce
eventually right whereas transducers
don't so the answer your question cool
great yes oh why the question was why do
why did I have only one alligator I
thought that's a very deep existential
question and we all have to talk about
it later that's okay yes
so yeah so the question is if you have
to do I owe our reducers right for you
and I believe actually on the closure
org or comm official documentation it
says you want to actually have all your
data in memory before you use the
reducers library and you know maybe
Correa's think or something would be
more appropriate for you with when
you're doing i/o but at adds irk we've
actually used the reducers library and
part of the parallelized task was
reading a file fairly large file to
begin with and I found that it still
helped dramatically so I would say and
just play with it and see if you get
good performance increase yes yeah so
the question was um did I just waste the
last 40 minutes of my time and the
answer is no no so sorry the so the real
question was do I do I actually have to
care about reducers should I should I
preemptively do reducers these reducers
in my code and I would say probably like
nowadays you know maybe I could doesn't
really matter but I definitely I would
say that if you are doing something they
can see this is kind of computationally
intensive then it's worthwhile to just
throw reducers at it well no actually
it's very easy to bolt it on lady later
it's you know you just require the
reducers namespace instead of having the
map function it's our dash map yeah I'm
sorry are in a slash map
what's yeah
I don't have experience with it so now I
don't I forgot to mention to you like
there other libraries like Claypool
tesslar which which do this kind of
parallel programming give you more
control over it
cool yes in the back yes so the question
is are there situations where the
fork/join
framework or executor is not a better
option than bolts and thread pools and
absolutely absolutely so fork/join is
used specifically for this kind of
recursive decomposition task whereas so
I mentioned earlier there are caching
thread pools which are used for just
one-off tasks that you want to handle
kind of synchronously or in the
background so the the answer is
definitely the the different thread
pools definitely all have their uses and
it's not the case that the fork/join
framework would would be better in all
or even most cases yeah
mm-hmm
yeah yeah so the question is if you
don't have something an embarrassingly
parallel task can you hand it off to the
fork/join framework it looks like my
time is gonna disappoint the last
question and the answer is absolutely
yes so even with with the reduced
function right the reduced function is
not embarrassingly parallel because it
does have these data pennant data
dependencies where it depends on
previous results all right so that's a
very simple example but the fork/join
framework is a very flexible and that
you can define how to decompose the
conditions under which you you decompose
the task into subtasks and I think
that's it for me thanks everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>