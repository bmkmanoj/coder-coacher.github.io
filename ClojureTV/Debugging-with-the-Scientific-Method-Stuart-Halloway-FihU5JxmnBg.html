<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Debugging with the Scientific Method - Stuart Halloway | Coder Coacher - Coaching Coders</title><meta content="Debugging with the Scientific Method - Stuart Halloway - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Debugging with the Scientific Method - Stuart Halloway</b></h2><h5 class="post__date">2015-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FihU5JxmnBg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so testing two different mics
here we all good
all right so I guess I should make a
couple of clarifications on the
introduction before we begin I'm pretty
sure that Alex just drew a chain of
causality whereby I can say that I'm
responsible for Strange Loop so I'm
pretty happy excited to have discovered
that and he wait to see my checks in the
mail the second thing that he implied a
little bit was that there's a lot of
opportunity to watch debugging happening
on the product team if you work at
context and actually you know diatomic
is really no bugs at all so I don't know
I don't know why he's talking about
there either but seriously you know in a
closure audience I really want to begin
this talk with a rationale right this is
something that rich has instilled as a
value in this community and so I put a
lot of effort in having a good rationale
or trying to and I'm gonna have actually
a multi-part rationale for this talk the
first one is debugging why should we
care about debugging and I think there
are there's an obvious reason because
bugs all right we have bugs that is a
you know there are some estimates that
that have it that 50 percent or more of
the costs of software development is
actually tracking down the bugs later
some of us you know work in environments
where we get to run away and somebody
else has to do that so depending on what
you do you might not always feel that
but if you own something for a while
you've come to realize that that's
pretty obvious the second thing is that
debugging is is really a quite
straightforward activity and so you know
Alex made some sort of use some sort of
positive adjectives to describe my
amazing awesomeness at this or whatever
it was that he said and the fact is this
is not hard to do but people think it's
hard to do and so you can look
disproportionately smart by doing it and
I am continually intimidated by the
intellect and the achievements of the
people in this room every time I talk to
somebody you know they have some new
area of knowledge that I didn't even
know existed about which they are expert
and so to be able to
something where without the expenditure
of neurons you can appear to be very
bright is is a valuable thing to have
and the final thing and this is quite
subtle and I won't call this out as much
in the talk I mean there's really
probably I probably have about eight
hours worth of material worked up right
now but there's there's a follow-on
strand which might lead to an
interesting conversation which is that
understanding the way that I and and by
proxy rich think about debugging might
help you understand some of the
aesthetic that goes into the design of
closure and the process and the
evolution of closure
I mean it's come to my attention that
that sometimes people out in the world
have more than one opinion about how to
do things in the closure community more
than one opinion about how to do the
getting started experience more than one
opinion about how to do documentation
more than one opinion about how to
handle exceptions we have lots of
different ways of doing these things and
I'm not going to lay out a here's one
way to do it all right it's fantastic
that there are multiple different ways
but I think that the scientific approach
to debugging when you when you adopt it
it influences what kinds of tools and
practices you're gonna reach for in
other areas and so that's a very
worthwhile of follow-on conversation
tonight those who continue on into
office hours office hours will be held
at you know whatever local bars you know
stay open the latest second thing is a
rationale for why to have me talk about
this there's a couple of reasons I'm a
closure screener
I am also I mean closure has a process
which is not identical to every other
github project out there as you may have
noticed and so I in fact push all of the
commits and have for many releases now I
read every line of code that goes I've
read every line of code that went into
closure since the 100 release so I've
seen a lot of things I've seen many more
things that didn't go into closure for
various reasons and I've seen things
that were bugs and things that were
thought to be bugs and not bugs
I also typically am the the place of
last resort and atomic support that
doesn't mean that you're on fire if you
call and you actually
I answer it answer the phone right we're
just rotating around but but I'm often
the back stop on the hard problems and
so that has given me an opportunity to
to own a system for a long period of
time and to observe bugs that we've had
and bugs that other people have found
that we had and bugs that people have
had in their own systems that we've
ended up helping them out with because
of the support relationship also I'm
gonna make a little bit of appeal to
authority here I earned this gray hair
I've been doing this for a long time and
and my hair was not great at all when I
first started debugging so I'm pretty
sure that that most of this you know is
directly related somehow the alternate
theory is that it has something to do
with children and finally I am really
lazy I'm not the most terribly lazy
person in the world because you'd have
to be competitive right to be that lazy
but I really
I really don't want to work hard and so
I want to have an approach to solving
problems that allows me on most days to
close up my laptop at five o'clock and
spend time with my children and have a
tasty dinner and a glass of wine
whatever watch whatever HBO is most
violent show you know it currently is on
TV and go to bed so I'm lazy and you can
benefit from you know programmers we're
all about laziness right we're all about
finding tricks that will let us do
things in easier ways and so I think
these all contribute then the question
is oK we've talked about why debugging
and why maybe I should talk about it why
should we talk about debugging in a
closure context well Karen Mayer one of
my co-workers pointed me at this thread
on reddit I believe and the text of this
doesn't really matter but but you can
sort of read it it as you know blahdy
blahdy blahdy blahdy blah errormsgs
blahdy blahdy blahdy blahdy blah and
it's basically the sort of beginner
experience of closure a lot of people
when they are new to closure
particularly struggle with debugging
their problems and figuring out what's
wrong when something doesn't work and so
there's this long thread on reddit where
people talk about the problem but this
was actually the the post that launches
it and it GATT and it sort of captures
you know some of the tone and a lot of
times people do things like well I had a
problem and I got a cryptic error
message or I was interacting with a
dependency or aligning and plug-in or
something I didn't understand even where
to start looking for my problem or I had
a air quotes type error in my system
that would have been trivially found if
I was working in Java and I now feel
flummoxed and so there are a lot of
people whose experience of debugging
closure is is fraught or unpleasant and
my experience has not been like that and
I'm not saying that there's they're
wrong and I'm right but I'm saying that
that you know I might have some ideas
that could make it Pleasant for others
and so that's kind of an objective here
now before I go further how many people
in here show of hands have helped to
develop tooling in the closure ecosystem
right worked on any kind of tool
so a lot of interesting tools in the
closure ecosystem there are a lot of
interesting tools about editing about
debugging about visualization of data
about visualization of flow people have
built a lot of different things and this
talk is not about that and I want to
make it clear upfront that this talk is
not anti that right I am a huge
proponent of using tools that being said
if you do a Google search for debugging
and I did this you know the vast
majority of the hits are about tools
right overwhelming well more than half
of the Google hits all the resources out
there are about tools and so I don't
think you'd be getting you're sort of
into the day keynote value-add if I told
you how to use tools because there's a
lot of good advice out there and the
thing is about tools and second that's
interesting
the thing with tools is we're not
directed by tools right there is no TDD
out there that stands for tool directed
development we wield the tools they
don't wield us and so when I think about
debugging I want to have a set of ideas
in my head that give me direction or as
that great philosopher of computer
science Yogi Berra said if you don't
know where you're going you might wind
up somewhere it place else this is one
of the biggest problems when people
start debugging a system right when from
my from my experience and a support
perspective you know what you discover
the first thing that happens when you
start trying to help somebody with a bug
the first thing you find yourselves
wanting to do is like I really want to
go back in time to when you first
thought there was something wrong before
you started doing anything I want to go
back to that point where you started
acting before you had a plan before you
knew where you wanted to go you know
it's so often the case that and I'm an
advocate of being a self-starter right
so by all means try things but have a
plan and then finally why would we talk
about the scientific method you know I
had considered for a while that the
approach that I used to debugging was a
scientific method but I had not you know
read the literature on the scientific
method
past having had a science degree in
college and I had not read what people
had to say specifically about the
scientific method so of course I turned
to Google to find out why I should care
about the scientific method and what I
discovered is that the scientific method
apparently is because rainbows which I
have no idea why I'm pretty sure that
all the sort of convey and you know the
edward tufte people out there are just
you know pulling their hair out like
what are these colors even mean and so
rather than use one of the pre-built
graphics with the meaningless come
colors i've made my own scientific
method for debugging graphic it's not
that different from any of these accepts
got all the colors removed so because
it's also sort of the limit of my
ability to manipulate OmniGraffle so
what is debugging all about well
debugging typically starts with a
failure so a failure and by the way in
in Kahn's tradition all the terms I'm
introducing I'm gonna give you something
from the etymology dictionary on them
advice to future you know proposal
submitters if you use two or three words
that are slightly unfamiliar and make
reference to etymology in your abstract
clearly clearly that's something that
that is valued in this space so what is
a failure a failure is a lack of success
or better it's an omission of expected
action alright I expected something and
I got something else now as a result of
this failure you form a hypothesis and a
hypothesis is an explanation that's made
without complete evidence right without
necessarily enough evidence to answer
the question actually maybe you don't
even know whether you have enough
evidence to answer the question maybe
you do but you just haven't sort of
worked through the process enough and
that acts as a starting point for
further investigation then given that
hypothesis you perform an experiment
which is a test or a trial and after
that experiment you make some
observations where an observation is
active acquisition of information from a
primary source now in a good experiment
one of two sort of equally good things
can happen you can discover that the
observation falsified your hypothesis so
falsification is a deductive process
using modus tollens you say I have a
high
this it implies there's something I
should not see there's some observation
oh that I shouldn't see I run the
experiment and I see oh well if I might
if my thinking has been rigorous and the
hypothesis says if this is true then not
oh should happen I see oh my hypothesis
is now dead and that's great right I
know it's disappointing if you're trying
to get grant money in in the kind of
regular everyday science but in
debugging this is good news right we've
now killed a possibility and that's good
the other possibility is that the
hypothesis is supported by the
experiment but maybe you still don't
have an answer that's sufficient to let
you say I've isolated it at which point
you need to do refinement and refinement
is removing impurities or unwanted
elements right it's subtractive so I
have some sort of story here now my
story may have to get bigger to remove
elements so I'm not actually saying the
number of words and your hypothesis is
going to go up or down but the
conceptual size of it is going to be
focusing in on whatever the actual
problem is and then you know eventually
and I'm not going to promise this
happens after two or five iterations but
eventually you have a hypothesis and the
hypothesis is something that offers I'm
sorry a theory is a hypothesis that
offers has been validated by predictions
now this does not mean a theory is
guaranteed to be true it does mean
though that it is true given all of the
information we have so far right so and
this is another place where people go
wrong and I see this all the time right
you know I have 99 pieces of evidence in
favor of this is where the bug is and I
have one little piece of evidence that
flat-out contradicts it and I don't
understand that piece of evidence and so
will 99 is pretty good I'm just going to
keep going right if you have that one
little piece of evidence that says
you're dead you're dead right you can't
end and we get attached to our theories
that's the other thing you get emotional
about it you're like you know what I
really like that theory and look it had
99 good pieces of evidence in favor of
it you can't do that now it turns out
and sort of I get dragged into the sort
of history of this you know researching
the you know what other people have said
about this for this talk it turns out
that there are all sorts of objections
to the notion that the scientific method
is house
actually gets done and some of the
objections are superficial or idiotic
perhaps but there are there are more
serious concerns as well so one of the
things that I studied in graduate school
is Thomas Kuhns book the structure of
scientific revolutions of pithy ways I
could unfairly summarize this book but
it essentially says that that science is
a little bit better than politics or
religion
in that you wait for people whose ideas
are going to die out to die instead of
actively going and killing them right
but it basically it basically paints a
pretty unpleasant picture that science
is an extremely social process right
that and how could it not be rites
performed by people and then once you
have this notion that science is a
social process it's opened up to what
are the social priorities behind science
and you get all these moral challenges
to science so these hot-button things
you know they change from decade to
decade one of the hot ones when I was in
academia was the bell curve right this
notion that we were going to use science
in air quotes to sort of split the human
race up into different groups and say
you know these groups are more
intelligent in these groups are less
intelligent that's not the case
certainly not justified by the evidence
but these challenges are real and here's
the funny thing right here's the good
news
right there's all kinds of stuff that I
just said that we could have very heated
debates about and get angry about and in
the context of debugging we don't have
to worry about anything any of that
stuff at all it turns out that debugging
if the scientific method is the measure
of how science is done debugging is
actually more like science than science
right science is hard it's difficult to
imagine how you could take that little
process I just had and turn a crank on
the side of it and come out with a
theory of gravity or the theory of
evolution right or any of the other most
important theories that people have ever
funke but that's not what we're doing
right that is not what we're doing we're
doing something that is far more
constrained right and debugging is
deductive not inductive right we're not
trying to come up with a grand theory of
everything right we have a grand theory
of everything it's called the software
system we're running we're trying to use
that to prove some very concrete thing
right this thing happened also there's
not a big political problem right
there's not usually I mean in my
experience debugging in and of itself
has not been politically fraught right
there are people who are experiencing
bugs and they want to see them fixed and
as the developers we want to see them
fix so we're not having these kinds of
arguments and there's not generally a
lot of moral outrage right and there's
not any kind of like academic left
saying you know there is no real reality
so you can't actually you know you can't
actually prove that there was a bug here
right there's there's none of that
so I mean seriously this is really good
news so so unfortunately the scientific
method may not be all that great for
science but it absolutely rocks for
debugging now having said that we can
drill in a little bit and we can start
to use some more specific terms that are
more about troubleshooting in debugging
so I'm gonna replace the word hypothesis
with cause we're looking for a cause and
the definition of a cause is an event
proceeding in effect without which that
effect would not have occurred that
sounds awesome right if I knew the cause
of the problem then you know I could
eliminate the effect from happening so I
caused strange loop right as we said
earlier so you know so the problem with
causes is that they're not sufficient
right then cause the universe exists
right if the universe didn't exist this
wouldn't happen that's a cause but it's
not a very interesting cause and it's
not one that is when we're debugging
software we're not in a much of a
position to do anything about so you
want to get from a cause to an actual
cause and an actual cause is the
difference between the actual world and
the closest possible world in which the
effect does not occur
and we I mean we could go down the rat
hole of getting rigorous about closeness
and I'm not going to do that in this
talk but we can be I'll just be
intuitive about closeness right if you
have you know this agglomeration of five
things that all contribute and with four
of them it doesn't happen and one of
them it does then that one is closer to
being the actual cause and so that's
what the pairing down process does we
want to start with an idea about a cause
and then we want to narrow down to an
actual cause and then we have this great
word fix in this context a fix is just
the last experiment right the first
experiment is reproducing the bug and
then there are a bunch of experiments
that are about hypotheses and a fix is
the last experiment this captures a
really important idea which is if you
haven't fixed it and run an experiment
that shows it's fixed you haven't really
identified the bug yet you may be
strongly suspicious that you've
identified the bug but you haven't
actually identified the bug until you've
fixed it so I'm going to take this
scientific method and I'm gonna apply it
in a really slapdash way to a problem
and I'm gonna start I'm going to start
with a problem that I grabbed off of
stack overflow this is a very tiny
debugging problem you can probably do it
in your heads so I'll show it to you
this is a question on Stack Overflow why
is this partial not working so I define
partial join as the partial of closure
at strings /join with comma and then I
call partial join on fubar and I get
back the scary error message class cast
exception
you cannot cast Java dot length string
to closure doubt Lang by fun and there's
a scary number in it so there's no way
we can possibly figure out what the
problem is with this terrible error
message
so what should we do well we have a lot
of choices here we could make error
messages better right we could have an
error message that says oh I see you
were trying to make a partial with stir
join and you know that has this exact
scenario we could anticipate every
possible scenario anybody can ever get
into and not to be unfair to error
messages we could go a long way towards
having better error messages without
being ridiculed on it error messages can
be made better it sounds like a good
plan to me we could have better Docs I
mean there's an answer to this problem
in the documentation we could use a
debugger in fact in this case it might
be even that syntax highlighting and
paren highlighting would have tipped you
off as to where the problem was and I
didn't give me that maybe static typing
would have helped I in fact static
typing would have helped right you can
imagine a scenario where static typing
would have solved this problem using
some sort of schema and schema
validation would have helped and in fact
this one's so easy that you might have
stared at it and just known what the
problem was so all of those things are
useful and all of those things should be
part of your toolkit except for maybe
staring at it all right that one's
actually really quite weak and we'll
come back to that but the important
thing to realize here is that science is
more general-purpose and requires less
on hand to do than any of these other
things I'm gonna say that again because
it's so important all right science is
more general the scientific method here
not science let's say the scientific
method or we can even be what is it that
the academics say hypothetical ISM right
let's just stick with scientific method
the scientific method is is better sauce
than these other things because it's
more general and because it doesn't
require anything to exist in the world
except your brain before we go in so
let's try that we're gonna go down this
road and I'm gonna do this as I said in
a really slapdash way my hypothesis is
gonna be well look there's only three
things here why is this partial not
working well join doesn't do what I
expected so it's a very fuzzy hypothesis
or partial doesn't do what I expected or
def doesn't do what I expected right
that's it has to be in one of those
three things and those are the only
three things there it's a very small
problem to check and so the experimental
approach
this I will propose a heuristic for
small problems like this which is you
should do a bottom-up check from the
repple right pick the form that's at the
bottom or that's on the inside and check
that and in this case we're done right
I checked the very bottom form here the
bottom form was closure dot string /join
comma which returns comma that seems
like almost certainly wrong because if
it was just going to return comma I
could have just passed in comma to begin
with and then when I look at the next
one partial join if we if we substitute
in the result of that write in the next
in the original form we're now partially
over comma which is using comma as a
function so by applying this now I got
lucky this time I might have had to do
three little things at the repple to
figure this out but I got lucky and I
only had to do one so weak science is
stronger than strong tools in this case
I had a poor problem statement I really
never got clarity on my problem
statement
I had really poor hypothesis each one of
my hypothesis was that was out that was
out sure that was out all right those
are not very specific my experiments
didn't even really have stated goals
right I just sort of took this heuristic
approach and I didn't have much domain
knowledge right rorke hypothesized that
that a beginning closure program are
doing this didn't have to have much
domain knowledge to figure it out so of
course at this point I have stacked the
deck in favor of the scientific method
by picking a trivial problem trivia
problem is matter what you do right any
one of the approaches would work so it
hardly matters so now we need to talk
about hard problems and what it would be
like to do the scientific method well so
what do we need we need to do each of
those steps way better than we just did
we need clear problem statements we need
efficient hypotheses we need good
experiments useful observations and we
need to write things down I'll start
with problem statements right the
antithesis of a good problem statement
is it didn't work right it didn't work
is hiding everything behind pronouns
that's no good right what you want is
the steps you took what you expected and
what actually happened this slide right
there that's worth the hundred bucks
send me your checks
all right this is not a hard thing to do
this is not harder than and in fact
sometimes saying this actually causes
you to realize what the problem was
going from the exercise of verbalizing
it didn't work from the exercise of
verbalizing you know I stepped away the
car and it started to all right I didn't
put it in neutral whatever so so problem
statements you know there's a lot as I
said I have eight hours of material
there's a lot we could say about problem
statements this is all you're going to
get right now but this is an order of
magnitude better than it didn't work so
start with this the next thing you want
to do is to and I am gonna give me one
second why no it's not working
it is debug this
you have a live performance now hmm so
my hypothesis is actually its keynote
right which is just terribly uh terribly
painful so I want fine I'm not going to
tell you how many people remember this
guy yeah I mean it's uh it's Casey
Affleck but it's the Malloy twins from
Ocean's eleven and there's this great
scene where they're playing 20 questions
and he they're playing 20 questions
and he's like am i alive yes am i person
yes Evel Knievel damn it and and it's a
joke on an idea that we all know it's
like the opposite of what you want to do
right it's what my five-year-old does
when we're playing 20 questions he's
like okay we're gonna play 20 questions
and he looks at me he goes are you a
tall oak tree it's like no here you know
what
we all know we all know what we need to
do here when we're forming a hypothesis
right you want to form a hypothesis that
ideally carves the world in half and so
it turns out that the naming around this
is contested right most of the time you
hear people say this is divide and
conquer but if you ask an algorithms
person they would say that's actually
not it because in divide and conquer you
go down both branches and so they want
to call it decrease and conquer which I
think kind of sucks because because it
doesn't you can't I mean decreasing
conquer could be a linear time thing
right you could decrease by one every
time and the important characteristic
here is proportional reduction right I
want to take the space of where the
problem is and I want to reduce it by
ideally half but as you know if you've
done any algorithm analysis right if I
can get rid of ten percent of the
possibilities on every step I'm gonna
have the answer really quickly right
that's only going to differ from getting
rid of half the possibilities by a
constant factor the number of steps that
I have to do so the question is then how
do you take the space of I don't know
what went wrong and turn it into you
know something that cuts the world in
half well this is going to require
domain knowledge right there's no two
ways about it having said that it does
imply that you should be super cautious
on your initial step if you don't have
much domain knowledge right if you don't
have much domain knowledge it's possible
that your initial step leaves out of the
entire universe of possibilities what
the actual problem was and so having
said that everybody knows where to look
for their bugs right if I showed you
your application stack and something
just stopped working all right how many
of you are gonna guess hey how often
right it just doesn't right how often is
it physics it's it's pretty rare right
physics is not usually I mean sometimes
it is and that's exciting but
and I have to say that on every open
source
you know project I've ever worked on you
know you get bug reports and some of
them are bugs and you fix them and some
of them are not bugs and some of them
are bugs intense something underneath
you right we've never had a closure bug
report that we had to forward onto
physics right yeah I haven't been any of
those I mean in fact very few even have
to do with you know JVM much less OS
operation and you could make other
stacks like this there's there's a whole
conversation we could have about sort of
developing a notion of what the possible
space is and you know where you can go
in it but my point is you don't have to
be that good at it anyway right as long
as you can get rid of some proportion of
the possible causes with your experiment
you're gonna quickly find the answer now
what is a good experiment a good
experiments reproducible right you start
by reproducing the bug it's driven by a
hypothesis right people say they're
experimenting when they're just trying
 right that's not hypothesis driven
you have an idea this is the case and
then you have an experiment that
provides more information to help you
refine that idea also experiments are
small and when you're when you're making
changes to a system you change one thing
at a time right because if you change
two things now you just have to go back
to figure out what the impact that was
when when something changes right you
don't you haven't actually gained
information so I'm gonna give you a quiz
on this if you have a bug let's say you
want to report a bug in your own app
right and you know you're handing it off
to another member of your team to help
you look at it which of the following
things should not be in your repro case
closure test cursive prismatic schema
midge potemkin in rebel lining and
lining and plugins core type to test
generative right unless you're actually
unless you think the bug is in one of
these things which is one of these it
really stands out as boy you really
wouldn't want to have that in a reaper
case and the answer is it's a trick
question you don't want any of these in
your repro case unless your theory is
that you know the shopping cart on my
system doesn't work when i'm developing
inside cursive and using closure dot
test then your repro case shouldn't
having
about cursive and closure tests and
callin and other people will thank you
for that because he he doesn't wanna get
bug reports about cursive that about
that kind of thing either so it's
incredibly important to remove things
that do not contribute to your
hypothesis statement and it's a freebie
right I was saying earlier that you have
to have this like mental model of the
universe to allow you to narrow down the
things that aren't yet right this is a
freebie right in your bug these things
are not it so start by taking them out
make a really tiny thing that shows the
problem now when you're making
observations what's that all about well
one thing you need to do is you need to
understand all the outputs all right if
your system adds outputs right if your
system has four or five outputs that you
understand and then it has one that's
unrelated to your current problem and
you don't understand it
brakes screech right if you don't
understand it how do you know if it's
related to your problem or not so you
need to understand the outputs from your
system and you need to be suspicious of
correlations where's the code it's in
the last the bug it's in the last five
lines of code you wrote right quite
often and so if anything correlates with
the prob with a failure appearing then
you want to suspect that and in order to
make operations you need good tools you
need debuggers you need logging you need
metrics all the kinds of things that let
you then basically all those things give
you more outputs right they turn things
that are blackbox into things that are
white box and it's amazing that we have
this inversion that when you do a Google
search for debugging thing more than 50%
of it is about just this one sub bullet
of one sub point it's important right
you need to have tools and in fact there
are more things that tools can do than
just it so we'll talk about those later
so I'll give you another example and
while I was writing this talk I had
several things that happened to me in
the course of the week so this is one
that happened to me last week I was
working on a closure app and I got this
error message in the log it was
completely unrelated to my problem right
my problem had nothing so log back is
the configuration file for log back
which is one of the Java logging things
which we're also happy that Java did
such a good job with and so I get this
message and I'm having this cryptic
problem in a subsystem that
I'm working on and I know this can't be
it because it just has to do with
logging except that actually this was it
alright so it turns out that after you
know two minutes looking at the problem
I said you know what before I think
about this problem I'm gonna go run down
what this log back thing is about
because I want this to be clean well log
back occurs multiple times on the class
path because some library because of a
build problem had been copied into a Lib
directory twice at two different version
points so I had food bar version 2.1 and
food bar version 2.2 now what the JVM
helpfully does in that scenario without
you know further configuration is if
those things have overlapping but not
entirely union set of named things in
them you can get some of them from one
and some from the other and they're not
compatible with each other
and it can lead to all manner of errors
that are absolutely cryptic which is
exactly what happened but by tracking
this down I was able to say you know
what I saved myself all that time and it
would have been even using the
scientific method and trying to sort of
bisect towards the problem it would have
taken a while from the symptom to come
up with a better cause to investigate
than the one that was standing in front
of my face
write things down this is the single
most important piece of advice I'm going
to give in this talk write things down
write the problem statement down don't
say it write it down write your height
right every hypothesis down write what
the experiment would show right why the
experiment makes sense
write a justification for the experiment
before you run it and write down your
observations and this is something that
we all know sort of intuitively consider
the gamemaster mind so in the gamemaster
mind that the colored pegs down at the
bottom represent a code that the player
is trying to guess and the colored pegs
across the top represent past guesses
and then the red little red and white
pegs represent information feedback that
you've gotten on your past guesses so if
you view this as a scientific method
right you have a series of hypotheses
about what the colors are and you're
getting feedback from an experiment
which is the human player is scoring
your guesses we don't have to talk about
the exact scoring if you can't remember
how mastermind works that doesn't matter
the thing that's dominant here is that
doing your experiments with write
without writing things down is like
paint playing mastermind like this right
you're saying that you're gonna keep in
your head the entire state space of all
the previous things you've tried and
even the one try you're currently on
you're gonna hold in your head so I'm
sure you can barely see one of those
through my color there and the thing is
that not writing it down right you might
think that of the seven deadly sins that
sloth but it's actually not it's hubris
it's amazingly arrogant not to write
things down because basically what
you're saying is I'm such a badass at
solving problems then I'm gonna solve
the meta problem of keeping track of
everything in my head just to show off
when that's actually probably a harder
problem than the debugging that you're
doing if people do this all the time and
staggering ok everything I've said so
far probably could more or less apply to
the use as a scientific method in a lot
of different domains and because we are
here at a software closure conference
I'm gonna give you some software
specific advice the first piece of
software a specific advice is something
I was reminded of as I was reading
through the literature on debugging and
what programmers
said about debugging in the past and I
usually don't say mean things like this
and it's gonna come off a meme and I
can't help it
don't you see hey hey I mean the history
of debugging is just riddled with oh my
god
this thing has so many pitfalls and
traps in it and and it's not that C is
bad I mean we could have a separate
conversation about it's that this is not
a level of abstraction you need to be
working at all right this I mean and
sometimes it is right and if you have a
domain that's like it would be
impossible to do this in Java or closure
or what Python or whatever then you can
use C but boy you shouldn't reach for
that as the default tool then in 2015 in
most domains a second piece of software
specific advice the failure is not the
defect right the failure is not the
defect and the way this comes up the
most often in software is assuming that
the exception has anything to do with
the actual problem I mean it does have
something to do with it that's how you
know right but that it's in some way
directly translatable into the problem
so I'll give you an example we had a day
Tomic system that had a large query high
CPU utilization and an illegal state
exception in Hornick q so your first
guess is you know Wow illegal State
exception hornik you must be broken
well let me just tell you Hornet Q is
not broken Hornick U is a pretty awesome
piece of software I've had fantastic
interactions day Tomic has had two
Hornet Q in it from the zero point zero
days I've had fantastic interactions
with the Hornick you team and the only
time that working with those people that
we ever got to the point where we
tracked a bug down to the point where it
was underneath the atomic and in Hornet
Q it actually was underneath Hornet Q
and in nettie right which is pretty
unusual by the way you don't find those
those kind mean a real this is a you
know a show-stopping nettie plus ssl
doesn't work in this scenario bug so
pretty darn unusual so this was not this
bug and I'll just tell you I'm sort of
giving you a little bit of unfair
advanced information the answer was not
it's hornet q and there's another
important philosopher of debugging who
can really help us out here in a couple
of steps and that is house so
House says it's never lupus well in
closure programming on the JVM we have a
kind of anti lupus
right we have the thing that it always
is but it doesn't look like it at first
and the thing that it always is is
garbage collection right the actual bug
is always garbage collection just as odd
just as in house it's never lupus
enclosure and in Java it's always
garbage collection and there's several
reasons for that one is most
applications are not designed to
deliberately induce out of memory errors
because they're not designed to
deliberately induce those those code
paths are not checked very much so
you're in kind of uncharted territory
the second one is out of memory can
happen anywhere so there's no line of
code where you can look oh look there's
the critical section where memory
doesn't possibly run out you can't do
that also out of memory can appear is
almost any other exception because once
something fails to allocate over here
you can get a cascading series of
reactions where the actual exception
that's reported back is radically
different finally well not even finely
sub finally when you get close to out of
memory you get a radical change in
thread scheduling as the GCE thread is
running all the time and all of a sudden
things are happening in orders that they
never happen in everyday life and so all
those race conditions that would
normally take your system 2,000 years to
expose by accident all of a sudden it
takes them 2,000 seconds to expose by
accident so lucky you thanks garbage
collection for helping us find those
race conditions and finally out of
memory related problems tend to cascade
so always be suspicious of memory when
you're having problems in a garbage
collected environment
now another piece of software specific
advice is to read the entire fracking
manual and so the thing is that a bug
almost by definition starts out as an
unknown unknown right if you knew more
about it you'd already be well on the
way to fixing it you don't know what's
wrong so you don't know which part of
the manual you need to read and this
means that if you want a set of docks
that are good for debuggers that set of
docks ideally is short and specification
like now this goes against other
objectives
right short Doc's may be very difficult
to consume they may not be very
narrative they may not be very tutorial
but they are a good place to say you
know if I have to read the whole thing I
would like for it to be 50 pages long
and not repeat itself if I want to learn
in a more you know tutorial kind of
environment I'd like for it to be a
thousand pages long and repeat itself in
various ways and anticipate problems
that people have every day so good Docs
for debuggers are specs and we'll go
back to the partial problem again so if
instead of doing experiments we had
chosen to read the docs here we could
have pulled up the doc string for join
and we would have seen that join has two
ERA T's and the first era T takes a
collection and then it returns a string
of all the elements in that collection
oops so one step + experiment would have
identified this problem one step +
reading the docs would have identified
this problem right would have been the
first thing you found when you went to
look for it so let's take some of these
ideas back to this more tricky debugging
problem that we encountered and that is
this large day Tomic query high CPU
utilization illegal Hornet state the
illegal state exception and now I want
to give you one more really piece of
interesting information it happens on
Cassandra in production but not on h2 in
development so what should we do right
now hmm
if we're going to apply the method what
we need to do right now is look very
suspiciously at that's last sentence is
that last sentence and observation I
hypothesis what is it it's actually not
really anything right it doesn't have a
subject alright it's a sentence without
a subject in it so there's some sort of
amorphous it that happens on Cassandra
and not on h2 so let's try to define
that it a little bit that certainly
sounds suspicious so let's make the
smallest possible thing that could show
us that so we'll create a test
environment with the same data that
production had and then we will run a
little file that we're gonna write which
is gonna be 10 or 15 lines long test dot
Java with an atomic peer and that's just
gonna come up and it's going to perform
the problem query in a loop and if
Cassandra or D Tomic or Cassandra
stay Tomic is trivially broken in some
way we're gonna see that there and then
we can run the same test again against
dev and not see that there and now we
have eliminated an entire universe of
complexity right all of the application
code that was in play all of the tools
all the third-party dependencies all
that other stuff has been mechanically
eliminated well so we did this and what
did we learn the problem happens on
Cassandra that's this is our statement
happens on Cassandra but not on h2 so we
create the trivial repro and it happens
with Cassandra and guess what it's lupus
so it turns out that this query was just
using more memory than the JVM had and
had no chance of winning and so then
it's like well that's pretty suspicious
because it's not at all obvious why
Cassandra would be such a memory hog and
use a ton more memory than h2 that seems
really you know because under has a good
reputation why would their driver you
know be messed up like that well you
know stop hypothesizing ahead of the
evidence let's go test h2 the dev mode
storage and guess what the trivial repro
happens there as well and that leads me
to the other important observation that
we get from house which is it everybody
lies right it turns out that it was not
the case that the cassandra and the h2
behavior was different it turns out that
there was a miscommunication about what
had been tried and so and by the way
never attribute to malice what you can
contribute to accidental
miscommunication this is not lying in
the sense of covering up your drug use
which is always what it is on house
right this is this is this is lying as
in saying a statement which turns out
not to be true I'm having a lot of
trouble teaching my kids not to always
call that lying right any statement
which turns out not to be true later or
turns out to be slightly mistaken
they're like daddy you lied like
actually we tend to use that word to
imply malice so this is the non malice
kind of lying this is the you know go
back and double-check statements and
again going through the exercise of
reproducing it
I mean imagine how much fun we would
have had if instead of doing that we had
sat across a conference table and the
diatomic team had pointed at the
Cassandra devs and the Cassandra devs
had pointed at that atomic deaths and
said you know you guys have messed up
somewhere because this works in other
contexts right we didn't even have a
problem statement you know until we did
that now the final piece of advice I'll
give you as software developers is that
debugging is fundamentally a search
operation
we should ponder that a little bit
because this is something we actually
know how to do right we actually know
how to write algorithms that do it so
instead of limiting our thinking about
debugging to the ability to see things
or stop things why don't we write
programs that do this job that I just
described right implement the scientific
method in code make something that
automatically generates a hypothesis and
then automatically runs an experiment
and some of you may have done that in a
small way using git bisect write get
bisect is actually partial automation of
the scientific method right you write a
little program that's going to test for
something and then get will bounce back
and forth cutting off cutting the world
in half every time until it finds the
boundary at which your test changes
right that's automating the experiment
part as and and we have you know we have
the set of possible world states already
given to us from get well this idea is
well suited to being taken further and
so I'm going to give you your first
maybe closure cons call-to-action this
year which is go and read this book why
programs fail by ondrea's seller and in
particular there's a lot of ideas in
this book that have now become standard
practice so there are chapters you can
skip but in particular read chapters 5
through 7 in chapters 11 through 14
which actually layout algorithms for
doing automatic debugging of programs
and so what they're doing is modeling
the entire state space of a broken
program and the entire state space of
similar programs that don't exhibit the
failure and then shrinking the
difference in those state spaces do we
know how to do shrinking in the closure
world we're pretty good at that right we
have tests check and test done or if
there are algorithms for this so I would
love to see maybe a closure west or
maybe a closure Kahn's next year
somebody giving a talk about taking the
ideas in this book and realizing them in
closure most of the ideas in the book
are demonstrated with Python code and I
don't think and pythons a great language
but we should not let them have all the
fun right this is a problem which is
well suited for closure because as you
might imagine one of the places where
this gets complicated is I can just
glibly say the entire state space of a
succeeding application and the entire
state space of a failing application but
how are you actually going to capture
that well that's a hard problem but it's
easier in a language that doesn't have
very much
date right so to the extent that it's
possible at all right in a ball of mud
Python program it ought to be a lot
easier to do in a closure program and oh
by the way having a language that hat
you know models code as data and passes
everything around is data we are
uniquely in a good position to implement
these kinds of algorithms so that's the
Sun that's the software specifics work
at a high level of extraction remember
the fault is not the defect
remember that GC did it read the manual
twice don't trust because people I
reproduce and let's go out and automate
some things and you know I've been told
that six bullets is too many to remember
at the end of a long day so I'll make
that even simpler and say let's back up
and talk about science a little bit it's
really easy know where you're going
right remember Yogi Berra if you have if
you're heading into a particular
direction you have a much better chance
of getting there and then make
well-founded choices this is where
developers both beginner and expert make
the most frequent mistakes they're under
pressure and something's not working and
you say and we've all done it and you
say you know what I'm gonna try this
stop and ask yourself why should I try
that stop and ask plushy Cthulhu you
don't even have to talk to another
person you could say plushy Cthulhu I
had this idea about what's going wrong
and why my keynote presentation didn't
do what it was supposed to do during the
middle of the talk maybe you and I can
sit down afterwards I'm gonna talk you
through I'm gonna give you a hypothesis
the effort of doing that and writing it
down that's the final thing once you've
decided you're gonna take any kind of
action justify that in writing first
write these those tooth steps making
good choices and writing your steps down
are gonna turn a haphazard random walk
around the problem into a directed
focused cruise to an easy solution and
an early night relaxing at the bar with
your friends thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>