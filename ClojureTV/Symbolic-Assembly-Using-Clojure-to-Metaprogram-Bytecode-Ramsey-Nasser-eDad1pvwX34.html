<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symbolic Assembly: Using Clojure to Meta-program Bytecode - Ramsey Nasser | Coder Coacher - Coaching Coders</title><meta content="Symbolic Assembly: Using Clojure to Meta-program Bytecode - Ramsey Nasser - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symbolic Assembly: Using Clojure to Meta-program Bytecode - Ramsey Nasser</b></h2><h5 class="post__date">2017-03-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eDad1pvwX34" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the last talk in this room
the title on the program is symbolic
assembly using closure to meta program
byte code and hopefully 40 minutes from
now more of these words will will have
meaning if they don't have meaning to
you immediately
my name is Ramsey Nasser this is where I
live on the Internet
I'm a game designer I'm an educator or
closure programmer and a programming
language nerd I'm just sort of really
enjoy hacking on compilers and thinking
about programming languages and I want
to think closure West for accepting this
talk I have a friend who pointed out
that I practice a form of TDD which is
talk driven development which means that
I tend to get stuff done when I have a
talk to give about it so a lot of
deadlines were met for this talk so
thanks for thanks for having me
so the subject that I'm going to talk on
I've given a couple of talks before that
were much deeper dives into the
technology and the theory of symbolic
assembly and byte code meta programming
there's about three hours of me waving
my hands on the internet talking about
this stuff but in the in the limited
time that I have today I'm gonna be sort
of giving you all just an overview and
an intro to this subject and I'll try
and provide links to these talks if
you're interested in getting deeper into
it so meta programming bytecode so what
I want to sort of define the word
bytecode here because this is a little
bit of a loose term really what I'm
talking about is a low-level target
language so byte code gets used bit code
gets used assembler gets used it really
depends on the platform that you're
targeting but generally you'll have some
kind of lower-level target that you're
interested in so maybe it's an actual
machine maybe it's a virtual machine and
that target will have some kind of
language that it natively understands
you know an actual Intel machine will
understand x86
machine code for example the Java
Virtual Machine understands JVM bytecode
and so on so this is a talk about
wrangling those lower-level
instructions and and generating them now
why would you want to generate a code in
a lower level language the most
immediate answer is you might be writing
a compiler and a compilers basic job is
to take source code that a user prepares
and convert it into some other language
generally speaking in practice it tends
to be language that's suitable to be run
on some kind of target platform this is
a closure function and a snippet of the
bytecode that it gets compiled to on the
common language runtime and the c-sharp
Virtual Machine the technique that I
want to share with you today is not
strictly speaking limited to compilers
but I am going to be focusing on
compilers because they're the most
immediate they benefit the most
immediately from from what I'm going to
talk about so just a show of hands who
here does compiler work a couple of
people dabble in compilers who here
recognizes this book alright great this
book to me is the most telling example
of the like perception of compiler
development in the software engineering
community it is a dragon poking its head
out of a computer at a programmer in
shining armor the dragon in case you
didn't get the metaphor is tattooed with
the phrase complexity of compiler design
this is not a fluke so this is the
second edition of the book the dragon is
now wearing a t-shirt this is the same
thing so it's I mean writing a compiler
is perceived as this intractably hard
thing to do right and there's there is a
lot of irreducible complexity to writing
a compiler you're trying to reconcile a
human thought and intent with the
semantics of some target language and
that's just a lot of that is just hard
and there's no trick that'll just make
that go away this is my favorite quote
from the like from the compiler
development community this is one of the
GCC developers
saying that trying to make the hippo
dance is not really a lot of fun and
that's how he's talking about
maintaining GCC like older a C compiler
that a lot of UNIX tools use I feel that
a lot of the pain of developing
compilers comes from the fact that
they're built in weaker languages with
sort of deficient semantics like C C++
the API is immutable the object-oriented
iterating is very slow so developing
you're kind of trying to tackle a hard
problem using weak tools and I think
that's where a lot of a pain comes from
this is taken from the closure compiler
on the on the common language runtime
c-sharp just as an example to be a
little concrete about what I just said
of what
what compiler development tends to look
like you'll see a lot of ilg dot omit
calls does everyone see those an ilg in
this case is an intermediate language
generator and that's what you use to
generate runnable bytecode and you get
an intermediate language generator from
a method builder and you get a method
builder from a type builder you get a
type builder from a module builder which
you get from an assembly build if you
don't have all of these things you can't
even begin talking about bytecode
there's no way to reason about the code
you're generating abstractly you have to
set up this entire framework to just
start spitting them out into memory or
onto disk and once you've generated them
you can't really query which what op
codes you've you've generated you can't
take them back once they're put out
there give this mutable object that you
just sort of changing in place and
trying to get it right so hopefully
that's a sort of glimpse into what
making the hippo dance looks like um and
and I know this from experience trying
to hack on compilers and trying to
improve them and it ends up being really
really hard so the approach that I've
find myself taking two hard problems is
just how do we throw a functional
programming edit and then the question
is how do we rephrase the problem of
generating target code as a functional
problem and that is this sort of
the idea here and we're gonna look at
some concrete examples of libraries that
have built that use this but the general
idea is that if you can represent your
target as data as inert functional data
you can then use a functional language
to manipulate that data to structure it
however you want to or need to and then
only at a last step generate actual
runnable code from that data that if you
can approach compiler development or a
code generation like this it just turns
into functional programming and you can
use a repple you can things become
testable all the properties of
functional programming that you know
people in the closure community I think
agree are good
just emerge from that and and I
presented here as a as a general idea
because it really could be applied to
any functional language targeting any
kind of target low-level code but the
first concrete example of that that I
want to share with you all is a library
that I built called mage mage stands for
the Morgan and grand
mitr and it's the first sort of
implementation of this idea of building
symbolic assembler mages named after the
cross streets of the studio in Brooklyn
that I was working at when I built it
this is the corner of Morgan and grand
this is the kitchen table coder studio
where David Nolan is a member and a lot
of the sort of foundational thoughts
with or theory behind this came out of
conversations that I had there with
kvass peluda
was a friend of the closure community
it's also named after how you feel when
you're just generating byte code from
the repple this is a photo of me from
earlier today
took my armor off so mage is a symbolic
representation and an emitter for the
Microsoft intermediate language more
informally known as c-sharp bytecode so
mage fulfills steps one and three of
that earlier slide mage provides you
with a functional representation of byte
code on the common language runtime the
c-sharp virtual machine and it provides
you the way to take that functional
representation and generate real byte
code from it so why the common language
runtime I think it's a really
interesting VM in general especially now
that Microsoft is open sourced it and
acquired mono it has value types it has
real generics things that the JVM I
think struggles with and has very low
overhead Interop with C so there are a
lot of things about it just as a VM that
I think you know make it an interesting
target how many people who knew that
closure runs on it
that's good that's getting better I've
had I've had interesting theories in the
past about people wondering how we got
you know code to pipe to the JVM and
back to the CLR as fast as we did like
not knowing who there's a version of
closure that runs so there's a reader
conditional you know it's a real closure
man named David Miller has been
maintaining the port for many years and
it's it's a it's a quite a good port and
it's this version of closure that me and
my collaborator Tim's Gardner built a
library called Arcadia on Arcadia
integrates the closure programming
language with a game engine called unity
which is scripted in in c-sharp unity is
an industry-standard game engine that
exports to every imaginable gaming
platform and we basically just wanted to
make video games in closure and
fast-forward three years to me talking
about my coda closure list so there's a
readi community of people using
Arkadia to make games this is a wacky
procedurally generated skateboarding
game called proc skater that Joseph
Parker and Ryan Ryan Jones made last
year so Arcadia is it exists it's usable
closure on the CLR exists and it's very
usable people are making things with it
but the bytecode that it generates isn't
always the best and and it could be
better and for games we really do need
every last bit of performance which is
what brought me to start investigating
you know better bytecode generation on
the CLR so that's why I'm particularly
interested in this in this VM so what
does symbolic bytecode look like I think
it's easiest to start with an example
from real bytecode so this c-sharp
method compiles to the bytecode that's
listed below so this is a method that
takes four floats multiplies the first
two multiplies the second two sums them
and returns a square root and just to
like make things interesting it cast
them to a double implicitly if you could
see the return type there now the
bytecode reveals a lot of the semantics
of ms IL it's a stack machine so all the
instructions are run against an
imaginary value stack that they're I
haven't really found a good way to
visualize it you kind of have to just
imagine it so the first thing that
happens is load arg1
so the VM will load the first argument
onto the value stack and then after that
load arc to will load the second
argument onto the value stack so now
there are two values on the stack the
first and second argument mol will pop
two items off the stack multiply them
and push the result back on and so on
and so forth the other arguments are
loaded multiplied and by the time we get
to instruction seven
there's just the the results of the two
multiplications that we add together and
then finally we call the function that's
what a function call looks like in
bytecode and then that value is returned
so this is yes
Oh con 4r8 that is convert into a real
with eight bytes so Castro a double it's
kind of like the matrix you start seeing
it after a while it's all blends
together yeah yeah so what does this
look like as symbolic byte code this is
the this comes out of a MSI L
disassembler this is what it'll it'll
it'll give you back so now I've moved
the byte code to the top and the excuse
me I'm the symbolic byte code is now on
the bottom you see that it is a
mechanical translation from the real
thing to the symbolic thing and and so
this is a little different from other
similar projects like Tara or Timothy of
Aldridge's Mule near which are also
techniques or libraries to meta program
lower level representations but they
kind of bring their own organ aam extend
semantics to the to the game mage the
rule in mages if it's not in the spec
mage doesn't get it basically and
there's a couple of reasons to that it
makes a translation very easy there are
some some situations when writing a
compiler I'll wonder like Oh what would
C sharp do in this situation and then
I'll just ask it I'll write that c-sharp
code and transcribe the byte code and
you can do that because there's a
one-to-one mapping so symbolic byte code
here is this vector of invocations that
I'm calling constructor functions and
we'll see what they return on the next
slide but you see that load arg1 becomes
IL / load arg1 and so on and so forth
the interrupt / method that's just doing
a reflective lookup to find a function
pointer to the method with that name and
that signature so this is a rebel
session invoking those forms one by one
you see that load arg1 evaluates to a
hash map that is you know named space
qualified keywords to values mage core
up code load arg and mage core argument
one this is what I mean by the
representation of byte code
data I could look at this hash map I
could print it out I could put it in a
vector I could do whatever I want with
it there's no weight of reason about
byte code on the CLR like this
there's no they're not tangible things
that you can move around and you see the
result at the bottom of the Interop
method call it's just a sort of pointer
to an object oh and the the opcodes are
mapping to enumeration values that print
nicely they could have been they could
be key words too but the CLR actually
gives a nice representation just for the
opcodes not for the full instruction so
mage does a little more than just rap
the bytecode mage will wrap all of the
structural elements of MS il including
so byte code lives in a method body and
a method has a signature and a name and
Method has to exist within a type and a
type a stay exists or that a module in
an assembly it's a very verbose VM but
mage will wrap all of it and if we
evaluated this you just get nested
hashmap sand vectors this is all just
closure data you can construct this
however you want if you look at the top
there's an emit bang function which
takes your symbolic form and produces
the equivalent byte code so this is step
three in the symbolic assembly structure
and this is the very effective closure
technique of pushing mutability to the
very edges of your system so the edge of
the system here is there's only one emit
bang function and you just call it at
the very end everything meaning up to
that is just functional programming with
functional data so you'll see that have
a some c-sharp code printed in the
bottom the MSI l byte code is actually
so rich that a lot of disassemblers can
infer the quote unquote original c-sharp
code from it so this is a c-sharp code
that no human ever wrote but this
assembler just kind of guessed and
sometimes it's a little bit more terse
to look at the c-sharp disassembly then
the raw byte code disassembly and I'll
use them somewhat interchangeably in the
next couple of slides
so a few more examples of the kind of
intricacy that something like mage
allows for you have there are no
conditional statements or loops at the
level of the bytecode they're only
branches the labels so labels are
represented symbolically as well the il
label function will generate a hash map
with a unique symbol we see that the the
gen seemed label symbol is different for
each one of these invocations and then
what you do to generate jumps excuse me
is you use closure let bindings to
generate a label in one place and then
pepper that value throughout your byte
code so here this is a method that given
a number if the number is greater than
or equal to the current year it'll
return the string the future otherwise a
little turns during the past so loads
the first argument it loads the it loads
the constant 2017 it compares the two to
see if the first is less than the second
pushing the number zero or one onto the
stack and then we branch if true to past
label and these two will have equivalent
values so we're leaning on closures
equality semantics here to be able to
structure our bytecode and if we don't
branch we return the future otherwise we
return the past
an example of something that is taken
care of for you that's very nice notice
that when we look at the byte code the
BR true instruction isn't branching to
some nice symbolic or abstract
representation it's jumping to an
address in memory effectively it's it's
jumping to an offset and instruction
offset having to manage that manually is
the hippo starts to show up again right
this is what you don't want to do so
being able to just to describe the shape
of your byte code abstractly is very
very powerful basically yeah
the the omission the c-sharp submission
machinery will do that mage doesn't
actually do that so I'm very grateful to
the men and women at Microsoft so state
is similar if you want to generate a
type that has a field fields can be
represented symbolically and you see
that there's you know additional
metadata for the kind of attributes that
this field should have and again using
closure like bindings you can using
closure let bindings you can sort of
generate your field in one place and
pepper it throughout your code so this
is the type that has a mutable field or
the getter and a setter so this is an
idea that I got from conversations with
Co boss to not dabble in binding forms
because they're extremely complex but
instead leaning on closure and closures
power to use its own binding forms to
construct the byte code that we want
local variables also work in the same
way instead of il feel that's a local so
this is just an example that you
wouldn't use in a compiler this is just
a raw code generation example just for
completeness a lot of repetitive code
can be just generated using meta
programming this is this is byte code
that'll generate a type that will have
an addition subtraction multiplication
and division method for every pair of
new
marek types on the CLR so you see that
we construct a vector of all of the
numeric types on the CLR we use a list
comprehension to generate every pair of
numeric types and then we generate we
invoke this binary method on every
single one of them you see that binary
method is a function that takes a var
and returns a function that takes two
arguments that will be the first and
second type and returns a little the
requisite bytecode to generate that that
method you see here op will be replaced
with AD sub Mull and div and I couldn't
fit it on a slide but it generates so
there are files in the closure runtime
that have these massive manually
maintained brute force files of for
numeric sand and other things and if
nothing else this is less lines of code
and this is a little bit easier to
maintain so that is a very sort of quick
intro to mage and and mage is sort of
sort of tames the the common language
run times intermediate language but it
doesn't really do more than than that
magic which is the next library that I
want to I want to show you all magic is
the what I've been calling the Morgan
and grand iron closure compiler and it's
an example of what the kind of symbolic
assembly that mage does enables so this
is an application of that approach you
could also be think of this as step two
in the the original slide with the three
steps of symbolic assembly this is the
dusim functional programming part Morgan
and grand
again the library is named after the the
cross streets of my original studio
Iren is this traditional name this
traditional prefix that's used for
dynamic languages that run on
net there's an iron Python and iron
scheme and ironruby this is my iron
closure the name is also an excuse for
me to just use this gif and every every
talk that I give I'm working up to it so
this is the architecture of the compiler
its goal is to compile all of closure
eventually not quite there yet but it's
it's getting there and and like any Lisp
compiler it takes your sort of
s-expressions it passes them through an
analyzer magic uses Nikola Meadows tools
analyzer which is an excellent closure
and Alice library written in closure I
wrote some additional CLR specific
passes for CLR types and and
host-specific semantics but the analyzer
produces what's called an abstract
syntax tree and we'll see what that
looks like on the next slide the
abstract syntax tree describes the sort
of semantics of the source code magic
takes that tree and magics job is to
take the syntax tree and produce
symbolic byte code that mage can then
pick up and turn to runnable code so
this is the sort of thousand ten
thousand feet view of the compiler so
this is what the analyzer it looks like
is the font sort of big enough small
HTML so the analyzer given a you know a
function that takes one argument and
returns that argument will analyze that
to a hashmap
and every form that it analyzes will
have a hash map with different contents
but every hash map will have this OP
keyword fair one sees that on the third
line from the top that sort of describes
the hash map this tells us that oh we're
looking at the analysis for an FN form
different forms will have will have
different off keywords and then the
function that turns
an abstract syntax tree value into byte
code is symbolized which is on the
bottom which may not be a great name
I keep sort of agonizing over this maybe
you know manifest is a better name or or
realize I don't know
it's called symbolized for now what
conjure ah right see stay on brand yeah
so so what the what symbolize does is it
takes at the this abstract syntax tree
value which is this hash map describing
you know a form and it takes a hash map
of symbolises which we'll we'll see on
the next slide
and it's job is to return a symbolic
byte code and you see that vlookup is
it's not a multi method but the OP
keyword is looked up in the hash map of
symbolizes and the ast arrow function
and we'll talk about why it's not a
multi method a few slides from now this
is a conscious design decision and this
is what the the hash map of symbolises
looks like it's an app of op keywords to
symbolizing functions and this is the
this is the hash map that's built into
the compiler so the thing to keep in
mind is so symbolize er takes an ast
node and a hash map and gives you
symbolic byte code and hash map contains
op keywords maps to symbolizing
functions it's the basic basic
architecture of the compiler and as we
look at some examples it may be maybe
more apparent why why I've constructed
that way so the simplest symbolize er is
the constant symbolize ER so a Const
node is a is any literal value in your
source so an integer a string these will
be analyzed to hashmaps whose op it is
Const and if you start reading at the
bottom you see that the count symbolize
er just extracts the Val
key from your ast so this will be the
actual value of the literal and it
passes it to load constant load constant
is a multi method that switches on the
type
of the value and for a string it'll
return the symbolic bytecode LD stir
which the CLR has a intrinsic load
string opcode for integers it will
return o LD CI for load constant integer
for bytes and it need not be single
value right this is a function so for a
boolean if K is true you wrote you load
the value 1 otherwise you load the value
0 so this is an example of building
compilers functionally and the
organizational power that you get you
can use closures multi methods namespace
semantics all the stuff that you would
love about closure to organize your code
you can start breaking down the
generation of your byte code that way
this is a little bit more involved this
is the do symbolize err so the semantics
of do in closure are that the body every
element or sorry every expression in the
body gets evaluated and it's a return
value is discarded except for the last
one right the last the return value of
the last expression and adieu is the
value of the whole expression so what do
does is it interleaves using closure
sequence function the it symbolizes each
statement and then cleans up after it
using interleave except for the last
statement which is ret the analyzer
actually separates them out for us so
this is another example of using
closures sequence tools to construct
bytecode however we want but also an
example of the symbolize err map being
passed through so you see that the do
symbolize err it gets the hashmap passed
in as a second argument and that hashmap
gets passed in to its body
so we're threading this map through the
whole compilation and an example of why
i want to do that instead of multi
methods follows but I need everyone to
take a deep breath because it's a
all right so this is the symbolize ER
for let bindings and this looks like a
lot but I will say that this is the
entirety of the logic for arbitrarily
nested let bindings in 27 lines of
closure did you that fit on one slide I
had to zoom out a little but it fits on
one slide in other compilers this logic
is spread out across a bunch of places
so there's a terseness here that's
actually very very very exciting and I
do a deeper analysis of this in in my
other talks but I just want to briefly
point out why the sort of threading of
the symbolising map is interesting and
valuable the the challenge in a let
binding is that you need to create and
assign local variables at the beginning
of the let binding and then at any point
in the body those locals may be
referenced and you need to connect the
byte code locals that you created in the
beginning to their reference of the very
very heart of the body and making that
connection is an interesting challenge
and some compilers have used global
variables and you know there's all kinds
of stuff what magic does is every let
binding sets up a binding map that's at
point one in the in the code and the
binding map here is just a map of unique
names of locals to their actual symbolic
byte code representation and then the
let's symbolize ER creates its own
symbolize ER map that's at point to the
specialized symbolize ER is local it
overwrites the local symbolize ER with
this function it's an anonymous function
that says okay if I encounter a local
somewhere down in my body and if that
locals name appears in the binding map
then emit a load local for that value
that symbolic bytecode value otherwise
we sort of recurse up we just
symbolized with whatever symbolizes we
got so you have this chain of closures
maintaining your local binding state and
you could look at it in one place and I
think when I got this to work I sort of
like it started to feel that this was
not a complete waste of time because
this was really really exciting and this
works that it nests you know arbitrarily
deep but this is also why I don't use
multi methods because I want to be able
to intervene on the compilation logic as
its flowing through the program another
another place where that shows up is the
way optimizations work optimizations
aren't hard-coded flags in magic they're
just functions that replace the
symbolization logic of certain ops so in
order to do intrinsic s-- magic can
actually replace certain static method
invocations with low-level direct
bytecode and what that looks like is we
replace the symbolization of static
methods with logic that looks up the
method in the intrinsic map which we'll
see on the next slide and if there's
some intrinsic bytecode for that method
we symbolize the arguments convert the
arguments and spit out the intrinsic
bytecode otherwise we just use the old
simbolize ER and then this is what the
intrinsic map looks like if you see a
float cast to it n64 replace it with a
call and for for if you see an array
length an RT a link to an array replace
it with a load length and convert it to
an int so this becomes very sort of
declarative and very a very
straightforward way of intervening on on
the way the bytecode is being generated
again none of this is possible that what
makes any of this possible is the fact
that we can talk about this little
snippet of bytecode abstractly not
attached to any method not attached to
any type just sort of hold on to it and
do with it whatever we want
on the suggestion of Michael
later on building Arcadia Tim's magic is
designed to be usable before it's
finished so and I want to show you some
some examples of that and specifically
what it's being used for and I've been
thinking a bit in terms of using closure
in the inner loop and that basically
means being able to use closure in
places where performance is extremely
critical and any kind of sloppiness is
really not not allowed so magic can
actually exist side by side with the
stock closure CLR compiler so you could
write a normal closure file which is the
namespace here and use a magic define to
create a magic function that lives
alongside the rest of your closure code
and it's bi-directional magic can call
code from the stock compiler and the
stock compiler can call code from magic
but magic will generate the absolute
fastest code it possibly can with all of
its optimizations for the function that
you've passed in so as long as you stick
to the subset of magic that's currently
supported you can start to build out
your functions like this this is an
example of some code that Tim's is
working on so as far as I'm concerned
this is code in the wild that magic is
able to compile another fun macro that's
a little cheeky
it's called faster faster let's you wrap
arbitrary expressions not even whole
functions and it will replace them with
a static invocation to a method that
implements that sort of has compiled all
that code via magic so if there's some
code that's not running super fast you
can make it faster and this allows you
to use features of closure that magic
doesn't support quite yet like anonymous
functions you see that in the filter
expression and still take advantage of
some of magics power an example of sort
of inner loop closure is I mean in many
cases we'll save something like 52 bites
of garbage per function which doesn't
sound like a whole lot but when you're
building a video game you're gonna want
to use that function potentially on many
hundreds of objects in this case we
calculated
were generating 26 kilobytes of garbage
per frame using that original function
to animate these cubes and that's just
one function and and the garbage comes
from the fact that the compiler will
move things to the heap when it really
doesn't need to and magic is a little
better at avoiding that so magic will
compile functions that don't allocate
any garbage if they don't need to in the
best case which makes this kind of
closure usable in in really tight areas
so I I want to wrap up with some future
work and and sort of to the point of
this being a general approach most of my
work is on the common language runtime
and c-sharp but this next thing that I'm
working on is which is working as of
Monday is what I've been calling the
symbolic assembler or Simba so it is
very young and very very new but the
basic idea is to do a mage for LLVM so
symbolic representation of LLVM bit code
that you can do to Ella Veeam what mage
and magic do to c-sharp MSI oh and this
is this is a screenshot of this is some
snippets of it working
much like mage there's you have a LLVM
has a module modules and inside of
modules there are functions with names
and signatures LLVM is not a stack
machine it's a register machine so
things are more tree shaped but this is
a function that will compare the first
and second parameters for equality and
return there are some if they're equal
otherwise return the second parameter
that evaluates to a nested hashmap much
like mage fed to an omission function it
compiles to LLVM and LLVM is very
exciting because of its many backends it
can compile to machine code it can be
compiled to GPU code it
run basically everywhere it's
optimizations are also kind of
unbelievably good
so this is the the unoptimized
compilation and I'm going to jump to the
optimized one watch this one line
so in unoptimized one in this branch we
add the first parameter to the second
one if they're equal LLVM we'll make the
observation that
oh you're adding two things that are
always equal you may as well just bit
shift at one to the left that's faster
and that my eyes kind of went wide when
I saw that and I realized the depths of
this thing is a capable to optimize for
so this is very very fresh but this is I
feel like this approach can be applied
to LLVM too and there's a lot of
exciting things that can be done there
and built on that all this stuff is open
source and online if you want to check
it out and I'll be around if you all
have any questions thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>