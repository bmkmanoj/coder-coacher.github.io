<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Invited Talk - Guy Steele | Coder Coacher - Coaching Coders</title><meta content="Invited Talk - Guy Steele - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Invited Talk - Guy Steele</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dCuZkaaou0Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">guys steal one of the problems with
introducing someone like guy is that
he's just accomplished so much and you
have to read through the long slog of
head of this chair of that edited this
did that and it takes forever and you
know somebody started this conference
yesterday by going long and I don't want
to be that guy so so if guy forgives me
I'm not gonna read through his his bio
you can read that on the web but you
know there's a there's a saying about
standing on the shoulders and he's a
tall guy but when I stood on his
shoulders it was amazing how thin the
air was and it's not exactly the kind of
thing where I needed to choose to stand
on the shoulders because as I did the
work research enclosure everywhere I
turned when I was looking to learn
something new or understand something
better or you know figure out how
something worked there was guy's name on
the top of the paper or on the book and
so he was a tremendous influence and his
work was a tremendous influence on
closure and in particular I would just
like to highlight how his work shows the
importance and really how essential a
precise and clear human language is to
both capturing and conveying the
semantics of what we do and I think we
can't lose touch of that in addition
he's shown us all how to do that really
well I don't really know what to say
except that I owe guy a tremendous debt
of gratitude and if you're sitting in
this room you do too so please join me
in both thanking and welcoming guy
Steele
well thank you very much rich I really
appreciate that I thought you're gonna
keep it short but I'm glad you did I
also stand on the shoulders of many
giants and I want to acknowledge that
and I've also been quite fortunate to be
in the right place at the right time on
several interesting projects and I'm
glad the results of those have proved to
be useful to others and someday some of
you perhaps all of you will be those
giants know that people stand on your
shoulders so keep that in mind okay I'm
here to tell you about my new obsession
it's a it's a programming language it is
in fact the most popular programming
language in computer science this is a
carefully crafted claim it has no
compiler interpreter or specification
and yet a lot of time has been spent
writing programs this language and hand
translating both to and from this
language between it and other languages
that do have compilers and interpreters
and specifications and it's developed
over quite a number of years some of the
contributors of this language are
actually quite well known here are some
of them Gerhart Jensen John Backus peter
naur and Alonzo Church they didn't know
they were designing this language and
there have been many other contributors
over the last 20 or 30 years who most of
whom were nameless and some of them
justly so I would tell you the name of
this language but it doesn't seem to
have one so I'm going to call it
computer science meta notation this is
the language that programming language
theorists used to talk to each other
about programming languages it's got
built-in data types that are assumed
boolean integer real complex sets lists
and arrays it's primitive expressions
are those of logic and mathematics
it's got user declared data types which
you can think of as abstract data types
or records or symbolic expressions we
call this BN f BN F is used to express
these the code is written in the form of
logic inference rules in the so called
Genson notation it's that horizontal
line look at examples that later and by
the way it's that fact of notation that
makes it very hard to search for in
literature the OCR doesn't capture that
horizontal line
it has conditioned several forms of
conditionals but the primary one is
dispatched inference rules through
non-deterministic pattern matching we'll
see an example of that and reason has
developed some interesting abbreviations
for repetition the uses of over lines or
ellipsis notations and sometimes
explicit iterators and it's got this
very weird operator of this unique this
language which is the so called capture
free substitution operation within a
symbolic expression substitute this
expression for that variable in this
other expression and that is attributed
to Alonzo Church so this is actually a
very beautiful language and
unfortunately I hate to report that it's
getting messed up discussants developed
some problems I've got some ideas about
how to fix that so I'm going to talk
about a little bit about it to you and
might even tell you a little bit what
that has Bidoof closure so here's a
quick example of a data declaration in
computer science meta notation this is
from a 2014 paper the principles of
programming languages conference we're
not going to dwell on this too long
because I'm going to want to discuss it
in detail later on but you can just see
that for example an expression can be
either a variable or an express
abstraction or an application and the
forms of each of these are described and
this is a very terse notation rather I'd
like to show you a bit of a code first
here's an example of code this is taken
from in fact the same 2014 Popple paper
and here we have two rules and we also
have a signature declaration which was
very considered the author because you
don't often see those so this declares a
rule called no conflict and it takes
four parameters there's also a comment
which is also very unusual it says okay
this is intended to check for applachian
conflicts whatever that means and there
are two different ways you can check to
make sure there's no conflict you can
either use the rule called MC a part or
the one called MC compatible and in
order to prove the thing that is under
the line all you have to do is prove the
things that are above the line by other
means and either rule will do this is a
non-deterministic language if you can
use either rule to prove there's no
conflict that's fine you can even try
both of them in parallel and see which
one finishes first doesn't matter so
there's an opportunity for parallelism
here that's interesting I also want to
draw your attention to the fact the uses
of over lines in various parts these
rules I've highlighted these with the
the light blue and we'll dive deeper
news notation just note that the third
argument no conflict is actually not the
the single symbol taw but a list of
things designated taw and for example
the function f takes a bunch of
parameters which are a bunch of rows and
in one place the in couple places the
over line notation is nested mmm okay
here's a different example of code this
is from a type checking algorithm from
last year's puffle conference that was
about a year and a half ago and if you
want to prove the type of an expression
using one of the rules below I have to
do is prove the things above and I'm not
going to go into too much detail here
except first I want to point out that
the authors intended we should read it
is actually having inputs and outputs so
this structure consisting of a turnstile
symbol and a colon says in the context
gamma if you have this input expression
I can figure out the type for you
highlighted in blue as an output and and
each of the rules has it has a structure
and furthermore this this set of rules
actually is deterministic because he for
regard the thing before the colon is an
input each rule has a different pattern
there so in fact only one of the rules
is going to match it's possible that no
rule will match the expression in which
case the type checking fails but at most
one rule will apply and that's the one
he the author wants you to use and so
for example if we look at the rule at
the upper right it says that if we have
an application of a function M to an
expression n that we'll have typed ah
providing we can prove that M is a
function from Sigma to tah and the
argument type is Sigma and so that's
what's that saying and this is the
language of type theorist used to talk
to each other the other thing I want to
draw your attention to is in the middle
rule there are some ellipses there
ellipses is the fancy word for the three
points dot dot dot and it's being used
in two different ways here in the in the
consequent on the bottom it's being used
to indicate that up takes a sequence of
expressions M and that runs from M 1 to
M of arity the operator so we're to
infer that there are a whole bunch of
M's in between possibly but there might
be only one or they might even be 0 of
them on top it's being used as an
iterator saying that the premise
proceeding that's that parenthesized
thing the gamma turnstile M sub K : tah
there's actually a bunch of such
premises 1 for each value that I takes
on so that's called an iterator and that
sort of a
meta iteration construct don't you think
it's ironic that type theorists who want
to talk about strongly typed languages
in a rigorous manner talk to themselves
with an untyped language that has no
reverse specification this is really
weird
okay but it's a very concise language
and that's why they use it okay so I
find I think I've convinced you that it
is a kind of programming language I
showed you one algorithm a type checking
algorithm and in fact I seen type
checking algorithm sitter 10 or 20 pages
long and those have served as a
specification for some phase of a
compiler and people hand translate that
or maybe they look at an existing
compiler and try to back translate it
into the type of specification language
and then prove some theorems about it so
it is a programming language you can
write programs in it in fact the
language feels a little bit like Prolog
and I've as an experiment I wrote a toy
compiler it translates the latech
expressions in my Emacs buffer for these
these notations I showed you translates
them into Prolog and I was able to run
the Prolog code and do the type checking
you know so that works as a toy
experiment but there's no full-blown
compiler for the language so I've shown
you as a programming language now to
complete my claim I need to show you
that it's actually popular or at least
popular at the Popolo conference I went
back and looked at all the papers and
all the poeple conferences for last 43
years actually did this last spring's I
hadn't yet seen the 2017 Popple and
breaking it down into five year
intervals I looked at every page of
every paper this is about 17,000 pages
of stuff yeah you go flipping through
the proceedings flip flip flip and you
can see the growth in time in the use of
inference rules at the popul conference
and in the last 20 years this notation
in you is used in over half of all
papers which makes it much more popular
than C or Python or you know or Haskell
or anything you might think type theory
mutagen let alone Fortran or COBOL so
it's a very popular language in that
sense and just to double check I went
and looked at some other ACM sigplan
conferences as well so I look at three
years worth appeal the I loop so ni CFP
and six years worth of P pop because I
wasn't finding much in the last three
years of P pop and you can see it's used
in a bit by now it's used in about a
third appeal the AI paper is about a
third of us love papers almost two
thirds of ICF P papers
and hardly at all at p-pop and that's
not at all surprising because principles
and practice of parallel programming
tends to be more about what's going on
at runtime and this notation tends to be
used primarily to describe what's going
on a compile time if you make that
separation okay so now I understand
something about how this language is
being used and it is popular I'm
continuing to research this those the
previous two popularity slides were my
research as of last spring and continue
to look at more conferences so in the
dark blue blue you can see that I've
examined all the poeple conferences and
I looked at a bunch of them in the 2010s
now I've gone back to 1971 and I'm
plowing forward again I've looked at a
lot of the early kind of random signal
and conferences that were one-off things
and I begun to plow through PLD I and
asked applause and Lisp and functional
programming which then became ICSP and
I'm hoping to learn more as I continue
my research on this topic but I'm going
so I've looked at a ad cover to silver
out of 216 and I'm gonna report on what
I found so far so this is the structure
of the talk I'm going to examine the
history and variety of five different
aspects of computer science method
notation the inference rules be enough
the substitution operator over line and
ellipses and I'm going to identify
problems that have arisen with the last
three and maybe even suggest some
solutions to that so here we go okay
inference rules back in 1935 Gerhart
Jensen published a paper from on what
for what he called natural deduction or
he kept called it the appropriate thing
in German this paper was written in
German I've got a couple of few excerpts
here and I don't expect you to read the
German but down at the bottom you can
see that first rule says if you can
prove a and you can prove B you are
entitled to conclude that a and B is
true and the second third will say if
you know a and B then you're entitled to
conclude that a is true or you may
conclude that B is true and finally if
you know a you are permitted to conclude
a or B and similarly for B okay and so
he's kind of redoing logic in this
rule-based form that's kind of cool so
what does that look like today in
today's modern computer science
inference rule notation about the only
difference is that we have too many
premises were allowed to stack them
horizontally as well as horizontally and
sometimes we label the rules now I
discovered a huge variation in the
labels in placement and separation and
capitalization
whether they're alphabet ocurred became
symbols the size and style of the font
what word separators are used for their
multi word labels whether use brackets
or parentheses or yadda yadda to enclose
them so there are dozens and dozens of
different label styles
it doesn't matter people find this
notation readable the differences are
not a problem and that's the only point
I wanted to make despite the variation
when you've seen in virtual like that
you know it's an infant rule and you
know what's going on okay now let's talk
about BNF its history goes back a bit
further about 2500 years ago panini
wrote a Sanskrit grammar which is a
wonderful scholarly work containing
numerous concise technical rules
describing Sanskrit morphology
unambiguously completely and if you
squint just a little bit it looks just
like me enough you know there are these
defined non terminals and if you want to
make a noun phrase then here here your
choices and eat you know those choices
will contain further non terminals and
you can expand them and when you're done
you've got a valid Sanskrit sentence you
know that that was really quite an
achievement for 2500 years ago okay jump
forward in modern times in 1914 axle 2
studied string rewriting systems defined
by rewrite rules in 1920s amyl post
studied a variant of this thesis called
tag systems in which symbols are
repeatedly replaced by associated
strings he did this work in the 20s he
didn't publish it until 1943 and that
matters to our story because it's about
then that Church and Turing were doing
their works about undecidability and in
1947 Andrey Markov and amyl post
independently proved that the word
problem for semigroups which is a
problem the T had posed is undecidable
and immediately this is related to the
works by the work by Church the work by
Turing on the undecidability of lambda
reduction in the undecidability of
deciding whether the Turing machines
will hold so all this stuff is happening
in the 1940s in the next decade Noam
Chomsky published he has three models
for description of language and this
described grammars with production rules
and what we now call the Chomsky and
hierarchy of grammars type 0 type 1 type
2 and type 3 type 0 corresponds to
regular expressions type 1 corresponds
to context-free grammars which we can
express with BNF and then there are two
kinds of context-sensitive grammars ok
so it's back with his background we
would expect that the fervor there
evolution of BNF and regular expression
to be very
related and that the regular expressions
would be a subset of BNF and they'd flow
naturally out of the Chomsky theory
wrong they had completely independent
historical developments okay so let's
look at regular expressions in one slide
in 1951 before Chomsky published Stephen
Clanny developed regular expressions a
way of describing a McCulloch Pitts
nerve nets what we would now call neural
networks and he used the the wedge
symbol for a choice and he considered
using a postfix star to mean zero or
more copies but he ended up making it a
binary operator so that X star Y meant
any number of copies of X but then you
have to have a Y and the reason for this
he was trying to avoid having MP strings
come out so if X was not emptying Y is
not empty then the binary effort star
will produce non empty string and his
work was published five years later in
1956 still had only the binary star he
didn't even talk about having consider
the the postfix case however two years
later two three other guys Co PL got and
right formulated re he's using a postfix
star he said this is obviously more
convenient we've got theorems to prove
okay uh four years later bruised auskey
used a very similar notation we needed
two things instead of he replaced the
wedge with a plus symbol to indicate
choice and but he also used a postfix
superscripted plus to indicate what the
choice of one or more things rather than
zero more things so this lab you decide
stuck the up string problem with star
when necessary
in 1968 the AED our word system this is
a programming system provided a form of
regular expression using slash which was
on the keyboard and set Union for choice
which is not on the keyboard understand
that or maybe it was and used to put the
postfix star that same year Ken Thompson
wrote the seminal seminal paper regular
expression search algorithm because the
vertical bar was on his keyboard he used
that for choice instead in the rest is
history as I say five years after that
thompson in response to a request by
Doug McIlroy Doug macro who's trying to
use the IDI editor to process these huge
text corpus as he was trying to process
and they wouldn't fit in main memory and
Edie insisted was a kind of editor than
cyst of the Year entire your entire file
fit and main member
and so Thompson extracted just the
regular expression search part and made
that be a separate utility which he
called grep and then two years later la
home made an extended version called
egress and in this he introduced the
possibility of parentheses for grouping
and also introduced the question mark
for the case of 0-1 and I asked a ho
about that and said did you invent that
he said I don't remember for sure but it
seemed like an obvious thing and you
have a question mark was there on the
keyboard yeah that was probably the
invention of the question mark but I
can't remember that was 40 years ago
okay there's one more slight deviation
which is that at carnegie mellon
university the alfred project used
regular expressions with star for zero
more one sorry plus for one and more and
the sharp sign for zero or one
if think we're using grep an e grep
around there by then but by three years
later all the technical papers at CMU
were using question mark instead of the
sharp sign so the sharp sign appeared
then disappeared again and I think that
was the influence of the spread of unix
and grep it was about then CMU is
emerging from using the pdp-10 operating
systems and beginning to use unix
systems so that that may be influenced
there and regular expression notation in
computer science has been pretty much
unchanged since then so there's a 30
year evolution and then we finally read
on something it's stuck and we can all
use rotor expressions and read them and
know what they stand for that's great by
contrast let's look at be enough be
enough got its start when Alan Perlis
and Clough Samuelson wrote the report on
the International algebraic language
which very soon after became renamed
Algol Algol 60 and they described some
forms for various language features what
they called functions which we would not
call function calls and it says a
function call can be an identifier
followed by a number of parameters which
we would now call arguments in
parentheses separated by commas and look
at this ellipsis in there it's not dot
dot dot its wiggle-wiggle-wiggle I love
that indicating more stuff can be in
here I've never seen that that usage
again I don't know where they got that
font and then down below arithmetic
expressions E and this looks a little
more familiar if we were to place that
swung - with a colon colon equals this
would look just like BNF we weren't
quite there yet okay so the next year
John Backus
I was writing this stuff up but he was
also influenced by the work of email
post remember post working on tag tag
systems so john backus borrowed posts
notation and wrote a specific syntax for
the production rules for a context-free
grammar for the International Eldrick
language and this is what one of his
productions looked like and you can see
that it looks much more familiar to us
it's got the angle brackets around the
non terminals and it's got a choice
operator written by the word or with an
over line over it and notice that all
these strokes and angle brackets are
written in my hand you know after having
typed it in some other way so this is
kind of a clunky notation and the next
year peter naur when editing it for
publication in the communications the
ACM decided to change the equivalents
colon to eat colon colon colon
equivalents to colon colon equals
because that could be key worded easily
and so when he got rid of the over line
or and replaced it with a vertical bar
you know and curiously enough ken
thompson chose that later for grep
because that was an independent choice
here and so this is the published form
of a couple of productions in the report
on a level 60 and now are introduced one
other important innovation which he said
we're gonna make the names than I term
of the non terminals in the syntax
identical to the English phrases we're
going to use in the informal text to
describe them and that will make it
easier for readers to connect the
formalism with the pros it may seem like
a trivial thing but you lay that down as
a rule and it's stuck at about the same
time an alternative Rose which is COBOL
meta notation the COBOL report didn't
use BNF at all it uses two dimensional
notation where choices are stacked
vertically and within braces and square
brackets are used to indicate optional
items and the ellipsis today indicates
that you may repeat the preceding item
what's interesting here is that in this
report the use of braces and brackets
was carefully documented and the use of
the ellipses was completely taken for
granted and not described at all seems
team seems like a curious oversight
okay he's neutered Haitians got
synthesized in 1965 in a description of
the po1 programming language
I've been specification combined to be
enough with COBOL not met a notation in
the top box you can see some rather
badly formatted BNF they could have too
as line breaks a lot better and at the
bottom you see use of something looks
much more like the columella notation
okay that's 1965 an ellipsis indicated a
nonzero number of repetitions of the
preceding item but by combining the use
of the ellipsis in the square bracket
you could get 0 or more repetitions and
notice that they wrote square bracket
items rather than square brackets with
the dot inside this means you can
actually have you can actually repeat
your choice to have 0 or 1 they didn't
think that closely about it I guess the
grammar comes out the same but if we
were making grammar parsing tools that
would matter to the parser which he says
okay I couldn't read footnote okay at
about that time Nicklaus severe
description of PL 360 used a
parameterised format BNF where the
non-terminals can have a parameter k in
them and that K can be filled in with
any data type so that's interesting so
there's actually more rules in the
grammar than there are than there are
lines on the page so to speak so one
rule here k register stands for a set of
rules defining long real register
integer register real register and so
forth and adrien vanwyngarden in
describing l 68 took that a step further
using what's now called a Venn
Weingarten grammar or a two-level
grammar in which the real grammar for
the language has an infinite set of
productions that's ok because they're
productions of that grammar defined by
another grammar this turned out to be
really hard to use it's fascinating to
try to read the lrl 68 report it's a
complete description of this type system
and it's in effect context-dependent
because of this two-level structure in
1970 the bliss language description
which is a systems programming language
used for the pdp-10 pb11
it was described using vnf but using the
right arrow instead of colon colon
equals that's probably this Chomsky use
right arrow and a lot of people who
speak of grammars as opposed to B and F
we use the right arrow rather than colon
colon equals and they took this notation
for granted on the other hand when
Digital Equipment Corporation took this
over as a corporate language their bliss
documentation used peel one style syntax
descriptions maybe they thought those
were more user friendly and there's this
interesting variation that appeared in
the 1970s called syntax chart two real
wait ray diagrams is no
turn ative to be enough and the idea is
you start at left hand sign you follow
the arrows like a choo-choo train going
round and round the tracks and whatever
you hit along the way by the time you
get to the end of the string by the time
you get to the out arrow on the right
you have formed a valid piece of syntax
why by following the railway diagrams if
you see me really popular but for about
a decade and then died out again okay
perhaps the best-known language that
used it was the red language but red got
beat out and green became ADA and blue
and yellow were never heard from again
either if red had become aid instead of
green we might be using syntax charts
today who knows accidents of history
okay
by 1977 Niklaus veer noticed there were
a bunch of variations in B NF + C syntax
charts and he wrote this lament and
published in C ACM said what can we do
about the unnecessary diversity of
notation for syntactic definitions you
know a lament only veered could offend
and he saw the problem of having too
many B infn variants by proposing yet
another and this paper is notable for
two things one is that as variant
actually caught on and became an ISO
standard and we now call it extended BNF
and the other is this is the first
occurrence I can find of BNF being used
to describe its own grammar and this is
possible because of the way he uses
double quotes as escaping symbols which
had not previously been a feature of BNs
he also introduced the ideas known as I
can tell of having a specific symbol in
this case the period to mark the end of
the production as a separator between
productions okay there are lots of other
variants and I'm not going to tax you
too much with them I will just point out
that a Stanford sale language the Alford
project the ADA specification used BNF
but it used boldface words rather than :
clone equals and the vertical bar used
bold faces in boldface or they're a
bunch of variants that synthesized
regular expressions in B and F and since
since a Sam Harvison and I were at CMU
at the time that was happening at CMU we
ended up using a version of BNF that
included regular expressions in our
books about C uncommon Lisp
so that's where that came from the
Python reference manual uses both the
postfix star and the postfix plus you
it's BNF but it uses brackets rather
than question mark for the optional
items why why not the Haskell 98 report
uses a variant Ruby uses an interesting
variant in what it calls pseudo BNF but
it also uses brackets arriving question
mark frost optional items and then
there's C style BNF which also got
adopted by Java and C sharp and F sharp
in which instead of colon colon equals
you just use a colon we don't bother
with the vertical bars we just make sure
that every alternate is on a separate
line unless we use the magic words one
of in which case we can string them all
on the same line which provides some
conciseness and when you're printing
books and so this is interesting
variation also and you're probably
familiar with this so we've seen a huge
variety of be enough variations in the
last six decades and I think the
consensus is by and large this also has
not been a problem and I think the
reason for that is that although the
punctuation changes the basic structure
of it is not you always see a
non-terminal and then some kind of
marker and then a bunch of choices and
they're separated by some kind of marker
and for each choice there are some
things concatenated you can further
expand them on terminals and yeah you
got to kind of infer what means there
were more and what means one or more and
if you're lucky the offer told you that
okay and so so we kind of get along and
it's fine hasn't been a huge problem
just two despite the variation okay now
I promised that I would show you this
this version of BNF from a computer
science paper and on the one hand it's
identify of a B and F it's got that
colon colon equals has got choices
separated by vertical bars and yet it
looks very different in a number of ways
first of all all the non terminals are
single letters what happened to Peter
now is dictum that the non terminal
should be an expressive phrase to tell
you what it means well instead they've
been replaced by single serve symbols
but they come they compensate for that
by putting comments over the right-hand
side saying well the choices are
variables and abstractions and
applications East stands for an
expression toss stands for a type so
this have seemed to be counter intuitive
why not just use the word expression and
type why make you remember that East
answer expression and toss stands for
type I've got a theory about that page
limits in computer science conferences
your entire paper has to fit in ten
pages you are going to compress that
like crazy until it fits and I think
that's what's happened here in computer
science meditation I'm not going to
dwell on it too much further except to
point out that here in the BNF this
particular that would be an F we're also
seeing use of over line notation to
indicate repetition rather than either
recursion or clean ease original star
operator ok so what's with these over
lines we'll return to that
but the point is is that BNF has using
precise meditation is the same as old
bns and yet very different in a couple
of ways that lend conciseness and that i
believe that conciseness is considered
to be one of the prime virtues of this
version of the notation now when talking
about some of the properties of
substitution of a B and F BNF has two
conventions that aren't often talked
about one is something I call the
consistent substitution convention now
we're used to saying that with BNF you
can replace the non-terminal left-hand
side with the thing on the right hand
side and if they're non terminals there
you can further replace them and on the
right-hand side the same non-terminal
might appear more than once and that's
okay
and you can expand each one differently
if you want to but we don't do that when
we're use it right using these same non
terminals and pros if we took use the
BNF convention that every non-terminal
can be replaced by any string derived
from that non terminal then we could
take an English sentence such as a value
of type tah may be assigned to any
variable of type tah and okay for the
first tall such an integer and for the
Sacramento substitute cool-cool value of
type int may be assigned to any variable
of type bool that's not what we want to
say so when we're using when when we're
merging BNF with English or other
natural languages we use this consistent
substitution dimensions because we want
to be able to talk about things and
mention them more than once so if I
mention the same non-terminal more than
once I insisted to be expanded the same
way at least within the same sentence if
not within the same paragraph so that
forces me to say a value of type int may
be assigned to any variable of type int
that makes sense so that's this
consistent substitution convention the
other property this language is
something I will call the decorated non
terminals commissioned by decoration I'm
doing things like subscripts or Prime's
or little hats
little decorations you add onto the
usually single letter uh non-terminal if
we took the definition should be enough
not literally then when we write a
sentence such as if tall one equals tah
to than tall one is a subtype of tattoo
then if I expanded it by replacing each
tall with int then I would get if inch
sub 1 equals inch sub 2 then in sub 1 is
it to sub type of n sub 2 which is
something I might want to say but it
usually isn't there aren't any types in
sub 1 in it some 2 rather I really mean
that the tall ones are type in the
AUTOSAR type and they have nothing to
each other and those can be expanded
independently but the two tall ones have
better stand for the same thing so what
I really will say is I the sentence up
top stands for things like if antique
pool school is then into the sub type of
wall which is a true sentence because in
fact the int is not equal to wall and I
can also say that if integrals Anthon
into sub type of image a true sentence
in fact int is int and india's a sub
type of int and everything's fine okay
so we need to understand this language
need to understand those two conventions
okay that's enough about be enough let's
talk about substitution completely
different set of issues here in 1932
Alonzo Church used this notation a
capital S and then a superscript
expression a subscript expression and
then an expression you off to the side
and then a vertical bar just close it
off that was his notation and that
represents substitution replacing the
variable X by the formula Y throughout
the South the formula you now in 1941
when he published the calculate lambda
conversion the seminal work on the
lambda calculus he had refined this to
say you may you can replace use this
replace operation
provided that the bound variables in M
are distinct both from the variable X or
substituting for and from the free
variables n so now instead of just
becoming a pure spring substitution this
has become something much more subtle
and technical the so-called capture free
substitution that avoids accidentally
capturing bound variables are being
captured by a bound variable nowadays in
computer science meditation we write
something like this for instead we write
an expression e and then in brackets we
write a value expression than a slash
and then a variable X and this means a
with V substituted for X throughout okay
so is this used in all computer science
papers not by a long shot
they're a bunch of variations as of BNF
how many variations would use
there have been in just the poeple
conferences in the last 43 years any
guesses 12 okay another yes okay in fact
I've cataloged 32 varieties and here
they are and they fall into some
categories whether whether the brackets
come before or after the expression
being substituted what kinds of brackets
he used whether used a slash or a
backslash or an arrow Oracle and equals
they're different authors who use
different ones one is far and away the
favorite that's when I showed you on the
previous slide that you see that in the
middle of line three
the second favorite is exactly the same
with E on the right instead of on the
left and that's used in about 1/3 of all
papers or or there's a 2:1 ratio I
should say and then that the other three
other most popular notations are
highlighted the one involving the colon
equals is probably popular because HP
Barendrecht
used it in his monograph on the lambda
calculus which is a well respected work
and I don't see it appearing in the
public conference until after
Barendrecht published now I
double-checked and looked to some other
conferences and so when I looked at five
years worth of poeple and three years
worth of other conferences I was able to
fill in the holes of the table I now see
33 varieties and of those sorry 34
varieties and of those 34 varieties 19
are still in live use within the last
five years so it's not just that
variations have appeared then been
superseded by others I've got other
charts that show that the number in live
use is been continuing to grow steadily
over the years and if you look at just
last five year interval 19 of them are
still in live use and this is scary for
a couple reasons one here's a moderate
problem by far the most popular form is
e with v /x but once every five years
you see e with x / v the author gets it
right gets it backwards for no apparent
reason and doesn't explain why i'm
probably just forgot it's that the
convention is the other order and you
can count on the variable names to keep
tip you off because different authors
actually use different variable names in
practice so all you see is the brackets
and the slash and you're not sure which
way the substitution is going so that's
a bit of a problem
one paper published last year actually
use both forms in the same paper it was
probably a typo but
but it happened and I thought I caught
it here's a bigger problem which is that
these forms with the maps to arrow and
the colon equals and other variants that
use prefix forms of races these forms
are frequently used for substitution in
about one sixth of all poeple papers
with a are also widely used to express a
completely different different operation
called function update or map update or
storage update where F but change so
that X maps to V when applied to Z means
that if Z is X then returned V and
otherwise return F of Z in other words
you're taking one argument of F and
changing its output and that's a very
different operation from substituting
throughout an expression but these
notations are used for both purposes in
the literature and sometimes both
notations are used in a single paper
worse yet I've even seen the bracket
slash notation normally used for
substitution also used for map update in
the same paper you know so and and what
makes it worse is that because the
continuing pressure on on to keep your
pay paper within ten pages or 15 or 20
or whatever the limit is now as the
program's you write get bigger
something's got to give and what's going
is the explanations in the notation
until authors are taking education for
granted and not explaining which
variation the rotation they're using and
this was what I think is producing a
minor crisis in in computer science
right now papers are hard to read
professors and students are noticing
this and I think we need to do something
about it
so I see three ways out wanted to take
my recommendations I'll to kind of
pretend me Niklaus beard here and say do
it my way and I've got a specific set of
a recommendation that makes sense I
won't bore you with them you can you can
look at a screen over the slides and
while I talk some possibilities do it my
way
another is to insist that every author
come up with a different variant when
every writes new paper because I knew
what we force to explain it the third
possibility would be some interesting
technology for reading the paper so that
when I read a paper I can substitute my
favorite substitution notation for the
one that loser used
so that's it that's a technical option
also but harder to implement but we've
got a problem with here is a case where
a variety of notation actually has
become a problem because we aren't just
seeing the same elements in the same
order in the notation the same elements
are appearing in different permutations
and with different punctuation and you
can't just look at it and sort it out
okay now let's talk about the over lines
good
okay history of over line notation back
in 1484 xu k used an underlined group
mathematical symbols together in 1525
rudolph used dots so when he wrote that
the surd sign for square root and then a
dot that meant there's a separation
between the screwed operation and
everything that follows and therefore
the 12th the plus 12 gets group of the
square root of 140 and then you take the
square root that's what the dot means
later that century tour Thalia used
parentheses for mathematical grouping
oh wow an innovation nearly a century
later William ah tread used two dots to
indicate to grouping why did he use two
dots well he wanted to use one dot for
something else in his notation sue said
okay well use two dots for the grouping
six years later Rene Descartes attached
an over line as a grouping symbol to the
third symbol producing what is now the
modern square root symbol and in this
form the over line notation survives to
this day and is widely used as a
grouping function not just as an
indication of square root everything
under the over line gets Square rooted
in 1640 yon stamp une used all three
together he wrote he added up three
things and then he put parentheses
around it and then an over line and then
a dot just to be sure he was taking no
chances of being misinterpreted worked a
little later van shoten editing the
works of the mathematician Viet aa used
over line exclusively for grouping in
editing the works that mathematician but
in the 1700s live knits began what who
during it the part of his career the
1600s had been using over lines decided
to switch to parentheses and the reason
was technologically said the parentheses
are so much easier for my friends the
typesetters than setting the over lines
because involve only only horizontal
things need to be squeezed in and then
mechanically clamped together it rises
over lines in you to clamp thing
vertically this is much harder process
and so he said he said yellow C's
parentheses and in fact a noted
mathematical Germans Journal said yeah
we're gonna switch to the light knit C
in the notation and we're gonna use
parentheses going forward and starting
for 1728 Euler and the Bernoulli
Brothers popularized these two
parentheses and I think it's due to them
that we are using parentheses today
primarily on the other hand in the
1800's piano the guy who did you know
piano or taken all that reintroduce the
dots and in fact he uses different
numbers of dots in different places and
you can think of the number of dots as
being the binding weakness there's no
dot between B and C so they bind
tightest tightest and then where there's
one dot okay then that binds to D and
then two dots you get the bind a so more
dots means things are pushed further
apart and those might have died out
completely accepted in 1910 Russell and
Whitehead when they wrote the famous
principia mathematica that great work
decided to use the dots and they kept
the dots alive when they could have died
out so we've had three different
notations for grouping duking it out for
five centuries the parentheses the over
line and the dots and I'm still not
quite sure who's gonna win although he's
parentheses seem to be popular right now
but as we will see now let's talk a
little about vectors vectors play a part
in this story you may have heard learned
about argon diagrams in high school for
graphing complex numbers and he speaks
of the square root of -1 as expressing a
rotation in the in the plane
geometrically and he proposed that
vectors be notated by writing the symbol
for the start point dissemble the end
point and a little arrow over it is that
over line arrow I'm interested in 20
years later Hamilton recast the theory
of complex numbers as a pure algebra and
a pair of reals he said let's not worry
about whether this whether the square
root of 1 is a legitimate thing to talk
about or not let's just talk about pairs
of reals and there's this interesting
Eldar and we can do things with it he
said okay we've got this great algebra
prove that it has all these wonderful
properties you know ulta location
distribute sorry yeah multiplication
distributes over addition and all these
wonderful things now let's do the same
thing for triplets and let's do the same
for quadruplets and all these things
surely those must have their souls to
look for them and he spent a fruitless
ten years trying to find the algebra on
triplets and kept failing and then
suddenly one day at a flashing insight
and discovered the quaternions
which are like complex numbers on
steroids instead of a plus B is a plus
bi plus C J Plus D K so you actually got
three square roots of minus one which
serve as serve as a basis vector for
three-dimensional space
you got this fourth thing this a which
is a scalar quantity of the tags along
and then once you discover there
quaternions he reformulated them without
the I to K coordinates and described the
quatrain Ian was more abstract thing as
simply the sum of a scalar and a vector
where the vector was a three-dimensional
part and James Maxwell in formulating
Maxwell's equations and writing other
things about physics used quaternions to
describe his theory but he said it must
be admitted that this notation is a
little bit clumsy for my purposes in
1881 Gibbs established the now-familiar
dot and X for what we now call the dot
product and cross product although those
are not true products they don't
actually have the properties of a
product but we call them products anyway
because they're sort of like products
they don't have all the algebraic
properties just some and it's the fact
that they only had some of the Eldrick
properties of a product but not all that
caused a mathematical backlash and in
1882 Heaviside the physicist said let's
just ditch the scalars and just use
vectors for describing physics that's
just the part we want and mathematicians
said no you can't do that the vectors
are Mathematica terrible they're ugly
and an elegant and the quaternions have
all these wonderful beautiful algebraic
properties we can prove theorems the
physics said we just want to all we want
is that when we take the dot product or
with itself and then take the square
root to get it to get the length of the
vector we don't want the square root to
be of a negative number because the
quaternions have this weird property
when you take the dot product it comes
out negative
when using platter nians the signs are
flipping all over the place and that
drove the physicists nuts they want to
take the square root so don't have to
worry about it so in the 1890s there's
this big fight between the colonists and
the vector ist's and the vector ist's 1
and modern formulations of physics use
vectors and a quaternion
quarter nians are still widely used in
computer graphics codes because they're
great for expression rotations of the
camera but vectors won out and the
reason this is relevant in computer
science is because then people started
thinking of vectors as tuplets of
integers and yeah they've got these
evening aldrick properties but there are
tuplets of integers there are arrays of
integers and by the time we get to the
19th
jeez we're taking things like interrupt
vectors and dope vectors in computer
science and that have nothing at all to
do physics and when I talk about my
physics friends about computer science
they cringe when I mentioned say oh I've
got this vector of characters what how
do you take the dot product of that you
know no I just mean wonderful ray why
didn't you say so what we call them
vectors you can't do that yeah
so this so now there's another fight you
can't win okay so how this feeds into
the Popple conference in the 1970s both
a little arrow over a symbol or just a
plane over line just dropping a little
Arrowhead because who needs it are used
to represent a vector or a list or a
sequence or a set but is it's defined as
being closed so a arrow means a bunch of
A's you know from a sub one through a
sub N and we'll separated by commas will
slightly MSI angle brackets around us
you know it's contained by in 1981 this
over line notation is beginning to be
taken for granted
in computer science papers at pople by
1989 there's another innovation which is
that an arrow over civil explicitly
indicates an unenclosed sequence because
then you I can write two of them next to
each other with just a common you get
concatenation for free you can use the
comment indicate condition rather
special concatenation operator okay so
so far the semantic model underlying
this use of an over line or little arrow
in computer science papers the orc
semantic model seems to be that marks of
variable is representing a sequence and
a syntactic model is you can make copies
of the over line name and attach
sequential subscripts so that a with an
arrow it means a sub 1 comma a sub 2
comma a sub 3 hover any as you like you
can expand at any number of A's you
please okay in 1990 we see the first
explicit claim that in fact the elements
aren't just variables they can be
metasyntactic variables may be non
terminals of a BNF and so even when we
write alpha sub 1 we need a bunch of
alphas and those alphas in turn can be
further expanded into something else in
1990 we also need to first use what I
call an implicit unit of replication an
author wrote M with an over line and
then 1 colon and then Sigma that an over
line and he says this doesn't mean a
bunch of M's and then a colon a bunch of
Sigma's this means M 1
and Sigma 1 comma comma n sub K : Sigma
K so even though the colon doesn't have
a line over it it's getting copied also
that's a little weird how do we know
what is the amount being copied there's
an inference being made there he didn't
explain how to do that in 1993 we first
see the claimant of paper the over line
may be applied to any syntactic object
so this offer instead wrote M : Sigma
with an overlying over the whole thing
and said that's the amount I want copied
this is fascinating we started with the
vector notation and by now in computer
science
it has reacquired the original grouping
function of an over line of mathematics
so it's funny how these things come
around but I've got a problem we know
how much just recopy but how do we know
where the subscripts are to be to be
attached why is the result M 1 : Sigma 1
rather than M : Sigma and then a single
subscript attached to the whole thing or
if you're saying well you well you can't
attach to signa have to attach to all
the symbols and why doesn't the : get us
get a subscript you know you can invent
plausible rules for how this ought to be
done but no one's written those rules
down and that's my point
in 1994 we start to see views invested
over lines now we can use them for
grouping it's becomes obvious you can
put one within another one and by 1996
the over line notation gets taken for
granted but we begin to see the explicit
statement of a convention that was
always there but it not been stated we
implicitly assume that when we group
some things together in fact we haven't
explicitly grouped it here but when we
write the Z when we substitute a bunch
of Z's for a bunch of Y's we assume that
we have the same number of Z's and Y's I
call this equal length Convention and
more generally if you over line and get
a bunch of things duplicated you want to
get the same number of each thing you're
duplicating and in 1997 a paper tries to
justify this by saying well we'll
explain this in terms of point wise
extension of scalar operations by
abusing notation operations on
Singleton's are implicitly extended
point wise to sequences and we
immediately run into a problem in that
paper what does gamma of b bar equals
substitute T's for X's and peas mean
well if you treat both equality and
substitution operators by point wise
extension you get the conjunction listed
at the top it says that gam of b1 equal
t-1 substitute of rex 1 and p 1 and so
forth that's very plausible
interpretation but in fact what the
authors had intended was the thing at
the bottom that all the substitutions
are the same each substitution has a
bunch of T's and bunch of X's and the
number of T's and X is completely
independent of the number of bees and
peas and somehow that's not quite
captured by that notation there in the
top line so a different problem so as
each offer tries to solve a perceived
problem in previous papers
new problems get introduced and so
language grows and evolves so the
reaction was perhaps we should use
nested over lines and some authors would
write it this way instead and
superficially this seems natural says
okay the T's and the X's are together
and the bees and the peas are together
but how do we know the gamma doesn't get
a subscript that's a letter to well
maybe you do a dimensional analysis ok
that's a semantic explanation rather
than tactical explanation and finally we
reach the point in about less 15 years
where different writers in functional
programming have found they want both of
these usages when they write an over
line over P equals the substitution
applied to Q where the substitution
itself has an over line that offer
almost invariably once the same number
of P's and Q's and the same number of
e's and x's but it's the same
substitution in each equation on the
other hand when they're describing the
syntax of a case statement they use the
same nested over line they say ok a case
statement has a bunch of clauses in each
clause as a constructor followed by a
bunch of arguments followed by an
expression but I want each case Clause
to have a different constructor and each
constructor may have a different number
of arguments so the Y's can be different
and with a purely syntactic theory we
can't have it both ways we can't stretch
this and you this use of nested over
lines to mean both things so I began to
study this formally is a problem in
language design what do we want from a
formal over line notation well first of
all we take any string of an over line
over we like that to represent any
number of copies of the string may be
separated by commas may be enclosed or
not those are details each copy of the
string can be expanded differently we
generally want that but within each copy
of the string within each copy the
string multiple occurrences of the same
BNF terminals
we expanded the same way is if we just
written one copy want that same
semantics if we write over lines stir
and mention it more than once in a
different context we want the expansion
of each of those Overland notations to
be the same for the same reason or for
BNF non-terminals in English prose we
need to be able to mention the same
thing more than once and have it mean
the same thing we'd also like to retain
the old tradition that if instead of
using BNF non terminals we just use
variable names if a variable name in
recursive and stir then copy I of the
stir should refer to V sub I we intend
the subscripting and finally all
variables occurring the stir should have
the same length we want to do continue
to impose the same length of convention
so what principles can we extract in the
set of desires well I see two one is
that if a notation explicitly appears in
more than one place on the paper if you
use different ink or different space on
the screen then all such explicit copies
have to be given the same expansion so
who write F of E and en e explicitly our
intent is that those are the same
expression E and you can override this
default by decorating on terminals if I
wanted the expression to be different I
could write F of E sub 1 e sub 2 es of 3
so I could say it either way
that's cool on the other hand if copies
of a string are made virtually through
the over line notation then we want the
copies to have different expansions
because that is the convenient thing for
us to use it for so when we write F of E
bar that means F can have a whole bunch
of a bunch of expressions arguments and
expressions can all be different and we
actually have a way to express this by
decorating on terminals if we say that X
bar expands to X sub 1 X 2 dou X sub 3
and plug that back in for the X bar and
then further say oh and by the way X
does it be enough non terminal well
they've been decorated they can expand
differently everything's cool and on the
other hand if X is a variable then they
it's correctly sub scripted so this is
great but we have no way in the notation
to override this default and that's the
problem that is the source of the
problem so here's I propose how I
propose to solve this in central country
to can't go to borrow an idea for pause
decoding one enclosure is called syntax
quoting a back quote followed by an S
expression means make a copy the s
expression but wherever there is a comma
that I'm speaking with Lisp acts
here that means except here don't make a
copy of it instead evaluate it and use
its value closer has this with a
slightly different syntax and Danny
Hillis and I used a similar idea to
express parallelism in connection
machine lists back in 1986 alpha in
front of an S expression means evaluate
many copies of this s expression one on
each processor but a bullet means except
here a bullet means take the value of
that expression and it will be a vector
Y a vector in the computer science sense
and use a different vector element on
each processor that used a different
element in each copy and this proved to
be a convenient notation so what I
propose to do in computer science
meditation is to add underlines and in
effect an underline cancels the effect
of an over line okay maybe it's too cute
but I've tried using in practice I've
seen I find it convenient I'm trying to
persuade other people
so in this modification over line ster
can expand any number of copies of the
strings stir and each copy of stir may
be expanded differently but an underline
means except here an underlined portion
of stir must be expanded the same way in
each copy and this gives you the way to
override the default so we can write a
big over line P equals the substitution
applied to Q and the substitution has
invested over line in it well then I put
an underline under the whole
substitution operator meaning it's going
to be the same substitution in all of
the outer copies of the equations and
that works great on the other hand if
I'm trying to describe case clauses I
don't use an underline and I'm allowed
to use different Y's in different case
clauses if I write this thing where
gamma implies the x's of type taw and
put an over line of the whole thing i
can underline the gamma to say underline
shouldn't it's the same gamma for each
one don't subscript that one just
subscript the X's and the toes and that
community ends and so forth and so on
and the dimensionality of each variable
is simply the number of over lines minus
the number of underlines so I can see
that gamma is a scalar because it's
gotten over line and and underlined and
X is a scalar because it's got two over
lines and one under line but on the
other hand Y is two-dimensional because
got two over lines and no cancelling
underlines
so Y is a vector of vectors and there's
a Mechanics for formal over line
notation
and I think for lack of time I'm not
going to stress that too much just to
point out that a for a formalization of
this notation needs to track two kinds
of constraints the requirements for
identical expansion wherever those
requirements come from and requirements
of variables be the same length and the
insight is that if you use the original
subscript attachment model for over
lines but then allow underlines to
override that automatic attachment then
the usual rules are be enough non
terminals including the same expansion
constraints and different and decoration
rules cause everything to fall out very
beautifully so that different so the
history notation actually feeds this
solution in a nice way and various
constraints and additions suggest that
over lines should be expanded first then
BNF non terminals and then substitutions
last and I'm going to skip over the over
the formal model because I want to talk
a little bit about ellipses and still
end on time because I don't understand
between you in the party okay here's
this great quote that captures it all
for me from a guy who wrote a book back
in 2003 most readers will have
encountered the dot dot dot notation
already it is a notation that is rarely
introduced properly mostly it is just
used without explanation as in for
example one plus two plus dot dot plus
20 equals to ten and we kind of infer
the numbers we think are missing there
and we hope that they are they form an
arithmetic progression and all that you
know but most authors including computer
science have used into notation for
decades and just assume you know what
they mean in fact it's use and
mathematics goes back a couple of
centuries now in the past we've used
ellipses to explain over line when
authors bother to explain their early
notations they say things like X bar
means X sub 1 comma comma X sub M but
what does that mean or what does it mean
when I write X 1 comma X 2 comma dot or
if I write e 1 comma comma E I comma
comma e n what does that mean exactly
and are those two dots are they working
together are they independent
you know you probably you probably can
infer what you think I mean by that I'm
just saying that the rules haven't been
written down and I propose in fact that
the right way to proceed is first to
provide a formalization of the over line
notation that formalization can be done
without using ellipses at all and then
explain the ellipsis notation using your
line notation
and therefore we not real I mean their
before we do not have any circularity in
the fight by trying to define ellipses
in terms of itself so the basic idea is
to predefined the set of standard
ellipses usage patters be supported that
maybe you could even have user-defined
ellipsis patterns make it be an
extensible language school but for each
set of ellipses expansion has to
identify a matching usage pattern and
each pattern includes one or more
occurrences of the ellipsis symbol some
number of copies a separator string and
some matchable strings and the idea is
that you match the pattern against the
text you're facing use unification to
find a common subsequent structure for
the matchable strings and thereby
producing a unifying substitution and
then use that to construct a
corresponding over line notation that is
intended to be the expand semantic
expansion so in fact I'm is proposing to
treat like yet another macro so here's
an example very simple example if I see
X sub 0 comma comma X sub minus 1 the
separator is comma the matchable strings
are X sub 0 and X sub n minus 1 the
common structure is X sub i where I get
the first one by substituting 0 for I
and I get the second by substituting I
minus 1 for I and then I plug that into
a template this is an extended version
the overlay notation the prides in
explicit range and the result is an over
line of X sub I where I is constrained a
lie between 0 and n minus 1 and then you
put and that semantics is then defined
separately over where you're defining
your reline notation and I've I won't
show you a bunch of more examples but
I've shown that I can take a bunch of
examples luces notation and translate
them in this way to over line notation
and then explain that another way so
that's the pros program so here my
conclusions computer science and a
notation is a symbolic programming
language and it has its own distinctive
syntax semantics and idioms it's got its
own peculiar user community and that
user community is subject to certain
resource constraints and some of them
are unusual such as conference paper
page limits and therefore a desire for
extreme concision which drives them to
make the notations very concise drives
them to be typeless they don't need type
declarations and a bunch of other
equalities and I think it's a neglected
stepchild of the computer science
community I mean it could be a viable
programming language I think it should
be an explicit object to study by
computer science
it's a living language some of the
notational ideas go back problems are
developing as they do in any growing
language I think these problems can be
fixed and I've pointed a way towards
trying to solve those problems I'd like
to develop a complete formal theory
language including the over line and
ellipsis notations which are fairly
recent innovations including the cases
of nesting then you can nested ellipsis
notation interesting addition nesting
the over line notation and you can use
them together and you can make sure that
how all that works is explained and they
interact with BNF and with substitution
I'd like to apply the techniques we've
developed for other languages to CSM to
build interpreters and compilers and ie
des and correctness checkers they would
probably involve proof checkers like
clock and so forth and other tools I'd
also like to point out that there is
serious opportunity for expressing
parallelism in this language and I think
that closure might be an ideal
implementation language for this
language and so if there's anyone here
who might be interested in having a foot
in each puddle I'd like to talk with you
and so that's the end of my talk thank
you very much
yeah I can have you do questions I think
we read exactly 510 but go ahead
you have question is do I see this as
primary sort of an introverted language
this talks about computations or is it
something is reasonable to talk to the
outside world like a databases and
machinery and the internet and stuff
like that I could see it growing into
that it would need to have a lot more
grafted onto it to talk to the outside
world right now its user community is
primarily type theorists in the 1980s
its primary community was people using
horror logic to describe the logics of
programs and then there was a phase
shift for the type theorist kind of took
it over so it has mostly been used for
those kinds of internal purposes it
might grow if we're given the right
tools or it might not
and it might continue serving this very
niche community but I think that niche
community should pay attention to its
own needs it's kind of like the Cobblers
children going barefoot you know and
they're I think they're doing themselves
and not by about writing not providing
proper tools for their own tools and I'm
using they I'm using the third-person if
you consider self part of that can I
consider myself part of that community
if you do to consider yourself included
in the first-person yeah
yep exactly that's that's the kind of
tool thinking I'm thinking about you
know try and fear how this notation fits
into a tool chain and and and community
usage that's exactly kind of think I'm
after thank you yeah
well I'm claiming that the program is
the type checker now let me go away back
to the beginning the talk way back to
the beginning there I claim this is a
type checking algorithm it's a very
small one it only checks seven different
kinds of syntax but if you plug a piece
of syntax in as an input the type will
pop out as the output and the way this
is executed is by taking a proposed
situation say okay I've got I happen to
have a lambda expression in hand I would
like to know its type oh and I've got up
I've got an environment gamma that gives
me the types of symbols that may be free
in this in this piece of syntax and in
order to prove the type of a lambda
expression it says it says the type will
be Sigma arrow tau provided I can prove
that that the expression M is a type T
when I use the environment gamma when
extended by saying that X is also known
to be of type Sigma and so in order to
prove the thing on the bottom all you
have to do is prove the things on top
you can think of this as a function call
you can think of the thing under the
line
counter-intuitively is the the program
header is the function header is the
method signature and the thing on the
top is the body it's the things you need
to prove in order be entitled to
conclude the thing on the bottom and
this is exactly the Prolog programming
language works the only thing is the
prologue has an inherently sequential
implementation and he used it uses
chronological backtracking I'd love to
see a parallel implementation that did a
better job
when do I expect this work we ready for
publication I am I've begun to write
papers about this myself I hope you get
something out within the next year or so
right now I'm giving given a talk or two
about I gave it a talk similar to this
at principles and practice parallel
programming last spring and I've gave
the talk with some students at MIT so
I'm giving it here yeah
yeah the question was is it is that I'll
stand on the question I noted that it's
hard to find these programs in
literature because OCR doesn't capture
them and furthermore they're hard to
write because they're hard to keyboard
and in fact most people who are writing
such programs nowadays are using low
tech and I would Prozak to just use low
tech as the keyboard the keyboarding
syntax or format for this language
knowing that can be typeset in the
beautiful form in much the same way
they'll go as a publication format
pardon well I think there could be a
reader that reads the latech and turns
it into some syntax tree that you could
then run an interpreter on and this is
how might why compiler demonstration
worked it actually read the latech
syntax from my Emacs before I wrote the
compiler in a lisp because it was
working on my Emacs buffer and it just
produces a stupid parse tree from the
low tech input and then runs over the
parse tree and frame it translates into
Prolog dumps that out of a file and then
invokes a Prolog interpreter runs the
program and you know it all worked you
know as I say on this one toy example
it's amazing we can doing you and you
can glue six things together we're never
ever designed to work together but you
can just get Python or even a makefile
just to get them to pipe together
that's also a great idea and that's
related to my idea of I don't want to
use his substitution notation I want to
see my substitution notation and my
preferred notation might be English
instead of the mathematics makes a lot
of sense look at this side of the room
any questions over here decide are we
done time for dinner and a party thank
you very much
free coffee
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>