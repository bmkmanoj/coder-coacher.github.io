<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ryan Neufeld - Simulant in Anger | Coder Coacher - Coaching Coders</title><meta content="Ryan Neufeld - Simulant in Anger - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ryan Neufeld - Simulant in Anger</b></h2><h5 class="post__date">2015-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RHf8ngqVKys" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the title of the talk today is
simulate in anger which is an experience
report about applying simulation testing
to our financial system but first let's
get started with a bit of background
about me my name is Ryan neufeld I am an
alumni of cognate sect as well as the
co-author of the closure cookbook as
well as the author of application
architecture for developers and since
working for cognate echt I've moved on
to found my own company called homegrown
labs which is a consulting and product
company that helps companies gain
confidence in critical systems and one
of our big clients today is a company
called visa which is an Irish financial
services company that I'm helping with a
few others one of them here in the
audience Howard lose ship build a mobile
payment system by the name of mix so
this talk is broken up into three parts
the first we're going to talk about what
is the mix system in the second part
we're going to talk about what is
simulation testing and in the third part
we're going to explore what we learned
from applying it to the mix system so we
start with mix mix is a next-generation
mobile wallet somewhat similar to Apple
pay if you're familiar that facilitates
smartphone oriented transactions between
consumers and merchants I take a little
bit of a closer look at what kinds of
entities were interacting with we have
consumers and issuers on the top if
you're not familiar with what an issuer
is usually this is your bank or some
third-party that's participating in
transactions on your behalf and on the
flip side you have merchants and
acquirers who represent merchants and
you both meet in the middle and you
agree that you'd like to pay for some
goods or services and one of the special
things that we add on top instead of
just being a simple payment mediator is
something called value-added services
and these are additional functionality
like loyalty programs couponing etc so
instead of having to print out a paper
coupon or carry a loyalty card with you
if you're shopping at a participating
store in Europe as it would have it you
can just see on your phone
that you can apply coupons or
participate in those programs right from
your phone which is pretty convenient so
zooming in a little bit behind the
covers mix is a micro service
architecture meaning that it's built up
of a cluster of services that
collaborate together to provide payment
services so that's all well and good but
we have one catch we're going to be
landing on Mars so to speak mix is a
completely green field system meaning we
have no prior on-the-ground experience
with how that application is going to
perform in this strange environment and
this isn't very dissimilar from the Mars
rover itself which was going to be faced
with extreme temperatures and
environments and NASA having no direct
way to test on Mars without the extreme
cost and you can imagine you wouldn't
run unit test where you shift a new
rover to mars every time you wanted to
check a single character change for us
our extreme environment is landing in
one hundreds of hundreds of stores on
day one and servicing thousands of
tellers the very moment that we deploy
which is kind of a big challenge and it
means that we're not going to get any
second chances we can't show up in the
stores and be the same untick lakay
today a crappy system that people hate
to use the very day we land there we
need to engender a number of qualities
to make sure that we are successful so
what does success mean for the mix
project what kind of constraints do we
have to work within when we're trying to
build the system and make sure that it's
fit for task so the first thing we need
to do is make sure that we can deliver
the thing on time we have real merchants
that have signed up to get this system
in their store later this year so we
need to be sure that we're finished on
time and that means excuse me and I'd
estimate we actually only have about 3.5
developers on the project on average
we've had three previously this year and
have only recently added a fourth and so
that means that we have very little
bandwidth for testing our features let
alone extensive testing of how the
system is going to operate so we have to
be very mindful of where we spend our
time
the second criteria for success the
system must not only be delivered
quickly it needs to be fast when it gets
their attention in patience for
technological interactions is
vanishingly small and consumers we can't
afford to be that slow system on day one
and finally we need to be able to scale
from that initial load of thousands of
tellers to any number more and to make
matters worse we've kind of stumbled
upon a rather novel technique where we
can actually introduce the mix system
into stores with no hardware and no
software changes at those point of sales
I'm not going to share with you how we
do that but to me as a developer on this
project I perceive that touch the mic I
perceive that as a significant risk that
we might be seeing a lot of load very
quickly after we launch so how did NASA
ensure that the rover was fit to task
well they did it by simulating reality
they replicated the conditions that the
rover would endure extremely high
temperatures that it would see during
entry the low temperatures that would
see traveling through space and the
harsh arid conditions then it would go
through when it actually got onto the
ground now we wanted to do something
similar for mix we wanted to throw it
off the deep end to bombard it with
traffic and to make sure that it could
survive but how we looked at a number of
tools we looked at performance testing
tools like siege an HTTP F which could
give us a very good idea about how
certain very specific endpoints
performed but rather too narrowly
focused for us to understand how the
entire system operated integration style
test with something such as jmeter or
selenium could give us some very useful
information about correctness and even a
little bit of scale but they're often
too rigid and prescribed so enter
simulation testing I would describe
simulation testing as a holistic
approach that can not only verify the
features that features in an application
are correct but validate properties like
the performance and robustness of that
system and to me this is the
quintessential black box
test it injects behavior on one end and
measures the output on the other but
didn't I just say we weren't doing
integration tests but then I described
integration test is the thing that
simulation testing was not quite there's
a little bit of a subtle difference here
integration tests are what you might
call a specification based approach to
testing they step through a predefined
set of actions validating the results
along the way simulation testing on the
other hand is what you would call a
generative testing approach rather than
defining exactly what a test will do a
simulation test defines what a test
could do and it lets the framework do
the less the rest of the work from there
so not only excuse me so this is this is
great both of these approaches can
certainly validate the correctness of
features to some degree that we know
about I think we're simulation tests
really shine being generative is that
they can actually explore unknown and
strange possibilities that your
developers would never really think of
happening in your system but your users
are bound to try but features alone
don't necessarily equal a
well-functioning system there's
something we've missed we're missing and
I've mentioned it a few times and that
is the qualities that our system exudes
and these are sometimes called
non-functional requirements in other
parlance a--'s and these are things like
your systems performance robustness or
security and they're the kinds of things
that round out your system taking it
from an unusable but feature complete
pile of junk to something that's truly
usable by the target audience so that
the last mile validating those qualities
is what we were seeking in a tool and
with all those other tools out of the
question that left us with one rather
not very well known option enter
simulant now simulate is a closure is a
closure based simulation testing
framework built on top of day Tomic from
the fine folks at cognate act and in
traditional cognate ecton we all know
this so well it d come
poses all of the essential aspects of
testing into a number of disparate
pieces and what results I think is
actually greater than the sum of its
parts and we'll see that sey shortly so
the very first stage of a simulation
test is the model phase and this is
where we capture all of the potential
user behavior that a user might partake
in our system into a probabilistic
Markov model like this and we can create
that either by a informed guess or by
directly analyzing what happens in real
life which we didn't have the
opportunity to do and so what this does
is it lets our sims explore states that
we might not have considered previously
if you can imagine that for some reason
hitting be ten times in a row might
cause a problem most normal tests might
not find that but with the simulation
test we're bound to run through a random
walk through this that cycles over be
just enough times to cause an error the
next phase after we have this model of
what our users could do is to take that
randomness and realize it into a
concrete set of planned actions which
simulate calls its tests and since this
sampling where we walk through this
model and come up with this stream of
actions happens before we actually run
the test we have the very nice added
benefit that we can revisit that test at
some point later in the future now my
why what you want to do that well if you
can imagine that you ran into some stew
some combination like this one here that
elicited a strange error in your
application you could debug that error
deploy a correction to it and then run
over the exact same set of inputs to
ensure that you'd actually corrected the
issue in this same way if you've
identified particularly problematic
combinations you could capture those
tests and run them every once in a while
to make sure that you haven't slipped
through on any strange edge cases and
not only can we generate a single stream
of actions we're actually able to
generate any number of users so in the
case of aviso we want to stress the
system not with one user but
tens or hundreds of users so using that
same model we can take different random
samplings through it and come up with a
full set of differing actions that each
agent can take now the sim phase of
simulant is where we finally run our
tests we have the opportunity at the
beginning here to do some setup things
like creating user accounts or gathering
a baseline for performance and then we
spawn up any number of processes either
as threads on one machine or across
multiple sheet multiple machines that
will run our planned actions and instead
of validating those as we go we write
all of the results and pertinent
information that we gather from that
system to an action log residing in day
Tomic and what that lets us do is really
cool since all of our validation
information is in a database most of our
validations can actually be written just
as simple queries if you can imagine you
want to find out if you had any slow
queries or any queries that error it out
that would simply be a matter of
selecting any action log entries that
had an error or were too slow beyond
that it also gives us the ability not
only to consider requests and responses
individually but an aggregate allowing
you to do things like collect
performance summaries for your run or as
something we'll see later gather how
many transactions per minute our system
can do by considering everything that we
did across the entire run and since like
tests this is separate from invocation
we can actually retro actively run new
validations against old simulations so
if your business managers or project
managers come up with some new condition
that your system should not fail you can
implement a validation for that and then
you can go back through time and find
out where you compliant through history
and if not do you need to enact any
retroactive changes or reconciliations
to correct for that error that you've
made so finally that brings us to our
last section simulant and mix so when we
set out to build a simulation test for
the mix
system we had a couple of goals the
first thing we wanted to do was gather a
baseline for performance in the system
we want to understand given where we
were going how close are we now how much
work do we need to do and how much of
our effort do we need to put towards
making sure our system is fit for task
for delivery and finally we wanted to
come out of the process of building a
simulation test with a tool that would
let us incra mentally identify and
correct hot spots at our application to
make sure that we could go that extra
mile so how long did it take to build
this well to get from nothing to a
simple smoke test simulation that
basically ran the acquirer side of the
equation took us about a week it's not
terrible but it's not great and it gets
worse when you consider that it probably
took us about another whole month after
that of one person's time to ramp up
that simulation to a full multi-user
flow which was issuers and acquirers
interacting and closing payments so why
did it take so long well at the time
tools and knowledge around simulant were
scarce we were lucky enough that I had
had some experience with the fine folks
from cognate eck working with Mike
Nygaard on other simulation tests that I
kind of knew where I was going but if
you don't have that kind of experience
it can be tough simulants api's are well
designed but with very few clear
examples of how to synthesize them into
a decent simulation to utilize it you
can run into problems where you might be
able to build a simulation that works in
some cases but have a lot of difficulty
going the last mile for it and to make
matters a tiny bit worse things don't
necessarily work entirely correctly
until you have a whole picture of your
simulation you have all of the pieces
from model to validation so it can be a
bit of a challenge and I think that
simulant is a little bit like closure
someone said this I don't know who I
couldn't find an attribution but someone
said wants a closure that it was hard to
learn but it pays you back and I think
that simulate really did pay us back in
developing the mix application so what
did we find how
did it pay us back well the first thing
we actually ran into and it actually
ballooned that time a little bit was
that we hit issues with our system
itself where it didn't behave like we
had expected even though our features
were tested they didn't behave quite the
same as they did under integration tests
as they did for truly external clients
in a deployed environment so this is
already one of the places where
simulation testing is very valuable we
were developing a system that was going
to be consumed by third parties that
would integrate against it and we had
the opportunity to experience ourselves
what that implementation looked like and
to make improvements to our api's
themselves and the documentation to make
sure that there were less road bumps
when it came to those merchants
integrating against our systems once we
actually got it running we found
something rather interesting we
discovered that even under the most
modest amounts of loads our system
started started to thrash very badly we
went searching for hotspots we found
some interesting things through
monitoring data in your kit we ended up
finding that even though we had planned
on our system being elastically scalable
outwards on AWS that it never did that
and the reason was because we had
written on auto scaling triggers to
presume that our system would have a
high amount of CPU load when there were
many users interacting with it as it
turned out our system was not cpu-bound
it was I Oh bound and as such it never
scaled out so we'll add that one to the
to-do list but there's something that's
still wrong here with the individual
services when I say modest amount of
traffic i don't mean hundreds of users i
mean we only had five to ten users
interacting with some of these services
and they were falling over and that's
just completely unacceptable we we can't
afford to have one instance of a service
running for every few tellers in a store
that's absolutely ridiculous and it's
not going to scale it's going to cost so
much money to run that system so we went
looking further and we found the culprit
one of our most important functions the
create payment endpoint seem to be
getting hit
ticularly bad under any load the
response times tended to be either 10
seconds or no seconds it just never
returned hence locking up the entire
simulation and making me pull my hair
out so being a micro service
architecture it turns out we were
tripped up by one of those little faux
paws that you can have where you can be
a little bit too chatty with between
your services so well we had tens of
requests pouring into the acquirer they
found out to tens of requests on the
transaction engine and tens of requests
on the value-added services all of which
had to make sure that the user they were
talking to was who they said they were
so all of them collectively beat the
crap out of the authentication service
and drag the entire system to a halt so
we had to figure out a way to fix this
and we figured what better way than to
just not do that at all we won't call
the authentication service ever to check
if a user is validated so how we did
that is actually pretty cool if you've
never heard of JWT's there there's
something called JSON web tokens and
they're this really cool piece of tech
that's actually about to become a
standard and what it is is it encodes
small payloads of JSON that contain
claims about who user is and what they
can do and when the token expires all
into a short little randomized looking
token and what more we can encrypt those
on the authentication service so a our
clients can't inspect what abilities
they have and B we can verify that the
token we're receiving actually did come
from our own service so with all of that
out of the way we made some pretty
substantial improvements and I'll just
make a note we're going to talk in
relative measures my clients asked me
not to share specifics about what our
exact response times are but i'll give
you ballparks so for starters we
improved our mean response time for
successful responses to times which
meant we got very well under the half a
second mark which is getting close to
that real time ish feel that we'd like
to have from a financial system we've
still got a ways to go but that
an improvement from where it was before
even better for the responses that
actually completed we improved our
reduced our maximum response time by a
third meaning that now at a very high
percentile almost all of our operations
are coming in under one second we're
just starting to get comfortable even
better we reduced our error rate in our
simulations from 70 a staggering 75
percent but basically not working at all
down to zero and that gave us a really
good indication that we should increase
the amount of load we're throwing at our
servers because the number of instances
it has can very easily handle the load
were throwing at it and this these
improvements applied not only to the
create function but across the entire
system we saw in almost five times
improvement in our overall throughput
for just basically running as many
transactions as we could per minute and
where we were before we started this
process was we could run 30 transactions
per minute no matter how many services
there were no how many users there were
we could never go past that watermark
and now we're up to a point where we can
run about a hundred and seventy
transactions per minute with very
minimal hardware and about 10 to 20
users interacting with the system at
once and that is also proportional to
the number of services and users which
means that yes we actually can scale
this to some point where we'd be able to
handle thousands of stores so what other
things do we find out by going through
this process other than just raw
performance changes one of the big
things that did for us was it eroded a
lot of false confidences we'd previously
held about the system and I think as
developers were inherently very
optimistic about the systems we build we
have a confidence that we haven't really
earned that those systems are just going
to work out when we put them into
production and it was it was really good
to get rid of those confidences those
false confidences and instead replace
them and some true confidences some hard
facts and measures about our systems
correctness and the qualities that it
exuded and it allowed us to answer some
important questions about
system namely are we there yet and how
far do we have to go and finally while
we're not at the finish line yet we now
have a tool to continuously improve and
change our application as we reach
towards our production goals and there's
a bit of an anecdote i mentioned how
long it took to build the simulation
test itself but never how long it took
to make those improvements to it as it
happens it only took about a week or two
once we have the simulation and we're
dedicated to improving that hotspot to
get it out of the way which i think is a
pretty fantastic improvements in
performance for such a short investment
of time so in closing I think simulation
testing is something that takes us the
extra mile and testing allowing us not
only to test the features that our
system has but the qualities that it
exudes and I think those are so
important no one would ever want to work
with a website that took one minute to
respond to every page request or leaked
all of your private information to
another another party and those things
are important in how your system
actually works so you might be asking is
simulation testing for your company and
I don't want to have any illusions here
I think simulation testing right now is
a pretty expensive proposition it does
take a decent amount of time and I would
consider this a little bit in the light
of a medical analogy if you can imagine
you have a system that's not terribly
large and something isn't quite working
right you might go to your doctor with
that system and they might prescribe
some over-the-counter remedies and those
aren't very expensive they're easy to do
and you're probably going to get the
results you're looking for if you fall
off a mountain and you break your legs
or your system does rather when you go
into the doctor they're not going to
suggest that you just go and take some
tylenol they're going to suggest some
advanced diagnostic medicine and I think
that's the kind of thing that a
simulation testing is when it really
hurts you need to resort to these kinds
of tools to get better insights on to
how your system is performing so the
biggest thing you have to ask if you're
considering if you want to implement
simulation testing on your own system or
if you should rather is your system high
risk is your company's reputation
it's money or it's lie or people's lives
at risk if you make a mistake in
implementing it and if the answer is yes
then I think the time that you would
spend on implementing a simulation test
would easily dwarf the potential upside
and as kind of an identity to this this
is actually something I'd like to change
in the future I'd like to lower that bar
from high risk meaning that you should
maybe use simulation testing to maybe
moderate risk and I want to do this but
through education tools and services
that make it more palatable for a
company to entertain simulation testing
and where we're going to start with that
with the journey that I'm taking on
educating people about simulation
testing starts tonight at an unsession
that is being held at 7pm I can't
remember the exact room but i'll be
there and what i'm going to do in that
on session is cover some of the basics
of getting your own smoke test
simulation test up and running and as a
very special announcement i'm going to
be releasing a simulation testing
template that will hopefully get you
through from zero simple in hours rather
than days so thank you everyone for
coming out we've got actually quite a
bit of time for questions I ran a little
bit fast you can find my slides at this
URL here if you have any questions that
don't fit within the hallway or the
confines of this talk feel free to reach
out online at homegrown do / ask me
anything and I'll get back to you in a
few days and finally if you're
interested in learning more about
simulation testing and hearing about
when this new helpful content comes out
or new tools and libraries are released
feel free to go to my website at
homegrown dial and sign up for my
mailing list thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>