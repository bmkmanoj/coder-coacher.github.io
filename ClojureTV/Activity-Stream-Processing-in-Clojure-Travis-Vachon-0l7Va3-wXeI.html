<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Activity Stream Processing in Clojure - Travis Vachon | Coder Coacher - Coaching Coders</title><meta content="Activity Stream Processing in Clojure - Travis Vachon - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Activity Stream Processing in Clojure - Travis Vachon</b></h2><h5 class="post__date">2013-01-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0l7Va3-wXeI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah so like Stewart said I'm at Utah
Street labs right now which is kind of
funny because I don't think we've called
it that is the name of the company but
we're building a site called copious
calm and at this point we just kind of
call it that so my story today has 4x I
my initial idea was to make it kind of
at this american life thing but that
seemed a little kind of like trying too
hard so I dialed it back but I kept the
ACT thing and I hope you'll forgive me
the the names of the acts have something
to do with each of the X so hopefully
it'll be interesting so Act one is
called web scale and I see that they're
people who are clapping you get what I'm
talking about people who aren't don't
worry about it it's really not worth it
so like I said I work for a company old
coppice com we're building a social
market place we have t-shirts that I'm
actually pretty proud of it's like the
first company I've worked out where I'm
really excited about the t-shirt because
it's giraffes and that's kind of cool
and this is the site as of yesterday as
you can see we have a lot going on the
core mechanic of the site is people
buying and selling things to each other
it's a marketplace so one of our users
Pamela Joyce designs is selling this
seven hundred dollar glitter art jimi
hendrix poster and she has 7,000
followers the core domain model is
something like twitter but with stuff in
the mix so rather than just following
people you can also follow things you
can follow collections of things so
there's lots of stuff you can do you can
like things he can comment on things you
can buy things you can unfollow people
or you can follow and unfollow people
down at the bottom we've got down below
where the page got cut off here we've
got this share thing so you can also
share things so it's it's a pretty large
collection of activities you can take on
the site those activities are AG
turned into stories about activities and
then aggregated into feeds the feed is
at this point kind of the core
experience of copious in the same way
that the news feed is the core
experience of facebook this is where we
drop people at the end of the onboarding
process this is where people go when
they're logged in by default and it's
another place where people can take lots
of actions on the site based on using
the internet I think this is pretty
common you know a lot of sites on the
internet no matter what the domain is
are incorporating these social mechanics
and when people talk about social I
think this is what they mean you can
take lots of actions in the site they
interact with other people and and a
core part of the experience is those
interactions with other people it's no
longer just about your content and it's
no longer just about what your site does
it's it's about how your site connects
you to other people okay cool cool so
I'm a user on our site I can follow
people I can follow and I'm sorry for
the transition it's just so awesome the
flame thing it was just I was looking
through transitions and I had to use it
you can follow things and this is a
t-shirt on the site you can go buy it
today so just saying and when you're
interested in people we assume that you
are interested in the things are selling
so this guy has a cool leather jacket
and a cool pair of sunglasses you can
buy those too although i think the
sunglass is sold so you can't buy that
but you can still see stories about it
which is another interesting thing
actions are turned into stories which
are aggregated into feeds and like i
said that's a kind of a key thing this
particular feed actually doesn't look
too dissimilar from what might be in my
feed I see stuff about Rob I see stuff
about Brad and so that's good so the
main thrust of this talk is going to be
about how this works how we take these
actions this huge stream this fire hose
of activity on the site and turn it into
feeds it's a fairly tricky problem I
think it's it
can seem easy at the outset but you know
once you start thinking about the
numbers involved when a single action
comes in we have to look at all of our
users figure out who's interested in it
and and stick it in there their feeds
there are a few other wrinkles in our
particular system that that made it even
more complicated and you are
occasionally in a position where you
need to look through a large collection
of old actions things that happen before
on the site and figure out how to build
a new feed for that person so when we
started building the system we had a few
MongoDB instances and production so we
said well let's use this tool that we
are already somewhat familiar with both
you know at a programming level and at
an operational level and see if we can
make the problem fit in that domain and
initially it seemed like yeah we could
we could actually model it pretty
cleanly so the first thing we did was we
stuck we stuck actions into MongoDB as
basically JSON documents not technically
true but good enough for what we're
doing embedded in those Jason documents
we kept a list of interested users so if
a new action flowed into the site we
would look up who might be interested in
that at that time and stick their user
IDs into a document within that action
when we wanted to build a feat and serve
a feed to somebody when somebody logged
into the site we would do a query for
that user ID and get the stories back
cool it worked at first and and in
development and in test and even in
production for a little while but it
ended up falling over pretty quickly so
I'm not going to go too deeply into
MongoDB because we're not in a manga
conference but to make a long story
short Mongo stores basically JSON
documents in a structured way and lets
you do things like query over the
internal structure of those Jason
documents which is cool it's interesting
except that it falls over in unexpected
ways so one thing
just blew up immediately was that
growing these interested embedded docs
can actually lock the database which I
mean and I don't mean like the rights I
mean for reads and I mean on like the
slaves and the whole thing because what
happens is they grow pretty big and when
I say big I mean we deployed this and we
have this community manager Caitlin and
she basically from when she started had
something like seventy thousand well
thirty thousand seventy thousand
followers because we stuck her in the
onboarding process and she was like tom
from myspace you know everybody follows
her and I am a hundred percent certain
after going through this that tom from
myspace alerted those myspace guys to a
lot of scaling problems way before they
would have had them otherwise because
that's what Caitlin did for us so when
Caitlin did something we would suddenly
have this this document with like 40,000
IDs in it and then what's worse is when
somebody followed Caitlin we would look
up all the stories like the recent
stories or something like that we've
tried to block some of this out of our
cultural memory but we'd look up all
those things and we add stuff when those
in embedded Doc's got too big they would
just explode we also query sometimes
required scanning through the embedded
doc which anybody who's ever watched a
table scan happen in a myschool database
will understand that's that's just not
going to be good the query optimizer
also made some some pretty bad decisions
when trying to do this and the net
result was that when this was
operational we were like just straight
up failing to serve ten to twenty
percent of requests because they were
timing out or because there were errors
or whatever so that was not really ideal
so we need a new approach act to set
theory so we said we step back from the
problem just a little bit and thought
about the domain so what are we doing we
have sets of stories about people in
listings and thieves are the unions of
those sets we Union along interest lines
so when we want to generate a feed we
figure out which sets people are
interested in by looking at their
interests and we Union those together to
make a feed so when I was putting
together the slide it was like so we
thought of
you know what technology is good at
working with sets and I was like
relational databases do this whole thing
but that was not the decision we made I
feel like that's the punchline of like
ninety percent of operational talks
these days but we actually ended up
going with a server called Redis and if
anybody's not familiar with Retta sits
it's kind of like it's it's like well
you have a problem where you'd like to
use see data structures because they're
really fast but you don't want to write
your whole system and see so what you do
is you use a server that basically
provides see data structures over a
network interface it's pretty cool and
one of the data structures that they
have is a sorted set they have set sort
of sets lists we took advantage of these
sorted sets and sets pretty heavily so
what we did this is kind of like a hard
to see illustration of the model that we
had in Redis the top thing there the
interest thing is a set it's a set of
interest the the actual things we stored
in the set were just strings t42 tag 42
l 32 was listing 32 and we also had
actors so like actor 17 those
essentially served as pointers not
literally in registry to resolve them
ourselves but pointers to story sets we
tagged the story you know the string
stories under the beginning of the
interest and then we issue a command to
go take a union so like I said the
client got the interest for a particular
user went down took the union of those
sets and then produce the feed so the
idea here the initial idea was that we
could do this at runtime essentially
when somebody wanted to feed we'd go and
do a build and and then return it to
them we anticipated that this might be
slow sometimes these unions stores would
be slow in some edge cases like if we
wanted to build a feed of somebody who
would fall
you know seven thousand things and some
of our users I swear to God logged onto
the site and just followed everybody
they could and I don't understand what
they were doing but they they had 7,000
followers like or 7,000 people they were
following and like 15,000 likes and it
was it made for some slow Union we
anticipated that we said okay cool if
this union store takes too long if you
needing that set state takes too long
stored in Redis and will maintain it
when new stories come in it's not really
super scalable but we were hoping it was
scalable enough to get us kind of past
the next six months it was not at all so
you know this is the initial idea when a
new story comes in we add it to the the
manage sets this this operation is
pretty slow because we have to actually
go to our my sequel database and say hey
who might be interested in the story and
then we have to go find the seeds that
need updating then we stick him in the
feeds what was cool is that initially
this took like 20 megs of ram Retta
stores everything in RAM and write stuff
out to a dump file eventually and it was
like it was it was just genius until
well so yeah the Ruby client did all the
heavy lifting we will push some work to
the background with rescue and then we
broke Redis so like I said predicates
and they did a sentence we always be
fast so it just exploded almost
immediately too many interests meant we
were pointing to too many story sets too
many story sets meant stole Union ops
unions ended up being almost always slow
was our fault we were blatantly misusing
the technology there were many managed
feeds management was super expensive and
it it just didn't they failed to serve
feeds so bummer so we need a new
approach act three the mini arm demon
and I don't if you can see that's I like
riches github gravatar
and I grabbed that demon from some kid
on the internet and I don't know if you
were watching this video kid I'll give
you 200 bucks for thing whatever I felt
a little bad about like the copyright
thing but it was yesterday so okay so
bees can't be built in a fast enough at
runtime we just decided that's that's
done so the the new solution is to
maintain all the feeds and reticent just
own that we need to be really good at it
it's going to mean a ton of RAM usage
but you know that's that's storage we
can pay for storage and we can deal with
that for a little while so what we did
was we well first off we thought hey can
we write this in Ruby and we decided no
that's kind of crazy because we needed
to be really performant will probably
there's lots of potential parallelism in
the system and and just doing that is
going to be problematic the reason we
thought about Ruby is because we use
rails so everyone in the team knows Ruby
and it's kind of nice to keep things in
that stable but I like closure a lot and
I was kind of excited about a project to
use it we were already using it for some
of our data warehousing with casco logs
so it was a natural fit so you have a
closure we wrote a closure demon that
reads from rescue it updates these
interest sets it updates story sets and
it updates feeds essentially everything
as a managed feed now and we decide what
to stick in what feeds and all that
right at runtime we also had a new
requirement we were doing a big press
push and the feeling from the people who
are doing the pres bush was that it
looked ridiculous to have multiple
copies of items in the feed so we needed
to get this out on a particular deadline
and it needed to roll stories up into
other stories so if somebody likes 130
things you don't want 130 things showing
up in someone's feet and just flooding
the feed we had a couple big sellers who
list in bulk and so they would just
flood the feed with all of their stuff
and it was kind of annoying we also
would digest along other lines not just
a single person doing
a bunch of things we would also say you
know jim and john loved this one thing
and jim loved and shared so we took
multiple actions against another thing
or jim and john loved and shared cool
sunglasses so multiple people did
multiple things this digesting actually
turned out to be one of the harder
problems in general to solve and one of
the bigger operational headaches the
problem is that digesting is is somewhat
complex and it's there's no server
there's no database that just does it
for you it's possible that we could have
come up with some clever scheme to use
some database but it didn't come to us
within the like two minutes that we
wasn't two minutes it was like a day
that we spent trying to think about this
and trying to figure out what we were
going to do so what we decided to do is
just maintain the head of each feed the
last six hours of every single feed in
memory and I told someone last night at
the bar that we were using 50 gigs of
ram and in a machine in ec2 and and this
is why so that's the kind of punch line
spoiler alert so we took the head of
each feed we stuck it in an atom and we
kept it in memory when new stories came
into coming to the system we add it to
each feed and then we write the feed out
to read us and clients for you from
Redis if you haven't used atoms before
they're real simple they are one of the
state mechanisms that closure provides
the semantics are dead simple you have
an atom you dear f it to get the actual
data structure it contains and then you
can update it using this swap function
the actual underlying implementation is
such that you don't worry about how that
function is applied and i'm ninety-nine
percent sure that function might
actually applied twice you have to be a
little bit careful but like it can't be
stateful i can't it shouldn't do things
with other systems but it was ideal for
what we wanted and essentially it let us
turn the parallelism of the system way
up it was so had this work out it was
slightly dangerous too
process multiple actions off the rescue
queue that we were reading from that
this demon was reading from at once
because we didn't want to get into a
situation where we didn't really know
what was in what was currently going
through the system it made it harder to
shut down and it was just a single demon
anyway so we didn't want things
interrupting each other long story short
tons of bottlenecks initial feed builds
story adds interest ads were all in the
same process there were some ways we
could work around this we didn't need
them all to be in the same process
necessarily but once you start thinking
about that i mean the problem starts
getting bigger and and and so we didn't
necessarily want to build a tall
ourselves so we needed to make it as
fast as possible that was the the
fastest way to victory just figure out
how we can make this single demon
process things office queue as fast as
possible fortunately our machine had
tons of cores and closure has these
atoms which let us maintain state in a
way that means we can throw more threads
at it with a minimum of effort so what
we did was we broke up we broke this
team and up into a few little pieces all
in different threads we have one thread
that just sticks stuff into new feeds we
have another thread that goes through
and expire speed so we don't have to
have that particular logic in line in
the story processing and we have another
thread that actually just looks at this
data structure inside the atom it
essentially gets a snapshot of the atom
it writes it all out to read us and then
it just does it again and it just keeps
going and writing and the reason this
worked and the reason adding this and
breaking this up into several pieces was
essentially free reveal was that a
closure head it has a very considered
approach the state and and it really
paid off I mean this was where I felt
all those years of reading about closure
and all the state stuff and I was very
excited we made one optimization we
actually instead of having one giant
Adam we broke it up into one atom that
contain essentially pointers to other
atoms so that each of those processes
could just work on a single feet at a
time and it wouldn't run into a
situation where it was frequently front
update this massive data structure
conflicting with somebody else who is
also trying to update the data structure
at the same time so yeah demon working
on multiple things closure was awesome
for this it was it was really good
considered approached state really paid
off more than once I went in and looked
at a function and i added p to the
beginning of map and it just went ten
times faster and it was like okay that
then that's a good day so I didn't hate
that so problems are we good I mean can
we go home yeah there were problems
storing all the feeds in Redis memory
was super expensive it's just I mean
like we have these enormous Redis
machine sitting in AWS and the kind of
machines that have 50 gigs of RAM in AWS
or not particularly cheap feed builds
are only mostly fast so you can't really
rely on them for onboarding they get
stuck behind story creation and
sometimes story creation can be really
slow because when a new story comes into
the system we have to go and load all of
the feeds that our inner that might be
interested in that story from Redis and
it turns out that when you try to load
like a hundred thousand things from
Redis at once and like they're not
really small things it's just not super
super fast so it was hard to mean we did
rely on them and do rely on them for
onboarding but it was it's a cagey
proposition and not really something
that it was just going to get worse it's
also not at all robust if the demon
breaks or needs to like take a 60-second
GC nap it's just there's nothing you can
do we is having a couple times it got
closed there was this one hilarious
instance where we transition from line
12 line 2 and the the profiles changed
and the the JVM arguments that we were
using we're actually in the old style
profiles and forgot to get them into the
new style profile so we went from using
like 50 gigs of ram to like 20 or
something and it ran for a month and
then just started garbage collecting
like a maniac me had no idea was going
on for a day that
not awesome we also had this weird
denormalization of data that didn't seem
totally necessary we already have all of
the interests they already exist in my
sequel databases the main my sequel
databases that serve the site and we're
like D normalizing them into these Retta
systems I mean that's a way to make
things fast and it wasn't bad but it was
it was I mean not super ideal also not
very extensible so I mean it kind of
worked for this but how do we add new
things to somebody's feed even though
they aren't following them you know we
we look at your facebook profile and we
say oh I see your interest in horses you
haven't specifically follow this person
listing a horse for sale but I think you
might be interested in it there's just
no mechanism to do that and thinking
about sticking some sort of online
machine learning thing in the middle of
this this pipeline it wasn't super
appealing and points to a larger problem
how do we make this processing smarter
and smarter and smarter this just it's
all bad it's all in the the hot path and
it just doesn't really work out tons
more parallelism possible in the system
interest sad story adds feed builds
they're all basically independent
processes so how do you get more
parallelism you break things up into
smaller chunks you take advantage of
more machines you build a system of
queues and workers so we build our own
build something on top of Redis with
like workers or rescue with workers
reading up rescue queues and then
sticking one other rescue cues I mean
sure yeah that sounds like fun but no
like I mean you'd need to be like half
insane and half a genius to build a
robust high quality system like that in
the middle of a you know startup
environment where you just need to be
shipping something we just need to get
something out so we can move on to the
next thing that is completely broken
with our site fortunately someone
already walked that route and did
exactly that and I so that's great
Nathan I don't know if he's here he was
talking at a thing in San Francisco last
night but we'll see has built a couple
of really impressive pieces of closure
software Casca log which is a Hadoop
abstraction that let's use like data log
on top of its pretty cool and a little
piece of software called storm so storm
is cool I mean I'm still in the like in
love phase with it and I haven't we
haven't logged a ton of production
experience so talk to me in six months
and maybe I'll be cursing it in the same
way that I curse the Mongo at the
beginning of this but it's it's a pretty
interesting piece of software and it
introduces an interesting abstraction on
top of parallel processing so here's the
basic idea you have spouts bolts streams
tuples serialization and deserialization
basically these sort of beige things are
computational units this the the
greenish black things are network
connections between those computational
units and there's serialization and
deserialization done on the the blobs of
data that you're throwing around tuples
are just arbitrary bags of data you can
access them positionally or by name and
you can store anything in them modulo
whatever serialization you're willing to
write if you can write a cry out
serialization krause a java
serialization library for your data type
and you can include it in the couple and
you can set it send it between notes all
sorts of things you can serialize and
deserialize and it's essentially
arbitrary and that's that's it it it's
actually kind of like the core unit of
data in this thing and it's that's it we
have these spouts spouts poll external
sources they emit tuples they
acknowledge successful processing it has
a hook essentially to tie into whatever
queuing system you're using and it
handles failure it enables reliability
it has an API that's specifically built
to allow you to build a reliable system
that guarantees message processing which
is something we didn't even like come
close to thinking about with the
original system it was it was priority
500
not that we didn't want people to see
things but if somebody doesn't see
something in their feed they probably
won't notice though if somebody doesn't
get a feed at all if a build feed
message fails that's less good so parts
of the system it is good to have
reliability form this enormous slightly
daunting block of code is a spout
definition in closure storm is a java
system but there's a DSL built on top of
it and this actually isn't as terrifying
as it looks the stuff kind of down at
the bottom it all looks like the same
kind of thing you do when you implement
a protocol with a type and it is
essentially the same thing you're
implementing Java methods I believe it
all blows up it you know d macro Rises
into a ray if I call but spouts can have
some state we have get a Redis up here
and then this next couple is the the
core the only thing you absolutely have
to implement and this one just reads
from rescue and it spits stuff off with
a random ID that we regenerate and that
idea is part of the reliability
mechanism if you provide an ID when when
you're emitting a spout it'll
essentially track the tuples that that
ID creates across the the across the
computational topology and then call
either a core fail depending on what
happens bolts provide raw processing
power they received tuples from streams
they can have arbitrary logic they can
query databases they can download the
internet whatever they want they can
carry state so you can do things like
build an aggregator that will wait for a
bunch of different tuples and you can
guarantee that it will be in memory and
and persisted in all that and then they
emit tuples back to streams adding
parallelism adding more than one of
these computational units is literally
adding colon P in the topology
definition and then providing a number I
want 15 of these running across the
cluster and so obviously it's somewhat
easy to to adjust
these are two bolt definitions and these
are a little cleaner than that spout
mostly because they don't have that that
on reliability stuff the really simple
bolt this is how we turn a feed into
Jason it just gets at uppal and then it
calls a serialized function that that
grabs the feet out of the tub and turns
it into Jason and emits it and actually
i left the emit thing you'll notice that
both of these ack the tupple that they
got and the reason they do that is to
tie into the system that allows the
spout to let to call app or fail the
second one is an example of a bolt with
some state it maintains a connection to
a ret estate abase so that it doesn't
have to do that every time it press it
tries to process something i'm going to
talk real quickly i don't even have the
transitions here with about
serialization mostly because
serialization is something you almost
don't need to worry about when you're
building your system when you're tuning
your system it is but it just works out
of the box for the most part it falls
back to java serialization which is dog
slow but i mean it is there so when
you're developing it's not a thing you
need to solve up front you can actually
solve your problem first and then worry
about the details later there's a ton of
power under the covers cryo is its own
ecosystem of stuff and it's it's pretty
cool so this is the topology we built we
have a Redis instance we have the spout
reading off it the spout emits to a
user's tupple and basically the user
tupple goes and finds all the users in
the system and then for each of those
users it emits a bolt or it emits a
couple so for each of those users that
emits a double right the double contains
the user ID and it contains the story we
have two other bolts these likes and
follows scores and they both subscribe
to the users couple they receive every
single double that comes out of that
they do a database lookup and they give
the story a score for that particular
user those scores are then aggregated in
this reducer
and so at the end of this this reducer
the score maintains a map with the it's
keyed on the ID in the story and it just
waits until it gets scores for each of
the the scores in the system and then it
omits the final tupple into this feed
builder so it's kind of weird i'm not
sure if anybody actually remembers back
to the other slide but the feed building
atoms inside atoms thing that we built
for the original system basically just
moved over to this directly it wasn't a
bad system it was just that when we
tried to put you know a huge number of
feeds in it it used tons of memory and
we needed to break it up across as many
systems as we wanted to really actually
make an infant infinitely scalable those
basically everything about that Adam and
Adam thing with its expire and it's it's
writer it all stayed the same oh oh I
think we write in line now because we
can and so it right side out your Redis
we just add parallelism as as needed and
we kind of sprinkle it on top and and we
go home one thing that I didn't really
talk about that enables this is that one
of the storms really cool features that
it allows you to define string groupings
so these essentially ensure that in this
particular case we have the score
kicking out tuples we use your ID story
and score and we put a stream grouping
on a user ID story on this stream that
means that all tuples with the same user
ID and story will go to the same in
memory the same memory space the same
bolt the same processing bolt which
means we can safely maintain state in a
single place and they'll all get it'll
get all the tuples that it cares about
not just some of them they won't be
distributed randomly which is cool and
it enabled this one of the cool things I
mean I just showed you this diagram but
this is this is translating that diagram
into closure code we have the spout at
the top we have the users bolt we have
the likes and follows bolts actually
have a seller follows
they didn't include their we have an
interest reducer and then we have this
thing that adds stuff to feeds like I
said adding parallelism is literally
just defining p and then you can see the
the various stream groupings defined
within the bold specs and I mean this
this is it this is the whole thing one
of the really nice things about this is
that a difficulty with closure is that
coming from an object-oriented program
in the world you're not really sure how
to organize your code it's super
flexible you have a lot of options for
code organization and you know the the
plethora of ways you could potentially
organize things leads to different
solutions leads to you know doing things
different ways in different services
this provides a really nice way for us
to think about the organization of the
code when you come into this code base
you look at this you look up the various
the various bolt implementations and you
can see directly how they're implemented
how that how this system actually works
super declarative totally rocks storm
also comes with this cool UI it shows
you the summer the topologies that are
active and then for each topology it'll
show you all sorts of stats about the
spouts about the bolts what the topology
been doing one neat thing is that once
you get a storm cluster up you can
deploy as many topologies on it as you
want so deploying you know 14 different
totally separate topologies is as simple
as deploying one I mean that's not at
all true but let's just let's just go
with it it's it's totally simple it but
it really is it is pretty neat and you
know not having a set up yet another
zookeeper to to do this is going to be
nice so awesome success we're gonna move
this along but wait we're still storing
all the feeds in Redis how do new feed
builds happen so it also has this storm
has this really cool piece of
functionality called dr pcs or
distributor remote procedural calls and
essentially it's a really simple thing
that gets tacked on to storm and it's a
little server it's a thrift server that
receives requests from anybody and
pushes tuples out through a
computational topology and then keeps
track of everything that's generated by
the topology there's a bunch of magic
bunch of coordination magic and then and
responses return the DRP server which
then returns a response to the original
client right so that's me so we took
this dr pc for this additional topology
we built another storm topology because
the recent actions thing in the build
feed is just another storm topology and
we actually tacked it onto the original
star topology they're all it's all just
apologies so when somebody comes into
the system and they build a new feed say
they're coming for the first time we go
look up recent actions we build their
feed we return a response to them but we
also on the side kind of stick it
shunted over to the original topology
and stick it in their feed so that the
next time they come into the system
they'll have a feed in Redis and they
won't have to go through this the RPC
server nonsense which is great okay so
this is really cool dr pc is it's just
storm primitives it's built on top of
storm it's pretty insane storm and if
you could look at implementation you
it's it's pretty cool there's a really
good blog post from Ben Howard I think
is his name I hope he's here because I
want to shake his hand so it plays nice
with a regular store in topology you can
connect the two together and they just
work I mean seriously though this is
like this is cool this is really cool
and when we got it working i was just
like her murdered because it was really
it worked so what does this mean this
means we can build feeds reliably so and
we can build them quickly so we don't
have to store all the feeds in Redis
anymore we can just store feeds for
active users in Redis and it lets us cut
our storage needs now by ninety-five
percent because ninety-five percent of
our users haven't been on in the past
like week right so cool we can shut down
these massive red eye we can build a
more reliable more scalable system and
we can easily extend this in the future
we can
add you know machine learning stuff in
additional scores and it's pretty clear
how we would make it better yeah it
works you know we have this in
production now it all works it's doing
the right thing it's really remarkably
cleaner than the old system and its it
was just it's one of those moments where
you step back from the system that you
just deployed and you're really happy
with it which is which is pleasant
closure rocks I mean it was really good
for this it allowed us to add easy
parallelism to even the original system
but also through you know storm the code
ended up really pretty clean it's it's
easy to come into the system and
understand kind of what's going on and
it's clear there's a clear path to
understanding the whole system there are
libraries available thanks to Java when
you need a library somebody's
implemented it in Java might not be
super idiomatic but it's there and it's
fast and capable you know it we were
reasonably happy with the performance we
knew we were doing horrible things so
can't complain and people build stuff
like storm I mean the stuff that's being
built in closure the the old
abstractions that are being dredged up
from you know the bowels of computer
science and and the new abstractions
that are being built on top of it it's
pretty cool the the power to build
domain-specific languages to build stuff
like this it's I think a lot of people
are going to be excited about that in
talks up here over the next couple days
but it I'll just put my plug into if
it's really neat mostly I mean
everything has some feelings mature
idiomatic library support is a little
wanting there is a lot of really
interesting work going on there the
closure works guys I don't know what
they're doing but they they have like 40
different libraries and they're all
pretty idiomatic they're getting to
mature and and so we use some of them
and we'll pretty happy with some of them
but you still do have to sometimes fix
bugs in the libraries that you try to
use which you know it's not the bad so
bad the JVM is totally the worst it's
it's horrible it's a pain in the ass but
you know everything else is even worse
so what are you gonna do laziness will
totally buy you at some point I'll of
laziness huge fan of it but you know
when you're working with a lot of
stateful databases and your
doing you're making a fair number of
calls that don't necessarily return a
value they're not pure functions it's
going to bite you it'll lead to some
frustration but whatever it's not that
big a deal best practices for code
organization polymorphism if they're in
their infancy we're getting there it's
just a place we need to go but it rocks
still my favorite language after a
significant project blood sweat tears
the number of times something just
worked was refreshing it storms a great
example of how closure can bring the
language to a domain seriously awesome
good times thank you thanks to rich
Hickey Nathan Mars the conjurer
organizers Rob's uber one of our
engineers who also happen to be the CTO
Kobe is for letting this happen and
we're hiring so you know come talk to me
I'm t bashing on Twitter drives the
coppice calm and that's it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>