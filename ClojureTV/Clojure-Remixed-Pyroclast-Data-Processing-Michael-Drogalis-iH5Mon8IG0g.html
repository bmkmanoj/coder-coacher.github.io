<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clojure Remixed: Pyroclast Data Processing - Michael Drogalis | Coder Coacher - Coaching Coders</title><meta content="Clojure Remixed: Pyroclast Data Processing - Michael Drogalis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Clojure Remixed: Pyroclast Data Processing - Michael Drogalis</b></h2><h5 class="post__date">2017-03-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iH5Mon8IG0g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everyone thanks for having me again
today's talk is titled closure remixed
and which we're gonna take a look at
something new my company is working on
named power classed we'll take a look at
what's going on here and specifically
what the relationship is between power
class and the closure program and
language itself I think you'll find to
have quite an interesting parallel so
it's it's good to be back I've been kind
of off the conference circuit for a
while doing being heads down doing a lot
of building and thinking and I'm really
happy to share what we've got today this
is the first time in ten months we're
gonna show off what we're doing here my
name is Mike regalis and you might
recognize me as the creator of the Onyx
platform and also the CEO of distributed
masonry so like I said today's talk is
about our next product power class and
to some degree how now we built some of
the particularly challenging pieces
enclosure but more around the fact that
power class has been designed to offer
closures core value propositions to
everyone in a very intentional way power
class the next logical step for onyx
we're almost at the end of the O 10
release for which we did a complete
rewrite of the internal streaming engine
as well as all the plugin interfaces to
set up what we're doing here so there's
a lot of upfront effort to get lonex in
shape to do something a lot more
commercially oriented and I definitely
won't say that onyx is feature complete
by any means we continue to work on it
receives lots of attention but I feel
like the core maintainer can start to
shift our attention from being strictly
an internal maintenance mode to maybe
building something on top of it that's a
little bit more targeted in a wider
audience
so we're evaluating what we're gonna do
next one of the obvious paths was well
why not make onyx properly available in
multiple programming language languages
beyond closure because you can use it
from other jvm languages but it's not
pretty and non-gm languages at this
point isn't they'll go for a whole bunch
of reasons and I think we could tackle
this we could do it in parallel it's a
it's a great goal and we should but I
also think that there's there's
opportunity to do something
significantly bigger because onyx from
the outset was designed to have a unique
level of flexibility in both from the
way we model our distribute data
processing programs as well as its
runtime execution and I think if you
exploit that properly you can really
push the envelope for the kinds of
systems that we're building and maybe
usher in a new gender
and distributed systems so to that end
we've opted to build something new
named pyro class which is built directly
on top of onyx that exposes closures
core value propositions to everyone not
just lispers this time but to any
developer who's doing stream processing
and this time it's a closed source
effort but my objective today is to show
what we're doing at the stream
processing level and how I genuinely
believe this would not be possible
without closure at least given the
constraints in terms of time and money
for our team so today's talk is gonna
have three parts we're gonna look at
what Park last is to give you some
context as quick as I can and then we're
going to look at how power class is
mimicking closure in that very
particular way and then I'm going to
talk about in depth about how we're
actually able to do this before I jump
into what power class does I'll frame it
in terms of what you do if we look at
your architecture for a moment you have
at least one database probably more
these days a set of applications some
API is and maybe your subscribing to
event streams yourselves and these
entities as they run are either
producing or receiving data as a
byproduct that will call event data it's
it's generating data about things that
happened and a really common goal is to
take that event data and apply a set of
functional transformations filters and
reductions over it into a data
processing pipeline if you will with the
goal of getting it into a pristine shape
or form that is usable either by those
original generating applications or
other downstream applications so that
you have this sort of self-sustaining
architecture where it's running and it's
learning from itself and you can build
all kinds of systems like this and the
idea was that we wanted to pick a strong
candidate use case for onyx and make a
proper platform as-a-service around it
because there are some concerns that you
really are should not properly scope at
the distributed streaming level and need
to be handled at the application level
but if you're doing a platform as a
service you have a little bit more
leeway to do a more holistic approach
and so the idea was we wanted to do this
a data process just a data pipeline
esque
application or sort of a CQRS
application and do it really well out of
the box and make it really simple and
like I said there are things no matter
what you pick onyx spark blank it
doesn't matter there's going to be
really difficult territory and getting
this right it's not impossible but it's
probably more work than you think it is
if you haven't done it before
I mean step one getting your data off of
your your source of emission and getting
it into your data pipeline can be tricky
because you may maybe you have to be
doing some back pressure because your
data pipeline is overloaded or it's
transiently offline for a couple seconds
and so your tools need to be able to
checkpoint that data locally and ensure
that it's eventually going to get into
the data pipeline you can't do that you
can't get the right answer in terms of
doing reductions and aggregates so step
one is actually even you know somewhat
difficult as far as being able to tune
performance even if you're really not
doing high-performance event streaming
you may run into a situations where you
have disaster and you need to do a
backfill now I'll maybe need back 400
million events in a day and that's the
worst time to find out that you really
don't know how to tune your system
because then you have two problems I
think the most difficult piece though is
recovering state during either a code
redeployment or failure traditionally
now we're just replaying the entire
event stream which it's fine but it can
either be really expensive it can be
impossible if you can't recover the
original event stream there's a whole
bunch of reasons why it is worth trying
to figure out a strategy for having less
downtime at least in terms of you know
your ability to catch up on the state
but maybe you can't afford to have steal
state as you're rebuilding your your
materialization of those aggregates and
then finally there's just the whole idea
of your monitoring infrastructure and
having good DevOps because this is a
distributed system and there's a lot of
harder knowledge to be able to keep one
of these things up and so pyroclastic I
was just going to put it on one slide to
sum up you know kind of as quickly as I
can it's a fully hosted cloud platform
for building real time event streaming
applications and the idea is that you or
we are able to ingest your event data
into our our permanent storage and will
stream it through data pipelines which
you have defined and will run for you
and then we'll push that data back to
you if you did a bunch of functional
transformations over that will push it
back into your stack and also will
expose micro-service endpoints that you
can get at your reduced state as it
streams and the idea behind having a
micro service instead of talking
directed of storage is that you won't
have visibility if we need to do a
transition of how we're doing it or if
we need to be offline for just a little
bit we can make sure that it's at least
steal within a particular
I'm out of time and what you get when
you do this is streaming as a service
and this is really exciting because now
it's very easy to do things like data
cleaning you can put power class at the
front of your architecture and then have
a set of services downstream and you can
ensure that you're going to get access
to that pristine shape of data and it
will kind of be by contract correct
that's good for enrichment so taking
your data and changing it particular
ways to add or adorn it high precision
continuous queries because underneath
power class is actually a CQRS
architecture so we are streaming your
data over queries immediately as it
enters the system which means it has the
property of having constant time read
access and that's really nice if you
have to offer that as an SLA in your
architecture and as well just you know
things of use onyx for it's good for
analytics and doing it that series
aggregation so that's more mundane
things and and just before I jump in
there like why just to make sure it's
totally clear because we're gonna look
at some stuff in a minute it's easy your
your application on the left it has a
set of language bindings and you are
pushing into our cloud everything on the
right is essentially a black box and
we're just flowing into a data pipeline
and then you can access micro services
through in an endpoint that we're gonna
offer per deployment and you'll be able
to pull that back in your stack so it's
meant to be really easy and that's
probably enough context for what it is
to move on to the story behind the
relationship between power class and
closure and I would say that there are
four pillars that are connecting these
two things together a really good sense
of interactivity a strong set of
functions out-of-the-box and models for
dealing with state and time these last
two in particular are really important
for reducing the amount of complexity
and if you can replicate those ideas
that we have the language level and then
turn them into something that is a
contract oriented at the platform level
you can make a lot of people's lives
easier and so these these four things
have analogs they exist in closure and
we have built things to sort of mirror
them in the streaming domain and I
talked to discuss these four concepts
and I did slides and I went through my
talk and it was two and a half hours so
we're just gonna talk about
interactivity this time so want to know
about how these other things work I've
got the material and I'm ready so
interactivity it it sounds simple
closure has a ripple we use the repple
and a story right
maybe not it's worth thinking about why
do we really like it it's easy to expose
how something works under the contract
that I give you a input a you're gonna
give me output B it is a throw away
environment for trial and error unless
you're doing something that's side
affecting on the world you feel isolated
like you have this sandbox where you're
kind of containing everything and it's
it's what makes corridor so close its
pleasant to use but I think a
non-obvious critical property is that it
has really fast turnaround time because
the time it takes to receive your output
is no longer than the time it takes to
actually invoke the function
I mean it's knocking Alex Miller this
morning we're just talking about the
java way back when it was like I I can't
write another static void main or
whatever it is and to test this it's
just gonna drive me insane
and then closure came around it was ago
this is this is fantastic so the idea
that we're able to experiment really
fast is is a critical property if you
look at event streaming platforms and
these kinds of distributed data
processing systems the ripples don't
really exist I mean spark has a ripple
but it's not in the vein that I'm trying
to to get at because I believe that the
hardest problem with eventually
processing is actually understand what's
going on as it runs because in a normal
a normal local program you have a call
stack where you call f of X then you
eventually get a return value and it's
kind of just moving up and up and down
the stack with event streaming we model
things traditionally as directed acyclic
graphs so you have a value that's sort
of gonna copy and split itself and it
may go into virgin directions and if
there's a problem at some lower branch
in dag it's actually quite cognitively
difficult to figure out what went wrong
upstream to to sort of fix that behavior
and I think what exacerbates this is
that the devtest deploy loop is so
crushingly slow for most of these
systems because they require a full äôt
compilation to be able to patch your
program as they run I mean there's
there's sort of techniques for getting
around this but it's just a fundamental
problem of the way these things are
architected and this is why onix's
deployment model is the way it is I get
asked that like why why not do it
exactly how SPARC and flank and storm do
it and the reason is that we need to cut
down this time because if you're for you
you're under that approach doing a
Newbridge R is like at least a minute
for a non-trivial project I think it's
two minutes for our
and if you're gonna re upload to the
cloud and go from your laptop to like
ec2 or something that's it's 2 or 3
minutes and by that point I'm on Twitter
I'm doing something stupid I come back
in 10 minutes and then I forgot what I
was doing in the first place
and that happens to everyone and so
we're trying to cut down the feedback
loop
from minutes into milliseconds and I
think a secondary but equally important
goal is to enable deep introspection and
by that I mean I give you input a you
give me output e if something went wrong
in the middle what would be C and D up
to so make that a little bit more clear
if I have function f and I pass in value
6 and I get 12 and I didn't expect 12
and then I go looking at the function
definition of F and I see it's G H of X
G H of I rather maybe I peeled apart and
I realized that H of G of X actually got
the wrong value notably I of H of G of X
doesn't even matter at that point
because they got a value that really it
wasn't supposed to so the rest of that
is essentially useless to me what I need
to do is unfold the stack and we have
tools like that for like light table and
now cider is doing that but it's it's a
little bit more nuanced when you're
working with a dag just visualize that
in case I lost you because it's a really
fundamental point if I have f of X and I
get X star what I want access to are the
horizontal stripes I want to see how X
match the X star and Y to y star so
that's if the call stack is going down
and the return values are coming back up
I need that linearization of matching
values so pyrrol class is trying to make
this better and we're trying to provide
a solution here this is really one of
our core tenants that's that's supposed
to reflect it delivers on the promise of
list that we have interactivity
everywhere and we call our repple the
simulator what we wanted to do was come
up with a method of assembling data
pipelines very rapidly and the ability
to find errors as you author them not
after you deploy not after you test it
that doesn't make any sense you know
yeah anyway so when we're deciding where
we're gonna actually run this we talk
about Emacs we talked about maybe
running this in IntelliJ but really the
browser wins when you need to do rich
data visualization and so this is this
is kind of the place to be and like I
said the the goal is to understand what
happens if a specific piece of data
enters my pipeline what are the
repercussions to that and the
stimulators goal is to be able to take a
pipeline and then attach some of your
sample data to it we're not every action
that you take we're gonna that changes
the pipeline we're gonna recompile your
pipeline and then run your sample data
through it and display the results with
deep introspection with very high
fidelity and by that I mean if it says
it's gonna do this in the simulator it
better do that on the distributed
cluster at runtime achieving that parity
is really hard but you need to have
accuracy so that I mean if you don't
believe it then like it has no point at
all and the effect is that power class
is actually maintaining a shadow
distributed systems program that is
recompiling and re executing on every
keystroke and that's a big claim but I'm
gonna show it now and hopefully my demo
works this is a pretty involved demo and
I am running the full stack locally cuz
I don't trust the Wi-Fi I had to figure
out how to move Windows yeah there we go
oh I can't see on my laptop now this is
gonna be really hard does anyone know
how to mirror it preferences where's my
cursor oh yeah thank you
yes you good good way out where my
window go oh there we go okay great now
it's huge thank you okay so this is
pirate glass when you log in you're able
to create topics which are places you're
able to push data to very much akin to a
cop a Kafka topic and you can create
services which are constructions around
topics where you're able to build a
pipeline and then access those those
micro services that I discussed and so
we record in your service the demo okay
service and what you see is is a two
layered approach on the Left you're able
to craft your data pipeline and set its
configuration and on the right is the
simulator and we're gonna show how that
works in a minute through the middle is
a visualization of the data pipeline and
actually how your data flows through so
the first thing at the head of every
data pipeline it's a topic to read some
messages from just because we're sort of
messing around here I don't have to pick
a topic it's pretty pretty laxed about
that and you can choose a functional
transformation in an aggregate or or a
filter and you'll notice that these
functions should look very familiar and
that's because closure has a wonderful
API and it works so let's not throw it
away we've added some more functions
from Java as well as some of our own
just to be able to cover most cases as
well as arbitrary JavaScript and so just
to get started we're into the
frequency's function and I'm gonna say
that we want to run frequencies across
the message key the simulator needs to
have a wide variety of ways of
interacting with it to target more
specific and more coarse-grained
scenarios so if I don't know what the
frequency is key does I have a doc
string here it says returns an app with
a distinct keys and target array to the
number of times they appear very similar
to closure and the idea is that I can
say I can give it any record it's just
gonna work with JSON I can say that
message is my target key I could say
hello world and as I type it's gonna go
ahead and update for me and what's nice
is that I can explore like does it work
with capitals and doesn't
if they are capitals it does it's case
sensitive how about white space it's
pretty cool
symbols spaces more symbols this is a
really nice way to explore now I left
this destination key empty and that's
because by default I'm just gonna
overwrite the key as I apply it in place
but I could just as easily say put it in
this output key and so I retain the
original message and now I've done the
structural modification which will let
you sort of move your date a little
better one interesting thing here is
that we're we're mindful that a lot of
data is nested now and you may have to
move things up and down hierarchies so
we implicitly support get in and
associate everywhere so if I did double
bracket and then I did X it's actually
just going to go ahead and understand
that and be able to put it down into the
specific nesting level so that is the
most targeted way to work with simulator
but I wanted to I wanted to have that
deep introspection if I know all these
functions what I'm doing I have a
purpose I have a goal I'm gonna go ahead
and try to maybe apply some kind of a
transformation to a piece of data so if
I'm working with something that says
message is closure or West Portland 2017
now the the pipeline is highlighting
blue meaning that data is flowing
through it successfully if we had a
filter that was like removing records
that didn't match the predicate the line
would halt and not be blue so it's a
visual indicator of what's going on and
I'm just gonna tear it apart I'm gonna
maybe split by whitespace and I'm gonna
take the message key and you'll see it
splits it into a vector of three
elements and if I put these together
they become one bug no bug bug in a bug
kind of cool now continue so I now I
have a vector of three elements but I
don't really like this being positional
what I would really like is a zip map
over a message and I would like to zip
it against the keys conference city and
here and so now I have those as key
values and there's flow to the message
key they're one level down and I don't
really like that I want to move them to
the top and so again structural
manipulation is really important because
we don't overdo
it's now I'm gonna sing in a move
message and it's gonna move them right
to the top by by default this is a
little bit of a divergence from get in
we needed a way to specify what it means
to go to the very top of the map so
double bracket is how you do that but if
I wanted to say nested under X it's
gonna know how to put it under that
particular key and I think nest
arbitrarily or go into vectors it's a
good good way of doing positional
transformations and I guess just to
finish it off I'm gonna do something
kind of useful I want to turn that last
value in an integer so I could say
here's my cast map and say that year is
most definitely an integer and I get the
right answer I can fold these things up
so it's really easy to understand and
look at it maybe I want to do conch was
in condos in Austin last year yeah 2016
and it's gonna have it go ahead and
change them I had these folded up but
really these are all changing as we go
and so this is this is what I'm after
this is what we want to be able to do
and change how we perceive a venge
stream processing now this is pretty
cool but considering one piece of data
at a time is is limiting when we work
with larger amounts of data we we work
with reductions in aggregates and we
want to understand how we're able to
compound pieces of data into smaller
smaller chunks so I'm gonna do is I'm
gonna remove all these tasks and I'm
gonna do something else please remove
this and we're gonna get some sample
data from I just made up this file this
is a set of players who are playing a
game they have a name they have an event
time when an action happened they scored
points a number of points from a
particular position and what we want to
do is an analysis over over this data
set so I can do a switch the simulator
mode from text mode to file mode and
drag my sample data file drop it here it
says I've loaded up a hundred ninety
eight records cool gonna go now I'm
gonna start doing some stuff I'm just
gonna do one I yet so I could keep my
demo moving along we give it a name
because we were able to query this
reduction through an HTTP endpoint so
I'm going to
some points and show pants how many
people are familiar with Google data
flow and the windowing abstractions not
too many so I'm gonna teach you now with
what I believe is the only interactive
method of looking at a data flow
visualization so there's multiple ways
to do an aggregate we're gonna start
with the simplest one which is a global
aggregate which means give me all data
and we'll do a sum over all the points
and that says cool I got nine thousand
nine hundred twenty-four points in your
data set pretty simple just ran all the
all the events through did a quick sum
and to be cleared so this is a helpful
way of understanding how your program is
gonna behave when we deploy it when you
deploy we don't actually get access to
this you you get access to a micro
service which gives you the backing raw
data but this is a handy cue for free
key for figuring out what your programs
gonna do but if it stopped here it
wouldn't really be all that interesting
because I mean this is Excel right just
do it some but maybe a little bit more
things we're gonna group by key I want
to know how many points each player
scored at so I could say group by name
and this will understand what that means
and go ahead and switch it to a bar
chart that text is very small we are
still figuring out how to do that for a
good interface design but now we have
Ron Gregg Cyndi Sally and Fred all
playing the game and they have a
particular number of points each and so
it showed me what the actual backing
data is as a visualization because
that's a lot easier to understand than
just the raw text so still that's that's
not terribly interesting because that is
taking all the data and then applying
one function to it the really powerful
kind of analytics are able to tap into
the event timestamps of each piece of
data and then logically group together
pieces of data that occurred around the
same time so we're gonna change this to
a fixed window I'm still gonna sell all
the points and Alice might ask me for a
window key which says give me the key in
your map that represents the time that
this event occurred so that was in my
data set event time and the last thing
I'm gonna pick is the range range is a
unit of measurement per time that's
gonna sort of tumble forward so if I say
I want to do a range of three minutes
what's actually going to happen is that
I'll get a timeline here with these blue
boxes and if i zoom in you'll notice
that these boxes are exactly three
minutes long so every piece of data will
fit into a three minute interval and
this is very different from a sequel
like query we're just going to group by
a certain number of minutes this is
actually able to backfill at run time
which three minutes slot is filled in
either forwards or backwards from the
UNIX epoch 1970 so it's able to check
that with constant time what you'll also
notice there's two things this white
space means no data has occurred here so
in zooming in my bar charts just totally
empty and as I'm zooming out these bars
are actually changing because it's only
showing what's within my particular
range of view so again if i zoom in i'll
see that text is terrible these these
time bounds are exactly three minutes
apart so it's saying within this three
minute interval this number of points
were scored and automatically were able
to slice by time more interesting way
again we'll group by by key I don't do
it by player because now I want to know
what happened on a player per player
basis per three-minute interval and I
get a split timeline for each of my
players and again I have these boxes
which are more dispersed now because
each player was sort of folded into one
timeline earlier and as i zoom in again
i'm gonna get get these graphs changing
these timestamps now are reflective of
three minute ranges but they're showing
each player so now I have a quantitative
analysis per category that I group by so
this is pretty powerful right out of the
box the last thing I want to demo is is
something called session windows and
these are the most complex to understand
which makes having a data visualization
really helpful and the idea behind
sessions is that we're gonna pick we're
gonna pick a session key which is going
to denote whose session it belongs to
and the idea of a session is somewhat
like a streak where a group of data
happen and we set a particular timeout
where if data happened within that
timeout we extend the session so you
could understand that a session is maybe
a logged in period for a user because we
don't we don't have explicit login and
logout so anymore for mobile
applications so if we receiving event
data we can really only tell when a
session of you playing the game was when
we received events
so this is one method for doing that
analysis so again if I say my session
key is name and I give it a timeout gap
of 20 minutes okay that same timeline
but now these blue boxes are not of
homogenous shape you'll notice that
cindy has a longer blue block box here
between about 14 37 and 1550 I can
actually get the exact numbers if I
wanted to look right here but this
indicates that activity occurred here as
number events within a 20-minute time
boundary and on the white the space is
where I see white specifically in here
there was no data to be able to collapse
these two sessions together so
presumably if there's more than 20
minutes of inactivity that constitutes
two different sessions under which we're
gonna take the sum of their points so we
figure out how many points they scored
per session if I don't have you know any
sort of other indicator and this sort of
analytics is really difficult to pull
off because you you need to be able to
dynamically resize windows as you
receive data specifically from the past
because those windows are gonna merge as
you receive either old or new data the
last thing I can do is group by key now
instead of grouping by name because I've
already done that with the session key
what's very interesting is that I could
group by the position in which they
scored and get some really targeted
analytics so now I have these sessions
for which each of my players scored from
the center from the left in the right
side of the screen in 20 minutes
sessions and if I go ahead and extend
that timeout gap a little bit further
now these sessions are growing you see
that they've enlarged a little bit
because there's a longer period during
which they could have idle activity
before we collapse those sessions
together again if i zoom in now I have a
specific analysis of when Cindy was
scoring from the center between 1534 and
1636 and these are these aren't
estimates these are exact they're able
to grow and to the corresponding
timestamps in either direction I would
do another time there's one more thing I
want to show I'm not sure if it's gonna
work because I slept my laptop and like
I said I'm running the whole stack
locally as I said you you don't get
these graphs when you launch i've
already launched a service to pick up
slack messages from our room and so when
you launch you get something like this
these are collapsed it's saying i'm
doing an aggregation where i'm counting
is over a fixed interval and you get
this end point it's like it's a copy and
now I can do there we go let me do a
couple things here size that up turn
that into HTTP you - - awful or more
readability and so this is an example
what you get when you run a service you
can tap into this the actual backing
data and so this is this is a block in
your architecture this isn't tableau
where you have a nice visual front-end
this is something when you you put at
the front to receive your data and you
build services around and it's a nice
tool for making sure that your state's
gonna upgrade correctly that your
services be online and that your
analysis is gonna be correct if you give
us the right data and so that is all I
have from a demo so far let me talk a
little bit about how all of this is
actually working
cuz it's that's interesting I've lost
speaker notes now
now that's why I'm not really need
mouths okay like I said it's ten minutes
left okay see you probably questions out
how exactly the simulator works and I'm
try to address these in the leave time
for questions so I'm have to go a little
quick and assume some knowledge what is
happening is that this is actually an
entire closure script step closures back
at the bottom we're running everything
through kubernetes with the atomic
serving the runtime for the simulations
that you saw we have all the metadata
stored and atomic because that makes a
nice data contract to be able to move
around the stack this is obviously built
on onyx and we're using DynamoDB to
store windowing contents at the top of
the stack we're using ohm next untangled
and a web worker to be able to make this
move quickly as well as some local
caching but the secret is that a lot of
this code base is actually written into
clj see all the functions that you saw
me invoking have corresponding
implementations between closure and
closure script and it wasn't as simple
as just using them directly because
there are some minor differences and the
idea was to get isomorphic behavior
where if you get behavior in enclosure
script you should get that behavior in
the JVM now we can't do this always
because things like closure or Java
scripts type system make that really
hard with integers but that's not really
something worth chasing in in my opinion
this was a two-step process to be able
to make the simulator work and work
really quickly so if you notice it was
as I was typing there was very little
lag it was not talking to the network it
was not talking to the internet he was
staying directly in JavaScript the whole
time and to pull that off you need two
things number one you need a really
really fast compiler one that which
which we wrote in C ljc
and this is the thing that maintains the
shadow copy of your distribute systems
program that is gonna mirror your data
pipeline and keep parity what's
interesting about the compiler is that
it doesn't use any code generation at
all it's a data to data compiler so it's
more of a loose term than like a you
know a hardliner you know compiler sort
of design but what's what's important
about this is that if you use code
generation to pull this off it would it
wouldn't be fast enough because you'd
have to generate a verbose amount of
stuff and then sync it to the server and
to run it somewhere and that just
doesn't work so that that that part will
not port also it's hard to reason about
generating
I think everyone here knows this so if
you generate data structures you're
gonna have a heck of a lot of time under
easier under time understanding what it
is you have produced in the first place
like I said that the compilation target
is the data structure and that's useful
because it is amenable to incremental
compilation so as I type in those text
boxes we know which part of the pipeline
you have changed and can let's just
recompile the minimum amount to make it
work data structures are amenable to
compression so that's good for saving on
memory and there's no negative
repercussions of sharing the stuff they
are closure data structures that have
structural sharing and they work just as
we expect them to but a data structure
is not enough to pull us off because if
you have that all you have is a contract
for what it should do it will not
actually do that so you need an
execution environment and that is where
are our other half of this comes in when
you need to have a way of running this
very very quickly so we have an
environment that takes a data structure
that represents the data pipeline in its
data and it's using a snapshotting
process by walking through the life
cycles of its execution and denoting
when values have passed through it so
you get that deep introspection back and
this is happening on every single
keystroke so that's for react 16
milliseconds to beat the UI thread and
give a parent's apparent synchronicity
and if we start to go over that will
fall back to a web worker and it's it's
still about just as fast like I said
this never talks to the Internet this is
really the big secret to making this as
snappy and as smooth experience as
possible so all the compilation and the
runtime is staying in closure script and
the benefits are huge not not just to
the client but also to us as a company
because we have no server load when this
happens like if tomorrow we had a
billion users all we need to do is serve
serve the runtime out of cash I mean
grants say if they're just using the
simulator and not actually deploying the
stuff like because the simulator stays
in JavaScript so all we had to do is
serve the runtime from cache and we're
golden that has no server load for us
that's great I'll fly mode I mean we
need some polishing but the fundamental
technical challenge of getting this done
was trivial because it was just part of
the game this was what we wanted to do
is to be able to have synchro's behavior
in JavaScript maybe the most fascinating
piece of this is that your sample data
has never left the browser because your
code isn't ever loved
the browser and so you have data that
you are mindful about being sure where
it goes it will stay in your browser
when you use this approach and so that's
a nice property I have assumed many
things somewhat because I knew I would
run out of time early but if I was just
going to clarify things really quick the
key to all this is that Onix is really
two things it is a model for creating
distributed data processing programs and
it's also an execution environment for
those models I get asked a lot why would
you build a separate execution
environment for this that's a huge
undertaking why not take onix's data
model and then run it on spark or storm
or flink and the reason is that you
would have trouble building something
like this this was really always the
target you need something that's going
to be a medalist execution across
different host environments now the
models and onyx are actually 1% data
structures so those correspond to the
things that the compiler is working
across and over the last year we've
slowly been converting onyx to C ljc all
the parts that are they have nothing do
with networking that I'll have to do I
owe all of the things that get you the
right answer with onyx core are now on
CL SJC which has allowed us to produce
onyx local RT which stands for runtime
and that is a pure stateless
implementation of onyx that has very
heavy code sharing with onyx distributed
and that is one of the reasons that we
get really good parity because we're
using roughly the same code that has
been tuned that has been tested and so
local RT is for the most part of briefs
to maintain because the code sharing is
so heavy and so those are the two pieces
that have been able to compiler and the
runtime our architecture is compilers
and decompilers all the way down what we
saw was one representation for building
a pipeline that has a correspondent
compiler and a D compiler and what we're
going to do is take that representation
turn it into an onyx job and hit an
optimizer and if we wanted to we could
go in the reverse direction with the end
goal of having multiple representations
so if you don't like our in browser way
of building pipelines you can use our
HTTP API which has its own compiler and
decompiler we also have a sequel
representation now the magic is that you
have a lingua franca in the middle which
is onyx it has a data contract and so
you have equivalencies across each of
these things so if you want to switch
modalities and say
cool that that pipeline representation
the browser was nice I'd like to switch
to sequel we can just round trip from
one of these things to a nice job and
then decompile back and you're gonna get
the right answer because because the
compilation model has contracts about a
year and a half ago at the cons when I
was introducing closure I noted that
what we needed here was more compilers
to make this interesting and that is the
top half we have compilers that turn
pieces of data into an Onix job what I
didn't talk about was where we run them
so this is this is now a many-to-many
relationship where i have many kinds of
expression modalities and i also have
many ways of running them because i can
turn into an Onix job which will run
equivalently either on an onyx cluster
and full distributed mode or on onyx
local RT and so this is really the
ultimate if you can get here with your
infrastructure the the flexibility is
just enormous so I've been saying that
this has been ready or almost ready for
a long time now it's a big ambitious
system and I think we are almost there
at this point so if you're interested in
checking it out just hit me up we always
like beta customers I just wanted to
thank my team I know that I'm up here
and sometimes I get a lot of just kind
of the public face of the project of
onyx and now sort of power class but
these are the people who work extremely
hard to make this happen this was an
incredibly ambitious goal and I think
when you get a set of people together
who really believe in something and want
to work really hard you can do just
about anything and so these are the
people who really pulled it off that is
all I have</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>