<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mesomatic: The Cluster is a Library - Pierre Yves Ritschard | Coder Coacher - Coaching Coders</title><meta content="Mesomatic: The Cluster is a Library - Pierre Yves Ritschard - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mesomatic: The Cluster is a Library - Pierre Yves Ritschard</b></h2><h5 class="post__date">2015-07-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/X-fVA5DxezE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright hi everyone thanks for coming in
today my name is Jay I've I'm currently
the CTO and co-founder of exascale we're
a Swiss cloud hosting company i'm also
an open source developer I I built
pistols and cyanide which are both
closure demons one for providing object
storage and the other for providing
telemetry storage and I also participate
in women and collected quite a lot today
I'm here to talk about mathematic which
is a library something I don't do as
often as fulham applications the aim of
this talk is basically to get to
discourage you from building distributed
systems on your own presenting mesos and
presenting a simple way to interact with
it and I promise I won't dive into the
cap theorem because every other talk
talks about it and I might not be the
best person to to talk about this we're
very lucky in the community to have Cal
worth doing a very good job at
presenting the compromise that you have
to make when you're building distributed
systems that's not what what I'll talk
about I'll talk about how you get to
having a distributed system because it
shouldn't be a and end in and of itself
I'll give a brief introduction to messes
and what it provides as a foundation to
build systems that are composed of
moving parts and then I'll dive into
mathematic the library and how you build
things with it so let's start with how
you end up with the distributed system
because you always start with a simple
product with a simple IG let's say you
want to change the world by destroying
the disrupting the job board industry so
it's a nice ID
you you you think you have a great
product but technologically speaking
it's not so complicated it probably fits
into a standard three tier app which
talks to a sequel database right it
doesn't fall into what we usually call a
distributed system even though it has
different moving parts so you start with
a very simple infrastructure it all fits
on a single machine you have your
closure app which is an which exposes an
HTTP service uses embedded jetty to
provide this and talks to a sequel
database let's say postgres for instance
right and all goes file for a while and
then you hit the first signs of success
your single server doesn't fit your
needs any more you realize you have to
split functionality into several
machines and you take the simple option
to have a load balancer that will span
request on to several machines and then
have a replicated sequel database server
because you want a bit more confidence
in your data persistence layer you you
do realize that logging becomes a bit
harder you have to reach for a
centralized logging solution and you end
up with something like this which is
still fairly simple fits in just about
everyone's head and let you move on for
quite a while but as traction grows you
realize that you have to add features
the first one that comes to mind is
subscription emails and you can do that
synchronously because it would be too
too painful and too slow would increase
the latency of your of your requests and
architectural is speaking it's not
really sound so you decide to split that
up and do that asynchronously which
means that you add worker machines you
also have a queuing mechanism for this
request to get Q into and that's when
you start to
switch from cooling machines by name and
treating them as homogeneous clusters of
identical machines there is arabic there
begins to have a switch in your head and
I mean the infrastructure is still
somewhat simple but gets a bit more
complicated and then your seed money
runs out even though your your product
ice traction and and that's when you try
to reach for monetization techniques in
your product you decide to have a a pay
for option which gives you advanced
analytics for instance but this you have
to compute from different sources of
data some comes from your logs some come
from your database everybody told you
that you should be doing with Hadoop or
spark and and you have to run this batch
job somewhere right so you spin up new
machines and there's this other company
that you partner with which has a very
weird legacy system but they do have
interesting data that you want to
exchange the only client library to
interact with their system is PHP that
doesn't fit on your existing machines
which only have Java and it start to get
a bit complicated and and you start to
be at a point where hmm some various
machines that you that don't quite fit
in your head can break and break the
functionality of your system altogether
and as the product grows so does your
infrastructure you have Jenkins slaves
to into servers your continuous
integration pipeline you had to split
metrics and monitoring on two separate
machines because it's it's quite a heavy
job given the amount of machines that
you have now you need a control command
and control solution because you realize
that when you go onto mmm machines
directly to service comments you mess
things up from time to time and you
realize it's also time to introduce a
configuration management solution in
your infrastructure beat puppet chef or
or whatever and what it comes down to is
that
when you look at the time that you you
spend dealing with operations in your
infrastructure and and building your
product it's starting to feel like
you're an ops person so if we take this
take back and see the road to to here
and and what you're doing you're taking
all the right boxes you're you have
continuous integration you have a fairly
simple product in terms of
infrastructure and and you're doing what
the industry recommends you to do but
your resource utilization on machines is
very low altogether most of your
machines spent twenty percent on cpu and
don't consume all their ram but you
still have pics that you cannot
completely service without degradation
and you have regular contention for
instance there are pics on your web
service layer which doesn't impact all
the other machines and you also have
pics on your continuous integration
machines on your batch machines when you
run analytics when you add new service
or new components into your
infrastructure it gets pretty hard the
first thing that you have to do is make
allocation decisions which machines
should this go into should I spin up
should I spin up or buy new machines and
and it's it doesn't logically follow
that your most active git repository
should be your config management one you
should be spending most of your time
actually building your application when
the handling failure is quite hard
you're monitoring your monitoring system
hopefully it's room and tells you when
when things break that works quite well
but you still have to shuffle things
around spin up new machines or move
service all over the place to
accommodate for these failures but with
when you look at it from a service or
platform point of view it all makes
sense you have your product platforms
your website your API your your back-end
code
you have your data platform that
provides sequel your batch layer and
your full text search layer monitoring
has a clear and separate platforms as
well and your support infrastructure is
rather limited so so this makes sense
but what you what you'd really like is a
way to avoid having to make these
allocation decisions and have a finished
state of machines that can accommodate
for different allocation topologies when
pics on on separate platforms occur
right so you look into alternatives for
this containers are a thing they do
provide isolation which means that you
can run a separate software in isolation
on machines but you still have to do the
location and the shuffling around by
yourself private pass or actually public
pass is an option but it's it's a sort
of a sandbox and you have to fit the
constraints that pass give you this
might work this might not and then there
are cluster alligators such as yarn or
Apache missiles and today we'll talk
about the latter but most of this comes
down to the fact that it's not your job
you should get out of the business of
shuffling instances or processes around
and back to mostly building your
products so let's see how mrs. can can
help with that if we look at what mrs.
advertises it does it says that is it
abstracts CPU memory storage and other
computer resources away from machine may
there be physical or virtual and enable
fault tolerant and elastic distributed
systems to easily be built and run
effectively which sounds nice if we look
at it from an architecture point of view
it has three component masters slaves
and schedulers and frameworks sorry
and it relies on zookeeper for
coordination but that's just details if
we look at how it works under the box
masters are here to coordinate slaves
expose resources and workloads which are
called frameworks come and sit on top of
it messes masters they expose an HTTP
and prob f API a web UI as well to
assert the state of the system and
they're the main entry point for
frameworks which provide workloads what
they mostly do is they gather and the
maintain slave ability availability and
capacity information and the present
frame works with resources that may they
may or may not take up um take upon
their highly available and horizontally
scalable which is necessary mrs. slave
they're used to launch actual payload or
workloads tasks in methods are isolated
by way of namespaces or ducker and they
expose their resources to masters and
resources maybe CPU RAM I please volumes
available ports and or arbitrary
attributes which are key value pairs or
laborers which which can be arbitrary
tags on top of this sit mesos frameworks
which is software which just instruments
a workload on a cluster it's it's
composed of two components schedulers
and executors schedulers the interact
they come and talk to the meso smashed
ER their receive offers from the message
masters and then they they receive also
task statuses they may launch task by
picking up on provided offers and
they're responsible for exposing
services if you need services to be
exposed
and then you have missed those executors
which are responsible for managing
workloads on me so slaves they report
the the status of their tasks best back
to masters which will then hand off that
information to schedulers and they're
optional mrs. slaves are able to launch
named spaced comments or docker
containers directly right just a quick a
quick word about names faced comments
when I talk about this I refer to the
sea groups functionality in Linux which
is a way to isolate processes you can
think of it as much more powerful chroot
it's the basis on which LXE has been
built and then it's also the basis on
which darker has been built and again
that's the the overall architecture
picture where scheduler come and talk to
masters they deploy executors onto
slaves which themselves will start tasks
let's come back a bit on the offer base
allocation model that meses provides a
scheduler will receive a list of offers
which say I have this and that amount of
ram cpu available ports or this and that
and decides on which it wants to pick up
and launch tasks on this puts a lot of
responsibility on the schedule because
it means that you have to be the one
making the decision of I'll start my
workload there or there the resource
offers that you get are decided by mesos
itself the I'll go through the
algorithms that missiles chooses which
scheduler to offer resources to is
pluggable but quite effective by default
those tasks again just name spaced
comments isolated or and and they did
you express their needs in terms of
capacity so a message task that should
be run says I will consume at most that
amount of RAM and at most that amount of
CPU I also need this and that available
tcp ports for instance and there are
again optional things that you mrs. task
can request it can provide a health
check to assert the fact that it's
correctly running it may ask for port
forwards to be done and also and this
makes me an interesting project it may
ask for persistence storage volumes and
and it's one of the rare projects that
tackles the complex persistence story
when you're dealing with clusters if we
look at some of the frameworks that that
have been built and work on mesas apache
aura and marathon are both frameworks
that could be seen as a private pass
type of project they allow you to spin
up docker containers or or comments and
quickly i scale them to additional
capacity or additional containers spend
several slaves and they're quite
effective in that regard Kronos is a bit
of a distributed cron of sorts it allows
you to scheduled tasks to be run Jenkins
has a plug-in that can be run on top of
mesos Hadoop hibachi kefka in Cassandra
also are available as mesos frameworks
what I'll talk about now is how you
build an actual framework and how ms o
matic can help in doing that mathematic
is a couple of artifacts a plain library
which is a feather on top of missiles
and there's a an optional
Cora async facet 448 as well why you
live very well first things is it's a
closure it's a closure project so
libraries first existing frameworks
might not exactly need you meteor
constraints and if you're building any
type of as a service product meses is a
great foundation for that and having the
ability to to service spin up and down
workloads from your application is
something that you might be interested
in and this was actually our need and
why we got into a building mathematic I
also I also think that many frameworks
even though they want to embed
functionality within the application
will have a common set of needs and Miss
o matic tries to accommodate for that as
well so if you look at the components
that ms o matic provides the first thing
is idiomatic closure types for the data
structure that mess this exposes a facet
for creating secures a facade for
creating schedulers a caressing version
of these facades and an allocation
helper to help you make these allocation
decisions because that's something
that's pretty common as a need for the
type conversion it's all sits in a
single namespace it's basically only
free functions there are records for
every 4d which are equivalents of every
mesos probe of types and you can use
probe of to data to pro labeouf or data
to protobuf to convert from one or the
other converting from probe of data is
really simple you use product of two
data and then you either can work with
plain maps using the to prod above
function and you'll have to give a hint
as to what structure you want to convert
to
or if you are coming from a record data
to Prada bath will be sufficient both
the executor and scheduler facades are
protocol-based your client code will
have to verify a protocol to input
message so basically to receive
callbacks and it will be able to perform
side effects through a driver which is
also implemented as a protocol this is
what the executor facade looks like i'll
i'll talk about the most important
functions if we look at input launch
task means that you were instructed to
start a task and framework message means
that you received a message from your
scheduler containing arbitrary data as
far as what you're able to do
interacting with messes as an executor
you're able to send messages let's say
you were building something like a
crawler the output of the crawl that
would be a task we would probably send
back to the scheduler for persistence
for instance and you're also able to
send status updates for the tasks that
you're currently running and there
you're a bit more limited into the with
regard to the payload that you're able
to send the scheduler facade is a bit
more involved again this is what the
input looks like that these are the
types of messages that you will be able
to receive and this is the functions
that you can call when interacting with
the framework as far as there are as
input is concerned the most important
ones are resource offers that's a
message a message that you receive when
there are new resources available when
you receive this you receive the full
topology of resources offer rescinded
means that there is an offer that was
taken back problem
used by another scheduler framework
message is when your executor sent you
something and status update is just
status at least four tasks that you ask
to be run the most important the two
most important functions when you're
dealing with workloads or lunch tasks
and cal task lunch task picks up on an
offer and says please run this reconcile
task is used when you're building highly
available frameworks it allows you to
get back the fullness of statuses when
you start over and when you need more
resources you can also ask the message
caster to give you more if if it's
available so what it comes down to is
for most of workload generations is you
receive offers you launched asks if we
look at what an offer looks like that's
what you receive when you're using
mathematic offers will come from a host
name the actual host name that the the
meso slave process runs on it will have
a list of of maps a vector of maps which
represents available resources and may
have arbitrary attributes in that case
there are none at ask for a name Space
Command has a name and an ID that should
be unique a list of resources that it
will consume and the associated common
that it wants to run if you if you'd
rather start docker containers that's
how it's going to look its weight quite
similar you also expose the resources
that you will consume you may ask for
port mappings and you just described the
docker image that you want to to run as
far as the caressing facets are
concerned it's an option dependency
again it's a separate artifact
and all it does is produce our incoming
messages from mesos on to a channel that
you can consume each each and every
message will come on in a map and the
map will have a type key which describes
the type of message that comes from from
mesos which means that a very bare-bones
scheduler could look like this you
you'll be consuming our messages i'm
using the reduce function in in
caressing and dispatching to handle
message message on the on the channel n
domicile message message can just then
dispatch on the type key and you'll be
able to quickly add support for any type
of message that you're interested in
dealing with and that's really all it
comes down to the last part that's
exposed by mathematic is the allocation
helper it's it's also protocol-based if
you wish to plug your own allocation
method it and it's a simple it's purely
functional in the sense that it doesn't
actually perform anything on mesos it
just gives you a tentative topology for
the tasks that you want to run there's a
single one for now that's a pure closure
sorry with no added dependency it does
account for a number of instances that
you want to spin up of a single task and
colocation factors which when you want
to say for instance which is quite
popular that you may want six processes
to run but only have a maximum of two on
a single slave if you want to account
for failures and this is expressed as
two additional fields on the task
description and Mexico location key and
the account key for the process
and that's all the library provides well
I'll spend some time now is is looking
at how you can easily build with a small
amount of code frameworks on top of
messes with mathematic let's say we want
to build something to run our regular
batch jobs we want something a bit like
cron but that accounts for failures so
which will launch tasks on arbitrary
machines right you have two jobs and you
decided that you wanted a static
configuration enclosure the first one is
your analytics job I should run every 20
minutes and the second one is your usage
metering job because you're doing a sum
some sort of as a service type product
and should run every 30 minutes both
consume half the cpu core and requests
512 megs of ram this state that you'll
need for this for this grunt to run is
just the dr de meses driver to interact
with launch tasks on and knowing which
which task are currently running so that
you do not launch concurrent processes
for instance if your if your metering
job takes longer than 30 minutes you'll
decide to skip an iteration for ticking
we will rely on Cronje which is does a
very good job at friday providing a
facade for a JVM scheduler and will send
a message on every tick on it on the
same channel that Miso's sends messages
on Cronje accepts a simple map for
configuration and we'll start with the
same structure that i displayed will
input will input every message on the
same channel and then we'll dispatch
with a handle message function and
that's what you end up with the ticker
produces payloads on the channel mesos
produces payloads on the channel and
then the channel also interacts with
with mesos by way of handle message to
end all resource offers you can Asajj
the offers that you got directly on your
state that's complete being regularly
updated when you reduce function to end
all tix that are sent by the ticker
you'll look if you have a job that's
currently running if not you can use the
allocate function that's provided by my
somatic to allow to find a suitable host
to run your task on and then just
launched the task and that's it as far
as a bleeding status information is
concerned when you when you launch a
task you'll get a task running message
batch from messes and you can update the
euro set of running tasks accordingly
again when the the task will have
terminated you'll get another message
and you can update the state accordingly
and that's that's just about it to build
a distributed cron what I wanted to
display here is that by having a very
small amount of code you can start into
useful things i'll show how we can go a
bit further than this because this is
rather simple and this could have been
done in a lower level language closure
gives us the nice expressivity and the
nice properties that were interested in
but this could have still have been done
in something like c++ quite simply the
library first approach is something that
i think was really interesting as i was
telling earlier and and we have plenty
of facilities to help us make better
decisions when the frameworks or
workloads that we want to run our a bit
more complex
let's say that we want to ensure that a
specific topology exists on our cluster
which is exactly where you want to be to
avoid having to shuffle tasks around
when they fail if we look at these three
simple things that are exposed by
closure the fact that atoms are
observable having closure data diff and
close your core match we can build a
very simple system to ensure this let's
say that we have our configuration
that's stored in an ad in an atom we
have an arbitrary watch namespace that
watches over configs maybe on the file
system and updates your configuration
Adam what we can do is add a watch on
that Adam for every change and it will
forward to a function of the reference
the old state and the new states a
simple implementation of that function
could just be a closure over a total
over the input channel called a decision
function on the old and new states which
will decide any side effect that should
be produced and then perform these side
effects decisions for decisions we can
easily leverage Claude read a diff which
will give you what was what was in a map
before what was in after and what
changed here I'm just looking at keys
that changed and then forwarding that to
a changed map function for every key and
this is the last the last bit changed
map for when given in an old and new
and newquay will assert the runtime and
status of both the old and new units and
try and make an informed decision and
this is where what I wanted to display
with these simple libraries that closure
exposes you're able to lay out your
decision process in a very simple manner
here if if the if the status changes
from stop to start the side effect that
I'm producing will just be to start a
new task and one of the most complex in
a sense one is when a runtime change
when you change from a an isolated
namespace come into a docker container
I'll just produce two side effects the
fact that I want to stop the old one and
start the new one and by by watching the
by maintaining the status states that
messes provides you with your expected
topology in an atom and constantly
checking this over in your reduced
function over a channel your you're able
to assert that you're wanted topology is
always produced or at least you're worth
trying to converge to your expected
topology in your infrastructure and
that's what it looks like you end up
having probably exposing an HTTP API to
have simple clients know the state of
the system watching over file system
changes to produce an atom configuration
run that for your decision process and
then produce side effects on a channel
which will interact with mesos which
itself will give you back if your if you
convert to the correct topology are not
so to wrap things up closure does help
dealing with the infrastructure in that
case caressing channel they help model
the flow of events in an infrastructure
very well
immutability is nice because if you look
at the actual structure that was built
here the only mutable piece of data that
you have is your configuration and I
think that core match even if I didn't
use advanced techniques here is that
helps a lot making clear decisions and
and laying out the decision process
quite well most of this is actually
taken from a project that's available
it's a bit more complex that what I
showed it's called bundas it's a work in
progress but there is a documentation
documentation site in there if you want
to dig into how it's built as far as our
feedback is concerned mesos did help
move towards not dealing as much with
the allocation process it's it's easy to
ensure that you're always having the
topology that you expect on your
infrastructure containers if you're
dealing with something that's your
product only they ensure good enough is
isolation that you can rely on them and
as far as the apps burden is concerned
it's still reduced because you're
dealing with homogeneous machines which
simplifies your configuration management
and then most of the workload definition
happens on a higher level and is
simplified as far as future things that
I'm interested in pushing in mathematic
a bigger contributed list would be very
nice I'll keep following mesos releases
closely because it's a moving target
which moves quite fast especially these
these times example frameworks would be
very great if people want to jump in and
provide those a pure JVM mesos client
would be nice because it still relies on
an actual C++ library to interact with
missiles which can be a bit of a burden
and then the the project that was
talking about bundes should be a bit
more polished before i head over to
questions if there are any
just say that if you're building closure
obligations I'd really encourage you to
look into uni log which is a nice
wrapper around the log back that I built
in has been doing us quite quite a lot
of good and that's it for me thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>