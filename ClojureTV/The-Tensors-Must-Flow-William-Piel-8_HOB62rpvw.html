<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Tensors Must Flow - William Piel | Coder Coacher - Coaching Coders</title><meta content="The Tensors Must Flow - William Piel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Tensors Must Flow - William Piel</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8_HOB62rpvw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay time has come good morning everyone
all right let's let's get serious I'm
here to talk about some things that may
well determine the fate of humankind
these things include machine learning a
machine learning library from Google
called tensorflow
and my recent efforts to bring
tensorflow to closure all right I
already did that nice all right first
bit about myself my name is Bill Peele
I live in the Philadelphia area I work
at Magento on their business
intelligence offering and I can get
pretty excited about programming oh yeah
I'm gonna say ml sometimes because I get
tired saying machine learning so many
times I'm also going to use the word
idiomatic
too often okay so there are there's
quite a few talks about machine learning
at cons this year and I'm very happy
about that
whoo raise your hand if you saw choices
on cortex yesterday that's really good
that's good because she did a great job
explaining things that maybe I am NOT
going to do a great job with but you
know she filled in the gaps that's good
also I believe Wilhoit from yet
analytics is talking about deal deep
learning for J tomorrow I would see that
one - if I were you and and Karen Mayer
is talking about some ml deep learning
things go to all those okay so why why
am I so happy about this I am I'm very
confident that machine learning deep
learning are going to have an
increasingly profound effect on the
landscape of software development in the
coming years and well just I mean look
at this food chain
this guy tweeted this picture I really
liked it machine learning is big and
it's getting bigger and closer needs to
be in the game I would love for ml to be
like a big growth opportunity for the
language that we take full advantage of
and despite the topic of my talk I don't
particularly care about which tool gets
us there I would be ecstatic for any of
these to bring closure to the front of
the machine learning the field of
machine learning
I think tensorflow might be a good
option for us but other tools such as
cortex and deal 4j which are being
presented as I said I really don't know
that much about them so they might be
great too or better so my allegiance
here is really to closure is that's my
point and if you feel similarly you have
to ask yourself what do you want the
future to look like this unfortunate
scenario or this
that's probably Ilan down there sorry
pythons fine I guess I have some good
friends that use Python it's just it's
not closure huh so from my part earlier
this year I was taking a deep learning
class online and I guess things got like
a little bit out of hand because I'm
excited to tell you that I wrote a
tensorflow library for closure it
attempts to deliver tensorflow to
closure in like a very idiomatic way
that enables the kind of like rapid
iterative development that we all love
using enclosure already so I call this
thing Gilman I so what is tensorflow
tensorflow
if you don't know is an open source
machine learning library it was released
in November 2015 by Google and it was
like mid 2016 that it got capi bindings
which allow other languages like closure
to to be order leverage the core of
tensorflow
it was a few months later that jvi Java
bindings were introduced tensorflow
works by exposing API that lets you
build execute and inspect a graph of
nodes each node applies some operation
to one or more tensors and of course
like the tensors flow
if thou has an happiness conceptually
this is not so different from like how
data gets passed from function to
function in typical software so don't be
scared all right
when I applied to give this talk you
know a few months ago Gil's mood was in
like a bit of a mess there's a rough
state but it was moving along very
quickly so I felt confident and I
indicated in my proposal that like
things would be all cleaned up and
presentable to the public by the time of
the conference I did not really keep
that promise I'm sorry Alex
but so instead of that what I did do
instead of that or trying to pursue like
feature parity with the Python client I
decided to focus on demonstrating like
potential the potential I saw in guilds
Minh and more generally the potential I
see for doing machine learning in
closure but like this crazy car guilds
Minh does work and we will see some live
demos to prove that this has already
come up a few in a few conversations
that I've had since the conference
started which is that there's this
strange situation where tensorflow is
like a giant in the field it's
relatively mature you can do all sorts
of things with it
and then there's guilds Minh which I
only started working on a few months ago
what functionality can Gil's men like
what functionality of tensorflow does
Gil's Minh actually expose and the
answer is like not all of it it's not
totally intuitive either like which
which does what stuff does get exposed
and what doesn't and why I'm gonna try
to get into that I'm trying to try to
give a good explanation of that because
I want you to get kind of like the full
context of where this thing stands
so this is the this is the architecture
diagram on the tensorflow site it has a
lot of details that are not important to
us right now here's my simplification
that's better
so note the capi that blue guy that's
important for a couple reasons
the tensorflow core beneath the capi is
written in C++ and that's great because
C++ is reusable by closure pythons not
and also Python is you know slowish
compared to like C++ C++ is fast
that's where tensorflow gets its speed
so I like that C API I like how it spans
like the entire horizontal space that's
a nice clean separation right is this
rounded rectangle real though does this
represent reality turns out this is more
plan than how things actually are laid
out right now but in the early nineties
'men I believed in this fantasy what the
functional Python Python has access to
this kind of cheater API that allows
things that to be possible that are not
available to the C API that the rest of
us are forced to use now rest assured
the Google tensorflow team is hard at
work porting functionality or well
improving the C API and porting Python
over to using the C API so everyone's
gonna be on a level playing field
so maybe that's okay
turns out there was a lot more Python
than I expected when I started this
project so this makes the prospect of
achieving feature parity between Python
the Python client and Guilds 'men
somewhat of a daunting task I persisted
despite this revelation beak
I I felt like I could achieve some sort
of like 8020 Pareto portion of
functionality that would still make
Gil's 'men worthwhile okay so while
we're talking about architecture
tensorflow does ship with a slim
relative to the Python offering a very
slim Java implementation these two the
two green layers are the ones that are
specific to Java the C++ layer nope
those letters Jay and I next to them
this stands for Java and NATO native
interface and that allows the JVM to
leverage native libraries and in this
case gets us access to the C API other
people some other people earlier this
year have published examples of using
tensorflow
enclosure and they've done so by just
leveraging java interrupts with these
provided classes that's the same
approach but not the one I took
you know me so here's what I did here's
the Gilman stack the green layers again
are ones that I implemented myself the
big difference here is that the Java
classes that I implemented are even
slimmer they're stateless they're
minimal they do very little more than
they don't do much more than just act as
like j'ni bindings so that brings
closure as close as possible to the
underlying C API and the tensorflow core
and I think just serves as a much better
foundation for everything else that I
did okay so I'm going to talk a little
bit more about the design of Gilman but
first I wanted to get concrete for a
minute put together a hello world
example that's just adding two numbers
together for fun I did them both in
Python and in Gilman and made both as
simple as I possibly could here's the
Python I guess I guess that's okay like
it seems like a lot of typing just to
add two numbers together we have a graph
that's instantiated for us you can't see
that but it exists it's used implicitly
but we still have to instantiate a
session to find the scope for it being
used that's better
this is the Gillman code details like
the graph and the session are completely
incidental for a case as simple as this
so in Gilman you're not forced to touch
them
another big difference between Python
and Guilds Minh and like a critical
design issue is that in Python you start
by building the graph all the node
building logic needs to have access to
that graph instance which means it needs
to exist first it needs to exist locally
on the same machine it also means that
any function that builds nodes is
necessarily doesn't necessarily has side
effects we don't like that upon
understanding all this I had an
immediate visceral reaction and I knew
that any kind of idiomatic closure
idiomatic tensorflow library for closure
could not work this way so here's what I
did in guilds Minh you start by creating
a plan and a plan is a data structure
it's in the shape of like nested Maps
each node is defined by a map and
contains the definitions of other nodes
that it takes inputs from for example
you can call a planning function and get
a map like this one so ad represents the
tensor flows add function and what we
get back is yeah there's pretty simple
map there's not too much going on it's
not scary and this approach affords us
the this approach affords us the
benefits of data ordering programming
that we all know and love now at this
point I wish that rich could just like
appear here say something really
eloquent blow our minds and we'd be done
with this slide but instead you have to
settle for me and my thoughts on this
are that there's a few different
benefits like first of all
data can be inspected directly you can
look at these plans so when you call a
planning function you know what it's
doing because you can look at the map
that's being returned it's not a the
results aren't being like hidden away in
tensorflow
or some Java object or something like
that and as a result yeah you don't need
any side effects also composing data is
composing data is a lot easier the
composing logic when you're composing
plans can be more easily decoupled from
the functions that are providing those
inputs plans are reusable across graphs
plans are reusable across graphs they
can be stored or transmitted maybe to
like another server for execution maybe
one that has a GPU you get deduplication
for free in Python if you create two
sets of nodes that are identical you
have the cost of that redundancy Guild's
men can tell that two node plans are
identical and just duplicate them for
you no big deal
finally the note planning logic does not
need to have access to or graph
incidence and I think this is good
because a tensor flow graph is
append-only which is a nice feature but
it's still stateful you do not want to
be touching it more than you have to in
total I think the separation of the
planning and building phases is probably
like the most impactful design decision
that I made with Gilman and it's what
allows Gilman to deliver like an
idiomatic experience
okay so adding two numbers together is
great but let's get on with learning and
we'll do something slightly more
advanced but not too crazy we are going
to look at this expression and if you do
some math you'll see that the lowest
possible value for this expression is
zero and you were wrong if at that point
when x equals two this is the absolute
value of X minus 2 if if that wasn't
clear so you arrive there when x equals
two but we're not going to tell the
Machine that it's gonna have to learn it
on its own so here's the code that you
can use to represent that equation in
gillman we start off by instantiating a
variable variables name is X we
initialize it to to a value of zero this
is the only staple part of our graph and
it represents the current state of what
we've learned and the solution to our
problem we subtract that we subtract two
and take the absolute value so that's
the end of planning we have yet to
actually touch tensorflow and it has no
idea what's coming
we feed that into build session and
that's when the build phase begins so
this is going to take our plan feed it
to tensorflow to instantiate a graph and
from there start a session all graph
execution takes place inside of the
context of the session there's a
function run global R as an it that
initializes the variables which in this
case is just X variable state also
exists only in the context of a session
so usually do that right after you
instantiate one and so we're doing that
and finally we feed our session and the
plan for X to produce this is when the
graph gets executed now all we're doing
is we're producing the value of X none
of the the rest the equations getting
involved yet and we haven't done
anything to change the state of X so it
returns zero nothing too exciting it
it's time to learn but how do we do that
the magnitude of error in machine
learning is represented by what's called
the loss function as in if it's too high
you're a loser
all right that worked
that's good in our case the loss
function is the absolute value of X
minus 2 our learning logic seeks to
reduce the value of the loss function
and if that value is going down it
thinks we're learning these are the
tools that tensorflow gives you in order
to support the education of machines I
put them in an order that makes sense to
me where gradient descent is the highest
because it's the one that developers
interact with most directly I'm going to
go through this as quickly as I can try
to give you some kind of sense of what
they are if you don't really have one
green descent is often represented by
pretty pictures like this one the
altitude at any given point represents
the value of the loss function and our
graph is essentially dropped in some
random place in this beautiful rainbow
hellscape gradient descent is the
iterative process of trying to find the
lowest point by taking steps in the
direction of steepest descent this is
similar to how like a ball would roll
down a hill this is not that's maybe not
like a perfect model but it's actually a
pretty solid conceptual foundation I
think ok so I'm going to hopefully read
these to you green descent attempts to
find the lowest value of the loss
function it steps in this direction of
steepest descent to do so back
propagation calculates the slope of the
loss function and as a special case of
automatic differentiation automatic
differentiation differentiation reduces
our lost function to atomic operations
takes the derivative of each and
combines them using the chain rule you
remember the chain rule from cow class
this is the only application of it I
think
okay so here's my attempt to just kind
of get all those all this information
boil it down into the relationships and
their simplest elements but the key
thing here is what's at the top and
what's at the bottom in order to do
machine learning and tensorflow
you have to have gradient
implementations meaning you have graph
Scott nodes if any of those nodes use an
operation that doesn't have a
corresponding gradient implementation
then that node cannot participate in
learning which is a big deal see so this
is actually one of the larger obstacles
towards achieving in the quest towards
achieving feature parity between Python
and Guilds 'men Python has all of its
own gradient implementations the
tensorflow Python client tensorflow
also has a bunch of implementations in
C++ and that's great because those are
reusable by enclosure or any other
language that's using the C API Guild's
main also has some of its own written in
closure why is that during the course of
development I learned things and there's
flipping and flopping and so now there's
both this is not a terrible scenario to
be in though because the c++ gradients
have been giving me trouble where
sometimes my JVM crashes and I'm not
sure whose fault that is yet minor
Google's but um
could go the closer ones the kosher ones
work consistently so that's what I'm
relying upon in order to not be
embarrassed during this presentation so
it's a maybe it's a large obstacle but
it's not insurmountable I had not
written any C++ since around the time
that I learned about the chain rule but
and despite that I was able to knock out
some of these gradients without too much
trouble it's not so bad because the
mathematical logic which can be scary is
already laid out in Python for you and
you just need to port that over the C++
so if you're comfortable with those two
languages it's not so bad
and I I definitely will tackle more of
these as I need them so if you are
interested in tensorflow or helping
kills 'men progress implementing
gradients in c++ for tensorflow
is not a bad way to go so here's the
code that we had here it is with
gradient descent introduced now notice
the two highlighted expressions the
first one adds our gradient descent node
we're naming it opt we set an alpha
value of 0.5 that's the like the size of
the steps that you take we're optimizing
our loss function the second highlighted
expression runs that optimizer four
times four times is not that many times
you can't usually learn anything with
four iterations but our case is so
simple that it'll work fine
look at that so this right this is um
this is the resulting graph now what
that is produced from our code that
giant red gray box came from automatic
differentiation so that what no look
there's like a few tiny little things at
the bottom that's the that's the
original thing and then the rest
that's one say it's it's big grains are
a big deal
Thanks thanks Mike okay I promise
live presentation or you know live demo
here it is so here's the code I just
showed you it's in my editor I'm gonna
run it - she was the right answer so
this machine has learned
that I'm glad you guys were happy about
that because I felt like that was
actually pretty anti-climatic but I just
I needed to get the liveness out of the
way so we can proceed tensorflow
development is one of those tricky
scenarios where the values flowing
through your program are not easily
accessible like you can't sprinkle
prints and deaths around to capture
those tensors flowing through your graph
and even Sayid is not going to help you
now that's why additional tooling is
critical so Google solution is a web
application called tensor board it's
pretty slick
it's got a lot of cool visualizations
and the graph interaction UI is
especially cool and you have full access
to this when you're using guilds 'men so
that's good here's a few screenshots
these are scalar values that have been
captured over time oh that we I should
say the way this works is that while a
graph is executing tensorflow can
capture values from any nodes that
you've specified write them to the
filesystem the tensor board server picks
those up pushes them to the browser for
you to look at so yet some scalars over
time we can see things look good look at
this these are histogram time series and
I find these to be equally like
beautiful useful and confusing there's
definitely I think take some time to get
used to interpreting these I'm still
coming up the learning curve this is
nice this is like am a national data set
projected onto two dimensions for easy
consumption by humans at this point the
plan was to quickly show you how like
that graph visualization works the
interactive graph visualization you know
no I I left it to the last minute last
night to get tensorflow installed and I
don't know if this is just like a
personal problem of mine but I've been
using Python like lightly for years and
the package management like I don't I
don't get it
nothing ever works the way it supposed
to do for me
you run stuff it's like oh python 2 or 3
and then you're in some sort of like a
virtual environment and and like I don't
know if I needed like an oculus rift in
order to figure this out but um I don't
know
Python let's talk about closure this
slide attempts to explain what to me is
like the most important feature of
closure this is the thing that's like
it's so beautiful you can't even you
can't even look at it without like
special glasses and that is a big deal
to me like I kind of see this as a core
design tenant and yeah if you're
familiar I'll plug it one more time if
you're familiar with my debugging
development tool that I wrote called
Syed the the sole driving force of that
project was my pursuit of tight feedback
loops I always have that principle like
front of mind and I did so with guilt
men as well I'll read this quote to try
to make it sound as exciting as possible
Russell Stuart AI researcher says at
Stanford the chance that you've
introduced a bug when coding up a
network from scratch is so high you'll
want to go into a special early
debugging mode before waiting on high
iteration counts the name of the game
here is to reduce the scope of the
problem over and over again until you
have a network the trains in less than
2,000 iterations here's what I think
about that
so training the neural network can take
a long time it can take hours or days
and with cycles that long closures like
rapid iterative development capabilities
like aren't really going to make a dent
but early on you constrict your problem
in such a way that cycles are much
shorter and now it's closures time to
shine so I guess Google's pretty good at
web applications but you know I've got
some strong feelings about development
and debugging tools and I had to make my
own I needed to explore closures
potential to deliver rapid iterative
development to tensorflow I said that
enough so as quickly as possible
I'm going to run through some live
demonstrations we will be solving a will
be attempting to solve a classic problem
in ml which is recognition of
handwritten characters handwritten
digits the M&amp;amp;S dataset has tens of
thousands of such images they are 28 by
28 and grayscale so the idea here is
that we're going to start with very
simple Network and then see how guilds
'men helps us easily iterate to
something more complex that hopefully
works better to to the code okay
okay so here's our first iteration we'll
walk through this line by line all the
action here is going on in this def
workspace expression and def workspace
is a macro that I put in the Guild's min
that attempts to serve as a focal point
for all the behavior that you would need
in order to develop and deploy a ML
algorithm and model it does not it does
not do all that but it does some do some
things especially it pushes data to the
gills 'men web app but this is like I'd
say this is a key piece of my rapid
iteration strategy so first we define
these two values when you're evaluating
an ml model you you train it with a
training set and then you test its
predictive capabilities against a test
set and that's how you get like you try
to get like an unbiased number about
like how well this is actually
performing this is a pretty wacky macro
that I developed for good reason it does
a bunch of things for us but let's just
let's just call it a threading macro
that gives you access to intermediate
values as well as the final value and
here we are we're calling some planning
functions we've got a placeholder this
is where image input goes we've got a
dense layer which I'll talk about in a
minute this is where the magic happens
arc max which
that's what outputs like it takes the it
takes the output from the dense layer
and tells you which value is being
predicted like the number eight and
again here's the labels labels are the
correct values so like here's an image
of eight predict an eight for example
and then there's a few functions here
related to defining like finishing off
our loss function implementing gradient
descent and very importantly there's an
accuracy node that's going to give us a
percentage of how many predict you
predictions were correct and that's what
we're going to look at the the accuracy
that we achieve with the test dataset is
what we're gonna look at to figure out
if we are progressing or not so here we
are building those plans these are the
nodes that we're going to be capturing
summary values of as we do training
here's the datasets we're training in
we're going to go through 100 iterations
or epochs and then every tenth step
we're going to log those summary values
and then we have our test data set which
we feed in also okay all right so here's
the kills Minh web app not impressive
yet but run this what happened Oh
so this is the guilt moon web app and it
is not as slick as tensor board but it
does some cool stuff for example oh
because what I didn't like about tensor
board is that it does all these cool
visualizations but then it divides them
out into tabs based on like which
plug-in is providing those
visualizations and I just felt like that
was kind of like an unnecessary
impediment to enlightenment
instead gills Minh focuses on giving
delivering rapid feedback so you click
on some node like this is already
clicked on this for us
you get all these pretty charts last
year at cons Jason Guild's Minh
presented is awesome proto rebel and it
went over very well and I thought I see
people like pretty pictures I'm going to
give them pretty pictures so here's your
pretty pictures so look at all these
things so what should we talk about
oh here's the sim the simple one is we
have our accuracy value make a bake so
here we can see in the end we treat for
our training set we achieved total
accuracy great except we only got 68
percent correct in the test set so
that's not so great maybe weak
what's next I promise I totally talked
about the dense layer I'm gonna skip it
skip it
sorry I'd rather show you cool stuff
okay so what did we do here
something that is important in neural
networks is to like pre process your
data you want it zero centered and you
want it normalized ideally so you can
see here that it's your data oh that's
not zero centered all the data is all
the way on the Left which is zero but
then it's hard to see but the rest of
the data is kind of like spread out just
in positive numbers not zero centered so
what are we gonna do let's zero Center
it so what I did was I did some
pre-processing I took all of our images
took the mean value of each pixel and
then I'm subtracting that from each
image subtraction gets added in right
here and now that we've made things more
palatable to our neural network things
should be things should go a little bit
smoother so what did we get this time
oh wait yeah here we go so here is the
data this was the data before being
processed this was the data after being
processed don't worry about that
I guess I'm not capturing that what you
would see is that we'd be able to see
that once it was processed the mean of
that data set was now zero and it's got
values on either side of it let's look
at the accuracy do we do any better this
thing is really see I told you I told
you this thing needed work because it's
definitely blowing up on me right now at
the worst possible time here we go okay
so here's our accuracy so this is
unfortunate and the problem with
practicing this talk is you never know
exactly what's going to happen but we
actually did worse I'm gonna I'm going
to go ahead and blame everything that
goes wrong on the fact that we've got to
do this whole process and very
compressed time I think if we were using
like larger data sets like more
realistic development scenarios that
these tried and true techniques would
actually always yield benefits but for
now I'm just going to wave hands and
make excuses
let us yep let's go straight to
something cool no no no sorry we're
gonna go straight to the big guns which
is convolutional neural networks or CN
NS there's two key components there
there's the convolutional layer which
does this it has one or more learn about
filters and they're smaller than the
image but it slides them across applying
them in each step to get this new layer
and this kind of gives the layer a sense
of like spatial locality awareness
here's cool this is cool
here's examples of like a three layer
CNN that's been trained on faces you can
see like each successive layer learns
more and more complex traits of faces so
I think this gives you like a very this
can give you like pretty intuitive feel
of what's going on in this crazy CNN the
other important layer in a CNN is the
Max or a max a pooling layer in this
case we're applying we're taking the
square applying it to four different
regions and just taking the maximum
value in each of those all right
I guess if you want to see like the
whole demonstration you gotta like I
don't track me down somewhere okay so
well that's much bigger and here's
what's cool
as the training is going on this thing
is updating in real time for us so we
can see like all right here's where we
stand are we're getting up to 83%
accuracy for the training but our test
accuracy is still very low but it's
climbing that's great now in the future
I want to make it so that if you see
things have gone like way off the rails
you can just stop this process and that
could save you a lot of time another
interesting thing that happened is that
the loss can go up and or the yep the
accuracy can go up and down so it's not
necessarily like a progressive thing
that happens where you're always getting
smarter I am officially out time I will
close by saying that a lot more great
content
and that I hope to have a very usable
alpha out before the end of the year I
hope I've even was a subset of my
content inspired enough excitement that
somebody's going to try this thing out
once it's ready and enjoy the rest of
the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>