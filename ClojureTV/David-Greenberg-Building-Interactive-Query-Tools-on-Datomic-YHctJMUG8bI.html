<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>David Greenberg -  Building Interactive Query Tools on Datomic | Coder Coacher - Coaching Coders</title><meta content="David Greenberg -  Building Interactive Query Tools on Datomic - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>David Greenberg -  Building Interactive Query Tools on Datomic</b></h2><h5 class="post__date">2014-03-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YHctJMUG8bI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I'm going to be talking about
building interactive query tools on day
Tomic my name is David Greenberg and I
work it to Sigma which is a hedge fund
located in New York now while I was
building this talk what I realized was
that the the key to building interactive
query tools on day Tomic is really
finding the design patterns that work
for day Tomic both the conceptual design
patterns that you can present to your
users so that they can understand your
system as well as the code design
patterns that you can use to organize
your code so this talk comes in three
parts first we're going to look at the
the problem that we faced and and sort
of to give you an idea about where the
design patterns and conceptual patterns
that we came up with came from then
we're going to look at the the actual
conceptual models and the UI paradigms
that I think work really well for
diatomic and particularly for building
analytics platforms on day Tomic and
then finally we're going to look at the
implementation and in that we're going
to both look at code design patterns
that you can use as well as performance
considerations that we learned while
building the system so this all started
about two years ago when matted erath a
co-worker of mine who's in the audience
and I we began working on a closure
research environment so to sigma as i
mentioned is a quantitative hedge fund
and what that means is that we have a
lot of mathematicians and statisticians
and they use all kinds of data sets to
try to build models of what's going on
on the stock markets now what we thought
was that by using closure and using the
repple using the the great vim and emacs
integration that we could really make a
more productive research workflow and so
the goal of this project was religious
to solve pain points and very quickly we
found a really juicy pain point which
was running jobs on our cluster we need
to run lots of numerical analyses and
lots of simulations of the stock markets
and there was a lot of little tools
there was some command line tools that
will you look at some parts of the
system there were some web apps that
Lisi other things and so what we did was
we just sort of hacked up with
that was I'm just a simple command line
program that integrated each of these
smaller components rewrote some of them
and just tried to present a better
experience for our users and that tool
was called cook now what cook did was it
allowed you to visually monitor jobs and
you can see here that what it did was it
presented these jobs in this big matrix
and a color-coded them based on their
status the actual cells in the matrix a
little progress bars and so you could
sort of get an idea of what was going on
with all your jobs on this cluster now
the other thing that it could do is it
tied together all these other pieces of
metadata that we're interesting things
like what machine is the job running on
or what the log files look like because
otherwise you'd have to use SSH you'd
have to use all kinds of little tools to
find this stuff out we just tied it all
together and so this is really
successful and our users really liked it
but there was a big problem some big
problems with it one of them was that
the UI was very tightly coupled to the
backend system this was my my first
experience sort of writing closure
script in JavaScript and I did a really
bad job of it there was the data
structures that we use on the server
would have to get directly reflected
onto the client and all the rendering
was very sort of hard coded and tightly
coupled together in addition the whole
system was just unscalable at the time
when we started we the problem we
thought was making it so you could run
10 or 20 simulations and understand them
but later on once we made it easy our
users started running orders of
magnitudes more jobs and so the system
that we built I basically created a
legacy system for myself and we had to
rewrite everything and that brought us
to the second version which was called
cook cloud because it was a cloud
architecture now the goal of this was we
wanted to track everything that was
happening on our cluster now you can see
here in this picture I'm not trying to
show you so that you can understand you
can look at this and say oh yeah I get
exactly how this works but I just want
you to have a feeling of the the level
of information density that we want to
communicate to our users now the goal of
this was that we wanted to provide
feedback to our users and allow them to
to query it analyzed
what's happening throughout their data
sets and we wanted to see why to be both
simple enough so that you could use it
without any kind of training but also
powerful enough so that power users
could really leverage it and answer
interesting questions for themselves now
I haven't really explained what these
jobs in cluster is and so I just want to
let you know so one of the important
things that we do is we run these lots
of Hadoop jobs to analyze large data
sets and we also run lots of back
testing simulations of the stock markets
to understand how our predictive models
are doing and so the context of all
these jobs that we're talking about are
we have this data center far away in
another city and we just want to
understand what's happening over there
so what are some of the kinds of
questions that these researchers our
users what do they want to ask well the
number one question that they want to
ask is when are their jobs going to
complete and the reason for this is that
it's very pragmatic if your jobs are
going to be done in a minute then you
shouldn't go to lunch right now you
should wait a minute start your next set
of analyses and then go to lunch and so
this is a very simple just productivity
thing that can make a huge difference
for our users and that they really cared
about the next type of question they
want to ask is how many resources are
they actually using perhaps the reason
that their jobs won't be done or so
maybe they go and they find oh you know
it's going to be a week before my job is
real completed maybe that's because
they're using too many resources or
maybe that's because some other user is
using too many resources and so then
they can either change their habits
maybe modify what they're doing to use
fewer resources or they can go ask
somebody else to tone it down while
they're working on their research the
last question which is really a question
that we didn't know they wanted to ask
until we built the first version of the
system was they wanted to identify
patterns and they're failing jobs so
jobs could fail for all kinds of reasons
there could be bugs in the users code
there could be bugs in the platform code
there could be data issues where
something was malformed it would just
throw an exception every time you try to
analyze that day's worth of data or
there could be transient system issues
or
work issues and so these were things
that once we provided you is to see this
kind of thing then our researchers could
just use their eyes and their brains to
do the thing that humans are great at
which is detecting patterns in visual
data and and use that to figure out how
they can solve their problems so that's
sort of just an overview of the problem
that we were trying to solve and now I
want to tell you about the design and
this design is both the mental models
that the developers keep in mind
throughout our system as well as the UI
design patterns that worked well for our
users so the goal of this remember is to
analyze tens of thousands of jobs at a
at a glance so here you can see that in
the top we have this little summary
visualization and in it we have like
13,000 waiting jobs 22,000 finished jobs
you know and so I think that in this
case we've succeeded in making it so
that at a glance you can understand the
the rough progress of tens of thousands
of jobs down below that that progress
visualization we have this tabular view
that breaks down each job as a row and
then various attributes of the jobs are
in columns and this is just because
tabular data is really natural for
people to understand and it's something
that it is natural not because it's
something that's part of us as humans
but just that everybody who works with
computers is used to seeing tables now I
think there's something really
interesting about this progress
visualization now the blue region is
waiting jobs and you can see that's very
low in height and the green region is
successful jobs which is very tall in
height and the yellow region is the
running jobs and what we've done here is
that the height of the bar actually
represents the progress of the job so
the waiting jobs are not yet done so
they're very short the running jobs are
tall or the successful jobs are tall so
they're completed and so when we look at
the running jobs you can see they sort
of form a line between the waiting jobs
and the successful jobs what that tells
us is that there's an equal number of
jobs at each percentage complete so
there are some jobs one percent done
some jobs
have done some seventy percent done and
it's sort of uniformly distributed if
instead we saw that all of the running
jobs were very similar in height to the
waiting jobs and only at the very end
did they shoot up to the same height as
successful jobs that would tell us that
most of the jobs are in the art just
started there they're not very far along
conversely if we saw that the running
section was almost the same height as
the successful section that would tell
us that we're nearly done and if we wait
just a couple minutes everything's going
to be ready for those of you with a
background in math or statistics what
we're actually showing here is the
empirical cumulative density function of
the progress percentages of the jobs now
even though I've just showed you how
this lets us sort of at a glance see
tens of thousands of jobs we still
needed to make sure that this you I made
it possible to understand one job and
that it was still pleasant to use when
you're just looking at one thing in
detail now we also needed to make sure
that we could find all the relevant
metadata that I mentioned before you
know which machine a job is running on
who launched the job what the
configuration options were what the log
files look like we wanted to tie that
all together into one single you I
because that just makes it easier to
access that data now another goal that
wasn't immediately obvious was that we
actually wanted to be able to scale the
development team when I began working on
this project it was just me working on
it and so if things were tightly coupled
or if the code organization wasn't great
it didn't really matter because I could
just try to keep it all in my head but
now we have six or seven people working
on it and so it was really important to
find to figure out ways to nicely
component eyes the different aspects of
the system and ensure that even if one
developer made a mistake it didn't take
out the whole system or slow down the
development process for everyone else so
the core idea in this you I was that we
use domain objects everywhere and a
domain object is in some sense just a
way to make a UI make sense to a user
and so what we do is we would choose a
canonical entity in our case we chose
jobs to be the canonical entity
and then all of the processing we do is
in terms of these domain objects so for
instance the UI lets you see things like
which jobs are running who ran the jobs
what the configuration options of the
jobs are now if domain objects don't
immediately seem familiar you've all
used them before on if you use amazon
com their domain object is products when
you type in the search field you're
searching for products all of the
filters in terms of stars or in terms of
shipping methods those are all in
working with products so domain objects
is something that that everyone who's
used a computer has become familiar with
whether they know it by this terminology
or not and so something that's kind of
interesting is that day Tomic gets a lot
of credit for being something that has a
good strong relational model in a strong
transactional model but you can also use
it as a document store and particularly
from an analytics perspective treating
daytime because a document store is a
really powerful way to present your data
to your users so what is what are these
queries actually look like well the
first thing we do is we're going to
we're going to do some kind of filter
step which is where we find the relevant
objects now an example of this might be
we're interested in all of the jobs that
have run in the past two days and then
once we've found these things that we're
interested in we're going to group them
by some attributes say by the users that
ran them and then once we've grouped
these jobs we're going to aggregate them
which means we're going to calculate
some kind of statistic over these
groupings say the CPU time used so if we
tie it all together one type of question
we might ask would be in the past two
days for each user how much CPU time
have they used this is this query model
is something that that if you've ever
used sequel databases than you've you
found you know you can do you can select
some of some column and group by
something else and it's it's a familiar
pattern queries but it's something that
I want to call out because although
we've used it in code we can also
present it this way in
you I now our users we're not going to
try to teach them data log which is the
query language for diatomic and you know
because even if they happen to know
sequel and many of our users do no
sequel it's it's a lot to ask for them
to learn a whole new query language just
to you know get at a glance information
of things that are interested in so
instead what we do is we create a
domain-specific query language here what
i'm showing is a screenshot from our UI
where we have a text field where you can
enter something that we call the prefs
language which is one of the languages
that we use to configure our simulations
and this language is just a bunch of key
value pairs and so the way that we use
this is a filter is that each key value
pair that you enter into the filter
field that restricts the jobs that
you're considering two jobs that were
launched with that key value pair and so
this is something that was done very
natural at our users because they're
already experienced writing job
configurations and so now they can
search for their jobs by the
configuration that they used now you
might be saying at this point well this
is really hard because you're saying
make a custom query language just so
that our users can be more productive
that's that's not easy and it wasn't
until insta parse came out insta parse
makes it really easy both to write the
queries because it supports all of the
common notations for for describing
languages but also provides great error
messages so you can see here that in our
UI we actually can send from the server
we can send down the error messages
remains to parse and they tell us things
like the line number and the column
number and the reason that that that
that query is invalid and so this really
cut down our development time for adding
new domain specific languages for a
query engine to maybe half of a day to a
day to add a new one now the way that
these query languages get compiled to
date ama queries is that they form a
sequence of clauses in each of those
clauses is going to further restrict the
set of domain objects that we
interested in so in this case on line
two we have this uh we're saying well
filter for jobs where the region is in
the u.s. region now to Sigma trades all
over the world and so what this means is
that we're looking for simulations that
we ran that we're simulating things that
happened in the United States region and
so each of these clauses that's further
restricting the domain objects that
we're interested in what we're going to
do is generate some data log fragments
that represent that filter now a data
log fragment is not something that is
part of day Tomic data log but it's sort
of this notion of sort of a part of a
query it does it's a query that doesn't
really stand on its own but you need to
take a bunch of fragments and compose
them together to get a real data log
query so I want to give you all just a
very brief tour of dynamic data log now
I'm not trying to teach you data log
here that's probably a 40-minute talk on
its own this is just a taste so you can
have some idea about how the rest of the
talk or how the rest of the system works
and and some of the features of data log
so one way that we can write data log
queries is by using pattern matching
which is where we have these matching
clauses so here we have a query where
what we're doing is we're looking for
pairs of jobs and users and what we're
doing is we're saying well each of those
jobs has some attribute which is the job
user and then we're going to find the
user for that so the result of this is
just going to be for each job who ran it
another way that we can write a query in
data log is we can use a query function
and so the way we make a query function
is first we just write a plain old
closure function in this case we have a
function done and done takes an argument
which is a status code and on our
cluster our jobs have four statuses they
could be in they could be waiting to run
they can be currently running they could
be okay which means they completed
successfully or they can be failed which
means that they completed but there was
some kind of error
the date and the results aren't valid
now we can actually integrate this into
our day Tomic queries and so this query
what it's doing is we're saying find me
all the jobs where first what we do is
we pattern match those jobs and we say
well all the jobs have statuses and then
what we're saying is we're actually
going to call out to this arbitrary
closure function that we've written that
can validate and do whatever we want to
in the middle of the query now this is a
really powerful idea that we're going to
leverage and trying to build analytics
platforms and atomic because what we can
do by using query functions is we're no
longer just restricted to the built-in
things in the database if you need to
call out to legacy sequel databases you
can do so in the middle of your date
hammock queries and if you need to join
with third party aap is you can do that
as well and so that's something that's
really powerful and we'll look at more
how we can do that later so now that
we've seen some of data log I want to
take a look at how and we've seen how we
can map filters I want to take a look at
how we can map grouping and aggregates
those those second and third steps of
our queries how I map those and it's
really very similar to the filters in
that we're just going to construct these
sequence of the data log fragments the
only difference is that we also have to
indicate the output variables things
that we're looking for now the reason
that I put them both on this one slide
is that it turns out that grouping and
aggregation are actually the same code
path and data log so here you can see on
the right we can see various column
selectors and some columns that we may
want to select are things like the pref
or the input that configure the job or
the launch time and those are sort of
unique things about each job but also we
can look for things like um like the
number of CPU hours or memory hours used
by the job and so when we present this
in the tabular results there is not
really any kind of distinction between
grouping and aggregating now the way
that we can do this in day Tomic data
log is that each of those output
variables and our find flaws either we
just write the output variable plane in
which case we're going to group by that
output or we can write that output
variable inside of an aggregation
action so these aggregation functions
it's very similar to if you've ever in
sequel written select some of some
column or select average of some column
it's basically the same idea except
instead of being forced to use in new
syntax where you write your group by at
the very end of your query in data log
it's just implicit for you're either
grouping by things or you're aggregating
things now another thing that we added
to this to this conceptual model of
queries that's not a part of data log is
this notion of drill down and the idea
here is that we want to sort of look
inside of a grouping so if we are
grouping jobs by the users that ran them
then there's going to be one row for
each user but maybe what we're
interested in as we see oh like that guy
you know Bob he's ran tons of jobs i
want to know more about that in
particular and so we can you know sort
of open up and look inside and see what
what bob has actually been doing so an
example of this here is here we have the
the main page of the of the query you I
we can see that we have you know our
tabular results and you can see from the
the summary visualization at the top
that there's a lot of jobs in their
consideration here and so if we move our
mouse over the first row then we can
click on it and what happens is that now
we're only looking at that one job that
was considered in the first row and
instead of seeing just the columns those
aggregated columns like you know which
user and the total cpu time now we can
actually look inside of that grouping
and we can see for each job what is the
name that the user assigned to that job
and what is the file that they they pass
to that job to launch it and so what we
can do is we can actually map each of
those groupings to a corresponding
filter so if we're grouping by user
there's this natural pattern that we can
go from we're grouping by user is the
same as filtering for a particular user
and this is something that we've built
into the code paths in our UI and we can
build into the back end to make this
really easy to add for all kinds of new
things you may want to filter by so now
that we've introduced the sort of the
conceptual model I want to actually tour
you through some of the code
particularly how we
generate de Tomic queries and some of
the performance characteristics that we
encounter so the big idea in the in the
code structure is that we have this
query builder and the way that it works
is that we're going to start out with
one variable our domain object will give
that the name aab and what we're going
to do is for each of those filter
fragment reach of those filter clauses
and for each of those output columns
we're going to generate all of those
data log clauses and the output
variables and then we're going to
compose them together programmatically
this is a really powerful idea because
queries are just data structures we can
use functions like Khan Jana sews that
we're used to to actually program them
in and manipulate them it's not like
working with something like sequel where
you have to use a lot of string munging
and which is very prone to errors and
where you have to wait for validation
from you know your sequel database to
actually tell you whether your queries
are malformed or not so I want to just
emphasize that the atomic queries are
really just data structures now usually
we see the atomic queries written as a
vector of clauses but you can also write
them as a map of clauses and so by
looking at is a map we can see that it's
much easier to use functions like esos
and update in to manipulate each of the
clauses of this query now the other
thing that I've done here is rather than
quoting the query at the very top I'm
only quoting the symbols inside of the
query and the reason that I'm doing this
is I really want to emphasize that the
entire query is a data structure that's
really easy to construct and all we need
to do is just make symbols and just drop
them in in the right parts of the query
and that we have our query so now that
we've seen how queries are data I want
to show you about query fragments and
how we actually make a query fragment so
to make a query fragment we define these
functions that are called query fragment
builders and as an argument they take
the data the symbol of the variable in
the query which is the domain object and
so that in this case is this ID and then
what we do is we just returned a little
part of a data log query and so in this
case this query fragment is saying
well the domain object has some
attributes see one we're going to bind
to that attribute to this query variable
a and that's we're going to be looking
up and now similarly we can do the same
thing for some other attributes e2 so we
take our query variable name as an
argument then we return the data log
fragment and now there's something that
that this really doesn't doesn't sit
well with me doing it this way so one of
our goals is that we want to be scaling
the development team and it's important
in the atomic data log the name of your
query variable tells you when those
things are related or not and so if we
used our query fragment builders this
way then we would have to ensure that
every single developer never reused the
name of a query variable that anybody
else used because if they do they're
fragments could interact in unexpected
ways and give us the incorrect answer so
we solve that by using a function called
Jensen and this functions in the core
closure library and Jensen is a function
that takes a string as an argument and
it returns a symbol and that symbol
starts with the string that's passed to
it and it ends with a number and every
time you call gems em that number
increases so what that means is that
every call to Jensen returns something
that Jensen is never returned before now
i want to point out that this is only
true within one JVM if you had multiple
JVMs every time every time you start up
closure jens em starts at zero so you
can't use this to communicate between
JVMs but within a JVM then this works
great so let's see how we can use Jensen
with those query fragment builders so
what we can do here now is we have our
same query fragment builder as before
but this time we're going to jen's him
that internal variable in the fragment
and that guarantees that is never going
to collide with anything else in our
system and so you know once again we're
taking that query the domain objects is
an argument and we're going to then
generate our internal query variables
and then we're going to construct our
prey fragment and we can do that for
both the c1 and the sea to query
fragment builders
now I would like to show you what it
looks like when we actually genzyme
aquaria fragment now this slide has a
lot of jensen's variables on it which
means there's a whole lot of numbers so
i want to show you this because although
it looks ugly at first it's something
that you can learn to understand and i
want to introduce this to you so that
when you go and build your own analytics
platforms on the atomic that you can
feel comfortable debugging them and
understanding them so our first query
fragment is taking our domain object and
we're looking for something that we're
calling 222 and 222 the way we get to
that is we started a domain object and
then we it has some attribute that we're
calling 134 and then that thing 134
itself has another attribute which is
222 so there's sort of two hops that
we're taking from our domain objects to
the thing that we're actually interested
in we can make another query fragment in
this case it's a little simpler we're
just finding the owner of the domain
object and we're going to call that
owner 574 now the really cool thing
about this is because we're using Jensen
we don't have to worry about these query
variables conflicting so we can actually
compose these queries together and the
way that we compose them is we just
concatenate the vectors in the fine
clause and we concatenate the vectors in
the where clause and so we end up with
for this third query fragment it does
the it does the functionality of the
first and the second fragment and it
just works so now I want to tie all this
together and show you how we can
actually build a pipeline like you
construct these queries programmatically
so we'll start by defining some domain
object query variable name and we're
just going to jen's him some mobs and so
this will be our domain object query
variable next we're going to use those
query fragment generators that we
defined before and we'll just construct
the query fragments that we need for our
query the next thing we're going to do
is we need some kind of base for our
query this is the thing that sort of
rounds out the query it makes it not
just be a fragment but something that's
actually interesting
what this base is doing is its defining
an aggregation function and what this
aggregation function does is when we
take all the domain objects that we're
considering and we group them by these
c1 and c2 attributes for each unique
pair of c1 and c2 we're going to find
this the the set of all of the domain
objects that had both of those
attributes and so this is something that
you can do just to sort of pass this
along to further stages in your
processing pipeline so now that we have
all these little fragments in our base
of the query we can just combine it all
together using merge with now everyone
is hopefully familiar with merge which
just takes a couple of maps and combines
them together into a bigger map and so
what merge with does is in case there's
conflicting keys in the map it lets us
provide a function that somehow merges
together the values of that map in this
case we're using into as our conflict
resolution function and what that does
is it concatenates the vectors together
which gives us exactly the behavior that
we want to compose together these data
log queries now that we have our query
we can just pass it to the queue
function which is the thing that runs a
day Tomic query for us and that's all
you have to do now if you're going to
take anything out of this talk it's this
you can compose together queries
programmatically using the same data
manipulation functions that you use
throughout your programs now I said
before that those query functions are
really powerful and that we'd come back
to them and I want to point out that
we're actually calling out to those
query fragment generators here where
we're and those pre fragment generators
are are creating any code that we want
them to and so what that means is that
if we decide that we really need to call
out to that legacy surface here or we
really need to call out to that
third-party API you can do that and you
can make that some something that your
users can actually choose should they
include this query fragment as part of
the overall query they're running or not
and so that's something that's really
powerful because you're not restricted
by the semantics of the atomic data
you can actually make your queries do
whatever you need them to do for your
analytics platform so we've seen some of
the code design patterns I want to walk
you through some of the performance
considerations and through some of the
things that we learned along the way so
the number one performance consideration
in data log is that you should write
your filter clauses before your
expensive query functions in day Tomic
data log it doesn't matter what order
you put the clauses you always get the
same result and that's really great but
you don't get the same performance
characteristics we've had cases where
with our initial ordering of clauses a
query took 20 or 30 minutes and when we
ordered them correctly the query only
took about 100 milliseconds so this is
the only thing that you can do in your
optimizations that will literally get
you multiple orders of magnitude of
performance improvement now I'm saying
that you should order your clauses
strictly from filtering to the expensive
functions that's not entirely true
there's actually a more sophisticated
mental model that you can learn that
will tell you exact weary and understand
exactly how to order the clauses that's
beyond the scope of this talk but it's
something that you can ask on the de
Tomic mailing list or on the IRC channel
and you know to find out more about the
next performance consideration that is
really important to keep in mind is
these query functions when they're run
in the de Tomic query engine they're not
paralyzed which mean that's great in
case you're calling out some you know
maybe a Java API that's not thread-safe
but that means that they can get really
expensive particularly if you're calling
out to sequel databases or web services
where each call is taking tens or
hundreds of milliseconds then this can
add up really quickly if you're doing
this for many entities in your database
we mitigated this by using core cash
which is a caching library that's
enclosure can trip and it's great
because you can compose together
different caching strategies like
bounded by the amount of memory you want
to use or bounded by the time to live
and so you can end up with a caching
strategy that works well for your query
pattern to these external services
another thing you can do to optimize
your performance is you can use day
Tomic partitions now I put the
partitions in quotes because they're not
the same as partitions on a laptop hard
drive or on you know a sharded database
like MongoDB instead a day Tomic
partition is just a locality hint it
gives you away by default the atomic
assumes that your data all has temporal
locality and so it tries to arrange your
data sets so that when you run a query
if you're querying for things that all
were transacted around the same time
then that'll be a little faster than if
you're querying across time by using
partitions you can tell the atomic about
other types of locality in your queries
for instance you could say in our case
we have jobs or something that has a lot
of locality or users you know a user
cares a lot about the details of their
particular job so we can try to use
partitions to group that stuff together
to improve query performance now this is
something that we're only just starting
to experiment with and we haven't really
needed yet but it is something that we
think can improve the performance but
the reason we haven't really needed to
worry about this is because of memcached
memcache is a built-in feature in de
Tomic when you start up your peers and
your trans actors you just give them the
URLs of your memcache cluster and they
automatically use it they use unique
keys to ensure that it doesn't conflict
with other applications that it's
sharing the memcache fluster with but it
just makes everything go a lot faster in
our case we found that using the
memcache cluster sped up all of our
queries by about 5x across the board and
that was for no work there's there's
almost no configuration and there's
nothing that you have to do in your
application to take advantage of this so
I highly recommend it our memcache
cluster is about 200 gigabytes and so as
a result we almost we have over a
ninety-nine point nine percent hit rate
on the memcache cluster even while more
querying data sets that are you know in
tens of gigabytes and so this is really
cool because it means that you don't
have to worry as much about your data
organization and instead you can focus
on making your application easy for your
your developers to work on and for your
users to actually find
answers they want the final
consideration to keep in mind is the way
that day Tomic queries are actually
executed their compiled locally into
java bytecode which makes them go a
whole lot faster but there are cached by
this raw representation this data
structure that is our query so you might
be thinking at this point well David you
said use Jensen to make sure that your
queries never conflict but if you use
genzoman all of your queries then every
single query will be different and so
you'll always have to recompile your
queries for every single one that you
run so we don't actually use the
genzoman closure core we use a variant
of it called stable Jensen and what this
does is it just resets the gems
encounter 20 at the beginning of each
request so as a result if two identical
requests come in they'll execute the
same code path and so they'll call jens
em in the same order and then their
queries will be the same they'll hit in
the query cache and we still get all the
benefits of using Jensen and we get all
the benefits of caching the queries so
one thing that I haven't really
addressed is is why we actually chose de
Tomic for this project and also sort of
what diatomic is to us so to me diatomic
is Earth not to me from the dynamic
website dynamic is a transactional
immutable cloud-ready database with
horizontal read scalability and to me
what this means as far as choosing
between de Tomic in a sequel database is
well both of them have transactions so
they're good for some kind of system of
with the data of record but de Tomic is
cloud ready so if you've ever tried to
set up replication in your sequel
database or set up PG bouncer or master
master replication these are really hard
to do is a lot of overhead and a lot of
operational overhead with de Tomic you
can just take the same transactor config
file start it up on three machines and
now you have three machine now you have
to lose all three machines before your
peers stop transacting there's no
special configuration on the application
side there's no special configuration on
the day Tomic side and so from an
operational standpoint it was much easy
than a sequel database now another thing
that you might be thinking is well this
problem seems like something that
elasticsearch is really suited for you
know we're taking all these sort of
documents and we're searching them and
grouping them by things and all that
stuff and it's true that elastic search
is good for this type of application but
the benefit of using de Tomic is that
you're already putting all your data in
de Tomic because elasticsearch is a
terrifyingly bad place to put your data
because it can just crash and corrupt
all your data so you don't want to put
your data directly into elasticsearch
the fact that the atomic has horizontal
read scalability means that if we need
to scale our queries then we can just do
it and it's really easy and because
we're not doing much full-text search
but instead we're querying on other
attributes of our data there's we don't
need that the killer feature of elastic
search which is that leucine integration
so they time against tree works really
well for a lot of analytics platforms
now the really key thing about the
atomic is this horizontal read
scalability you don't have to do
anything to increase the read capacity
in the query capacity of your
application so particularly when you're
building some kind of analytics platform
you can have one application connected
to your day Tomic database that's
handling all the transactions and all of
the real time part of your system and
then you can have another one connected
to the same database with no more work
that does all your analytics queries and
as the analytics get more popular you
can just start more copies of that one
and as the real time part gets a higher
workload you can just start more copies
of that one and so the fact that you can
just scale this out really easily means
that you don't need to try to come up
with some kind of complex architecture
than mirrors your data from one place to
your query location now I want to also
just mention that de jamac is actually
holding up for you know a reasonably
large amount of data our database has
about seven and a half billion unique
datums as of a month ago and about 17
billion indexed atoms when you sum
together the size of all the indices and
we use react as our back-end this takes
about 2 terabytes of space in our react
so you know they Tomic is something that
it's not just working in smaller
installations you know we have
substantially more data even than fits
in memory but we're still seeing great
performance so in summary the way that
you can build a query platform for an
atomic is you know choose the domain
object that you're going to present to
you the users of your query engine and
then come up with some kind of
domain-specific filter language and I
mentioned this prefs filter language
that we have but we also actually
implemented a subset of sequel also
using insta parse and you know each time
that we've had some new domain objects
in some new filter it's only taken maybe
half a day to the day to make this
little facade for our users to find it
easier to run these queries the next
thing that you need to do is figure out
all the columns and all the day Tomic
fragments and generate all of that now
the number one performance consideration
particularly if you're seeing
pathologically bad performance like
seconds to minutes for queries is your
claws ordering is the biggest thing you
can do to improve your queries
performance at that point and finally
the fact that day Tomic has the real
time components and it's mirroring out
all that data to the rest of it means
that you've essentially automatically
connected your real time system to your
analytics platform and so for those of
you who worked on etl systems where you
have to bulk load your data from one
database to another database you just
don't have to worry about that it's all
just works for you and it's part of the
day Tomic architecture so I think that
that ended up making this a simpler
cleaner more reliable system than
anything else that we came up with and
so the thought I want to leave you with
is that queries as data are awesome
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>