<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bare Metal Clojure with clojure.Spec - Michael Reitzenstein | Coder Coacher - Coaching Coders</title><meta content="Bare Metal Clojure with clojure.Spec - Michael Reitzenstein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bare Metal Clojure with clojure.Spec - Michael Reitzenstein</b></h2><h5 class="post__date">2016-11-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yGko70hIEwk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone my name is michael ranson
Steen and I am a full-stack closure
developer at book well a start-up in
Melbourne Australia and we are actually
hiring so if you don't mind a bit of a
commute see me after the show what I'm
presenting today is a compiler that
takes closure code and translates it
into C taking advantage of closures at
one point 9s new spec and generator
features in order to strip away
abstractions and boilerplate the
resulting code respects closures
semantics but it elides the work
required to do it where possible running
at a compile time rather than at runtime
and this means that it looks a lot more
like something that you would write in
hand in C so Y compilers why am i
interested in compilers
in short the death of Moore's Law
low-budget hardware design is sort of
catching up to the blazing edge as the
blazing edge is moving more slowly
graphics card companies even skipped a
process node recently which is
unprecedented and consequently we're now
experiencing a bit of a renaissance in
novel hardware architecture for example
Z on fire FPGA is in the data center
like Microsoft is doing chips with
dozens of low-power arm cause an even
exotic tech like adaptive is new
thousand core processor and CPU stacked
on top of memory and of course the
elephant in the room GPU computing is so
common now that 5 out of 45 of the
available ec2 instance types feature
graphics processors now I would assert
that this would not have happened if x86
scaling had kept the pace that it set in
the 90s and early 2000s you need to be
an order of magnitude better with your
Hardware before people are going to port
their programs over to use your system
and when your competition is itself
doubling in performance every 18 months
it's not gonna happen a short delay
means you're no longer better enough to
warrant the porting process a big delay
means you no longer better at all and if
you don't believe me look at what
happened to Intel's own Itanium
processors
I actually blanked on the name for a bit
until I remembered that people used to
call it the I tannic which is sort of a
two-for-one indication of just how that
went but we need to be able to program
these novel architectures without
rewriting all of our code or they're
still not going to get made because they
won't get mainstream adoption and that
means a next generation of compilers the
performance trade-offs for these systems
is extremely complicated and that the
methods used to program them often boils
down not to analysis but modify my
program bench market and repeat in a
loop a sort of human powered genetic
search process if we even need to
manually hold our compilers hand when
dealing with non-uniform memory
architectures what hope do we have of
breaking down complicated programs and
scheduling scheduling them to be run
across a thousand cause each of which
has slightly different communication
characteristics and wire closure a large
subset of the language maintains
immutable semantics with referential
transparency this means that you can
catch results rerun code remove code if
it's not needed and swap it out any way
you would like to without having to
worry about
mutability it also provides a powerful
set of abstractions that can be feasibly
alighted or optimized down for sections
of code and I think you could even go a
bit further
for instance what about first-class spec
based dispatch it's a possibility
closures home iconicity also allows for
easy code rewriting and indeed the
intermediate language of my compiler
remains
valid closure code right until the point
at which it is translated to see for
instance an unboxed native integer is
represented by a hash map like this and
if you're paying attention yes that is
an unboxed and being represented by a
boxed boxed int the aims for the project
I aim to compile purely functional
numeric closure by rewriting inlining
moving simplifying and specialize in
code constructs such as closures hash
that's a closure is in not closure the
closure is in the capturing of variables
hash maps and multi dispatch should be
alighted where possible or replaced by
simplified construct that performs the
same job but still sir but these
constructions still be supported in full
when required
I also aim to build tooling which will
be proved useful for testing closure
code in general testing optimizations
after all is just a subset of of just
testing code in general so more on that
later what is not a name for this
project primarily supporting mutability
as mentioned before the compiler relies
heavily on referential transparency and
it makes things just so much easier
because of this the input programs must
always be purely functional
although the compiler itself is free to
reintroduce mutability as would like to
as an optimization for instance with
caching like a simple memorize is
actually immutability it's it's it's
updating an object somewhere I'm also
not claiming to support interrupts with
Java and other platforms basically
because the main reason is reference
referential transparency needs to be
maintained and so if you if you are
using plus that's fine because you can
call it as many times as you'd like to
but if you're calling write a file the
compiler is not free to call that
function twice it's not free to not call
it at all
and so it's not possible to support that
with this system so without some
prerequisite knowledge on spec and Jen
this talk is just going to be gibberish
so I'm just going to go a quick overview
or just a few minutes although I suspect
most of you are sort of up to date so
it's a new data validation and
generation framework being added to
closure 1.9 and it's sort of becoming a
key part of the overall closure solution
it's sort of answer to static typing any
predicate that in existing closure code
can act as a spec for instance integer
float you call valid on which checks if
a if a value is valid according to the
spec and it just works including
arbitrary user functions there's there's
sort of no code behind the scenes in
order to check a spec of a predicate
specs can be arbitrarily combined for
instance this is a tuple of integer and
float and then that will match if we
pass it an integer and a float and it
won't match if we pass it an integer and
an integer generators sort of perform
the reverse function where they take a
spec and then they output data that
conforms to the spec it even works on
compounds like our example before of
tuple integer float it can generate that
without problem or down the bottom where
I've got ranged int which is an integer
between 0 and 5 you can also generate
that now the generators themselves
aren't magic you actually have to
specify how to generate for your custom
predicates this isn't the syntax that
you'd use but I just wanted to condense
it down for the slides so it's actually
pretty easy to convert most closure code
to see if you don't care at all about
performance firstly you define a boxed
data type which can contain for example
float int or a linked list element then
you simply stub out reimplementation of
the core closure functions such as count
cons map filter
that operate on these data types and
then your code can be mechanically
translated and the hardest part is
literally generating correct C syntax
and supporting closures but closure is a
highly dynamic language for example
simply to add two numbers together
various forms of distraction are
involved plus itself takes a variable
number of arguments and the arguments
can be of type float int etc and they
can be mixed requiring a numeric tower
which converts between them and runtime
even truthiness itself has specific
semantics in closure it's and not nil
not false but closure is not a slow
language and if it's doing that much
work why is that on all three of its
commonly used platforms the JVM dotnet
and JavaScript the various JavaScript
platforms it's really standing on the
shoulders of giants the JIT compilers
provided by these platforms are
extremely fast they they monitor the
execution of the program in real time
with real data flowing through it and
then optimize the code for the usage
patterns that you're actually
experiencing at runtime and if you've
ever been frustrated by a task that
you're working on spirit thought for the
engineers of those kids they take
unoptimized code that is currently
running and hot-swap in an optimized
version on stack then when there's an
error in the optimized code they
determine where it would have happened
in the D optimized code and then pretend
that it happened there instead to give
you your sack traces etc it's honestly a
miracle that this stuff works at all and
this is an example of some closure of
boilerplate functions written in C so
we've got first and cons and abs is a
struct which holds the boxed data types
so the compilers method is to analyze
enclosure codes statically in order to
make inferences about its behavior is
very complicated but it's not difficult
at all to analyze closure code that's
running as you would do in a jet but we
have closure specs generators and we can
generate sample data which we can feed
through our program ahead of time
observing the execution of our program
furthermore if we treat the behavior of
the program under this generator data as
exhaustive we can perform a significant
enlightening and allied any paths not
taken now this differs from a standard
compiler which built-in guards so if the
program data under execution changes the
behavior remains correct the performance
just suffers however in the pursuit of
high-end performance the more extreme
approach is required the nice thing
about immutability is we can actually
bail out of the execution of our code
and D optimize at any time unlike the
the optimization performed by a standard
yet during execution where it's got a
performance bookkeeping to reconcile the
two versions of the code with the system
like this you can just bail out as far
as you'd like and then rerun the code it
doesn't matter if you run it twice it's
all immutable and if we find an error in
our in our optimization we can actually
run an optimized version of the code
side-by-side with a D optimized version
of the code and then strip away the
optimizations until we actually find the
place at which the error creeps in and
because we're running the compilation
ahead of time we can afford to take
significantly longer amount of time
analyzing the functions it's all
immutable data as well you're optimizing
a piece of code against a spec both of
which are immutable and that means that
you can cache the results indefinitely
so even though the performance is slow
you don't pay that penalty over and over
and over again so let's take an example
a closer look at adding two numbers
together this is the source code for a
reimplementation of Plus that takes a
variable number of arguments and you can
see here that in the case statement it
is branching depending on the number of
arguments that you're passing to plus of
course if you call plus without any
arguments it just returns 0 if you call
it with one argument it just returns
that argument and two then it will
perform the plus and then more than two
it will recursively call itself it will
recursively recursively call itself and
then and then call the internal plus on
the results now this is the numeric
tower which is the version of plus that
operates on just two arguments now I've
sort of left off whole sections because
in order to just support a float and an
integer it would be four times longer
than this and it's just too long for the
slide and so this is a single-story
numeric tower which I don't actually
think is a thing but you probably get
the idea of how it works so let's try
optimizing this code so we're optimizing
plus where the odds that we're passing
in is a tuple of integer and integer the
first thing you'll notice is that count
hogs no matter what data is generated
and passed in is always going to be two
and so we can actually replace count
hogs in the function body with just the
constant and then furthermore because
case who is it's always going to return
the branch of to the case is now a
constant we can actually strip away the
rest of the case statement just leaving
the branch that deals with two and so
our plus has been simplified has been
specialized to
handle the case of passing in two
arguments only now let's look at the
numeric Tower which is being called by
our plus an a is an integer and B is an
integer now of course this is that
there's only one branch here and so it's
a bit it's a bit silly to do this but
the same principle would apply if you
had full numeric tower with lots of
branches but essentially integer a the
call to integer a and the call to
integer B will always return true no
matter what jeanna data is generated and
so we just replace those function calls
with the constants and then we propagate
it through and of course con true is
always going to return that branch as
opposed to nil and so we're just
stripping away the code and then we in
line plus 2 into our original plus 2 get
this and what this does is it simply
explicitly gets the first of the array
of arguments unboxes it it gets the
second of the array of arguments unboxes
that it calls a native plus intent on it
which is sort of our native plus in C
that would just be plus and then it
boxes the integer again after it's been
calculated in order to become a closure
value again so we'd also like to
specialize the fact that it only takes
two arguments we can actually transform
the code to take explicit arguments
because we all know it's only two and to
do this we replace args the variable
arguments with just two parameters and
then recreate our arguments in the body
of the code so we've got first X Y and
second X Y and then simple pattern
matching will alight that and then there
we have it it's essentially a an almost
and native plus function so that's how
the process works but let's take a look
at it
exactly how it happens in the compiler
itself so there are there are few more
parts to it and this is a bit more of a
complete implementation we've got + + 2
+ + an implementation of tricky for
closure closures version of truthiness
and we can actually optimize by sliding
this slider to see the various stages of
optimization that take place now this is
actually this is actually optimizing to
make it a bit more interesting this is
optimizing it to three arguments called
- + and the reason for that is just so
that the recursion of + into itself
takes place and we can see that it's
alighted as well and so first we see
that the okay this is it might be a bit
difficult to read and so don't don't pay
too close of attention I'm going to
speed through but essentially we
specialize the code simplify it and then
in line in a few passes which start at
the start makes it a lot longer and then
as it's simplified it gets cleaned up
until we get to this which is similar to
what I showed you before except it
handles three arguments not two and then
we take the step of explicitly defining
our arguments and then the last step is
to unbox those arguments and so instead
of taking a closure integer or three
closure integers and then returning a
closure integer we're actually going to
take a unboxed integer and return an
unboxed integer and all that involves is
stripping away the boxes essentially and
so this is a version of closures plus
that takes three integers and returns an
integer and it operates entirely on
native objects so let's take a look at
the C output of the compiler now this is
the naive way the naive way of adding
three numbers in closure when translated
to C this is what it takes
closures compiler isn't that bad I mean
it does some optimizations of a siren
clearly because the program's run but
this is about a thousand times slower
than just adding three numbers in C but
it works it works fine and at each stage
of the optimization it actually receipt
code that is defined here it runs it's
always going to run the question is how
quickly and so we're performing the same
optimizations on the code with the same
slider and so you can kind of get us get
a picture of the other side of it you
can see when it in lines there it's just
a mass of code but then it gets
specialized away with the same process
and then we end up with the same the
same result that we had in the closure
slide this is the seaside where it's
taking a list of arguments its
extracting the the first three arguments
from the list even though we know it's a
list it's not built that way
it unboxes them adds them together with
seasoned native addition function which
is just the plus sign and then we make
an integer out of it now we can also
perform the explicit arguments passed
that we did before in which case we're
just passing in three closure objects we
unbox them add them and make a new
integer and then when we when we do the
unboxing pass onto it then we just have
exactly what you would have written in C
but it's important to note that this
process respects closures semantics if
instead of if closures truthiness
instead of and not nil not false we're
instead for some crazy reason and not
nil not false not seven that would still
be respected in this output process I
suspect that the result would be the
same but if it were not this code would
actually be different closures semantics
are in fact being run at compile time
not at run time and that is what gives
us our speed
so so far so far I haven't really showed
you anything that you couldn't do with a
static compiler easily but where things
get interesting is sort of the
higher-order optimizations so let's just
take the naive implementation of the
fibonacci sequence I think you're all
pretty familiar with it and of course it
runs in exponential time which is it's
it's it's a lot because if you run fib
100 on the command line it will never
never get there but then you wrap in fib
10 and it's nearly instantaneous now the
traditional way to optimize Fibonacci is
of course to memorize it just like so so
it's exactly the same function body the
only difference is that that Nimoy's
will remember the result to each call of
Fibonacci and so they're not repeated
and it just returns the value from cache
when it exists and that transforms the
algorithm into an into a linear time
which is obviously a very big jump but
there's an interesting property of the
Fibonacci sequence so say we're passing
an integer that we know to be at least
equal to 5 and we specialized our
function to that the first thing we can
notice is that this branch of is n less
than 2 well we know that's not true so
we can strip away that branch entirely
it becomes false and then it's stripped
away but then we can actually inline the
next level of the Fibonacci sequence
so fib take n 1 we can actually take the
next level the next call to Fibonacci
and inline it into our body which
becomes this which we can then simplify
to this and so even in effect we've
skipped a level of the Fibonacci
function
so we've skipped a level of the
Fibonacci function and so if you run if
you run this on code one second so
obviously here we have to recurse into
our original Fibonacci function for this
optimization to remain valid because
eventually n is going to be equal to
zero
but essentially we're skipping a level
of the Fibonacci sequence and there's no
limit to how far you can take this
optimization the inlining can continue
as long as your spec allows it and so if
you know that X is greater than eight
you can actually skip eight levels down
if X is greater than sixteen you can
skip sixteen levels down if it's greater
than thirty-two you can skip 32 levels
down and you can actually build a
version of the Fibonacci function
completely automatically by segmenting
it into these little exponential boxes
where we're going is X greater than
eight or 16 etc and then we build this
I'm a bit sorry about the about the
wrapping but essentially it's a version
of the Fibonacci function that is now
linear in time complexity because when
our value is large it's skipping a lot
of levels and when it's smaller
obviously it performs the same it's now
linear so that's the same time
complexity as memorized but we can
actually combine the two optimizations
and so if we memorize that new Fibonacci
sequence fib is now o log in and that is
a completely automatic generalizable
optimization there's nothing specific to
Fibonacci about it it'll be valid for
some code it'll be useful for some code
I should say most code no but if we can
maintain a library of these sorts of
optimizations techniques that we can try
and code then we can test them all and
essentially see
and that's not all because obviously if
you want to support Fibonacci and say
we've got long like arbitrary sized
integers our con statement is going to
get very very large obviously each
doubling of the value that we want to
support is going to add a new branch to
the concert but we can even further than
that
turn the cond into a binary search in
order to avoid the cost of that so dear
audience this is where I've failed you
this was meant to be the part of the
talk where I actually demo outputting to
Java instead of C and then using a
dynamic class loader at runtime to run
our optimized Fibonacci but there were
some technical issues but so you're just
going to have to take my word for it
that it works I will release the open
source version of assume but it is
completely possible to perform the same
the same optimization progress it is
completely possible to perform the same
optimization process output Java and
then run it in your wrapper in real time
but does take a little while to compile
so can you build a production compiler
like this I've been doing it for six
months I've been working on this for six
months and I'm still not sure to be
perfectly honest but I think at the very
least it's an interesting way to
approach the problem so one advantage of
the technique is that it is so simple to
do
if you've if you've ever tried to do
some optimization based on type
inference or or data flows like it is it
is seriously difficult with this the
process is just trivial to do we can
also allied abstractions that can't even
run on the machine that we're targeting
for instance if you would like to
program GPU shaders but you would like
to
program them as if they had hashmaps but
in a sensible way so they can still be
transformed into structs at runtime you
can do that and you can even use for
instance keys and like zip map on them
another advantage is that the the work
can be effectively memorized and the and
that work shared so how many times has
minecraft being recompiled it must be in
the billions and it's performing the
same work over and over and over again
but what if with each recompilation
instead of recompiling minecraft we
added to the global body of knowledge of
how to run minecraft very quickly I
think that would be an interesting
approach to take and it also allows for
building multiple specialized versions
of the same code you can perform some
analysis of array processing and you can
figure out that if it's greater than a
thousand elements long it's beneficial
to spawn some new threads to process it
using multiple cores but if it's short
then it's not beneficial and so you can
actually build both versions of that
function at compile time and then
dispatch them at runtime so what are the
disadvantages so the compile times are
really quite long if you're doing
obviously like I said before you can
memorize the stages but when you're
doing new work it does actually take
quite some time and the safety you want
it to be with are your specs going to
generate the right values the longer you
have to run it plus takes about a minute
to compile from scratch and the
fibonacci sequence that I showed earlier
where where it does the full
optimizations and inlines into itself
takes about 15 minutes to compile to
java
quite a lot of work it's also difficult
to optimize between pieces of the
program the ideal case is that you sort
of section off chunks purely functional
chunks of a program and then you
optimize them individually but
if you want to sort of optimize between
them it is you you sort of have to
combine them into one big chunk and then
run the optimizations over both of them
at the same time although that said you
can actually perform from some pretty
nice optimizations between them strip
away a lot of code and so it's a
double-edged sword there and of course
the elephant in the room is sort of the
probabilistic nature of the compiler the
data generated by the specs needs to be
exhaustive if you've got a rare edge
case that's never tripped you could be
in the dark until production until your
program behavior is incorrect and
there's sort of a trade-off between how
deep you let your generated values run
down into the program because at some
point you have to stop in further spec
and then regenerate new values in order
to reach even further down because
otherwise you're just not going to get
the program coverage that you otherwise
would have had and this losses potential
relationships in the test data that the
inference of the spec may not understand
but I sort of see three main ways to
mitigate the probabilistic nature of it
so the first is fuzzing the program if
we take a closure compiler that a
closure eval function that has hooks
into it so it can actually analyze the
the code as is written we can take the
initial generated data and mutate it
genetically until it reaches all of the
branches of the code and where we see
novelty in the in the program when
there's a new piece of input data that
reaches a new part of the program we can
then further work on regenerating that
input data to see if we can even explore
further avenues the second is a hybrid
approach which maintains the D optimized
path just like in a JIT
and it only elides when a proper type R
a type checker proves that it's safe but
this would get rid of the ability to for
example use hash maps and then
get GPU shaders for instance and thirdly
maintaining a shared volume body of
knowledge about our code and to me this
is the most interesting one we're
performing all of this analysis on the
code and why can't we sort of keep this
body of knowledge around that lists
properties that are invariant of the
code for instance if you pass in an
integer 2 plus it will return that same
integer if you pass in two integers 2
plus it will return an integer and if
these properties are ever found to be
violated then we can share that
knowledge to the central database and
this made me feel a bit better about the
whole thing because each individual
error made by the compiler in theory
would only happen once and it sort of
decides to pick method isn't it so I
think it's an interesting Avenue to take
any questions it's looks like it looks
like we've got some time ok so the
question was the specs written by humans
could you create a spec that causes the
program to look forever um I think the
answer is yes and of course you've got
the halting problem there and that would
be something to keep in mind any other
questions
so could you talk up a bit
so the question is when you modify Plus
to take unboxed integers you also have
to change the course site and that's
absolutely correct that's part of the
process and it helps when you when
you've when you're calling a lot of code
that is constantly boxing and unboxing
integers essentially the purpose of it
is to just maintain your unboxed
integers and get rid of the wasteful and
unnecessary boxing and unboxing any
other questions
oh that looks like it's about it thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>