<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mike Anderson - Enter the Matrix | Coder Coacher - Coaching Coders</title><meta content="Mike Anderson - Enter the Matrix - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/ClojureTV/">ClojureTV</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mike Anderson - Enter the Matrix</b></h2><h5 class="post__date">2014-01-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_h9TLJtjSJo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'm going to be talking about
caught up matrix I think it's quite
fitting that I'm talking about it today
the closure cons
because this put it actually came about
as a direct result of many conversations
I had at last year's closure cons and
the focus of these discussions very much
about how we could make numerical
computing better enclosure so the
solution I'd been working on over the
past year along with quite a few other
collaborators is caught up matrix which
offers array programming as a language
extension to closure and when I say
language extension it is of course in
the sense that closure seems to have
this great ability to absorb new
paradigms just by plugging in new
libraries so we already had the great
pure functional programming features
from like foreign languages like Haskell
in closure core we've got things like
core logic adding logic programming we
more recently you have call that they
sync adding you know ideas from
communicating sequential processes and
process algebra and Cortot matrix is
designed very much in the same way is to
provide away programming capabilities
and if we want to trace the route of
away programming we can go back all this
all the way to this very interesting
language called APL APL sorry aap there
so that's the paradigms APL is this very
interesting language of the venerable
history it was invented in 1957 by Ken
Iverson was particularly interesting it
wasn't actually invented as a
programming language at all it was
actually invented as a notation for
describing mathematical operations but
it turned out it could be turned into a
programming language and got implemented
at IBM in the early 60s I love the fact
that it has its own keyboard because it
has its all these custom symbols that
you need to write any code in and it's
all inspired by mathematical notation
and you do get some crazy code out of it
this this code behind me here is is
Conway's Game of Life implemented in APL
and you can see it is really concise
there was incredible expressive power in
just a few symbols now you can't really
read it but you know it still apparently
works so this might seem like a bit of a
quaint dinosaur of a language nowadays
but I think it actually has quite a lot
to teach us and indeed
there's been a bit of a renaissance in a
way programming over the past few years
and I think this is just natural due to
the increasing importance of data
science and numerical computing and but
we see things like our which is this
amazing statistical environment a lot of
statisticians using it for big data
analytics and you've got numpy which is
a way programming implementers library
for python you've even got julia which
is a completely new language which is
very much built around away programming
principles so there's clearly something
valuable in all of this and what I
wanted to do is bring away programming
into closure three main reasons I mean
firstly there's this big data science
focus in enclosure a lot of people are
doing very interesting and innovative
things with data so it's a natural it's
a natural fit for this kind of paradigm
secondly closure provides this great
platform to build upon I mean I always
wonder why you should introduce a whole
new stack a whole new programming
language just to get it just to get
access to a new to a new paradigm you
know we shouldn't have to give up the
advantages of having a good general
purpose language and a powerful platform
and enclosure gives us that platform you
know it has the advantages of the JVM it
has a lot of the different libraries you
might need and it has a great community
so I want to do my data science
enclosure and I think College is
compelling for many philosophical
reasons you know and concurrency
mutability functional programming and a
lot of these philosophical reasons are
actually quite compatible with how you
can do a rade programming so I think the
way program is actually quite a natural
fit so I'm going to talk about core
matrix today with three different lenses
first I'm going to talk about the
abstraction what are these n dimensional
arrays that I'm talking about secondly
I'll talk about the API which is really
the functions that the operations you
can perform in a ways and finally I'll
talk about the implementation how do you
make all of this work so let's get
started talking a little bit about the
the abstraction of arrays and I'm going
to start off with one of my favorite
quotes from Alan Perlis it is better to
have a hundred functions operate on one
data structure than ten functions on ten
data structures I've included this
because it has a lot of wisdom it also
has a very major mistake of call
we should be talking about an
abstraction here rather than a concrete
data structure and a great example of
this kind of approach is the sequence
abstraction enclosure itself and there's
literally hundreds of functions
enclosures our core that work on
sequences and because all of the
functions produce and consume sequences
you can plug them together you can build
these data processing pipelines and it's
more than just a closed about core API
you can use the same abstraction on your
own code it also extends to libraries
and this is power that the key to
building systems using simple composable
component components is about working
from shared abstractions and we've taken
this principle very much to heart in in
core dot matrix our abstraction of
courses is is the array itself and the
rest of core metal matrix is really
about giving you a lot of different
functions and operations that you can
use on the array abstraction so what is
in a way it's basically just a grid of
values have rectangular grid of values
with with it with varying number of
dimensions now this concept should be
familiar to anyone tube to most
programmers most languages have some
concept of an indexed array data
structure one dimensional away you
typically called a vector a two
dimensional array would be a matrix and
those are terms we borrow for
mathematics you can have any
dimensionality you want arbitrary n
dimensional arrays but they all share a
you know effectively the same properties
they all have dimensions which are
ordered and indexed ways of organizing
information the dimensions all have
sizes and together the size of each
dimension to find a shape of the array
which is kind of the structure of the
data and each of the each of the
elements of the array is some kind of
some kind of a value now normally those
are going to be numbers but there's no
reason why you can't have strings or
keywords it can be any any kind of value
can be in it can be in an array so why
are these why these are ways interesting
I think the answer is that arrays are
actually a fundamental design pattern
that lets you express facts about
relationships between different sets of
objects and you have to think of each
dimension in an array as corresponding
to a different set and then if you do
that every element of the array is a
fact about the relationship between two
members of that set covering all the
different combinations and in some ways
the action of looking up an element of a
way in an n-dimensional array is very
much similar to calling an n-dimensional
soja at our TN function to get a result
back so there's a sort of analogue
between arrays and functions that you
can draw and this is an important
insight you can write pretty much
anything that you could write with
functions using arrays instead now this
may or may not be a good idea in
practice but there are certainly many
situations where arrays actually might
be preferable to functions and one thing
is that you've got all this data pre
compute it in an array it's as if you
memorized every possible function call
to a function and you've got very very
fast order one access and this is
obviously giving up space you're using
memory to store this potentially quite
large array but you don't have to
recompute the values as you need them
and this is probably going to be a net
gain if you're going to be accessing
those values many many times and this is
actually a common case if you're doing
data analysis and running lots of enough
analytics over the same the same data
sets and secondly arrays give you the
opportunity to have efficient
computation with bulk operations so
rather than commuting each element
individually through a function call you
do these bulk operations to compose
entire rays together those can take
advantage of optimized code they can
even take advantage of specialized
Hardware concurrency distribution is
that there's a lot of potential for
making this run extremely fast involved
and finally it's actually a very
data-driven representation you know the
result of a computer way computation is
just data it's easy to serialize it's
easy to visualize if you want to change
a single element you know it's quite
hard to do that to a function but
because you'd have to add a special case
in a way you just change it you can just
change an element so it's a very
flexible way of forming a lot of the
data oriented work it also brings you
quite a lot of expressivity to think
about programming with arrays so this is
this is an example of
of some Java to add together a couple of
three dimensional arrays basically it's
a big messy nested loop there's lots of
index fiddling going on it's quite hard
to read but more importantly it's very
very easy to make a mistake in this kind
of code you can have off by one hours
you can get indexes the wrong way around
I've lost count of the number of times
I've got an eye in a Jace watt round and
I've got the wrong answer but I don't
know why it takes it takes hours debug
debug that kind of thing so it's not
very nice the way you typically do it in
close U is actually quite a lot nicer
you know you can use the mapping kind of
functions which avoid the need for the
indexes but you still have this sort of
three level nested structure that has to
mirror the structure of the array that
you're dealing with now in core matrix
it's about as simple as you can get you
just add two arrays together you know
that that's it and this highlights a key
principle of array programming you know
what we're really trying to do is we're
trying to generalize operations on
regular scalar values to
multi-dimensional to these to this to
all this multi-dimensional data ok so
let's talk a little about the API that
we can use to do some of this so we've
defined our array abstraction this
section is all about functions you can
apply to aways and one thing to just
know to the start is a ways can be
represented using nets nested closure
vectors now this is not necessarily the
best the best way for doing heavier way
computation but it's very convenient
very idiomatic enclosure so I'll use
this as a way of as a way of just
demonstrating the API the first thing
you need to do is you want to create an
array the array function does this you
can construct you know 1d a 1d a vector
from any any sequence or you can use
nested sequences to put to produce you
know high dimensional arrays so this is
an easy way of constructing arrays and
it fits very nicely with a lot of
closures sequence functions once you've
gotten away there's probably some
properties you may be interested in
shape is probably the most fundamental
attribute of an array and this is just a
vector that describes size
each dimension so the shape of a three
by two matrix is just three to regular
values have no shape just an ill shape
closely related to shape is the concept
of dimensionality dimensionality is just
a number of dimensions so a matrix has
two dimensions a vector has one it's
exactly equal to the length of the shape
vector and regular regular values we
give zero dimensionality and you can
test if something is actually in a way
or not or if it's a scalar so a scalar
zero dimensional value often it doesn't
matter you know because you can actually
think of a scalar sort of like a special
case of an array that doesn't have any
dimensions and most of the chordal
matrix operations allow you to use a
scalar as a zero dimensional array you
obviously need to access elements so the
EM gate function gives you indexed
lookup in an array you just need to
provide one index for each dimension so
that's fairly simple bit more
interesting is is slicing at access so
here I'm taking a slice of a mate of a
matrix which is the row at index 1 along
the first by mention now what's very
interesting here is that a slice of a 2d
matrix is a 1d vector so a slice of an
array is an array arrays are
compositions of arrays now this fact
makes me really really happy because to
me it's it's exciting that you've got a
good abstraction if it can be defined
recursively in terms of itself so that
to me this this this this is giving me a
good feeling about arrays as a construct
this leads quite naturally to you know
the slices function which is just going
to cut an array into a sequence of its
slices and it's very common to want to
do this imagine each each row of a
matrix
you've got a big matrix where each row
represents a different data sample
you're quite likely to want to see all
the slices all the rows in sequence in
order to do some data processing you
know you can do things then like you
know add together all the slices to like
you know get results and do some
aggregate calculation
you can actually slice on in any
dimension but you know normally
conventionally you'd be slicing on the
first dimension brings us on to the
operators so we define ray versions of
all the all the standard mathematical
operators these actually use the same
names as closure core so you need to
we've put them in a separate namespace
closure doc or docx matrix dot operators
which you need to import if you want to
use these and the good news is they
still work the same way on regular
numbers so they won't break anything if
you if you import it so you've got
addition it's just adding the elements
corresponding elements in different
arrays you've got multiplication works
the same way you get an error if you if
you try and do an operator on arrays of
different sizes I think this is this is
the right answer because you've almost
certainly done something wrong this is
the array programming equivalent of a
type error if you're trying to do
something with incompatible shapes now
the last examples a bit interesting here
here we're dividing a vector by a number
you know and intuitively the results
seem sensible we're dividing each of the
element by ten but here I've just broken
my previous rule you know these are not
arrays with the same shape so what's
actually going on here well we have this
feature called broadcasting and which
allows a lower dimensional away to be
treated as if it was a higher
dimensional array and logically what's
going to happen here is we duplicate the
value to fill up the complete array
shape that it needs in order to make the
operation complete and then you
calculate the result in a normal way
just element by element it turns out
this is quite useful otherwise you'd be
spending a lot of time you know
reshaping and resizing arrays to to make
that to make the same shape you can also
apply broadcasting to two arrays
themselves so this disk idea generalizes
and the semantics of the same we know if
you're if you're adding a matrix to a
vector well we're just going to
duplicate the vector so it fills out the
shape of the matrix so this is this is
just a convenience feature really
let's talk about some higher-order
functions so here's two of my favorite
closure functions map and reduce and
insec of course these are just the
regular closure functions probably
everyone's everyone's use these many
times what are the interesting
observations about array programming is
you can sort of see it as a
generalization of the sequence
abstraction to multiple dimensions so
it's probably not too surprising then
that the sequence functions in closure
often have a very nice away programming
equivalent so e map is the equivalent of
map is going to just apply one function
to all of the all of the elements of an
array
the key difference here is it actually
preserves the structure for a way so if
you if you do an e map over two by two
matrix you're going to get a two by two
matrix back irreducible introduced over
the elements of it away so it's going to
be trait over all all the elements of
the array and you get e SEC as well
which is just going to return all of the
all the values of an array in order so
you can use these to start composing
pipelines of array functions we have
some specialized matrix constructors you
might want the big 0 matrix if you're
going to start some processing you can
define an identity matrix you can define
a permutation matrix these are very
useful in various kinds of mere
mathematical uses and sort of matrix
algorithms you can also do
transformations on arrays the most
obvious case is a is transpose and what
this is going to do is going to swap the
rows and the columns of a matrix more
generally what transpose is doing is
it's reversing the order of the
dimensions and therefore reversing or
the order of all the index all the
indexes you use where the analog is you
know you said of a relationship from a
set a to set B you're getting back as
the same relationship with set B to a
set as a set a you have matrix
multiplication so this is defined
exactly like in mathematics you know
what we're going to do is we're going to
take a row from one matrix a column from
another matrix we're going to multiply
the corresponding elements you add them
all together and
creates one of the elements in the
result matrix and you're going to do
that for every every possible
permutation and this is a key function
in many mathematical applications of
away programming so for example you can
use this to implement some some some
nice geometric features so this little
what function is producing a two by two
matrix that represents a rotation in a
Euclidean vector space you then apply
the rotation to a vector just using
regular matrix multiplication so the mo
what one over eight of the vector three
four is going to rotate the vector three
four one eighth of a turn around the
origin so now this is this is the kind
of way you can build up geometric
features note the use of the circle
constant tau which is defined as two
times pi it's also very much more
elegant than PI's if anyone wants to
research that I strongly recommend
reading the Tau manifesto it's very it's
very interesting okay so I will do a
quick demo and what I'm going to do is
implemented well PageRank algorithm with
quarter matrix no pay tract is a pretty
simple but very valuable algorithm that
that works out the popularity of a page
effectively by simulating how a
population of users would move between
different sites by clicking on clicking
on links so let me just switch over to a
sits over to Apple here
so we can start with a start with the
matrix of all the links this is this is
effectively how many links are going
from each of the each of each of the
each of ten pages to the same set of
pages so here you've got the sixth page
has two outbound links going to the
first page you can see that the fourth
page has a lot of links going to it so
you'd expect that the fourth page would
get quite a high PageRank
there which we can use to check later
but that's basically the data that we're
going to be working with we can define
the the number of what the number of
actual got ten sites here so it just as
a convenience to finding in the first
function we're going to do is a
proportions function now because we're
trying to model the movement of a
population or interested in what
percentage of the population is going to
move between each site so you want to
normalize these vectors and all we need
to do here is we need to divide the
vector by the sum of all of the elements
and that's going to normalize it to it
to a size of one so if I just quickly
test that with a with a small vector
here you can see it's it's divided
divided that out and the sum of that
yeah and so the summer that's going to
be one you can see it's using closures
numeric tower so you can use rationals
and all of these all of these kind of
things as well so what we could then use
that is to work out where the outbound
visitors are going rather than buy a
number of links but as a proportion of
as a proportion of the links so we're
going to define an outbound matrix which
is just mapping the proportions function
over all of the rows of the original
links matrix see what that looks like so
we can see we've got not-so-nice ratios
there that doesn't print too nicely so
there's an extra function called p.m.
which is print matrix with just formats
this a bit for you so you can see you've
got a nicer you've got a nice nice
matrix of all these values and every one
of those rows should be adding up to one
because we've normalized it now actually
we're interested in the inbound link
so what we going to do is just use a
transpose transpose of that matrix to
get the where the inbound links are
coming from and finally we're going to
just define an extra constant here for
our simulation we're going to say that
85% of the time a users going to click
that click through and the 15% of the
time they're gonna get bored and go home
and they're going to get replace by a
new user who's gonna arrive at some some
random page so we're gonna run this
simple simulation with like an iterative
message so we need a we need an initial
State
so we're gonna say okay there's a
million users for each each site
initially or visiting each site
initially there's a bit too big so let's
let's take the proportions again and
save let's take report what's the
proportion of users on each site and
there we've got a we've got a vector
with you know 10% of the users in each
in each place
and then we're going to say well what's
gonna happen in each step well in each
step there's going to be a portion which
click through and they're what we're
going to do is we just do a matrix
multiplication of the inbound matrix
with the state yeah which is affecting
going to be working out that adding
working out and adding up the
proportions of all of the people who are
coming into each site from all of the
other sites so you can see there's a big
loop actually being implemented in just
in just one matrix operation and then
the other the other proportion of the
users are going to drop off and what
we're going to do is we're going to get
some more users back according to the
initial state according to the sort of
starting distribution so that's going to
be a step of our simulation if I do one
step on the initial state you can see
that a bunch of these moves as users
have moved around and you've got you've
got you've got people in different
places and so that's expected the fourth
website has got nought there's got 31.6%
of the users so that's obviously
benefiting from a lot of inbound links
of course we want to run it as a
simulation so we want to iterate these
steps so we can just use the regular
closure iterate function to run the step
on lots of or lots of states so the
first the first one of the page ranks
should be just the initial state do you
run four iterations
you know we've got some some different
states and the nice thing about this
algorithm is you can actually even
actually prove that it will always
converge to some kind of steady state if
you do enough steps so ten steps you've
got this distribution hundred steps
you've got this distribution and I think
it's probably it's probably converged by
this point they can test that just by
subtracting two states from each other
and it should all be zeros so yeah so
after 100 you do another 200 steps to
300 steps nothing's changing so that's
that appears to have converged nicely if
you want to visualize this data and all
the convergence process you can do
things like make an array out of an out
of some of the steps in sequence so I
can take an array of the first eight
iterations and that should there should
show nicely how some of the convergent
is actually happening if you if you look
down down these columns now the other
way of course you can calculate this is
if you if you do a bit of mathematics
there's a way of calculating exactly
where it's going to converge to
algebraically so I won't go through the
details of the maths here but that's
basically the same infinite sequence of
steps translated into a matrix formula
so it's just a couple of a couple of
lines of code and if you do the if I
calculate the sort of that I think I get
exactly the same distribution but that's
effectively calculating the infinite end
State if you did an infinite number of
iterations so it's just a good example
of you know the kind of manipulations
and things that you can use this for and
you know you can implement some quite
complex algorithms with a relatively
small number of lines of code and
noticed nowhere here was I playing with
indexes or iterations or loops it all
falls naturally out of the array array
programming paradigm
so you probably noticed that everything
in the demo there was actually using
pure functions it was all immutable
stage it was all nice idiomatic closure
which brings me onto a I guess one of
the really thorny dilemmas here which is
are we actually going to allow
mutability in a raise and I guess I'd
really like to say no to this question
but I think if we want to aspire to be a
general purpose away pogoing solution I
think actually some mutability actually
is necessary there are some no there are
some trade-offs here basically
mutability is horrible it's bad for all
of the reasons that we knew you should
be avoiding it as much as you can but
you know it does make a difference to
performance and performance does matter
for numerical work and I think
controlled mutability is okay if you use
it sensibly so if you're doing if you're
a librarian from implementer you might
need it to implement a fast algorithm
that's the kind of place where you might
consider using it so when you're
constructing a value you might want to
mutate it and then once it's constructed
you want to make it immutable and and
and not change it anymore you know it is
it is quite significant I did a bunch of
benchmarking on this kind of thing
seemed to be a go about for time
performance benefit if you use a mutable
edition to accumulate values in a ways
so you know that usually wouldn't be
worth it but it happens to be the
bottleneck in a big numerical
application you know you kind of have to
say well we'd actually like to be able
to to do that
the syntax for mutability I mean
basically we've got equivalents to the
functions we just add a exclamation mark
or a bang at the end so if you can try
and you can just you can do and bang
will add to the the first argument if
you obviously if you try it out on a
closure vector it's immutable so you're
going to get an exception but there's a
function which lets the mutable function
lets you convert coerce and a very
valuable that I saw in a wave an array
to a mutable format so they'll give you
back a different format of a way that
actually is mutable and once you've got
like a vector to here you can add one to
it and it'll update each of the elements
accordingly so let's get on to a bit
about the implementation
and you know surely you know someone's
created a lot of good libraries of you
know we can use to make a wrap up and
build a nice close your API so I
actually had to look around at the
different libraries and one of the
things you find in them in the matrix
space is just the sheer number of
different matrix like these and these
are just the java matrix libraries that
I could find I'm sure there's another
ten of them hiding around somewhere and
there's there's hundreds of these things
and you know she's sort of to say you
know this is insane why there's so many
matrix live is you know why is everyone
reinventing the wheel all the time hey
you know surely you know you just build
one and it should work well it turns out
there's lots of trade-offs here and
different people want different things
from their matrix implementations and
this is what's driving all of the
different permutations of libraries I
think so some people want to be able to
use you know optimized native code other
people want to be pure JVM
some people want mutability some people
want immutability you know some people
want all the full multi dimension
dimensional arrays some people happy
just with 2d matrices so there's all
these different choices that affect the
complexity of the implementation and you
know how you can make it work and you
know just as a very simple example of
some of these trade-offs you know what's
the best data structure what's the best
implementation for a length 50 range
vector so nothing very complicated here
you might say okay well it's very
natural that's a closure vector you know
just with with with 50 elements in it
you know that's good it's nice and it's
nice and clean but let's say you wanted
it to be mutable you actually want to be
able to do some accumulation in this
away well the closer vector is obviously
not going to work there you might say
well in fact I'm gonna get some better
performance if I directly drop down and
level and use a Java double away yeah no
that's a mutable implementation of this
vector if you go the other way you might
get really clever and say well look it's
a range vector I want it to be immutable
I don't need to stall of these values I
can just compute these from start in an
end on a range so you could define a
custom type which can simulate that away
very efficiently and it's immutable so
you know you don't you know you there's
no problem with using that as that's a
perfectly good
by implementation which is to start in
an end or you might say well I want to
use some native libraries so I'm gonna
have some some wrapper class something
like J blasts which wraps the native
libraries there because I want to use it
a whole bunch of native computations so
there's a whole bunch of different ways
that you can do this very very simple
vector so the point is there's never
going to be a perfect answer when you're
choosing a concrete datatype to
implement an abstraction there's always
going to be inherent advantages of
different approaches so you know what
are we going to do what how are we gonna
choose a matrix implementation that you
know keeps everyone happy well luckily
we sort of have this secret weapon and I
think this is actually one of the things
that really distinguishes caught-up
matrix for most other array programming
systems and and it means that we can use
any underlying implementation that we
like and of course this is closure
protocols so here's an example you have
a a summable protocol a simple protocol
that lets you compute the sum of all the
values of the array and you know there's
three important things about protocols
they define an abstract interface which
is what we need to implement you know
the array abstraction they support open
extension which is the ability to extend
the protocol the classes that don't know
anything about core dot matrix or don't
know anything about the protocols that
we define which is what enables us to
extend it to all of these different java
matrix libraries and the third feature
is very fast polymorphic dispatch which
is what you need for performance and we
do a bunch of benchmarking around this
and protocols are really the sweet spot
for polymorphism here they're nearly as
fast as just straight function calls but
they have this open extension capability
that normal function calls don't have
multi methods are great they're very
general purpose and they also have open
extension but they're much slower they
had quite a bit of function call
overhead
which you know doesn't matter if you're
doing a function call that's going to do
a you know onel a matrix of a million
elements but if you're doing relatively
small matrices that actually could
actually dominate your runtime the
typical Cortot matrix call path you have
user code calling a function like e some
that's calling into the core dot matrix
API which you know defines
the API function but that delegates
effectively pretty much straight away to
a protocol call and then the
implementation code is extending that
protocol to a specific type and then
implementing whatever optimized way of
doing the computation you need now you
think that probably maybe a lot of
protocol so if you want to implement if
there's a hundred different API
functions and that you know doesn't that
make it a bit hard to do these it's true
there are quite a lot of protocols but
the good thing is nearly all of the
protocols are actually optional it's
very easy to make a working core dot
matrix implementation you only need the
entire API is gonna work if you just
implement four or five of these of these
protocols and that they're the obvious
things yeah how do you get an element
for an indexed element how do you query
the shape of an array
those are like fundamental features that
you need and everything else can be
implemented on top of that so a caught
up matrix implementation you can start
with the mandatory protocols but then
just add other protocols over time which
will improve performance for several
operations that you you want to optimize
so hopefully this is a smooth
development path for implementations to
improve over time of course we can do
this through a trick which is default
implementations so what we do is we
create a default implementation for each
protocol that gets called if you haven't
extended it otherwise so we're extending
a protocol we have an implementation for
any number so the sum of the sum of a
number is just it's just itself and then
we have an implementation for an
arbitrary object and there we're just
delegating to a lower level protocol
call you know that the sum of an
elements are just reducing all the
elements with with plus so that's a
generalized way of writing you know the
add the elem implement in the elements
um if you actually want to extend it so
let's say you wanted to do P summable
for Java double away you just extend it
to your particular class and there's
some horrible way of specifying what to
double away is in closure and then you
write your optimized implementation so
you can use all of your primitive
arithmetic
you know whatever whatever you whatever
you need and so yeah that's the class
double away put Adam you type in to
avoid reflection you know write some
optimized code and this is really fast
yeah so you can get big speed ups if you
implement these these these optional
protocols for certain operations so you
know there's standard the standard ease
some gives you you know we know it's
okay this is nanosecond so it's not too
slow but you know reasonable one time if
you just do a straight reduce so without
the protocol overhead you know it's a
bit faster if you aren't specialized be
some you know it's a lot faster you know
yet that's the typical kind of
performance benefit you'd get if you
sort of do the sort of hand optimization
of some of these protocols so we have a
bunch of different internal
implementations in core dot matrix top
one there's a persistent vector one so
it makes closure persistent vectors
workers workers of ways also java double
aways because they put they're quite
useful one worth calling out is the MDL
a this is sort of a general purpose
arbiter shape of a it defines in a way
as it defines in a way is having an
underlying storage array and then it has
a way of calculating indexes using
offsets and strides again I won't go
into all the details but it enabled a
lot of quite clever things so if you
want to transpose a matrix you don't
actually have to change any of the
underlying data you just check you just
fiddle the indexing and the striding and
you know you allocate a very small light
weight object and you've got a
completely new view over the same day to
in a completely different shape so
without doing any copying which is which
is a it's quite a useful format this is
exactly the same format that numpy uses
for their internal representation
because they had to pick one of them
then there's some external
implementations I'll just highlight two
of them and one is one is the vector clj
implementation this is pure JVM it's
designed mostly for small vectors and
small matrices so I used it I built it
originally for simulation and some
machine learning tasks it's pretty fast
and it's probably the most mature
caught-up matrix implementations simply
because I use it to testbed all the
protocols then this clerics and Patricks
is a great library it uses the Blas
libraries which are these sort of native
libraries extremely faster than the gold
standard for the linear algebra work and
they're very fast they're particularly
good if you like doing big linear
algebra work on large matrices and
that's what the encounter package uses
so those probably the two most
interesting implementations at the
moment if any wants to come up with any
good ones or of course very welcome you
can switch implementations very easily
so they're you know standard away you've
just be a persistent vector if you
switch the implementation two vectors
for example then you get like a vectors
type back and you can also explicitly
say what kind of way you want if you
want to set come to implementation is a
bit of a hack it's got a bit of mutable
state but that's if you want to just
work in an environment with one
particular implementation you use that
otherwise you can pass a parameters say
exactly what type you want you can also
mix implementations and I think this is
a this is a fairly unique feature but
you know you can you can add or multi
you can multiply arrays from different
invitations quite happily they can be
mixed the only thing is the behavior
depends on the first argument because
protocols are just doing dispatch on the
on the phone the first type so you get
two slightly different results if you if
you multiplied them in different orders
but you know usually that's okay because
they all implement the same abstraction
and they're all interchangeable future
roadmap well I'd like to get to a
version one release hopefully in the
next few months finalize the API
performance tweaks these kind of things
there's some options to do some
interesting things with data types so
for exam
complex numbers were perfectly good
typed have no ways maybe some workaround
expression compilation so how can you
take complex expressions and produce
optimized functions to run them and then
there's a bunch of domain-specific
extension so you can do similar symbolic
computation differentiations a great
library called expresso that has been
experimenting with that stats functions
you know geometry linear algebra there's
plenty of sort of domain-specific areas
that can do some more work and finally
there's encounter integration and
Cantor's this er you know a great
library which has all the visualization
tools data statistics tools data science
tools and it can't already uses Klat
record a matrix in implementation
there's just a bit of work to make the
to work together together with you even
more nicely most important of course do
invite everyone to get involved as lots
of interesting stuff to do here and you
know I do think this is a pretty
important project if we want closure to
be a top-tier language in the in the
data data science and numerical space so
I think that is something we can achieve
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>