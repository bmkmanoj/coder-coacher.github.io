<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Baird's Counter Example | Coder Coacher - Coaching Coders</title><meta content="Baird's Counter Example - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Baird's Counter Example</b></h2><h5 class="post__date">2015-07-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ijyfo_LneEg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what I'd like to do is actually step
through a concrete counter example that
really does show that a particular way
of doing this kind of training can fail
in the worst case and this is an example
due to Lehmann Baird and it's you know
devious and clever so here's a really
simple MDP you ready it's got six states
and then a seventh absorbing state okay
for each state there's exactly one
action there's no stochasticity there's
no rewards of any kind they're all zero
so this is a really simple example right
sure it's pretty straightforward
alright so then then what is the the
optimal value function and policy for
this example the optimal value function
or the optimal policy I asked for both
there is no optimal policy you just
doesn't matter what you do you just have
to do the one thing there's only one
policy and it is optimal by definition
of optimal in the value function the
proper value function would be zero
everywhere zeros everywhere right
because there's no rewards anywhere so
right so this is super duper simple and
if we had to learn this using Q learning
or something like that presumably this
would actually learn quite quickly what
we're gonna do though to make life
difficult is we're gonna do this in a
function approximation setting linear
function approximation with this
following set of features so there's
there's the seven states in the picture
and associated with each state is a
feature vector and there's eight
features that are part of that feature
zero one two three four five six seven
feature zero is one for all these states
and seven for this for the repeating
state and interestingly we have it set
up so that these other features are
actually completely indicative of which
state you're in so state one has feature
one being true state two has featured
two being true
state 3 has feature three being true or
at least and by true I mean - I guess -
yeah and so that is that that's kind of
the the representation here so state six
looks like one and then zeros everywhere
except for feature six which has a 2 and
then a 0 right so this feature
representation is actually really close
to being just a table yeah so I'm gonna
say near tabular in the sense that if we
get rid of this first feature there's
going to be one weight that
represents the value for state one and
one wait that represents a value for
state two and so forth in one way that
represents a value for state seven and
so it's exactly just a table one one
weight or one table entry per state
right and all we've done this kind of
made it a little bit you know more by
having this extra feature that we ought
to be able to just ignore right and we
should be able to represent things
pretty well you know so in fact I think
that's it's worth asking can we
represent the optimal value function
using this set of weights right so if
our parameters are w0 through w7 is
there a way to set them so that the
value function that we get is the actual
value function for this MDP sure there's
one easy one
one what's that so tomorrow right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>