<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Log Cleaning - Georgia Tech - Advanced Operating Systems | Coder Coacher - Coaching Coders</title><meta content="Log Cleaning - Georgia Tech - Advanced Operating Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Log Cleaning - Georgia Tech - Advanced Operating Systems</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xyalTQzbdUw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I mentioned log cleaning that has to be
done now as client activities go on log
segments evolve on the disk so for
instance if on a particular client node
some blocks were written to the log
segment a log segment may fill up like
this and now it is sitting on the disk
so the blocks that are containing this
log segment corresponds to write to file
blocks one two and five and these file
blocks may belong to different files but
it is okay so as far as the file system
is concerned segment 1 is a contiguous
file it is a log segment but it is a
file and that's the one that is residing
on the disk on the client node this
particular block one may get overwritten
due to activity on the client so now we
have a new content for the same file
block 1 1 double prime and once this new
content has been created this is no
longer valid and so block 1 is
overwritten which means we have to kill
the old block that was in this segment
so there's a new segment in which we
wrote the contents of the new file block
1 double prime and we have to go back to
this old segment and kill this
particular copy of that block which is a
stale copy of the same block so we don't
need that anymore so you can see that as
client activities progress we're going
to create holes in the segments remember
that these segments are persistent data
structures on the disk this segment was
valid at some point of time but once
that particular block one was
overwritten on the client node this
segment contains the latest and the
current copy of that same file block and
therefore we nuke this particular file
block and so we create a hole in this
log segment and subsequently let's say
that the client writes to other file
blocks 3 &amp;amp; 4 so the log segment 2
contains 1 double prime three prime and
4 prime are the blocks that it contains
segment one contains two prime and Phi
prime one prime is not relevant anymore
because it has been overwritten by one
double Prime activities continue on the
client box and a third segment is
created and the third segment contains
two double prime that is this block
number two is overwritten by new
contents two double Prime and when this
happens the file system has to create
another hole by nuking this two prime to
indicate that this block is not relevant
anymore because there is a more recent
version of the block in segment three so
the block two is overwritten killing the
old block now you see that as client
activities progress in the entire
distributed system we are creating a
number of these segments log segments on
the desk and these log segments may
progressively have holes in them because
they have been overwritten by new
contents by activities on the client
machine and this is what log cleaning is
all about it has to do with cleaning up
the discs and getting rid of all of this
junk so that we don't fill up the disk
with unnecessary junk so for example
what we want to do is recognize that we
have three segments here and the
segment's have holes in them and what
we're going to do is we're going to
aggregate all the live blocks from all
of these segments into a new segment so
we've got five Prime from this segment
that is still alive and from this
segment we've got 1 double prime three
prime and four prime that are still
alive and from this segment we've got
two double prime that is still alive so
we coalesced all of the lives blocks
from the existing segments into one new
segment now once we have aggregated this
into one new segment all of the old log
segments can be garbage collected and
that's what log cleaning is all about
and this is very similar if you think
about it to the way we describe cleaning
up the diff files that are created in
the DSM system
Marx and the same thing is happening
except that these data structures are on
the desk and we're conserving the space
on the disk by getting rid of all the
old log segments and garbage collecting
them and saving the space once we have
aggregated all the live blocks in these
segments into a new segment so this is
what log cleaning is all about and in
the distributed file system there is a
lot of garbage that is being created all
across the storage service and we don't
want this to be done by a single manager
we would ideally like it to be done in a
distributed manner this is another step
towards a true distributed file system
by making this lock cleaning activity
also a distributed activity so a lock
cleaners responsibilities include the
following it has to find the utilisation
status of the old log segments then it
has to pick some set of log segments to
clean and once it picks a certain number
of log segments to clean it has to read
all the live blocks that it finds in
these log segments that it has chosen
for cleaning write it into a new log
segment and once it has done that it can
garbage collect all of those log
segments so this is a cleaning activity
that an elephants cleaner has to do and
an X of s they distribute this log
cleaning activity as well now remember
that this log cleaning activity is
happening concurrently with writing to
files on the nodes in the distributed
system so that lots of subtle issues
involved in managing this log cleaning
in parallel with new activity that may
be creating new log segments are writing
to existing log segments I encourage you
to read the paper that I've assigned to
you the exercise paper to get a good
feel for all the subtleties that are
involved in managing log cleaning
concurrently with writing to the files
so in exif s they make the clients also
responsible for law cleaning
there is no separation between client
and server any node can be a client or a
server depending on what it is doing and
each client meaning a node that is
generating log segments it is
responsible for the segment utilization
information for the files that they are
writing after all the activity is
happening at the client end in terms of
creating new files and new file rights
are manifesting as creating blocks and
log segments in XFS and so the clients
are responsible for knowing the
utilization of the segments that are
resident at that client node and since
we have divvied up the entire space of
servers into stripe groups each stripe
group is responsible for cleaning
activity that is in that set of servers
and every stripe group has a leader and
the leader in that stripe group is
responsible for assigning cleaning
services to the members of that stripe
group recall that the manager is
responsible for the integrity of the
files because it is doing metadata
management and requests for reading and
writing files are going to come to the
manager on the other hand the log
cleaning responsibility is going to the
leader of a stripe group and the manager
is the one that is responsible for
resolving conflicts that may arise
between client updates that want to
change some log segments and cleanup
functions that want a garbage collect
some log segments those conflicts are
resolved by the manager these again are
subtle details which I want you to read
carefully in the paper</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>