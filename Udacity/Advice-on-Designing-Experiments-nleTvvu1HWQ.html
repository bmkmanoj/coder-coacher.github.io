<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Advice on Designing Experiments | Coder Coacher - Coaching Coders</title><meta content="Advice on Designing Experiments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Advice on Designing Experiments</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nleTvvu1HWQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">before we conclude this lesson I'd like
to spend a little more time to talk
about designing experiments it sounds
like it's easy we just need to run bunch
of test cases gather the metrics and
show the results not so fast actually
you running tests gettering matrix and
plotting the results is not as
straightforward as it might seem there
isn't actually a lot of thought and
planning that should go into designing
relevant experiments by relevant
experiment I'm referring to an
experiment that will lead to certain
statements about a solution that are
credible that others will believe in and
that are also relevant that they will
cure for for example the paper we
discussed this full of relevant
experiments they're the authors provided
detailed descriptions of each of the
experiments so that we could understand
them and we could believe that those
results are seen and then we were also
able to make well-founded statements
about flash and the amp at model versus
all of the other implementations let's
continue talking about the web server as
an example for which will try to justify
what makes some experiments relevant
well the clients using the web server
they care for the response time how
quickly do they get a web page back the
operators for instance running that web
server that website they care about
throughput how many total client
requests can see that web page over a
period of time so this illustrates that
you will likely need to justify your
solution using some criteria that's
relevant to the stakeholders for
instance and you if you can show that
your solution improves both response
time and throughput everybody is
positively impacted so that's great if
you can show that your solution only
improves response time but doesn't
really affect throughput well okay that
I'll buy that too it serves me some
benefit if I see a solution that
improves response time and actually the
grades throughput that still could be
useful perhaps for this improved
response time I can end up charging
clients more that ultimately will give
me the revenue that I'm losing due to
the negative throughput or maybe I need
to define some experiments in which I'm
trying to understand how is the response
time that the clients see how is it
affected when the overload of the web
server increases when the requests rate
increases so by understanding the
stakeholders and the goals that I want
to meet with respect to these
stakeholders I'm able to define what are
some metrics that I need to pay
attention to and that will give me
insight into useful configurations of
the experiments when you're picking
metrics a rule of thumb should be what
are some of the standard metrics that
are popular in the target domain for
instance for web servers it makes sense
to talk about the client request rate or
the client response time this will let
you have a broader audience more people
will be able to understand the results
and to relate to them even if those
particular results don't give you the
best punch line then you absolutely have
to include metrics that really provide
answers to questions such as why am i
doing this work what is it that I want
to improve or understand by doing these
experiments who is it that cares for
this answering these questions implies
what are the metrics that you need to
keep track off for instance if you're
interested in clients performance
probably the things that you need to
keep track of our things like response
time or number of requests that have
timed out or if you're interested in
improving the operator costs then you
worry about things like throughput or
power costs and similar once you
understand the relevant metrics you need
to think about the system factors that
affect those metrics one aspect will be
things like system resources this will
include hardware resources such as
number and type of CPUs or amount of
memory that's available on the server
machines and also the software specific
resources like number of threads or the
size of certain cues or buffer
structures that are available in the
program then there are a number of
configuration parameters that define the
workload things that make sense for a
web server include the requests trade
the file size the access pattern things
that were varied also in the flesh
experiments and now that you understand
the configuration space a little bit
better make some choices
choose some subset of the configuration
parameters that probably our most
impactful when it comes to changes in
the metrics that you're observing pick
some ranges for these variable
parameters these ranges must also be
relevant don't show that your server
runs well with one two and three threads
so don't vary the number of threads in
your server configuration if you look
out and you see that real-world
deployments they have servers with
thread counts in the hundreds or don't
go and varied the file sizes to have
sizes of ten hundred and one kilobyte if
you look at what's happening in the real
world file sizes range from now maybe
from tens of bytes up to tens of
megabytes and hundreds of megabytes and
beyond so make sure that the Rangers are
representative of reality again these
ranges must somehow correspond to some
realistic scenario that's relevant
otherwise nobody will care for your
hypothetical results that is unless your
hypothetical results are concerned with
demonstrating the best or the worst case
scenario best and worst case scenarios
do bring some value because they in a
way they demonstrate certain limitations
or certain opportunities that are there
because of the system that you've
proposed because of the solution you
have proposed so these are the only
times we're picking a non-realistic
workload make sense like for instance in
the flash paper case they had an example
in which every single one of the request
was accessing one single file and there
was some value in the results that were
obtained through that experiment for the
various factors that you're considering
pick some useful combinations there will
be a lot of experiments where the
results simply reiterate at the same
point it really doesn't make sense to
make endless such results few are good
it's good to confirm that some
observation is valid but including tens
of them it really doesn't make any sense
a very important point compared apples
to apples for instance let's look at one
bad example we have saw one company
nation in which we run an experiment
with a large workload in a small size of
resources and then a second experiment
the second run of the experiment in
which we've changed the workload so now
we have a small workload and then we
have also allocated more resources for
so for instance more threats and then we
look at these results and we see that in
the second case for support a second
experimental run the performance is
better so then we may draw a conclusion
oh well I've increased the resource i
have added more threats and therefore my
performances improved so i must be able
to conclude that performance improves
when I increase the resources that's
clearly wrong I have no idea whether
performance improved because I've added
more resources or because I have changed
the workload so I'm adding I'm using as
much smaller workload in the second case
this is what we mean by make sure that
you're comparing apples to apples
there's no way you can draw a conclusion
between these two experiments and what
about the competition what is the
baseline for the system that you're
evaluating you should think about
experiments that are able to demonstrate
that the system you're designing the
solution you're proposing in some way
improves the state of the art otherwise
it's not clear why use yours and if it's
not really the state of the art then at
least what's the most common practice
that should be improved and perhaps
there's some other benefits over the
state of the art that are valuable or at
least think about evaluating your system
by comparing with some extreme
conditions in terms of the workload or
resource assignment so some of the best
or worst case scenarios that will
provide insights into some properties of
your solution alike how does it scale as
the workload increases for instance</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>