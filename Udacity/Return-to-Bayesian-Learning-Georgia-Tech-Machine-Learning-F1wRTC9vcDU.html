<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Return to Bayesian Learning - Georgia Tech - Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Return to Bayesian Learning - Georgia Tech - Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Return to Bayesian Learning - Georgia Tech - Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/F1wRTC9vcDU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Michael so that was pretty good
with the quiz I want to do another
derivation I want you to help me with it
okay mmm cool okay so Michael we have
similar setup to what we've had before
we're given a bunch of training data
x.i inputs and D I outputs but this time
we're dealing with real valued functions
so the way D eyes are constructed is
there some deterministic function f that
we pass the X through and that gives us
some value and that's really what we're
trying to figure out what is the
underlying F but to make our job a
little bit harder we have noisy outputs
so for every single di that is generated
there's some small error epsilon that is
added to it
now this particular error term is in
fact drawn from a normal distribution
with zero mean and some variance we
don't know what the variance is it's
going to turn out it doesn't actually
matter there's some variance going on
here
the important thing is that there's zero
mean so you got it and it's important
that it's probably the same variance for
all the AIRC all the data that's right
in fact each of these epsilon sub i's
are drawn iid and is that f are we
assuming it's linear nope we're not
assuming that it's linear okay it's just
some function all right I'm with ya
okay you got it yep all right so here's
my question to you what is the maximum
likelihood hypothesis do we know F can I
just say F nope we don't know if all we
see you're exercising DS and I do you
guys but we know there is some
underlying F and we know that it's noisy
according to some normal distribution I
don't know how I would just find that
well let's try to walk it through so we
know how to find the maximum likelihood
hypothesis at least we know an equation
for it the maximum likely hypothesis is
simply the one that maximizes this
expression right that was when we
assumed a uniform prior on the
hypotheses exactly and so we this is
sort of the easiest case to think about
where it turns out that finding the
hypothesis that bets best fits the data
is the same as finding the hypothesis
that describes the date of the best if
you make an assumption about a uniform
distribution or a uniform prior okay so
this is all we have to do now is figure
out what we're gonna do to expand this
expression so what do you think we
should do first
probably the data given the hypothesis
right so each we assumed iid
actually helpfully even wrote that down
so we can expand that into the product
overall the data elements of the
probability of that data element given
the hypothesis and X ok so let's do that
my all to write that out so finding the
hypothesis that maximizes the data that
we see as you point out is just a
product over each of the independent
data that we see for datums so that's
good that's one nice step so we've gone
from talking about all of the data
together to each of the individual
training data that we see so what do we
do next what is the probability of
seeing one particular D sub I given that
we're in a world where HS true so okay
so given that H is true that means that
whatever the corresponding X I is if we
push that through the F function then
then the di
is going to be f of X I plus some error
term and so I guess if we took di minus
FXI that would tell us what the error
term is and then we just need an
expression for saying how likely it is
that we get that much error right so
what is the expression that tells us
that I'm guessing it's something that
uses the normal distribution it probably
has an e in it I think that's absolutely
right so let's be particular about what
you said so when you say that we should
push it to f of X let's be clear that
that's basically what H is supposed to
be our goal here is given all of this
training data let's recover what the
true f of X is and that's what our H is
each of our hypotheses is a guess about
what the true underlying deterministic
function f is so if we have some
particular label some particular value D
sub I that is at variance with that
what's the probability of us seeing
something that far away from the true
underlying F well it's completely
determined by the noise model and the
noise is a Gaussian so we can actually
write down a gasps do you remember what
the equation for a Gaussian is yes it's
exactly something that has an e in it
that's right so I'll see if I'm gonna
start writing it see if you remember any
of what I'm writing now e to the no okay
good one over e to the no okay
square root it's coming back to you now
pi sigma squared okay
time I was gonna put that in after okay
so now you get your e so e to the what
it's gonna be the value which in our
case is like H of X I minus D I yeah and
then I feel like we probably square that
yep and then we divide by Sigma squared
right
really yeah and you're missing one tiny
thing
there's needs to be another twos yes and
in fact it's minus 1/2 got it so this is
exactly the form of the Gaussian of the
normal distribution and what it
basically says is the probability of me
seeing some particular point in this
case D of AI given that the mean is H of
X which is to say that's the underlying
function is exactly this expression e to
the minus 1/2 of the distance from the
mean squared divided by the variance
okay and that's just you either remember
that or you don't but that's just the
definition of a gaucho so that means the
probability of us seeing the data is the
product of the probability of us seeing
each of the data items and that's just a
product of this expression here good now
we need to simplify this we could stop
here because this is true but we really
need to simplify this and I think it's
pretty not too hard to do it's pretty
easy what kind of trick do you think we
would do here to simplify this the first
thing I would do is notice that the 1
over square root 2 Pi Sigma squared
doesn't depend on I at all and maybe
move it outside the pie but then realize
well actually since we're doing an Arg
max anyway it's not gonna have any
impact at all so I would just like cross
that baby out I like that no point in
keeping it all right now I'm hoping that
the other Sigma squared we can make that
go away too so I'm tempted to just cross
it out but I'd ride be much more happy
if I had a good explanation for why
that's okay well so what's the normal
trick so we're trying to maximize a
function right and what you just said is
we can get rid of this particular
constant expression because it doesn't
affect the max what's making it hard for
you to get rid of the Sigma squared here
is that it's being passed through some
exponential and you can't remember off
the top of your head what clever tricks
you can do with constants inside of
Exponential's so it'd be nice if we
could get rid of the exponential very
good so because law
is concave no because it's monotonic
mm-hmm
we can take the log of the whole shebang
so so this is gonna be equal to the Arg
max of the sum of the log of that
expression which is going to move the
thing to the outside and then the log of
EE and so that's all going to be good so
it's gonna be the sum of the superscript
thing the power right so let's write
that down okay so just to be sure that
that was clear to everybody let's just
point out that we basically took the log
of both the natural log of both sides
and so we said instead of trying to find
the maximum of hypothesis or the maximum
likely hypothesis by evaluating this
expression directly we instead evaluated
the log of that expression and as you
recall from intermediate algebra the log
of a product is the same as the sum of
the logs and the log of e to something
is just that thing as long as we do
natural log as long as we do natural log
when we have e if we were doing
something to the 2 to the power of
something we would want to do log base 2
okay got it and you said we did it to
both sides but we didn't really need to
do it to both sides we just needed to do
it inside the thing we're taking the Arg
max that's correct okay so we've got
here so there any other simplifying that
we can do yeah yeah now it seems much
clearer so the the the negative 1/2
divided by Sigma squared all can move
outside the sum because it doesn't
depend on Y at all right and then the
Sigma squared
you said that before you said that that
wasn't gonna turn out to matter both
Sigma squares ended up you know getting
tossed into the rubbish heap that's
right and I want to be careful with the
negative sign like I feel like the half
can go and the Sigma squared can go but
the negative has to stay you're right
the half can go and the Sigma square can
go and that leaves us with this
expression so I've taken gotten rid of
the 1/2 like you suggested got rid of
the Sigma square like you suggested and
I moved the minus sign outside of the
summation and I'm left with this
expression I always thought about
getting rid of that minus sign well how
would you get rid of the minus sign so
the max of a negative is the
is the min right so we can get rid of
the minus sign by just simply minimizing
instead of maximizing that expression we
end up with this expression
nice that's much simpler than where we
started that yi is gone it's much
simpler we got rid of a bunch of these
we got rid of a bunch of turns out
extraneous constants we got rid of
multiplication we did a bunch of stuff
and we ended up with this you know we
got rid of two PI's well kind of sad I
would like some pie mmm
okay so we got this expression and
that's really nice on its own you say
but actually it's even nicer than that
what what does this expression remind
you of Michael this sum of squares this
is exactly it this is in fact the sum of
squared errors which is awesome yeah
whoever decided it would be good idea to
model noise as a Gaussian but it's
really on to something
mm-hmm now think about what this means
Michael we just took using Bayesian
learning a very simple idea of
maximizing likelihood we did nothing but
substitution here and there with the
noise model we got rid of a bunch of
things that we didn't have to get rid of
we cleverly use the natural log notice
that the minus sign can be taken away
with a min and we ended up with sum of
squared error which suggests that all
that stuff we were doing with back
propagation and all these other kinds of
things you were doing with perceptrons
is the right thing to do
minimizing the sum of squared error
which we've just been doing before is in
fact the right thing to do according to
Bayesian learning right in this case
meaning what a Bayesian would say
meaning what a Bayesian would say which
I believe is sort of right by definition
more importantly here it is leave it
well they do frequently oh I see what
you did there no one will get that but
anyway the thing is this is the thing
you should minimize if you're trying to
find the maximum likelihood hypothesis
now I just want to say something that is
beautiful absolutely that you do
something very simple like finding the
maximum like that hypothesis and you end
up deriving sum of squared error so just
to make sure that I'm understanding
because I see some beauty here but maybe
not all of it we didn't talk about what
the hypothesis class here was right and
so the we if you don't know what the
hypothesis class is you're you're kind
of stalled at this point but if we say
the hypothesis classes say linear
functions then what we're saying
is we can do linear regression because
linear regression is exactly about
minimizing the sum of the squares right
so linear regression comes popping out
of this kind of Bayesian perspective
just like that so is that part of what
makes it so cool that is part of what
makes it cool but does think more
generally about gradient descent right
wait gradient descent works you take a
derivative by stepping in this in the
space of the error function which is the
sum of squared errors nicely so you get
creating into sent to yes you get all of
the stuff that people have been doing
now there's a piece of beauty there
which is that we derive things like
gradient descent and linear regression
and all the stuff we were talking about
before and we have an argument for why
it's the right thing to do at least in a
Bayesian sense but there's an even
deeper beauty here which is tied in with
ugliness which is the reason this is the
right thing to do is because of the
specific assumptions that we've made so
what were the assumptions that we made
we assumed that there was some true
deterministic function that was mapping
our X's to our in this case our DS and
that they were corrupted say
transmission error or lying noise or
however you want to think about it
they're corrupted by some noise that has
a very particular form uncorrelated
independently drawn Gaussian noise with
mean zero so the less pretty way of
thinking about it is whenever you're
trying to minimize the sum of squared
error you are in fact assuming that the
data that you have has been corrupted by
Gaussian noise and if it's corrupted by
some other noise or you're actually not
trying to model deterministic function
of this sort then you are in fact
possibly in fact most likely doing the
wrong thing mmm I mean are there other
noise models that lead to some other
kinds of learning sure pick any other
model in here that doesn't like a
Gaussian at all and you would end up
with something else I don't know what
you'd end up with because you know you
couldn't do all these cute tricks with
natural logs but yeah
you would end up with something
different and so one question you might
ask yourself is well if I try to do
minimizing the sum of squared errors for
something for which this model did not
was not the right one what sort of bad
things might happen here let me give you
one example so let's imagine that we're
looking at this here and our X's are I
don't know measurements of people okay
so hide and wait something like that
and in fact let's make it let's make it
a let's make it even simpler that let's
imagine that our exes are height and our
outputs are DS our say weight and what
we're trying to learn is some kind of
function from height to weight now this
doesn't make a lot of sense to have a
true determine but I'm trying to make a
point here so what we're saying here is
that we measure a height and then we
measure weight that there's some simple
relationship between them that's
captured by F but when we measure the
weight we get a sort of noisy version of
that weight okay that seems reasonable
but what's not reasonable is we're
saying our measurement of the weight is
noisy but our measurement of height is
not because of the X's are noisy then
this is not a valid assumption I see so
it seems to work a lot of the time and
we have an argument for when it will
work but it's not clear that this
particular assumption actually makes a
lot of sense in the real world even
though it's in practice it seems to do
just fine okay got it I think so though
I feel like if the error if you put an
error term inside the F along with the X
and F is say linear mm-hmm then maybe it
pops out and it just becomes another
part of the noise term and and it all
still goes through like I feel like
lines are still pretty happy even with
that no I think about lines would be
happy here because lynnie I mean linear
functions are very nicely behaved in
that way but of course they would have
to be the same noise model in order for
it to work the way you want it to work
they'd have to both be gaussian they'd
have to both have zero mean and they'd
have to be independent of one another so
your measuring device that gives you an
error for your height would also have to
give you an independent normal error or
the weight yeah though I feel like my
scale and my yardstick actually are
fairly independent in their Gauss room
Oh mine is clearly Gaussian yeah yeah
well at least they're normal they
normally are mm-hmm okay good so let's
move on the next thing Michael let's
write one more example of this and then
I hope that means you got it okay sure
beautiful</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>