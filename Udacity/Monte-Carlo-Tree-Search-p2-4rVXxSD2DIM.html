<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Monte Carlo Tree Search p2 | Coder Coacher - Coaching Coders</title><meta content="Monte Carlo Tree Search p2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Monte Carlo Tree Search p2</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4rVXxSD2DIM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's do the first stage we're gonna
select well how do we do selection in NM
DP well we follow a policy right so
there's gonna be some policy that we
have let's not worry about where it
comes from right now but some policy
based on experience it tells us what we
ought to be doing in some state so let's
say for the sake of argument I'm in this
particular state and my policy tells me
that I should take a particular action
and I take that action let's say it's
this one and it ends me up in this state
well once we end up in this particular
state we let's say the policy still
tells us what to do and it says take an
action and I'm going to take that action
I end up in yet another state and the
policy says well from here you should
take a particular action so I take that
action and that I end up in this state
ok so I have a policy the policy tells
me how to select what to do what actions
I should take and it gets me through the
tree until I get to a place where I
don't know what to do so at this point
in the tree and in fact just so you know
the way I've drawn this tree all the
leaves or the tree are places where I
don't know where to go next I don't have
a policy for it I've got to figure out
what to do so here's what I'm gonna do
I'm at this state I don't know what to
do next I have to do something so I'm
going to do expansion and then I'm going
to do simulation so here's how the
expansion state the expansion step works
from this state I can take a bunch of
actions right and from those actions I
can get to a bunch of states in fact I
can just look at my transition model and
I can see all the possible next States I
might end up at based upon the actions
that I take and often that's what you
would do that's what you would do in a
normal kind of game AI classical tree
search the problem here of course is
that we might have many many many many
many many many many many many many many
many many many many states and so we
don't want to expand out the tree that
much so instead of what we're going to
do is we're going to kind of do a sort
of sampling step we're gonna say well
for each of the actions that I might
take why don't I take that action and
then simulate for one step what state I
might end up in and then do that for
another action and then do that for
another action and so on and so forth
until I have a few possible next state
action pairs that I might see and maybe
that number is really small maybe it's
six maybe it's one hundred it sort of
depends upon your
state-space so I've done the expansion
step I figured out where I might end up
next given all the actions that I might
take so I'm not drawing this because
there's not a lot of room but each one
of these edges represents some
particular action I could have taken or
I did take in my imagination and the
state that I ended up in or what the
nodes are what the nodes represent and
so each one of these edges has some
action that's associated with it I'm
just not writing it down because of
space okay mm-hmm all right so now I've
got this so I've now expanded sort of
the fringe here's all the things that I
might do and where I might end up next
and now I have to use that kind of
decide what I actually ought to do now
if we were doing the normal kind of tree
search like we talked about with game
search I would just use my evaluation
function but I don't have an evaluation
function so instead I'm gonna do
simulation and what that means is I'm
gonna follow some other policy we
typically have a name for that we call
it the rollout policy and just for the
sake of discussion here let's just say
it's a random policy so I'm gonna say I
took this particular action I ended up
in this particular state and then I'm
just going to behave randomly for a
while and then I'm gonna do the same
thing yeah that looks random I'm gonna
do that here and then I'm gonna do that
here and then I'm gonna do that here and
then I'm gonna do that here and I'm
going to do that here and then we get a
whole bunch of spaghetti
now the spaghetti actually has nice
little bit I know spaghetti is delicious
the spaghetti actually has all kinds of
nice information associated with it as I
move through this path and through this
trajectory by behaving randomly say from
this particular state I actually see a
bunch of rewards along the way right and
I can just take that out as far as I
need to
given my horizon say my discount factor
so I do this for a long time and I
collect rewards along the way that gives
me an estimate of being in this state
and taking this particular action and
since I might take the same action
multiple times and get to these states I
now can do a sort of average over all of
these possibilities get lots and lots of
sort of estimates of this and then that
gives me an estimate of a Q value for
each of the actions that I might take so
all I'm really doing is building an
evaluation function by doing well Monte
Carlo simulation from there so I'm in
the state I took a bunch of actions that
gives me a concrete set of next States
from there
I just behaved randomly for a while I
use that to gather a bunch of estimates
of rewards
I add them all up I averaged them
appropriately and now that gives me an
estimate of the cue function but by the
way it does more than that now that I
have an estimate of the cue function for
this state backed up from here I can
actually back up information all the way
to the top and that updates the estimate
of this particular node this particular
node and this particular node</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>