<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Experimental Results | Coder Coacher - Coaching Coders</title><meta content="Experimental Results - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Experimental Results</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xjid7cg5SaY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Let's now look at
the experimental results.
We will start with
the best case numbers.
To gather the best case numbers,
they used a synthetic load in which
they varied the number of requests that
are issued against the web server,
and every single one of the requests
is for the exact same file.
Like for instance,
every single one of the requests
is trying to get index.html.
This is the best case
because really in reality
clients will likely be asking for
different files, and
in this pathological best case it's
likely basically the file will be in
cash so every one of these requests
will be serviced as fast as possible.
There definitely won't be any need for
any kind of disk IO.
So for the best case experiments,
they measure bandwidth and
they do that, they vary the file
size of zero to 200 kilobytes and
they measure bandwidth as the n, the
number of requests, times the file size
over the time that it takes to process
the n number of requests for this file.
By varying the file size,
they varied the work that both the web
server performs on each request but
also the amount of bytes that
are generated on a request.
You sort of assume that as we increase
the file size that the bandwidth
will start increasing.
So let's look at the results now.
The results show the curves for every
one of the cases that they compare.
The flash results are the green bar,
SPED is the single process
event driven model, MT,
multi-threaded, MP, multi-process,
Apache, this bottom curve, corresponds
to the Apache implementation And Zeus,
that corresponds to the darker blue.
This is the SPED module that
has two instances of SPED so
the dual process event driven model.
We can make the following observations.
First, for all of the curves,
initially when the file size is small,
bandwidth is slow, and as the file size
increases, the bandwidth increases.
We see that all of the implementations
have very similar results.
SPED is really the best.
That's the single process event driven,
and that's expected because it doesn't
have any threads or processes among
which it needs to context switch.
Flash is similar but it performs that
extra check for the memory presence.
In this case,
because this is the single file tree.
So every single one of the requests is
for the single file, there's no need for
blocking I/O.
So none of the helper processes
will be invoked, but nonetheless,
this check is performed.
So that's why we see a little
bit lower performance for flash.
Zeus has an anomaly.
Its performance drops here a little bit,
and
that has to do with some misalignment
for some of the DMA operations.
So not all of the optimizations are
bug-proof in the Zeus implementation.
For the multi-thread and
multi-process models, the performance
is slower because of the context
switching and extra synchronization.
And the performance of
Apache is the worst,
because it doesn't have any
optimizations that the others implement.
Now, since real clients don't
behave like the synthetic workload,
we need to look at what happens
with some of the realistic traces,
the Owlnet and the CS trace.
Let's take a look at
the Owlnet trace first.
First we see that for the Owlnet trace,
the performance is very similar to
the best case with SPED and Flash
being the best and then Multi-thread and
Multi-process and Apache dropping down.
Note that we're not including
the Zeus performance.
The reason for this trend is because
the Owlnet trace is the small trace,
so most of it will fit in the cache and
we'll have a similar behavior like what
we had in the best case, where all the
requests are serviced from the cache.
Sometimes, however,
blocking I/O is required.
It mostly fits in the cache.
Given this,
given the blocking I/O possibility,
SPED will occasionally block.
Where as in Flash their helper processes
will help resolve the problem.
And that's why we see here that the
performance of Flash is slightly higher
than the performance of the SPED.
Now if we take a look at what's
happening with the CS trace, this,
remember, is a larger trace.
So it will mostly require I/O.
It's not going to fed in the cache,
in memory in the system.
Since the system does not support
asynchronous I/O operations,
the performance of SPED
will drop significantly.
So relative to where it was,
close to Flash, now it's significantly
below Flash and, in fact,
it's below the multi-process and
the multi-threaded implementations.
Considering the multi-thread and
the multi-process,
we see that the multi-threaded is
better than the multi-process, and
the main reason for that is that
the multi-threaded implementation has
a smaller memory footprint.
The smaller memory footprint means that
there will be more memory available to
cache files,
in turn that will lead to less I/O, so
this is a better implementation.
In addition, the synchronization and
coordination and
contact switching between threads in a
multi-thread implementation is cheaper,
it happens faster than long processes
in a multi-process implementation.
In all cases, Flash performs best.
Again, it has the smaller memory
footprint compared to multi-threaded and
the multi-process, and that results
in more memory available for
caching files or caching headers.
As a result of that,
fewer requests will lead to a blocking
I Operation which further
speeds things up.
And finally, given that everything
happens in the same address space,
there isn't a need for
explicit synchronization like with the
multi-threaded or multi-process model.
And this is what makes Flash
perform best, in this case.
In both of those cases, Apache performed
worse, so let's try to understand
if there's really an impact of
the optimizations performed in Flash.
And here the results represent
the different optimizations.
The performance that's scattered with
Flash without any optimizations
that's the bottom line.
Then Flash with the path only
optimizations, so the path only, that's
the directory lookup caching, so that's
like the computation caching part.
Then the red line here,
the path and maps, so
this includes caching of the directory
lookup plus caching of the file.
And then the final bar, so
the final line, the black line,
that includes all of the optimization.
So this is the directory lookup,
the file caching as well as
the header computations of the file.
And we see that as we add
some of the optimizations,
this impacts the connection rates of
the performance that can be achieved
by the web server
significantly improves.
We're able to sustain
a higher connection rate
as we add these optimizations.
This tells us two things.
First, that these optimizations
are indeed very important.
And second, they tell us that the
performance of Apache would have been
also impacted,
if it had integrated some of
these same optimizations as
the other implementations.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>