<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Algorithms Requiring Rescaling Solution - Intro to Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Algorithms Requiring Rescaling Solution - Intro to Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Algorithms Requiring Rescaling Solution - Intro to Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oEhevl5DWpk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so Katie what do you think which ones
are the right answers here the right
answer so the the ones that do need
rescaled features will be the SVM and
the k-means clustering so both in them
support vector machines in k-means
clustering
you're really trading off one dimension
to the other when you calculate the
distance so take for example support
vector machines and we look at the
separation line that maximizes the
distance in there you calculate the
distance and that distance calculation
trade-offs
one dimension against the other so if we
make one twice as big as the other it
counts for twice as much the same is
true coincidentally for k-means
clustering we have a cluster center and
you compute the distance of the cluster
center to all the data points and that
distance itself has exactly the same
characterization if you make one
variable twice as big it's going to
count for twice as much so as a result
support vector machines in k means both
are affected by feature V scaling so can
you tell me about the sitting trees and
then your question why aren't they
included decision trees aren't going to
give you a diagonal line like that right
they're going to give you a series of
vertical and horizontal lines so there's
no trade-off you just make a cut in one
direction then a cut in another so you
don't have to worry about what's going
on in one dimension when you're doing
something with the other one if you
squeezed this little area over here to
half the size because you risk a the
feature we are the image line lines well
it aligns different place but the
separation is kind of logically the same
as before it scales with it so there's
no trade-off between these two different
variables and how about linear
regression something similar happens in
linear regression remember that in
linear regression each of our features
is going to have a coefficient that's
associated with it and that coefficient
and that feature always go together
what's going on with feature a it
doesn't affect anything with the
coefficient of feature B so they're
separated in the same way in fact if you
were to double the variable scale of one
specific variable that feature will just
become half as big and the alcove of the
exactly the same as before so it's
really interesting to see and for some
algorithms rescaling is really potential
you can reuse it father's don't even
bother</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>