<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Problem with Minimizing Absolute Errors - Intro to Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Problem with Minimizing Absolute Errors - Intro to Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Udacity/">Udacity</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Problem with Minimizing Absolute Errors - Intro to Machine Learning</b></h2><h5 class="post__date">2015-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/U46D7oEijlI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now you've thought about that let me
show you why this matters let me take
the example of the middle line first my
errors are going to look something like
this I'm sketching them in right now
it's just the distance from each point
to sort of the predicted value for that
point on the regression line and fewer
just summing the absolute value of the
errors you would just sum up all of
these orange distances and you would
have your answer but now let's look at
the top line we could do the same thing
here too so now we start drawing in the
distances so while it would be closer to
all of the points above it it would be
further away from all of the points
below it so taking these two points as
concrete examples it'd be a little bit
closer to the top point a little bit
farther away to the bottom point but
overall the total error for these two
example points would be exactly the same
as a total error for these two points to
the middle line and in fact the same
thing would be true for the bottom
regression line as well and if you have
equal number of points above and below
each of these lines then in general
that's always going to be true there's a
fundamental ambiguity when you use the
absolute value of the errors in terms of
exactly where the regression can fall it
could be anywhere in this range in other
words there can be multiple lines that
minimize the absolute errors there's
only going to be one line that minimizes
the error squared in other words this
ambiguity does not exist when the metric
that we use is not the absolute value of
the distance the absolute value of the
distance squared there's one more thing
that I should add as well this is a
little bit more of a practical concern
and that is this using the sum of the
squared error as the way of finding the
regression also makes the implementation
underneath the hood of the regression
much easier it's much easier to find
this line when what you're trying to do
is minimize the sum of the squared
errors instead of the sum of the
absolute errors and this is something
that we have the luxury of not worrying
about too much when we're using SK learn
to do most of the computational heavy
lifting for us but of course if you're
actually writing the code that goes in
and does the linear algebra to find the
regression or maybe the calculus to find
the result of what your regression
answer should be then this is a big
concern to you this is another reason
why are going to be minimizing the sum
of the squared error is because the
computationally
just much easier to do that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>